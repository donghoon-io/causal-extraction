<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntactically Diverse Adversarial Network for Knowledge-Grounded Conversation Generation</title>
				<funder ref="#_T4UwvEb">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_vfgSkJA #_bPcpjcn #_er72Urm #_eQz33Ms">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fuwei</forename><surname>Cui</surname></persName>
							<email>fuweicui@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
							<email>dihui@toshiba.com.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Toshiba (China) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<region>China</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongjie</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazushige</forename><surname>Ouchi</surname></persName>
							<email>kazushige.ouchi@toshiba.co.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toshiba (China) Co</orgName>
								<address>
									<settlement>Ltd</settlement>
									<region>China</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
							<email>zliu@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
							<email>jaxu@bjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntactically Diverse Adversarial Network for Knowledge-Grounded Conversation Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative conversation systems tend to produce meaningless and generic responses, which significantly reduce the user experience. In order to generate informative and diverse responses, recent studies proposed to fuse knowledge to improve informativeness and adopt latent variables to enhance the diversity. However, utilizing latent variables will lead to the inaccuracy of knowledge in the responses, and the dissemination of wrong knowledge will mislead the communicators. To address this problem, we propose a Syntactically Diverse Adversarial Network (SDAN) for knowledgegrounded conversation model. SDAN contains an adversarial hierarchical semantic network to keep the semantic coherence, a knowledgeaware network to attend more related knowledge for improving the informativeness and a syntactic latent variable network to generate syntactically diverse responses. Additionally, in order to increase the controllability of syntax, we adopt adversarial learning to decouple semantic and syntactic representations. Experimental results show that our model can not only generate syntactically diverse and knowledge-accurate responses but also significantly achieve the balance between improving the syntactic diversity and maintaining the knowledge accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, conversation generation has become a research hotspot because of its wide application, such as voice assistant, customer service assistant and chat robot <ref type="bibr">(Cui et al., 2021)</ref>. The goal of conversation model is to generate diverse and informative responses like human. Although the existing models have achieved promising performance, they still suffer from generating general and meaningless responses <ref type="bibr" target="#b28">(Wu et al., 2020)</ref>, which significantly disrupt the user experience. Consequently, it is * Jinan Xu is the corresponding author.</p><p>very crucial and urgent to generate high-quality responses.</p><p>To generate high-quality responses, many researches have been proposed to improve informativeness or diversity of responses. For informative responses, some early studies utilize context information to the decoding process <ref type="bibr" target="#b25">(Sordoni et al., 2015;</ref><ref type="bibr" target="#b33">Yao et al., 2015)</ref>. After that, researchers extract topic information from context <ref type="bibr" target="#b9">(Hedayatnia et al., 2020)</ref> or add external topic to the decoder <ref type="bibr" target="#b30">(Xing et al., 2016</ref><ref type="bibr" target="#b31">(Xing et al., , 2017))</ref>. Lately, researchers focus on fusing knowledge into conversation model <ref type="bibr" target="#b7">(Ghazvininejad et al., 2018;</ref><ref type="bibr" target="#b36">Zhou et al., 2018;</ref><ref type="bibr" target="#b14">Lian et al., 2019;</ref><ref type="bibr" target="#b28">Wu et al., 2020;</ref><ref type="bibr" target="#b16">Lin et al., 2020)</ref>. Although the knowledge-grounded model can generate informative responses with accurate knowledge, which may generate responses that lack diversity. For diverse responses, previous studies generally adopt beam search algorithm <ref type="bibr">(Li et al., 2016b)</ref> and its variants to improve diversity <ref type="bibr" target="#b27">(Vijayakumar et al., 2016)</ref>. In recent year, latent variables are widely used in conversation model, and can significantly enhance the diversity <ref type="bibr" target="#b22">(Serban et al., 2017;</ref><ref type="bibr" target="#b35">Zhao et al., 2017;</ref><ref type="bibr" target="#b18">Park et al., 2018;</ref><ref type="bibr" target="#b24">Shen et al., 2019;</ref><ref type="bibr" target="#b19">Ruan et al., 2019;</ref><ref type="bibr">Cui et al., 2021)</ref>, and generative adversarial networks (GAN) <ref type="bibr" target="#b32">(Xu et al., 2018)</ref> and reinforcement learning (RL) <ref type="bibr" target="#b20">(Sankar and Ravi, 2019)</ref> are also adopted to generate diverse responses. Although the introduction of hidden variables can increase diversity while maintaining semantic consistency, it may lead to inaccuracy in decoding specific knowledge, because the latent variables may generate semantically similar responses with a certain probability. For example, as shown in Table <ref type="table" target="#tab_0">1</ref>, there is a song name (Be Your Girl All Your Life) in query, where the response R1 will be generated by the variational latent model. In R1, the song name may be decoded as Be Your Woman in The Next Life, which is another song name. Then, the wrong responses will be generated. How to improve diversity of responses and preserve the accuracy of knowledge simultaneously is a huge challenge in knowledge-grounded conversation generation.</p><p>To tackle this challenge, we propose a Syntactically Diverse Adversarial Network (SDAN) for knowledge-grounded conversation generation. First, we utilize a hierarchical network to model the semantic information of context and an adversarial network to prevent semantic information from affecting syntactic information. Next, we adopt a knowledge-aware network to represent the knowledge related to the query, which takes attention mechanism to capture more important knowledge. Then, we design a syntax encoder to model syntax information and use a latent variable to keep the syntactic diversity. Finally, the encoded knowledge, syntax and context are concatenated together to initialize the decoder. Additionally, we employ adversarial network to keep the separation of syntax and semantics to prevent their mutual influence. The results of experiments on KdConv datasets show that our model can achieve better trade-off between improving diversity and maintaining knowledge accuracy than baselines.</p><p>Our main contributions are as follows:</p><p>â€¢ To best of our knowledge, we are the first to adopt syntactic latent variable to simultaneously improve the diversity and maintain the accuracy of knowledge in knowledgegrounded conversation generation, and propose a novel Syntactically Diverse Adversarial Network.</p><p>â€¢ Our model gains competitive diversity scores and the best knowledge-accurate scores than baselines.</p><p>â€¢ We further conduct extensive ablation studies on the proposed several components. These analyses explore intuitive interpretability of why do the adversarial network, knowledge and syntactic latent variable have an effect on our model, and provide a reference for future model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variational Autoencoder</head><p>Since our model adopts latent variables, we briefly review the architecture of Variational Autoencoder (VAE) (Kingma and Welling, 2014), a generative model which utilizes a latent variable z to encode the information of the utterance x, and then decodes the original x from z. The probability of x can be computed as follows:</p><formula xml:id="formula_0">p(x) = p(x, z)d z = p(z)p(x|z)d z (1)</formula><p>where p(z) is the prior distribution, p(x|z) is given by the decoder. Since the integral is unavailable in closed form <ref type="bibr" target="#b1">(Blei et al., 2017)</ref>, the VAE is trained by maximizing the evidence lower bound (ELBO), which is defined as follows:</p><formula xml:id="formula_1">logp(x) â‰¥ ELBO = E q(z|x) [logp(x|z)] -D KL (q(z|x)||p(x)) (2)</formula><p>where q(z|x) is posterior distribution obtained by the encoder, E is mathematical expectation, D KL (â€¢||â€¢) indicates the Kullback-Leibler(KL) Divergence which is utilized to represent the similarity of two distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generated Adversarial Learning</head><p>Generated Adversarial Learning (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> is widely used in the generation of image and text, which consists of a Generator (G) </p><formula xml:id="formula_2">â„ !"# $%&amp; ğ‘¢ !"# â„ !"' $%&amp; ğ‘¢ !"' ğ‘¢ ! ğ‘¢ !"# ğ¾ !"' ğ¾ !"# SynEncoder ğ‘  !"'</formula><formula xml:id="formula_3">min G max D V (D, G) =</formula><p>(3)</p><formula xml:id="formula_4">E xâˆ¼P data (x) [logD(x)] + E zâˆ¼Pz(z) [log(1 -D(z))]</formula><p>where G is utilized to obtain the generated distribution p g (x) from noisy distribution p z (z) to approximate the true distribution p data (x), and D is used to distinguish the distribution of p g (x) and p data (x). G attends to reduce the value of V to make the generated distribution unrecognized, but D intends to enlarge the value of V to effectively identify the true and false classes of data. In the process of training, G and D are optimized alternately, and the optimal solution can be achieved by iterating for many times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation and Model Overview</head><p>Formally, we assume the training data D consists of N samples of conversations{c 1 , c 2 , ..., c N } where each c i is a sequence of utterances {u 1 , u 2 , ..., u n } which is expressed as {u t } n t=1 . We consider the {u t } n-1 t=1 as query, the {u t } n t=2 as response. Each query has m related knowledge (k 1 , . . . , k m )ï¼Œwhere each knowledge k i is a triplet (h i , r i , t i ), and h i , r i and t i are the head entity, the relation and the tail entity, respectively. Each utterance has the syntax s i . The goal of our method is to generate informative and diverse responses, so we will fuse knowledge and syntax to the generative model.</p><p>The overview of SDAN is shown in Figure <ref type="figure">1</ref>. The Adversarial Hierarchical Semantic Network consists of encoder layer and context layer, which is utilized to model the semantic information. The Knowledge-Aware Network adopts attention mechanism to focus the more important knowledge. The Syntactically Latent Variable Network adopts a latent variable to generate responses with diverse syntax. Finally, the semantic information, knowledge and syntax from above three networks are concatenated together to the Decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Hierarchical Semantic Network</head><p>The Hierarchical Semantic Network consists of two layer neural networks. Each input utterance u i is encoded into a vector h enc t by the encoder RNN, which is shown as follows:</p><formula xml:id="formula_5">h enc t = f enc Î¸ (u t ) t = 1, . . . , n<label>(4)</label></formula><p>where f enc Î¸ (â€¢) is a bidirectional gated recurrent unit (BiGRU).</p><p>The context vector h ctx t represents the historical information, which updates its hidden states by using the encoder vector h enc t and is calculated by:</p><formula xml:id="formula_6">h ctx t = f ctx Î¸ (h ctx t-1 , h enc t )<label>(5)</label></formula><p>where the initial value of h ctx t is 0. The semantic information from the hierarchical semantic network may contain the syntactic information, which can lead to poor syntactic controllability. In order to solve this problem, we introduce adversarial network to prevent semantic information from containing syntactic information. Specifically, we introduce a discriminator to predict the syntax tree sequence s t according to the semantic information of the context h ctx t . The context layer and encoder layer can be regarded as the generator. The generator is trained to learn the semantic information to prevent the discriminator predicting the syntax from the semantic information and to cheat the discriminator by maximizing the adversarial loss, that is, minimizing the following formulaï¼š</p><formula xml:id="formula_7">loss adv syn = t=n t=1 logp adv (s t |h ctx t )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge-Aware Network</head><p>The knowledge can be retrieved from the knowledge base to select the related knowledge. The knowledge used in this paper is given in the dataset and one query may have multiple knowledge, so we employ attention mechanism to pay more attention to the important knowledge, which is similar to <ref type="bibr" target="#b37">(Zhou et al., 2020)</ref>. We assume that there is m related knowledge (k 1 , . . . , k m ) given for a query u t , and each knowledge k i is a triplet (h i , r i , t i ). First, we treat the average word embeddings of h i and r i as the key vector kv i (i = 1, . . . , m). Then, we use the word embedding of the query u to attend to kv i :</p><formula xml:id="formula_8">Î± i = softmax i (emb(u t ) T kv i ) (7)</formula><p>where emb(â€¢) is the embedding vector, softmax(â€¢) is a generalization of the logistic function which normalizes all values between 0 and 1. After that, we obtain the knowledge k t by summing all the weighted tail entity t i :</p><formula xml:id="formula_9">k t = i=m i=1 Î± i t i (8)</formula><p>Finally, we utilize a BiGRU to encode the knowledge to model the knowledge vector h kno t , which is computed as follows:</p><formula xml:id="formula_10">h kno t = f kno Î¸ (k t )<label>(9)</label></formula><p>where f kno Î¸ (â€¢) is a BiGRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Syntactically Latent Variable Network</head><p>Each utterance contains syntactic information, which is usually represented by syntactic tree. The syntactic tree can be modeled by a neural network or obtained by the parser toolkit. In this paper, we first utilize the Stanford Parser toolkit<ref type="foot" target="#foot_0">foot_0</ref> to process all the utterances in the dataset to get their syntactic tree sequences, which contain the syntactic tokens and the brackets (the brackets represent the syntactic structures). Then, a SynEncoder is employed to represent the syntactic vector h syn t , which is shown as follows:</p><formula xml:id="formula_11">h syn t = f syn Î¸ (s t )<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">f syn Î¸ (â€¢) is a BiGRU, s t is the syntactic tree sequence.</formula><p>Finally, in order to generate syntactically diverse responses, we adopt a syntactic latent variable z s t to control the syntactic information. We define the prior distribution of z s t as:</p><formula xml:id="formula_13">p Î¸ (z s t |s t ) = N (z|Âµ s , Ïƒ s 2 I)<label>(11)</label></formula><p>where N (â€¢) is a Gaussian distribution, Âµ s and Ïƒ s are the means and the diagonal variances of the prior distributions, respectively, which are calculated as:</p><formula xml:id="formula_14">Âµ s = MLP Î¸ (h syn t ) (12) Ïƒ s = Softplus(Âµ s ) (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where </p><formula xml:id="formula_16">q Ï† (z s t |s t , s t+1 ) = N (z|Âµ s , Ïƒ s 2 I)<label>(14)</label></formula><p>where</p><formula xml:id="formula_17">Âµ s = MLP Ï† (h syn t , h syn t+1 )<label>(15)</label></formula><p>Ïƒ s = Softplus(Âµ s ) (16)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decoder</head><p>From the three networks mentioned above, we obtain the representation of semantics, knowledge and syntax, We concatenate them together to be the initial state of the decoder,which is shown as follows:</p><formula xml:id="formula_18">h dec ini = [h ctx t , h kno t , z s t ]<label>(17)</label></formula><p>Finally, we output the response u t+1 , which is shown as follows:</p><formula xml:id="formula_19">u t+1 = f dec Î¸ (h dec ini )<label>(18)</label></formula><p>where f dec Î¸ (â€¢) is a GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training Objective</head><p>Because of the existence of latent variables in our model, the training objective of latent variables is to maximize the following ELBO:</p><formula xml:id="formula_20">ELBO = loss rec + loss KL -E q Ï† [log p Î¸ (u n |{u t } n-1 t=1 , {k t } n-1 t=1 , {s t } n-1 t=1 )] + D KL (q Ï† (z s |{s t } n t=1 )||p Î¸ (z s |{s t } n-1 t=1 )) (19)</formula><p>where loss rec is the reconstruction loss, loss KL is the KL divergence to represent the similarity of the posterior distribution and the prior distribution of the latent variable z s t . Then, the final objective is to minimize the following formula:</p><formula xml:id="formula_21">min[loss adv syn ] + min[-ELBO]<label>(20)</label></formula><p>where the two losses are optimized iteratively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Design</head><p>We evaluate the generated responses from two aspects: automatic evaluation metrics and manual evaluation metrics.</p><p>For automatic evaluation metrics, we utilize four classes of evaluation metrics: Token-level Metrics: Perplexity (PPL) is used to evaluate whether the generated response is grammatical and fluent. Overlapping-based Metrics: We adopt the BLEU-2/3 <ref type="bibr" target="#b17">(Papineni et al., 2002)</ref> to evaluate the reconstruction performance, which can reflect how well the model could preserve information from knowledge and ground truth response. Embedding-based Metrics: Average, Greedy and Extrema are adopted to measure the semantic similarity between words in generated response and the ground truth. Diversity: We employ Dist-1/2 <ref type="bibr">(Li et al., 2016a)</ref> to measure the diversity of the responses, which are defined as the ratio of distinct uni/bi-grams. Knowledge Utilization: E match is the averaged number of the entities matched with the related knowledge triplets in the responses <ref type="bibr" target="#b37">(Zhou et al., 2020;</ref><ref type="bibr" target="#b28">Wu et al., 2020)</ref>.</p><p>For manual evaluation metrics, three evaluation metrics are adopted, which range from 1 to 5:</p><p>Coherence (Cohe) denotes the semantic similarity of response and query: â‘  score 1: The response and query are completely different and semantically different. â‘¡ score 2: The response and query are completely different, but a little semantically similar. â‘¢ score 3: The response and query are partly the same, but semantically similar. â‘£ score 4: The response and query are mostly the same, but semantically very similar. â‘¤ score 5: The response and query are exactly the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Average  Fluency (Flu) represents the grammatical problem: â‘  score1: The response can not understand. â‘¡ score2: The response has more than four grammatical errors and is difficult to understand. â‘¢ score3: The response has three or four grammatical errors and is not fluent. â‘£ score4: The response has one or two grammatical errors and is fluent. â‘¤ score5: The response has no grammatical errors and is fluent.</p><p>Informativeness (Info) is designed to measure whether the response is relevant to the knowledge information: â‘  score 1: The response does not contain the relevant knowledge and relevant to the context. â‘¡ score 2: The response does not contain the relevant knowledge, but relevant to the context. â‘¢ score 3: The response only contains one relevant knowledge. â‘£ score 4: The response contains part of the relevant knowledge. â‘¤ score 5: The response contains all the relevant knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of Automatic Evaluation</head><p>The results of automatic evaluation metrics are shown in Table <ref type="table" target="#tab_5">2</ref>. We analyze the results from the following perspective:</p><p>The influence of semantic and syntactic latent variables:</p><p>1) Although our improvement on some domains is limited, but we achieve balance between syntactic diversity and knowledge accuracy.</p><p>2) In terms of embedding-based metrics (Average, Extrema and Greedy), there is little difference among the three models. So we can conclude that adopting the semantic and syntactic latent variables  have little effect on the semantics of responses.</p><p>3) Compared with HRED+know, VHRED +know obtains lower BLEU-k scores and higher Dist-k scores, and SDAN performs better in these two aspects. We can find that although semantic hidden variables can significantly improve the diversity, but also greatly reduce the accuracy of responses. But the syntactic latent variables can not only improve the diversity but also enhance the accuracy of responses. The reason is that semantic latent variables may utilize other words with similar semantics, which will lead to the inaccuracy of the knowledge, while the syntactic latent variables only change the syntax of responses, which has no influence on the accuracy of knowledge. The "-adv", "-know" and "-syn" mean that we eliminate the adversarial network (discriminator), knowledge-aware network and syntactically latent variable network, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>4) It can be seen that the Dist-k scores of VHRED+know is higher than SDAN, which indicates that semantic latent variables are more effective than syntactic latent variables in improving diversity. The reason may be that the vocabularies of semantics are much larger than syntactic vocabularies.</p><p>5) For PPL, VHRED+know obtains the best results and SDAN performs better than HRED+know, which denotes that both of the semantic and syntactic latent variables have the positive influence on generating fluent responses and the former works better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison between domains:</head><p>The performance on BLEU-k improves from film domain to travel domain, because there are 1,837 entities and 318 relations in the film domain and 699 entities and 7 relations in the travel domains. The more diverse knowledge increases the difficulty of knowledge selection for knowledgeaware network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results of Manual Evaluation</head><p>The results of manual evaluation metrics are shown in Table <ref type="table" target="#tab_7">3</ref>. The scores of three evaluation metrics range from 1 to 5. Additionally, we choose 3 annotators to evaluate the responses generated by the above models, and randomly select 50 conversa-tions from the test set.</p><p>For Coherence, the three models are similar in maintaining semantic consistency, which agrees with the results of automatic evaluation. VHRED+know achieves the best Fluency scores and the worst Informativeness scores, which proves that the semantic latent variable can lead to the inaccuracy of knowledge, but can improve the fluency of responses again. Our model obtains the competitive Coherence, Fluency scores and the best Informativeness scores, which indicate that our model can not only generate informative responses but also keep the semantic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To analyze which components are driving the improvements, we present an ablation study in Table <ref type="table" target="#tab_8">4</ref>. We eliminate the adversarial network (discriminator), knowledge-aware network and syntactically latent variable network one by one, which result in four models. The four models are represented as "-adv", "-adv-know", "-adv-syn" and "-adv-knowsyn" respectively. By comparing the four models with our SDAN, we can make some conclusions as follows:</p><p>1) After eliminating the adversarial network (comparing SDAN with "-adv"), "-adv" has worse performance than SDAN, which indicates that the  adversarial network is effective to enhance the semantics, knowledge accuracy, distinct and fluency, and it is necessary to decouple semantics from syntax.</p><p>2) When further removing the knowledge-aware network (comparing "-adv" with "-adv-know"), all the results are worse again, especially the decline of BLEU-k scores is obvious, which denotes that introducing knowledge is essential for conversation generation.</p><p>3) While eliminating the syntactically latent variable (comparing "-adv" with "-adv-syn" or comparing "-adv-know" with "-adv-know-syn"), it can be seen that there is a slight improvement in the scores of Average, Extrema, Greedy and BLEU-k, and a bit of lower in the scores of Dist-k, which prove that adopting syntactically latent variable can slightly reduce the semantic consistency and knowledge accuracy, but improve the diversity. Moreover, when the syntactic information and semantic representation exist simultaneously, it certainly need to decouple them by utilizing adversarial network to prevent the influence between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>The generated responses of HRED, all baselines and our model sampled from test set in film domain are shown in Table <ref type="table" target="#tab_10">5</ref>. As it can be seen, HRED tends to generate generic or irrelevant responses. After introducing knowledge, HRED+konw can generate coherent and informative responses related to the given knowledge. When adopting semantic latent variable, VHRED+know prefer generating responses relevant to the context. while utilizing knowledge and syntactically latent variable, our model can generate knowledge-coherent and diverse responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Sequence-to-sequence (Seq2Seq) model <ref type="bibr" target="#b26">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b23">Shang et al., 2015)</ref> with attention <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b4">Cho et al., 2015)</ref> has been widely used in the conversation generation. However, models tend to generate meaningless and generic responses <ref type="bibr" target="#b22">(Serban et al., 2017)</ref>. To alleviate this issue, researchers have utilized context <ref type="bibr" target="#b25">(Sordoni et al., 2015;</ref><ref type="bibr" target="#b33">Yao et al., 2015)</ref>, topic information <ref type="bibr" target="#b30">(Xing et al., 2016</ref><ref type="bibr" target="#b31">(Xing et al., , 2017;;</ref><ref type="bibr" target="#b29">Wu et al., 2019)</ref> or knowledge <ref type="bibr" target="#b14">(Lian et al., 2019;</ref><ref type="bibr" target="#b28">Wu et al., 2020;</ref><ref type="bibr" target="#b16">Lin et al., 2020)</ref> to enhance response quality. The studies of knowledge-grounded conversation generation mainly focus on the method of knowledge retrieval <ref type="bibr" target="#b14">(Lian et al., 2019)</ref> or knowledge fusion <ref type="bibr" target="#b28">(Wu et al., 2020;</ref><ref type="bibr" target="#b16">Lin et al., 2020;</ref><ref type="bibr" target="#b34">Ye et al., 2020;</ref><ref type="bibr" target="#b15">Liang et al., 2021)</ref> with copy mechanism. The knowledge-grounded models can improve the ac-curacy of knowledge, but the responses generated by some of them may lack the diversity, which is also a significant reason for generating generic responses.</p><p>Recently, to tackle the lack of diversity, researchers have begun to introduce the beam search algorithm <ref type="bibr">(Li et al., 2016b;</ref><ref type="bibr" target="#b27">Vijayakumar et al., 2016)</ref> to decoder or latent variables <ref type="bibr" target="#b22">(Serban et al., 2017;</ref><ref type="bibr" target="#b18">Park et al., 2018;</ref><ref type="bibr" target="#b24">Shen et al., 2019)</ref>. Adopting latent variables can significantly improve the diversity of responses, but it will lead to the inaccuracy of knowledge. To the best of our knowledge, this problem has not been investigated in conversation generation so far.</p><p>Different from all the models mentioned above, our approach introduces syntax to conversation generation. We propose a syntactically diverse adversarial network, which utilizes latent variables to control the syntactic diversity. Additionally, we utilize adversarial learning to preserve the disentanglement of syntax and semantics for preventing them from influencing each other. Our model can not only generate sentences with diverse syntax but also keep the accuracy of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a Syntactically Diverse Adversarial Network for knowledge-grounded conversation model, which utilizes adversarial hierarchical semantic network, knowledge-aware network and syntactical latent variable network to model the semantics, knowledge and diverse syntax information. Moreover, our model adopts adversarial learning to enhance the controllability of syntax. According to automatic and manual evaluation, our model competitively improves the quality of generated responses, and obtains better trade-off between improving the diversity and preserving the knowledge accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Who is singer of Be Your Girl All Your Life? The singer of Be Your Girl All Your Life is Elva Hsiao. The singer of Be Your Woman in The Next Life is Meizi Long. An illustrative example. Response1 shows the response generated with semantic latent variable, Re-sponse2 shows the response generated with syntactic latent variable.âˆš and Ã— denote that the responses are right and wrong, respectively.</figDesc><table><row><cell>Example</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-Aware Network Syntactically Latent Variable Network Adversarial Hierarchical Semantic Network</head><label></label><figDesc>K i is the relevant knowledge. s i is the syntax of u i obtained by the Parser Toolkit. z s i denotes the syntactic latent variable. The more details of SDAN are shown in Section 3.</figDesc><table><row><cell></cell><cell>Decoder</cell><cell>Decoder</cell><cell></cell></row><row><cell>ğ‘§ ("' )</cell><cell cols="2">SemEncoder KnowEncoder KnowEncoder SemEncoder KnowledgeDiscriminator Parser Toolkit</cell><cell>Discriminator Parser Toolkit</cell><cell>SynEncoder ğ‘  !"# ) ğ‘§ ("#</cell></row><row><cell cols="5">Figure 1: Overview of SDAN, combining a adversarial hierarchical semantic network to model the semantics,</cell></row><row><cell cols="5">a knowledge-aware network to represent knowledge and a syntactically latent variable network to control the</cell></row><row><cell cols="4">diversity of syntax. u i denotes the i-th utterance. h ctx i represents the context information. and a Discriminator (D). The training objective of</cell></row><row><cell cols="2">GAN is defined as follows:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MLP Î¸ (â€¢) is a feed-forward neural network and Softplus(â€¢) is an activation function which can keep the result positive.</figDesc><table><row><cell>For the posterior distribution of z s t , we use h s t and h s t+1 to calculate it in training set (h s t in test</cell></row><row><cell>set):</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation results on KdConv Corpus. The best results are in bold. The "+know" means the models are enhanced by the knowledge base. The VHRED+know and SDAN have the semantic and syntactic latent variables, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Manual evaluation results on Kdconv Corpus. Îº is the Fleiss' kappa value.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on KdConv Corpus.</figDesc><table><row><cell></cell><cell cols="3">Average Extrema Greedy</cell><cell>BLEU-2/3</cell><cell>Dist-1/2</cell><cell>PPL</cell><cell>E match</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Film</cell><cell></cell><cell></cell></row><row><cell>SDAN(Ours)</cell><cell>0.838</cell><cell>0.637</cell><cell>0.683</cell><cell>9.614 / 5.795</cell><cell cols="2">0.243 / 0.551 18.714</cell><cell>1.02</cell></row><row><cell>-adv</cell><cell>0.828</cell><cell>0.615</cell><cell>0.658</cell><cell>9.106 / 5.491</cell><cell cols="2">0.240 / 0.529 17.848</cell><cell>0.97</cell></row><row><cell>-adv-know</cell><cell>0.774</cell><cell>0.546</cell><cell>0.575</cell><cell>4.145 / 2.275</cell><cell cols="2">0.109 / 0.231 20.551</cell><cell>0.51</cell></row><row><cell>-adv-syn</cell><cell>0.842</cell><cell>0.638</cell><cell>0.681</cell><cell>9.454 / 5.503</cell><cell cols="2">0.238 / 0.488 27.950</cell><cell>0.99</cell></row><row><cell>-adv-know-syn</cell><cell>0.814</cell><cell>0.587</cell><cell>0.635</cell><cell>4.491 / 2.315</cell><cell cols="2">0.031 / 0.044 22.615</cell><cell>0.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Music</cell><cell></cell><cell></cell></row><row><cell>SDAN(Ours)</cell><cell>0.843</cell><cell>0.648</cell><cell>0.718</cell><cell cols="3">14.882 / 10.150 0.288 / 0.595 15.006</cell><cell>1.21</cell></row><row><cell>-adv</cell><cell>0.838</cell><cell>0.635</cell><cell>0.704</cell><cell cols="3">14.082 / 9.150 0.276 / 0.575 15.161</cell><cell>0.16</cell></row><row><cell>-adv-know</cell><cell>0.811</cell><cell>0.577</cell><cell>0.611</cell><cell>4.623 / 2.440</cell><cell cols="2">0.126 / 0.234 19.632</cell><cell>0.53</cell></row><row><cell>-adv-syn</cell><cell>0.840</cell><cell>0.645</cell><cell>0.714</cell><cell cols="3">14.653 / 9.799 0.274 / 0.567 25.359</cell><cell>1.19</cell></row><row><cell>-adv-know-syn</cell><cell>0.794</cell><cell>0.548</cell><cell>0.591</cell><cell>4.754 / 2.472</cell><cell cols="2">0.026 / 0.034 19.818</cell><cell>0.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Travel</cell><cell></cell><cell></cell></row><row><cell>SDAN(Ours)</cell><cell>0.852</cell><cell>0.689</cell><cell>0.761</cell><cell cols="3">22.451 / 18.030 0.276 / 0.571 8.069</cell><cell>1.25</cell></row><row><cell>-adv</cell><cell>0.832</cell><cell>0.639</cell><cell>0.711</cell><cell cols="3">21.987 / 17.993 0.256 / 0.531 8.148</cell><cell>1.21</cell></row><row><cell>-adv-know</cell><cell>0.766</cell><cell>0.547</cell><cell>0.575</cell><cell>3.772 / 1.935</cell><cell cols="2">0.148 / 0.262 11.320</cell><cell>0.54</cell></row><row><cell>-adv-syn</cell><cell>0.858</cell><cell>0.684</cell><cell>0.761</cell><cell cols="3">22.499 / 18.001 0.268 / 0.547 10.604</cell><cell>1.25</cell></row><row><cell>-adv-know-syn</cell><cell>0.747</cell><cell>0.500</cell><cell>0.595</cell><cell>3.561 / 1.935</cell><cell cols="2">0.053 / 0.061 11.176</cell><cell>0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>è¿˜æ˜¯ä¸€éƒ¨ç”±ç¾å›½åˆ¶ç‰‡çš„å‘¢ï¼Œä½ çŸ¥é“ä¸»æ¼”éƒ½æœ‰è°å—ï¼Ÿ It's a movie made in America. Do you know who the stars are? SDAN(Ours): æˆ‘çŸ¥é“ï¼Œæ˜¯ç¾å›½çš„å…‹æ—ç‰¹â€¢ä¼Šæ–¯ç‰¹ä¼å¾·åˆ¶ç‰‡çš„ã€‚ ä½  çŸ¥é“è¿™éƒ¨ç”µå½±çš„ç¥¨æˆ¿æ€ä¹ˆæ ·å—ï¼Ÿ I know, it's made by Clint Eastwood in the United States. Do you know what the box office of the film is like?</figDesc><table><row><cell>Conversation (Film)</cell><cell>Head Entity</cell><cell cols="2">Knowledge Triple Relation</cell><cell>Tail Entity</cell></row><row><cell>User1: çœ‹è¿‡ã€Šå®Œç¾çš„ä¸–ç•Œã€‹å—ï¼Ÿ</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Have you seen A Perfect World?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>User2: çœ‹è¿‡ï¼Œè¿™æ˜¯ä¸€éƒ¨å¾ˆå¥½çœ‹çš„æƒŠæ‚šç‰‡ã€‚</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yes, it's a good thriller.</cell><cell></cell><cell>ç±»å‹</cell><cell></cell><cell>æƒŠæ‚šç‰‡</cell></row><row><cell>User1: å—¯ï¼Œæˆ‘ä¹Ÿå–œæ¬¢çœ‹è¿™ç±»çš„ç”µå½±ï¼Œ138åˆ†é’Ÿçš„ç‰‡é•¿æœ‰ç‚¹å—æŠ˜ç£¨</cell><cell></cell><cell>Type</cell><cell></cell><cell>Thriller</cell></row><row><cell>å•Šï¼Œå“ˆå“ˆã€‚</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Well, I also like watching this kind of movies. The 138 minute film</cell><cell></cell><cell></cell><cell></cell></row><row><cell>is a bit tortured. Ha ha.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>User2: å“ˆå“ˆï¼Œå…‹æ—ç‰¹â€¢ä¼Šæ–¯ç‰¹ä¼å¾·æ‰§å¯¼çš„ç”µå½±å¯æ˜¯å¾ˆç»†è‡´çš„å“¦ã€‚ Ha ha, the film directed by Clint Eastwood is very detailed. HRED: æ˜¯çš„ï¼Œä»–æ˜¯ä¸€åæ¼”å‘˜ï¼Œå¯¼æ¼”å’Œç¼–å‰§ã€‚ä½ çŸ¥é“ä»–æ˜¯å“ªå¹´å‡ºç”Ÿ çš„å—ï¼Ÿ Yes, he is an actor, director and screenwriter. Do you know when he was born? HRED+know: æ˜¯çš„ï¼Œè¿™éƒ¨ç”µå½±è¿˜æ˜¯ç”±å…‹æ—ç‰¹â€¢ä¼Šæ–¯ç‰¹ä¼å¾·åˆ¶ä½œçš„ å‘¢ï¼ŒçŸ¥é“ç¼–å‰§æ˜¯è°å—ï¼Ÿ</cell><cell>ã€Šå®Œç¾ ä¸–ç•Œã€‹ A Perfect World</cell><cell>ç”µå½±æ—¶é•¿ Film length</cell><cell cols="2">138åˆ†é’Ÿ / 125åˆ†é’Ÿ(å¾· å›½ç”µè§†ç‰ˆ) 138 minutes / 125 minutes (German TV Version)</cell></row><row><cell>Yes, the film is still made by Clint Eastwood. Do you know who the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>screenwriter is?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VHRED+know:</cell><cell></cell><cell></cell><cell></cell><cell>å…‹æ—ç‰¹â€¢ä¼Š</cell></row><row><cell></cell><cell></cell><cell>å¯¼æ¼”</cell><cell></cell><cell>æ–¯ç‰¹ä¼å¾·</cell></row><row><cell></cell><cell></cell><cell>Director</cell><cell></cell><cell>Clint</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Eastwood</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Examples generated by HERD, all baselines and our SDAN from film domain.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://nlp.stanford.edu/software/lex-parser.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research work descried in this paper has been supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2019YFB1405200</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61976015</rs>,  <rs type="grantNumber">61976016</rs>, <rs type="grantNumber">61876198</rs> and <rs type="grantNumber">61370130</rs>). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_T4UwvEb">
					<idno type="grant-number">2019YFB1405200</idno>
				</org>
				<org type="funding" xml:id="_vfgSkJA">
					<idno type="grant-number">61976015</idno>
				</org>
				<org type="funding" xml:id="_bPcpjcn">
					<idno type="grant-number">61976016</idno>
				</org>
				<org type="funding" xml:id="_er72Urm">
					<idno type="grant-number">61876198</idno>
				</org>
				<org type="funding" xml:id="_eQz33Ms">
					<idno type="grant-number">61370130</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2017.1285773</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>JÃ³zefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-08-11">2016. 2016. August 11-12, 2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoderdecoder networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2015.2477044</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on learning-based approaches for modeling and classification of human-machine dialog systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2985588</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1418" to="1432" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling semantic and emotional relationship in multi-turn emotional conversations using multi-task learning</title>
		<author>
			<persName><forename type="first">Fuwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazushige</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-021-02683-x</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1573" to="7497" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="5110" to="5117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Policy-driven neural response generation for knowledge-grounded dialogue systems</title>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Behnam Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><surname>Hakkani-TÃ¼r</surname></persName>
		</author>
		<idno>CoRR, abs/2005.12529</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14">2014. April 14-16, 2014</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2016a. A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016">June 12-17, 2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>CoRR, abs/1611.08562</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10">2019. 2019. August 10-16, 2019</date>
			<biblScope unit="page" from="5081" to="5087" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Infusing multisource knowledge with heterogeneous graph neural network for emotional conversation generation</title>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="13343" to="13352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating informative conversational response using recurrent knowledgeinteraction and knowledge-copy</title>
		<author>
			<persName><forename type="first">Xiexiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="41" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hierarchical latent structure for variational conversation modeling</title>
		<author>
			<persName><forename type="first">Yookoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Promoting diversity for end-to-end conversation response generation</title>
		<author>
			<persName><forename type="first">Yu-Ping</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno>CoRR, abs/1901.09444</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for modeling chit-chat dialog with discrete attributes</title>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-09-11">2019. 2019. September 11-13, 2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12">2016. February 12-17, 2016</date>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1503.02364</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling semantic relationship in multi-turn conversations with hierarchical latent variables</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolan</forename><surname>Zhan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1549</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5497" to="5502" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno>CoRR, abs/1610.02424</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diverse and informative dialogue generation with context-specific commonsense knowledge awareness</title>
		<author>
			<persName><forename type="first">Sixing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghai</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.515</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="5811" to="5820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Proactive human-machine conversation with explicit conversation goal</title>
		<author>
			<persName><forename type="first">Wenquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1369</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3794" to="3804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Topic augmented neural response generation with a joint attention mechanism</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/1606.08340</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI&apos;17)</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diversity-promoting GAN: A crossentropy based generative adversarial network for diversified text generation</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3940" to="3949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attention with intention for a neural network conversation model</title>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<idno>CoRR, abs/1510.08565</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge-grounded response generation with deep attentional latent-variable model</title>
		<author>
			<persName><forename type="first">Kai-Lin</forename><surname>Hao-Tong Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2020.101069</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech Language</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101069</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>EskÃ©nazi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30">2017. 2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/643</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. 2018. July 13-19, 2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kdconv: A chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.635</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7098" to="7108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
