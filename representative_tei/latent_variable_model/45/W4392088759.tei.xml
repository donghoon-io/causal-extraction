<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Full-Atom Peptide Design with Geometric Latent Diffusion</title>
				<funder ref="#_CyuFQ8R">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_PuHTbcK">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_BTPAjJP #_3TtqF2t #_kYDfkNJ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-30">30 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangzhe</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for AIR</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinjun</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>5 Shanghai</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for AIR</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Full-Atom Peptide Design with Geometric Latent Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-30">30 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.13555v4[q-bio.BM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom Peptide design with Geometric LAtent Diffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes fullatom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Peptides are short chains of amino acids and acts as vital mediators of many protein-protein interactions in human cells. Designing functional peptides has attracted increasing attention in biological research and therapeutics, since the highly-flexible conformation space of peptides allows brand new possibility to target binding sites previously undruggable with antibodies or small molecules <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>. The key of peptide design is to generate peptides that interact compactly with target proteins (see Figure <ref type="figure" target="#fig_4">1</ref>), since they mostly exhibit flexible conformations <ref type="bibr" target="#b19">[20]</ref> unless bound to these receptors <ref type="bibr" target="#b59">[60]</ref>.</p><p>Conventional simulation or searching algorithms rely on frequent calculations of physical energy functions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, which are inefficient and prone to poor local optimum. Recent advances illuminate the remarkable success of exploiting geometric deep generative models, particularly the equivariant diffusion models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b71">72]</ref>, for molecule design <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>, antibody design <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34]</ref> and protein design <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b27">28]</ref>, as well as latent diffusion models further enhancing the performance <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b17">18]</ref>. Inspired by these successes, a natural idea is leveraging diffusion models for peptide design as well, which, yet, is challenging in two aspects. From the dataset aspect, existing databases (PepBDB <ref type="bibr" target="#b64">[65]</ref>, Propedia <ref type="bibr" target="#b45">[46]</ref>) merely collect data from Protein Data Bank (PDB) <ref type="bibr" target="#b4">[5]</ref>, neither performing filters according to practical relevance <ref type="bibr" target="#b48">[49]</ref> and redundancy, nor providing an adequate split for evaluation.</p><p>Figure <ref type="figure" target="#fig_4">1</ref>: Left: Peptide design requires generating peptides that form compact interactions with the binding site on the receptor. The intricacy of protein-peptide interactions demands efficient exploration in the vast space for sequence-structure co-design. Right: Different binding sites (a, b, c, d) adopt disparate center offsets and geometric shapes, approximating variable 3D Gaussian distributions that deviate from N (0, I). We propose to convert the geometry into a standard space approximating standard Gaussian, via an affine transformation derived from the binding site ( §3.3).</p><p>Therefore, this paper first curates a benchmark from PDB <ref type="bibr" target="#b4">[5]</ref> and the literature <ref type="bibr" target="#b58">[59]</ref>, and then systematically evaluates the generative models in terms of diversity, consistency, and binding affinity.</p><p>From the methodology aspect, it is nontrivial to adopt latent diffusion models to characterize the geometry of protein-peptide interactions. The first nontriviality stems from the full-atom geometry, which determines the comprehensive protein-peptide interactions in the atomic level, yet difficult to preserve. Throughout the generation process, the type of each amino acid always changes and thus requires us to generate different number of atoms, which is unfriendly to diffusion models that prefer fixed-size generation. Current latent diffusion models on molecules <ref type="bibr" target="#b70">[71]</ref> or protein backbones <ref type="bibr" target="#b17">[18]</ref> still tackle tasks with fixed number of atoms, thus leaving this challenge untouched. The second nontriviality lies in the variable binding geometry. Diffusion models are typically implemented directly in the data space, which might be suitable for regular data (e.g. images with fixed value range), yet ill-suited for our case on 3D coordinates where the value range is not fixed and even cursed with high variances due to the rich diversity in protein-peptide interactions. These variances define divergent target distributions of Gaussian with disparate expectation and covariance, which hinders the transferability of the diffusion process across different binding sites and thereby yields unsatisfactory generalization capability. Unfortunately, this point is seldom investigated previously.</p><p>To address the above problems, we propose a powerful model for full-atom Peptide design with Geometric LAtent Diffusion (PepGLAD) with the following contributions:</p><p>• We construct a new benchmark from PDB and literature based on practical relevance and non-redundancy, then systematically evaluate available sequence-structure co-design models on the task of target-specific peptide design. • To capture the full-atom geometry, we first learn a Variational AutoEncoder (VAE) to obtain a fixed-size latent representation (including a 3D coordinate and a hidden feature) for each residue of the input peptide, and then conduct the diffusion process in this latent space, both of which are conditioned on the binding site to better model protein-peptide interactions. Notably, the proposed design enables our model to accommodate full-atom input and output. • Regarding the variable binding geometry, we derive a shared standard space from the binding sites by proposing a novel skill-receptor-specific affine transformation. Such affine transformation is computed by the center offset and covariance Cholesky decomposition of the binding site coordinates, serving as a mapping from the binding site distribution to standard Gaussian. With the affine transformation applied to both the binding sites and the peptides, we are able to project the shape of all complexes into approximately standard Gaussian distribution, which facilitates generalization to diverse binding sites.</p><p>Favorably, all the aforementioned models and processes meet the desired symmetry, i.e., E(3)equivariance, as proved by us. Experiments on sequence-structure co-design and complex conformation generation demonstrate the superiority of PepGLAD over the existing generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Peptide Design Conventional methods directly sample residues <ref type="bibr" target="#b5">[6]</ref> or building blocks from libraries containing small fragments of proteins <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>, with guidance from delicate physical energy functions <ref type="bibr" target="#b1">[2]</ref>. These methods are time-consuming and easy to be trapped by local optimum. Recent advances with deep generative models mainly focuses on target-agnostic 1D language models <ref type="bibr" target="#b47">[48]</ref>, antimicrobial peptides <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b62">63]</ref>, or a subtype of peptides with α-helix <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. While geometric deep generative models are exhibiting notable potential in other domains of target-specific binder design (e.g. antibodies), their capability of target-specific peptide design remains unclear, which is the first problem we answer in this paper. Other contemporary work includes peptide design algorithms with flow matching frameworks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Geometric Protein/Antibody Design Protein design primarily aims to generate stable secondary or tertiary structures <ref type="bibr" target="#b26">[27]</ref>, where diffusion models demonstrate inspiring performance <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b71">72]</ref>. In particular, RFDiffusion <ref type="bibr" target="#b63">[64]</ref> first generates backbones via diffusion, and then designs the sequences through cycles of inverse folding and structure refining with empirical force fields. Chroma <ref type="bibr" target="#b27">[28]</ref> adopts a similar strategy, but further explores controllable generative process with custom energy functions. Antibody design, encompassing a special family of proteins in the immune system to capture antigens, mainly focuses on inpainting complementarity-determining regions (CDRs) at the interface between the antigen and the framework <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b60">61]</ref>, where the geometric diffusion models exhibit promising potential <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> in co-designing sequence and structure. Unlike antibodies which are constrained by framework regions, peptides exhibit a more irregular binding pattern and greater flexibility, adapting to binding sites upon interaction <ref type="bibr" target="#b34">[35]</ref>. Thus the target distributions are remarkably divergent on different binding sites, posing an urgent need for more robust generative modeling.</p><p>Geometric Latent Diffusion Models Diffusion models learn a denoising trajectory to generate desired data distribution from a prior distribution, commonly standard Gaussian <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b24">25]</ref>. Recent literature extends diffusion to 3D small molecules satisfying the E(3)-equivariance <ref type="bibr" target="#b69">[70]</ref>, which triggers subsequent advances in geometric design of macro molecules (e.g. antibody, protein) as aforementioned. Further efforts are made to latent diffusion models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b17">18]</ref>, which implement the generative process in the compressed latent space of pretrained auto-encoders, to improve the performance. Compared to the literature that either encodes atom-wise representation in the latent space for small molecule generation <ref type="bibr" target="#b70">[71]</ref>, or compress a fixed number of atoms into one latent node for protein backbone generation <ref type="bibr" target="#b17">[18]</ref>, we explore compression of the full-atom geometry by directly generating different residues with variable number of atoms in the latent space. Moreover, we propose a novel technique, namely the data-specific affine transformations, to enhance the generalization ability of diffusion models, which is barely explored before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method: PepGLAD</head><p>We first define the notations in the paper and formalize peptide design in §3.1. The overall workflow of our PepGLAD is presented in Figure <ref type="figure" target="#fig_0">2</ref>, which consists of three modules: (1) An autoencoder that defines the joint latent space for sequences and structures conditioned on the full-atom context of the binding site ( §3.2); (2) An affine transformation derived from the binding site to project the 3D geometry into a standard space approximating standard Gaussian distribution ( §3.3); (3) A latent diffusion model trained on the standard latent space ( §3.4). Finally, we summarize the training and the sampling procedures in §3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions and Notations</head><p>We represent binding sites and peptides as geometric graphs G = {(x i , ⃗ X i )}, where each node i is a residue with its amino acid type x i and the coordinates of all its c i atoms ⃗ X i ∈ R ci×3 . In later sections, we use the simplified notations i ∈ G to denote that a node i is in the geometric graph G, and |G| to denote the total number of nodes in G. We use G p and G b to represent the geometric graph of the peptide and the binding site, respectively. In this work, the binding site incorporates residues on the target protein within 10Å distances to the peptide residues based on C β atoms which alleviates leakage of the side-chain interactions. Note that the threshold (10Å) is chosen to be large to better reduce leakage of the peptide geometry. Task Definition Given the binding site G b , we aim to obtain a generative model p θ conforming to the distribution of binding peptides q(G p |G b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational AutoEncoder</head><p>The autoencoder <ref type="bibr" target="#b61">[62]</ref> consists of an encoder E ϕ that encodes the peptide G p in the presence of the binding site G b into a latent state G z , and a decoder D ξ that reconstructs the peptide from the latent state to obtain</p><formula xml:id="formula_0">G ′ p = {(x ′ i , ⃗ X ′ i )}.</formula><p>To encourage E ϕ to learn contextual representations of residues, we corrupt 25% of the residues in G p with a [MASK] type to obtain Gp as the input:</p><formula xml:id="formula_1">G z = E ϕ ( Gp , G b ), G ′ p = D ξ (G z , G b ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">G z = {(z i , ⃗ z i )|i ∈ G p } contains the latent states z i ∈ R h (h = 8 in this paper) and ⃗ z i ∈ R 3 sampled from the encoded distribution N (z i ; µ i , σ i ) and N (⃗ z i ; ⃗ µ i , ⃗ σ i</formula><p>) using the reparameterization trick <ref type="bibr" target="#b31">[32]</ref>. We borrow the adaptive multi-channel equivariant encoder in dyMEAN <ref type="bibr" target="#b33">[34]</ref> for both E ϕ and D ξ to capture the full-atom geometry. In the decoder D ξ , we factorize the joint distribution of sequences and structures as follows:</p><formula xml:id="formula_3">p ξ (x ′ i , ⃗ X ′ i |G z , G b ) = p ξ1 (x ′ i |G z , G b )p ξ2 ( ⃗ X ′ i |x ′ i , G z , G b ),<label>(2)</label></formula><p>where the sequence is first decoded and then the all-atom geometry, initialized with replications of ⃗ z i , is reconstructed. The training objective of the autoencoder consists of the reconstruction loss L recon and the KL divergence L KL to constrain the latent space. The reconstruction loss includes cross entropy on the residue types, mean square error (MSE) on the full-atom structures, and an auxilary loss L aux on bond lengths and angles <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_4">L recon (i) = H(p(x i ), p(x ′ i )) + MSE( ⃗ X i , ⃗ X ′ i ) + L aux (i),<label>(3)</label></formula><p>where H denotes cross entropy. We include details of L aux in Appendix A. The KL divergence constrains z i and ⃗ z i with the prior N (0, I) and N (⃗ r i , I), respectively, where ⃗ r i denotes the coordinate of the alpha carbon (C α ) in node i:</p><formula xml:id="formula_5">L KL (i) = λ 1 • D KL (N (0, I)∥N (µ i , diag(σ i ))) + λ 2 • D KL (N (⃗ r i , I)∥N (⃗ µ i , diag(⃗ σ i ))),<label>(4)</label></formula><p>where D KL denotes the KL divergence, λ 1 and λ 2 reweight the contraints on the sequence and the structure, respectively. L KL prevents the scale of z i from exploding and constrains ⃗ z i around C α to retain necessary geometric information. Such regularization also helps ensure consistent scales between the peptide latent coordinates and the pocket, mitigating potential issues arising from their different levels of abstraction. Then we have the overall training objective of the variational autoencoder as follows:</p><formula xml:id="formula_6">L AE = i∈Gp (L recon (i) + L KL (i))/|G p |.<label>(5)</label></formula><p>We have explored E(3)-invariant latent space, which appears to have difficulties in reconstructing the full-atom structures since it lacks information of geometric interactions with the pocket atoms (Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Receptor-Specific Affine Transformation</head><p>With the latent space given by the autoencoder, we further exploit a standard space obtained from receptor-specific affine transformations, which enhances the transferability of diffusions on disparate binding sites (see Figure <ref type="figure" target="#fig_4">1</ref>). Most peptides fold into complementary shape upon binding on the receptor <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b40">41]</ref>. Thus, the target distribution is inherently characterized by the shape of the binding site. Given the wide disparity in binding geometries, directly implementing diffusion in the data space yields minimal transferability among different binding sites. To address this deficiency, we propose to implement the diffusion process on a shared standard space converted via an affine transformation derived from the binding site. Formally, denoting the C α coordinates of the residues in a given binding site</p><formula xml:id="formula_7">G b as ⃗ R ∈ R 3×|G b | , we can derive their center ⃗ µ = E[ ⃗ R] ∈ R 3 and covariance ⃗ Σ = Cov( ⃗ R, ⃗ R) ∈ R 3×3</formula><p>, so that these coordinates can be regarded as sampled from the distribution N (⃗ µ, ⃗ Σ). We then calculate the Cholesky decomposition <ref type="bibr" target="#b18">[19]</ref> of ⃗ Σ:</p><formula xml:id="formula_8">⃗ Σ = ⃗ L ⃗ L ⊤ , ⃗ L ∈ R 3×3 ,<label>(6)</label></formula><p>where ⃗ L is a lower triangular matrix. ⃗ L is unique <ref type="bibr" target="#b18">[19]</ref> and invertible since the covariance matrix is a real-valued symmetric positive-definite matrix<ref type="foot" target="#foot_0">foot_0</ref> . Then we can define the affine transformation F : R<ref type="foot" target="#foot_1">foot_1</ref> → R 3 , which enables the projection of the geometry into the standard space approximating standard Gaussian F ( ⃗ R) ∼ N (0, I). Further, we can easily obtain the inverse of F as:</p><formula xml:id="formula_9">F (⃗ x) = ⃗ L -1 (⃗ x -⃗ µ), F -1 (⃗ x) = ⃗ L⃗ x + ⃗ µ.<label>(7)</label></formula><p>With the above definitions, for each given binding site G b , we transform the geometry via the derived F to obtain the standard space, where the diffusion model is implemented, and recover the original geometry with F -1 (see Figure <ref type="figure" target="#fig_0">2</ref>) after generation. Notably, we have the following proposition to ensure that the equivariance is maintained under the proposed affine transformation with scalarization-based equivariant GNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16]</ref>: Proposition 3.1. Denote the invariant and equivariant outputs from a scalarization-based E(3)equivariant GNN as f ({h i , ⃗ x i }) and ⃗ f ({h i , ⃗ x i }), respectively. With the definition of F in Eq. 7, ∀g ∈ E(3), we have f</p><formula xml:id="formula_10">({h i , F (⃗ x i )}) = f ({h i , F g (g • ⃗ x i )}) and g • F -1 ( ⃗ f ({h i , F (⃗ x i )})) = F -1 g ( ⃗ f ({h i , F g (g • ⃗ x i )}))</formula><p>, where F g is derived on the coordinates transformed by g. Namely, the E(3)-equivariance is preserved if we implement the GNN on the standard space and recover the original geometry from the outputs.</p><p>The proof is in Appendix B. This is vital since it indicates the Markov kernel is E(3)-equivariant, and thus ensures the E(3)-invariance of the probability density in the diffusion process <ref type="bibr" target="#b69">[70]</ref>. Note that our variational autoencoder ( § 3.2) and latent diffusion model ( § 3.4) are already designed to be equivariant even without the proposed affine transformation here. The purpose of defining such component is to encourage better generalization of the diffusion processes. Indeed, it is nontrivial to analyze whether such an implementation will break the equivariance of our workflow. Luckily, Proposition 3.1 manages to prove that scalarization-based equivariant networks <ref type="bibr" target="#b22">[23]</ref>, which is used in our autoencoder and diffusion model, are seamlessly compatible with such affine transformation, naturally preserving equivariance without any requirements of adaption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Geometric Latent Diffusion Model</head><p>With the aforementioned preparations, the discrete residue types are encoded as continuous latent representations {z i }, and the full-atom geometry is also compressed and standardized into 3D vectors {⃗ z i } ∼ N (0, I). Therefore, we are ready to implement a diffusion model on the standard latent space to generate z i and ⃗ z i . The forward diffusion process gradually adds noise to the data from t = 0 to t = T , resulting in the prior distribution N (0, I). The reverse diffusion process generates data distribution by iteratively denosing the distribution from t = T to t = 0. We denote</p><formula xml:id="formula_11">⃗ u t i = [z t i , ⃗ z t i ] and G t z = {(z t i , ⃗ z t i )}</formula><p>as the intermediate state for node i and the entire peptide at time step t, respectively. For simplicity, we assume both G t z and the binding site G b are already standardized via the transformation F b in Eq. 7. Then we have the forward process as:</p><formula xml:id="formula_12">q(⃗ u t i |⃗ u t-1 i ) = N (⃗ u t i ; 1 -β t • ⃗ u t-1 i , β t I),<label>(8)</label></formula><formula xml:id="formula_13">q(⃗ u t i |⃗ u 0 i ) = N (⃗ u t i ; √ ᾱt • ⃗ u 0 i , (1 -ᾱt )I),<label>(9)</label></formula><p>where β t is the noise scale increasing with the timestep from 0 to 1 conforming to the cosine schedule <ref type="bibr" target="#b50">[51]</ref>, and ᾱt = s=t s=1 (1 -β s ). Then the state at timestep t can be sampled as:</p><formula xml:id="formula_14">⃗ u t i = √ ᾱt ⃗ u 0 i + (1 -ᾱt )ϵ i ,<label>(10)</label></formula><p>where ϵ i ∼ N (0, I). Following Ho et al. <ref type="bibr" target="#b24">[25]</ref>, the reverse process can be defined with the reparameterization trick as:</p><formula xml:id="formula_15">p θ (⃗ u t-1 i |G t z , G b ) = N (⃗ u t-1 i ; ⃗ µ θ (G t z , G b ), β t I),<label>(11)</label></formula><formula xml:id="formula_16">⃗ µ θ (G t z , G b ) = 1 √ α t (⃗ u t i - β t √ 1 -ᾱt ϵ θ (G t z , G b , t)[i]),<label>(12)</label></formula><p>where α t = 1 -β t , and ϵ θ is the denoising network also implemented with the equivariant adaptive multi-channel equivariant encoder in dyMEAN <ref type="bibr" target="#b33">[34]</ref> to retain full-atom context of the binding site during generation and preserve the equivariance under affine transformations (Proposition 3.1). Finally, we have the objective at time step t as MSE between the predicted noise and the added noise in Eq. 10, as well as the overall training objective L LDM as the expectation with respect to t:</p><formula xml:id="formula_17">L LDM = E t∼Uniform(1...T ) [ i ∥ϵ i -ϵ θ (G t z , G b , t)[i]∥ 2 /|G t z |].<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Sampling</head><p>Training The training of our PepGLAD can be divided into two phases where a variational autoencoder is first trained and then a diffusion model is trained on the standard latent space. We provide the overall training procedure in Algorithm 1 (see Appendix D). Note that a smooth and informative latent space is necessary for the consecutive training of the diffusion model, thus we resort to unsupervised data from protein fragments apart from the limited protein-peptide complexes for training the autoencoder, which we describe in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling in Ordered Subspace</head><p>The sampling procedure includes generative diffusion process on the standard latent states, recovering the original geometry with the inverse of F in Eq. 7, and decoding the sequence as well as the full-atom structure of the peptide (see Algorithm 2 in Appendix D). A problem here is that the unordered nature of graphs is not compatible with the sequential nature of peptides, thus the generated residues may have arbitrary permutation on the sequence order. Inspired by the concept of classifier-guided sampling <ref type="bibr" target="#b14">[15]</ref>, we first assign an arbitrary permutation P on the sequence order to the nodes. Then we steer the sampling procedure towards the desired subspace conforming to P with the following empirical classifier p(1|{⃗ z t i }), which estimates the probability of the current coordinates belonging to the desired subspace:</p><formula xml:id="formula_18">p(1|{⃗ z t i }) = exp(- P(i)-P(j)=1 E(∥⃗ z t i -⃗ z t j ∥)),<label>(14)</label></formula><formula xml:id="formula_19">E(d) =    d -(µ d + 3σ d ), d &gt; µ d + 3σ d , (µ d -3σ d ) -d, d &lt; µ d -3σ d , 0, otherwise,<label>(15)</label></formula><p>where µ d and σ d are the mean and variance of the distances of adjacent residues in the latent space measured from the training set. Intuitively, this classifier gives higher confidence if the adjacent (defined by P) residues are within reasonable distances aligning with the statistics from the training set. Nevertheless, the effect of the guidance is relatively minor, which is only a technical trick to enhance the robustness. We provide more details in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Task We evaluate our PepGLAD and baselines on the following tasks: (1) sequence-structure co-design ( §4.2) aims to generate both the sequence and the structure of the peptide given the specific binding site on the receptor (i.e. protein). ( <ref type="formula" target="#formula_3">2</ref>) Binding Conformation Generation ( §4.3) requires to generate the binding state of the peptide given its sequence and the binding site of interest.</p><p>Dataset We first extract all dimers from the Protein Data Bank (PDB) <ref type="bibr" target="#b4">[5]</ref> and select the complexes with a receptor longer than 30 residues and a ligand between 4 to 25 residues <ref type="bibr" target="#b58">[59]</ref>. Then we remove the duplicated complexes with the criterion that both the receptor and the peptide has a sequence identity over 90% <ref type="bibr" target="#b55">[56]</ref>, after which 6105 non-redundant complexes are obtained. To achieve the cross-target generalization test, we utilize the large non-redundant dataset (LNR) from Tsaban et al. <ref type="bibr" target="#b58">[59]</ref> as the test set, which contains 93 protein-peptide complexes with canonical amino acids curated by domain experts. We then cluster the data by receptor with a sequence identity threshold of over 40%, and remove the complexes sharing the same clusters with those from the test set. Finally, the remaining data are randomly split based on clustering results into training and validation sets, yielding a new bechmark calling PepBench. Further, we exploit 70k unsupervised data from protein fragments (ProtFrag) to facilitate training of the variational autoencoder. We also implement a split on PepBDB <ref type="bibr" target="#b64">[65]</ref> based on clustering results for evaluation. We show details and statistics of these datasets in Appendix E.</p><p>Baselines We first borrow three baselines from the antibody design domain. HSRN <ref type="bibr" target="#b29">[30]</ref> autoregressively decodes the sequence while keeps refining the structure hierarchically, from the C α to other atoms. dyMEAN <ref type="bibr" target="#b33">[34]</ref> is equipped with an full-atom geometric encoder and exploits iterative non-autoregressive generation. DiffAb <ref type="bibr" target="#b43">[44]</ref> jointly diffuses on the categorical residue type, the coordinate of C α as well as the orientation of each residue. Next, we explore two baselines from the general protein design. RFDiffusion <ref type="bibr" target="#b63">[64]</ref> exploits a pipeline that first generates the backbone via diffusion and then alternates between inverse folding <ref type="bibr" target="#b13">[14]</ref> and structure refining based on a physical energy function <ref type="bibr" target="#b1">[2]</ref>. AlphaFold 2 <ref type="bibr" target="#b30">[31]</ref> is the well-known model for protein folding, which also shows certain abilities on peptide conformation prediction <ref type="bibr" target="#b58">[59]</ref>. We also include two traditional methods.</p><p>AnchorExtension <ref type="bibr" target="#b25">[26]</ref> designs peptides by first docking an existing scaffold to the binding site, and then optimizing the peptide with cycles of mutations guided by energy functions. FlexPepDock <ref type="bibr" target="#b41">[42]</ref> is designed for flexible peptide docking via optimization in the landscape of a physical energy function <ref type="bibr" target="#b1">[2]</ref>. Implementation details are provided in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence-Structure Co-Design</head><p>Metrics A favorable generative model should produce diverse candidates while maintaining fidelity to the desired distribution. To comprehensively evaluate the models, we generate 40 candidates for each receptor and employ the following metrics: (1) Diversity. Inspired by <ref type="bibr" target="#b71">[72]</ref>, we measure the diversity via unique clusters of sequences and structures. Specifically, we hierarchically cluster the structures based on pair-wise root mean square deviation (RMSD) of C α . The diversity of structures Div struct is defined as the number of clusters versus the number of candidates. A similar procedure can be applied to the sequences to obtain Div seq , utilizing the similarity <ref type="bibr" target="#b35">[36]</ref> derived from alignment <ref type="bibr" target="#b23">[24]</ref>. Then the co-design diversity is Div seq Div struct . (2) Consistency. We measure how well the models learn the 1D&amp;3D joint distribution by the sequence-structure consistency, quantified via Cramér's V <ref type="bibr" target="#b11">[12]</ref> association between the clustering labels (as in Diversity) of the sequences and the structures. High consistency indicates that candidates with similar sequences also have similar structures, implying that the generative model effectively captures the dependency between 1D and 3D.</p><p>(3) ∆G. Aligned with the literature <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44]</ref>, we employ the binding energy (kcal/mol) provided by Rosetta <ref type="bibr" target="#b1">[2]</ref>, a widely-used suite for biomolecular modeling with physical energy functions, to evaluate the binding affinity of the generated candidates. Lower ∆G indicates stronger binding between the peptide and the target. (4) Success. We report the proportion of successful designs (i.e. ∆G &lt; 0, indicating no severe atomic clashes or twisted conformations) among all the candidates. For all metrics except ∆G, we first compute values for each receptor individually and then average the results across different receptors. For ∆G, we identify the best candidate on each receptor as the outputs and report the median value across different receptors. Details about the metrics are provided in Appendix H, including discussion on AAR (Appendix H.1) and consistency (Appendix H.2).</p><p>Results Table <ref type="table" target="#tab_0">1</ref> illustrates that our PepGLAD generates significantly more diversified and consistent peptides with better binding energy and success rates compared to the baselines. When benchmarking HSRN, dyMEAN, and DiffAb, which perform well on antibody CDR design, we observe a notable performance gap between non-diffusion baselines (i.e. HSRN, dyMEAN) and the diffusion-based baseline (i.e. DiffAb), suggesting the higher complexity in peptide design and the need for stronger modeling capabilities. Compared to DiffAb, which operates on categorical residue types, C α coordinates and orientations, our PepGLAD (1) better captures the dependency between sequence and structure, as indicated by higher diversity and consistency, since diffusion is implemented on the latent space where the representation of sequence and structure are nicely correlated by the autoencoder; (2) more effectively captures the intricate protein-peptide interactions, demonstrated by better ∆G and success rates, since we leverage the full-atom context of the binding site and enhances generalization capability by converting the geometry into a standard space. We showcase two candidates designed by our PepGLAD with favorable binding energy given by Rosetta in Figure <ref type="figure" target="#fig_1">3</ref>. Furthermore, the diversity within successful designs is 0.632, which is higher than that of all designs (0.506), indicating the high structural flexibility of peptides upon successful binding. Bottom: A generated candidate with complementary shape to the binding site (PDB=3pkn, ∆G=-33.32). Both candidates form compact interactions at the interface.</p><note type="other">PepGLAD PepGLAD Receptor Peptide Hydrophilic Hydrophobic</note><p>We also evaluate our PepGLAD against two sophisticated pipeline systems in Table <ref type="table" target="#tab_1">2</ref>. The traditional method (i.e. AnchorExtension) is limited by low efficiency, thus we can only afford outputting 10 candidates for each receptor. For a relatively fair comparison with RFDiffusion, we refine the structure of the generated candidates using the empirical force field in RFDiffusion. However, the comparison may still disadvantage our PepGLAD, given that RFDiffusion is finetuned from a model pretrained on a large-scale dataset <ref type="bibr" target="#b63">[64]</ref>. Nevertheless, as demonstrated in Table <ref type="table" target="#tab_1">2</ref>, our model still exhibits marvelous superiority on diversity, consistency, and success rate, while achieving competitive binding energy ∆G, with obviously higher efficiency. Results As shown in Table <ref type="table" target="#tab_2">3</ref>, our PepGLAD surpasses all the baselines in terms of both RMSD Cα and DockQ by a large margin, highlighting the superiority of incorporating the full-atom context and the binding-site shape into the latent diffusion process. Additionally, we present the distribution of the best RMSD Cα on different test receptors using box plots and showcase a generated conformation highly resembling the reference in Figure <ref type="figure" target="#fig_3">4</ref>. The distribution reveals that our model achieves favorable performance on RMSD Cα with lower variance on the test set compared to other baselines, exhibiting robust generalization ability across disparate binding sites.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We conduct the following ablations: the full-atom geometry (Full-Atom); the affine transformation (Affine); the unsupervised data from protein fragments (ProtFrag) and the mask policy (Mask) when training the autoencoder. Note that generative performance is assessed from various aspects, and improvement in one aspect at the disproportionate expense of others might be meaningless. Thus, we additionally compute the average of all the metrics to evaluate the comprehensive effect of each module, where ∆G is normalized by the statistics on the test set. Table <ref type="table" target="#tab_3">4</ref> demonstrates the following observations:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Despite the promising results, we acknowledge several limitations which might be addressed by future work. First, the binding affinity assessment relies on the Rosetta scoring function as a proxy for wetlab experiments. There may be discrepancies between the predicted and actual binding energies.</p><p>The ultimate test of a peptide utility is its performance in vivo, which is too costly for large-scale evaluation. Nevertheless, this is a problem confronting the entire community, and we hope future research might propose more reliable in silico proxies to bridge the gap. Second, while this paper addresses peptide design from the aspect of proteins, it might also be reasonable to think from the aspect of small molecules if the peptides are short enough. Under such circumstances, it is also beneficial to further explore counterparts of methods for small molecule design <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b38">39]</ref>, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we first assemble a dataset from Protein Data Bank (PDB) and literature to benchmark generative models on target-specific peptide design in terms of diversity, consistency, and binding energy. Subsequently, we propose PepGLAD, a powerful diffusion-based model for full-atom peptide design. In particular, we explore diffusion on the latent space where the sequence and the full-atom structure are jointly encoded by a variational autoencoder. We further propose a receptor-specific affine transformation technique to project variable geometries in the data space into a standard space, which enhances the transferability of diffusion processes on disparate binding sites. Our PepGLAD outperforms the existing models on sequence-structure co-design and binding conformation generation, exhibiting high generalization across diverse binding sites. Our work represents a pioneering effort in the exploration of deep generative models for simultaneous design of 1D sequences and 3D structures of peptides, which could inspire future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and Data</head><p>The curated PepBench and ProtFrag are available at <ref type="url" target="https://zenodo.org/records/13373108">https://zenodo.org/records/13373108</ref>. The codes for our PepGLAD are open-sourced at <ref type="url" target="https://github.com/THUNLP-MT/PepGLAD">https://github.com/THUNLP-MT/PepGLAD</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statements</head><p>This paper aims to advance the field of peptide design through the construction of a benchmark and the development of a novel latent diffusion model, PepGLAD, which addresses key limitations in current methods. Our work represents a step forward in computational peptide design, with the potential to impact both scientific research and practical applications in various domains. For instance, more precise peptide design could lead to enhanced drugs in the pharmaceutical industry, and could facilitate the creation of new biomaterials, sensors, and other innovative technologies in biology and materials science. We wish our paper could inspire future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reconstruction of Full-Atom Geometry A.1 Auxilary Loss for Training the AutoEncoder</head><p>To better recover the all-atom geometry, we employ an auxilary structural loss similar to the violation loss in Jumper et al. <ref type="bibr" target="#b30">[31]</ref> including the supervision on C α coordinates, bond lengths, and side-chain dihedral angles. First, since the C α is critical in deciding the global geometry of the peptide, we exert additional loss on its coordinates to distinguish it from other atoms:</p><formula xml:id="formula_20">L CA (i) = MSE(⃗ r i , ⃗ r ′ i ),<label>(16)</label></formula><p>where ⃗ r ′ i and ⃗ r i are the reconstructed and the ground truth coordinates of C α in node i. Next, we implement L1 loss on the bond lengths:</p><formula xml:id="formula_21">L bond (i) = b∈B(i) |b -b ′ |/|B(i)|,<label>(17)</label></formula><p>where B(i) includes all chemical bonds in node i, and b ′ denotes the reconstructed bond length. For simplicity, he bonds between residues are included into the bonds of the former residue. Finally, we supervise on the χ 1 to χ 4 side-chain dihedral angles <ref type="bibr" target="#b72">[73]</ref>:</p><formula xml:id="formula_22">L angle (i) = χ∈A(i) |χ -χ ′ |/|A(i)|,<label>(18)</label></formula><p>where A(i) includes all side-chain dihedral angles in node i, χ ′ and χ denotes the reconstructed and the ground truth angles, respectively. The overall auxilary loss for node i is then given by:</p><formula xml:id="formula_23">L aux (i) = λ CA L CA (i) + λ bond L bond (i) + λ angle L angle (i),<label>(19)</label></formula><p>where we set λ CA = 1.0, λ bond = 1.0, λ angle = 0.5 in our experiments. We find that it is necessary to set λ angle with a relatively small value to make the training process stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Idealization of Local Geometry</head><p>Preserving atom instances enhances the modeling of side chain interactions but introduces challenges due to potential twisting in local geometry. While in sequence-structure co-design, samples undergo fast relax by physical force field to ensure a valid local geometry, in binding conformation generation, we use an alignment technique to place the idealized side chains on the generated atom instances. Specifically, the idealized side chain can be represented as at most 4 dihedral angles (χ-angles), treating fragments like phenyl group as rigid bodies. Suppose there are n i χ-angles and c i atoms in node i, we can define a function to map from the χ-angles with the backbone coordinates ⃗ B i ∈ R 4×3 to atom instances as M i : R ni × R 4×3 → R ci×3 , which is luckily differentiable <ref type="bibr" target="#b27">[28]</ref>. Thus we can optimize χ-angles via gradient descent to minimize the MSE between the coordinates constructed from χ-angles and those generated by the model:</p><formula xml:id="formula_24">χ * i = arg min χ∈[0,2π] n i ∥M i (χ, ⃗ B i ) -⃗ X i ∥ 2 ,<label>(20)</label></formula><p>where ⃗ X i ∈ R ci×3 denotes the coordinates of c i atom instances in node i generated by the model. Now it should be easy to construct an idealized side chain maintaining fidelity to the generated atom instances by M (χ * i , ⃗ B i ). In the experiments of conformation generation on PepBench, such idealization gives slightly better DockQ and RMSD atom than Rosetta side-chain packing algorithm but with higher efficiency.</p><p>B Proof of Proposition 3.1 Proposition 3.1. Denote the invariant and equivariant outputs from a scalarization-based E(3)equivariant GNN as f ({h i , ⃗</p><p>x i }) and ⃗ f ({h i , ⃗ x i }), respectively. With the definition of F in Eq. 7, ∀g ∈ E(3), we have f</p><formula xml:id="formula_25">({h i , F (⃗ x i )}) = f ({h i , F g (g • ⃗ x i )}) and g • F -1 ( ⃗ f ({h i , F (⃗ x i )})) = F -1 g ( ⃗ f ({h i , F g (g • ⃗ x i )}))</formula><p>, where F g is derived on the coordinates transformed by g. Namely, the E(3)-equivariance is preserved if we implement the GNN on the standard space and recover the original geometry from the outputs.</p><p>For simplicity, given F derived from a set of coordinates {⃗ x i } following Eq. 7, we use F g to indicate the affine transformation derived from {g • ⃗</p><p>x i }, where g ∈ E(3). Additionally, we keep the terminology "standard space" for describing the space after the data-specific affine transformation F . We begin by proving a key lemma, that is, the E(3)-invariance of distances between two nodes in the standard space converted by F : Lemma C.1. Given two nodes i and j in the geometric graph G, denoting their coordinates as ⃗</p><p>x i and ⃗ x j , their distance in the standard space is E(3)-invariant. Namely, ∀g ∈ E(3), ∥F (⃗</p><formula xml:id="formula_26">x i ) -F (⃗ x j )∥ = ∥F g (g • ⃗ x i ) -F g (g • ⃗ x j )∥.</formula><p>Proof. ∀g ∈ E(3), g can be instantiated as an orthogonal matrix Q ∈ O(3) (including rotation and reflection), and a translation vector ⃗ t ∈ R 3 . Denoting all coordinates in the geometric graph as ⃗ X ∈ R 3×|G| , and the number of nodes in G as n, we can derive the E(3)-equivariance of the expectation of the coordinates:</p><formula xml:id="formula_27">E[g • ⃗ X] = 1 n i g • ⃗ x i = 1 n i (Q⃗ x i + ⃗ t) (21) = 1 n Q( i ⃗ x i ) + ⃗ t = QE[ ⃗ X] + ⃗ t (22) = g • E[ ⃗ X].<label>(23)</label></formula><p>With the E(3)-equivariance of the expectation, it is easy to derive the following equation on the covariance matrix:</p><formula xml:id="formula_28">Cov(g • ⃗ X, g • ⃗ X) = 1 n -1 (g • ⃗ X -E[g • ⃗ X])(g • ⃗ X -E[g • ⃗ X]) ⊤ (24) = 1 n -1 (g • ⃗ X -g • E[ ⃗ X])(g • ⃗ X -g • E[ ⃗ X]) ⊤ (25) = 1 n -1 (Q( ⃗ X -E[ ⃗ X]))(Q( ⃗ X -E[ ⃗ X])) ⊤ (26) = 1 n -1 Q( ⃗ X -E[ ⃗ X])( ⃗ X -E[ ⃗ X]) ⊤ Q ⊤ (27) = QCov( ⃗ X, ⃗ X)Q ⊤ . (<label>28</label></formula><formula xml:id="formula_29">)</formula><p>Based on the Cholesky decomposition used in the derivation of F , we denote</p><formula xml:id="formula_30">Cov( ⃗ X, ⃗ X) = ⃗ L ⃗ L ⊤ and Cov(g • ⃗ X, g • ⃗ X) = ⃗ L g ⃗ L ⊤</formula><p>g , with which we can immediately derive:</p><formula xml:id="formula_31">⃗ L g ⃗ L ⊤ g = Q ⃗ L ⃗ L ⊤ Q ⊤ .<label>(29)</label></formula><p>Considering Q -1 = Q ⊤ , we further have the following equation:</p><formula xml:id="formula_32">⃗ L -⊤ g ⃗ L -1 g = Q ⃗ L -⊤ ⃗ L -1 Q ⊤ . (<label>30</label></formula><formula xml:id="formula_33">)</formula><p>Given that</p><formula xml:id="formula_34">F (⃗ x) = ⃗ L -1 (⃗ x -E[ ⃗ X]</formula><p>), we are now ready to prove Lemma C.1 as follows:</p><formula xml:id="formula_35">∥F g (g • ⃗ x i ) -F g (g • ⃗ x j )∥ = ∥ ⃗ L -1 g (g • ⃗ x i -E[g • ⃗ X]) -⃗ L -1 g (g • ⃗ x j -E[g • ⃗ X])∥ (31) = ∥ ⃗ L -1 g (g • ⃗ x i -g • ⃗ x j )∥ = ∥ ⃗ L -1 g Q(⃗ x i -⃗ x j )∥ (32) = ( ⃗ L -1 g Q(⃗ x i -⃗ x j )) ⊤ ( ⃗ L -1 g Q(⃗ x i -⃗ x j ))<label>(33)</label></formula><formula xml:id="formula_36">= (⃗ x i -⃗ x j ) ⊤ Q ⊤ ⃗ L -⊤ g ⃗ L -1 g Q(⃗ x i -⃗ x j )<label>(34)</label></formula><formula xml:id="formula_37">= (⃗ x i -⃗ x j ) ⊤ ⃗ L -⊤ ⃗ L -1 (⃗ x i -⃗ x j )<label>(35)</label></formula><formula xml:id="formula_38">= ( ⃗ L -1 (⃗ x i -⃗ x j )) ⊤ ( ⃗ L -1 (⃗ x i -⃗ x j )) = ∥ ⃗ L -1 (⃗ x i -⃗ x j )∥ (36) = ∥ ⃗ L -1 (⃗ x i -E[ ⃗ X]) -⃗ L -1 (⃗ x j -E[ ⃗ X])∥ = ∥F (⃗ x i ) -F (⃗ x j )∥<label>(37)</label></formula><p>With Lemma C.1, we are able to give the proof of Proposition 3.1 as follows.</p><p>Proof. A first observation is that to prove Proposition 3.1, we only need to prove the equivariance in the 1-layer case, since the multi-layer case can be decomposed into the 1-layer case by inserting I = F • F -1 between layers, where I is the identical mapping. Generally, each layer in scalarizationbased E(3)-equivariant GNN has the following paradigm:</p><formula xml:id="formula_39">m ij = ϕ m (h i , h j , ∥⃗ x i -⃗ x j ∥ 2 , e ij ),<label>(38)</label></formula><formula xml:id="formula_40">⃗ x ′ i = ⃗ x i + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij ),<label>(39)</label></formula><formula xml:id="formula_41">⃗ h ′ i = ϕ h (h i , j∈N (i) m ij ),<label>(40)</label></formula><p>where N (i) denotes the neighborhood of node i, and ϕ x outputs a scalar. Therefore, for the invariant part, we have:</p><formula xml:id="formula_42">f i ({h i , F g (g • ⃗ x i )}) = ϕ h (h i , j∈N (i) m ij,Fg )<label>(41)</label></formula><formula xml:id="formula_43">= ϕ h (h i , j∈N (i) ϕ m (h i , h j , ∥F g (g • ⃗ x i ) -F g (g • ⃗ x j )∥ 2 , e ij ))<label>(42)</label></formula><formula xml:id="formula_44">= ϕ h (h i , j∈N (i) ϕ m (h i , h j , ∥F (⃗ x i ) -F (⃗ x j )∥, e ij ))<label>(43)</label></formula><formula xml:id="formula_45">= ϕ h (h i , j∈N (i) m ij,F )<label>(44)</label></formula><formula xml:id="formula_46">= f i ({h i , F (⃗ x i )})<label>(45)</label></formula><p>For the equivariant features, recalling</p><formula xml:id="formula_47">F -1 (⃗ x) = ⃗ L⃗ x + E[ ⃗ X],</formula><p>we have:</p><formula xml:id="formula_48">F -1 g ⃗ f i ({h i , F g (g • ⃗ x i )})<label>(46)</label></formula><formula xml:id="formula_49">= F -1 g (F g (g • ⃗ x i ) + j∈N (i) (F g (g • ⃗ x i ) -F g (g • ⃗ x j ))ϕ x (m ij,Fg )) (47) = F -1 g (F g (g • ⃗ x i ) + j∈N (i) ⃗ L -1 g Q(⃗ x i -⃗ x j )ϕ x (m ij,F )) (48) = F -1 g ( ⃗ L -1 g (Q⃗ x i + ⃗ t -E[g • ⃗ X]) + j∈N (i) ⃗ L -1 g Q(⃗ x i -⃗ x j )ϕ x (m ij,F )) (49) = F -1 g ( ⃗ L -1 g (Q⃗ x i + ⃗ t -g • E[ ⃗ X]) + j∈N (i) ⃗ L -1 g Q(⃗ x i -⃗ x j )ϕ x (m ij,F )) (50) = F -1 g ( ⃗ L -1 g (Q⃗ x i -QE[ ⃗ X]) + j∈N (i) ⃗ L -1 g Q(⃗ x i -⃗ x j )ϕ x (m ij,F )) (51) = F -1 g ( ⃗ L -1 g Q(⃗ x i -E[ ⃗ X]) + j∈N (i) ⃗ L -1 g Q(⃗ x i -⃗ x j )ϕ x (m ij,F )) (52) = F -1 g ( ⃗ L -1 g Q(⃗ x i -E[ ⃗ X] + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F ))) (53) = Q(⃗ x i -E[ ⃗ X] + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F )) + E[g • ⃗ X] (54) = Q(⃗ x i + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F )) -QE[ ⃗ X] + g • E[ ⃗ X]<label>(55)</label></formula><formula xml:id="formula_50">= Q(⃗ x i + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F )) + ⃗ t (56) = g • (⃗ x i + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F ))<label>(57)</label></formula><p>By replacing g with identical element I in E(3), since F = F I , we can immediately derive:</p><formula xml:id="formula_51">F -1 ⃗ f i (({h i , F (⃗ x i )}) = (⃗ x i + j∈N (i) (⃗ x i -⃗ x j )ϕ x (m ij,F )),<label>(58)</label></formula><formula xml:id="formula_52">g • F -1 ⃗ f i (({h i , F (⃗ x i )}) = F -1 g ⃗ f i ({h i , F g (g • ⃗ x i )}),<label>(59)</label></formula><p>which concludes Proposition 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Algorithm for Training and Sampling</head><p>We present the pseudo codes for training in Algorithm 1 amd sampling in Algorithm 2. Gp ← mask(G p ) {Mask 25% Residues} 6:</p><formula xml:id="formula_53">{(µ i , σ i , ⃗ µ i , ⃗ σ i )} ← E ϕ ( Gp , G b ) {Encoding} 7: {(ϵ i , ⃗ ϵ i )} ∼ N (0, I) {Reparameterization} 8: G z ← {(µ i + ϵ i ⊙ σ i , ⃗ µ i + ⃗ ϵ i ⊙ ⃗ σ i )} 9: G ′ p ← D ξ (G z , G b ) {Decoding} 10: L AE = i∈Gp (L recon (i) + L KL (i))/|G p | 11:</formula><p>ϕ, ξ ← optimizer(L AE ; ϕ, ξ)</p><p>12:</p><p>end while  </p><formula xml:id="formula_54">Sample (G p , G b ) ∼ S 20: {(µ i , σ i , ⃗ µ i , ⃗ σ i )} ← E ϕ (G p , G b ) {Encoding} 21: G 0 z ← {(µ i , ⃗ µ i )} 22: F ← affine(G b ) {Affine Transformation} 23: G 0 z , G b ← F (G 0 z ), F (G b ) {Standard Geometry} 24: {(z 0 i , ⃗ z 0 i )} ← G 0 z 25: t ∼ U(1, T ), {(ϵ i , ⃗ ϵ i )} ∼ N (0, I) 26: G t z ← { √ ᾱt [z 0 i , ⃗ z 0 i ] + (1 -ᾱt )[ϵ i , ⃗ ϵ i ]} 27: L t LDM = i ∥[ϵ i , ⃗ ϵ i ] -ϵ θ (G t z , G 1 , t)[i]∥ 2 /</formula><formula xml:id="formula_55">ϵ θ ← TrainLatentDiffusion(E ϕ , D ξ , S) 36: return E ϕ , D ξ , ϵ θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Data Preparation</head><p>We show details for constructing the datasets used in our paper here. Further statistics are presented in Table <ref type="table" target="#tab_6">5</ref> and distribution of peptide lengths in Figure <ref type="figure" target="#fig_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Unsupervised Data from Protein Fragments (ProtFrag)</head><p>We exploit unsupervised data from monomer proteins to enrich the training of the autoencoder. Specifically, we first extract all single chains from the Protein Data Bank (PDB) before December 8th, 2023, and remove the duplicated chains on a sequence identity threshold of over 90%. Then, for each chain, we extract fragments satisfying the following criteria:</p><formula xml:id="formula_56">Algorithm 2 Sampling Algorithm of PepGLAD input decoder D ξ , denoising network ϵ θ , binding site G b output peptide G p 1: F ← affine(G b ) {Affine Transformation} 2: G b ← F (G b ) {Standard Geometry} 3: {(z T i , ⃗ z T i )} ∼ N (0, I) 4: for t in T, T -1, • • • , 1 do 5: G t z ← {(z t i , ⃗ z t i )} {Latent Denoising Loop} 6: ⃗ u t i ← z t i , ⃗ z t i 7: ε i = [ϵ i , ⃗ ϵ i ] ∼ N (0, I) 8: ⃗ u t-1 i ← 1 √ α t (⃗ u t i -β t √ 1-ᾱt ϵ θ (G t z , G b , t)[i]) + β t ε i 9: [z t-1 i , ⃗ z t-1 i ] ← ⃗ u t-1 i 10: end for 11: G z ← {(z 0 i , ⃗ z 0 i )} 12: G z , G b ← F -1 (G z ), F -1 (G b ) {Data Geometry} 13: G p ← D ξ (G z , G b ) {Decoding} 14: return G p 1. Length:</formula><p>The fragment should consist of 4 to 25 residues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Balanced constitution:</head><p>No single amino acid should constitute more than 25% of the fragment; Hydrophobic amino acids should comprise less than 45% of the fragment, with charged amino acids accounting for 25% to 45%.</p><p>3. Isolated Stability: Instability <ref type="bibr" target="#b21">[22]</ref> should be below 40; Considering the surrounding amino acids as interaction partners, the fragment should have a buried surface area (BSA) above 400Å</p><p>2 <ref type="bibr" target="#b9">[10]</ref>, with a relative BSA above 20%.</p><p>We use FreeSASA <ref type="bibr" target="#b46">[47]</ref> to calculate the surface area of fragments. Let SA bound represent the surface area of the isolated fragment when considering surrounding amino acids, and SA unbound represent the surface area when not considering them. The buried surface area is then calculated as BSA = SA unbound -SA bound , and the relative BSA is calculated as BSA rel = BSA/SA unbound . In total, we obtain 70,645 fragments meeting these criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Construction of Our PepBench</head><p>Here we illustrate the details for constructing the supervised dataset (PepBench) used in our paper. Similar to literature <ref type="bibr" target="#b64">[65]</ref>, we also exploits available data from the Protein Data Bank (PDB) <ref type="bibr" target="#b4">[5]</ref>. We first extract all dimers in PDB deposited before December 8th, 2023, and filter out the complexes with a receptor longer than 30 residues and a ligand between 4 to 25 residues, which aligns with Tsaban et al. <ref type="bibr" target="#b58">[59]</ref>. Peptides with lengths in this range are more relevant to practical applications such as drug discovery, as they exhibit favorable biochemical properties <ref type="bibr" target="#b48">[49]</ref>. Then we remove the duplicated complexes with the criterion that both the receptor and the peptide has a sequence identity over 90% <ref type="bibr" target="#b55">[56]</ref>, after which 6105 non-redundant complexes are obtained. We use MMseqs2 for clustering based on sequence identity:</p><p># create database from the sequences mmseqs create seqs.fasta database # clustering with sequence identity above 90% mmseqs cluster database database_clusters results --min-seq-id 0.9 -c 0.95 --cov-mode 1</p><p>To achieve the cross-target generalization test, we utilize the large non-redundant dataset (LNR) introduced by Tsaban et al. <ref type="bibr" target="#b58">[59]</ref> as the test set, which is curated by domain experts. LNR originally includes 96 protein-peptide complexes. We obtain 93 complexes after excluding the ones with non-canonical amino acids. We then cluster the LNR along with the PDB data by receptor with a sequence identity threshold of over 40%. Subsequently, we remove the complexes sharing the same clusters with those from the test set and those including non-canonical amino acids in the peptides. Finally, the remaining data are randomly split based on clustering results into training and validation sets. The characteristics of different splits are presented in Table <ref type="table" target="#tab_6">5</ref>. In addition, the binding site contains residues on the receptor within 10Å distances to the peptide, where the distance between two residues is measured by the distance between their C β coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Split of PepBDB</head><p>Similar to the split of PepBench, we use MMseqs2 for clustering and randomly split the data into training, validation, and test sets based on the clustering results. For the test set, we randomly select one protein-peptide complex in each cluster to avoid redundancy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F E(3)-Invariant Latent Space</head><p>We found that the E(3)-equivariant latent vectors were significant, which can convey sufficient geometric information. If we use E(3)-invariant latent space without these geometric latent vectors, the reconstruction ability (RMSD, DockQ) of the VAE deteriorates significantly (Table <ref type="table" target="#tab_7">6</ref>). It is reasonable since E(3)-invariant latent space lacks geometric interactions with the pocket atoms, leading to difficulties in reconstructing the full-atom structures on the binding site. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Guidance on Sequence Orders for Sampling</head><p>Peptides consist of linearly connected amino acids, which exert constraints on the 3D geometry. Specifically, residues adjacent in the sequence should also be close in the structure, since they are connected by a peptide bond. However, 3D graphs are unordered and do not incorporate such induct bias, which means the generated nodes might have arbitrary permutation on sequence orders. To tackle this problem, we take inspiration from classifier-guided diffusion <ref type="bibr" target="#b14">[15]</ref>, which adds the gradient of a classifier to the denoising outputs to guide the generative diffusion process towards the subspace where the classifier gives high confidence. We utilize an empirical classifier p(1|{⃗ z t i }) defined as follows:</p><formula xml:id="formula_57">p(1|{⃗ z t i }) = exp(- P(i)-P(j)=1 E(∥⃗ z t i -⃗ z t j ∥)),<label>(60)</label></formula><formula xml:id="formula_58">E(d) = d -(µ d + 3σ d ), d &gt; µ d + 3σ d , (µ d -3σ d ) -d, d &lt; µ d -3σ d , 0, otherwise,<label>(61)</label></formula><p>where µ d and σ d are the mean and variance of the distances of adjacent residues in the latent space measured from the training set. With p(1|{⃗ z t i }), we are able to assign an arbitrary permutation on sequence orders to the nodes, and steer the sampling procedure towards the desired subspace conforming to P, since this classifier gives higher confidence if the adjacent (defined by P) residues are within reasonable distances aligning with the statistics from the training set. In particular, the coordinate denoising outputs are refined as follows:</p><formula xml:id="formula_59">⃗ ϵ t i = ⃗ ϵ θ (G t z , G b , t)[i] -λ √ 1 -ᾱt ∇ ⃗ z t i log p(1|{⃗ z t i }),<label>(62)</label></formula><p>where λ adjusts the weight of the guidance. Besides the constraints on the distance between adjacent residues, we can also include guidance on avoiding clashes between non-adjacent residues by defining the following energy term:</p><formula xml:id="formula_60">C(d) = µ d -d, d &lt; µ d , 0, otherwise,<label>(63)</label></formula><p>Subsequently, we just need to revise the empirical classifier as:</p><formula xml:id="formula_61">p(1|{⃗ z t i }) = exp(- P(i)-P(j)=1 E(∥⃗ z t i -⃗ z t j ∥) - P(i)-P(j)̸ =1 C(∥⃗ z t i -⃗ z t j ∥) - i∈Gz,j∈G b C(∥⃗ z t i -⃗ r j ∥),<label>(64)</label></formula><p>where ⃗ r j is the C α coordinate of node j in the binding site. We observe a slight improvement upon including the clash energy term. Nevertheless, the guidance is only a small technical trick with minor enhancement on the performance as shown in Table <ref type="table" target="#tab_8">7</ref>.  While the amino acid recovery (AAR) is widely used in antibody design <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>, we find it not informative enough for evaluating generative models on learning the distribution of binding peptides, due to the vast and highly diverse solution space. To elucidate this, we conducted an analysis of AAR using a sequence dataset from Xia et al. <ref type="bibr" target="#b66">[67]</ref> including 328 receptors and 600k peptides with binary binding labels, from which we filter out 102 receptors with well-explored solution space (i.e. with at least 500 binders). For each receptor, we randomly select one binder as the reference and sample N candidates according to a specified positive ratio r, resembling the scenario of evaluating generative models for peptide design. For example, setting r = 0.1 and N = 100 involves sampling 10 binders and 90 non-binders to construct the candidates. Then we compute the best AAR of these candidates with respect to the reference as the result. This process is repeated for 10 times per receptor, and the results are averaged across different receptors, which can be interpreted as the evaluation score of AAR on a generative model that can generates candidates with a positive ratio of r. We select N = 10, 40, 100 and enumerate the choices of r to derive the relation plot in Figure <ref type="figure" target="#fig_7">6</ref>, where the results of a random sequence generator is also included for comparison. It can be derived that the gap of AAR between the worst model (r = 0.0) and the best model (r = 1.0) is insignificant. Furthermore, all models, including the best model (r = 1.0) which always produces positive samples, exhibit performance akin to the random sequence generator, with consistent trends regardless of the choice of N . We attribute this to the vast solution space of peptide design, where evaluating with dozens of candidates relative to a single reference is unreliable. In other words, achieving a high AAR is improbable since the model would need to fortuitously explore the subspace around the reference, which is arbitrary, on every test receptor; Conversely, a low AAR does not necessarily denote a poor generative model, as the model may be exploring distinct solutions from the single reference. Moreover, we calculate the Spearman correlation between the receptor-level best AAR and the positive ratio r of the candidates, yielding 0.23, 0.22 and 0.24 for N = 10, 40, and 100, respectively, indicating very weak correlation. Based on this analysis, we assert that AAR is unsuitable for evaluating target-specific peptide design. The comparison of AAR and success rates on PepBench (Table <ref type="table" target="#tab_9">8</ref>) also indicates that models with similar AAR can have distinct success rates. While it is common to evaluate consistency by comparing generated structures with AF2-predicted structures <ref type="bibr" target="#b6">[7]</ref>, such a "supervision-based" method suffers from severe limitations in the situation where AF2 <ref type="bibr" target="#b30">[31]</ref> fails to achieve an acceptable performance. As highlighted not only in Tsaban et al. <ref type="bibr" target="#b58">[59]</ref> but also demonstrated by our experiments, even state-of-the-art models like AF2 struggles to consistently produce high-quality structures of protein-peptide complexes. Our findings reveal that only 36% of test samples could be accurately predicted within a 5Å RMSD (considered as near-native conformation) by AF2, indicating even the ground truth can only achieve 36% success rates if we use such supervison-based consistency for evaluation, making such evaluation not reliable in the domain of peptide design.</p><p>In light of this limitation, we propose an "unspervised" evaluation framework to assess consistency, that is, the statistical association between the clustering results of sequences and structures. Theoretically, this serves the necessary condition for true consistency. Namely, if a model truly captures the consistency between sequence and structure, it will necessarily achieve a high score on the proposed metric. Conversely, if a model fails to attain a high score on the proposed metric, it is not possible to capture true consistency.</p><p>In our experiments, we found that our proposed consistency metric effectively distinguishes nonconsistent modeling methods, such as HSRN, which tend to produce disparate sequences while sharing identical structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Implememtation and Elaboration on Metrics</head><p>Indeed, evaluating generative models comprehensively is crucial, requiring assessment from multiple perspectives. Generally, these evaluations can be categorized into two main aspects: diversity and fidelity to the desired distribution. Regarding diversity, we take inspiration from Yim et al. <ref type="bibr" target="#b71">[72]</ref> and quantify it with the number of unique clusters relative to the number of candidates. This metric provides insight into the variety and richness of the generated samples. For fidelity, the primary focus should be the binding affinity, for which we adopt the physical energy from Rosetta <ref type="bibr" target="#b1">[2]</ref> since it is widely used in various domains and exhibit robust generalization capability <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">64]</ref>. Further, considering the dependency between sequence and structure, we propose the consistency metric, which is critical for distinguishing whether the generative model is capturing the 1D&amp;3D joint distribution and thereby truly facilitating "co-design". We have discussed the reason for implementing the consistency metric with the "unsupervised" fashion in the above section. Below, we outline the implementation of these metrics.</p><p>Diversity We hierarchically cluster the sequences and the structures with aligning score <ref type="bibr" target="#b35">[36]</ref> and RMSD of C α , respectively. In particular, we implement the aligning score using Biopython <ref type="bibr" target="#b10">[11]</ref> with BLOSUM62 matrix <ref type="bibr" target="#b23">[24]</ref> and Needleman-Wunsch algorithm <ref type="bibr" target="#b49">[50]</ref>. The thresholds for clustering is similarity above 0.6 and RMSD below 4.0 for sequence and structure, respectively. We provide the python codes below for clearer presentation: Denoting the diversity of the sequences and the structures as Div seq and Div struct , respectively, we calculate the co-design diversity as Div seq Div struct . Success Since a negative ∆G value typically indicates a potential for binding, we report the ratio of all generated candidates that satisfy this threshold. Moreover, candidates with ∆G &lt; 0 usually are at least physically valid (i.e. without obvious atomic clash), thus it is meaningful to use this success rate to evaluation the generative ability of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Experiment Details I.1 Implementation of PepGLAD</head><p>We train PepGLAD on a GPU with 24G memory with AdamW optimizer. For the autoencoder, we train for 60 epochs with dynamic batches, ensuring that the total number of edges (proportional to the square of the number of nodes) remains below 60,000. The initial learning rate is 10 -4 and decays by 0.8 if the loss on the validation set has not decreased for 3 consecutive epochs. Regarding the diffusion model, we train for 500 epochs with the same batching strategy as for the autoencoder. The learning rate is 10 -4 and decay by 0.6 if the loss has not decreased for 3 consecutive validations, where the validation is conducted every 10 epochs. In the experiment of binding conformation generation, we only use the supervised dataset, thus we extend the training epochs for the autoencoder and the diffusion model to 500 and 1000, respectively. Consequently, the patience of learning rate decay is extended to 15 epochs for training the autoencoder. We keep other settings unchanged. The hyperparameters of PepGLAD used in our experiments are provided in Table <ref type="table" target="#tab_10">9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Implementation of the Baselines</head><p>For HSRN <ref type="bibr" target="#b29">[30]</ref>, dyMEAN <ref type="bibr" target="#b33">[34]</ref>, and DiffAb <ref type="bibr" target="#b43">[44]</ref>, we directly integrate their official implementation into the same training framework as our PepGLAD, and adjust the batch size, learning rate, and training epochs to obtain the optimal performance, which we present in Table <ref type="table" target="#tab_11">10</ref>.</p><p>We outline the implementation of other baselines below: RFDiffusion <ref type="bibr" target="#b63">[64]</ref> We follow the official instruction to randomly select 20% of residues on the binding site as "hotspots", and generate the backbone via diffusion followed by cycles of inverse folding with ProteinMPNN <ref type="bibr" target="#b13">[14]</ref> and full-atom structural refining with the officially provided Rosetta protocol.</p><p>AnchorExtension <ref type="bibr" target="#b25">[26]</ref> It is with the Rosetta suite, thus we resort to the official release of the pipeline protocols for docking and optimizing. We use the default parameters and generate 10 candidates for each receptor due to its limitation of efficiency. We randomly pick one peptide in the training set with the same length as the reference peptide as the initial motif for docking.</p><p>FlexPepDock <ref type="bibr" target="#b41">[42]</ref> We follow its official tutorial to implement this baseline in the C++ version of Rosetta.</p><p>AlphaFold2 <ref type="bibr" target="#b30">[31]</ref> We borrow the results from <ref type="bibr" target="#b58">[59]</ref>, which explores two strategy to use AlphaFold2 on peptide conformation generation, including modeling the receptor and the peptide as separate chains or link them together with long loops. The results contain a total of 10 candidates for each receptor, with 5 from the separate strategy and 5 from the linked strategy.</p><p>NeurIPS Paper Checklist Justification: We have an independent section ( § 6) to discuss limitations. Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: We provide codes and data for our model and the experiments.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code.</p><p>• Please see the NeurIPS code and data submission guidelines (<ref type="url" target="https://nips.cc/public/guides/CodeSubmissionPolicy">https://nips.cc/ public/guides/CodeSubmissionPolicy</ref>) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e. • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes] Justification: See § 4.1, Appendix E and Appendix I.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p><p>Answer: <ref type="bibr">[No]</ref> Justification: The size of the dataset is large, and we report results on two different datasets for more solid evaluation.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).</p><p>• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p><p>Answer: [NA]</p><p>Justification: The paper does not pose such risks.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p><p>12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p><p>Answer: [Yes] Justification: They are properly credited.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture of PepGLAD. (A) Variational AutoEncoder ( §3.2): compressing the sequence and the structure {(x i , ⃗ X i )} of the peptide into the latent space {(z i , ⃗ z i )} with the encoder E ϕ , and decoding the sequence and full-atom geometry from the latent states with the decoder D ξ . (B) Affine Transformation F ( §3.3): projecting the geometry to approximately N (0, I) via the receptor-specific affine transformation derived from the binding site, and recovering the data geometry with the inverse of F after the diffusion generative process. (C) Latent Diffusion ( §3.4): jointly generating z i and ⃗ z i in the standard latent space.</figDesc><graphic coords="4,118.09,87.64,57.65,72.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top: A generated candidate confined within the binding site (PDB=4cu4, ∆G=-34.21).Bottom: A generated candidate with complementary shape to the binding site (PDB=3pkn, ∆G=-33.32). Both candidates form compact interactions at the interface.</figDesc><graphic coords="8,301.20,355.86,112.03,80.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Reference (PDB: 3vxw)Generated by PepGLAD RMSDC⍺ =1.86Å RMSD atom =2.96Å DockQ =0.846</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The distribution of RMSD Cα on the test set of PepBench and a visualized sample.</figDesc><graphic coords="9,116.66,404.28,129.30,94.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>Discarding the full-atom context results in a significant degradation on all metrics, especially the success rate, implying the necessity of the full-atom context in capturing the intricate protein-peptide interactions; (2) Implementing the diffusion directly on the data space without the proposed affine transformation incurs a notably adverse impact on all metrics, indicating the remarkable enhancement on the generalization capability made by the affine transformation;<ref type="bibr" target="#b2">(3)</ref> Training without the unsupervised data leads to a less informative latent space, exerting a negative effect on the binding energy and success rate; (4) Removal of the mask policy reduces the correlation between sequence and structure in the latent space, thus harms the consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 2 :Initialize E ϕ , D ξ 3 :</head><label>123</label><figDesc>Training Algorithm of PepGLAD input geometric data of protein-peptide complexes S output encoder E ϕ , decoder D ξ , denoising network ϵ θ 1: function TrainAutoEncoder(S) while ϕ, ξ have not converged do 4: Sample (G p , G b ) ∼ S 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Peptide (or protein fragment) length distribution of three datasets.</figDesc><graphic coords="20,108.13,293.49,133.85,98.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Best amino acid recovery (AAR) on samples constructed according to specified positive ratios. Each point contains the averaged result across 102 receptors.</figDesc><graphic coords="21,108.00,577.69,133.70,100.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>from math import sqrt from typing import List from Bio . Align import substitution_matrices , P a ir wi s eA li g ne r import numpy as np def align_score ( sequence_A , sequence_B ) : # load matrix sub_matrice = s u b s t i t u t i o n _ m a t r i c e s . load ( ' BLOSUM62 ') aligner = P a ir wi s eA l ig ne r () aligner . s u b s t i t u t i o n _ m a t r i x = sub_matrice # align alns = aligner . align ( sequence_A , sequence_B ) best_aln = alns [0] aligned_A , aligned_B = best_aln # normalize base_A = aligner . score ( sequence_A , sequence_A ) base_B = aligner . score ( sequence_B , sequence_B ) base = sqrt ( base_A * base_B ) similarity = aligner . score ( sequence_A , sequence_B ) / base return similarity def seq_diversity ( seqs : List [ str ] , th : float =0.4) -&gt; float : ''' th : sequence distance (1 -similarity ) ''' dists = [] for i , sequence_A in enumerate ( seqs ) : dists . append ([]) for j , sequence_B in enumerate ( seqs ) : dists [ i ]. append (1 -align_score ( sequence_A , sequence_B ) ) dists = np . array ( dists ) Z = linkage ( squareform ( dists ) , ' single ') cluster = fcluster (Z , t = th , criterion = ' distance ') return len ( np . unique ( cluster ) ) / len ( seqs ) def s t r u c t _ d i v e r s i t y ( structs : np . ndarray , th : float =4.0) -&gt; float :''' structs : N * L *3 , alpha carbon coordinates th : threshold for clustering ( distance &lt; th ) ''' ca_dists = np . sum (( structs [: , None ] -structs [ None , :]) ** 2 , axis = -1) rmsd = np . sqrt ( np . mean ( ca_dists , axis = -1) ) Z = linkage ( squareform ( rmsd ) , ' single ') cluster = fcluster (Z , t = th , criterion = ' distance ') return len ( np . unique ( cluster ) ) / structs . shape[0]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>r o s e t t a _ i n t e r f a c e _ e n e r g y ( pdb_path , receptor_chains , ligand_chains ) : # binding energy pose = pyrosetta . pose_from_pdb ( pdb_path ) interface = ' '. join ( ligand_chains ) + '_ ' + ' '. join ( r e ce pt o r_ c ha in s ) = I n t e r f a c e A n a l y z e r M o v e r ( interface ) mover . s e t _ p a c k _ s e p a r a t e d ( True ) mover . apply ( pose ) return pose . scores[ ' dG_separated ']    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation on sequence-structure co-design. On each target, 40 candidates are generated for evaluation. Div. and Con. are abbreviations for diversity and consistency, respectively.</figDesc><table><row><cell>Model</cell><cell cols="4">PepBench Div.(↑) Con.(↑) ∆G(↓) Success</cell><cell cols="4">PepBDB Div.(↑) Con.(↑) ∆G(↓) Success</cell></row><row><cell>Test Set</cell><cell>-</cell><cell>-</cell><cell cols="2">-35.25 95.70%</cell><cell>-</cell><cell>-</cell><cell cols="2">-35.96 95.79%</cell></row><row><cell>HSRN 3</cell><cell>0.158</cell><cell>0.0</cell><cell>≥ 0</cell><cell>10.46%</cell><cell>0.111</cell><cell>0.0</cell><cell>≥ 0</cell><cell>10.86%</cell></row><row><cell>dyMEAN</cell><cell>0.150</cell><cell>0.0</cell><cell cols="2">-2.26 14.60%</cell><cell>0.150</cell><cell>0.0</cell><cell>-1.92</cell><cell>6.26%</cell></row><row><cell>DiffAb</cell><cell>0.427</cell><cell>0.670</cell><cell cols="2">-21.20 49.87%</cell><cell>0.269</cell><cell>0.463</cell><cell cols="2">-18.40 41.45%</cell></row><row><cell cols="2">PepGLAD (ours) 0.506</cell><cell>0.789</cell><cell cols="2">-21.94 55.97%</cell><cell>0.692</cell><cell>0.923</cell><cell cols="2">-21.53 48.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on sequence-structure co-design with two well-established systems. Time cost is measured as the total time spent divided by the number of designed candiates. For each receptor, we generate 10 candidates and report the median value of the following metrics across different receptors to measure how well the generated distribution can recover the reference conformation: (1) RMSD Cα : Root mean square deviation on the coordinates of C α between a candidate and a reference structure with the unit Å. (2) RMSD atom : RMSD on all atoms to measure the quality of the full-atom geometry. (3) DockQ<ref type="bibr" target="#b3">[4]</ref>. A comprehensive metric evaluating the full-atom similarity on the interface between a candidate and a reference complex. It ranges from 0 to 1, with values above 0.23 and 0.49 considered as acceptable and medium quality, respectively.</figDesc><table><row><cell>Model</cell><cell cols="4">Div.(↑) Con.(↑) ∆G(↓) Success Time</cell></row><row><cell cols="2">AnchorExtension 0.245</cell><cell>0.423</cell><cell cols="2">-26.80 84.30% 735s</cell></row><row><cell>RFDiffusion</cell><cell>0.259</cell><cell>0.696</cell><cell>-33.82 79.68%</cell><cell>61s</cell></row><row><cell>PepGLAD (ours)</cell><cell>0.506</cell><cell>0.789</cell><cell>-29.36 92.82%</cell><cell>3s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on binding conformation generation. On each target, 10 candidates are generated to calculate the optimal recall of the reference conformation.</figDesc><table><row><cell>Model</cell><cell cols="3">PepBench RMSD Cα (↓) RMSD atom (↓) DockQ(↑)</cell><cell cols="3">PepBDB RMSD Cα (↓) RMSD atom (↓) DockQ(↑)</cell></row><row><cell>FlexPepDock</cell><cell>6.43</cell><cell>7.52</cell><cell>0.393</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AlphaFold 2</cell><cell>8.49</cell><cell>9.20</cell><cell>0.355</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>dyMEAN</cell><cell>7.96</cell><cell>8.35</cell><cell>0.374</cell><cell>17.64</cell><cell>17.56</cell><cell>0.142</cell></row><row><cell>HSRN</cell><cell>6.02</cell><cell>7.59</cell><cell>0.508</cell><cell>9.28</cell><cell>9.72</cell><cell>0.394</cell></row><row><cell>DiffAb</cell><cell>4.23</cell><cell>7.60</cell><cell>0.586</cell><cell>13.96</cell><cell>13.12</cell><cell>0.236</cell></row><row><cell>PepGLAD (ours)</cell><cell>4.09</cell><cell>5.30</cell><cell>0.592</cell><cell>8.87</cell><cell>8.62</cell><cell>0.403</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablations on different components. Avg. computes the average of all metrics, where ∆G is first normalized by the median value of the references on test set.</figDesc><table><row><cell>Ablations</cell><cell cols="3">Div.(↑) Con.(↑) ∆G(↓) Success Avg.</cell></row><row><cell>PepGLAD</cell><cell>0.506</cell><cell>0.789</cell><cell>-21.94 55.97% 0.619</cell></row><row><cell cols="2">w/o Full-Atom 0.441</cell><cell>0.751</cell><cell>-20.87 51.18% 0.574</cell></row><row><cell>w/o Affine</cell><cell>0.450</cell><cell>0.740</cell><cell>-19.08 52.39% 0.564</cell></row><row><cell>w/o ProtFrag</cell><cell>0.535</cell><cell>0.760</cell><cell>-20.16 52.15% 0.597</cell></row><row><cell>w/o Mask</cell><cell>0.422</cell><cell>0.741</cell><cell>-20.45 57.44% 0.579</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>function TrainLatentDiffusion(E ϕ , D ξ , S)</figDesc><table><row><cell>13:</cell><cell>return E ϕ , D ξ</cell></row><row><cell cols="2">14: end function</cell></row><row><cell>15:</cell><cell></cell></row><row><cell>16: 17:</cell><cell>Initialize ϵ θ</cell></row><row><cell>18:</cell><cell>while θ have not converged do</cell></row><row><cell>19:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the constructed datasets.</figDesc><table><row><cell>Split</cell><cell cols="2">#entry #cluster source</cell></row><row><cell>PepBench (Training)</cell><cell>4,157</cell><cell>952 PDB [5]</cell></row><row><cell>PepBench (Validation)</cell><cell>114</cell><cell>50 PDB [5]</cell></row><row><cell>PepBench (Test)</cell><cell>93</cell><cell>93 LNR [59]</cell></row><row><cell>PepBDB (Training)</cell><cell>8,434</cell><cell>1,617 PepBDB [65]</cell></row><row><cell>PepBDB (Validation)</cell><cell>370</cell><cell>95 PepBDB [65]</cell></row><row><cell>PepBDB (Test)</cell><cell>190</cell><cell>190 PepBDB [65]</cell></row><row><cell>ProtFrag</cell><cell>70,645</cell><cell>-monomers in PDB</cell></row></table><note><p>(A) PepBench (B) ProtFrag (C) PepBDB</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of reconstruction ability of variational autoencoders with E(3)-equivariant latent space and E(3)-invariant latent space.</figDesc><table><row><cell>Latent Space</cell><cell cols="3">AAR↑ RMSD ↓ DockQ↑</cell></row><row><cell cols="2">E(3)-equivariant 95.1%</cell><cell>0.79</cell><cell>0.898</cell></row><row><cell>E(3)-invariant</cell><cell>93.4%</cell><cell>1.75</cell><cell>0.823</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results on binding conformation generation with and without guidance on sequence orders.Guidance RMSD Cα ↓ RMSD atom ↓ DockQ↑</figDesc><table><row><cell>w/</cell><cell>4.09</cell><cell>5.30</cell><cell>0.592</cell></row><row><cell>w/o</cell><cell>4.10</cell><cell>5.34</cell><cell>0.582</cell></row></table><note><p>H Metrics for Evaluating Sequence-Structure Co-Design H.1 Why not Amino Acid Recovery (AAR)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Amino acid recovery and success rates of different models on PepBench.</figDesc><table><row><cell>Model</cell><cell cols="2">AAR Success (∆G &lt; 0)</cell></row><row><cell>HSRN</cell><cell>35.8%</cell><cell>10.46%</cell></row><row><cell>DiffAb</cell><cell>37.1%</cell><cell>49.87%</cell></row><row><cell cols="2">PepGLAD 36.7%</cell><cell>55.97%</cell></row><row><cell>H.2 "Unsupervised" Consistency</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters of PepGLAD in sequence-structure codesign and binding conformation generation.</figDesc><table><row><cell>Name</cell><cell cols="2">Value codesign conformation</cell><cell>Description</cell></row><row><cell></cell><cell></cell><cell cols="2">Variational AutoEncoder</cell></row><row><cell>embed size</cell><cell>128</cell><cell cols="2">128 Size of embeddings for residue types.</cell></row><row><cell>hidden size</cell><cell>128</cell><cell cols="2">128 Size of hidden layers.</cell></row><row><cell>h</cell><cell>8</cell><cell cols="2">-Size of the latent variable for residue types in the sequences.</cell></row><row><cell>layers</cell><cell>3</cell><cell cols="2">3 Number of layers.</cell></row><row><cell>λ 1</cell><cell>0.1</cell><cell cols="2">0.0 The weight of KL divergence on the sequence.</cell></row><row><cell>λ 2</cell><cell>0.5</cell><cell cols="2">0.5 The weight of KL divergence on the structure.</cell></row><row><cell>λ CA</cell><cell>1.0</cell><cell cols="2">1.0 The weight of C α loss in L aux .</cell></row><row><cell>λ bond</cell><cell>1.0</cell><cell cols="2">1.0 The weight of bond loss in L aux .</cell></row><row><cell>λ angle</cell><cell>0.5</cell><cell cols="2">0.5 The weight of side-chain dihedral angle loss in L aux .</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Latent Diffusion Model</cell></row><row><cell>hidden size</cell><cell>128</cell><cell cols="2">128 Size of hidden layers in the denoising network.</cell></row><row><cell>layers</cell><cell>3</cell><cell cols="2">3 Number of layers.</cell></row><row><cell>steps</cell><cell>100</cell><cell cols="2">100 Number of diffusion steps.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Hyperparamenters for training the baselines.</figDesc><table><row><cell>Model</cell><cell cols="3">Batch Size Learning Rate Epoch</cell></row><row><cell>HSRN</cell><cell>8</cell><cell>1.0e-4</cell><cell>50</cell></row><row><cell>dyMEAN</cell><cell>32</cell><cell>1.0e-4</cell><cell>100</cell></row><row><cell>DiffAb</cell><cell>32</cell><cell>1.0e-4</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.2. LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes]   </figDesc><table><row><cell>1. Claims</cell></row><row><cell>Question: Do the main claims made in the abstract and introduction accurately reflect the</cell></row><row><cell>paper's contributions and scope?</cell></row><row><cell>Answer: [Yes]</cell></row><row><cell>Justification:  § 3 and  § 4 illustrate the claims well from the methodology and the empirical</cell></row><row><cell>aspects.</cell></row><row><cell>Guidelines:</cell></row><row><cell>•</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The binding site has at least</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>nonoverlapping nodes, namely rank( ⃗ R) = 3, thus we can ignore the corner case of semi-positive definite matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>HSRN and dyMEAN generate homogeneous structures that are clustered together yet still sample very different sequences, leading to zero association between squence and structure.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is jointly supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No.<rs type="grantNumber">2022ZD0160502</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61925601</rs>, No. <rs type="grantNumber">62376276</rs>, No. <rs type="grantNumber">62276152</rs>), and <rs type="programName">Beijing Nova Program</rs> (<rs type="grantNumber">20230484278</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CyuFQ8R">
					<idno type="grant-number">2022ZD0160502</idno>
				</org>
				<org type="funding" xml:id="_BTPAjJP">
					<idno type="grant-number">61925601</idno>
				</org>
				<org type="funding" xml:id="_3TtqF2t">
					<idno type="grant-number">62376276</idno>
				</org>
				<org type="funding" xml:id="_kYDfkNJ">
					<idno type="grant-number">62276152</idno>
					<orgName type="program" subtype="full">Beijing Nova Program</orgName>
				</org>
				<org type="funding" xml:id="_PuHTbcK">
					<idno type="grant-number">20230484278</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consistency The clustering process in the calculation of diversity will assign each sequence and each structure with a clustering label. The labels on the sequences and those on the structures can be regarded as two nominal variables. Since similar sequences should produce similar structures, these two variables should be highly correlated if the model really learns the joint distribution. Naturally, we quantify the consistency via Cramér's V <ref type="bibr" target="#b11">[12]</ref> association between these two variables, 1.0 indicating perfect association, and 0.0 means no association.</p><p>∆G We use Rosetta <ref type="bibr" target="#b1">[2]</ref> to calculate the binding energy with the "ref2015" score function. Both the candidates and the references first endure the fast relax protocol in Rosetta to correct atomic clashes at the interface before the computation of binding energy. We use the implementation in pyRosetta (i.e. the python version of Rosetta), which is borrowed from Luo et al. <ref type="bibr" target="#b43">[44]</ref>:</p><p>import pyrosetta from pyrosetta . rosetta import protocols from pyrosetta . rosetta . protocols . relax import FastRelax from pyrosetta . rosetta . core . pack . task import TaskFactory from pyrosetta . rosetta . core . pack . task import operation from pyrosetta . rosetta . core . select import r e s i d u e _ s e l e c t o r as selections from pyrosetta . rosetta . core . select . movemap import MoveMapFactory , mo ve _ ma p _a ct i on from pyrosetta . rosetta . core . scoring import ScoreType from pyrosetta . rosetta . protocols . analysis import I n t e r f a c e A n a l y z e r M o v e r • The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In addition to the methodology section ( § 3), we provide implementation details in Appendix I. Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Open access to data and code</head><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>• It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments Compute Resources</head><p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: See Appendix I.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Code Of Ethics</head><p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <ref type="url" target="https://neurips.cc/public/EthicsGuidelines">https://neurips.cc/public/EthicsGuidelines</ref>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [Yes]</head><p>Justification: We ensure the research to conform to the NeurIPS Code of Ethics.</p><p>Guidelines:</p><p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p><p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Broader Impacts</head><p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p><p>Answer: [Yes]</p><p>Justification: See § 7.</p><p>Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p><p>• If this information is not available online, the authors are encouraged to reach out to the asset's creators. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general framework for computational antibody design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adolf-Bryfogle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalyuzhniy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Weitzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Rosettaantibodydesign</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1006112</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The rosetta all-atom energy function for macromolecular modeling and design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jeliazkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>O'meara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kappel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3031" to="3048" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Protein structure and sequence generation with equivariant denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Achim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15019</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dockq: a quality measure for protein-protein docking models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wallner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">161879</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate de novo design of hyperstable constrained peptides</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cheneval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Buchko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Pulavarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eletsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7625</biblScope>
			<biblScope unit="page" from="329" to="335" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Se (3)-stochastic flow matching for protein backbone generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akhound-Sadegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evobind: in silico directed evolution of peptide binders with alphafold</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2027" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design of protein-binding proteins from the target structure alone</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coventry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sheffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Jude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">U</forename><surname>Kadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Verschueren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7910</biblScope>
			<biblScope unit="page" from="551" to="560" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principles of protein-protein recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chothia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Janin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="issue">5520</biblScope>
			<biblScope unit="page" from="705" to="708" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Biopython: freely available python tools for computational molecular biology and bioinformatics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Antao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dalke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Friedberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamelryck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kauff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1422</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mathematical methods of statistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cramér</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Princeton university press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cipcigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="613" to="623" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust deep learning-based protein sequence design using proteinmpnn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Wicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6615</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07511</idno>
		<title level="m">A hitchhiker&apos;s guide to geometric gnns for 3d atomic systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Peptide therapeutics: current status and future directions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fosgerau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="128" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A latent diffusion model for protein structure generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mcthrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komikado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maruhashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning on Graphs Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
		<imprint>
			<publisher>JHU press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The x-pro peptide bond as an nmr probe for conformational studies of flexible linear peptides</title>
		<author>
			<persName><forename type="first">C</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wüthrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers: Original Research on Biomolecules</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2025" to="2041" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d equivariant diffusion for targetaware molecule generation and affinity prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correlation between stability of a protein and its dipeptide composition: a novel approach for predicting in vivo stability of a protein from its primary sequence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guruprasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Pandit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Engineering, Design and Selection</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="161" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07230</idno>
		<title level="m">Geometrically equivariant graph neural networks: A survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="10915" to="10919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anchor extension: a structure-guided approach to design cyclic peptides targeting enzyme active sites</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rettie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pardo-Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3384</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Illuminating protein space with a programmable generative model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ng-Thow-Hing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Van Vlack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Iterative refinement graph neural network for antibody sequence-structure co-design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Antibody-antigen docking and design via hierarchical equivariant refinement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06616</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.06073</idno>
		<title level="m">Conditional antibody design as 3d equivariant graph translation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00203</idno>
		<title level="m">End-to-end full-atom antibody design</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive review on current advances in peptide drug development and design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of molecular sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2383</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deep-learning framework for multi-level peptide-protein interaction prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5465</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Macromolecular modeling and design in rosetta: recent methods and frameworks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Weitzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adolf-Bryfogle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aprahamian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="665" to="680" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Full-atom peptide design based on multi-modal flow matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Functional-groupbased diffusion for pocket-specific molecule generation and elaboration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ppflow: Targetaware peptide design with torsional flow matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The structural basis of peptideprotein binding strategies</title>
		<author>
			<persName><forename type="first">N</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schueler-Furman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="199" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rosetta flexpepdock web server-high resolution modeling of peptide-protein interactions</title>
		<author>
			<persName><forename type="first">N</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schueler-Furman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">suppl_2</biblScope>
			<biblScope unit="page" from="249" to="W253" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A 3d generative model for structure-based drug design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6229" to="6239" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9754" to="9767" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Abdiffuser: Full-atom generation of in-vitro functioning antibodies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ludwiczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafrance-Vanasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hotzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.05027</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Propedia: a database for proteinpeptide identification based on a hybrid clustering algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mariano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Queiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D S</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>De Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Freesasa: An open source c library for solvent accessible surface area calculations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitternacht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">F1000Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recurrent neural network model for constructive peptide design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="472" to="479" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Trends in peptide drug discovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muttenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Alewood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews Drug discovery</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="325" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pocket2mol: Efficient molecular sampling based on 3d protein pockets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17644" to="17655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Tertiary motifs as building blocks for the design of protein-binding peptides</title>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grigoryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Keating</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4322</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04119</idno>
		<title level="m">Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Harnessing protein folding neural networks for peptide-protein docking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ben-Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khramushin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schueler-Furman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Computational design of peptide ligands</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vanhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Van Der Sloot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Verschueren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schymkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in biotechnology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><surname>Abode</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01005</idno>
		<title level="m">Ab initio antibody design using conjoined odes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Accelerating antimicrobial peptide discovery with latent sequence-structure model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09450</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">De novo design of protein structure and function with rfdiffusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Eisenach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7976</biblScope>
			<biblScope unit="page" from="1089" to="1100" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pepbdb: a comprehensive structural database of biological peptide-protein interactions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Protein structure generation via folding diffusion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alamdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1059</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Computational prediction of mhc anchor locations guides neoantigen identification and prioritization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcmichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker-Hapak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Onyeador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buchli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcclain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Supabphol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Richters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science immunology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page">2200</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Helixgan a deep-learning methodology for conditional de novo design of α-helix structures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Valiente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Helixdiff, a score-based diffusion model for generating all-atom α-helical structures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Valiente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1001" to="1011" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Geodiff: A geometric diffusion model for molecular conformation generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02923</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Geometric latent diffusion models for 3d molecule generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38592" to="38610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Se (3) diffusion model with application to protein backbone generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02277</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01794</idno>
		<title level="m">Diffpack: A torsional diffusion model for autoregressive protein side-chain packing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
