<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Cause Effect Estimation with Disentangled Confounder Representation</title>
				<funder ref="#_Bcn5vHP #_Hpcd9jg">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<postCode>22904</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
							<email>aidong@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<postCode>22904</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<postCode>22904</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Cause Effect Estimation with Disentangled Confounder Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One fundamental problem in causality learning is to estimate the causal effects of one or multiple treatments (e.g., medicines in the prescription) on an important outcome (e.g., cure of a disease). One major challenge of causal effect estimation is the existence of unobserved confounders -the unobserved variables that affect both the treatments and the outcome. Recent studies have shown that by modeling how instances are assigned with different treatments together, the patterns of unobserved confounders can be captured through their learned latent representations. However, the interpretability of the representations in these works is limited. In this paper, we focus on the multicause effect estimation problem from a new perspective by learning disentangled representations of confounders. The disentangled representations not only facilitate the treatment effect estimation but also strengthen the understanding of causality learning process. Experimental results on both synthetic and real-world datasets show the superiority of our proposed framework from different aspects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One major challenge of causal effect estimation is the existence of unobserved confounders [Rosenbaum and <ref type="bibr">Rubin, 1983]</ref>, i.e., the unobserved variables which influence both treatment assignment and outcomes <ref type="bibr">[Shalit et al., 2017]</ref>. Unobserved confounders can cause confounding bias to the estimated treatment effect. A line of recent studies infers unobserved confounders by modeling how instances are assigned with different treatments together. The problem is known as the multiple treatment effect (MTE) estimation <ref type="bibr">[Wang and Blei, 2019]</ref>. A typical example is to estimate how intervening the cast of movies would change their potential revenues, e.g., "how much does the revenue (outcome) increase or decrease if Oprah Winfrey is in the movie?". Here, the genre of the movie is an unobserved confounder that affects which actors would star the movie as well as the revenue; and different actors in the cast (multiple treatment assignment) can provide complementary insights in revealing the movie genre.</p><p>Specifically, recent studies of MTE estimation capture the unobserved confounders by learning their latent representations <ref type="bibr">[Wang and Blei, 2019;</ref><ref type="bibr" target="#b10">Saini et al., 2019]</ref> through the interactions between instances and treatments. However, the interpretability of the learned representations is limited, which could be an unknown mixture of several latent confounders' representations. In the movie cast example, the movie genre can be an unobserved confounder, but establishing the connection between this unobserved confounder and a particular part of the learned representations is difficult. In fact, it can greatly facilitate the human understanding of the confounding bias by separating the distinct, informative factors of variations in the confounders' representations <ref type="bibr" target="#b6">[Locatello et al., 2019]</ref>. Motivated by the recent progress of disentangled representation learning <ref type="bibr" target="#b1">[Higgins et al., 2016;</ref><ref type="bibr" target="#b12">Tran et al., 2017]</ref> which learns factorized representations of the independent data generative factors, we investigate the MTE estimation problem from a new perspective by learning disentangled representations for confounders to improve the interpretability of causality learning.</p><p>However, learning disentangled representations of confounders for MTE estimation remains nascent due to the following challenges: (1) Different latent confounders are not only mixed together but also can exhibit hierarchical patterns (e.g., high-level latent confounders such as "the movie is an animation movie" and low-level latent confounders like "the animation movie is from Disney"), which further increases the complexity of disentangled representation learning. (2) When estimating the treatment effects, most existing works take different treatments separately <ref type="bibr" target="#b7">[Lopez et al., 2017]</ref> (i.e., constructing a prediction model for each treatment), which cannot capture the inherent dependencies between different treatments (e.g., two treatments could be similar if many instances are assigned with them simultaneously).</p><p>To address these challenges, we propose DIRECT -a novel framework of Disentangled multIple tReatment EffeCT estimation with the following desiderata: (1) To capture the hierarchical patterns of mixed confounders, we propose to disentangle the representations of latent confounders at two different levels. We first assume that treatments can be grouped into different clusters, as observed in many real-world scenarios <ref type="bibr">[Schnabel, 2016]</ref>. Then by separately inferring con-founders from the interactions between instances and each cluster of treatments (e.g., comedy actors or action actors), the learned confounder representations will become disentangled at the macro-level. Meanwhile, at the micro-level, we force different dimensions of the learned confounder representations to capture isolated factors with a carefully designed variational autoencoder (VAE) framework. (2) To tackle the issue that different treatments are often processed separately, we jointly consider multiple treatments simultaneously by leveraging their inherent dependencies. Specifically, we learn a trainable function to obtain the representation for each treatment based on treatment assignments. One appealing byproduct is that the framework can be generalized to new treatments that are not in the training data. Our main contributions include: 1) Problem: We formulate a new problem of disentangled multiple treatment effect estimation; 2) Framework: We propose a novel framework DIRECT to address this problem by learning disentangled confounder representations at two granularity levels; 3) Experiments: We conduct extensive experiments to show the superiority of DI-RECT w.r.t. MTE estimation and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>We use {A, Y } to denote the observational data, where A = {a i } n i=1 denotes the treatment assignment, and a i = {a i,j } m j=1 refers to the assignment of m different treatments on the i-th instance. Without loss of generality, we focus on treatments with binary values, i.e., a i,j ∈ {0, 1}. The observed outcome is denoted by Y = {y i } n i=1 , and y i ∈ R. We build our framework upon the potential outcome framework <ref type="bibr" target="#b13">[Vemuri, 2015]</ref>. We represent the potential outcome in the multiple treatment setting by Y a = {y i (a)} n i=1 , where y i (a) is the value of the outcome that would be observed if the i-th instance receives the treatment assignment a ∈ {0, 1} m . Then the ITE for the i-th instance over a is defined as τ i,a = y i (a) -y i (0), where y i (0) refers to the potential outcome when no treatment is assigned to the i-th instance. Definition 1. (Disentangled multiple treatment effect estimation) Given observational data {A, Y }, our goal is to: (1) learn disentangled representations for the latent confounders.</p><p>(2) estimate the ITE τ i,a for each instance i under any treatment assignment a.</p><p>In our work, we relax the strong ignorability assumption [Rosenbaum and <ref type="bibr">Rubin, 1983]</ref> (i.e., no unobserved confounders) by assuming that there may exist confounders Z = {z 1 , ..., z n } which (might be unobserved) can causally influence A and Y . Conditioning on the confounders, the treatment assignment is randomized, i.e., Y i (a) ⊥ ⊥ A i |Z i 1 for any treatment assignment a ∈ {0, 1} m . Following <ref type="bibr">[Wang and Blei, 2019]</ref>, we also assume that for each instance i, the assignment of different treatments is indepen-1 We use non-italicized capital letters to denote random variables, and italicized letters to denote specific realization. Among the italicized letters, non-bold letters denote scalars, bold lowercase letters denote vectors, and bold uppercase letters denote matrices or sets. For example, Zi is a randomly chosen vector of confounders, Z is the set which contains confounders of all instances, and zi denotes the values of confounders for instance i.</p><p>dent with each other conditioned on the confounders, i.e., A i,1 ⊥ ⊥ ... ⊥ ⊥ A i,m |Z i . Other assumptions in this work include the Positivity, Consistency, and SUTVA assumptions <ref type="bibr" target="#b10">[Rubin, 2005]</ref>, which are widely-adopted in causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Framework</head><p>The causal graph of the studied problem is shown in Fig. <ref type="figure">1</ref>, where Z i denotes the confounders which influence the outcome and the assignment of at least one treatment for each instance i. Our framework DIRECT learns disentangled representations of confounders Z with a carefully designed variational autoencoder (VAE) architecture. At the macro-level, we learn the embedded cluster structure of different treatments to better learn and interpret the confounder representations. At the micro-level, we force each dimension of the learned representation to capture an isolated factor. Furthermore, by leveraging the dependencies among treatments, we learn a parameterized function to obtain the representation of each treatment. In this way, the model can be generalized to estimate the effects of treatment assignment including new treatments without retraining from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Description</head><p>Our framework includes three sets of latent variables: Z, T , and C, where T = {t j } m j=1 is the representation of treatments, C = {c j } m j=1 is the cluster assignment of the treatments, each c j is an one-hot vector and c j,k = 1 denotes that treatment j belongs to cluster k. As shown in Fig. <ref type="figure">1</ref>, the distribution p(A, C, T, Z) can be factorized as:</p><formula xml:id="formula_0">p(Θ) = i,j p(A i,j |Z i , T j , C j )p(C j )p(T j |C j )p(Z i ),<label>(1)</label></formula><p>where Θ = {A, C, T, Z}. Assume that the treatments can be divided into K clusters, we then divide the treatment assignments into K groups corresponding to the treatment clusters. The confounders are learned separately for the treatment assignments in each group for a macro-level disentanglement. The confounder representation learned from the assignment of treatments in cluster k is denoted by</p><formula xml:id="formula_1">Z (k) = {z (k) 1 , z (k) 2 , ..., z (k) n }. Each Z (k) i</formula><p>is assumed to follow an isotropic unit Gaussian prior: Z (k) i ∼ N (0, I). Thus the distribution in Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be further factorized as:</p><formula xml:id="formula_2">p(Θ) = i,j p(A i,j |Z i , T j , C j )p(C j )p(T j |C j ) K k=1 p(Z (k) i ).</formula><p>(2) An illustration of the proposed framework DIRECT is shown in Fig. <ref type="figure" target="#fig_0">2</ref>, which follows a classical VAE architecture with the inference network and the generation network. The inference network infers the variational distributions of treatments and confounders based on the treatment assignment. The generation network reconstructs the input (i.e., treatment assignment), and predicts the potential outcome of each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference Network</head><p>Since the true posterior distribution of the latent variables q(Z, T, C|A) is intractable, we use an inference network to</p><formula xml:id="formula_3">Z " A ",% A ",&amp; …… Y " A A ",(</formula><p>),*-1 approximate it based on the mean-field approximation. The approximate posterior q(Z, T, C|A) can be factorized as:</p><formula xml:id="formula_4">q(Z, T, C|A)= m j=1 q(T j |a * ,j )q(C j |T j ) n i=1 K k=1 q(Z (k) i |a i, * ).</formula><p>(3) We learn the representations of treatments and confounders through the variational inference mentioned above, where * represents all the indices, e.g., a i, * refers to a i,1 , ..., a i,m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Treatment Representation Learning</head><p>The proposed framework explicitly learns the treatments' representations from the observed treatment assignments. Specifically, the inference network specifies the form of the variational distributions of T j to be Gaussian posteriors: q(T j |a * ,j ) = N (µ T (a * ,j ), diag(σ 2 T (a * ,j ))). In the inference network, the mean and variance of the posterior are inferred by two separate neural network modules µ T (•) and σ T (•). We assume that the representations of treatments in the observational data have an inherent cluster structure composed with K components, where K is a hyperparameter. A clustering module is introduced in the inference network to approximate the cluster distribution q(C j |t j ) = M ult(f c (t j )), where f c (•) is a function of the clustering module. The output of f c (•) is a K-dimensional vector, where each element inside corresponds to the probability that the treatment belongs to each cluster. And the multinoulli distribution is implemented by a softmax layer. To enable clustering, the inference network specifies the prior of T j to be N (µ cj , diag(σ 2 cj )), where µ cj and σ cj are parameters to be learned, referring to the mean and variance of the distribution of treatments in the cluster containing the j-th treatment. As the latent variable C j is discrete, the reparameterization trick based sampling is not differentiable for backpropagation, thus we apply Gumbel-Softmax sampling <ref type="bibr" target="#b3">[Jang et al., 2016]</ref> to approximate samples from the categorical distribution.</p><p>The treatment representation learning module enables the model to handle unseen treatments. For a new treatment m + 1, we can obtain its representation t m+1 and predict its cluster based on its treatment assignments and the trained model. Then, the potential outcome of those treatment assignments which involve the new treatment can be predicted with t m+1 and the learned confounder representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangled Confounder Representation Learning</head><p>Inspired by <ref type="bibr" target="#b9">[Ma et al., 2019]</ref>, we learn disentangled representations of the confounders at two different levels. At the macro-level, the treatment assignment is divided into K groups according to the sampled C. We learn the representation Z (k) of confounders from each group k separately, and the final representation Z is the concatenation of Z (1) , ..., Z (K) . Specifically, the learned Z k is expected to correspond to the treatments in cluster k. For each Z k , we infer the posteriors distributions as: q(Z (k)</p><formula xml:id="formula_5">i |a i, * ) = q(Z (k) i |a (k) i, * ) = N (µ I (a (k) i, * ), diag(σ 2 I (a (k) i, * )))</formula><p>, where a</p><p>i, * is the i-th instance's treatment assignment which only contains the treatments in cluster k. µ I (•) and σ I (•) are two neural network modules to infer the mean and variance of the distribution of q(Z (k) i |a (k) i, * ), respectively. At the micro-level, to achieve disentanglement among dimensions of learned representations, we specify a weight β 1 for the Kullback-Leibler (KL) divergence between the isotropic unit Gaussian prior and the learned distribution of each Z (k) to encourage the dimensions to reflect isolated latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation Network</head><p>In the generation model, we reconstruct the treatment assignment with a neural network module f a : p(A i,j |z i , t j , c j ) = Ber(sigmoid(f a (c j , z i , t j ))). We use a sigmoid layer to map the output of network f a (•) into (0, 1) as the probability of taking the treatment a i,j . In order to better capture the latent confounders, we also use the observed outcomes as a supervision signal. Specifically, we use a neural network module f y (Z i , A i , T) to predict the potential outcome y i (a) for any treatment assignment a. We assume the prediction Ŷi (a) follows the Gaussian distribution N (y i (a), σ 2 e ), where σ 2 e is the variance of the prediction error. We use the observed outcome y i as target and minimize the outcome prediction loss:</p><formula xml:id="formula_7">L y = - n i=1 log p( Ŷi = y i |z i , a i , T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>Following the classical VAE schema, the evidence lower bound (ELBO) L ELBO can be derived as (the subscript q denotes q(Z, T, C|A) by default, and we drop the instance index i and treatment index j for notation simplicity):</p><formula xml:id="formula_8">E q [log p(A|Z, T, C)]-KL(q(Z, T, C|A)||p(Z, T, C)) = E q [log p(A|Z, T, C)] -E q(T|A) KL(q(C|T)||p(C)) -E q(C|T) KL(q(T|A)||p(T|C)) - K k=1 E q(T|A)q(C|T) KL(q(Z (k) |A)||p(Z (k) )).</formula><p>(4)</p><p>The ELBO consists of the reconstruction term and the KL term, which contains three terms: 1) the clustering prior term, where we use the uniform prior; 2) the treatment prior term, which drives the treatment clustering as described in the section of treatment representation learning; 3) the confounder prior term, which leads to disentanglement among dimensions by utilizing the isotropic nature of the prior. It is impractical to calculate the expectations over the variational distribution analytically, thus these terms are instead estimated by Monte Carlo samples from q(Z, T, C|A). By putting all the aforementioned components together, we obtain the loss function of the proposed framework:</p><formula xml:id="formula_9">L = -E q [log p(A|Z, T, C)] + E q(T|A) KL(q(C|T)||p(C))</formula><p>+ E q(C|T) KL(q(T|A)||p(T|C)) + λL y + β K k=1 E q(T|A)q(C|T) KL(q(Z (k) |A)||p(Z (k) )).</p><p>(5)</p><p>Hyperparameters β and λ are used to control the effect of different parts of the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed method on one synthetic dataset and two semi-sythetic datasets from real-world scenarios.</p><p>The detailed statistics of these datasets are shown in Table <ref type="table" target="#tab_0">1</ref>, including the number of instances, treatments, treatment clusters, and the average ratio of treatments assigned to instances. Synthetic Dataset. We first conduct experiments on a synthetic dataset. This dataset is generated as follows:</p><formula xml:id="formula_10">Z (k) i ∼ N (0, I), C j ∼ M ult(π), T j |c j ∼ K k=1 N (µ k , diag(σ 2 k )) c j,k , µ k ∼ N (0, I), σ k ∼ rand(0, I), A i,j ∼ Ber(sigmoid(z (cj ) i t j )),<label>(6</label></formula><p>) where i = 1, ..., n and j = 1, ..., m, π is a K-dimensional vector, corresponding to the probability that the treatment belongs to each cluster. We set d I (the dimension of each z</p><formula xml:id="formula_11">(k) i )</formula><p>and d T (the dimension of T j ) both as 20. The potential outcome of instance i under a treatment assignment a is simulated as y i (a) = a T T W 1 z i , where z i is the concatenation of z (k) i (k = 1, ..., K), and T = [t 1 , ..., t m ] T . W 1 is a matrix of parameters with dimensions d T × Kd I .</p><p>Real-world Datasets. It is notoriously hard to obtain the ground truth treatment effect as we only observe one of the potential outcomes for each instance. Thus, we create two semi-synthetic datasets (Amazon-3C and Amazon-6C) based on the real-world Amazon review data<ref type="foot" target="#foot_1">foot_1</ref> . In each dataset, we select three/six categories of items. In each category, we select the top-1000 products with the highest number of reviews as instances. We aim to investigate the effect of the keywords in reviews on the future sales of each product: (1) Treatment:</p><p>We first generate a dictionary of keywords by performing unsupervised feature selection <ref type="bibr" target="#b5">[Li et al., 2017]</ref> on the bag-ofwords features of reviews, then randomly select three words from the dictionary as a treatment, (e.g., if an item receives reviews containing all the three words in a treatment, then we say the treatment is assigned to the item). ( <ref type="formula">2</ref>) Potential outcome: The future amount of sales of each product is the outcome and is simulated in the same way as that for the synthetic data. (3) Condounders: The confounders are the latent attributes of the products, which affect what words would appear in the reviews, as well as the product sales. We simulate the confounders by training a neural network to fit the treatment assignment, and take the output of a middle layer as confounders, then use it to simulate the potential outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>To evaluate our proposed framework in MTE estimation, we compare it with several state-of-the-art baselines in the following three categories: (1) traditional regression methods: least square regression (OLS/LR) and random forest (RF). These methods can take treatment assignment as features and predict the outcomes; (2) representation learning based ITE esitmation methods for single treatment: Causal Effect Variational Autoencoder (CEVAE) <ref type="bibr" target="#b8">[Louizos et al., 2017]</ref>, Treatment-Agnostic Representation Network (TARNet) <ref type="bibr">[Shalit et al., 2017]</ref>, and counterfactual regression with Wasserstein metric (CFR) <ref type="bibr">[Shalit et al., 2017]</ref>. (3) Multiple treatment effect estimation methods: Bayesian Additive Regression Trees (BART) <ref type="bibr" target="#b2">[Hill, 2011]</ref> -though widely used in single-cause ITE estimation, can be naturally extended to multi-treatment setting by extending the input vectors in the Bayesian regression tree. Multi-cause deconfounder <ref type="bibr">[Wang and Blei, 2019]</ref> utilizes the dependencies among the assigned causes to capture the confounders. We apply two different forms (linear and quadratic) in the potential outcome prediction, denoted as Deconf-l and Deconf-q respectively. As an ablation study of our proposed method, we disable the disentanglement by setting K = 1 and β = 1.0, maintaining the same dimension of representation. This variant of our method is denoted by DIRECT-ND. 4.91 ± 0.36 2.26 ± 0.08 5.89 ± 0.34 2.85 ± 0.16 6.37 ± 0.13 3.38 ± 0.12 DIRECT (ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>3.42 ± 0.12 1.33 ± 0.08 4.57 ± 0.31 2.04 ± 0.14 5.04 ± 0.09 2.37 ± 0.08 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic</head><p>3.21 ± 0.07 1.26 ± 0.05 3.04 ± 0.08 1.22 ± 0.05 Amazon-3C 4.62 ± 0.48 2.30 ± 0.13 4.59 ± 0.55 2.24 ± 0.41 Amazon-6C 5.41 ± 0.09 2.52 ± 0.12 5.23 ± 0.12 2.49 ± 0.13 perparameters are set as β = 20, λ = 0.4. By default, we set K as the same number of true treatment clusters, then we alter K to test the performance and disentanglement in Section 4.4. All the results are averaged over ten executions.</p><p>Metrics. Two evaluation metrics are widely used in treatment effect estimation -Rooted Precision in Estimation of Heterogeneous Effect (PEHE) <ref type="bibr" target="#b2">[Hill, 2011]</ref> and Mean Absolute Error on ATE ( AT E ) <ref type="bibr" target="#b14">[Willmott and Matsuura, 2005]</ref>. Following <ref type="bibr" target="#b10">[Saini et al., 2019]</ref>, we extend them into the multi-treatment setting. The evaluation is performed on a predefined set of R different treatment assignments, A = {a 1 , ..., a R }, where 0 &lt; R &lt; 2 m . For each a r ∈ A, we have: P EHE r = n i=1 (τ i,a r -τi,a r ) 2 /n, where τi,a r = ŷi (a r ) -ŷi (0) is the predicted treatment effect over a r . The average over the R treatment assignments is:</p><formula xml:id="formula_12">P EHE = 1 R R r=1 P EHE r .</formula><p>Similarly, another metric AT E can also be extended to the multiple treatment setting:</p><formula xml:id="formula_13">AT E = 1 R R r=1 | 1 n n i=1 τ i,a r -1 n n i=1 τi,a r |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MTE Estimation</head><p>To evaluate the proposed method in MTE estimation, we compare it with the aforementioned baselines. CEVAE, TAR-NET and CFR are designed for single cause ITE estimation, following <ref type="bibr" target="#b14">[Yoon et al., 2018]</ref>, we apply them into multi-cause setting: we randomly select three treatments, choose the assignment {0, 0, 0} as the control group, and other seven treatment assignments as treated group. In this way, we create seven separate single cause ITE estimation tasks, and calculate the averaged PEHE and AT E over the seven tasks.We show the results of all methods when we randomly select three treatments in Table <ref type="table" target="#tab_2">2</ref>. We observe that DIRECT consistently outperforms the baselines. The regression methods OLS/LR and RF cannot capture the confounders and thus suffer from the confounding bias. CEVAE, TARNET and CFR model each treatment separately, thus cannot capture the dependency among treatments. BART is limited in the strong ignorability assumption. Deconf-l and Deconf-q may capture latent confounders by utilizing the assignment of multiple treamtents, but they do not utilize the observed outcome, and also lack disentanglement, which is also the limitation of DIRECT-ND. We attribute the superiority of DIRECT to two key factors: (1) our framework leverages the multiple treatment assignment and observed outcome to capture more latent confounders;</p><p>(2) the disentangled representation often leads to higher performance, which is in line with the conclusion in <ref type="bibr" target="#b9">[Ma et al., 2019]</ref>.</p><p>Generalization for New Treatments. We assess how the proposed framework can be generalized to predict the effects of treatments that are unseen in the training data. Since none of the baseline methods can handle unseen treatments, in each dataset, we randomly hold out 20% treatments and compare the performance of our framework in predicting the causal effect of the treatment assignment over the held out treatments with/without their assignment data. The results in Table <ref type="table" target="#tab_3">3</ref> show that our model can achieve comparable performance for new treatments without retraining, which benefits from the trainable network for treatment representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Disentanglement &amp; Interpretability</head><p>We visualize the treatment representation and color them w.r.t. their true/predicted clusters in Fig. <ref type="figure">3</ref>. We observe that the predicted clusters are very close to the ground-truth.</p><p>As the treatments' representation is aligned with the confounders' representation, it indicates good macro-level disentanglement of the representations of confounders. Due to space limit, we only show the results in the synthetic dataset, but the observations are similar in other datasets.</p><p>To investigate the relation between the disentanglement of confounders' representations and the MTE estimation performance, we vary the hyperparameter K and β to control the level of disentanglement, and Fig. <ref type="figure" target="#fig_2">4</ref> shows how the estimation performance varies w.r.t. different levels of disentanglement of the confounders' representations. Here the level of disentanglement of representation with dimension d is calculated by 1 -2 d(d-1) i,j |corr(i, j)|, where corr(i, j) is the correlation between dimension i and j. Due to space limit, we only show the results on datasets Synthetic and Amazon-3C, but similar observations can be found on the other dataset. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, treatment clustering (K &gt; 1) benefits the disentanglement, and higher levels of disentanglement often leads to better MTE estimation performance.</p><p>To further show the interpretability of the learned disentangled representations, we investigate their semantics in microlevel. On Amazon-3C, after training, we use a similar way to evaluate the micro-level disentanglement as <ref type="bibr" target="#b9">[Ma et al., 2019;</ref><ref type="bibr">Wang et al., 2020]</ref>. We modify one dimension of the learned confounder representations by multiplying it with a temperature factor τ = 10, while keep all other dimensions fixed. Then we list the treatments with the biggest changes w.r.t. the predicted treatment assignment after modification in Table 4. Generally, we have two observations: 1) the treatments that are significantly affected can match the cluster of the modified dimension. This indicates a high-level interpretation, e.g., when we modify a dimension in z (k) , most of the top influenced treatments are about the musical instruments. This may imply that the cluster k corresponds to latent attributes related to musical instrument products; 2) most of the top influenced treatments contain a common word or semantically related words, which indicates that the model can capture the fine-grained latent factors by disentangled representation, e.g., when we modify a dimension, the top influenced treatments share the word "tune", which provides human-understandable semantics for the modified dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Multiple Treatment Effect Estimation. Traditional methods for the single cause can be extended to the multi-cause setting <ref type="bibr" target="#b7">[Lopez et al., 2017;</ref><ref type="bibr" target="#b14">Zanutto et al., 2005;</ref><ref type="bibr" target="#b4">Lechner, 2001]</ref>, and recent work <ref type="bibr" target="#b11">[Sharma et al., 2020]</ref> applies the neural network. However, these works are still based on the strong ignorability assumption. To mitigate this problem, a relaxed assumption called single strong ignorability is proposed in <ref type="bibr">[Wang and Blei, 2019]</ref> for the multi-cause scenarios, which assumes that there do not exist unobserved singlecause confounders that causally affect the outcome and only one of the treatments. Despite its success in applications such as recommender systems <ref type="bibr" target="#b13">[Wang et al., 2018]</ref> and medication analysis <ref type="bibr" target="#b15">[Zhang et al., 2019]</ref>, their captured latent confounders might be highly entangled and hard to interpret. Disentangled Representation Learning. Disentangled representation learning has attracted significant attention recently. <ref type="bibr" target="#b9">[Ma et al., 2019]</ref> introduces a macro-micro disentangled representation learning framework for recommender systems, which achieves macro disentanglement by inferring high-level user intentions and micro disentanglement to force each dimension capture an isolated factor. We use a similar way of hierarchical disentanglement but focus on the causal inference domain. In causal inference, a line of work <ref type="bibr" target="#b0">[Hassanpour and Greiner, 2019;</ref><ref type="bibr" target="#b15">Zhang et al., 2020]</ref> identifies disentangled representations to separate the latent factors which influence the treatment assignment, the outcome, or both of them. Our work differs from them as the disentanglement in our work focuses on the confounders mixed in hierarchical patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we study a novel problem of disentangled multiple treatment effect estimation, and analyze its importance and challenges. We develop a novel framework DIRECT to learn disentangled representations of latent confounders for MTE estimation. Specifically, we improve the interpretability of the learned representations of confounders at both macro level and micro level. Then, we learn a trainable function to obtain the representation for each treatment by leveraging their inherent dependencies, which can be further generalized to unseen treatments. We conduct extensive experiments on different datasets, and the experimental results validate the effectiveness and interpretability of our proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigureFigure 2 :</head><label>2</label><figDesc>Figure 1: Causal graph of the studied problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Treatment clusters in the synthetic dataset.</figDesc><graphic coords="5,327.44,224.75,98.70,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MTE estimation performance w.r.t. different levels of disentanglement of the representations of confounders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detailed statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Synthetic Amazon-3C Amazon-6C</cell></row><row><cell># of instances</cell><cell>2, 500</cell><cell>3, 000</cell><cell>6, 000</cell></row><row><cell># of treatments</cell><cell>500</cell><cell>104</cell><cell>325</cell></row><row><cell># of clusters</cell><cell>4</cell><cell>3</cell><cell>6</cell></row><row><cell cols="2">Avg ratio of treated 42.3%</cell><cell>21.4%</cell><cell>18.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Each dataset is randomly split into 60%/20%/20% training/validation/test set. Unless otherwise specified, hy-</figDesc><table><row><cell>Method</cell><cell cols="2">Synthetic P EHE</cell><cell>AT E</cell><cell cols="2">Amazon-3C P EHE</cell><cell>AT E</cell><cell>Amazon-6C P EHE</cell><cell>AT E</cell></row><row><cell>OLS/LR</cell><cell>10.07 ± 1.28</cell><cell cols="2">5.31 ± 0.88</cell><cell>11.15 ± 1.93</cell><cell cols="2">5.21 ± 1.13</cell><cell>11.27 ± 1.48</cell><cell>6.51 ± 0.52</cell></row><row><cell>RF</cell><cell>10.26 ± 1.23</cell><cell cols="2">5.22 ± 0.64</cell><cell>10.25 ± 1.81</cell><cell cols="2">5.17 ± 0.75</cell><cell>10.53 ± 1.62</cell><cell>5.82 ± 0.41</cell></row><row><cell>CEVAE</cell><cell>16.58 ± 1.99</cell><cell cols="2">7.38 ± 0.51</cell><cell>19.11 ± 1.21</cell><cell cols="2">9.93 ± 0.69</cell><cell>17.52 ± 0.92</cell><cell>8.08 ± 0.26</cell></row><row><cell>TARNET</cell><cell>12.07 ± 1.30</cell><cell cols="2">5.77 ± 0.84</cell><cell>9.27 ± 1.26</cell><cell cols="2">5.12 ± 1.24</cell><cell>9.51 ± 0.34</cell><cell>4.31 ± 0.20</cell></row><row><cell>CFR</cell><cell>12.78 ± 1.49</cell><cell cols="2">6.03 ± 0.94</cell><cell>8.37 ± 0.43</cell><cell cols="2">3.92 ± 1.04</cell><cell>9.32 ± 0.92</cell><cell>4.29 ± 0.19</cell></row><row><cell>BART</cell><cell>10.92 ± 2.20</cell><cell cols="2">5.30 ± 1.05</cell><cell>9.91 ± 1.77</cell><cell cols="2">6.03 ± 1.49</cell><cell>11.02 ± 2.15</cell><cell>5.11 ± 1.41</cell></row><row><cell>Deconf-l</cell><cell>8.26 ± 1.37</cell><cell cols="2">3.16 ± 0.24</cell><cell>7.34 ± 0.48</cell><cell cols="2">3.86 ± 0.41</cell><cell>8.16 ± 0.65</cell><cell>4.53 ± 0.53</cell></row><row><cell>Deconf-q</cell><cell>8.54 ± 1.28</cell><cell cols="2">3.42 ± 0.33</cell><cell>7.18 ± 0.52</cell><cell cols="2">3.21 ± 0.30</cell><cell>8.68 ± 0.72</cell><cell>4.25 ± 0.28</cell></row><row><cell>DIRECT-ND (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of multiple treatment effect estimation for different methods.</figDesc><table><row><cell>Treatment</cell><cell>Hold out 20% P EHE AT E</cell><cell>P EHE</cell><cell>Together</cell><cell>AT E</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model generalization for new treatments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Examples of the top-5 influenced treatments after modifying a dimension of confounder representation.</figDesc><table><row><cell>Manipulated dimension</cell><cell></cell><cell cols="2">Top-5 treatments</cell><cell></cell><cell></cell></row><row><cell></cell><cell>tune</cell><cell>tune</cell><cell>loud</cell><cell>tune</cell><cell>finger</cell></row><row><cell>In Cluster 1</cell><cell cols="2">musician tuner</cell><cell>tune</cell><cell>bass</cell><cell>player</cell></row><row><cell></cell><cell cols="2">capo recording</cell><cell>bass</cell><cell>price</cell><cell>tune</cell></row><row><cell></cell><cell>size</cell><cell>long</cell><cell>longer</cell><cell>sizing</cell><cell>size</cell></row><row><cell>In Cluster 2</cell><cell>sizing</cell><cell>size</cell><cell>little</cell><cell>felt</cell><cell>longer</cell></row><row><cell></cell><cell>width</cell><cell>old</cell><cell>classic</cell><cell>price</cell><cell>feel</cell></row><row><cell></cell><cell>battery</cell><cell>long</cell><cell cols="2">headphone light</cell><cell>access</cell></row><row><cell>In Cluster 3</cell><cell cols="2">charge battery</cell><cell cols="3">battery charger connect</cell></row><row><cell></cell><cell>phone</cell><cell>old</cell><cell>quick</cell><cell cols="2">cost battery</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://jmcauley.ucsd.edu/data/amazon/index 2014.html Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This material is, in part, supported by the <rs type="funder">National Science Foundation (NSF)</rs> under grants #<rs type="grantNumber">2006844</rs> and #<rs type="grantNumber">1938167</rs>.   </p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Bcn5vHP">
					<idno type="grant-number">2006844</idno>
				</org>
				<org type="funding" xml:id="_Hpcd9jg">
					<idno type="grant-number">1938167</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for counterfactual regression</title>
		<author>
			<persName><forename type="first">Greiner ; Russell</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><surname>Higgins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><surname>Jang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identification and estimation of causal effects of multiple treatments under the conditional independence assumption</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><surname>Lechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Econometric evaluation of labour market policies</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature selection: A data perspective</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of causal effects with multiple treatments: a review and new ideas</title>
		<author>
			<persName><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><surname>Louizos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rosenbaum and Rubin, 1983] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1983">2019. 2019. 1983</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
	<note>Learning disentangled representations for recommendation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple treatment effect estimation using deep generative model with task embedding</title>
		<author>
			<persName><forename type="first">Rubin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Saini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2019. 2019. 2016. 2016. 2017</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ranjitha Prasad, Arnab Chatterjee, Lovekesh Vig, and Gautam Shroff. Multimbnn: Matched and balanced causal inference with neural networks</title>
		<author>
			<persName><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13446</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for poseinvariant face recognition</title>
		<author>
			<persName><forename type="first">Tran</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Causal inference for statistics, social, and biomedical sciences: An introduction by guido w. imbens and donald b. rubin</title>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blei ;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Blei ; Yixin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06581</idno>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Hongye</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">An</forename><surname>Jin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiangnan</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tat-Seng</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Chua</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2019. 2019. 2018. 2018. 2020</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Disentangled graph collaborative filtering</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advantages of the mean absolute error (mae) over the root mean square error (rmse) in assessing average model performance</title>
		<author>
			<persName><forename type="first">Matsuura</forename><forename type="middle">;</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2018. 2018. 2005</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Climate Research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The medical deconfounder: Assessing treatment effects with electronic health records</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10652</idno>
	</analytic>
	<monogr>
		<title level="m">Treatment effect estimation with disentangled latent factors</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Machine Learning for Healthcare Conference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
