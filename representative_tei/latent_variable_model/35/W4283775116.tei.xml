<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A small sample correction for factor score regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jasper</forename><surname>Bogaert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Analysis Ghent University</orgName>
								<orgName type="department" key="dep2">Department of Data Analysis</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Henri Dunantlaan 2</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><forename type="middle">Wei</forename><surname>Loh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Analysis Ghent University</orgName>
								<orgName type="department" key="dep2">Department of Data Analysis</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Henri Dunantlaan 2</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yves</forename><surname>Rosseel</surname></persName>
							<email>yves.rosseel@ugent.be.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Analysis Ghent University</orgName>
								<orgName type="department" key="dep2">Department of Data Analysis</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Henri Dunantlaan 2</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Rosseel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Analysis Ghent University</orgName>
								<orgName type="department" key="dep2">Department of Data Analysis</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Henri Dunantlaan 2</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A small sample correction for factor score regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1177/00131644221105505</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Factor score regression (FSR) is widely used as a convenient alternative to traditional structural equation modeling (SEM) for assessing structural relations between latent variables.</p><p>But when latent variables are simply replaced by factor scores, biases in the structural parameter estimates often have to be corrected, due to the measurement error in the factor scores. The method of Croon (MOC) is a well known bias correcting technique. However, its standard implementation can render poor quality estimates in small samples (e.g., less than 100). This paper aims to develop a small sample correction (SSC) that integrates two different modifications to the standard MOC. We conducted a simulation study to compare the empirical performance of (i) standard SEM, (ii) the standard MOC, (iii) naive FSR, and (iv) the MOC with the proposed SSC. In addition, we assessed the robustness of the performance of the SSC in various models with a different number of predictors and indicators. The results showed that the MOC with the proposed SSC yielded smaller mean squared errors than SEM and the standard MOC in small samples and performed similarly to naive FSR. However, naive FSR yielded more biased estimates than the proposed MOC with SSC, by failing to account for measurement error in the factor scores.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A small sample correction for factor score regression</head><p>In the educational, behavioral, and social sciences, researchers often explore relationships between latent variables such as motivation and intelligence. The most obvious and widely adopted tool of choice to analyze these relationships is structural equation modeling (SEM). SEM is a statistical modeling procedure that estimates the measurement model and the structural relations simultaneously, and is (in combination with maximum likelihood estimation) often regarded to be the gold standard <ref type="bibr" target="#b3">(Bollen, 1989)</ref>. However, there are several disadvantages to (traditional) SEM. For example, a large sample size is a desideratum and a misspecification of the model might affect the estimation of all parameters <ref type="bibr" target="#b2">(Bentler &amp; Yuan, 1999;</ref><ref type="bibr" target="#b4">Boomsma, 1985;</ref><ref type="bibr" target="#b19">Nevitt &amp; Hancock, 2004)</ref>.</p><p>When substantive interest is primarily in assessing the structural relations between latent variables, a straightforward alternative to SEM is factor score regression (FSR) <ref type="bibr" target="#b25">(Skrondal &amp; Laake, 2001)</ref>. In FSR, the parameters are estimated in two steps. In a first step, confirmatory factor analysis is used to estimate the parameters of the measurement model, and factor scores are predicted for each latent variable. In a second step, the factor scores are used in place of the latent variables to estimate the (linear) relationships among the latent variables. When the structural model is recursive (i.e., no feedback loops), the analysis in the second step boils down to a series of linear regressions <ref type="bibr" target="#b13">(Devlieger et al., 2016)</ref>. When the structural model is non-recursive, path analysis can be used <ref type="bibr" target="#b12">(Devlieger &amp; Rosseel, 2017)</ref>. In this paper, we use the term factor score regression for both settings. Factor scores can be calculated in various ways, leading to different sets of observed factor scores. When using Regression <ref type="bibr" target="#b28">(Thomson, 1934;</ref><ref type="bibr" target="#b29">Thurstone, 1935)</ref> or Bartlett factor scores <ref type="bibr" target="#b1">(Bartlett, 1937)</ref>, these are by construction always a linear combination of the observed indicators measuring the latent variable. Therefore, they still contain measurement error and naively using them in place of latent variables will result in biased regression coefficients <ref type="bibr" target="#b3">(Bollen, 1989;</ref><ref type="bibr" target="#b18">Lastovicka &amp; Thamodaran, 1991;</ref><ref type="bibr" target="#b24">Shevlin et al., 1997)</ref>. We describe other issues that can arise when using this method in the Discussion section. In this paper, we refer to the method described abovewithout any form of bias-correction -as factor score regression (FSR).</p><p>It is nevertheless possible to remove the bias in the estimated regression coefficients.</p><p>One possible approach is based on the work of <ref type="bibr" target="#b10">Croon (2002)</ref>, and termed the method of Croon (MOC) by <ref type="bibr" target="#b13">Devlieger et al. (2016)</ref>. The MOC proceeds by removing the measurement error from the observed variance-covariance matrix of the factor scores in order to obtain a consistent estimator of the variance-covariance matrix of the latent variables. This (co)variance matrix can then be used to obtain asymptotically unbiased estimators of the regression coefficients. <ref type="bibr" target="#b13">Devlieger et al. (2016)</ref> and <ref type="bibr" target="#b27">Takane and Hwang (2018)</ref> show in their simulation studies that the MOC is a good performing alternative for SEM. As illustrated in <ref type="bibr" target="#b12">Devlieger and Rosseel (2017)</ref>, a major advantage of the MOC is its robustness against local misspecifications of the model.</p><p>While SEM and the MOC are useful methods, both methods can perform poorly in small samples (N &lt; 100). Some of the known problems that may occur if SEM is used with (very) small samples are: (a) convergence problems, (b) inadmissible solutions (e.g., Heywood cases), (c) biased estimates for variance components and (d) incorrect confidence intervals and fit statistics <ref type="bibr" target="#b2">(Bentler &amp; Yuan, 1999;</ref><ref type="bibr" target="#b4">Boomsma, 1985;</ref><ref type="bibr" target="#b19">Nevitt &amp; Hancock, 2004;</ref><ref type="bibr" target="#b23">Rosseel, 2020)</ref>.</p><p>The reason is that the (frequentist) estimation methods that are typically used in SEM (whether it be maximum likelihood estimation or generalized least squares) only work well if the sample size is sufficiently large. To tackle the problem of non-convergence and inadmissible solutions, De Jonckere and Rosseel (2022) suggest using bounded estimation. Their method effectively decreases the rate of non-converge in small samples, but the variability of the estimated regression coefficients in the structural model is still substantial. <ref type="bibr" target="#b26">Smid and Rosseel (2020)</ref> show that the MOC, being a two-step method, performs slightly better, but still exhibits a lot of variability when the sample size is very small (say, N &lt; 50). Nonetheless, as long as the models are correctly specified, the estimates of the regression coefficients obtained by SEM or the MOC remain unbiased, even in very small samples <ref type="bibr" target="#b26">(Smid &amp; Rosseel, 2020)</ref>. Indeed, it is well known that SEM and the MOC take into account the measurement error, and therefore give unbiased results. What is less known by the users of both methods, is the price to pay for these unbiased results. The bias-correction performed by SEM and the MOC leads to a large variability of the parameter estimates, in particular when the sample size becomes small. This is related to the concept of bias-variance trade-off, which implies that a decreasing bias will lead to an increasing variance and vice versa <ref type="bibr" target="#b16">(Hastie et al., 2006;</ref><ref type="bibr" target="#b17">James et al., 2013)</ref>. Therefore, in addition to bias, we also focus on the estimators' variability and mean squared error (MSE) in this paper. <ref type="bibr" target="#b9">Cox and Hinkley (1979)</ref> stress that an estimate with a small bias and a small variance might be preferable to one with no bias but a large variance.</p><p>Small sample corrections have already been presented in the general SEM framework (e.g., see <ref type="bibr">Ozenne et al. (2020)</ref>). However, this is not the case for the MOC. Hence, in this paper, we propose to modify the MOC with a small sample correction. Our modification is based on earlier work reported in the measurement error models literature <ref type="bibr" target="#b7">(Carroll et al., 2006;</ref><ref type="bibr" target="#b15">Fuller, 1987;</ref><ref type="bibr" target="#b30">Wall &amp; Amemiya, 2000)</ref>. In the context of linear regression with measurement error in the predictors, <ref type="bibr" target="#b14">Fuller (1980)</ref> proposes two modifications in order to improve small sample estimation. His aim is to reduce the variability by allowing some bias, resulting in a better MSE accordingly. The goal of this paper is to incorporate these two modifications into the MOC, and examine its performance.</p><p>The rest of this paper is structured as follows. First, we introduce a simple structural equation model and the notation. We then describe different estimation methods to analyze the relationships between latent variables: traditional SEM, FSR, and the MOC. Next, information concerning measurement error models and Fuller's modifications is given.</p><p>Thereafter, we show how to integrate these different adjustments into a single small-sample correction for the MOC. Subsequently, two different simulation studies are described and the results are presented. Lastly, we discuss the performance of the estimators, the robustness of their performance in various models and provide some concluding thoughts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A simple structural equation model</head><p>To facilitate the presentation of the correction techniques developed in this paper, we consider a structural equation model with one latent predictor (denoted by η x ) measured by p indicators, and one latent dependent variable (denoted by η y ) measured by q indicators. An example with p = q = 3 is given in Figure <ref type="figure" target="#fig_4">1</ref>. We use this simple model because it allows us to present many formulas in an easy to understand scalar form. For general settings, we refer the reader to Appendix A where all results are presented using matrix notation. The joint model consists of two models: a measurement model and a structural model. The measurement model can be written as</p><formula xml:id="formula_0">x = ν x + λ x η x + ϵ x and y = ν y + λ y η y + ϵ y ,<label>(1)</label></formula><p>where x and y are the p × 1 and q × 1 vectors capturing the observed indicators measuring η x and η y respectively; ν x and ν y are the vectors of intercepts, λ x and λ y the vectors containing the factor loadings, and ϵ x and ϵ y the vectors of the error variables. We assume that E(ϵ x ) = E(ϵ y ) = 0 and denote Var(ϵ x ) = Θ x and Var(ϵ y ) = Θ y .</p><p>The structural model is given by</p><formula xml:id="formula_1">η y = α + βη x + ζ, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where η x is the latent predictor and η y is the latent dependent variable; α is the intercept, β the regression coefficient, and ζ the residual error term. We assume that E(ζ) = 0,</p><formula xml:id="formula_3">Cov(η x , ζ) = 0, and Cov(ϵ x , ζ) = Cov(ϵ y , ζ) = 0. We denote Var(η x ) = Ψ x and Var(ζ) = Ψ y .</formula><p>Without loss of generality, we assume that the observed variables are centered and therefore, ν x = ν y = 0 and α = 0. The remaining free parameters to be estimated in this model are the free elements of λ x , λ y , Θ x , Θ y , as well as the scalar parameters Ψ x , Ψ y , and β. We collect these free parameters in the parameter vector θ. The model-implied variance-covariance matrix is written as Σ(θ) to stress that it is a function of the parameter vector θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum likelihood estimation in structural equation modeling</head><p>When all observed variables are continuous, a common estimator in structural equation modeling is maximum likelihood estimation (MLE). When using MLE, the parameter estimates are found by minimizing the discrepancy function</p><formula xml:id="formula_4">F M L = tr(SΣ( θ) -1 ) -log|Σ( θ)| + log|S| -k,<label>(3)</label></formula><p>with S the observed variance-covariance matrix of the observed indicators, Σ( θ) the model-implied variance-covariance matrix of the observed indicators, and k the number of observed variables. Note that all the parameters in the parameter vector θ are estimated simultaneously.</p><p>As mentioned before, SEM suffers from convergence issues in small samples.</p><p>Nonconvergence occurs when the applied optimization method (e.g., quasi-Newton optimization) fails to acquire a solution satisfying certain criteria <ref type="bibr" target="#b0">(Anderson &amp; Gerbing, 1984;</ref><ref type="bibr" target="#b4">Boomsma, 1985)</ref>. De Jonckere and <ref type="bibr" target="#b11">Rosseel (2022)</ref> show that bounded estimation leads to a major decrease in the nonconvergence rate, without negative effects on the point estimates for the unbounded parameters. Given the focus on small samples in this paper, bounded estimation (using standard bounds) is used for all estimators in this paper in order to avoid convergence problems. Importantly, applying standard bounds prevents the elements of Θ x and Θ y from being negative, thereby avoiding Heywood cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factor score regression</head><p>FSR is a two-step method. In the first step, confirmatory factor analysis (CFA) is used to estimate the parameters of the measurement model. As with SEM, convergence issues may also occur here. Therefore, we again use MLE with bounded estimation for the CFA. Because the two measurement models (for x and y) do not share any parameters (no cross-loadings or residual correlations), we can estimate both measurement models separately. Once the parameters of the measurement model are estimated, we can compute factor scores for both latent variables as follows:</p><formula xml:id="formula_5">f x = a x x and f y = a y y,<label>(4)</label></formula><p>where a x and a y are the factor score vectors. There are multiple ways to compute the factor score vector a, leading to different factor scores. The two most common predictors are the Regression <ref type="bibr" target="#b28">(Thomson, 1934;</ref><ref type="bibr" target="#b29">Thurstone, 1935)</ref> and <ref type="bibr">Bartlett predictor (Bartlett, 1937)</ref>. Using the Bartlett method, the factor score vectors a B x and a B y are defined as:</p><formula xml:id="formula_6">a B x = (λ T x Θ -1 x λ x )λ T x Θ -1 x T and a B y = (λ T y Θ -1 y λ y )λ T y Θ -1 y T .</formula><p>(5)</p><p>In the Regression method, the factor score vectors a R x and a R y are defined as:</p><formula xml:id="formula_7">a R x = Var(η x )λ T x Σ -1 x T and a R y = Var(η y )λ T y Σ -1 y T . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>In the second step, we use the factor scores to estimate the regression coefficient β of the structural model. The ordinary least-squares estimator of β can be found from the variance-covariance matrix of the factor scores as follows:</p><formula xml:id="formula_9">βfs = Var(f x ) -1 Cov(f x , f y ),<label>(7)</label></formula><p>where the 'f s' subscript indicates that the estimate is based on factor scores. This estimator is biased regardless of the sample size, except in the trivial setting where Cov(f x , f y ) = 0, or when there is no measurement error (ϵ x = ϵ y = 0) <ref type="bibr" target="#b15">(Fuller, 1987)</ref>. But see also <ref type="bibr" target="#b25">Skrondal and Laake (2001)</ref> where an unbiased estimate only under specific settings can be obtained using a bias-avoiding FSR method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The method of Croon</head><p>The source of the bias when using naive FSR is that the variance-covariance matrix of the factor scores differs from the variance-covariance matrix of the latent variables. Hence, the MOC <ref type="bibr" target="#b10">(Croon, 2002)</ref> performs a correction to the variance-covariance matrix of the factor scores so that the corrected variance-covariance matrix is consistent for that of the latent variables. For clarity, we call the elements of the initial variance-covariance matrix the (co)variances of the factor scores, in this case denoted by Var(f x ), Var(f y ), and Cov(f x , f x ). The elements of the corrected variance-covariance matrix are named the estimated (co)variances of the latent variables, denoted by Var(η x ) Croon , Var(η y ) Croon , and Cov(η x , η y ) Croon .</p><p>The variance of a latent variable based on the variance of the factor scores can be obtained by the formula:</p><formula xml:id="formula_10">Var(η x ) Croon = scale -2 x Var(f x ) -offset x = (a T x λ x ) -2 Var(f x ) -λ T x Θ x λ x . (8)</formula><p>Furthermore, the covariance of a latent variable based on the covariance of the factor scores can be obtained by:</p><formula xml:id="formula_11">Cov(η x , η y ) Croon = (scale x × scale y ) -1 Cov(f x , f y ) = (a T x λ x a T y λ y ) -1 Cov(f x , f y ). (9)</formula><p>Thereafter, it is possible to obtain an unbiased estimate of the regression parameter using the corrected variance in Eq. ( <ref type="formula">8</ref>) and covariance in Eq. ( <ref type="formula">9</ref>):</p><formula xml:id="formula_12">βCroon = Var(η x ) -1 Croon Cov(η x , η y ) Croon . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>The formulas above simplify when the Bartlett predictor is used to compute the factor scores, because in this case: scale x = scale y = 1. For a more comprehensive description of the MOC, we refer readers to <ref type="bibr" target="#b13">Devlieger et al. (2016)</ref> and <ref type="bibr" target="#b12">Devlieger and Rosseel (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuller's modifications for small sample estimation</head><p>We now present the two modifications for the small sample estimator suggested by <ref type="bibr" target="#b14">Fuller (1980)</ref>. Consider a simple linear regression model with one predictor and one independent variable. Using the notation of <ref type="bibr" target="#b15">Fuller (1987)</ref>, we can write this as:</p><formula xml:id="formula_14">Y i = α + β x i + e i , i = 1, 2, . . . , N,<label>(11)</label></formula><p>where the x i are assumed to be independent drawings from a N (µ x , σ xx ) distribution, and the e i are independent N (0, σ ee ) random variables. It is also assumed that e i is independent of x i .</p><p>If we write m xx and m xY for the sample (co)variance of x i (and Y i ), then the least squares</p><formula xml:id="formula_15">estimator βols = N i=1 (x i -x) 2 -1 N i=1 (x i -x)(Y i -Ȳ ) = m -1 xx m xY , (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>is unbiased for β. The estimator βols is also the maximum likelihood estimator for β, and has the smallest variance of unbiased linear estimators <ref type="bibr" target="#b15">(Fuller, 1987)</ref>. Next, consider a measurement error model where it is impossible to observe x i directly. Instead, we observe the sum</p><formula xml:id="formula_17">X i = x i + u i , (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where u i is a N (0, σ uu ) random variable, indicating the measurement error. In the absence of measurement error, σ uu equals zero. This model is referred to as the classical measurement error model <ref type="bibr" target="#b6">(Buonaccorsi, 2010;</ref><ref type="bibr" target="#b7">Carroll et al., 2006)</ref>. In this measurement error model the</p><formula xml:id="formula_19">least squares estimator βnaive = N i=1 (X i -X) 2 -1 N i=1 (X i -X)(Y i -Ȳ ) = m -1 XX m XY , (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>will now be biased. The impact of the measurement error is attenuation of the regression coefficient toward zero. However, if an estimate of σ uu &gt; 0 is available, it is possible to obtain an unbiased estimator for β by using the method of moments <ref type="bibr" target="#b6">(Buonaccorsi, 2010;</ref><ref type="bibr" target="#b7">Carroll et al., 2006;</ref><ref type="bibr" target="#b14">Fuller, 1980</ref><ref type="bibr" target="#b15">Fuller, , 1987))</ref>:</p><formula xml:id="formula_21">βmm = (m XX -σ uu ) -1 m XY . (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>Note that σ uu in Eq. ( <ref type="formula" target="#formula_21">15</ref>) plays the same role as offset x in Eq. ( <ref type="formula">8</ref>). <ref type="bibr" target="#b14">Fuller (1980</ref><ref type="bibr" target="#b15">Fuller ( , 1987) )</ref> suggests an alternative estimator with better small sample properties. To construct this alternative estimator, Eq. ( <ref type="formula" target="#formula_21">15</ref>) needs two adjustments. <ref type="bibr" target="#b14">Fuller (1980</ref><ref type="bibr" target="#b15">Fuller ( , 1987) )</ref> proposes the following alternative estimator</p><formula xml:id="formula_23">βλ,α = Ĥxx + α (N -1) -1 σ uu -1 m XY , (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>where Ĥxx is an estimator of σ xx , and α &gt; 0 is a fixed number to be specified. Furthermore,</p><formula xml:id="formula_25">Ĥxx =          m XX -σ uu if λ ≥ 1 + (N -1) -1 m XX -λ -(N -1) -1 σ uu if λ &lt; 1 + (N -1) -1 , (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>with λ the root of the determinantal equation</p><formula xml:id="formula_27">m ZZ -λ diag(0, σ uu ) = 0,<label>(18)</label></formula><p>where m ZZ is the sample variance-covariance matrix of both X i and Y i , containing the elements m XX , m XY and m Y Y in its upper triangular part.</p><p>Fuller's correction involves two modifications. First, a decision based on λ is implemented in Eq. ( <ref type="formula" target="#formula_25">17</ref>) to ensure Ĥxx is positive definite. This modification is named the λ-correction from here on. Second, including the parameter α in Eq. ( <ref type="formula" target="#formula_23">16</ref>) ensures a less right skewed distribution of the estimated regression coefficients, thereby reducing the variability at the cost of some bias as we explain in the next section. This modification is named the α-correction throughout this paper. Both modifications have different functions and can be employed separately from each other. Together they form the alternative estimator presented by <ref type="bibr" target="#b14">Fuller (1980</ref><ref type="bibr" target="#b15">Fuller ( , 1987) )</ref> for better small sample estimation in the presence of measurement error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining Fuller's modifications and the method of Croon</head><p>In this section, we show how the two modifications of <ref type="bibr" target="#b14">Fuller (1980)</ref> can be incorporated into the MOC estimator. For the considered model, only Var(η x ) Croon needs to be corrected. First, the λ-correction is performed:</p><formula xml:id="formula_28">Var(η x ) λ =          scale -2 x Var(f x ) -offset x if λ ≥ 1 + (N -1) -1 scale x -2 Var(f x ) -λ -(N -1) -1 offset x if λ &lt; 1 + (N -1) -1 , (<label>19</label></formula><formula xml:id="formula_29">)</formula><p>where λ is the smallest root of the determinantal equation:</p><formula xml:id="formula_30">Var(f x ) -λ offset x = 0. (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>This decision based on λ ensures that Var(η x ) λ is positive. Thereafter, the α-correction is performed:</p><formula xml:id="formula_32">Var(η x ) ssc = Var(η x ) λ + scale -2 x α (N -1) -1 offset x , (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>where α &gt; 0 is a fixed number to be specified. In this model, the offset terms are not needed for the covariances, and therefore Cov(η x , η y ) ssc = Cov(η x , η y ) Croon . Now that we have obtained the corrected terms, we can again find an estimate of the regression parameter β as usual:</p><formula xml:id="formula_34">βssc = Var(η x ) -1 ssc Cov(η x , η y ) ssc . (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>We call the adjusted estimator in Eq. ( <ref type="formula" target="#formula_34">22</ref>) with the two modifications the method of Croon with a small sample correction (SSC). The two modifications can be employed separately by leaving out the decision based on λ or by setting α equal to zero. As a consequence, it is possible to implement solely the λ-correction into the MOC. This latter extension of the MOC is named the MOC-λ method from here on.</p><p>If we assume λ ≥ 1 + (N -1) -1 , Eq. ( <ref type="formula" target="#formula_32">21</ref>) can be rewritten differently, giving more insight into the rationale behind the α-correction:</p><formula xml:id="formula_36">Var(η x ) ssc = scale -2 x Var(f x ) -1 -α (N -1) -1 offset x . (<label>23</label></formula><formula xml:id="formula_37">)</formula><p>The rationale of the correction is that the variance of the factor scores is subtracted with only a fraction (1 -α (N -1) -1 ) of the offset x parameter. This results in less variability at the cost of a certain amount of bias. The amount of bias and reduction in variability depends on the value of α. Meaningful values for α range from 0 to N -1, where using 0 is equivalent to the MOC and using N -1 is identical to FSR when working with the Bartlett predictor. Two suggestions have been made in the literature regarding the value of α. On the one hand, Fuller <ref type="bibr">(1980,</ref><ref type="bibr">1987)</ref> proposes to use α = p + 1 (with p the number of predictors). On the other hand, <ref type="bibr" target="#b30">Wall and Amemiya (2000)</ref> suggest using α = p + 5. In our simulation study, we also consider α = (N -1)/2, which assumes a mid-position between FSR and the MOC. This way, the effect of the correction is larger and adjusts to the sample size at hand. On the other hand, the correction term no longer vanishes if the sample size increases, resulting in biased estimates even when the sample size is very large. This version of the correction is therefore only useful when the sample size is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation studies</head><p>Two simulation studies were performed. In the first study, we examined the operating characteristics of the MOC with SSC estimator as compared to other estimators. We have considered 12 different conditions varying in sample size and reliability of the indicators.</p><p>Given the focus on small sample estimation, the following sample sizes were chosen: <ref type="bibr">30, 50, 100, 200, and 2000.</ref> In the second study, we assessed the robustness of the performance of the three specifications of α in different models (varying in the amount of latent variables and indicators). We only considered models where the amount of observations is larger than the number of variables in the model. For this reason, we left the smallest sample size (N = 20) out in the second simulation study, resulting in 10 distinct conditions per model. The data generation and analysis were done using R (version 4.1.0) (R Core Team, 2020) and the lavaan package (version 0.6-10) <ref type="bibr" target="#b22">(Rosseel, 2012)</ref>.</p><formula xml:id="formula_38">N = 20,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation study 1</head><p>For the first simulation study, two data generating models based on Eq. ( <ref type="formula">25</ref>) were considered. They differed only in terms of the reliability of the indicators. The first model had a reliability of 0.5 for all indicators, while in the second model the reliability was set to 0.8.</p><p>The conditions corresponded respectively to low and high reliability or, in other words, a condition with a higher and a lower amount of measurement error <ref type="bibr" target="#b5">(Brunner &amp; Austin, 2009;</ref><ref type="bibr" target="#b20">Nunnally &amp; Bernstein, 1994)</ref>. Both models contained three correlated (latent) predictors (η u , η v , η w ) and one (latent) dependent variable (η y ). All latent variables were measured by three observed indicators. In this study, the parameters of interest were the regression coefficients.</p><p>The true parameter values β for both models were: estimation was used for all estimators to obtain a higher number of converged solutions. Note that SEM-MLE refers to the estimation method (using MLE) and not the modelling procedure. The MOC-λ method was added to the simulation to see whether the λ-correction is beneficial for the MOC.</p><formula xml:id="formula_39">β u = 0.5, β v = 2,</formula><p>To evaluate each estimator, the following performance criteria were considered: the proportion of simulated datasets where the method converged (from here on named the convergence rate), the relative mean bias, the empirical standard deviation (ESD), and the mean squared error (MSE) of the estimators. Table <ref type="table" target="#tab_1">1</ref> summarizes the performance criteria.</p><p>We simulated 10000 samples, for each of the 6 × 2 = 12 settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation study 2</head><p>In the second simulation study, six data generating models were considered. They differed in terms of the reliability of the indicators, as well as in the number of latent predictors and indicators. The first three models had a reliability of 0.5 for all indicators, while in the last three models the reliability was set to 0.8. Hence, there was again a condition with low and high reliability <ref type="bibr" target="#b5">(Brunner &amp; Austin, 2009;</ref><ref type="bibr" target="#b20">Nunnally &amp; Bernstein, 1994)</ref>. Model 1 and 4 both contained three correlated (latent) predictors (η u , η v , η w ) and one (latent) dependent variable (η y ). All latent variables were measured by three observed indicators.</p><p>Model 2 and 5 included three correlated (latent) predictors (η u , η v , η w ) and one (latent) dependent variable (η y ) as well. However, in contrast to the other models, all latent variables were measured by six observed indicators. Model 3 and 6 contained six correlated (latent) predictors (η r , η s , η t , η u , η v , η w ) and one (latent) dependent variable (η y ). As in model 1 and 4, all latent variables were measured by three observed indicators. As before, the parameters of interest were the regression coefficients. For convenience the true parameter values β for all models were set to one: (β r = 1, Analogous to the first simulation study, we have compared the same seven estimators.</p><formula xml:id="formula_40">β s = 1, β t = 1,) β u = 1, β v =</formula><p>Bounded estimation was again used for all estimators to obtain a higher number of converged solutions. In contrast to the first simulation study, the interest lies here in the robustness of the performance of the different specifications of α in the SSC. In order to asses the performance, we used the averaged mean squared error: the mean squared error was averaged over the regression coefficients for every estimator in each model separately. We simulated 10000 samples, for each of the 10 settings (5 different samples sizes x 2 reliabilities) per model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results simulation study 1</head><p>Based on the bias-variance trade-off, a certain pattern in the results was anticipated.</p><p>On the one hand, we expect SEM and the MOC to be the best performing estimators with respect to bias, but the worst regarding variability. On the other hand, we foresee FSR and the SSC with the largest α-value to have the largest bias, but the lowest variability. Moreover, we expect the bias and variability to converge towards zero as the sample size grows larger.</p><p>Except for FSR and the SSC with the largest α-value we anticipate the bias to remain, even in large sample sizes. Finally, it is expected that the patterns will be less distinct when the reliability is high. Tables with the values of the considered performance criteria for the various estimators and settings can be found in the online supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence rate</head><p>Because methods ( <ref type="formula" target="#formula_1">2</ref>)-( <ref type="formula" target="#formula_9">7</ref>) were adaptations of FSR, they would all either converge or fail to converge for the same dataset. Hence, we grouped these methods together only when assessing convergence rates, and compared SEM-MLE and FSR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias</head><p>As expected, the results plotted in Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> display that the mean bias decreases when the sample size grows. Similarly, the differences between the estimators become smaller, but remain larger with low reliability. In contrast to SEM-MLE, the MOC, and MOC-λ, the bias of FSR and the SSC does not converge to zero, as anticipated. Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> show that the bias remains largest for FSR and the SSC with the largest α-value. This is in particular true for the model with low reliability.</p><p>The findings show identical results for the MOC and MOC-λ regarding (relative) bias in most conditions. The λ-correction was used (which means Var(η) Croon was not positive definite) substantially when the reliability was low and the sample size was small. The correction was used about 4.43% when N = 20 and 1.58% when N = 30. It was used less frequently in the following three conditions: 0.20% when the reliability was low and N = 50, 0.01% when the reliability was low and N = 100, and 0.06% when the reliability was high and N = 20. The findings suggest that the λ-correction does not have a beneficial effect for the (relative) bias.</p><p>Interestingly, the (relative) bias seems to depend on how large the regression coefficient is. If the true value is small (e.g., β u = 0.5), then the bias is positive. If the true value becomes larger (e.g., β w = 4), then the bias is negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical standard deviation</head><p>As expected, the results suggest that the empirical standard deviation reduces as the sample sizes grows. The differences between the estimators become smaller as well, especially in case of high reliability. The findings reveal no major differences between MOC-λ, FSR, and the SSC. The lowest ESD value is found for FSR and the SSC with the largest α-value.</p><p>Figures 5 and 6 display similar results for all estimators from a sample size of 100 onwards.</p><p>The results imply that SEM-MLE is the worst performing estimator in the smaller sample sizes (N ≤ 50), in particular when the reliability was low. Figures <ref type="figure">5</ref> and<ref type="figure">6</ref> show that the MOC performed well in most conditions, except in the condition with the smallest sample size (N = 20) and low reliability. In this condition, MOC-λ outperformed the MOC.</p><p>Therefore, suggesting the λ-correction contributes to a lower variability of the estimator. As mentioned above, the λ-correction is only used substantially when the reliability is low for the two smallest sample sizes. In all other conditions, the MOC and MOC-λ have identical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean squared error</head><p>As expected, the findings show decreasing mean squared error values as the sample size grows larger. Figures <ref type="figure">7</ref> and<ref type="figure">8</ref> indicate that the differences between the estimators become smaller as well, even more so in case of high reliability.</p><p>The overall worst performing method is SEM-MLE, specifically in the conditions with low reliability and small sample sizes. From a sample size of 100 onwards, SEM-MLE performs similar to the other estimators. The MOC shows comparable results to the other FSR estimators. However, its performance is poor in the conditions with low reliably and a sample size of 20 or 30. Hence, the λ-correction contributes to a lower MSE in these conditions.</p><p>In most conditions, the best performing method in terms of MSE is FSR, followed by the SSC with the largest α-value. On the other hand, FSR is the worst performing estimator when considering the MSE of βw in the two largest sample sizes <ref type="bibr">(N = 200, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results simulation study 2</head><p>As in the first simulation study, the findings show decreasing average mean squared error values as the sample size grows larger. Figures 9 and 10 indicate that the differences between the estimators become smaller as well, especially in case of high reliability.</p><p>Regarding the best and worst performing methods, the same results as in the first simulation study can be observed. The worst performing methods regarding average MSE are SEM-MLE and the MOC. The best performing method is FSR, followed by the SSC. From a sample size of 100 onwards, the methods perform more alike. Additionally, the results reveal once more the relevance of the λ-correction in the smallest sample sizes (N = 30, 50) when reliability is low. This can be verified in Figures <ref type="figure">9</ref> and<ref type="figure" target="#fig_4">10</ref>, which summarize the average MSE values for the three models with low and high reliability (respectively).</p><p>In line with the earlier findings, FSR is the best performing method with respect to average MSE in most conditions (i.e., when N ≤ 200). To assess the robustness of the results obtained from the three specifications for α specifically, we examine the average MSE of the methods relative to FSR. By using this relative average MSE, the differences in performance compared to other methods can be easily illustrated. The relative average MSE values are shown in Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table">4</ref> for the three models with low and high reliability (respectively).</p><p>As expected, the largest relative average MSE values can be found for SEM-MLE and MOC(-λ). The differences become smaller when the sample size grows and when the reliability of the indicators is high. In the results, the same patterns can be observed across the three different models. After FSR, the SSC with α = (N -1)/2 has the lowest average MSE values, followed by the specifications α = p + 5 and α = p + 1 (respectively). Despite of the different number of latent predictors and indicators, the findings suggest that the performance of the SSC with the three specifications for α is similar. Furthermore, the results indicate that the model with more indicators per latent variable has lower average MSE values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of simulation studies</head><p>In the first simulation study, the overall convergence rates were high for both SEM-MLE and the FSR methods. This high number of converged solutions suggests a beneficial effect from the use of bounded estimation (in this case standard bounds), as discovered earlier by De Jonckere and Rosseel (2022). The λ-correction was often used in the smallest sample sizes when reliability is low. Nevertheless, it had a profound impact on the ESD and MSE in the condition with the very small sample sizes. The results for the FSR methods were acquired using the Bartlett predictor. However, identical results can be obtained using the Regression predictor.</p><p>As mentioned above, the SSC performed well in terms of MSE in the considered conditions. The rationale of the SSC is to introduce some bias for less variability. This trade-off has a beneficial effect for the MSE in small sample estimation. Out of the three suggestions, the estimator with α = (N -1)/2 performed best in terms of MSE. Although it had the largest bias, combined with the smaller ESD it resulted in a lower MSE for most cases. We suggested the value (N -1)/2 as it adjusts to the sample size at hand and assures a mid-position between FSR and the MOC, but for use in small samples only. Additionally, the SSC with the suggestions from <ref type="bibr" target="#b14">Fuller (1980</ref><ref type="bibr" target="#b15">Fuller ( , 1987) )</ref> and <ref type="bibr" target="#b30">Wall and Amemiya (2000)</ref> surpassed SEM-MLE, MOC, and MOC-λ in all conditions.</p><p>SEM-MLE and the MOC were the worst performing estimators regarding ESD and MSE, particularly in the smaller sample sizes. However, both estimators outperformed FSR and the SSC regarding bias in most conditions. SEM-MLE and the MOC were particularly superior in terms of bias in case of low reliability and larger sample sizes. The results also showed decreasing bias of SEM-MLE and the MOC as the sample size grows. Furthermore, the ESD will also reduce, which will eventually (in lager sample sizes) result in a better performance compared to FSR and the SSC.</p><p>For the second simulation study, in each of the considered models, highly similar patterns were present in the results. This illustrates the robustness of the performance of the three α-values for the SSC in models varying in the number of latent predictors and indicators. Furthermore, the estimators performed slightly better in the model where latent variables were measured by more indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The goal of this paper was to (a) evaluate the performance of (i) standard SEM, (ii) the standard MOC, (iii) naive FSR, and (iv) the MOC with the proposed SSC in small samples sizes and (b) assess the robustness of the performance of the different specification for the SSC in different models. We assessed the estimators in terms of convergence rates, relative mean bias, empirical standard deviation, and mean squared error. The comparison was performed in 12 different conditions, with varying sample sizes and reliability of the indicators.</p><p>The findings reveal FSR as the best overall performing predictor, followed by the SSC with the largest α-value. Only for the conditions where N = 200 or N = 2000 and reliability was low, FSR was not the best performing estimator for the regression coefficient β w . In fact, FSR was then the worst performing predictor in terms of MSE in both cases. Therefore, the results suggest that both the reliability and sample size at hand play an important role. Our findings are in line with those of <ref type="bibr" target="#b13">Devlieger et al. (2016)</ref> where FSR was the worst performing estimator (compared to SEM-MLE and the MOC) regarding MSE in sample sizes above 300.</p><p>Furthermore, the findings demonstrated that the performances of the different specifications of α were similar across the considered models. The average MSE values of all methods were lowest in the model with more indicators.</p><p>Although the FSR and SSC estimators had the best performance in most of the conditions,they do not take the measurement error (entirely) into account. This results in a lower MSE in small samples. However, despite the beneficial effect on the MSE, the consequences of not taking measurement into account should not be neglected. On the one hand, the regression parameters can be over-or underestimated depending on the correlation structure between the latent variables <ref type="bibr" target="#b8">(Cole &amp; Preacher, 2014)</ref>. On the other hand, an inflation of the type I error rate may occur under certain circumstances <ref type="bibr" target="#b5">(Brunner &amp; Austin, 2009)</ref>. Moreover, the more complex the model becomes, the worse the consequences of both problems.</p><p>When analyzing relationships between latent variables, we encourage researchers to not choose a method ill-considered. Instead, we recommend to consider the sample size, complexity of the model, reliability of the indicators at hand, and to use the best method for the situation. If the sample size is large enough (N ≥ 200), one might be better off using SEM-MLE or the MOC instead of FSR or the SSC. However, if the sample size is small (N ≤ 100), one might consider using FSR or the SSC instead of SEM-MLE or the MOC.</p><p>Additionally, the goal of the research should be taken into account. If bias is important, unbiased methods such as SEM and the MOC are favored in larger sample sizes, or the SSC with the smallest α-value (= p + 1) if the sample size is small. If bias is not of primary importance (and a lower variability is), methods such as FSR and the SSC with the largest α-value (= (N -1)/2) are favored. The small sample correction lets researchers choose a compromise between ignoring measurement error (i.e., using FSR) and taking into account measurement error (i.e., using SEM or the MOC).</p><p>Like all simulation studies, our study was not without any limitations. The first limitation of this study is that only certain conditions and models were examined. Although these conditions and models may be commonly found in the literature, different settings may lead to different results. The second limitation is that we have not considered the type I error and the power as performance criteria. Future research should look into hypothesis testing to see how well the SSC and other estimators perform in (small) sample sizes, preferably using a different simulation model to see how general the results obtained in this paper are.</p><p>In the Regression method <ref type="bibr" target="#b28">(Thomson, 1934;</ref><ref type="bibr" target="#b29">Thurstone, 1935)</ref>, the factor score matrix is defined as:</p><formula xml:id="formula_41">A R = Var(η)Λ T Σ -1 . (<label>30</label></formula><formula xml:id="formula_42">)</formula><p>In the second step (once the factor scores are calculated), an estimate of the regression parameters can be obtained as follows:</p><formula xml:id="formula_43">βfs = Var(f x ) -1 Cov(f x , f y ). (<label>31</label></formula><formula xml:id="formula_44">)</formula><p>This estimator is biased regardless of the sample size, except in the trivial setting where Cov(f x , f y ) = 0, or when there is no measurement error (ϵ = 0) <ref type="bibr" target="#b15">(Fuller, 1987)</ref>.</p><p>The method of Croon <ref type="bibr" target="#b10">Croon (2002)</ref> presented his correction formulas in scalar form. We provide here the formulas in matrix form:</p><formula xml:id="formula_45">Var(η) Croon = D -1 [Var(f ) -E] D -T<label>(32)</label></formula><p>with E an m × m matrix defined as</p><formula xml:id="formula_46">E = AΘA T , (<label>33</label></formula><formula xml:id="formula_47">)</formula><p>and where D is an m × m matrix defined as</p><formula xml:id="formula_48">D = AΛ.<label>(34)</label></formula><p>Note that when Bartlett factor scores are used, D = I, but this is not true for regression factor scores. Using the Croon corrected (co)variance matrix in Eq. ( <ref type="formula" target="#formula_45">32</ref>), we can obtain unbiased estimates for the regression parameters:</p><formula xml:id="formula_49">βCroon = Var(η x ) -1 Croon Cov(η x , η y ) Croon . (<label>35</label></formula><formula xml:id="formula_50">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance criteria Formulas</head><p>Convergence rate  The latent variables were handled separately in the FSR methods, implying that convergence was only achieved if the factor analysis was successful for all four measurement models.     </p><formula xml:id="formula_51"># estimator converged R Mean bias 1 R R i=1 ( bi -b) Relative mean bias 1 R R i=1 bi b -1 Empirical standard error 1 R-1 R i=1 ( bi -b) 2 with b = 1 R R i=1 bi Mean squared error R i=1 ( bi -b) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Relative average mean squared error in case of high reliability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3</head><p>Relative bias in case of low reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Relative bias in case of high reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>Empirical standard deviation in case of low reliability (some values lie out of scale, these can be found in the corresponding table in the online supplemental material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>Empirical standard deviation in case of high reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head><p>Mean squared error in case of low reliability (some values lie out of scale, these can be found in the corresponding table in the online supplemental material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8</head><p>Mean squared error in case of high reliability (some values lie out of scale, these can be found in the corresponding table in the online supplemental material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9</head><p>Average mean squared error in case of low reliability (some values lie out of scale, more information regarding these values can be found in Table <ref type="table" target="#tab_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10</head><p>Average mean squared error in case of high reliability (some values lie out of scale, more information regarding these values can be found in Table <ref type="table">4</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and β w = 4. The true standardized parameter values β z for both models were: β z u = 0.072, β z v = 0.302, and β z w = 0.502. Figure 2 depicts the data-generating model with the true parameter values. For each simulated dataset, we fitted the same model in Figure 2. We then compared seven different estimators: (1) traditional SEM (SEM-MLE), (2) the standard MOC, (3) the MOC-λ, (4) FSR (using Bartlett factor scores), and (5-7) the SSC with one of the three suggested values for α, being: α = p + 1 = 4, α = p + 5 = 8, and α = (N -1)/2. Bounded</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1, and β w = 1. The true standardized parameter values β z for all models were: (β z r = 0.134, β z s = 0.155, β z t = 0.161,) β z u = 0.134, β z v = 0.155, and β z w = 0.161. A visualisation of the models and their specific parameter values can be found in the online supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 A</head><label>1</label><figDesc>Figure 1A simple structural equation model.</figDesc><graphic coords="30,162.25,75.19,270.77,67.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Simulation model. The variances of the latent predictors varied in both models. For the model with low reliability, these were: Var(η u ) = 1.2, Var(η v ) = 1.3, and Var(η w ) = 0.9. For the model with high reliability, these were: Var(η u ) = 4.8, Var(η v ) = 5.2, and Var(η w ) = 3.6.</figDesc><graphic coords="31,136.31,75.19,322.65,210.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="32,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="33,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="34,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="35,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="36,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="37,128.41,75.19,338.45,183.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="38,128.41,75.19,338.45,182.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="39,128.41,75.19,338.45,182.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reveals a high overall</cell></row></table><note><p>fact, a converged solution is acquired 9930 out of 10000 times when N = 20, and 9989 out of 10000 times when N = 30, both in case of low reliability. From a sample size of 50 onwards, the convergence rates reach approximately 100% for all conditions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Overview of performance criteria. For a given simulated dataset, the estimator is denoted by bi and its true value denoted by b. The number of simulations is denoted by R.</figDesc><table><row><cell></cell><cell cols="2">low reliability</cell><cell cols="2">high reliability</cell></row><row><cell>N</cell><cell>SEM-MLE</cell><cell>FSR</cell><cell>SEM-MLE</cell><cell>FSR</cell></row><row><cell>20</cell><cell>99.30</cell><cell>99.84</cell><cell>99.96</cell><cell>99.96</cell></row><row><cell>30</cell><cell>99.89</cell><cell>99.94</cell><cell>100</cell><cell>100</cell></row><row><cell>50</cell><cell>100</cell><cell>99.97</cell><cell>100</cell><cell>100</cell></row><row><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>200</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>2000</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Overview of convergence rates in percentage. Bounded estimation was used for all estimators.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Relative average mean squared error in case of low reliability.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: formulas in matrix notation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The statistical model in structural equation modeling</head><p>Consider a structural equation model with p observed variables and m latent variables.</p><p>The measurement model is defined as</p><p>where y, ν, and ϵ are p × 1 vectors of respectively the observed variables, intercepts, and residual errors. The p × m matrix Λ contains the factor loadings relating the latent to the observed variables. We denote Var(ϵ) = Θ. The structural model is defined as </p><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factor score regression</head><p>The factor scores can be obtained by</p><p>with A the m × p factor score matrix. Using the Bartlett method <ref type="bibr" target="#b1">(Bartlett, 1937</ref><ref type="bibr">(Bartlett, , 1938))</ref>, the factor score matrix is defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small sample correction</head><p>To ensure that the small sample correction formulas are valid for both <ref type="bibr">Bartlett and</ref> regression factor scores, we first define</p><p>and</p><p>Eq.( <ref type="formula">32</ref>) can then be rewritten as</p><p>The λ-correction can be expressed as follows:</p><p>where N is the sample size, and λ is the smallest root of the determinantal equation | Var(f ) -λ E| = 0. This decision based on λ ensures that Var(η) λ is positive definite. The α-correction then proceeds as follows:</p><p>where α is a fixed number to be specified. Once the elements of the variance-covariance matrix are corrected, it is possible to obtain estimates of the regression parameters β: βssc = Var(η x ) -1 ssc Cov(η x , η y ) ssc .</p><p>(41)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The effect of sampling error on convergence, improper solutions, and goodness-of-fit indices for maximum likelihood confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gerbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="173" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The statistical conception of mental factors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="97" to="104" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structural equation modeling with small samples: Test statistics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="197" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<title level="m">Structural equations with latent variables</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonconvergence, improper solutions, and starting values in LISREL maximum likelihood estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boomsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inflation of type i error rate in multiple regression when independent variables are measured with error</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="46" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Measurement error: Models, methods, and applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Buonaccorsi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Chapman; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Measurement error in nonlinear models: A modern perspective</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stefanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Crainiceanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Manifest variable path analysis: Potentially serious and misleading consequences due to uncorrected measurement error</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Preacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">300</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Hinkley</surname></persName>
		</author>
		<title level="m">Theoretical statistics</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using predicted latent scores in general latent structure models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent variable and latent structure models</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Marcoulides</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Moustaki</surname></persName>
		</editor>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="195" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using bounded estimation to avoid nonconvergence in small sample structural equation modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Jonckere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factor score path analysis: An alternative for SEM?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Devlieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypothesis Testing Using Factor Score Regression: A Comparison of Four Methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Devlieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<idno type="DOI">10.1177/0013164415607618</idno>
		<ptr target="https://doi.org/10.1177/0013164415607618" />
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="741" to="770" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Properties of some estimators for the errors-in-variables model</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Fuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="422" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Fuller</surname></persName>
		</author>
		<title level="m">Measurement error models</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: Data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">An introduction to statistical learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Common factor score estimates in multiple regression problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lastovicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thamodaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating Small Sample Approaches for Model Test Statistics in Structural Equation Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nevitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15327906MBR3903\_3</idno>
		<ptr target="https://doi.org/10.1207/S15327906MBR3903\_3" />
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="478" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Small sample corrections for wald tests in latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nunnally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Budtz-Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychometric theory</title>
		<imprint>
			<publisher>McGraw-Hill, Inc. Ozenne</publisher>
			<date type="published" when="1994">1994. 2020</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="841" to="861" />
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing. R Foundation for Statistical Computing</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">lavaan: An R package for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
		<ptr target="https://www.jstatsoft.org/v48/i02/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Small sample solutions for structural equation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Small sample size solutions: A guide for applied researchers and practitioners</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Van De Schoot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">M</forename><surname>Miocević</surname></persName>
		</editor>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="226" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Summated rating scales. a monte carlo investigation of the effects of reliability and collinearity in regression models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Bunting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="676" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regression among factor scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skrondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="563" to="575" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SEM with small samples: Two-step modeling and factor score regression versus Bayesian estimation with informative priors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Smid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Small sample size solutions: A guide for applied researchers and practitioners</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Van De Schoot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">M</forename><surname>Miocević</surname></persName>
		</editor>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="239" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparisons among several consistent estimators of structural equation models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Takane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="188" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The meaning of i in the estimate of g</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1934">1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Vectors of Mind</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimation for polynomial structural equation models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amemiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">451</biblScope>
			<biblScope unit="page" from="929" to="940" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
