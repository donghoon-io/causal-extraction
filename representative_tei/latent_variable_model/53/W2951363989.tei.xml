<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the limits of learning: Segregation of information integration and response selection is required for learning a serial reversal task</title>
				<funder ref="#_Y8pmg4U">
					<orgName type="full">CONICET postdoctoral fellowship</orgName>
				</funder>
				<funder ref="#_j6J7YJr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-10-27">October 27, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Camilo</forename><forename type="middle">Juan</forename><surname>Mininni</surname></persName>
							<email>mininni@dna.uba.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Ingenierı ´a Biome ´dica</orgName>
								<orgName type="institution">Universidad de Buenos Aires</orgName>
								<address>
									<settlement>Buenos Aires Argentina</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><forename type="middle">Silvano</forename><surname>Zanutto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Ingenierı ´a Biome ´dica</orgName>
								<orgName type="institution">Universidad de Buenos Aires</orgName>
								<address>
									<settlement>Buenos Aires Argentina</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Instituto de Biologı ´a y Medicina Experimental</orgName>
								<orgName type="institution" key="instit2">Consejo Nacional de Investigaciones Cientı ´ficas y Te ´cnicas</orgName>
								<address>
									<settlement>Buenos Aires</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tokai University</orgName>
								<address>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the limits of learning: Segregation of information integration and response selection is required for learning a serial reversal task</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-27">October 27, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pone.0186959</idno>
					<note type="submission">Received: July 28, 2017 Accepted: October 10, 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Animals are proposed to learn the latent rules governing their environment in order to maximize their chances of survival. However, rules may change without notice, forcing animals to keep a memory of which one is currently at work. Rule switching can lead to situations in which the same stimulus/response pairing is positively and negatively rewarded in the long run, depending on variables that are not accessible to the animal. This fact raises questions on how neural systems are capable of reinforcement learning in environments where the reinforcement is inconsistent. Here we address this issue by asking about which aspects of connectivity, neural excitability and synaptic plasticity are key for a very general, stochastic spiking neural network model to solve a task in which rules change without being cued, taking the serial reversal task (SRT) as paradigm. Contrary to what could be expected, we found strong limitations for biologically plausible networks to solve the SRT. Especially, we proved that no network of neurons can learn a SRT if it is a single neural population that integrates stimuli information and at the same time is responsible of choosing the behavioural response. This limitation is independent of the number of neurons, neuronal dynamics or plasticity rules, and arises from the fact that plasticity is locally computed at each synapse, and that synaptic changes and neuronal activity are mutually dependent processes. We propose and characterize a spiking neural network model that solves the SRT, which relies on separating the functions of stimuli integration and response selection. The model suggests that experimental efforts to understand neural function should focus on the characterization of neural circuits according to their connectivity, neural dynamics, and the degree of modulation of synaptic plasticity with reward.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Natural environments are complex places in which animals strive to survive, with hidden variables and stochastic factors such that the information available at any moment is partial, and it must be sampled at several time points and integrated. What is more, the rules governing the environment might change with time, leading to conflicting information. For example, an animal might learn how and where to seek for food, but if the place for feeding cyclically changes, or the means of obtaining food change, the animal has to switch strategies along <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In this case, no unique strategies exist, but several strategies must be learned. More importantly, the value of a response not only depends on the current scenario, but in the history of events, for example, the history of recent success of a given strategy. Therefore, it is relevant to study tasks in which rules might change over time in such a way that the reinforcement of stimulus/ response pairings is inconsistent, i.e. Inconsistent-Reinforcement Tasks (IRTs). In particular, the Serial Reversal Task (SRT) is an IRT in which two rules alternate over time, demanding the animal to keep track of previous events in order to maximize reward <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. With enough training, animals learn to adapt their behaviour as soon as a reversal occurs. However, learning an SRT through a neural network model can be problematic: since each stimulus/response pairing is positively and negatively reinforced in the long run, learning of one rule may lead to the erasure of information regarding other rules, conforming a case of catastrophic forgetting <ref type="bibr" target="#b4">[5]</ref>. On the other hand, although brain regions like the prefrontal cortex <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and the striatum <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> have been found necessary for learning the SRT, the precise neural mechanisms involved are not well understood.</p><p>The goal of this work is to find the essential properties required by biologically plausible neural networks to solve an IRT, taking the SRT as paradigm. We focus on stochastic spiking neural networks (SSNN), a very general kind of neural network model that has been employed to explain how key features of neural circuits, like excitatory-inhibitory balance <ref type="bibr" target="#b9">[10]</ref> and spike timing-dependent plasticity (STDP) <ref type="bibr" target="#b10">[11]</ref>, can lead to Bayesian inference <ref type="bibr" target="#b11">[12]</ref> and reinforcement learning <ref type="bibr" target="#b12">[13]</ref>. For a very general family of SSNNs, we show analytically that strong limitations to learning the SRT emerge when the functions of integration of stimuli information and response selection are conducted by the same neural population. We propose a model that is able to learn the SRT and discuss the implications of the results in relation to the neural mechanisms of decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We will study the characteristics of an agent controlled by a biologically plausible neural network that learns to solve a SRT, conforming to what we will define as the hypothesis of functionality by learning, which states that the set of configurations that gives functionality is a small subset of the set of initial configurations. In this way, functionality is acquired by a learning mechanism that always leads the system from any random initial condition to one of the functional configurations. The hypothesis implies that the system is not initially designed to solve a given task from start.</p><p>A SRT is a discrimination task in which the mapping between the stimulus and the correct response is reversed after a given (random) number of trials (Fig <ref type="figure" target="#fig_0">1a</ref>). One out of two possible cue stimuli (s 1 or s 2 ) is presented to the agent. During cue presentation the agent has to execute one out of two possible responses (R 1 or R 2 ) in order to get a reward. Which response is correct depends on the current rule (rule L 1 : s 1 ! R 1 , s 2 ! R 2 ; rule L 2 : s 1 ! R 2 , s 2 ! R 1 ). A reward stimulus is shown after cue presentation: r 1 for correct responses or r 0 for incorrect ones. One rule withstands until a switch of rules occurs at random. Switching occurs with low probability, to ensure that a considerable number of trials with the same rule are presented.</p><p>The structure of the task implies that any agent that follows only one stimulus/response mapping as strategy will fail to get reward in half of the trials. Moreover, information provided by the stimuli is useless unless the agent is capable of retaining information about the current rule. Optimal performance can be achieved by adhering to a successful strategy, and to switch strategies when the current one is no longer successful.</p><p>We will consider an agent that is controlled by a spiking stochastic neural network composed of a sensory module Y and an integration/decision module K (Fig <ref type="figure" target="#fig_0">1b</ref>). Neurons in module Y code the sensory stimuli and project to module K, while neurons in module K project to the response neurons and to other K neurons. One half of the K population projects to response neuron R 1 (the K R 1 subset of module K), the other half to response neuron R 2 (the K R 2 subset of module K). We assume that the firing of any neuron within a K R group is enough to trigger the corresponding behavioural response. Therefore, the K module integrates sensory information together with information from within the network, and at the same time it defines the response that is going to be executed. The firing state of module K will be represented by a vector n(t), where each element n i (t) ∊ {0, 1} represents the firing state of the ith K neuron. Similarly, we define a vector y(t) where each element y i (t) ∊ {0, 1} represents the firing state of neuron y i . As a shortcut, we will use p (n i (t) = 1) and p(n i (t)) as equivalent expressions that represent the probability of neuron i of being active at time t. The same holds for p(y i (t) = 1) and p(y i (t)).</p><p>We will consider a network with a Y module composed of 4 neurons such that each stimulus is perfectly codified by one specific neuron, i.e. (p(y i |S i ) = 1 and p(y i |S j ) = 0 8I 6 ¼ j) where S i is the ith element of S = (s 1 , s 2 , r 1 , r 0 ). Module K is composed of 8 neurons, which is the minimum number of neurons required to solve the SRT: one neuron for each stimulus (cue or reward) for each rule. Each trial T has two time points (t and t + 1), one for cue presentation and another for reward stimulus presentation. The K R 1 group comprises neurons from 1 to 4; K R 2 comprises neurons from 5 to 8. Only one Y neuron and one K neuron fire at each time point, and the decision is evaluated during cue presentation (Fig <ref type="figure" target="#fig_0">1c</ref>). Then, each neuron in module K has a probability of firing that is given by:</p><formula xml:id="formula_0">pðn i ðt þ 1Þ ¼ 1jwðtÞ; yðtÞ; nðtÞÞ ¼ f ðw i ðtÞzðtÞÞ X N K j f ðw j ðtÞzðtÞÞ ;<label>ð1Þ</label></formula><p>where w stands for all synaptic weights in the network, w i is a vector containing the synaptic weights of afferent connections from all Y and K neurons onto the ith neuron in module K, and z is a vector containing the firing states of all Y and K neurons such that w ij is the synaptic weight of the jth neuron with firing state z j that projects to neuron i. The function f can be any function with the sole condition of being strictly increasing with w ij . Eq (1) endorses the K module with characteristics of a "soft winner-take-all" circuit in which a highly excited neuron inhibits the other neurons in the module through a global inhibitory circuit <ref type="bibr" target="#b11">[12]</ref>. Synaptic weights w ij change according to the local pre/post synaptic activity and the reward stimuli r. The change Δw ij of a synaptic weight w ij is given by a function g: Dw ij ðtÞ ¼ gðz i ðtÞ; z j ðt À 1Þ; rewðtÞÞ ¼ d z i ;z j ;rewðtÞ ; ð2Þ where z i (t) and z j (t -1) are the respective firing states of the pre and post synaptic neurons, and rew is a function of the delivery of reward, such that rew = 1 during the cue and reward presentation for trials in which the response was correct, and rew = 0 otherwise. The g function can be in principle any function taking real values δ, with one δ for each combination of pre and post synaptic state and reward function rew.</p><p>We assume that the neural network sketched in Fig 1b <ref type="figure">fulfils</ref> the Markov condition: the firing state of the system (i.e. which neuron is firing at time t) is only dependent on the firing state of the network at the previous time. This means that information about past events can only by carried on in the current state of the system. In the case of a SRT, a cue stimulus should elicit either the response R 1 or R 2 , depending on the current rule. For example, s 1 should elicit response R 1 only during rule L 1 , or R 2 only during rule L 2 . This implies that s 1 should elicit a response from a subset of the K R 1 group when L 1 rule is current, or from the K R 2 group when L 2 rule is current. Since there is no explicit stimulus acting as a cue of the rule, the differential response of the K module in front of the same stimulus can be achieved only if the K neurons integrate inputs from the Y module together with inputs from the K module itself. This means that each stimuli must be coded by different groups of K neurons depending on the current rule. Then, the occurrence of an error should act as pivot, leading the system to the set of states associated with the other strategy.</p><p>We can write the transition probability of the Markov chain that describes the dynamics of the whole system (network, stimuli and rules): </p><formula xml:id="formula_1">p½sðt þ 1Þ; Lðt þ 1Þ; yðt þ 1Þ; nðt þ 1Þ; wðt þ</formula><formula xml:id="formula_2">:p½yðt þ 1Þjsðt þ 1Þ:p½sðt þ 1Þ:p½Lðt þ 1Þ :<label>ð3Þ</label></formula><p>Eq (3) is obtained by applying the chain rule of conditional probabilities, and using the fact that L is independent of stimulus, and that firing state n(t + 1) is independent of any other variable when conditioned to n(t), y(t) and w(t). Note that, since plasticity is assumed deterministic, Eq (3) is true if w(t + 1) is the resulting synaptic weight configuration of applying function g given (n(t), y(t), n(t + 1)). Any transition to a different synaptic weight configuration will have zero probability. Now we can find the transition probabilities that solve the SRT and study under what conditions a learning process is capable of reaching the solution. Fig <ref type="figure" target="#fig_1">2</ref> shows the directed graph for the transitions in the state space that solve the SRT. Under rule L 1 , neurons n 1 and n 2 fire with cue s 1 and cue s 2 respectively, while neuron n 3 codes r 1 and n 4 codes r 0 . For rule L 2 , neurons n 5 and n 6 fire with cue s 1 and cue s 2 , while neuron n 7 codes r 1 and n 8 codes r 0 . Neurons n 4 and n 8 are responsible for the strategy switching in the behaviour of the agent. Each time a transition between rules occurs, an error is committed, and the corresponding error neuron fires. Eq 1 tells us that the only way to change the transition probabilities is by adjusting the synaptic weights. Since the f function is strictly increasing with w ij , weights must be increased to favour a transition, or decreased to make a transition less probable.</p><p>The transition probabilities depicted in Fig 2 <ref type="figure">lead</ref> to specific transition probabilities for the firing state of each neuron in module K, conditioned to the firing state of their respective presynaptic neurons (Fig <ref type="figure" target="#fig_2">3</ref>). For example, if stimulus s 1 is presented at time t and neuron n 3 fired at time t -1, then the firing probability of neurons n 1 at time t should be high, while the firing probability of the other neurons should be low. This is because each combination of stimulus and rule must be coded by one specific neuron of the eight neurons that compose module K. This specificity in transition probabilities required to solve the SRT translates into a specificity in the solution weight matrix (Fig <ref type="figure" target="#fig_3">4a</ref>), due to the strictly incremental relation between firing probability and synaptic weight depicted in Eq <ref type="bibr" target="#b0">(1)</ref>.</p><p>Based on the hypothesis of functionality by learning, we can say that the network learns to solve the SRT only if the plasticity function g leads the system to the solution weight matrix of Fig <ref type="figure" target="#fig_3">4a</ref>, regardless of the initial conditions. However, the SRT is problematic in that there are no combinations of cue stimulus and behavioural response that are always rewarded. To understand this point, we can compare the SRT with another task, a discrimination task (DT), which comprises two stimuli and two responses as in the SRT. Moreover, there are two possible rules which define which stimulus/response pairing is rewarded, as shown in Fig 1a . The difference with the SRT strives in that in the DT each rule is cued by a specific stimulus (different from s 1 and s 2 ), which are codified in turn by neurons in the Y module. In this way, the network has direct information about which rule is current at a given moment. This means that the set of stimulus/response pairings that leads to reward and the set that leads to no reward are disjoint sets. Fig <ref type="figure" target="#fig_3">4b</ref> shows the synaptic weight matrix that allows the network to solve a DT in which the set of stimulus/response pairings {s 1 , L 1 , R 1 } and {s 2 , L 2 , R 2 } are always rewarded, the DT, it is enough to increment the synaptic weights of connections from the Y neurons that codify the cues to the K neurons that exert the correct response. This fact is what makes possible to find a network that converges to the solution matrix for the DT by choosing a suitable g function, such like a Hebbian plasticity function that leads to increments in the synaptic weights only when a reward is obtained.</p><p>have very low probability. Under these transition probabilities, for each rule there is a different set of neurons that codes each stimulus, and one neuron per rule that elicits the transition between rules when an error occurs.</p><p><ref type="url" target="https://doi.org/10.1371/journal.pone.0186959.g002">https://doi.org/10.1371/journal.pone.0186959.g002</ref>  However, in the case of the SRT there are no disjoint sets of stimulus response pairings that separates reward from no reward. In fact, since we assume that the system is initiated without any information about how to solve the task, it can be seen that:</p><formula xml:id="formula_3">pðr 1 js x ; n y Þ ¼ pðr 0 js x ; n y Þ ¼ 1 2 8x</formula><p>; y, and p(n|r 1 ) = p(n|r 0 ) = p(n). In particular:</p><formula xml:id="formula_4">pðz i ; z j jr 1 Þ ¼ pðz i ; z j jr 0 Þ ¼ pðz i ; z j Þ:<label>ð4Þ</label></formula><p>This allows us to write the average change hΔw ij i for a given w ij :</p><formula xml:id="formula_5">hDw ij i ¼ X z i X z j pðz i ; z j jr 1 Þpðr 1 Þd z i ;z j ;r 1 þ pðz i ; z j jr 0 Þpðr 0 Þd z i ;z j ;r 0 ¼ X z i X z j pðz i ; z j Þpðr 1 Þd z i ;z j ;r 1 þ pðz i ; z j Þpðr 0 Þd z i ;z j ;r 0 ¼ 1 2 X z i X z j pðz i ; z j Þðd z i ;z j ;r 1 þ d z i ;z j ;r 0 Þ ¼ 1 2 " p z i ;z j " d ;<label>ð5Þ</label></formula><p>where "</p><formula xml:id="formula_6">p z x ;x y ¼ ðpðz x ¼ 1; z y ¼ 1Þ; ðpðz x ¼ 1; z y ¼ 0Þ; ðpðz x ¼ 0; z y ¼ 1Þ; ðpðz x ¼ 0; z y ¼ 0ÞÞ and " d ¼ ðd 111 þ d 110 ; d 101 þ d 100 ; d 011 þ d 010 ; d 001 þ d 000 Þ.</formula><p>The hΔw ij i can be understood as the inner product between the vector " p z x ;z y representing the probability distribution of the pre/post synapses pair, and " d, which contains the net change in w ij for each pre/post configuration. The inner product implies a kind of correlation between the two vectors, and changing a pair of synaptic weights in specific directions requires a precise adjustment of this inner product: </p><formula xml:id="formula_7">hDw ij i &gt;&gt; hDw mn i ) " p z i ;z j " d &gt;&gt; " p z m ;z n " d<label>ð6Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to solve a SRT requires segregation of stimulus history coding from decision making</head><p>The incapacity of the model depicted in Fig <ref type="figure" target="#fig_0">1b</ref> for solving the SRT stems from the fact that the solution weight matrix cannot be reached by any plasticity function g. Conversely, this characteristic arises from two facts:</p><p>1. Correct stimulus/response pairings change over time, and there are no cues that give information about the current rule. Thus, in order to keep information about the current rule, the response of the system towards the stimuli must be specially conditioned by the previous states of the system.</p><p>2. The population that codes information about the current rule is the same population that defines the behavioural motor response.</p><p>Fact number 1 implies that the task cannot be solved as a DT, since the reward does not separates stimulus/response pairs into any two disjoint subsets. Fact number 2 implies that coding of stimuli cannot be done freely, because when a neuron codes a stimulus by firing, it is also defining a motor response that is expected to lead to reward. Fact number 1 cannot be avoided because it stems from the very nature of the task. But fact number 2 can be circumvented in a model in which coding and decision functions are performed in separated neural populations. Fig 5a depicts such a model (referred to as complex network; see Methods for a detailed description of its implementation). There, module K integrates information about cues and reward as before, and about the response executed as well, but does not defines the motor response. Neurons in the integration module K project to two decision neurons D 1 and D 2 . The decision neuron that fires univocally defines which response neuron (R 1 or R 2 ) will activate, leading to the corresponding motor response.</p><p>Therefore, module K needs to codify all the information required to solve the task. Ideally, it would suffice that neurons in module K codified the cue presented and the current rule. Nevertheless, no cue informs about the current rule, and module K only sees stimuli. Therefore, information about the current rule must be extracted from the history of perceived stimuli. For example, the sequence (s 1 , R 1 , r 1 ) shows that rule L 1 was currently working, and it should continue to do so except a reversal occurs, which is unpredictable but relative rare. In this manner, a possible solution is that neurons in module K codify each stimulus differently, depending on the previous stimulus history or contingency. This can be done following the model presented in Kappel et al <ref type="bibr" target="#b13">[14]</ref>. There, it was shown that stochastic spiking neural networks with lateral excitation and a global inhibitory feedback, in combination with spike timing-dependent plasticity (STDP), have as an emergent property the formation of neural assemblies that encode external stimuli differently depending on the sequence of stimuli that preceded. In our case, module K should divide in groups of neurons codifying sequences of 4 stimuli: (s(T -1), R(T -1)), r(T -1), s(T)), implying 16 possible contingencies.</p><p>The SRT structure for the following simulations is depicted in Fig 5b . Each trial starts with the presentation of one cue, 25 ms long. At t decision = 15 ms from trial onset, the state of neurons in the R module are actualized based on which neuron is firing in module D. At the same time, the response is characterized as correct or incorrect. During the interval [15 ms,25 ms] the synapses from module K to module D are modified following Eq <ref type="bibr" target="#b23">(24)</ref> (see Methods section). The states of the R neurons are sustained unaltered between actualizations. The reward stimulus, also 25 ms long, is presented immediately after cue offset, being r 1 or r 0 depending on the correctness of the response. The rule is reversed every 15-20 trials, unless otherwise stated.</p><p>Conceptually, learning is achieved in two steps. In the first place, neurons in module K need to form subpopulations that respond differently to each cue at time t, given the past contingency up to the cue presentation at trial T -1. This is achieved by plasticity rule described by Eq <ref type="bibr" target="#b22">(23)</ref>, provided that the system has enough memory so that events in trial T -1 have an impact during trial T. Next, neurons in module D need to read the firing of module K, mapping each contingency coded in module K to the correct response. This is achieved thanks to the learning rule described by Eq <ref type="bibr" target="#b23">(24)</ref>, which is proved to reduce the distance between module Besides, the executed motor response gives sensory feedback, such that each response is also coded by module Y. Module Y connects to all neurons in the integration module K, which in turn connect with each other and with each neuron in the decision module D. Each neuron D is hardwired to one neuron R, so that the response executed is entirely defined by the D module. Synapses between module Y and module K, and within module K are plastic, subject to plasticity rule defined in Eq <ref type="bibr" target="#b22">(23)</ref>, which is applied at all times and is not dependent on reward. Synapses between module K and module D are plastic, subject to plasticity rule defined in Eq <ref type="bibr" target="#b23">(24)</ref>, which depends on reward. (b) Serial reversal protocol for training the network depicted in (a). Stimuli are presented for 25 ms, and the motor response to be executed is chosen at t decision = 15 ms from cue onset. Plasticity between K and D neurons is applied only if there was reward and within a window spanning from t decision to the end of cue presentation.</p><p><ref type="url" target="https://doi.org/10.1371/journal.pone.0186959.g005">https://doi.org/10.1371/journal.pone.0186959.g005</ref> D firing probabilities p(d|r 1 ) and p(d), leading in turn to an increase in p(r 1 ) (see Rueckert et al. <ref type="bibr" target="#b14">[15]</ref>).</p><p>The model effectively learns to solve a SRT, as can be seen in After learning, neurons in module K fire in sequences (Fig <ref type="figure" target="#fig_8">8</ref>) which presumably contain the information employed by module D to choose the right response. We studied the firing profile of the K module by computing the probability of firing of each K neuron during t decision (Fig <ref type="figure" target="#fig_9">9a</ref>). It can be seen that each one of the 16 possible contingencies has a firing profile that is almost unique. Some contingencies are codified by s single neuron (for example, contingency 15), while other contingencies are codified by a set of neurons that fire more evenly (for example, contingency 14). This can be seen more clearly by computing a Similarity Index (SI) for pairs of firing profiles (Fig <ref type="figure" target="#fig_9">9b</ref>). Most pairs have a small SI, and many contingencies are coded by unique sets of neurons. Therefore, the firing state of the K module together with the response executed conform a set of states that can be separated in two disjoint subsets when conditioned to reward, which allows the D module to map each firing state in module K to the correct motor response by means of plasticity rule described in Eq <ref type="bibr" target="#b23">(24)</ref>.</p><p>It is interesting to note that only half of the 16 contingencies are possible within blocks of trials under rule L 1 , being the other half only possible within the block of trials under rule L 2 . This implies that learning the contingencies could be subjected to a problem of catastrophic forgetting. However, this was seldom the case as can be seen from Fig <ref type="figure" target="#fig_9">9</ref>, at least for the protocol of 15-20 trials for each rule. To further explore this issue, we trained networks in a SRT during 10000 trials under protocols with blocks of crescent number of trials with the same rule, and computed the SI and average performance (Fig <ref type="figure" target="#fig_10">10a</ref> and<ref type="figure" target="#fig_10">10b</ref>). Performance dropped as quickly as the SI values went up, as trials per block were increased, reaching a plateau for the  ms, and is only changed after t decision , meaning that its coding demands the least memory and thus is expected to be the easiest to code, along with s(T). Coding of reward stimulus r(T -1) demands more memory from the system, but nevertheless is coded with similar proficiency to that of s(T) and R(T). On the other hand, the CP of s(T -1) grows following a sigmoid-shaped function that resembles the temporal dynamics of the synaptic weights within the K module. Within the contingency vector, s(T -1) is the first stimulus to be presented, and presumably the one having the strongest memory requirements. Moreover, it is followed by r(T -1), which could act as an interferent. The coding dynamics of s(T -1) is almost identical to the coding dynamics of the entire contingency vector, and also grows similar to the growth in behavioural performance (Fig <ref type="figure" target="#fig_6">6a</ref>), suggesting that coding s(T -1) is the bottleneck for contingency coding, and presumably for behavioural learning.</p><p>Results in Fig <ref type="figure" target="#fig_12">11a</ref> show that module K has enough memory to retain information for at least 50 ms. To further explore the memory capacity of the system, we tested the model that learned the SRT by simulating 2000 trials without plasticity. Trials were sorted according to their membership to each contingency and a Naive Bayes classifier was trained to classify trials according to their membership to a given contingency, based on the activity of the K neuron population at time points ranging from the start of s(T) to the end of s(T + 5) (300 ms of  The estimated firing probability of each neuron in module K computed at t decision , for each one of the 16 possible contingencies. Each row in the heat map represents the population firing profile p c for a given contingency C. It can be seen that firing profiles do not show significant overlapping. (b) Similarity index (SI) between pairs of contingencies firing profiles, which is inversely proportional to the 1-norm between firing profiles, and normalized to the interval between 0 (no similarity) and 1 (total similarity). In general the SI values are low. The highest SI was equal to 0.23, between contingencies 8 and 16, which only differs in their s(T -1). The second highest SI value was equal to 0.06, computed between contingencies 7 and 8, which only differs in s(T). There was a tendency for SI values to be high for pairs of contingencies that share the same s(T -1) or s(T). <ref type="url" target="https://doi.org/10.1371/journal.pone.0186959.g009">https://doi.org/10.1371/journal.pone.0186959.g009</ref>   the response in the contingency being analysed, explaining that the maximum global performance is found at t decision .</p><p>For good performance, information about the previous trial must be retained until t decision . We can see that the memory of the system far exceeds this minimum requirement, with a CP of 22% at t = 300 ms. Notably, CP values per contingency are clustered in two well defined The information conveyed by the K module about the contingencies was estimated by employing tree-bagger classifiers trained on the K module firing profile to classify trials according to their membership to a given group of contingencies that share some specific element, depicted in the legend. Probe simulations were run before beginning training (Trial = 0) and then every 1000 trials. Firing profiles where computed at t decision . Information about s(T -1) takes more training to be acquired, acting as a bottleneck for the coding of the whole contingency. (b) Memory about the occurrence of each contingency was estimated by assessing the classification performance of a Naive Bayes classifier trained to correctly classify the 16 contingencies based on the K module firing profile computed from t = 0 of trial T, to the end of trial T + 5 (being T the trial when s(T) of the target contingency was presented). The CP value picks around t decision as expected since the contingency may change after that time. For contingencies which involved the r 1 stimulus, information is retained above chance levels long after the time of decision. On the contrary, information about contingencies involving r 0 was retained for a shorter period, suggesting that information retention is proportional to the frequency of occurrence of the contingency. (c) When reward is delivered at random, differences in information retention between contingencies involving r 1 and r 0 disappears. <ref type="url" target="https://doi.org/10.1371/journal.pone.0186959.g011">https://doi.org/10.1371/journal.pone.0186959.g011</ref> groups that differ in how fast classification performance drops. The group of contingencies for which the system has shorter memory (CP drops fast) is composed of contingencies where r(T -1) = r 0 (red curves), while memory is longer for rewarded contingencies. It is important to note that r 0 is less and less presented as learning progresses, leading to an underrepresentation of contingencies containing r(T -1) = r 0 . This suggests that the number of times a given contingency is presented during training defines for how long the system retains information about that contingency. To test this hypothesis, we performed a new training in which both r 1 and r 0 have equal chances of been presented regardless of the chosen motor response. In this case, the CP of all contingencies followed a similar temporal course, as expected (Fig <ref type="figure" target="#fig_12">11c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this work we have studied under what conditions a biologically plausible neural network is capable of solving a serial reversal task. The distinctive feature of this paradigm is that each stimulus/response pairing is eventually reinforced, since correct responses depend on the current rule. Thus, the sole information about the perceived stimulus and executed response collected at any single point in time is not sufficient to solve the task. This problem is reminiscent of the problem of catastrophic forgetting, also called the stability/plasticity dilemma, which is usually stated as the difficulty that many neural network models have in acquiring new information without erasing old information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. Catastrophic forgetting studies usually focus on paradigms where a set of stimulus response pairings must be learned sequentially. Thus, the difficulty of the task strives in the distributed representation of stimuli in the neural network, where the same set of synaptic weights are modified each time a new pairing is presented. It has been shown that forgetting can be alleviated in models that incorporates different levels of plasticity, i.e. mataplasticity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Moreover, previously acquired information can be preserved in the correlated firings of the neural population <ref type="bibr" target="#b18">[19]</ref>. Thus, it might be reasonable to think that similar mechanisms could be at work in a behavioural paradigm like the SRT. However, the results presented in this work show that no plasticity rule or neural activation function is sufficient to guarantee good performance in the SRT without contradicting the hypothesis of functionality by learning. In particular, we showed that the SRT cannot be learned by any network in which the same neural population integrates stimuli information and at the same time defines the motor response through non-plastic connections.</p><p>It is assumed that learning occurs through neural mechanisms that drive the network to a configuration that solves the task. A prerequisite for learning is that the probability of sequences of stimuli and responses must be different when conditioned to reward than when conditioned to no reward; the non-fulfilment of this prerequisite means that reward delivery is not dependent on behaviour and there is nothing to be learned. Then, the network must achieve two properties: to differentially code in its states the sets of rewarded and nonrewarded sequences of stimulus/response pairings, and to map network states to the correct motor response. It is important to note that this last property (mapping) is only attainable after the first property (coding) is achieved. In the simple network, once coding is achieved the mapping is completely defined, since motor responses are pre-defined based on the activity of the integration/decision module K. But adequate mapping requires appropriate coding as a prerequisite, implying that simple networks will achieve mappings that allow high performance only by chance, which although not impossible, since we considered stochastic networks, is something that can hardly be regarded as learning. Moreover, the probability of finding a solution in this way would be very low, since the solution trajectories are only a small subset of all possible trajectories. For example, the module K ruled by Eqs <ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref> is capable of coding the 16 (s(T -1), r(T -1), R(T -1), s(T)) sequences. Let's consider 16 K neurons, half of them leading to R 1 and the other half leading to R 2 . We may assume that each neuron will code one of the 16 possible contingencies at random, since initial conditions were randomly chosen. Then, there are 8! x 8! out of 16! possible assignments between K i neurons and contingencies C j that lead to 100% of correct responses. This means that, by choosing an initial random condition, this K module will exhibit 100% performance with a probability of 7.8x10 -5 .</p><p>Building from the restrictions exhibited by the simple network scheme, we proposed a neural network model capable of solving the SRT, which relies in assigning the functions of contingency coding and response selection to different neural populations (integration module K and decision module D in Fig <ref type="figure" target="#fig_4">5a</ref>). In this way, all the information required to solve the SRT (i.e. the coding of the (cue,response,reward) contingencies) is firstly acquired in module K, and then the module D adapts its response through reinforcement learning in order to maximize reward. Besides the SRT, the model should perform well in any task that implies unpredictable changes of rules. Also, other related phenomena, like the overtraining reversal effect, could be recapitulated in the model by the addition of attentional mechanisms, such as reward-modulated stimulus gain <ref type="bibr" target="#b19">[20]</ref>.</p><p>It is interesting to note that, although the separation of functions achieved in the complex model allows to untie the problem generated by the reversal paradigm, the coding of the contingencies themselves implies a possible problem of catastrophic forgetting, because it is the same set of synaptic weights that is required to change to learn contingencies which are presented in a sequential schedule. Nevertheless, the soft winner-take-all network implemented as module K showed a remarkable resilience to forgetting. Although information of contingencies within a block of trials with the same rule could persist long enough into the other block, this is not likely, since the memory of the system declines considerably after 6 trials (Fig <ref type="figure" target="#fig_12">11b</ref>). A possible explanation for the resilience to forgetting could be found by noticing that the distribution of synaptic weights attained among neurons in module K is sparse (as shown in Fig 7c ), a fact that could decrease the chances of interfering representations <ref type="bibr" target="#b20">[21]</ref>.</p><p>The impossibility result shown here has special meaning for brain regions typically related to decision making like the prefrontal cortex (PFC). The PFC is key to several high level cognitive process such as behavioural plasticity <ref type="bibr" target="#b21">[22]</ref>, working memory <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, rule learning <ref type="bibr" target="#b25">[26]</ref> and decision making <ref type="bibr" target="#b26">[27]</ref>. Experiments involving brain lesions have shown that different sub regions within the PFC and the striatum are differentially involved in the SRT. In particular, it has been found that the orbitofrontal cortex (OFC), the medial PFC, and the medial and dorsomedial striatum are required for learning a SRT <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. In most cases, lesions of the involved areas led to slower learning of the SRT, with a higher rate of perseverative errors. In our model, perseverative errors occur if module K codifies stimuli but does not have enough memory to codify cues, reward and responses taking place in the previous trial. Since the coding capacity in module K stems from the competitive dynamics between neurons that occurs through inhibition, a failure in the inhibitory system would harm coding capacity of module K, leading to perseverative errors. This is consistent with <ref type="bibr" target="#b28">[29]</ref> in which mutant mice with deficit in frontal cortical inhibitory neurons showed more perseverative errors, and impaired learning in the SRT.</p><p>The experimental results enumerated before, together with our theoretical results suggest that, in order to understand the neural mechanisms required for solving the SRT, and the IRTs in general, it would be of great value to characterize subpopulations of neurons according to their afferent and efferent projections and in relation to their firing profile. It could be expected for example, that the PFC neurons could be sorted in populations of coding neurons, that code complex contexts and stimuli histories, and decision neurons, that integrate contingency information from the coding population and projects to motor structures like the dorsal striatum, or the motor cortices. Another interesting possibility is that simple and complex networks coexist within different brain regions, for example as circuits spanning the PFC and the basal ganglia. If this two kind of networks are somehow segregated, then a specific brain lesion could damage more complex networks than simple networks. The damage in complex networks would hamper learning in tasks like the SRT, while the remaining simple networks would still be capable of solving other non-IRT tasks like a simple discrimination, or a delayed matching-to-sample task.</p><p>Synaptic plasticity in the model depicted in Fig 5a fulfils two different functions. In module K, plasticity allows the system to classify stimuli contingencies. The modulation of plasticity by reward would make no difference there, since all contingencies are equally rewarded, at least during the beginning of learning. Evidence of sustained plasticity have been found experimentally, in the form of the continuous formation and erasure of synaptic spines in cortex, which occurs even in the absence of any obvious reward <ref type="bibr" target="#b29">[30]</ref>. On the other hand, plasticity between module K and module D has the function of allowing the D module to read the firing of K neurons that carries contingency information, and to map it with the correct response. In this case a reward-modulated form of synaptic plasticity is essential, and related experimental evidence can be found in the known effects that the neuromodulator dopamine (DA) has on synaptic plasticity in brain regions like the cerebral cortex <ref type="bibr" target="#b30">[31]</ref>, hippocampus <ref type="bibr" target="#b31">[32]</ref> and striatum <ref type="bibr" target="#b32">[33]</ref>, and in the fact that DA neurons code reward and reward-predicting cues <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. This fundamental difference in plasticity modes in the model suggests that experimental approaches to understand neural computation should focus on searching for subpopulations based in their synaptic plasticity profile, dissecting populations of neurons according to how sensitive their synaptic changes are to neuromodulators related to reward. Understanding the relationship between connectivity, firing profile, and reward and non-reward modulated plasticity could help to discover the building blocks of neural computation.</p><p>In brief, the study of a well-known task as the SRT allowed to gain new insights into the computational limits of an important set of biological neural networks that are commonly considered as models of learning and decision-making, and to give new theoretical support to the experimental exploration of the anatomy and function of neural circuits. Future work should focus on the rules of connectivity that allows greater memory for coding more complex contingencies, and in the kind of algorithms that can be learned by combining different circuit motifs with reward and non-reward modulated plasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Proofs for the impossibility of simple neural networks to learn to solve the SRT Achieving high performance in the SRT implies that the network responds to stimuli according to the transitions depicted in Fig 2 . The behaviour of the network will be inherently stochastic, since it is required to respond to stimuli that are themselves stochastic. However, given the state of the network at time t, the transition probability for the correct response is expected to be close to one, with all other responses having transition probabilities close to zero. Without the stochasticity of the stimuli, the network would follow a deterministic limit cycle, in which n(t) = n(t + m), being m the length of the cycle. In this manner, we say that the transition probability matrix is a deterministic probability matrix, and that the network follows a stochastic limit cycle, where the stochastic component of the behaviour is given by external factors that do not depend on the activity of the network.</p><p>With this concepts in mind, we will prove that the reduced neural network cannot learn to solve the SRT by showing that, given any excitation function f and plasticity function g, the network either does not converge, or it converges to one of many possible stochastic limit cycles, where only a small subset of these limit cycles allows high performance in the SRT.</p><p>First, we will study the convergence properties of the reduced neural network, assuming that external stimuli are not stochastic. We build on the mathematical framework of decision systems as presented in <ref type="bibr" target="#b35">[36]</ref>. There, a decision system is defined, which is composed of a state space X, a decision space D, and transition probabilities p i (x) ≔ p(i|x) and p i (x, A) ≔ p(x ∊ A| x,i), where x ∊ X, i ∊ D, and A is any element of the sigma algebra on X. At each time t, a decision i is taken given x(t) and p i (x), obtaining i(t + 1). Then, x(t + 1) is obtained, conditioned to i(t +1) and x(t) through P i (x(t), x(t + 1)).</p><p>The evolution of a stochastic spiking network can be represented within this framework by the following representation: D = {(z i , z j )}, the set of all possible pairs of firing network states the network can assume. Vectors z i is the ith vector of the set of all possible firing state vectors z X = {(w, z, rew)}, the set of all possible combinations of whole system synaptic weights configurations (w), networks firing states (z) and reward function rew.</p><formula xml:id="formula_8">p i ðxÞ ¼ pððz i ; z j Þjðw m ; z q ; rewÞÞ ¼ Fðw m ; z i ; z j Þ q ¼ i 0 q 6 ¼ i ; ( P i ðx; x 0 Þ ¼ pðw l ; z k ; rew 0 jðz o ; z v Þ; ðw m ; z q ; rewÞÞ ¼ 1 z o ¼ z q ; z v ¼ z k ; w l is reachable 0 otherwise ; (</formula><p>where w i is the ith vector of the set of all possible synaptic weight configurations for the whole network. By reachable we mean that w l is the whole synaptic configuration that is obtained when applying plasticity function g after transition from z o to z v , having rew the value corresponding to that trial given z v , s and L. Theorem 1 in <ref type="bibr" target="#b35">[36]</ref> shows that a decision system converges with probability 1 to a limit cycle if and only if for each state x there is a decision i such that:</p><formula xml:id="formula_9">φ i ðxÞ ¼ lim n!1 Q n i ðx; XÞ ! c &gt; 0;<label>ð7Þ</label></formula><p>where c is a constant and <ref type="bibr" target="#b6">(7)</ref> is fulfilled only when the probability of transitioning from firing state z i to z j infinitely often does not vanish, which happens only if the probability converges to 1.</p><formula xml:id="formula_10">Q n i is defined inductively as Q nþ1 i ðx; AÞ ¼ ð x 0 2X Q i ðx 0 ; AÞQ n i ðx; dx 0 Þ, with Q 1 i ðx; AÞ ¼ Q i ðx; AÞ ¼ p i ðxÞP i ðx; AÞ. Intuitively, condition</formula><p>In the case of a reduced network</p><formula xml:id="formula_11">Fðw; z i ; z j Þ ¼ f ðw l z i Þ X N K m ðf ðw m z i ÞÞ ;<label>ð8Þ</label></formula><p>where l is the K neuron that is active in state j. The function f is any function with the condition that is strictly increasing with w ∊ R.</p><p>It is the fact that synaptic weights change deterministically the reason why P i (x, x 0 ) is either 1 or 0. This allows us to simplify condition <ref type="bibr" target="#b6">(7)</ref> to:</p><formula xml:id="formula_12">φ l;z;rew ¼ Y n!1 f ðT n m;q;rew ðw l ÞzÞ X N K j f ðT n m;q;rew ðw j ÞzÞ ! c &gt; 0;<label>ð9Þ</label></formula><p>where l is the active neuron in the destination state j, z is the source state,</p><formula xml:id="formula_13">T m;q;rew ðw l Þ ¼ T 1 m;q;rew ðw l Þ ¼ v : v k ¼ w l;k ; k = 2 fm; qg; v k ¼ w l;k þ</formula><p>gðz l ; z k ; rewÞ; k 2 fm; qg, and T n m;q;rew ðw l Þ ¼ T nÀ 1 m;q;rew ðw l Þ. The transformation T takes the vector of synaptic weights of inputs to neuron l and applies a synaptic change according to plasticity function g (Eq 2) to the weights corresponding to presynaptic neurons q and m (one for a neuron Y, the other for a neuron K) that are active, i.e. z m = z q = 1.</p><p>Eq <ref type="bibr" target="#b8">(9)</ref> holds only if logφ l,z,rew converges, which is an infinite sum of logarithms. In turn, the sum converges if the application of T leads to an increase in the transition probability. Since f is strictly increasing with w, Eq (9) holds if δ 1,1,rew &gt; 0. In other words, the network will converge to a limit cycle if for each pair of active neurons Y and K there is a transition to a neuron n l such that, if the transition is repeated infinite times, the probability of the transition increases, something that occurs if the pre/post activation leads to potentiation of the synapse, i.e. Hebbian plasticity.</p><p>As stated before, a neural network that must learn to solve the SRT will not reach a limit cycle since it is bonded to follow stimuli that are stochastic. However, the evolution of the network can be segmented in transitions that eventually reach probability 1. Namely, for states defined as (s, n i , L) and (r, n i , L), we can consider transitions conditioned to a given s and L, i.e. the external stochastic factors which are independent of the network behaviour. For example, the transition between a given source state (s, n i , L) and destination states (r, n j , L) for any neuron n j , can be considered a decision system. Then, if condition (9) is fulfilled, any of these decision systems will converge to a "limit cycle" in which only one destination state (r, n j , L) is chosen. The same holds for transitions between source state (r, n i , L) and destination states (s, n j , L 0 ). It is important to note that a neural network that solves the SRT needs to converge to a unique decision even for incorrect trials, i.e. for source states (r 0 , n, L). This means that g(n i , n j , rew) &gt; 0 for any rew ∊ {1, 0}.</p><p>Any pair of source and destination states can become the limiting transition, the probability of this happening depending on the initial transition probability, which depends on the initial synaptic weights. In particular, for networks in which Eq (9) holds, any limiting transition is attracting since the transition probability rises with probability equal to itself. The SRT is solved with high performance for only a small subset of all the possible limiting transitions. Therefore, a simple network which is initialized with random synaptic weights will reach a synaptic configuration that solves the SRT with very low probability. In particular, the probability of reaching the solution will be high only if the initial transition probabilities are close to the solution probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A more general definition for the simple neural network</head><p>The reduced neural network can be extended to a more general definition of simple network, with arbitrary number of neurons and for which the impossibility result holds. In this case, the networks dynamics develops in discrete time steps of 1 ms. The SRT is structured in trials composed of cue stimulus presentation followed by a reward stimulus presentation, each one lasting t stimulus in ms. The response is observed in the interval [t cue offset -Δt response , t cue offset ]. The sequence of firing states of module K during this time interval univocally defines the behavioural response R.</p><p>We will consider a simple neural network composed of N Y neurons in module Y and N K neurons in module K. The firing state of the ith neuron in module Y will be represented by the variable y i , the ith element of vector y. The firing state of the ith neuron in module K will represented by the variable n i , the ith element of vector n. Neurons in module Y fire independently of each other, conditioned to the stimulus presented:</p><formula xml:id="formula_14">pðyjSðiÞÞ ¼ Y i2I 1 y pðy i jSðiÞÞ Á Y j2I 0 y ð1 À pðy j jSðiÞÞÞ;<label>ð10Þ</label></formula><p>where I 1 y is the set of indexes of Y neurons that are active in vector y, and I 0 y is the set of indexes of Y neurons that are inactive in vector y.</p><p>The postsynaptic potential PP i,j elicited by the train of spikes of neuron j onto neuron i is defined as the product of the post synaptic potential time course x j and the corresponding synaptic weight: PP i;j ðtÞ ¼ x j ðtÞw i;j ðtÞ:</p><p>The variable x j , the postsynaptic potential time course associated with the spike train of neuron j, is defined as:</p><formula xml:id="formula_16">x j ðtÞ ¼ X t 0 ðt À t 0 Þ;<label>ð12Þ</label></formula><p>where ∊ is a kernel function, and t 0 runs over all the firing times up to time t at which the jth neuron of the module fired. The excitability of neuron i in module K is defined as:</p><formula xml:id="formula_17">u i ðtÞ ¼ X j f ðPP i;j Þ:<label>ð13Þ</label></formula><p>Conversely, its probability of firing is:</p><formula xml:id="formula_18">pðn i ðtÞjPP i ðt À 1ÞÞ ¼ Fðu i ðtÞÞ;<label>ð14Þ</label></formula><p>where sign(f(PP i,j )) = sign(PP i,j ), lim</p><formula xml:id="formula_19">w i;j !w max f ðw ij Þ ¼ u þ and lim w i;j !À w max f ðw i jÞ ¼ u À . The function F is such that lim u!u þ FðuÞ ¼ 1, lim u!u À FðuÞ ¼ 0, F(u) = 1 for u ! u + and F(u) = 0 for u u -.</formula><p>In this way, neuron i will fire with probability 1 with the sole firing of a neuron j, provided that w ij is maximal, and will remain silent with probability 1 if w ij is inhibitory (negative) and maximal in absolute value. Any number of neurons may fire at the same time, and all neurons are conditionally independent of each other given PP. Thus, the probability of an activation state n(t) of the whole module K is given by: </p><p>where I 1 nðtÞ and I 0 nðtÞ are sets of indexes of neurons that are respectively active and inactive in n (t).</p><p>Neurons are plastic all the time. Synaptic weight w i,j changes according to the function g, defined as: Dw i;j ðtÞ ¼ gðz i ðtÞ; PP i;j ðtÞ; w i;j ðtÞ; rewðtÞÞ: ð16Þ</p><p>The Δw i,j values depend on w i,j in such a way that lim n!1 T n z i ;PP i;j ;rew ðw i;j Þ &lt; jw max j. This assures that synaptic weights remain within reasonable prefixed limits. In this case, the variable rew is defined as:</p><formula xml:id="formula_21">rewðtÞ ¼ X t 0 gðt À t 0 Þ<label>ð17Þ</label></formula><p>with γ a kernel function and t 0 the onset times of stimulus r 1 .</p><p>For a simple neural network defined according to Eqs <ref type="bibr" target="#b9">(10)</ref> to <ref type="bibr" target="#b16">(17)</ref>, the impossibility result holds. In particular, since the plasticity rule is deterministic, transitions with probability one will be possible if the corresponding value of Δw i,j is positive. In this case, all the variability in the network will stem from the stochastic nature of stimuli presentation and rule switching, and from the uncertainty in the coding of stimuli by the sensory module Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of the complex network</head><p>In the implementation of the complex network sketched in Fig 5a, module Y was composed of two neurons for coding each cue stimulus, two neurons for each reward stimulus, and one neuron for coding each response. Module K was composed of N K = 150 neurons. All initial synaptic weights were sampled from a normal distribution of mean = 0 and standard deviation = 1/64. There were no self-connections (w i,j = 0).</p><p>Each neuron i in module K has a variable u i : </p><p>where w iy is a vector containing the synaptic weights for the connection from each neuron in the module Y to the ith neuron in module K, while w iK is an analogous vector for the inputs that neuron i receives from the other neurons in module K. The vector products w i,j (t)x y (t) and w i,K (t)x K (t) represent the postsynaptic potentials (PP) at time t associated with the train of spikes at each afferent synapse from module Y and K, respectively. The ith element any vector x represents the temporal course of the PP, which only depends on the spike emission times, and is defined as:</p><formula xml:id="formula_23">xðtÞ ¼ X t 0 ðt À t 0 Þ;<label>ð19Þ</label></formula><p>where t 0 runs over all the firing times up to time t at which the ith neuron of the module fired, and ∊ is a double exponential kernel function:</p><formula xml:id="formula_24">ðtÞ ¼ ðe À t t 1 À e À t t 2 Þ Á Y;<label>ð20Þ</label></formula><p>where τ 1 = 2 ms, τ 2 = 20 ms, and Θ stands for the Heaviside function. The parameter b i controls the excitability of the neuron. This parameter was adjusted at each time t following the homeostatic mechanism described in Habenschuss et al. <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_25">Db i ðtÞ ¼ m 1 N K À 1 n i ðtÞ ¼ 1 m 1 N K n i ðtÞ ¼ 0 ; ð21Þ 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :</formula><p>which assures that each neuron in the module fires with equal probability, helping to exploit all neurons in the module, avoiding silent neurons and thus favouring learning. The parameter μ was set to 0.1.</p><p>The firing probability of neurons in the Y module where defined by the stimulus they coded, such that pðy 1</p><p>x i jx i Þ ¼ pðy 2 x i jx i Þ ¼ 0:95 and pðy 1 x i jx j Þ ¼ pðy 2 x i jx j Þ ¼ 0; 8i 6 ¼ j, where y q x i is the qth Y neuron coding stimulus x i . The response executed was coded by one Y neuron each, such that pðy R</p><formula xml:id="formula_26">1 jR 1 Þ ¼ pðy R 2 jR 2 Þ ¼ 1, and pðy R 1 jR 2 Þ ¼ pðy R 2 jR 1 Þ ¼ 0.</formula><p>Within module K, the firing probability of neuron i is defined as: pðn i ðtÞ ¼ 1Þ ¼ e u i ðtÞ X j e u j ðtÞ ; ð22Þ with index j going through all neurons in module K.</p><p>The firing probability of the two neurons in module D are defined just as for neurons in module K, with the sum in Eq <ref type="bibr" target="#b21">(22)</ref> encompassing only the two D neurons. Only one neuron in module K and module D fires at each time t.</p><p>Connections from module Y to module K, from module K to module D, and between neurons in module K are plastic. The connections from neurons Y to neurons K and between neurons in module K change at each time t according to Δw ij : Dw ij ðtÞ ¼ ðe À w ij ðtÞx j ðtÞ À 1Þa 1 ; ð23Þ</p><p>where index i refers to the postsynaptic neuron, index j to the presynaptic neuron, x is the time course of the postsynaptic potential associated with neuron j, and α 1 = 5x10 -4 is a learning constant. This plasticity rule is a kind of STDP rule that leads the model to codify each stimulus by a different population of neurons. Note that the rule does not depend on reward, and weight changes are applied at each time t. Connections from module K to module D change over time according to:</p><p>Dw ij ðtÞ ¼ ðd i ðtÞ À u i ðtÞÞx j ðtÞrewðtÞa 2 ; ð24Þ</p><p>where d i stands for the firing state of decision neuron i, u i is its excitability variable, x j the PP time course of afferent neuron j and α 2 = 8x10 -4 is a learning constant. The variable rew equals 1 only during the decision window and only if the motor response was correct. Otherwise, rew = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulations and analysis</head><p>A training session in the SRT consisted of 10000 trials, while a test session consisted of 2000 trials. For the results in Fig <ref type="figure" target="#fig_10">10</ref>, one network per point in the plot was trained during 10000 trials. Each of these trainings had a specific (fixed) number of trials per block with the same rule, starting from 20 trials per block and increasing the number by factors of powers of 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 .</head><label>1</label><figDesc>Fig 1. Serial Reversal protocol and simple network connectivity. (a) Each trial is composed of a cue stimulus presentation, during which the behavioural response must be executed, and a reward stimulus presentation. Correct responses depend on the stimulus presented and the current rule, which changes with probability p switch . (b) Diagram representing the general connectivity of the simple network.Neurons in module Y codify both cue and reward stimuli, and projects to the K module. K neurons connect with each other and project to one of the two response neurons. Therefore, K neurons can be sorted in two halves depending on whether they project to neuron R 1 (K R 1 neurons) or neuron R 2 (K R 2 neurons). Firing of any K neuron elicits their target R neuron to fire. Connections between module K and module R are assumed to be hardwired prior to any learning, such that firing in module K completely defines the executed response. (c) An example sequence of 3 trials of the SRT for the model depicted in (b) prior to learning, with a minimal K module composed of 8 neurons in which K R 1 ¼ fn 1 ; n 2 ; n 3 ; n 4 g and K R 2 ¼ fn 5 ; n 6 ; n 7 ; n 8 g. The current rule is L 1 .https://doi.org/10.1371/journal.pone.0186959.g001</figDesc><graphic coords="3,95.98,78.01,479.91,307.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 .</head><label>2</label><figDesc>Fig 2. Graph representing the transition probabilities of the Markov chain associated with the simple network of Fig 1 solving the SRT. The active Y and R neurons are excluded from the global state to simplify the representation, since Y neurons are entirely defined by the stimulus, and R neurons are entirely defined by the active K neuron. The size of the arrow head represents the magnitude of the transition probability. Dashed lines depict transitions for which a change of rule occurs. Transitions that have no arrow are considered to</figDesc><graphic coords="6,200.01,78.01,330.86,575.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. Specificity of responses in module K given the firing of presynaptic neurons. The matrices show the probabilities of postsynaptic K neurons being active at time t given the state of the presynaptic neurons in module Y and module K at time t -1. Probability magnitudes are consistent with the Markov chain of Fig 2.This representation gives a hint about how the synaptic weights ought to be. High transition probabilities can be achieved by setting high synaptic weights between a given presynaptic pair and the target postsynaptic neuron, and low synaptic weights for all other postsynaptic neurons. https://doi.org/10.1371/journal.pone.0186959.g003</figDesc><graphic coords="7,200.01,239.24,298.49,386.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. Synaptic weights between neurons of modules Y and K. (a) Synaptic weights configuration that allows the model to solve the SRT, consistent with the transition probabilities shown in Fig 2. It can be seen that a specific arrangement of synaptic weights are required. (b) Synaptic weights configuration that allows the model to solve a DT. In contrast with the SRT, all high synaptic weights correspond to prepost synaptic neurons that are systematically active when reward is obtained. Neurons L 1 and L 2 codify the stimuli that signal which rule is current in a given trial of the DT. https://doi.org/10.1371/journal.pone.0186959.g004</figDesc><graphic coords="8,95.98,78.01,479.91,204.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 5 .</head><label>5</label><figDesc>Fig 5. Serial Reversal protocol and complex network connectivity. (a) Diagram representing the general connectivity of the complex network. Each cue and reward stimulus is coded by the Y neuron population, like in the simple network.Besides, the executed motor response gives sensory feedback, such that each response is also coded by module Y. Module Y connects to all neurons in the integration module K, which in turn connect with each other and with each neuron in the decision module D. Each neuron D is hardwired to one neuron R, so that the response executed is entirely defined by the D module. Synapses between module Y and module K, and within module K are plastic, subject to plasticity rule defined in Eq<ref type="bibr" target="#b22">(23)</ref>, which is applied at all times and is not dependent on reward. Synapses between module K and module D are plastic, subject to plasticity rule defined in Eq<ref type="bibr" target="#b23">(24)</ref>, which depends on reward. (b) Serial reversal protocol for training the network depicted in (a). Stimuli are presented for 25 ms, and the motor response to be executed is chosen at t decision = 15 ms from cue onset. Plasticity between K and D neurons is applied only if there was reward and within a window spanning from t decision to the end of cue presentation.</figDesc><graphic coords="10,95.98,78.01,479.91,259.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig 6a. After 10000 trials of training, the model is capable of changing strategies in the trial immediately following rule reversal (Fig 6b). The dynamics of synaptic weights along training depends on each kind of connection (Fig 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 6 .</head><label>6</label><figDesc>Fig 6. The complex neural network learns to solve the SRT. (a) Performance of the model during training, computed as percentage of correct responses in non-overlapping windows of 100 trials. Reversals during training occurred every 15-20 trials. (b) The trained model was tested without further plasticity in 2000 trials, with reversals every 20 trials, and performance was computed for each trial, aligning from the trial where the reversal took place. Performance is low immediately after reversal, but improves quickly. In both panels, mean ± std is plotted, for N = 10 network initializations). https://doi.org/10.1371/journal.pone.0186959.g006</figDesc><graphic coords="11,95.98,72.57,479.91,216.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 7 .</head><label>7</label><figDesc>Fig 7. Evolution of synaptic weights of a complex network along training. Synaptic weights as they evolve during training are shown, together with the synaptic weights distribution at the end of training. (a) Weight distribution for Y ! K connections is bimodal, with large values appearing early during training. (b-c) Synaptic weights for R ! K and K ! K connections follow a strongly skewed distribution. (d) Connections between module K and module D follow a symmetric distribution around zero. https://doi.org/10.1371/journal.pone.0186959.g007</figDesc><graphic coords="12,95.98,78.01,479.91,388.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 8 .</head><label>8</label><figDesc>Fig 8. Emergence of sequential firing in the K module. Spiking activity (a) and corresponding postsynaptic potential time courses (b) of the complex network during 4 consecutive trials of the SRT after achieving high performance. Neurons in the K module fire in sequences of sustained bursts of activity. Postsynaptic potentials allow each spike to have an influence tens of milliseconds after their emission, linking the neurons activity across different stimuli presentations. Note that neurons in the D module change their activity after stimulus onset and short before t decision . Rule L 2 was current along the four trials. Colour bars are in arbitrary units. https://doi.org/10.1371/journal.pone.0186959.g008</figDesc><graphic coords="13,95.98,78.01,479.91,327.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 9 .</head><label>9</label><figDesc>Fig 9. Population coding of stimuli contingencies in module K. (a)The estimated firing probability of each neuron in module K computed at t decision , for each one of the 16 possible contingencies. Each row in the heat map represents the population firing profile p c for a given contingency C. It can be seen that firing profiles do not show significant overlapping. (b) Similarity index (SI) between pairs of contingencies firing profiles, which is inversely proportional to the 1-norm between firing profiles, and normalized to the interval between 0 (no similarity) and 1 (total similarity). In general the SI values are low. The highest SI was equal to 0.23, between contingencies 8 and 16, which only differs in their s(T -1). The second highest SI value was equal to 0.06, computed between contingencies 7 and 8, which only differs in s(T). There was a tendency for SI values to be high for pairs of contingencies that share the same s(T -1) or s(T).</figDesc><graphic coords="14,95.98,78.01,479.91,184.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 10 .</head><label>10</label><figDesc>Fig 10. Effect of trials per block on model performance. Networks were trained in the SRT during 10000 trials, and average SI (a) and performance (b) were computed in 2000 trials without plasticity. Each point in the plot belongs to one network trained with the number of trials per block specified in the x axis. Average SI values were computed from the SI values between pairs of contingencies with shared s(T) or s(T -1), which are the contingencies with the highest SI, as shown in Fig 9.</figDesc><graphic coords="14,95.98,468.28,479.91,179.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>https://doi.org/10.1371/journal.pone.0186959.g010</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 11 .</head><label>11</label><figDesc>Fig 11. Contingency coding and memory after training. (a)The information conveyed by the K module about the contingencies was estimated by employing tree-bagger classifiers trained on the K module firing profile to classify trials according to their membership to a given group of contingencies that share some specific element, depicted in the legend. Probe simulations were run before beginning training (Trial = 0) and then every 1000 trials. Firing profiles where computed at t decision . Information about s(T -1) takes more training to be acquired, acting as a bottleneck for the coding of the whole contingency. (b) Memory about the occurrence of each contingency was estimated by assessing the classification performance of a Naive Bayes classifier trained to correctly classify the 16 contingencies based on the K module firing profile computed from t = 0 of trial T, to the end of trial T + 5 (being T the trial when s(T) of the target contingency was presented). The CP value picks around t decision as expected since the contingency may change after that time. For contingencies which involved the r 1 stimulus, information is retained above chance levels long after the time of decision. On the contrary, information about contingencies involving r 0 was retained for a shorter period, suggesting that information retention is proportional to the frequency of occurrence of the contingency. (c) When reward is delivered at random, differences in information retention between contingencies involving r 1 and r 0 disappears.</figDesc><graphic coords="15,95.98,78.01,479.91,418.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>j ðtÞjPP j ðtÞÞÞ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>u i ðtÞ ¼ w i;y ðtÞx YðtÞ ðtÞ þ w i;K ðtÞx K ðtÞ þ b i ðtÞ;</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS ONE | https://doi.org/10.1371/journal.pone.0186959October 27, 2017  </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Sergio Lew</rs> for helpful comments.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported in part by projects <rs type="grantNumber">PICT 2012-1519</rs> (<rs type="affiliation">Agencia Nacional de Promocio ´n Cientı ´fica y Tecnolo ´gica</rs>, www.agencia. mincyt.gob.ar) and <rs type="grantNumber">PIP 112 201101 01054</rs> (<rs type="projectName">Concejo Nacional de Investigaciones Cientı ´ficas y Te</rs> ´cnicas, www.conicet.gov.ar), granted to BSZ. CJM is supported by a <rs type="funder">CONICET postdoctoral fellowship</rs>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_j6J7YJr">
					<idno type="grant-number">PICT 2012-1519</idno>
				</org>
				<org type="funded-project" xml:id="_Y8pmg4U">
					<idno type="grant-number">PIP 112 201101 01054</idno>
					<orgName type="project" subtype="full">Concejo Nacional de Investigaciones Cientı ´ficas y Te</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All relevant data are within the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The similarity index employed in Fig 9b was defined as:</p><p>where p c is a vector in which the ith element is the estimated probability of firing of neuron i conditioned to contingency C. The SI adopts values from 0 (when firing probabilities under both contingencies are equal for each neuron) to 1 (when every neuron fire with probability 1 under one contingency, and with probability 0 under the other contingency.</p><p>We employed classifiers to obtain a measure of the information conveyed by the neuron population of module K about contingencies. Specifically, for the result shown in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Serial reversal learning in bumblebees (Bombus impatiens)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Sherry</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10071-013-0704-1</idno>
		<idno type="PMID">24218120</idno>
		<ptr target="https://doi.org/10.1007/s10071-013-0704-1" />
	</analytic>
	<monogr>
		<title level="j">Anim Cogn</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="723" to="734" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Successive Olfactory Reversal Learning in Honeybees</title>
		<author>
			<persName><forename type="first">B</forename><surname>Komischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giurfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lachnit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malun</surname></persName>
		</author>
		<idno type="DOI">10.1101/lm.44602</idno>
		<idno type="PMID">12075000</idno>
		<ptr target="https://doi.org/10.1101/lm.44602" />
	</analytic>
	<monogr>
		<title level="j">Learn Mem</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors underlying improvement in serial reversal learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mackintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgonigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Holgate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanderver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can J Psychol Can Psychol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="1968">1968</date>
			<publisher>University of Toronto Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The neural basis of reversal learning: An updated perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Brigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Rudebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroscience.2016.03.021</idno>
		<idno type="PMID">26979052</idno>
		<ptr target="https://doi.org/10.1016/j.neuroscience.2016.03.021" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience. IBRO</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="12" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
		<idno type="PMID">10322466</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inactivation of the medial prefrontal cortex of the rat impairs strategy set-shifting, but not reversal learning, using a novel, automated procedure</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Floresco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mtl</forename><surname>Tse</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbr.2008.02.008</idno>
		<idno type="PMID">18359099</idno>
		<ptr target="https://doi.org/10.1016/j.bbr.2008.02.008" />
	</analytic>
	<monogr>
		<title level="j">Behav Brain Res</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orbital prefrontal cortex mediates reversal learning and not attentional set shifting in the rat</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mcalonan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbr.2003.09.019</idno>
		<idno type="PMID">14643463</idno>
		<ptr target="https://doi.org/10.1016/j.bbr.2003.09.019" />
	</analytic>
	<monogr>
		<title level="j">Behav Brain Res</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selective lesions of the dorsomedial striatum impair serial spatial reversal learning in rats</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castañe ´anna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deh</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbr.2010.02.017</idno>
		<idno type="PMID">20153781</idno>
		<ptr target="https://doi.org/10.1016/j.bbr" />
	</analytic>
	<monogr>
		<title level="j">Behav Brain Res</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lesions of the Medial Striatum in Monkeys Produce Perseverative Impairments during Reversal Learning Similar to Those Produced by Lesions of the Orbitofrontal Cortex</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1521-08.2008</idno>
		<idno type="PMID">18945905</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.1521-08.2008" />
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="10972" to="10982" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neocortical Network Activity In Vivo Is Generated through a Dynamic Balance of Excitation and Inhibition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haider</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.5297-05.2006</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.5297-05.2006PMID" />
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">16641233</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spike Timing-Dependent Plasticity: From Synapse to Perception</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">Y</forename></persName>
		</author>
		<idno type="DOI">10.1152/physrev.00030.2005</idno>
		<idno type="PMID">16816145</idno>
		<ptr target="https://doi.org/10.1152/physrev.00030.2005" />
	</analytic>
	<monogr>
		<title level="j">Physiol Rev</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1033" to="1048" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003037</idno>
		<idno type="PMID">23633941</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1003037" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reward-based stochastic self-configuration of neural circuits</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Habenschuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">STDP Installs in Winner-Take-All Circuits an Online Approximation to Hidden Markov Model Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003511</idno>
		<idno type="PMID">24675787</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1003511" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent Spiking Networks Solve Planning Tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kappel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tanneberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pecevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep21142</idno>
		<idno type="PMID">26888174</idno>
		<ptr target="https://doi.org/10.1038/srep21142" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Sci Rep. Nature Publishing Group</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Memory retention-The synaptic stability versus plasticity dilemma</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robins</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tins.2004.12.003</idno>
		<idno type="PMID">15667929</idno>
		<ptr target="https://doi.org/10.1016/j.tins.2004.12.003" />
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Limits on the memory storage capacity of bounded synapses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1859</idno>
		<idno type="PMID">17351638</idno>
		<ptr target="https://doi.org/10.1038/nn1859" />
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="485" to="493" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascade models of synaptically stored memories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2005.02.001</idno>
		<idno type="PMID">15721245</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2005.02.001" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="599" to="611" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long-Term Memory Stabilized by Noise-Induced Rehearsal</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Koulakov</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3929-12.2014</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3929-12.2014PMID" />
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">25411507</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reward-dependent gain and bias of visual responses in primate superior colliculus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hikosaka</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0896-6273(03)00464-1</idno>
		<idno type="PMID">12925282</idno>
		<ptr target="https://doi.org/10.1016/S0896-6273(03)00464-1" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="693" to="700" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Medial Frontal Cortex Mediates Perceptual Attentional Set Shifting in the Rat</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<idno type="PMID">10818167</idno>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4320" to="4324" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mnemonic coding of visual space in the monkeys dorsolateral prefrontal cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Goldman-Rakic</surname></persName>
		</author>
		<idno type="PMID">2918358</idno>
	</analytic>
	<monogr>
		<title level="j">J Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cellular basis of working memory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Goldman-Rakic</surname></persName>
		</author>
		<idno type="PMID">7695894</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="477" to="485" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Medial prefrontal activity during delay period contributes to learning of a working memory task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1256573</idno>
		<idno type="PMID">25342800</idno>
		<ptr target="https://doi.org/10.1126/science" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="458" to="463" />
			<date type="published" when="2014">2014. 1256573</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single neurons in prefrontal cortex encode abstract rules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1038/35082081</idno>
		<idno type="PMID">11418860</idno>
		<ptr target="https://doi.org/10.1038/35082081" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="page" from="953" to="956" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The role of medial prefrontal cortex in memory and decision making</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Euston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2012.12.002</idno>
		<idno type="PMID">23259943</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2012.12.002" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1057" to="1070" />
			<date type="published" when="2012">2012</date>
			<publisher>Elsevier Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dissociable roles of the medial prefrontal cortex, the anterior cingulate cortex, and the hippocampus in behavioural flexibility revealed by serial reversal of three-choice discrimination in rats</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kosaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbr.2011.10.039</idno>
		<idno type="PMID">22061799</idno>
		<ptr target="https://doi.org/10.1016/j.bbr.2011.10.039" />
	</analytic>
	<monogr>
		<title level="j">Behav Brain Res</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2012">2012</date>
			<publisher>Elsevier B.V</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interneurons are necessary for coordinated activity during reversal learning in orbitofrontal cortex</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Bissonette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Powell</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsych.2014.07.023</idno>
		<idno type="PMID">25193243</idno>
		<ptr target="https://doi.org/10.1016/j.biopsych.2014.07.023" />
	</analytic>
	<monogr>
		<title level="j">Biol Psychiatry</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="454" to="464" />
			<date type="published" when="2015">2015</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-term in vivo imaging of experience-dependent synaptic plasticity in adult cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Trachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Welker</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature01273</idno>
		<idno type="PMID">12490942</idno>
		<ptr target="https://doi.org/10.1038/nature01273" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">420</biblScope>
			<biblScope unit="page" from="788" to="794" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dopamine in Motor Cortex Is Necessary for Skill Learning and Synaptic Plasticity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hertler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schubring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Molina-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pekanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Luft</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0007082</idno>
		<idno type="PMID">19759902</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0007082" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dopamine Regulates Aversive Contextual Learning and Associated In Vivo Synaptic Plasticity in the Hippocampus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Broussard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsetsenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.celrep.2016.01.070</idno>
		<idno type="PMID">26904943</idno>
		<ptr target="https://doi.org/10.1016/j.celrep.2016.01.070" />
	</analytic>
	<monogr>
		<title level="j">Cell Rep. Elsevier Ltd</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1930" to="1939" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dopamine Receptor Activation Is Required for Corticostriatal Spike-Timing-Dependent Plasticity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pawlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jnd</forename><surname>Kerr</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.4402-07.2008</idno>
		<idno type="PMID">18322089</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.4402-07.2008" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2435" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Neural Substrate of Prediction and Reward</title>
		<author>
			<persName><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montague</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.275.5306.1593</idno>
		<ptr target="https://doi.org/10.1126/science.275.5306.1593" />
	</analytic>
	<monogr>
		<title level="j">Science (80-)</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1593" to="1599" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dopamine responses comply with basic assumptions of formal learning theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Waelti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schultz</surname></persName>
		</author>
		<idno type="DOI">10.1038/35083500</idno>
		<idno type="PMID">11452299</idno>
		<ptr target="https://doi.org/10.1038/35083500" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reinforced walk on graphs and neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Myjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rudnicki</surname></persName>
		</author>
		<idno type="DOI">10.4064/sm189-3-4</idno>
		<ptr target="https://doi.org/10.4064/sm189-3-4" />
	</analytic>
	<monogr>
		<title level="j">Stud Math</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Habenschuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
