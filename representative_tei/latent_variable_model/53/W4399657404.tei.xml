<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotional Conversation: Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-12">12 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of CSE</orgName>
								<orgName type="laboratory">State Key Laboratory of VR Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of CSE</orgName>
								<orgName type="laboratory">State Key Laboratory of VR Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
							<email>lufeng@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of CSE</orgName>
								<orgName type="laboratory">State Key Laboratory of VR Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of CSE</orgName>
								<orgName type="laboratory">State Key Laboratory of VR Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Emotional Conversation: Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-12">12 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.07895v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Driving Audio Emotion Label Reference Image &quot;Angry&quot; &quot;Contempt&quot; &quot;Sad&quot; Output OR OR Angry Contempt Sad Expression-Emotion Alignment Pose-Emotion Alignment Gaze-Emotion Alignment Input Emotionally Aligned Facial Cues Digital Human</term>
					<term>Talking Face Generation</term>
					<term>AIGC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: We propose an advanced two-step framework for synthesizing vivid emotional talking faces with emotionally aligned facial cues. Initially, our approach utilizes the provided driving audio and an emotion label to generate three sequences of fine-grained facial cues (expression, head pose, and gaze) tailored to the specified emotion. Subsequently, these facial cues align with the specified emotion through self-supervised learning. Finally, utilizing these facial cues, we can produce vivid emotional talking face videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The task of talking face generation involves creating a video of a talking face using a still identity image of the speaker and an audio track of their speech content. Furthermore, the generation of emotional talking face videos, featuring precise lip synchronization and vivid facial cues such as expressions, gaze, and head pose holds considerable potential for future applications. We found that these facial cue sequences exhibit consistent patterns across videos corresponding to specific emotions. For instance, in a talking face depicting contempt, individuals typically narrow their eyes, tilt their heads upward, and shift their gaze horizontally. Conversely, in videos portraying surprise, individuals generally widen their eyes while maintaining a forward-facing head pose and gaze. The alignment of these facial cues with emotions is crucial for synthesizing realistic talking face videos. Therefore, the primary challenge of this task is to produce realistic facial videos that not only accurately reproduce lip movements in response to the driving audio but also maintain consistency between the various facial cues and corresponding emotions. </p><formula xml:id="formula_0">â€¢ â â€¢ â â â â â â â â â€¢ Head Pose â€¢ â—— â€¢ â—— â—— â â—— â â â â—— â€¢ Gaze â€¢ â â â â â â â â â â—— â—— Emotion â€¢ â â â â â€¢ â€¢ â€¢ â€¢ â€¢ â—— â€¢</formula><p>With the development of AI-Generated Content (AIGC), numerous advanced methods for generating emotional talking face have emerged. Corresponding methods can be mainly divided into audiodriven talking face generation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref> and video-driven talking face generation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. Audio-driven talking face generation synthesizes a sequence of portrait images by using both an identity image and the corresponding audio as inputs. However, most existing works focus exclusively on lip movement and often lack the ability to generate associated facial cues. The latest work, SPACE <ref type="bibr" target="#b10">[11]</ref>, is a multi-stage generative framework that achieves fine-grained control over facial expressions, emotion categories, head poses, and gaze directions of generated faces by manipulating the intermediate landmarks. Unfortunately, in SPACE, the control and generation of gaze and head pose sequences are unrelated to emotion. Although the generated faces display high visual quality, the final videos lack vividness and fail to differentiate emotions effectively. Video-driven talking face generation involves using a single identity image and multiple driving source videos as inputs. By employing a contrastive learning strategy, relevant features are extracted from various source videos to generate high-quality target talking face videos. These methods allow for the editing of facial attributes, such as head pose, facial expression, gaze, blinking, and audio, by modifying the corresponding source videos. However, these methods are not only expensive in the training stage but also incur additional costs in the inference stage due to the need for source video collection. We find that the fine-grained facial cues of the talking face video, such as expression, gaze and head pose, are closely related to emotional categories. Independently controlling these facial cues without considering emotion can lead to unrealistic results. Therefore, we require an audio-driven method that combines the low cost of audio-driven methods with the high alignment between facial cues and emotion to synthesize vivid emotional talking face videos.</p><p>In this paper, we propose an advanced audio-driven method to synthesize vivid emotional talking faces with emotionally aligned facial cues. We decompose the task into two sub-tasks: speech-tolandmarks synthesis and landmarks-to-face generation. Given input speech, an emotion label, and an identity image, the proposed speechto-landmarks module can generate sequences of normalized facial landmarks (representing expressions), gaze, and head pose in an auto-regressive manner. To address the issue of substantial variations in gaze, we discretized the eye region and modeled gaze prediction as a classification task, successfully predicting gaze sequences for the first time. The alignment of emotional labels with these facial cues is achieved through self-supervised learning. These generated facial cues are synthesized into relocated 3D facial landmarks through coordinate correction and rotation. The relocated facial landmarks, driven by these cues, not only synchronize with the input audio but also enhance the alignment of expression, gaze, and pose movements with the corresponding emotion labels. We also build a collaborative emotion classifier to model the intrinsic relationships among these facial cues. Specifically, the classifier takes aggregated intermediate features from facial cues as input ensuring consistency among these facial cue sequences. The proposed landmarks-to-face module utilizes a latent keypoints space <ref type="bibr" target="#b21">[22]</ref>, capable of producing more realistic faces compared to traditional facial landmarks. Specifically, this module maps the relocated landmarks, generated by the speechto-landmarks, to latent feature points. It then employs a pre-trained generator <ref type="bibr" target="#b30">[31]</ref> to synthesize high-quality facial images.</p><p>To the best of our knowledge, this is the first attempt to align normalized facial landmarks, gaze, and head pose concurrently with emotional categories. Compared to existing works, these explicitly aligned facial cues significantly improve the intensity and accuracy of emotional expressions in generated talking faces. Additionally, other contributions are as follows:</p><p>â€¢ We introduced an advanced speech-to-landmarks synthesis auto-regressive algorithm. This algorithm can generate emotionally aligned facial cues, including normalized facial landmarks, gaze, and head pose, in an auto-regressive manner. â€¢ We designed a specific eye region discretization strategy that efficiently generates gaze sequences by modeling gaze prediction as a classification task. â€¢ Extensive experiments on the MEAD <ref type="bibr" target="#b28">[29]</ref> dataset demonstrate the superiority of the proposed method in terms of lip synchronization and the quality of generated faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the following section, we provide an overview of prior research on audio-driven talking face generation and video-driven talking face generation. Table <ref type="table" target="#tab_0">1</ref> outlines a summary of the key differences between our approach and other state-of-the-art methods. Specifically, a solid circle indicates that the method can automatically generate the relevant attribute or be driven by other source videos to control the relevant attribute. A half-filled circle denotes that the related attribute can only be controlled by other source videos. Conversely, an empty circle represents that the method cannot generate or control the related attribute. Audio-driven Talking Face Generation. The objective of Speechdriven Talking Face Generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> is to establish a mapping from the input speech to facial representations. MakeItTalk <ref type="bibr" target="#b37">[38]</ref> disentangles audio content and speaker information to control lip motion and facial expressions, and effectively works with various portrait styles. Audio2Head <ref type="bibr" target="#b29">[30]</ref>   dataset and proposed a method that conditions talking head generation based on emotion labels. However, MEAD primarily focuses on controlling only the mouth region while leaving other parts unchanged, leading to a lack of continuity in the generated videos. Compared to MEAD, which uses a single emotion label as input to control the generation of video emotion categories, EAMM <ref type="bibr" target="#b13">[14]</ref> achieves precise emotional control over the synthesized video by adopting features extracted from the emotion source video. However, the method still ignores the movement of the gaze direction and head pose, which results in less realistic generated videos. EMMN <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b34">[35]</ref> employ memory networks and textual prompts, respectively, to control the emotions in the generated videos. SPACE <ref type="bibr" target="#b10">[11]</ref> achieves fine-grained control over facial expressions, emotion categories, head poses, and gaze directions of generated faces by decomposing the generation task into multiple subtasks. While these methods are user-friendly and straightforward, they often struggle to generate emotionally aligned facial cues for generated videos.</p><p>Video-driven Talking Face Generation. The goal of video-driven talking face generation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref> is to accurately map facial movements from a source video onto a target image. Wave2Lip <ref type="bibr" target="#b19">[20]</ref> constructs an expert discriminator to ensure precise alignment between the lip movements and the input audio. PC-AVS <ref type="bibr" target="#b36">[37]</ref> utilizes an implicit low-dimension pose code to separate audio-visual representations to achieve accurate lip-syncing and pose control. EVP <ref type="bibr" target="#b14">[15]</ref> decomposes speech into two decoupled spaces to generate dynamic 2D emotional facial landmarks. PD-FGC <ref type="bibr" target="#b26">[27]</ref> allows for the editing of facial attributes, such as head pose, facial expression, gaze, blinking, by modifying the corresponding source videos. These methods are not only costly during the training stage, but also entail additional expenses during the inference stage due to the requirement of source video collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TWO-STAGE TALKING FACE GENERATION</head><p>The proposed method takes the driving audio and a reference image, along with an emotion label, and produces an emotional talking face. It decomposes this task into two steps: (1) Speech-to-Landmarks Synthesis: Given a reference image, it extracts normalized landmarks, gaze labels, and head poses, and predicts their per-frame motions driven by the input speech and emotion label. Specifically, our proposed method innovatively generates cohesive sequences of normalized facial landmarks, gaze, and head pose simultaneously. Furthermore, we accomplish the collaborative alignment of these facial cues with the corresponding emotional labels using self-supervised learning.</p><p>(2) Landmarks-to-Face Generation: In this step, the per-frame relocated facial landmarks are mapped to latent keypoints, which are then fed into the pre-trained model <ref type="bibr" target="#b30">[31]</ref> to generate the final emotional talking face. This decomposition offers multiple advantages. Firstly, it enables fine-grained control over the output facial expressions. Secondly, the two-stage training approach can reduce the training complexity and accelerate the convergence of each module. By leveraging a pretrained face generator, we can effectively reduce the training cost while obtaining high-quality emotional talking faces. The overall framework is shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>3.1 Speech-to-Landmarks Synthesis.</p><p>Given an input speech, emotion label, and an identity face image, the proposed speech-to-landmarks synthesis is capable of generating sequences of normalized facial landmarks (representing expressions), gaze, and head pose in an auto-regressive manner. These facial cues are further aligned with specific emotions through self-supervised learning. Specifically, the network for speech-to-landmarks synthesis consists of three modules: Landmark Sequentializer, Gaze Sequentializer, and Pose Sequentializer, each performing auto-regressive prediction on different facial cues sequences. Landmark Sequentializer. In talking face generation, the quality of facial landmark generation is paramount as these landmarks directly control lip movements and expressions. Given the input audio MFCC features and the normalized 3D facial landmarks of the input image. This is represented by</p><formula xml:id="formula_1">f ğ‘™ ğ‘› = S landmark (C ğ‘›-1 , a ğ‘› , e), C ğ‘› = Linear(f ğ‘™ ğ‘› ),<label>(1)</label></formula><p>where f ğ‘™ ğ‘› and a ğ‘› are the facial landmarks features and audio features at time ğ‘›, respectively. S landmark represents the auto-regressive generator, and C ğ‘› âˆˆ R 147Ã—3 is the normalized 3D facial landmarks at time ğ‘›. The input emotion label embedding e âˆˆ R ğ· corresponds to the indexed vectorial representation in the embedding matrix E âˆˆ R ğ· Ã—ğ¾ for an emotion dictionary containing ğ¾ emotion categories, which is learned together with the whole model. We employ a convolutional neural network (CNN) to encode the audio data, while a multi-layer perceptron (MLP) is used to encode the 3D facial landmarks. The network is trained utilizing an L1 loss function, which reduces the L1 distance between the predicted facial landmarks and the normalized ground truth landmarks. Notably, we assign a higher loss scale to the y-axis to emphasize vertical motion errors during training <ref type="bibr" target="#b10">[11]</ref>. Using facial landmarks as an intermediary representation is advantageous as it facilitates the explicit manipulation of facial features. For instance, by manipulating the landmarks of the eyes, it becomes possible to incorporate eye blinks into the face. We discovered that conducting predictions in the normalized space is crucial to simplify the mapping between phonemes and lip motions. Pose Sequentializer. The natural movement of the head can effectively enhance the vividness of the generated video, but its motion pattern is also influenced by the emotional category. Hence, we also train an auto-regressive S â„ğ‘’ğ‘ğ‘‘ğ‘ğ‘œğ‘ ğ‘’ that predicts the rotation and translation for the facial landmarks. The rotation is represented by three angles: yaw, pitch, and roll, and the corresponding translation is the displacement of the 3D landmarks in the x-axis, y-axis, and z-axis. The prediction is represented by</p><formula xml:id="formula_2">r ğ‘› = Liner(f ğ‘Ÿ ğ‘› ), r ğ‘› = [m ğ‘› , b ğ‘› ], f ğ‘Ÿ ğ‘› = S â„ğ‘’ğ‘ğ‘‘ğ‘ğ‘œğ‘ ğ‘’ (r ğ‘›-1 , a ğ‘› , e), (<label>2</label></formula><formula xml:id="formula_3">) where m ğ‘› = [ğ‘¦ğ‘ğ‘¤, ğ‘ğ‘–ğ‘¡ğ‘â„, ğ‘Ÿğ‘œğ‘™ğ‘™], b ğ‘› = [Î” ğ‘¥ , Î” ğ‘¦ , Î” ğ‘§ ]</formula><p>and S headpose represents the pose sequentializer. The poses, whether predicted or extracted from a reference video, are applied to the frontal normalized landmarks predicted by our Landmark Sequentializer. This transformation maps the normalized landmarks back to the image space after applying an appropriate scaling factor. The Pose Sequentializer is also trained utilizing an L1 loss. Gaze Sequentializer. The eyes, as important organs for human interaction, contain abundant information in gaze direction, which can impact the emotion category of generated videos. In contrast to the prediction of facial landmarks and head pose, we transform </p><formula xml:id="formula_4">ğ‘™ğ‘’ ğ‘“ ğ‘¡ ğ‘› âˆˆ [0, ğ‘† -1], u ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ ğ‘› âˆˆ [1, ğ‘† -1]</formula><p>] at time n can be modeled as:</p><formula xml:id="formula_5">f ğ‘” ğ‘› = S ğ‘”ğ‘ğ‘§ğ‘’ (v ğ‘›-1 , a ğ‘› , e)., v ğ‘› = u ğ‘™ğ‘’ ğ‘“ ğ‘¡ ğ‘› + ğ‘† Ã— u ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ ğ‘› ,<label>(3)</label></formula><p>Formally, gaze decoder performs classification at ğ‘›-th time step based on the hidden states f ğ‘” ğ‘› by</p><formula xml:id="formula_6">ğ‘£ ğ‘› = argmax(p ğ‘– ğ‘› ) ğ‘– âˆˆ [1,ğ‘† Ã—ğ‘† ] , p ğ‘› = Softmax(Mf ğ‘” ğ‘› ),<label>(4)</label></formula><p>where M denotes a linear transformation. p ğ‘› âˆˆ R ğ‘† Ã—ğ‘† represents calculated probabilities for a total of ğ‘† Ã— ğ‘† classification entries. ğ‘£ ğ‘› is the entry with the maximal probability, from which we can infer the corresponding gaze label [u</p><formula xml:id="formula_7">ğ‘™ğ‘’ ğ‘“ ğ‘¡ ğ‘› , u ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ ğ‘›</formula><p>]. Finally, we utilize the cross-entropy loss to optimize the gaze direction prediction model.</p><formula xml:id="formula_8">Loss ğ‘”ğ‘ğ‘§ğ‘’ = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘›=1 Loss CE (p ğ‘› , pğ‘› ).<label>(5)</label></formula><p>After obtaining these three types of facial cues, we integrate the gaze and head pose data into the normalized landmarks to obtain the relocated landmarks as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. We also investigate the inherent relationships within these facial cues in the specific emotion. Therefore, we construct a collaborative emotion classifier to push consistency among these facial cue sequences. </p><formula xml:id="formula_9">l ğ‘› = F ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘– ğ‘“ ğ‘¦ ( f ğ‘› ), Loss ğ¶ = L CE ( l ğ‘› , l ğ‘› )<label>(6)</label></formula><p>where L CE is cross entropy loss and p ğ‘¡ is calculated probabilities for total emotion classification entries. The total loss for the stage of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>speech-to-landmarks synthesis is</head><p>Loss ğ‘›ğ‘œğ‘Ÿğ‘š = Loss ğ‘™ğ‘ğ‘›ğ‘‘ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘  + Loss ğ‘ğ‘œğ‘ ğ‘’ + Loss ğ‘”ğ‘ğ‘§ğ‘’ + Loss ğ¶ . (7)</p><p>In fact, the generation of all facial cues incorporates emotional labels as input. By minimizing the discrepancy between the generated facial cues and the ground truth detected by off-the-shelf detectors (see Sec.4.2), we achieve alignment of expressions, gazes, and head poses with emotion labels through self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Landmarks-to-Face Generation.</head><p>The field of face generation has undergone rapid development, we find that using latent keypoints as input can generate high-quality facial images. Following SPACE <ref type="bibr" target="#b10">[11]</ref>, we utilized the pre-trained model face-vid2vid <ref type="bibr" target="#b30">[31]</ref>, a state-of-the-art framework for generating faces from latent keypoints. This approach avoids the need to learn a facial image generator from scratch, thereby reducing computational requirements and improving efficiency. Specifically, we first use the relocated landmarks in the 3D space generated in the speech-to-landmarks synthesis stage, along with the input speech and emotion labels, as input to perform autoregressive prediction of latent keypoints in a self-supervised learning manner, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><formula xml:id="formula_10">R ğ‘› = F ğ‘Ÿğ‘’ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘’ (C ğ‘› â€¢ m ğ‘› + b ğ‘› , u ğ‘™ğ‘’ ğ‘“ ğ‘¡ ğ‘› , u ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ ğ‘›</formula><p>),</p><formula xml:id="formula_11">K ğ‘› = S ğ¾ğ‘’ğ‘¦ (K ğ‘›-1 , R ğ‘› , a ğ‘› , e),<label>(8)</label></formula><p>where the F ğ‘Ÿğ‘’ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘’ converts the normalized 3D facial landmarks</p><formula xml:id="formula_12">C ğ‘› âˆˆ R 147Ã—3 to relocated facial landmarks R ğ‘› âˆˆ R 147Ã—3 using the gaze label [u ğ‘™ğ‘’ ğ‘“ ğ‘¡ ğ‘› , u ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ ğ‘›</formula><p>] and head pose r ğ‘› = [m ğ‘› , b ğ‘› ] obtained in the stage of speech-to-landmarks. S ğ¾ğ‘’ğ‘¦ is auto-regression generator for 3D key points. Then, given the predicted latent keypoints K ğ‘› âˆˆ R 10Ã—3 and the initial face, the pre-trained model generates high-quality facial images by using a flow-based warping field as intermediate variables. Finally, we can generate high-quality facial images with a resolution of 256, which is generally superior to previous works. By breaking down the generation of 3D facial landmarks into the collaborative production of three facial cues, we have simplified the task. Consequently, we selected a lightweight Bi-LSTM as the backbone for all auto-regressive generators (S ğ‘™ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿğ‘˜ , S â„ğ‘’ğ‘ğ‘‘ğ‘ğ‘œğ‘ ğ‘’ , S ğ‘”ğ‘ğ‘§ğ‘’ , and S ğ¾ğ‘’ğ‘¦ ), enabling the generation of high-quality facial cues and 3D keypoints with minimal computational expense.</p><p>This work fundamentally differs from SPACE <ref type="bibr" target="#b10">[11]</ref>. SPACE focuses on precise control of the generated faces but neglects the alignment of related facial cues with emotions. Consequently, this oversight results in low differentiation among emotion categories and diminished vividness in the generated emotional talking faces. Conversely, our research prioritizes maintaining consistency between the facial cues and the driving emotion labels in generated emotional talking face videos. Furthermore, utilizing our proposed eye region discretization strategy, we have successfully generated gaze sequences for the first time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Implementation Details.</head><p>The videos were sampled at a rate of 30 frames per second (FPS), while the audio was pre-processed to 16 kHz. To extract audio features, we computed 28-dimensional MFCC using a window size of 30. We trained and evaluated our method using the MEAD dataset, an audio-visual emotional dataset comprising 60 actors/actresses and eight different emotion categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Preprocessing.</head><p>The proposed method adopts a self-supervised learning approach in both Speech-to-Landmarks Synthesis and Landmarks-to-Face Generation. To achieve this, we preprocess the emotional talking face videos to obtain training pairs by extracting the facial landmarks and latent keypoints for each frame. Variations in head poses within videos lead to a greater diversity of landmark movements, potentially diminishing prediction accuracy. Consequently, our work performs face normalization. Specifically, videos are aligned by centering on the nose tip and resized to a uniform resolution of 256 Ã— 256 pixels, a data processing approach prevalent in related works. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36]</ref>. Given a talking-head video, we first extract per-frame facial landmarks and head poses. We adopt the Mediapipe <ref type="bibr" target="#b18">[19]</ref> landmark detector to extract 478 3D facial landmarks from each frame.To reduce computational costs and enhance model inference speed, we have selected a total of 147 facial landmarks for training while retaining the landmarks in the eye area. The per-frame head pose is obtained by the 3DDFA <ref type="bibr" target="#b9">[10]</ref> landmark detector. We then rotate the face such that the nose tip faces straight towards the camera, aligned with the camera axis. The per-frame frontalized 3D facial landmarks will be scaled to obtain normalized landmarks with the same facial width.</p><p>The majority of existing speech-driven talking face generation methods have neglected the generation of gaze direction sequences, resulting in individuals in the generated videos maintaining their gaze forward. This omission weakens the authenticity of the generated videos. In this paper, to achieve more effective prediction of gaze direction, we discretize gaze direction and model the prediction as a classification problem rather than a regression problem. Specifically, we divide each eye area into 10 regions based on facial landmarks. During the training process, we directly predict into which region the pupil will fall. The pipeline of gaze discretization is illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>." In addition to landmarks, we also extract latent keypoints per frame using the pretrained face-vid2vid encoder <ref type="bibr" target="#b30">[31]</ref>. This provides per-frame pairs of (facial landmarks, latent keypoints).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics.</head><p>To evaluate the alignment between the generated face and the input audio, we calculate the Euclidean distance of facial landmarks between the generated images and the ground truth images in the mouth region (MLD <ref type="bibr" target="#b3">[4]</ref>). We also evaluate the accuracy of facial expressions by measuring the landmarks difference on the whole face (FLD). We use the confidence scores of SyncNet <ref type="bibr" target="#b4">[5]</ref> to evaluate the consistency between the generated face and the driving audio at the feature level. For the visual quality of the synthesized face, we use Structural Similarity (SSIM), Peak Signal to Noise Ratio (PSNR), and Frechet Inception Distances (FID) <ref type="bibr" target="#b11">[12]</ref> for quantitative analysis of the generated results. Additionally, we need to conduct a quantitative assessment of the effectiveness of gaze and head pose in talking face videos. Given the diversity of these facial cues, calculating their frame-by-frame differences from the ground truth is not meaningful. Therefore, we adopt Dynamic Time Warping (DTW) to measure the similarity between the generated facial cues sequences and GT. Specifically, for head pose, we separately calculate the DTW for Pitch, Yaw, and Roll sequences. For gaze, we calculate the DTW for the pupil movement speed sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Facial Cues.</head><p>Evaluation of Normalized Landmarks. The accuracy of the generated facial landmarks critically influences the alignment of lip  movements with the driving audio and the congruence between expressions and the corresponding emotion labels. The normalization of facial landmarks removes the influence of different head poses on the movement direction of facial landmarks, allowing our method to focus on predicting the vertical motions of the landmarks in the mouth and eye regions, which is crucial for prediction accuracy. We provide a qualitative comparison between the proposed Landmark Sequentializer and state-of-the-art methods for emotional talking face generation in Fig. <ref type="figure" target="#fig_2">4</ref>. We found that only our method can effectively predict eye blinks and generate lips with better alignment (see the red boxes), which is consistent with the landmarks errors presented in Tab. 3</p><p>Evaluation of Head Pose. The generation and evaluation of head pose sequences are still challenges in this task of audio-driven talking face generation. Due to the complex relationship between driving audio and the resultant head pose sequences, which are not mapped one-to-one, quantitatively assessing head pose generation is inherently difficult. Therefore, we randomly selected generated videos across various emotional categories. Then, we plotted the pitch, yaw, and roll of the generated head pose sequences and corresponding ground truth (GT) as line charts, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Across different emotion categories, the generated head pose sequences exhibit similar trends to the ground truth (GT), demonstrating that the head poses generated by our proposed method can translate into emotionally aligned head movements. Additionally, the quantitative results presented in Tab. 2 corroborate the effectiveness of the Pose Sequentializer, consistent with trends shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Evaluation of Gaze. The direction of gaze is a critical facial cue in emotional talking face generation, significantly influencing the vividness of the generated video. Based on the gaze discretization strategy mentioned in Sec. 4.2, we verify the effectiveness of the proposed Gaze Sequentializer by analyzing the spatial distribution of the pupils in the eye region. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, pie charts were used to represent the gaze distribution of different methods across various emotion categories. We found that MEAD <ref type="bibr" target="#b28">[29]</ref> and EAMM <ref type="bibr" target="#b13">[14]</ref> exhibit identical gaze distributions across different emotion categories, with the majority of the pupils falling within the eye region "5". It implies that these two methods are incapable of generating available gaze sequences. The EVP <ref type="bibr" target="#b14">[15]</ref> can only generate gaze distributions similar to the ground truth (GT) in the "happy" category, while it produces random gaze distributions in other emotion categories. Our method is capable of generating gaze distributions similar to the GT across all categories of emotions. In addition, we have further validated the effectiveness of the Gaze Sequentializer by calculating the DTW of pupil movement speed sequences as shown in Tab. 2. These results demonstrate that the gaze sequences generated by our method effectively align with the corresponding emotional labels.</p><p>To the best of our knowledge, we are the first speech-driven talking face generation method capable of generating vivid gaze sequences related to emotion categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-art Methods.</head><p>Quantitative Evaluation. To conduct a comprehensive evaluation, we performed quantitative comparisons between our method and other approaches in both emotional and non-emotional talking face generation. Tab. 3 reports the quantitative experimental results. Apart from FID and SSIM, our method outperforms other approaches significantly in terms of MLD, FLD, and the confidence of SynNet, owing to the joint contributions from all three modules we proposed. The notable performance of EVP <ref type="bibr" target="#b14">[15]</ref> on the FID metric primarily results from their substantial investment in independently training face generation models for each individual. The improvements in MLD, FLD and SynNet shows that our approach can generate more accurate facial landmarks and achieve better lip alignment, consistent with the qualitative results shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The results of SSIM, PSNR, and FID further confirm that the proposed method can generate high-quality facial images. We also conducted an ablation Qualitative Evaluation. Fig. <ref type="figure" target="#fig_6">7</ref> presents the qualitative comparison results of our method with other state-of-the-art emotional talking face generation methods. Due to the high alignment between facial cues and emotion categories, our method significantly outperforms other approaches in terms of normalized landmarks, gaze, and head poses. Specifically, our method can generate lip shapes that are more consistent with the input speech, attributable to the high precision of normalized landmark generation. Additionally, the natural variations in head pose and gaze in the generated face sequences further demonstrate the effectiveness of the proposed method. Relevant comparison videos are provided in the supplementary materials.</p><p>User Study. We conducted a user study to compare our method with three SOTA models: MEAD, EVP, and EAMM. Specifically, we randomly selected five videos from each emotion category in the MEAD test set and evaluated both the overall quality and the accuracy of emotion expression in the generated videos. Human subjects were asked to vote for the video with the highest overall quality and the most accurate emotional expression. The rank-1 ratio for each method is presented in Fig. <ref type="figure" target="#fig_7">8</ref>. Our method achieves 52% emotion accuracy and 57.88% overall quality from 20 collected human subjects, significantly outperforming other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a two-step framework to generate vivid talking faces with emotionally aligned facial cues. In the first step, we aligned facial cues, including normalized facial landmarks, gaze, and head pose, with corresponding emotion labels in a self-supervised learning manner. In the second step, we adopted latent key points as intermediate variables and utilized a pre-trained generative model to map various facial cues into high-quality facial images. Extensive experiments on the MEAD dataset demonstrate that our model advances the state-of-the-art performance significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the proposed method, which performs emotional talking face generation in two steps. In Step 1, we innovatively achieved the simultaneous generation of facial cue sequences, including normalized landmarks, gaze, and head pose. These cues are then aligned with emotional labels via self-supervised learning. In Step 2, we utilize the emotionally aligned facial cues from Step 1 as inputs, employing a pre-trained model to produce vivid emotional talking face videos.</figDesc><graphic coords="3,318.17,332.98,81.13,81.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pipeline of gaze direction discretization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative comparison of generated normalized landmarks between our method and three other methods on the MEAD dataset. the prediction of gaze direction from regression to classification by discretizing the eye regions as shown in Fig 3. It effectively enhances the accuracy of gaze direction prediction. However, in the process of predicting the sequence of gaze directions, the current gaze direction heavily relies on the previous gaze. We adopt S ğ‘”ğ‘ğ‘§ğ‘’ to model this dependency relationship. Specifically, the gaze prediction of two eyes [u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of left eye gaze distribution between our model and state-of-the-art models across different emotion categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the head pose sequences in the pitch, yaw, and roll directions under different emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative comparison of our model and other state-of-the-art models for emotional talking face generation.</figDesc><graphic coords="7,66.87,382.25,493.32,71.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: User study results of talking face video generation quality and emotion accuracy.study to validate the efficacy of the collaborative emotion classifier. The results, as presented in Tab. 3 and 2, demonstrate that the absence of the emotion classifier, denoted as 'Ours w/o C', results in a significant decline in all quantitative metrics. This decrease is primarily attributed to the intrinsic connection established by the emotion classifier among landmarks, gaze, and head pose, enhancing their consistency. Qualitative Evaluation. Fig.7presents the qualitative comparison results of our method with other state-of-the-art emotional talking face generation methods. Due to the high alignment between facial cues and emotion categories, our method significantly outperforms other approaches in terms of normalized landmarks, gaze, and head poses. Specifically, our method can generate lip shapes that are more consistent with the input speech, attributable to the high precision of normalized landmark generation. Additionally, the natural variations in head pose and gaze in the generated face sequences further demonstrate the effectiveness of the proposed method. Relevant comparison videos are provided in the supplementary materials. User Study. We conducted a user study to compare our method with three SOTA models: MEAD, EVP, and EAMM. Specifically, we randomly selected five videos from each emotion category in the MEAD test set and evaluated both the overall quality and the accuracy of emotion expression in the generated videos. Human subjects were asked to vote for the video with the highest overall quality and the most accurate emotional expression. The rank-1 ratio for each method is presented in Fig.8. Our method achieves 52% emotion accuracy and 57.88% overall quality from 20 collected human subjects, significantly outperforming other methods.</figDesc><graphic coords="8,441.57,84.66,118.38,101.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison of related works across various criteria is shown on the left.</figDesc><table><row><cell>Facial Cues</cell><cell>Ours</cell><cell>Wave2Lip</cell><cell>MakeItTalk</cell><cell>PC-AVS</cell><cell>Audio2Head</cell><cell>MEAD</cell><cell>EVP</cell><cell>EAMM</cell><cell>EMMN</cell><cell>Xu</cell><cell>PD-FGC</cell><cell>SPACE</cell></row><row><cell>Landmarks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of ablation study in DTW, including the absence of Gaze Sequentializer, denoted as Ours w/o Gaze, and the absence of Pose Sequentializer, denoted as Ours w/o Pose.</figDesc><table><row><cell>Method</cell><cell>Pitch â†“</cell><cell>Yaw â†“</cell><cell>Roll â†“</cell><cell>Left eye â†“</cell><cell>Right eye â†“</cell></row><row><cell>MEAD[29]</cell><cell>0.62</cell><cell>0.32</cell><cell>0.53</cell><cell>0.85</cell><cell>0.91</cell></row><row><cell>EVP[15]</cell><cell>0.74</cell><cell>0.77</cell><cell>0.62</cell><cell>0.88</cell><cell>0.95</cell></row><row><cell>EAMM[14]</cell><cell>0.76</cell><cell>0.74</cell><cell>0.31</cell><cell>0.81</cell><cell>0.88</cell></row><row><cell>Ours w/o Gaze</cell><cell>0.58</cell><cell>0.38</cell><cell>0.33</cell><cell>0.83</cell><cell>0.89</cell></row><row><cell>Ours w/o Pose</cell><cell>0.70</cell><cell>0.65</cell><cell>0.42</cell><cell>0.68</cell><cell>0.71</cell></row><row><cell>Ours w/o C</cell><cell>0.55</cell><cell>0.42</cell><cell>0.27</cell><cell>0.64</cell><cell>0.70</cell></row><row><cell>Ours</cell><cell>0.53</cell><cell>0.31</cell><cell>0.18</cell><cell>0.62</cell><cell>0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results of different talking face generation models on MEAD dataset.</figDesc><table><row><cell>Method</cell><cell>MLD â†“</cell><cell>FLD â†“</cell><cell>SynNet â†‘</cell><cell>SSIM â†‘</cell><cell>PSNR â†‘</cell><cell>FID â†“</cell></row><row><cell>ATVG[4]</cell><cell>3.14</cell><cell>3.87</cell><cell>2.24</cell><cell>0.57</cell><cell>28.58</cell><cell>67.6</cell></row><row><cell>SDA[26]</cell><cell>3.99</cell><cell>4.5</cell><cell>1.88</cell><cell>0.44</cell><cell>28.54</cell><cell>-</cell></row><row><cell>Wave2Lip[20]</cell><cell>3.43</cell><cell>3.80</cell><cell>2.24</cell><cell>0.57</cell><cell>29.03</cell><cell>-</cell></row><row><cell>MakeItTalk[38]</cell><cell>3.80</cell><cell>3.92</cell><cell>2.20</cell><cell>0.56</cell><cell>28.92</cell><cell>-</cell></row><row><cell>PC-AVS[37]</cell><cell>2.97</cell><cell>2.74</cell><cell>2.10</cell><cell>0.60</cell><cell>29.02</cell><cell>-</cell></row><row><cell>Song[23]</cell><cell>2.54</cell><cell>3.49</cell><cell>-</cell><cell>0.64</cell><cell>29.11</cell><cell>36.33</cell></row><row><cell>MEAD[29]</cell><cell>2.52</cell><cell>3.16</cell><cell>-</cell><cell>0.68</cell><cell>28.61</cell><cell>22.52</cell></row><row><cell>EVP[15]</cell><cell>2.45</cell><cell>3.01</cell><cell>-</cell><cell>0.71</cell><cell>29.53</cell><cell>7.99</cell></row><row><cell>EAMM[14]</cell><cell>2.41</cell><cell>2.55</cell><cell>2.26</cell><cell>0.66</cell><cell>29.29</cell><cell>-</cell></row><row><cell>Xu[35]</cell><cell>2.31</cell><cell>-</cell><cell>3.57</cell><cell>0.75</cell><cell>30.10</cell><cell>15.89</cell></row><row><cell>EMMN[24]</cell><cell>2.78</cell><cell>2.87</cell><cell>3.57</cell><cell>0.66</cell><cell>29.38</cell><cell>-</cell></row><row><cell>Ours w/o C</cell><cell>2.21</cell><cell>2.11</cell><cell>4.53</cell><cell>0.69</cell><cell>30.14</cell><cell>9.12</cell></row><row><cell>Ours</cell><cell>2.08</cell><cell>1.99</cell><cell>4.72</cell><cell>0.74</cell><cell>30.98</cell><cell>8.62</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural head reenactment with latent pose descriptors</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Pasechnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13786" to="13795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Talking-head generation with rhythmic head motion</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Megaportraits: One-shot megapixel neural head avatars</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Drobyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Chelishev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taras</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dae-talker: High fidelity speech-driven talking face generation with diffusion autoencoder</title>
		<author>
			<persName><forename type="first">Chenpeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4281" to="4289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Faceformer: Speech-driven 3d facial animation with transformers</title>
		<author>
			<persName><forename type="first">Yingruo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<idno>CVPR. 18770-18780</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flnet: Landmark driven fetching and learning network for faithful talking facial animation</title>
		<author>
			<persName><forename type="first">Kuangxiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10861" to="10868" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space: Speech-driven portrait animation with controllable expression</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gururani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20914" to="20923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth-aware generative adversarial network for talking head video generation</title>
		<author>
			<persName><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3397" to="3406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eamm: One-shot emotional talking face via audio-based emotionaware motion model</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chen Change Loy, Xun Cao, and Feng Xu. 2021. Audio-driven emotional video portraits</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="14080" to="14089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Li-net: Large-pose identity-preserving face reenactment network</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME. IEEE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic-aware implicit neural audio-driven video portrait generation</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="106" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Camillo</forename><surname>Lugaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadon</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcclanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esha</forename><surname>Uboweja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuo-Ling</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Guang</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhyun</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08172</idno>
		<title level="m">Mediapipe: A framework for building perception pipelines</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1982">2023. 1982-1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion representations for articulated animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13653" to="13662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ran He, and Chen Change Loy. 2022. Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="585" to="598" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emmn: Emotional motion memory network for audio-driven emotional talking face generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22146" to="22156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasio</forename><surname>Garcia Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven facial animation with temporal GANs</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</title>
		<author>
			<persName><forename type="first">Duomin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17979" to="17989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook</title>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13844" to="13853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mead: A large-scale audio-visual dataset for emotional talking-face generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="700" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Audio2head: Audio-driven one-shot talking-head generation with natural head motion</title>
		<author>
			<persName><forename type="first">Suzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10039" to="10049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Latent image animator: Learning to animate images via latent space navigation</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speech-Driven 3D Face Animation with Composite and Regional Facial Movements</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6822" to="6830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12452</idno>
		<title level="m">One-shot identity-preserving portrait reenactment</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6609" to="6619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4176" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Makelttalk: speaker-aware talking-head animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
