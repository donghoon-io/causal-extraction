<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UvA-DARE (Digital Academic Repository) Variational prototype inference for few-shot semantic segmentation</title>
				<funder ref="#_wDhK4NE">
					<orgName type="full">National Key Scientific Instrument and Equipment Development Project</orgName>
				</funder>
				<funder ref="#_sUUmD4h #_26uA7a3">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Y5CjXrt">
					<orgName type="full">National Security Major Basic Research Program of China</orgName>
				</funder>
				<funder ref="#_BtfVB2H">
					<orgName type="full">Guangxi Municipal Science and Technology Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-10-14">14 Oct 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haochen</forename><surname>Wang</surname></persName>
							<email>haochenwang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Secretariat, Singel 425</addrLine>
									<postCode>1012 WP</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yandan</forename><surname>Yang</surname></persName>
							<email>yangyandan@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Secretariat, Singel 425</addrLine>
									<postCode>1012 WP</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
							<email>xbcao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Secretariat, Singel 425</addrLine>
									<postCode>1012 WP</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Ministry of Industry and Information Technology of China</orgName>
								<orgName type="laboratory">Key Laboratory of Advanced Technology of Near Space Information System</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data-Based Precision Medicine</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Xiantong Zhen</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cees</forename><surname>Snoek</surname></persName>
							<email>cgmsnoek@uva.nl</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@ieee.org</email>
							<affiliation key="aff5">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UvA-DARE (Digital Academic Repository) Variational prototype inference for few-shot semantic segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-14">14 Oct 2025</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/WACV48630.2021.00057</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>You will be contacted as soon as possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation perceives the visual-world with pixel-level precision to help recognize and localize objects with rich details. Deep learning based models have achieved astonishing progress in semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. However, they usually require a large amount of pixelwise annotations for supervision which is expensive to obtain in practice. Moreover, the categories of objects to be segmented in the test stage must always be included in the training stage, which restricts its generality for practical use. Thus, few-shot semantic segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref> has recently emerged as a popular task to deal with the aforementioned issues in traditional semantic segmentation. The goal of few-shot segmentation is to segment the object of an unseen category in a query image with the support of only a few annotated images.</p><p>A critical challenge in few-shot semantic segmentation is the scarcity of annotated data for each object category to be segmented. Hence, faithfully extracting the class of the objects from the support images is key to guiding the segmentation of objects in the query image. Inspired by the prototype theory from cognitive science <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref> and prototypical networks for few-shot classification <ref type="bibr" target="#b27">[28]</ref>, the prototypebased framework has recently become popular for few-shot segmentation as well <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26]</ref>. Generally, the prototype refers to some characteristic representation of a category, which is obtained by a deep neural network that takes support images and segment annotations as input. Subsequently, it guides the segmentation procedure of a query image by learning a certain metric <ref type="bibr" target="#b3">[4]</ref>. The prototype-based methods have achieved good progress in few-shot semantic segmentation tasks. By mapping scarce support images to a deterministic class prototype, those methods learn transferable knowledge for segmenting arbitrary unseen classes. However, deterministic models suffer from two shortcomings: <ref type="bibr" target="#b0">(1)</ref> Representing the prototype by a deterministic vector can be ambiguous and is vulnerable to noise because of the limited training data. <ref type="bibr" target="#b1">(2)</ref> Capturing the information of objects by a single vector is inadequate, since objects in the same category usually exhibit great intra-class variations, as illustrated in <ref type="bibr">Fig 1 (a)</ref>. We address few-shot semantic segmentation by a new, probabilistic model that tackles these shortcomings. The deterministic model embeds the support set into a single deterministic vector as the prototype, measuring the distance between the prototype vector and feature vectors of pixels on the query image. The deterministic prototype tends to be biased and lacks the ability to represent categorical concepts. The proposed probabilistic model infers the distribution of the prototype, which is treated as a latent variable, from the support set. The probabilistic prototype is more expressive of categorical concepts and endows the model with better generalization to unseen objects.</p><p>Our main contribution is to provide the first probabilistic framework for few-shot semantic segmentation. We model the class prototype as a distribution rather than a deterministic vector, which is able to better handle the uncertainty caused by limited support images and enhances the generalization for handling large intra-class variations of objects. Our second contribution is an optimization formulated as a variational inference problem, which we call variational prototype inference (VPI). The optimization objective is built upon a newly derived evidence lower bound, which well fits the few-shot segmentation problem and offers a principled way to incorporate the prototype into segmentation by conditional inference. To evaluate our proposal, we conduct extensive experiments on three benchmarks, i.e., Pascal-5 i [25], MS-COCO <ref type="bibr" target="#b15">[16]</ref> and FSS-1000 <ref type="bibr" target="#b31">[32]</ref>. The ablative results demonstrate the benefit of the proposed probabilistic modeling for few-shot semantic segmentation. The comparison results show that our VPI outperforms the previous deterministic models on both 1-shot and 5-shot semantic segmentation tasks, showing its effectiveness for few-shot semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many-Shot Semantic Segmentation Semantic segmentation aims to segment given images within several predefined classes and is often regarded as a pixel-level classification task. State-of-the-art semantic segmentation methods based on deep convolutional neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref> have achieved astonishing success. The fully convolutional network <ref type="bibr" target="#b16">[17]</ref> was the first model to introduce end-to-end convolutional neural networks into segmentation tasks, in which a fully convolutional architecture was designed. DeepLab <ref type="bibr" target="#b1">[2]</ref> introduced the dilated convolution operation to enlarge the perception field while maintaining the resolution. However, to achieve good performance, fully convolutional networks must be heavilyparameterized and trained on a large number of images with pixel-level annotations, which are laborious to obtain. Moreover, the deep semantic segmentation models usually perform modest on new categories of objects that are unavailable in the training set, which restricts their use in practical applications.</p><p>Few-Shot Semantic Segmentation In contrast to manyshot semantic segmentation, few-shot semantic segmentation aims to segment images from arbitrary classes by learning transferable knowledge with limited annotated support images. It has recently gained popularity in computer vision due to its promise in practical applications. Shaban et al. <ref type="bibr" target="#b24">[25]</ref> introduced the first few-shot segmentation network based on a two-branch architecture, which uses a support branch to predict the parameters of the last layer of the query branch for segmentation. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> follow the two-branch architecture for few-shot semantic segmentation. Dong and Xing. <ref type="bibr" target="#b3">[4]</ref> introduced the idea of prototype learning from few-shot recognition for few-shot segmentation. They designed the PLNet in which the first branch learns a prototype vector that takes images and annotations as input and outputs the prototype; while the second branch takes both a new image and the prototype as input and outputs the segmentation mask. Since then, the prototype-based methods have been further developed in different ways <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. Rakelly et al. <ref type="bibr" target="#b19">[20]</ref> concatenated the pooled support features and the query image to generate the segmentation maps. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> introduced a masked average pooling operation to extract the representative prototype vector from support images and then estimated the cosine similarity between the extracted vector and the query feature map for predicting the segmentation map. These works have demonstrated the effectiveness of prototype learning for few-shot semantic segmentation. However, a deterministic prototype vector is not sufficiently representative for capturing the categorical concept of the objects and therefore can cause bias and reduced generalization for handling huge variations of objects in the same categories.</p><p>Variational Inference Variational inference <ref type="bibr" target="#b10">[11]</ref> approximates the probability densities of an unknown quantity through optimization given input data. The variational autoencoder (VAE) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> is a generative model that introduces variational inference into the learning of directed graphical models. Sohn et al. <ref type="bibr" target="#b28">[29]</ref> developed the conditional variational auto-encoder (C-VAE) by extending VAE into the conditional generative model for supervised learning. Kohl et al. proposed the probabilistic U-net <ref type="bibr" target="#b13">[14]</ref> which combines C-VAE with U-net <ref type="bibr" target="#b22">[23]</ref> for medical image segmentation. It learns a distribution over segmentation masks to handle ambiguities in medical images. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b6">7]</ref> introduced probabilistic models to few-shot learning to handle the uncertainty caused by scarce training data. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> deployed a latent variable to denote the distribution of the entire dataset, which is inferred from support set. They also showed that their variational learning strategy can be modified to classify proposals for instance segmentation <ref type="bibr" target="#b17">[18]</ref>. Finn et al. <ref type="bibr" target="#b6">[7]</ref> proposed a probabilistic meta-learning algorithm by extending the model agnostic meta-learning <ref type="bibr" target="#b5">[6]</ref> to a probabilistic framework. The model incorporates a parameter distribution that is trained via a variational lower bound, which handles uncertainty by sampling from the inferred parameter distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>For few-shot semantic segmentation, our purpose is to train a model on the training set D train and then perform segmentation on a test set D test where a few annotated images are available for each category. Note that the object categories in D test are disjoint from those in D train . We utilize the episodic paradigm <ref type="bibr" target="#b29">[30]</ref> for training and testing in a k-shot segmentation scenario. Specifically, both D train and D test contain several episodes. Each episode is composed of (1) a support set S = {(x i s , y i s )} k i=1 where the x i s ∈ R h×w×3 denotes the support image, where h and w denote the height and width, respectively, and y i s ∈ R h×w denotes the corresponding support mask; (2) a query set Q = {(x q , y q )} where x q is the query image and y q is the associated ground-truth mask of the object to be segmented. In particular, the input of the model is the support set S for learning transferable knowledge and a query image x q to be segmented, and the output is the segmentation map ỹq for x q . Once the model is trained on D train , we evaluate performance on the test set D test across all the episodes.</p><p>We address few-shot semantic segmentation based on prototype learning by a probabilistic latent variable model. We treat the prototype that represents the concept of the object category as a latent variable. We model the prototype as a distribution instead of a single deterministic vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Variational Prototype Inference</head><p>We introduce variational prototype inference (VPI), which finds a variational posterior to approximate the true posterior over the prototype through optimization based on the evidence lower bound (ELBO). Evidence Lower Bound From a probabilistic perspective, a few-shot semantic segmentation model aims to find the conditional predictive distribution p(y q |x q , S) over the segmentation map y q given the associated query image x q and the support set S. We assume that the class prototype z is generated from a prior distribution p θ (z|x q , S). Here, similar to previous variational models for supervised learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref>, we also use a modulated prior by making z dependent on the query image x q and the support set S. The segmentation map y q is modeled by a conditional generative distribution p ψ (y q |z, x q , S).</p><p>In order to infer the latent variable z, we maximize the conditional log-likelihood log p(y q |x q , S), which is expanded by incorporating the prior over z: log p(y q |x q , S) = log p ψ (y q |z, x q , S)p θ (z|x q , S)dz = log q φ (z|x q , y q ) p ψ (y q |z, x q , S)p θ (z|x q , S) q φ (z|x q , y q ) dz,</p><p>(1) where we introduce a proposal distribution q φ (z|x q , y q ) to approximate the intractable true posterior. By applying the Jensen's inequality to (1), we obtain log p(y q |x q , S) ≥ q φ (z|x q , y q ) log p ψ (y q |z, x q , S)p θ (z|x q , S) q φ (z|x q , y q ) dz = -D KL [q φ (z|x q , y q )||p θ (z|x q , S))</p><formula xml:id="formula_0">+ E q φ (z|xq,yq) [log p ψ (y q |z, x q , S)] = ELBO. (2) The D KL [•]</formula><p>is the Kullback-Leibler (KL) divergence between the estimated posterior distribution q φ (z|x q , y q ) and the prior distribution p θ (z|x q , S). The second term of the ELBO is the expectation of a conditional generative distribution p ψ (y q |z, x q , S). We derive a variational objective based on the above ELBO.</p><p>Variational Objective Based on the ELBO, we construct a simplified variational objective, which allows efficient optimization and easy implementation. We replace the prior with p(z|S) by conditioning it solely on the support set. We  further remove the condition on S in the predictive posterior, which makes it computationally cheaper. We therefore attain the following objective:</p><formula xml:id="formula_1">L = -D KL [q φ (z|x q , y q )||p θ (z|S)] + E q φ (z|xq,yq) [log p ψ (y q |z, x q )]<label>(3)</label></formula><p>In the above optimization objective, minimizing the KL term narrows the gap between the posterior distribution q φ (z|x q , y q ) and the prior distribution p θ (z|S). This encourages the inferred prototype from the query image to match that from the support images. Maximizing the expectation of log-likelihood log p ψ (y q |z, x q ) guarantees maximally precise prediction of the segmentation map. Based on (3), we attain the empirical objective for stochastic optimization as follows:</p><formula xml:id="formula_2">L = i D KL [q φ (z|x i q , y i q )||p θ (z|S i )] + 1 L L l=1 -log p ψ (y i q |x i q , z (l) ) ,<label>(4)</label></formula><p>where i indexes over the number of support-query pairs in the training data D train , z (l) ∼ p θ (z|S) and L is the number of samples. We take the multivariate Gaussian with a diagonal covariance structure for the distributions, and then the prior p θ (z|S) and the posterior q φ (z|x q , y q ) can be parameterized by N (z; µ prior , σ 2 prior ) and N (z; µ post , σ 2 post ). As for the second term in the empirical loss L, we adopt pixelwise cross-entropy loss to penalize the difference between the predicted segmentation map ỹq and the ground truth y q . We deploy the reparameterization trick proposed in <ref type="bibr" target="#b12">[13]</ref> to solve the non-differentiable problem existing in the sampling process. Specifically, the class prototype z is obtained by z (l) = (l)  σ prior + µ prior , where denotes the element-wise multiplication and (l) ∼ N ( ; 0, 1). The number of samples L is set to 1 during training, as suggested in <ref type="bibr" target="#b12">[13]</ref>.</p><p>In the learning stage, as shown in Fig. <ref type="figure" target="#fig_2">2</ref> (a), we estimate the prior distribution p θ (z|S) over the prototype z and the posterior distribution q φ (z|x q , y q ) conditioned on the query image x q and ground truth y q . To efficiently train the parameters with gradient descent, we rely on Monte Carlo sampling to draw L samples {z (l) } L l=1 from p θ (z|S) and combine them with the query image to generate the segmentation map.</p><p>Inference The inference of segmentation maps is shared across learning and test stages. As shown in Fig. <ref type="figure" target="#fig_2">2</ref> (b), we utilize Monte Carlo sampling to draw L potential prototypes {z l } L l=1 from p θ (z|S). The ỹq is obtained by taking the average of L segmentation maps based on the samples z.</p><formula xml:id="formula_3">ỹq = 1 L L l=1 p ψ (y q |x q , z (l) ), z (l) ∼ p θ (z|S). (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>For the k-shot setting, we generate a prior by each of the k pairs of support images and masks:</p><formula xml:id="formula_5">{N i (z i ; µ i , σ 2 i )} k i=1</formula><p>, obtaining k priors. We aggregate those k priors with a variance-weighted average operation, which produces the overall aggregated distribution N (z; µ, σ 2 ):</p><formula xml:id="formula_6">µ = k i=1 1 σ 2 i µ i k i=1 1 σ 2 i , σ 2 = k k i=1 1 σ 2 i .<label>(6)</label></formula><p>In contrast to the equal-weighted average operation, the variance-weighted average operation lets the distributions with small variance receive larger weights, resulting in the more representative distributions being enhanced, and less important being constrained.</p><p>Algorithm 1: Variational Prototype Inference Learning: Input: D train = {S i , (x i q , y i q )} Ntrain i=1 ; Initialized θ, ψ and φ for S i , (x i q , y i q ) ∈ D train do p θ (z|S) : z i ← µ i prior + σ i prior , ∼ N (0, 1), µ i prior , σ i prior ← PriorNet(S i ; θ) q φ (z|x q , y q ) : z i ← µ i post + σ i post , ∈ N (0, 1), µ i post , σ i post ← PostNet(x i q , y i q ; φ) p ψ (y q |z, x q ): ỹi q = SegNet(z i , x i q ; ψ) g ← ∇ θ,φ,ψ L(θ, φ, ψ; x i q , S i , y i q , ỹi q ) Update parameters θ, ψ, and φ end Output: p θ (z|S), q φ (z|x q , y q ), p ψ (y q |z, x q ) Inference: Input: A query image x q and a support set S p θ (z|S): z ← µ prior + σ prior , ∼ N (0, 1), µ prior , σ prior ← PriorNet(S; θ). p ψ (ỹ q |z, x q ): ỹq = 1 L L l=1 SegNet(z (l) , x q ; ψ) Output: Segmentation Map ỹq</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation with Amortized Networks</head><p>We implement the proposed VPI with neural networks of the auto-encoder architecture using the amortization technique <ref type="bibr" target="#b12">[13]</ref>. The networks that parameterize the three distributions p θ (z|S), q φ (z|x q , y q ) and p ψ (y|x q , z) are called the prior net, the posterior net and the segmentation net, respectively. Specifically, as depicted in Fig. <ref type="figure" target="#fig_2">2,</ref><ref type="figure" target="#fig_0">1</ref>) the prior net embeds the support set S into a latent space, where the conditional prior distribution p θ (z|S) of the latent variable z represents the class-specific prototype learned from the support set S; 2) the posterior net learns to recognize a proposal posterior distribution q φ (z|x q , y q ) in the latent space to approach the true posterior given a query image x q and the ground truth y q ; 3) the segmentation net takes the query image x q and the prototype vector z sampled from the prototype distribution to predict the segmentation map ỹq , which is represented as the conditional generative distribution p ψ (ỹ q |x q , z). The parameters of the CNN-based encoders for feature extraction are shared by the prior net, posterior net and the segmentation net. All the parameters of the three nets are jointly optimized end-to-end with respect to the objective (4). The optimization of VPI is summarized in Algorithm 1. Prior Net The prior net deploys a CNN encoder to extract the deep features of the support image. Then the support mask is used to filter the background feature while retaining the foreground features from average pooling <ref type="bibr" target="#b25">[26]</ref>. Hence, the feature map is squeezed into a single vector. As mentioned above, we assume that the prior takes the form of a diagonal covariance Gaussian distribution, so we map the feature vector to a mean vector µ prior and variance vector σ 2</p><p>prior in the latent space by two fully connected layers: z ∼ p θ (z|S) = N (z; µ prior , σ 2 prior ).</p><p>Posterior Net Similar to the prior net, the posterior net utilizes the same CNN encoder to extract the features of the query image x q , and then uses the ground-truth mask y q to acquire a global feature vector. Finally, a mean vector µ post and a variance vector σ 2 post are output from the posterior net for the posterior distribution:</p><formula xml:id="formula_8">z ∼ q φ (z|x q , y q ) = N (z; µ post , σ 2 post ). (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Segmentation Net The segmentation net takes the concatenation of the deep feature of the query image x q and the prototype vector z sampled from the prior (see also Fig. <ref type="figure" target="#fig_2">2</ref> (a)). Taking the feature representations of the query image x q and the sampled z as input, a CNN-based decoder produces the output segmentation map:</p><formula xml:id="formula_10">ỹq ∼ p ψ (ỹ q |x q , z).<label>(9)</label></formula><p>In the decoder, we deploy a multi-layer skip-connections structure <ref type="bibr" target="#b22">[23]</ref> to incorporate more spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Datasets We conduct experiments on three commonlyused benchmarks including the PASCAL-5 i , COCO-20 i and FSS-1000 datasets. PASCAL-5 i , we follow the setting in <ref type="bibr" target="#b24">[25]</ref> dividing the 20 original classes in PASCAL VOC12 <ref type="bibr" target="#b4">[5]</ref> and Extended SDS <ref type="bibr" target="#b7">[8]</ref> into four folds and conduct cross-validation among those folds. Specifically, 15 object classes are used during training, while the remaining 5 classes are for testing for each fold. During evaluation, we sample the same test set containing 1,000 support-query pairs for each category as in <ref type="bibr" target="#b24">[25]</ref> for a fair comparison. COCO-20 i [10], the 80 classes in MSCOCO are split into four folds and we conduct four-fold cross-validation. Similar to the setting in PASCAL-5 i , 60 object categories are used during training, while the remaining 20 categories are used for testing. In each fold, we sample 1000 supportquery pairs from the selected 20 test classes, following <ref type="bibr" target="#b18">[19]</ref>. FSS-1000 collected by <ref type="bibr" target="#b31">[32]</ref>, consists of 1000 object classes, we choose the same 240 sub classes as in <ref type="bibr" target="#b31">[32]</ref> for evaluation and the remaining classes for training.</p><p>Implementation Details We deploy the ResNet101 <ref type="bibr" target="#b8">[9]</ref> backbone pre-trained on ImageNet <ref type="bibr" target="#b2">[3]</ref> as the encoder. The decoder is composed of three convolutional blocks to generate feature maps, each of which is concatenated with the corresponding encoded feature through the skip connections. The support and query images are randomly cropped Table <ref type="table">1</ref>. The benefit of probabilistic modeling on PASCAL-5 i . The proposed probabilistic modeling shows consistent advantages over deterministic models in terms of different metrics, with different backbone networks and under both 1-shot and 5-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG ResNet50 ResNet101</head><p>Class-IoU Binary-IoU Class-IoU Binary-IoU Class-IoU Binary-IoU k-shot We adopt two metrics for evaluation, Class-IoU <ref type="bibr" target="#b24">[25]</ref> and Binary-IoU <ref type="bibr" target="#b19">[20]</ref>. Class-IoU measures the Intersection-over-Union IoU = TPc TPc+FPc+FNc , where TP, FP and FN are the number of pixels that are true positives, false positives and false negatives of the predicted segmentation masks for each foreground class c. Binary-IoU treats all object classes as foreground class and averages the IoU of foreground and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Benefit of Probabilistic Modeling The main difference between our probabilistic model and previous deterministic models is that we estimate the distribution of the class prototype in the latent space instead of learning a deterministic prototype vector. To demonstrate the advantage of the proposed probabilistic modeling, we implement a deterministic counterpart. For fair comparison, we roughly keep the same network architecture and predict a deterministic class proto-type vector by the µ branch and remove the KL divergence term during training. We implement both models with a VGG-16 <ref type="bibr" target="#b26">[27]</ref>, ResNet50 <ref type="bibr" target="#b8">[9]</ref>, and ResNet101 <ref type="bibr" target="#b8">[9]</ref> backbone, which are commonly adopted in previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>The results on PASCAL-5 i are shown in Table <ref type="table">1</ref>. Our variational prototype inference achieves better performance than the deterministic models on both the 1 and 5-shot settings for the Class-IoU as well as the Binary-IoU metric. This is because the proposed probabilistic modeling of prototypes is more expressive of object classes and can better capture the categorical concept of objects, compared to the deterministic representation of prototypes. Therefore, the learned model is endowed with a stronger generalization ability to query images that usually exhibits huge variations. The results verify the advantage of probabilistic modeling for few-shot semantic segmentation. Note that, as expected the ResNet101 backbone outperforms the one with VGG16 as well as the one with ResNet50, and henceforth we adopt ResNet101 as our backbone network in our experiments.</p><p>Effect of Monte Carlo Sampling During the inference of segmentation maps, we utilize the Monte Carlo sampling to obtain multiple prototypes z to produce multiple segmentation maps, which are aggregated into the final segmentation map. We study the effect of the Monte Carlo sampling on the segmentation results. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the segmentation map for each sampled prototype is not always  adequate. For example, in the first row of the left side of Fig. <ref type="figure" target="#fig_3">3</ref>, the segmentation map generated by sample 2 introduces some noise, and the segmentation map generated by sample 3 does not completely recover the object. By averaging the segmentation maps produced by the individual samples, the final segmentation map tends to be more precise and robust. The segmentation results are more accurate given more samples, but it will take more time for inference. We provide the accuracy and the inference time of VPI under different numbers of the samples to find a satisfactory trade-off between performance and computation time on the right side of Fig. <ref type="figure" target="#fig_3">3</ref>. We can see that the performance tends to saturate when L reaches 6, but the inference time keeps going up. Therefore, in our experiments, we set L to 6 during inference to achieve precise segmentation maps with acceptable inference time.</p><p>Comparison on Distribution Aggregation For k-shot learning, we adopt variance-weighted operation in Equation <ref type="formula" target="#formula_6">6</ref>for distribution aggregation. Here we compare it with another ordinary average operation:</p><formula xml:id="formula_11">µ o = k i=1 µ i k , σ 2 o = k i=1 σ 2 i k . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Results in Table <ref type="table">2</ref> shows the variance-weighted aggregation of µ and σ outperforms the direct average operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art</head><p>PASCAL-5 i In Table <ref type="table" target="#tab_0">3</ref>, we compare the performance of VPI with the state-of-the-art deterministic methods on PASCAL-5 i in terms of the Class-IoU metric. In both 1shot and 5-shot settings, our VPI outperforms other methods by considerable margins. We improve over the state-ofthe-art set by Nguyen and Todorovic <ref type="bibr" target="#b18">[19]</ref> by 1.1% and 0.5% for the 1-shot and 5-shot settings. The performance advantage of our VPI is larger on the 1-shot setting, which is more challenging compared to the 5-shot setting. Due to the probabilistic modeling of the prototype in VPI, it better captures the nature of objects even with only one support image. Table <ref type="table" target="#tab_1">4</ref> shows the state-of-the-art comparison in terms of the Binary-IoU metric. Our VPI achieves the best scores in both the 1-shot and 5-shot settings with 70.3% and 72.1%. Some qualitative results on PASCAL-5 i are visualized in Fig. <ref type="figure" target="#fig_5">4</ref>. The proposed VPI achieves accurate segmentation maps in various challenging scenarios, where the query images exhibit variation in appearance and object size from the associated support images. For instance, in the second  column, the size and viewpoint of the plane in the query image is considerably different from the annotated plane in the support image; in the third column, the annotated bottle in the support image is much smaller than the one the query image. Moreover, the bottle in the support image is also partially occluded.</p><p>COCO-20 i Compared to PASCAL-5 i , the scenes in COCO-20 i are more complicated with large intra-class diversity, which poses greater challenges for few-shot semantic segmentation. Therefore few-shot segmentation on COCO-20 i has more ambiguity and it is difficult to acquire a precise class-specific deterministic prototype. As can be seen in Table <ref type="table" target="#tab_2">5</ref>, our method outperforms the state-of-the-art set by PANet <ref type="bibr" target="#b30">[31]</ref> by 2.5% and 1.9% in terms of the Class-IoU and Binary-IOU metrics in the 1-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSS-1000</head><p>We evaluate our method following the same settings as in Wei et al. <ref type="bibr" target="#b31">[32]</ref>. The metric used for FSS-1000 is the intersection-over-union (IoU) of positive labels in a binary segmentation map, which we adopt for a fair comparison. The performance comparison with previous methods in terms of Positive-IoU is shown in Table <ref type="table" target="#tab_3">6</ref>. We improve over the state-of-the-art set by Wei et al. <ref type="bibr" target="#b31">[32]</ref> by 10.8%and 7.6% in the 1-shot and 5-shot settings, showing the effectiveness of our proposal for few-shot semantic segmentation with a large number of categories. Fig. <ref type="figure" target="#fig_6">5</ref> shows the class-wise performance comparison between Wei et al. <ref type="bibr" target="#b31">[32]</ref> and our proposed VPI. On most categories, VPI outperforms Wei et al. <ref type="bibr" target="#b31">[32]</ref> by a good margin. Moreover, we observe that the performance of VPI does not change much across classes, indicating its robustness and general-ization ability. The qualitative segmentation results on the FSS-1000 dataset are illustrated in supplementary materials, where VPI produces accurate segmentation close to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new, probabilistic model for few-shot semantic segmentation. We formulate the class prototype as a latent variable, the distribution over which is inferred from data. We develop variational prototype inference (VPI) to leverage the technique of variational inference for efficient optimization. By probabilistic modeling, we are able to estimate a more robust class prototype distribution that takes the inherent uncertainty in few-shot segmentation into account. Moreover, the probabilistic representation of prototypes better captures the categorical information of objects, which enhances the generalization ability of the model to new unseen categories of objects. We perform comprehensive experiments on three benchmark datasets. The thorough ablation studies demonstrate the benefit of our VPI by probabilistic modeling and the extensive comparison with state-of-the-art methods shows the performance advantage of VPI for few-shot semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Deterministic model (previous work) vs. probabilistic model (this work):The deterministic model embeds the support set into a single deterministic vector as the prototype, measuring the distance between the prototype vector and feature vectors of pixels on the query image. The deterministic prototype tends to be biased and lacks the ability to represent categorical concepts. The proposed probabilistic model infers the distribution of the prototype, which is treated as a latent variable, from the support set. The probabilistic prototype is more expressive of categorical concepts and endows the model with better generalization to unseen objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Variational prototype inference for one-shot semantic segmentation, implemented in the amortized neural network of an autoencoder architecture. The prior net produces the prior distribution p θ (z|S); the posterior net infers the posterior q φ (z|xq, yq) over z; and the segmentation net takes the query image xq and the prototype z sampled from the prior distribution to generate a distribution of the segmentation map: p ψ (yq|z, xq).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect of Monte Carlo Sampling. Left: Segmentation maps produced by sampled individual prototypes tend to be noisy, but the final segmentation maps by the aggregated prototype have less noise. Right: Trade-off between Class-IoU and inference time according to the number of samples L. We consider L=6 a good trade-off.</figDesc><graphic coords="7,293.02,71.31,237.60,146.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 2 . 2 oto 384 ×</head><label>22384</label><figDesc>Comparison of different distribution aggregation on PASCAL-5 i under the 5-shot setting. Class-IoU Binary-IoU µ o , σ 384 and augmented by random horizontal flipping and random rotation operations. The model is trained with the Adam optimizer [12] using a batch size of 16 on 4 NVIDIA GeForce TITAN X GPU for 40,000 iterations. The learning rate is fixed to 1e -6 for the backbone and 1e -5 for other layers, and the BN layers are frozen during training. The number of the samples L is set to 6 during the test phase, which is analyzed in detail by our ablation study in Sec. 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of one-shot segmentation results on the PASCAL-5 i dataset.</figDesc><graphic coords="8,274.65,345.62,52.60,53.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Class-wise comparison between VPI and Wei et al. [32] on FSS-1000.</figDesc><graphic coords="9,297.92,166.04,252.20,129.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art in terms of Class-IoU on PASCAL-5 i . fold-1 fold-2 fold-3 mean fold-0 fold-1 fold-2 fold-3 mean OSLSM<ref type="bibr" target="#b24">[25]</ref> 33.6 55.3 40.9 33.5 40.8 35.9 58.1 42.7 39.1 43.9 Co-FCN [20] 36.7 50.6 44.9 32.4 41.1 37.5 50.0 44.1 33.9 41.4 AMP [26] 41.9 50.2 46.7 34.7 43.4 40.3 55.3 49.9 40.1 46.4 SG-One [36] 40.2 58.4 48.4 38.4 46.3 41.9 58.6 48.6 39.4 47.1 PANet [31] 42.3 58.0 51.1 41.2 48.1 51.8 64.6 59.8 46.5 55.7 CANet [34] 52.5 65.9 51.3 51.9 55.4 55.5 67.8 51.9 53.2 57.1 PGNet [33] 56.0 66.9 50.6 50.4 56.0 57.7 68.7 52.9 54.6 58.5 FWB [19] 51.3 64.5 56.7 52.2 56.2 54.8 67.4 62.2 55.3 59.9 VPI 53.4 65.6 57.3 52.9 57.3 55.8 67.5 62.6 55.7 60.4</figDesc><table><row><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>fold-0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-art in terms of Binary-IoU on PASCAL-5 i .</figDesc><table><row><cell></cell><cell>1-shot 5-shot</cell></row><row><cell cols="2">Co-FCN [20] 60.1 60.2</cell></row><row><cell>AMP [26]</cell><cell>60.1 62.1</cell></row><row><cell cols="2">PL+SEG [4] 61.2 62.3</cell></row><row><cell cols="2">A-MCG[10] 61.2 62.2</cell></row><row><cell cols="2">OSLSM [25] 61.3 61.5</cell></row><row><cell cols="2">SG-One [36] 63.9 65.9</cell></row><row><cell cols="2">CANet [34] 66.2 69.6</cell></row><row><cell>PANet [31]</cell><cell>66.5 70.7</cell></row><row><cell cols="2">PGNet [33] 69.9 70.5</cell></row><row><cell>VPI</cell><cell>70.3 72.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Comparison on COCO-20 i .</figDesc><table><row><cell></cell><cell cols="4">Class-IoU Binary-IoU</cell></row><row><cell></cell><cell cols="4">1-shot 5-shot 1-shot 5-shot</cell></row><row><cell cols="2">A-MCG [10] -</cell><cell>-</cell><cell cols="2">52.0 54.7</cell></row><row><cell>FWB [19]</cell><cell cols="2">21.2 23.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">PANet [31] 20.9 29.7 59.2 63.5</cell></row><row><cell>VPI</cell><cell cols="4">23.4 27.8 61.1 62.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison on FSS-1000.</figDesc><table><row><cell></cell><cell cols="2">Positive-IoU</cell></row><row><cell></cell><cell cols="2">1-shot 5-shot</cell></row><row><cell>OSLSM [25]</cell><cell>70.3</cell><cell>73.0</cell></row><row><cell>Co-FCN [21]</cell><cell>71.9</cell><cell>74.3</cell></row><row><cell cols="2">FSS-1000 [32] 73.5</cell><cell>80.1</cell></row><row><cell>VPI</cell><cell>84.3</cell><cell>87.7</cell></row><row><cell>1-shot</cell><cell>5-shot</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Universiteit van Amsterdam. Downloaded on May 31,2022 at 08:41:46 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgemets</head><p>This work was supported in part by the <rs type="funder">National Key Scientific Instrument and Equipment Development Project</rs> under Grant <rs type="grantNumber">61827901</rs>, the <rs type="funder">National Security Major Basic Research Program of China</rs> under Grant <rs type="grantNumber">15001303</rs>, the <rs type="funder">Guangxi Municipal Science and Technology Project</rs> under Grant <rs type="grantNumber">31062501</rs>, and <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61976060</rs> and <rs type="grantNumber">61871016</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wDhK4NE">
					<idno type="grant-number">61827901</idno>
				</org>
				<org type="funding" xml:id="_Y5CjXrt">
					<idno type="grant-number">15001303</idno>
				</org>
				<org type="funding" xml:id="_BtfVB2H">
					<idno type="grant-number">31062501</idno>
				</org>
				<org type="funding" xml:id="_sUUmD4h">
					<idno type="grant-number">61976060</idno>
				</org>
				<org type="funding" xml:id="_26uA7a3">
					<idno type="grant-number">61871016</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-based multi-context guiding for few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6965" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One-shot segmentation in clutter</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName><forename type="first">Khoi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot segmentation propagation with guided networks</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07373</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Eleanor</forename><forename type="middle">H</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural categories. Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="350" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amp: Adaptive masked proxies for few-shot segmentation</title>
		<author>
			<persName><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5249" to="5258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12347</idno>
		<title level="m">Fss-1000: A 1000-class dataset for fewshot segmentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiushuang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9587" to="9595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Variational few-shot learning</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09091</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to learn variational semantic memory</title>
		<author>
			<persName><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
