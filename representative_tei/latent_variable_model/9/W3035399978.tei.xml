<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guided Variational Autoencoder for Disentanglement Learning</title>
				<funder ref="#_YFZ6yUV">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Tsinghua Academic Fund for Undergraduate Overseas Studies</orgName>
				</funder>
				<funder ref="#_EdrfZuG">
					<orgName type="full">Qualcomm Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-04-02">2 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijian</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Qualcomm, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Qualcomm, Inc</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Guided Variational Autoencoder for Disentanglement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-02">2 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.01255v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an algorithm, guided variational autoencoder (Guided-VAE), that is able to learn a controllable generative model by performing latent representation disentanglement learning. The learning objective is achieved by providing signals to the latent encoding/embedding in VAE without changing its main backbone architecture, hence retaining the desirable properties of the VAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE and observe enhanced modeling and controlling capability over the vanilla VAE. In the unsupervised strategy, we guide the VAE learning by introducing a lightweight decoder that learns latent geometric transformation and principal components; in the supervised strategy, we use an adversarial excitation and inhibition mechanism to encourage the disentanglement of the latent variables. Guided-VAE enjoys its transparency and simplicity for the general representation learning task, as well as disentanglement learning. On a number of experiments for representation learning, improved synthesis/sampling, better disentanglement for classification, and reduced classification errors in meta learning have been observed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The resurgence of autoencoders (AE) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref> is an important component in the rapid development of modern deep learning <ref type="bibr" target="#b16">[17]</ref>. Autoencoders have been widely adopted for modeling signals and images <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50]</ref>. Its statistical counterpart, the variational autoencoder (VAE) <ref type="bibr" target="#b28">[29]</ref>, has led to a recent wave of development in generative modeling due to its two-in-one capability, both representation and statistical learning in a single framework. Another exploding direction in generative modeling includes generative adversarial networks (GAN) <ref type="bibr" target="#b17">[18]</ref>, but GANs focus on the generation process and are not aimed at representation learning (without an encoder at least in its vanilla version).</p><p>Compared with classical dimensionality reduction methods like principal component analysis (PCA) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> and * Authors contributed equally.</p><p>Laplacian eigenmaps <ref type="bibr" target="#b3">[4]</ref>, VAEs have demonstrated their unprecedented power in modeling high dimensional data of real-world complexity. However, there is still a large room to improve for VAEs to achieve a high quality reconstruction/synthesis. Additionally, it is desirable to make the VAE representation learning more transparent, interpretable, and controllable.</p><p>In this paper, we attempt to learn a transparent representation by introducing guidance to the latent variables in a VAE. We design two strategies for our Guided-VAE, an unsupervised version (Fig. <ref type="figure" target="#fig_0">1</ref>.a) and a supervised version (Fig. <ref type="figure" target="#fig_0">1</ref>.b). The main motivation behind Guided-VAE is to encourage the latent representation to be semantically interpretable, while maintaining the integrity of the basic VAE architecture. Guided-VAE is learned in a multi-task learning fashion. The objective is achieved by taking advantage of the modeling flexibility and the large solution space of the VAE under a lightweight target. Thus the two tasks, learning a good VAE and making the latent variables controllable, become companions rather than conflicts.</p><p>In unsupervised Guided-VAE, in addition to the standard VAE backbone, we also explicitly force the latent variables to go through a lightweight encoder that learns a deformable PCA. As seen in Fig. <ref type="figure" target="#fig_0">1</ref>.a, two decoders exist, both trying to reconstruct the input data x: The main decoder, denoted as Dec main , functions regularly as in the standard VAE <ref type="bibr" target="#b28">[29]</ref>; the secondary decoder, denoted as Dec sub , explicitly learns a geometric deformation together with a linear subspace. In supervised Guided-VAE, we introduce a subtask for the VAE by forcing one latent variable to be discriminative (minimizing the classification error) while making the rest of the latent variable to be adversarially discriminative (maximizing the minimal classification error). This subtask is achieved using an adversarial excitation and inhibition formulation. Similar to the unsupervised Guided-VAE, the training process is carried out in an endto-end multi-task learning manner. The result is a regular generative model that keeps the original VAE properties intact, while having the specified latent variable semantically meaningful and capable of controlling/synthesizing a specific attribute. We apply Guided-VAE to the data modeling and few-shot learning problems and show favorable results on the MNIST, CelebA, CIFAR10 and Omniglot datasets.</p><p>The contributions of our work can be summarized as follows:</p><p>• We propose a new generative model disentanglement learning method by introducing latent variable guidance to variational autoencoders (VAE). Both unsupervised and supervised versions of Guided-VAE have been developed.</p><p>• In unsupervised Guided-VAE, we introduce deformable PCA as a subtask to guide the general VAE learning process, making the latent variables interpretable and controllable.</p><p>• In supervised Guided-VAE, we use an adversarial excitation and inhibition mechanism to encourage the disentanglement, informativeness, and controllability of the latent variables.</p><p>Guided-VAE can be trained in an end-to-end fashion. It is able to keep the attractive properties of the VAE while significantly improving the controllability of the vanilla VAE. It is applicable to a range of problems for generative modeling and representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Related work can be discussed along several directions. Generative model families such as generative adversarial networks (GAN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> and variational autoencoder (VAE) <ref type="bibr" target="#b28">[29]</ref> have received a tremendous amount of attention lately. Although GAN produces higher quality synthesis than VAE, GAN is missing the encoder part and hence is not directly suited for representation learning. Here, we focus on disentanglement learning by making VAE more controllable and transparent.</p><p>Disentanglement learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> recently becomes a popular topic in representation learning. Adversarial training has been adopted in approaches such as <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>. Various methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref> have imposed constraints/regularizations/supervisions to the latent variables, but these existing approaches often involve an architectural change to the VAE backbone and the additional components in these approaches are not provided as secondary decoder for guiding the main encoder. A closely related work is the β-VAE <ref type="bibr" target="#b19">[20]</ref> approach in which a balancing term β is introduced to control the capacity and the independence prior. β-TCVAE <ref type="bibr" target="#b7">[8]</ref> further extends β-VAE by introducing a total correlation term.</p><p>From a different angle, principal component analysis (PCA) family <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7]</ref> can also be viewed as representation learning. Connections between robust PCA <ref type="bibr" target="#b6">[7]</ref> and VAE <ref type="bibr" target="#b28">[29]</ref> have been observed <ref type="bibr" target="#b9">[10]</ref>. Although being a widely adopted method, PCA nevertheless has limited modeling capability due to its linear subspace assumption. To alleviate the strong requirement for the input data being pre-aligned, RASL <ref type="bibr" target="#b44">[45]</ref> deals with unaligned data by estimating a hidden transformation to each input. Here, we take advantage of the transparency of PCA and the modeling power of VAE by developing a sub-encoder (see Fig. <ref type="figure" target="#fig_0">1</ref>.a), deformable PCA, that guides the VAE training process in an integrated end-to-end manner. After training, the subencoder can be removed by keeping the main VAE backbone only.</p><p>To achieve disentanglement learning in supervised Guided-VAE, we encourage one latent variable to directly correspond to an attribute while making the rest of the variables uncorrelated. This is analogous to the excitationinhibition mechanism <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref> or the explaining-away <ref type="bibr" target="#b51">[52]</ref> phenomena. Existing approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref> impose supervision as a conditional model for an image translation task, whereas our supervised Guided-VAE model targets the generic generative modeling task by using an adversarial excitation and inhibition formulation. This is achieved by minimizing the discriminative loss for the desired latent variable while maximizing the minimal classification error for the rest of the variables. Our formulation has a connection to the domain-adversarial neural networks (DANN) <ref type="bibr" target="#b14">[15]</ref>, but the two methods differ in purpose and classification formulation. Supervised Guided-VAE is also related to the adversarial autoencoder approach <ref type="bibr" target="#b39">[40]</ref>, but the two methods differ in the objective, formulation, network structure, and task domain. In <ref type="bibr" target="#b23">[24]</ref>, the domain invariant variational autoencoders method (DIVA) differs from ours by enforcing disjoint sectors to explain certain attributes.</p><p>Our model also has connections to the deeply-supervised nets (DSN) <ref type="bibr" target="#b35">[36]</ref>, where intermediate supervision is added to a standard CNN classifier. There are also approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> in which latent variables constraints are added, but they have different formulations and objectives than Guided-VAE. Recent efforts in fairness disentanglement learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b46">47]</ref> also bear some similarity, but there is still a large difference in formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Guided-VAE Model</head><p>In this section, we present the main formulations of our Guided-VAE models. The unsupervised Guided-VAE version is presented first, followed by introduction of the supervised version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VAE</head><p>Following the standard definition in variational autoencoder (VAE) <ref type="bibr" target="#b28">[29]</ref>, a set of input data is denoted as X = (x 1 , ..., x n ) where n denotes the number of total input samples. The latent variables are denoted by vector z. The encoder network includes network and variational parameters φ that produces variational probability model q φ (z|x). The decoder network is parameterized by θ to reconstruct sample x = f θ (z). The log likelihood log p(x) estimation is achieved by maximizing the Evidence Lower BOund (ELBO) <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_0">ELBO(θ, φ; x) = E q φ (z|x) [log(p θ (x|z))] -D KL (q φ (z|x)||p(z)).<label>(1)</label></formula><p>The first term in Eq. ( <ref type="formula" target="#formula_0">1</ref>) corresponds to a reconstruction loss q φ (z|x) × ||x -f θ (z)|| 2 dz (the first term is the negative of reconstruction loss between input x and reconstruction x) under Gaussian parameterization of the output. The second term in Eq. ( <ref type="formula" target="#formula_0">1</ref>) refers to the KL divergence between the variational distribution q φ (z|x) and the prior distribution p(z). The training process thus tries to optimize:</p><formula xml:id="formula_1">max θ,φ n i=1 ELBO(θ, φ; x i ) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised Guided-VAE</head><p>In our unsupervised Guided-VAE, we introduce a deformable PCA as a secondary decoder to guide the VAE training. An illustration can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>.a. This secondary decoder is called Dec sub . Without loss of generality, we let z = (z def , z cont ). z def decides a deformation/transformation field, e.g. an affine transformation denoted as τ (z def ). z cont determines the content of a sample image for transformation. The PCA model consists of K basis B = (b 1 , ..., b K ). We define a deformable PCA loss as:</p><formula xml:id="formula_2">L DP CA (φ, B) = n i=1 E q φ (z def ,zcont|xi) ||x i -τ (z def ) • (z cont B T )|| 2 + k,j =k (b T k b j ) 2 ,<label>(3)</label></formula><p>where • defines a transformation (affine in our experiments) operator decided by τ (z def ) and k,j =k (b T k b j ) 2 is regarded as the orthogonal loss. A normalization term</p><formula xml:id="formula_3">k (b T k b k -1)</formula><p>2 can be optionally added to force the basis to be unit vectors. We follow the spirit of the PCA optimization and a general formulation for learning PCA can be found in <ref type="bibr" target="#b6">[7]</ref>.</p><p>To keep the simplicity of the method we learn a fixed basis B and one can also adopt a probabilistic PCA model <ref type="bibr" target="#b48">[49]</ref>. Thus, learning unsupervised Guided-VAE becomes:</p><formula xml:id="formula_4">max θ,φ,B n i=1 ELBO(θ, φ; x i ) -L DP CA (φ, B) . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The affine matrix described in our transformation follows implementation in <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_6">A θ = θ 11 θ 12 θ 13 θ 21 θ 22 θ 23<label>(5)</label></formula><p>The affine transformation includes translation, scale, rotation and shear operation. We use different latent variables to calculate different parameters in the affine matrix according to the operations we need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Supervised Guided-VAE</head><p>For training data X = (x 1 , ..., x n ), suppose there exists a total of T attributes with ground-truth labels. Let z = (z t , z rst t ) where z t defines a scalar variable deciding the tth attribute and z rst t represents remaining latent variables. Let y t (x i ) be the ground-truth label for the t-th attribute of sample x i ; y t (x i ) ∈ {-1, +1}. For each attribute, we use an adversarial excitation and inhibition method with term: where w t refers to classifier making a prediction for the t-th attribute using the latent variable z t . This is an excitation process since we want latent variable z t to directly correspond to the attribute label.</p><formula xml:id="formula_7">L Excitation (φ, t) = max wt n i=1 E q φ (zt|xi) [log p wt (y = y t (x i )|z t )] ,<label>(6)</label></formula><p>Next is an inhibition term.</p><formula xml:id="formula_8">L Inhibition (φ, t) = max Ct n i=1 E q φ (z rst t |xi) [log p Ct (y = y t (x i )|z rst t )] ,<label>(7)</label></formula><p>where C t (z rst t ) refers to classifier making a prediction for the t-th attribute using the remaining latent variables z rst t . log p Ct (y = y t (x)|z rst t ) is a cross-entropy term for minimizing the classification error in Eq. <ref type="bibr" target="#b6">(7)</ref>. This is an inhibition process since we want the remaining variables z rst t as independent as possible to the attribute label in Eq. ( <ref type="formula" target="#formula_9">8</ref>) below.</p><formula xml:id="formula_9">max θ,φ n i=1 ELBO(θ, φ; x i ) + T t=1 [L Excitation (φ, t) -L Inhibition (φ, t)] .<label>(8)</label></formula><p>Notice in Eq. ( <ref type="formula" target="#formula_9">8</ref>) the minus sign in front of the term L Inhibition (φ, t) for maximization which is an adversarial term to make z rst t as uninformative to attribute t as possible, by pushing the best possible classifier C t to be the least discriminative. The formulation of Eq. ( <ref type="formula" target="#formula_9">8</ref>) bears certain similarity to that in domain-adversarial neural networks <ref type="bibr" target="#b14">[15]</ref> in which the label classification is minimized with the domain classifier being adversarially maximized. Here, however, we respectively encourage and discourage different parts of the features to make the same type of classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first present qualitative and quantitative results demonstrating our proposed unsupervised Guided-VAE (Figure <ref type="figure" target="#fig_0">1a</ref>) capable of disentangling latent embedding more favorably than previous disentangle methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> on MNIST dataset <ref type="bibr" target="#b34">[35]</ref> and 2D shape dataset <ref type="bibr" target="#b41">[42]</ref>. We also show that our learned latent representation improves classification performance in a representation learning setting. Next, we extend this idea to a supervised guidance approach in an adversarial excitation and inhibition fashion, where a discriminative objective for certain image properties is given (Figure <ref type="figure" target="#fig_0">1b</ref>) on the CelebA dataset <ref type="bibr" target="#b38">[39]</ref>. Further, we show that our method is architecture agnostic, applicable in a variety of scenarios such as image interpolation task on CIFAR 10 dataset <ref type="bibr" target="#b30">[31]</ref> and a few-shot classification task on Omniglot dataset <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Guided-VAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Qualitative Evaluation</head><p>We present qualitative results on the MNIST dataset first by traversing latent variables received affine transformation guiding signal in Figure <ref type="figure" target="#fig_1">2</ref>. Here, we applied the Guided-VAE with the bottleneck size of 10 (i.e. the latent variables z ∈ R 10 ). The first latent variable z 1 represents the rotation information, and the second latent variable z 2 represents the scaling information. The rest of the latent variables z 3:10 represent the content information. Thus, we present the latent variables as z = (z def , z cont ) = (z 1:2 , z 3:10 ).</p><p>We compare traversal results of all latent variables on MNIST dataset for vanilla VAE <ref type="bibr" target="#b28">[29]</ref>, β-VAE <ref type="bibr" target="#b19">[20]</ref>, Joint-VAE <ref type="bibr" target="#b11">[12]</ref> and our Guided-VAE (β-VAE, JointVAE results are adopted from <ref type="bibr" target="#b11">[12]</ref>). While β-VAE cannot generate meaningful disentangled representations for this dataset, even with controlled capacity increased, JointVAE can disentangle class type from continuous factors. Our Guided-VAE disentangles geometry properties rotation angle at z 1 and stroke thickness at z 2 from the rest content information z 3:10 .</p><p>To assess the disentangling ability of Guided-VAE against various baselines, we create a synthetic 2D shape dataset following <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20]</ref> as a common way to measure the disentanglement properties of unsupervised disentangling methods. The dataset consists 737,280 images of 2D We present the latent space traversal results in Figure <ref type="figure" target="#fig_2">3</ref>, where the results of β-VAE and FactorVAE are taken from <ref type="bibr" target="#b27">[28]</ref>. Our Guided-VAE learns the four geometry factors with the first four latent variables where the latent variables z ∈ R 6 = (z def , z cont ) = (z 1:4 , z 5:6 ). We observe that although all models are able to capture basic geometry factors, the traversal results from Guided-VAE are more obvious with fewer factors changing except the target one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Quantitative Evaluation</head><p>We perform two quantitative experiments with strong baselines for disentanglement and representation learning in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>. We observe significant improvement over existing methods in terms of disentanglement measured by Z-Diff score <ref type="bibr" target="#b19">[20]</ref>, SAP score <ref type="bibr" target="#b31">[32]</ref>, Factor score <ref type="bibr" target="#b27">[28]</ref> in Table <ref type="table" target="#tab_0">1</ref>, and representation transferability based on classification error in Table <ref type="table" target="#tab_1">2</ref>.</p><p>All models are trained in the same setting as the experiment shown in Figure <ref type="figure" target="#fig_2">3</ref>, and are assessed by three disentangle metrics shown in Table <ref type="table" target="#tab_0">1</ref>. An improvement in the Z-Diff score and Factor score represents a lower variance of the inferred latent variable for fixed generative factors, whereas our increased SAP score corresponds with a tighter Gender Smile Figure <ref type="figure">4</ref>. Comparison of Traversal Result learned on CelebA: Column 1 shows traversed images from male to female. Column 2 shows traversed images from smiling to no-smiling. The first row is from <ref type="bibr" target="#b19">[20]</ref> and we follow its figure generation procedure.</p><formula xml:id="formula_10">Model (d z = 6) Z-Diff ↑ SAP ↑ Factor ↑ VAE [29]</formula><p>78.2 0.1696 0.4074 β-VAE (β=2) <ref type="bibr" target="#b19">[20]</ref> 98.1 0.1772 0.5786 FACTORVAE (γ=5) <ref type="bibr" target="#b27">[28]</ref> 92.4 0.1770 0.6134 FACTORVAE (γ=35) <ref type="bibr" target="#b27">[28]</ref> 98.4 0.2717 0.7100 β-TCVAE (α=1,β=5,γ=1) <ref type="bibr" target="#b7">[8]</ref> 96 coupling between a single latent dimension and a generative factor. Compare to previous methods, our method is orthogonal (due to using a side objective) to most existing approaches. β-TCVAE <ref type="bibr" target="#b7">[8]</ref> improves β-VAE <ref type="bibr" target="#b19">[20]</ref> based on weighted mini-batches to stochastic training. Our Guidedβ-TCVAE further improves the results in all three disentangle metrics.</p><p>We further study representation transferability by performing classification tasks on the latent embedding of different generative models. Specifically, for each data point (x, y), we use the pre-trained generative models to obtain the value of latent variable z given input image x. Here z is a d z -dim vector. We then train a linear classifier f (•) on the embedding-label pairs {(z, y)} to predict the class of digits. For the Guided-VAE, we disentangle the latent variables z into deformation variables z def and content variables z cont with same dimensions (i.e. d z def = d zcont ). We compare the classification errors of different models with multiple choices of dimensions of the latent variables in Table <ref type="table" target="#tab_1">2</ref>. In general, VAE <ref type="bibr" target="#b28">[29]</ref>, β-VAE <ref type="bibr" target="#b19">[20]</ref>, and FactorVAE <ref type="bibr" target="#b27">[28]</ref> do not benefit from the increase of the latent dimensions, and β-TCVAE <ref type="bibr" target="#b7">[8]</ref> shows evidence that its discovered representation is more useful for classification task than existing methods. Our Guide-VAE achieves competitive results compare to β-TCVAE, and our Guided-β-TCVAE can further reduce the classification error to 1.1% when d z = 32, which is 1.95% lower than the baseline VAE.</p><p>Moreover, we study the effectiveness of z def and z cont in Guided-VAE separately to reveal the different properties of the latent subspace. We follow the same classification task procedures described above but use different subsets of latent variables as input features for the classifier f (•). Specifically, we compare results based on the deformation variables z def , the content variables z cont , and the whole latent variables z as the input feature vector. To conduct a fair comparison, we still keep the same dimensions for the deformation variables z def and the content variables z cont . Table <ref type="table" target="#tab_3">3</ref> shows that the classification errors on z cont are significantly lower than the ones on z def , which indicates the success of disentanglement as the content variables should determine the class of digits. In contrast, the deformation variables should be invariant to the class. Besides, when the dimensions of latent variables z are higher, the classification errors on z def increase while the ones on z cont decrease, indicating a better disentanglement between deformation and content with increased latent dimensions.  We first present qualitative results on the CelebA dataset <ref type="bibr" target="#b38">[39]</ref> by traversing latent variables of attributes shown in Figure <ref type="figure">4</ref> and Figure <ref type="figure" target="#fig_3">5</ref>. In Figure <ref type="figure">4</ref>, we compare the traversal results of Guided-VAE with β-VAE for two labeled attributes (gender, smile) in the CelebA dataset. The bottleneck size is set to 16 (d z = 16). We use the first two latent variables z 1 , z 2 to represent the attribute information, and the rest z 3:16 to represent the content information. During evaluation, we choose z t ∈ {z 1 , z 2 } while keeping the remaining latent variables z rst t fixed. Then we obtain a set of images through traversing t-th attribute (e.g., smiling to non-smiling) and compare them over β-VAE. In Figure <ref type="figure" target="#fig_3">5</ref>, we present traversing results on another six attributes.</p><p>β-VAE performs decently for the controlled attribute change, but the individual z in β-VAE is not fully entangled or disentangled with the attribute. We observe the traversed images contain several attribute changes at the same time. Different from our Guided-VAE, β-VAE cannot specify which latent variables to encode specific attribute information. Guided-VAE, however, is designed to allow defined latent variables to encode any specific attributes. Guided-VAE outperforms β-VAE by only traversing the intended factors (smile, gender) without changing other factors (hair color, baldness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Quantitative Evaluation</head><p>We attempt to interpret whether the disentangled attribute variables can control the generated images from the supervised Guided-VAE. We pre-train an external binary classifier for t-th attribute on the CelebA training set and then use this classifier to test the generated images from Guided-VAE. Each test includes 10, 000 generated images randomly sampled on all latent variables except for the particular latent variable z t we decide to control. As Figure <ref type="figure" target="#fig_5">6</ref> shows, we can draw the confidence-z curves of the t-th attribute where z = z t ∈ [-3.0, 3.0] with 0.1 as the stride length. For the gender and the smile attributes, it can be seen that the corresponding z t is able to enable (z t &lt; -1) and disable (z t &gt; 1) the attribute of the generated image, which shows the controlling ability of the t-th attribute by tuning the corresponding latent variable z t .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Image Interpolation</head><p>We further show the disentanglement properties of using supervised Guided-VAE on the CIFAR10 dataset. ALI-VAE borrows the architecture that is defined in <ref type="bibr" target="#b10">[11]</ref>, where we treat G z as the encoder and G x as the decoder. This enables us to optimize an additional reconstruction loss. Based on ALI-VAE, we implement Guided-ALI-VAE (Ours), which adds supervised guidance through excitation and inhibition shown in Figure <ref type="figure" target="#fig_0">1</ref>. ALI-VAE and AC-GAN <ref type="bibr" target="#b2">[3]</ref> serve as a baseline for this experiment.</p><p>To analyze the disentanglement of the latent space, we train each of these models on a subset of the CIFAR10 dataset <ref type="bibr" target="#b30">[31]</ref> (Automobile, Truck, Horses) where the class label corresponds to the attribute to be controlled. We use a bottleneck size of 10 for each of these models. We follow the training procedure mentioned in <ref type="bibr" target="#b2">[3]</ref> for training the AC-GAN model and the optimization parameters reported in <ref type="bibr" target="#b10">[11]</ref> for ALI-VAE and our model. For our Guided-ALI-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Automobile-Horse ↓ Truck-Automobile ↓ AC-GAN <ref type="bibr" target="#b2">[3]</ref> 88   VAE model, we add supervision through inhibition and excitation on z 1:3 .</p><p>To visualize the disentanglement in our model, we interpolate the corresponding z, z t and z rst t of two images sampled from different classes. The interpolation here is computed as a uniformly spaced linear combination of the corresponding vectors. The results in Figure <ref type="figure" target="#fig_6">7</ref> qualitatively show that our model is successfully able to capture complementary features in z 1:3 and z rst 1:3 . Interpolation in z 1:3 corresponds to changing the object type. Whereas, the interpolation in z rst 1:3 corresponds to complementary features such as color and pose of the object.</p><p>The right column in Figure <ref type="figure" target="#fig_6">7</ref> shows that our model can traverse in z 1:3 to change the object in the image from an automobile to a truck. Whereas a traversal in z rst 1:3 changes other features such as background and the orientation of the automobile. We replicate the procedure on ALI-VAE and AC-GAN and show that these models are not able to consistently traverse in z 1:3 and z rst 1:3 in a similar manner. Our model also produces interpolated images in higher quality as shown through the FID scores <ref type="bibr" target="#b18">[19]</ref> in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Few-Shot Learning</head><p>Previously, we have shown that Guided-VAE can perform images synthesis and interpolation and form better representation for the classification task. Similarly, we can apply our supervised method to VAE-like models in the few-shot classification. Specifically, we apply our adversarial excitation and inhibition formulation to the Neural Statistician <ref type="bibr" target="#b12">[13]</ref> by adding a supervised guidance network after the statistic network. The supervised guidance sig-nal is the label of each input. We also apply the Mixup method <ref type="bibr" target="#b53">[54]</ref> in the supervised guidance network. However, we could not reproduce exact reported results in the Neural Statistician, which is also indicated in <ref type="bibr" target="#b29">[30]</ref>. For comparison, we mainly consider results from Matching Nets <ref type="bibr" target="#b50">[51]</ref> and Bruno <ref type="bibr" target="#b29">[30]</ref> shown in Table <ref type="table" target="#tab_7">5</ref>. Yet it cannot outperform Matching Nets, our proposed Guided Neural Statistician reaches comparable performance as Bruno (discriminative), where a discriminative objective is fine-tuned to maximize the likelihood of correct labels. 41.7% 63.2% 26.7% 42.6% BASELINE CLASSIFIER <ref type="bibr" target="#b50">[51]</ref> 80.0% 95.0% 69.5% 89.1% MATCHING NETS <ref type="bibr" target="#b50">[51]</ref> 98.1% 98.9% 93.8% 98.5% BRUNO <ref type="bibr" target="#b29">[30]</ref> 86.3% 95.6% 69.2% 87.7% BRUNO (DISCRIMINATIVE) <ref type="bibr" target="#b29">[30]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Guided Autoencoder</head><p>To further validate our concept of "guidance", we introduce our lightweight decoder to the standard autoencoder (AE) framework. We conduct MNIST classification tasks using the same setting in Figure <ref type="figure" target="#fig_1">2</ref>. As Table <ref type="table">6</ref> shows, our lightweight decoder improves the representation learned in autoencoder framework. Yet a VAE-like structure is indeed not needed if the purpose is just reconstruction and representation learning. However, VAE is of great importance in building generative models. The modeling of the latent space of z with e.g., Gaussian distributions is again important if a probabilistic model is needed to perform novel data synthesis (e.g., the images shown in Figure <ref type="figure">4</ref> and Figure <ref type="figure" target="#fig_3">5</ref>). Table <ref type="table">6</ref>. Classification error over AE and Guided-AE on MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Geometric Transformations</head><p>We conduct an experiment by excluding the geometryguided part from the unsupervised Guided-VAE. In this way, the lightweight decoder is just a PCA-like decoder but not a deformable PCA. The setting of this experiment is exactly the same as described in Figure <ref type="figure" target="#fig_1">2</ref>. The bottleneck size of our model is set to 10 of which the first two latent variables z 1 , z 2 represent the rotation and scaling information separately. As a comparison, we drop off the geometric guidance so that all 10 latent variables are controlled by the PCA-like light decoder. As shown in Figure <ref type="figure" target="#fig_10">9</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Adversarial Excitation and Inhibition</head><p>We study the effectiveness of adversarial inhibition using the exact same setting described in the supervised Guided-VAE part. As shown in Figure <ref type="figure" target="#fig_10">9</ref> (c) and (d), Guided-VAE without inhibition changes the smiling and sunglasses while traversing the latent variable controlling the gender information. This problem is alleviated by introducing the excitation-inhibition mechanism into Guided-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a new representation learning method, guided variational autoencoder (Guided-VAE), for disentanglement learning. Both unsupervised and supervised versions of Guided-VAE utilize lightweight guidance to the latent variables to achieve better controllability and transparency. Improvements in disentanglement, image traversal, and meta-learning over the competing methods are observed. Guided-VAE maintains the backbone of VAE and it can be applied to other generative modeling applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Model architecture for the proposed Guided-VAE algorithms.</figDesc><graphic coords="3,309.06,72.00,222.74,150.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Latent Variables Traversal on MNIST: Comparison of traversal results from vanilla VAE [29], β-VAE [20], β-VAE with controlled capacity increase (CCβ-VAE), JointVAE [12] and our Guided-VAE on the MNIST dataset. z1 and z2 in Guided-VAE are controlled.</figDesc><graphic coords="4,56.09,72.99,494.93,94.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of qualitative results on 2D shape. First row: originals. Second row: reconstructions. Remaining rows: reconstructions of latent traversals across each latent dimension. In our results, z1 represents the x-position information, z2 represents the y-position information, z3 represents the scale information and z4 represents the rotation information. shapes (heart, oval and square) generated from four ground truth independent latent factors: x-position information (32 values), y-position information (32 values), scale (6 values) and rotation(40 values). This gives us the ability to compare the disentangling performance of different methods with given ground truth factors. We present the latent space traversal results in Figure3, where the results of β-VAE and FactorVAE are taken from<ref type="bibr" target="#b27">[28]</ref>. Our Guided-VAE learns the four geometry factors with the first four latent variables where the latent variables z ∈ R 6 = (z def , z cont ) = (z 1:4 , z 5:6 ). We observe that although all models are able to capture basic geometry factors, the traversal results from Guided-VAE are more obvious with fewer factors changing except the target one.</figDesc><graphic coords="5,58.84,190.87,218.79,94.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Latent factors learned by Guided-VAE on CelebA: Each image shows the traversal results of Guided-VAE on a single latent variable which is controlled by the lightweight decoder using the corresponding labels as signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Experts (high-performance external classifiers for attribute classification) prediction for being negatives on the generated images. We traverse z1 (gender) and z2 (smile) separately to generate images for the classification test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Interpolation of images in z, z1:3 and z rst 1:3 for AC-GAN, ALI-VAE and Guided-ALI-VAE (Ours).</figDesc><graphic coords="7,317.33,164.08,237.58,144.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>In Figure 8 ,Figure 8 .</head><label>88</label><figDesc>Figure 8. (Top) Sampling Result Obtained from PCA (Bottom) Sampling Result obtained from learned deformable PCA (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Model d z = 16 ↓</head><label>16</label><figDesc>d z = 32 ↓ d z = 64 ↓ AUTO-ENCODER (AE) 1.37%±0.05 1.06%±0.04 1.34%±0.04 GUIDED-AE (OURS) 1.46%±0.06 1.00%±0.06 1.10%±0.08</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Ablation study on Unsupervised Guided-VAE and Supervised Guided-VAE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Disentanglement</figDesc><table><row><cell>.8</cell><cell>0.4287 0.6968</cell></row></table><note><p>: Z-Diff score, SAP score, and Factor score over unsupervised disentanglement methods on 2D Shapes dataset. [↑ means higher is better]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Representation</figDesc><table /><note><p>Learning: Classification error over unsupervised disentanglement methods on MNIST. [↓ means lower is better] † The 95 % confidence intervals from 5 trials are reported.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification</figDesc><table /><note><p>on MNIST using different latent variables as features: Classification error over Guided-VAE with different dimensions of latent variables [↑ means higher is better, ↓ means lower is better] 4.2. Supervised Guided-VAE 4.2.1 Qualitative Evaluation</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Image Interpolation: FID score measured for a subset of CIFAR10<ref type="bibr" target="#b30">[31]</ref> with two classes each. [↓ means lower is better] † ALI-VAE is a modification of the architecture defined in<ref type="bibr" target="#b10">[11]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Few</figDesc><table /><note><p>-shot classification: Classification accuracy for a few-shot learning task on the Omniglot dataset.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work is funded by <rs type="funder">NSF</rs> <rs type="grantNumber">IIS-1618477</rs>, <rs type="grantNumber">NSF IIS-1717431</rs>, and <rs type="funder">Qualcomm Inc.</rs> ZD is supported by the <rs type="funder">Tsinghua Academic Fund for Undergraduate Overseas Studies</rs>. We thank <rs type="person">Kwonjoon Lee</rs>, <rs type="person">Justin Lazarow</rs>, and <rs type="person">Jilei Hou</rs> for valuable feedbacks.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YFZ6yUV">
					<idno type="grant-number">IIS-1618477</idno>
				</org>
				<org type="funding" xml:id="_EdrfZuG">
					<idno type="grant-number">NSF IIS-1717431</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flexibly fair representation learning by disentanglement</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marissa</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connections with robust pca and the role of emergent sparsity in variational autoencoder models</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1573" to="1614" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning disentangled joint continuous and discrete representations</title>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent constraints: Learning to generate conditionally from unconditional generative models</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ¸ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation for cross-domain disentanglement</title>
		<author>
			<persName><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006">2017. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psycholog</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation by mixing them</title>
		<author>
			<persName><forename type="first">Qiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Attila</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diva: Domain invariant variational autoencoders</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Worshop Track</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation with cycleconsistent variational auto-encoders</title>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Veeravasarapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006">2018. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006">2014. 1, 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A deep recurrent model for exchangeable data</title>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joni</forename><forename type="middle">Dambre</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational inference of disentangled latent concepts from unlabeled observations</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Modeles connexionnistes de lapprentissage</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>These de Doctorat, Universite Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring explicit domain supervision for latent space disentanglement in unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detach and adapt: Learning cross-domain disentangled deep representation</title>
		<author>
			<persName><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Chien</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Michael F Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/,2017.4" />
		<title level="m">Disentanglement testing sprites dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiplicative gain changes are induced by excitation or inhibition alone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="10040" to="10051" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reconstruction-based disentanglement for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rasl: Robust alignment by sparse and lowrank decomposition for linearly correlated images</title>
		<author>
			<persName><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2233" to="2246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning controllable fair representations</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Challenges in disentangling independent factors of variation</title>
		<author>
			<persName><forename type="first">Attila</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Explaining&apos;explaining away</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Wellman</surname></persName>
		</author>
		<author>
			<persName><surname>Henrion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="292" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neocortical excitation/inhibition balance in information processing and social dysfunction</title>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Yizhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fenno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Prigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><surname>Oshea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vikaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbal</forename><surname>Sohal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Goshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanne</forename><forename type="middle">T</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><surname>Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">477</biblScope>
			<biblScope unit="issue">7363</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
