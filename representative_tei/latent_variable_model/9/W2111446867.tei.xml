<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
				<funder ref="#_tGMRs35">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_Q29y2p9">
					<orgName type="full">Spanish Ministry of Economy and Competitiveness</orgName>
				</funder>
				<funder ref="#_JtqUStC">
					<orgName type="full">ERA-Net</orgName>
				</funder>
				<funder ref="#_VHp6WcU #_gUywduE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Robòtica i Informàtica</orgName>
								<orgName type="institution">Industrial (CSIC-UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat Politècnica de Catalunya (UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Torras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Robòtica i Informàtica</orgName>
								<orgName type="institution">Industrial (CSIC-UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Robòtica i Informàtica</orgName>
								<orgName type="institution">Industrial (CSIC-UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 3D human pose using a single image is a severely under-constrained problem, because many different body poses may have very similar image projections. In order to disambiguate the problem, one common approach is to assume that an underlying deformation model is available. Linear models <ref type="bibr" target="#b4">[5]</ref> or sophisticated dimensionality reduction methods have been used for this purpose <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref>. Alternatively, other techniques have focused on learning the mapping from 2D image observations to 3D poses <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref>. In any event, most of these generative and discriminative approaches rely on the fact that 2D features, such as edges, silhouettes or joints may be easily obtained from the image.</p><p>In this paper, we get rid of the strong assumption that data association may be easily achieved, and propose a Figure <ref type="figure">1</ref>: Simultaneous estimation of 2D and 3D pose. Top left: Raw input image. Top middle: Ground truth 2D pose (green) and the result of our approach (red). Additionally, we plot a few part detectors and their corresponding score, used to estimate 3D pose. Reddish areas represent regions with highest responses. Top right: 3D view of the resulting pose. Note that despite the detectors not being very precise, our generative model allows estimating a pose very close to the actual solution. Below we show an example of a challenging scene with several pedestrians. novel approach to jointly detect the 2D position and estimate the 3D pose of a person from one single image acquired with a calibrated but potentially moving camera. For this purpose we formulate a Bayesian approach combining a generative latent variable model that constrains the space of possible 3D body poses with a HOG-based discriminative model that constrains the 2D location of the body parts. The two models are simultaneously updated using an evolutionary strategy. In this manner 3D constraints are used to update image evidence while 2D observations are used to update the 3D pose. As shown in Fig. <ref type="figure">1</ref> these strong ties make it possible to accurately detect and estimate the 3D pose even when image evidence is very poor. We evaluate our approach numerically on the HumanEva dataset <ref type="bibr" target="#b22">[22]</ref> and qualitatively on the TUD Stadtmitte sequence <ref type="bibr" target="#b2">[3]</ref>. Results are competitive with the state-of-the-art despite our relaxation of restrictions as we do not use any 2D prior and instead use directly raw detector outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Without using prior information, monocular 3D human pose estimation is known to be an ill-posed problem. In order to be disambiguated, many methods to favor the most likely shapes have been proposed.</p><p>One of the most straightforward approaches consists of modeling the pose deformations as linear combinations of modes learned from training data <ref type="bibr" target="#b4">[5]</ref>. Since linear models are prone to fail in the presence of non-linear deformations, more accurate dimensionality reduction approaches based on spectral embedding <ref type="bibr" target="#b26">[26]</ref>, Gaussian Mixtures <ref type="bibr" target="#b10">[11]</ref> or Gaussian Processes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30]</ref> have been proposed. However, these approaches rely on good initializations, and therefore, they are typically used in a tracking context. Other approaches follow a discriminative strategy and use learning algorithms such as support vector machines, mixtures of experts or random forest to directly learn the mappings from image evidence to the 3D pose space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Most of the aforementioned solutions, though, oversimplify the 2D feature extraction problem, and typically rely on background subtraction approaches or on the fact that image evidence, such as edges or silhouettes, may be easily obtained from an image, or even assume known 2D <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>With regard to the problem of directly predicting 2D poses on images, we find that one of the most successful methods is the pictorial structure model <ref type="bibr" target="#b8">[9]</ref> (later extended to the deformable parts model <ref type="bibr" target="#b7">[8]</ref>), which represents objects as a collection of parts in a deformable configuration and allows for efficient inference. Modern approaches detect each individual part using strong detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref> in order to obtain good 2D pose estimations. The deformable parts model has also been extended to use 3D models for 3D viewpoint estimation of rigid objects <ref type="bibr" target="#b17">[17]</ref>.</p><p>Recently, the estimations of an off-the-shelf 2D detector <ref type="bibr" target="#b29">[29]</ref> have already been used for 3D pose estimation in <ref type="bibr" target="#b24">[24]</ref>. Yet, and in contrast to the solution we propose here, the 2D estimations are not updated while inferring the 3D shape, and thus, the final 3D pose strongly depends on the result of the 2D detector. The same applies for <ref type="bibr" target="#b2">[3]</ref>, which computes 2D and 3D pose in two consecutive steps with no feedback. In addition, this work is applied in a 3D tracking domain where temporal constraints play an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Model for 2D and 3D Pose Estimation</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows an overview of our model for simultaneous 2D people detection and 3D pose estimation. It consists of two main components: a 3D generative kinematic model, which generates pose hypotheses, and a discriminative part model, which weights the hypotheses based on image appearance. Drawing inspiration from the approach proposed in <ref type="bibr" target="#b1">[2]</ref> for 2D articulated shapes, we represent this model using a Bayesian formulation.</p><p>With this purpose, we represent 3D poses as a set of N connected parts. Let L = {l 1 , . . . , l N } be their 2D configuration with l i = (u i , v i , s i ). (u i , v i ) is the image position of the center of the part, and s i a scale parameter which will be defined below. In addition let D = {d 1 , . . . , d N } be the set of image evidence maps, i.e., the maps for every part detector at different scales and for the whole image. Assuming conditional independence of the evidence maps given L, and that the part map d i only depends on l i , the likelihood of the image evidence given a specific body configuration becomes:</p><formula xml:id="formula_0">p (D | L) = N i=1 p (d i | l i ) .</formula><p>(1)</p><p>In <ref type="bibr" target="#b1">[2]</ref>, Eq. ( <ref type="formula">1</ref>) is further simplified under the assumption that the body configuration may be represented using a tree topology. This yields an additional efficiency gain, as it introduces independence constraints between branches, e.g., the left arm/leg does not depend on the right arm/leg. Yet, this causes the issue of the double counting, where the same arm/leg is considered to be both the left and right one. In <ref type="bibr" target="#b3">[4]</ref> this is addressed by first solving an optimal tree and afterwards attempting to correct these artefacts using loopy belief propagation. Instead of using two stages, we directly represent our 3D model using a Directed Acyclic Graph, which enforces anthropomorphic constraints, and helps preventing the double counting problem.</p><p>Let X = {x 1 , . . . , x N } be the 3D model that projects on the 2D pose L, where x i = (x i , y i , z i ) is the 3D position of i-th part center. We write the posterior of X given the image evidence D by:</p><formula xml:id="formula_1">p (X | D) ∝ N i=1 (p (d i | l i ) p (l i | x i )) p (X) .</formula><p>In order to handle the complexity of directly modeling p (X), we propose approximating X through a generative model based on latent variables H. This allows us to finally write the problem as:</p><formula xml:id="formula_2">p (X | D) ∝ p (H) p (X | H) generative N i=1 (p (d i | l i ) p (l i | x i ))</formula><p>discriminative where the discriminative and generative components become clearly separated. We will next describe each of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Discriminative Detectors</head><p>Recent literature proposes two principal alternatives for discriminative detectors: the shape context descriptors built by applying boosting on the limbs <ref type="bibr" target="#b1">[2]</ref>, and the HOG template matching approach <ref type="bibr" target="#b29">[29]</ref>. For our purposes we have found the HOG-based template matching to be more adequate because it matches our joint-based 3D model better as we can place a detector at each joint part instead of having to infer the limb positions from the joints. In addition, the HOG template matching yields smoother responses, which is preferable when doing inference.</p><p>As mentioned above, each part l i has an associated scale parameter s i . This parameter is used to pick a specific scale among the evidence maps. Intuitively, if a part is far away, We visualize the response at different octaves for three different part detectors. We overlay the projection of the 3D ground truth in white to get an idea of the accuracy. The outputs are normalized for visualization purposes, with the dark blue and bright red areas corresponding to lower and higher responses respectively. Note that while some detectors, such as the head one in the first row, generally give good results, others do not, such as the left hand detector in the middle row. Our approach can handle these issues by combining these 2D part detectors with a generative model.</p><p>it should be evaluated by a detector at a small scale (high resolution). We therefore approximate the scale s i as:</p><formula xml:id="formula_3">s -1 i = α -1 βz i (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where α is the focal length of the camera and is used for normalization purposes. The parameter β will be learned off-line and used during inference. Note that despite this parameter constant, the scale at which each part is evaluated is different as it depends on the z i coordinate.</p><p>Let W = {w 1 , . . . , w N } be the set of templates in the HOG space associated to each body part. These templates are provided in <ref type="bibr" target="#b29">[29]</ref> <ref type="foot" target="#foot_0">foot_0</ref> . Given a body part l i , its image evidence d i is computed by evaluating the template w i over the entire image for a range of scales s i . Figure <ref type="figure" target="#fig_1">3</ref> illustrates the response of three part detectors at different scales. By interpreting each detector as a log-likelihood, the image evidence of a configuration L can be computed as: where k i is a weight associated to each detector, which is learned offline. It is meant to adapt the 2D detectors and 3D generative model due to the fact they were trained independently on different datasets.</p><formula xml:id="formula_5">log p (L | D) ≈ score(L) = N i=1 k i d i (u i , v i , s i ) ,<label>(3)</label></formula><p>Additionally, when evaluating a part detector at a point, we consider a small window from which we use the largest detector value in order to give additional noise tolerance to the detector. We find this necessary as small 2D errors can have large consequences in 3D positioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Generative Model</head><p>Our goal is to learn a compression function: φ(X L ) : X L → H that maps points in the high dimensional local 3D pose space X L to a lower dimensional space H. The local pose space consists of aligning the coordinates X to a local reference so that they are independent of the global position in the world and represent only local deformation. We wish to build this compression function so that it efficiently approximates functions of X L by sampling H. To create the compression function we define a latent variable model of the joint distribution of a set of variables H ∈ H and a pose X L ∈ X L .</p><p>As a first step we map the R N ×3 continuous pose space to a discrete domain by doing vector quantization of groups of 3D joint positions. More specifically, we group the joints into five coarse parts: right arm (ra), left arm (la), right leg (rl), left leg (ll) and torso+head (th). Thus, we can map every pose to a discrete vector T = [ra, la, rl, ll, th] ∈ K 5 where ra, la, rl, ll, th are cluster indexes belonging to K = {1, 2, .., k} of the corresponding 3D joint positions.</p><p>We will now define a joint distribution over latent variables H ∈ H = {1, .., n} (where n is the number of latent states), and observed variables T . The model is given by the following generative process:</p><p>• Sample a latent state i according to p (h 0 ) • For all the parts associated with arm and leg posi-tions, sample discrete locations: &lt; ra, la, rl, ll &gt; and states &lt; j, l, m, n &gt; according to the conditional distributions:</p><formula xml:id="formula_6">p (h ra = j, ra | i), p (h la = l, la | i), p (h rl = m, rl | i) and p (h ll = n, ll | i)</formula><p>• Sample a pair of latent states: &lt; q, w &gt; (associated with the positions of the upper body and lower body joints) according to p (h u = q | h ra = j, h la = l) and</p><formula xml:id="formula_7">p (h l = w | h rl = m, h ll = n) • Sample a discrete location th and a state r from p h (th) = r, th | h u = q, h l = w</formula><p>Given this generative model we define the probability of a discrete 3D position T = [ra, la, rl, ll, th] as:</p><formula xml:id="formula_8">p (T ) = H p (T, H) = H p (h0) p (hra, ra | h0) p (h la , la | h0) p (h rl , rl | h0) p (h ll , ll | h0) p (hu | hra, h la ) p (h ll , ll | h0) p (hu | hra, h la ) p (h l | h rl , h ll ) p (h th , th | hu, hl) .</formula><p>Figure <ref type="figure" target="#fig_2">4</ref> illustrates the graphical model corresponding to this joint distribution, where the graph G specifies the dependencies between the latent states. Since H is unobserved, Expectation Maximization can be used to estimate the model parameters from a set of training poses. Given that G is a Directed Acyclic Graph we can compute all required expectations efficiently with dynamic programming <ref type="bibr" target="#b11">[12]</ref>. Once we have learned the parameters of the model we define our compression function to be:</p><formula xml:id="formula_9">φ(X L ) = arg max H p X L , H</formula><p>and our decompression function to be:</p><formula xml:id="formula_10">φ -1 (H) = arg max X L p X L , H .</formula><p>Note that the decompression function is not technically speaking the true inverse of φ(X L ), clearly no such inverse exists since φ(X L ) is many to one. However, we can regard φ -1 (H) as a "probabilistic inverse" that returns the most probable pre-image of H. Our compression function maps points in K 5 to points in H 8 . For example, when k = 300 and n=9 we reduce the search space size from 10 11 to 10 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter Learning</head><p>To reduce the number of parameters we need to learn, we take into account the symmetry within the human body, that is, we give the same parameter values to the left and right sides of the body. This allows us to use only 12 parameters instead of 22. Additionally we set a restriction k i = 1 in order to learn relative weighing and reduce the parameters by one. This leaves a total of 12 parameters to be learnt including the detector scale factor β.  <ref type="formula" target="#formula_5">3</ref>)) with darker values indicating higher scores. The ground truth is displayed in green. Right: Representation of the trained part weight k i as a disk with an area proportional to the value. Note that part detectors which one would assume to be most useful, such as the head, have low values. This is caused by the mismatch between the annotations used to train the 2D detectors and the 3D ground truth.</p><p>These parameters are learned by translating and rotating random pose samples from the 3D training set and using them as negatives such as those seen in Fig. <ref type="figure" target="#fig_3">5</ref>. The parameters are optimized over the difference of the logarithm of expectations of the score from Eq. ( <ref type="formula" target="#formula_5">3</ref>):</p><formula xml:id="formula_11">arg max k,β log E score(L + ) -log E score(L -)</formula><p>with L + and L -being the sets of positive and negative samples respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>The inference problem consists of computing:</p><formula xml:id="formula_12">&lt;X * &gt;= arg max X N i=1 p (di | li) p (li | xi) p (X | H) p (H) .</formula><p>We treat this as a global optimization problem where, given a set of 2D detections corresponding to the different parts from a single image, we optimize over both a rigid transformation and the latent states. Drawing inspiration in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, we do this using a variation of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) <ref type="bibr" target="#b9">[10]</ref>, which is a black box global optimizer that uses a Gaussian distribution in the search space to minimize a function. In our case we perform:</p><formula xml:id="formula_13">arg max R,t,H score proj R,t (φ -1 (H)) + log p φ -1 (H) , H</formula><p>where proj R,t (•) is the result of applying a rotation R, a translation t and projecting onto the image plane. We then Table <ref type="table">1</ref>: The influence of the number of latent states per node on the average reconstruction error (in mm). We compare the upper-lower grouping (UL) used in this paper to a right-left grouping (RL), which can be seen to perform roughly the same. The values used are highlighted in bold. take X * =Rφ -1 (H * ) + t, that is, we obtain the most probable X L * given H * and perform a rigid transformation to the world coordinates to obtain X * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We numerically evaluate our algorithm on the Hu-manEva benchmark <ref type="bibr" target="#b22">[22]</ref>, which provides 3D ground truth for various actions. In addition, we provide qualitative results on the TUD Stadmitte sequence <ref type="bibr" target="#b2">[3]</ref>, a cluttered street sequence. For both cases, we compute the 2D observations using the detectors from <ref type="bibr" target="#b29">[29]</ref> trained independently on the PARSE dataset <ref type="bibr" target="#b19">[19]</ref>. The 3D model we use consists of 14 joints, each roughly corresponding to one of the 26 detectors from <ref type="bibr" target="#b29">[29]</ref>. Additionally, for the limbs we associate an extra detector for a total of 22 detectors (see Fig. <ref type="figure" target="#fig_3">5-right</ref>). The latent generative model consists of 8 nodes with 6 latent states. The effect of the number of latent states and structure of the model is shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We train our 3D generative model separately for the walking and jogging actions of the HumanEva dataset using the training sequence for subjects S1, S2 and S3. To avoid overfitting we use part of the training sequence exclusively as a validation set. The part weights k i and scale factor β are learnt conjointly on the walking and jogging actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>We consider three error metrics: 2D error, 3D error and 3D pose error. The 2D error measures the mean pixel difference between the 2D projection of the estimated 3D shape, and the ground truth. 3D error is the mean euclidean distance, in mm, with the ground truth, and the 3D pose error is the mean euclidean distance with the ground truth after performing a rigid alignment of the two shapes. This error is indicative of the local deformation error. We evaluate three times on every 5 images using all three cameras and all three subjects for both the walking and jogging actions, for a total of 1318 unique images.</p><p>With no additional constraints our optimization framework may take about 30 minutes per frame. This would make the task of evaluating all 3 × 1318 images extremely ) and cameras (C1,C2,C3). We compare with the 2D error obtained using the 2D model from <ref type="bibr" target="#b29">[29]</ref>, based on the same part detectors we use. 2D, 3D and Pose Errors are defined in the text. Ideal detector corresponds to our approach using Gaussians with 20px covariance as 2D input instead of the 2D detections.</p><p>slow. To speed up the process we have provided a rough initialization to our method. First, instead of considering full images, we crop the original images to have a 60 pixel border around the 2D projection of the 3D ground truth, which on average is 89x288px. Note that this is a criteria which can be easily met with current 2D body detectors.</p><p>In addition we have roughly initialized the initial 3D pose parameters with a hyper-Gaussian distribution centered on the ground truth values for R, t and H with the following deviations: π 4 rad on the rotation around the vertical axis for R; 50 mm deviation in each Cartesian axis for t; and 25% of the full latent variable standard deviation for the H. Additionally, we stop the CMA-ES <ref type="bibr" target="#b9">[10]</ref> algorithm after 100 iterations. With these assumptions, each 3D pose can be estimated in roughly one minute. Yet, note that while we define the mean to be centered on the ground truth, the CMA-ES algorithm does move in a large area of the search space, as it performs a global and not local optimization. This is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. The left column shows an example of optimization process using the constraints just mentioned. Observe that the initial set of explored poses consider very different configurations and space positions.</p><p>Table <ref type="table" target="#tab_0">2</ref> summarizes the results of all experiments. We compare our approach using both Gaussians (20px Cov.) and the detector outputs as inputs. We see that using ideal detectors, even with large covariances, the absolute error is reduced to 45% of the full approach. An interesting result is that we outperform the 2D pose obtained by <ref type="bibr" target="#b29">[29]</ref>, using their own part detectors. This can likely be attributed to our joint 2D and 3D model. Nonetheless, although <ref type="bibr" target="#b29">[29]</ref> is not an action specific approach as we are, this is still an interesting result as <ref type="bibr" target="#b17">[17]</ref> reports performance loss in 2D localization when using a 3D model. Figure <ref type="figure" target="#fig_6">7</ref> shows some specific examples. As expected, performance is better when there are fewer self-occlusions. A full sequence for the jogging action with subject S2 is shown in Fig. <ref type="figure">8</ref>.</p><p>We also compare our results with <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">24]</ref>. This is just meant to be an indicative result, as the different methods are trained and evaluated differently. Table <ref type="table" target="#tab_2">3</ref> summarizes the results using the pose error, corresponding to the "aligned error" in <ref type="bibr" target="#b24">[24]</ref>. The two algorithms that use temporal information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> are evaluated using absolute error. Moreover, <ref type="bibr" target="#b6">[7]</ref> uses two cameras, while the rest of the approaches are monocular. Due to our strong kinematic model we outperform all but <ref type="bibr" target="#b5">[6]</ref>. Yet, in this work the 2D detection step is relieved through background subtraction processes.</p><p>Finally, we present qualitative results on the TUD Stadmitte sequence <ref type="bibr" target="#b2">[3]</ref>, which represents a challenging realworld scene with the presence of distracting clutter and occlusions. In addition, since the ground truth is unknown, no ). The next two columns show typical failures cases of our algorithm. In the fourth column we see that occasionally we suffer from depth errors, where the 3D pose is correct but its depth is not. In the last column we plot other failures, mostly caused by very large errors of the 2D detector, due to mis-classifications or self-occlusions.  We present results for both the walking and jogging actions for all three subjects and camera C1.</p><p>priors on the initialization of the 3D pose were provided, except the rough bounding boxes for each person (Fig. <ref type="figure" target="#fig_5">6</ref>right). Some results can be seen in Fig. <ref type="figure">9</ref>. Note that while the global pose seems generally correct, there are still some errors in the 3D pose due to the occlusions and to the fact that the walking style in the wild is largely different from that of the subjects of the HumanEva dataset used to train the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>The approach presented in this paper addresses the illposed problem of estimating the 3D pose in single images using a Bayesian framework. We use a combination of a strong kinematic generative model based on latent variables with a set of discriminative 2D part detectors to jointly estimate both the 3D and 2D poses. The results we have ob-tained are competitive with the state-of-the-art in both 2D and 3D, despite having relaxed the strong assumptions of other methods. Furthermore, the Bayesian framework used is flexible enough to allow extending it further to multi-view sequences, temporal sequences and to handle occlusions.</p><p>We believe the model we have presented in this paper is a step forward to combining the works of 2D pose estimation with 3D pose estimation. We have shown it is not only possible to estimate 3D poses by applying part detectors used in 2D pose estimation, but that it is also beneficial to the 2D pose estimation itself, as the 2D deformations are being generated by an underlying 3D model.</p><p>Future work includes handling occlusions -a weakness of approaches based on 2D detectors-, and improving the handling of rotations by learning a prior distribution and incorporating it in the model. More exhaustive research on different graphical models that can better represent human poses and a deeper analysis of the hyper-parameters chosen are also likely to improve the current method. Figure <ref type="figure">8</ref>: Detection results for subject S2 in the jogging action for all three cameras. For this specific action, most of the error comes from only allowing the 3D shape to rotate around the vertical axis. We show two specific frames corresponding to large and small error. The ground truth is displayed in green and the estimated 3D pose in red. On frame 131 we can see that, although the 2D detection is quite accurate, the 3D solution shows large amounts of error due to the strong inclination of the pose. In contrast, frame 271 is accurately detected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Method overview. Our approach consists of a probabilistic generative model and a set of discriminative 2D part detectors. Our optimization framework simultaneously solves for both the 2D and 3D pose using an evolutionary strategy. A set of weighted samples are generated from the probabilistic generative model and are subsequently reweighted by the score given by the 2D part detectors. This process is repeated until convergence of the method. The rightmost figure shows results at convergence where the red shapes are the estimated poses and the green ones correspond to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 2D Part Detector.We visualize the response at different octaves for three different part detectors. We overlay the projection of the 3D ground truth in white to get an idea of the accuracy. The outputs are normalized for visualization purposes, with the dark blue and bright red areas corresponding to lower and higher responses respectively. Note that while some detectors, such as the head one in the first row, generally give good results, others do not, such as the left hand detector in the middle row. Our approach can handle these issues by combining these 2D part detectors with a generative model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: We use a probabilistic graphical model with latent variables G to represent the possible 3D human pose T from a large set of discrete poses. Latent variables can either be mapped to the conjoint motion of various parts or be used as internal states containing internal structure of the pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left and center: Three negative examples used in training. They are coloured by their discriminative score (Eq. (3)) with darker values indicating higher scores. The ground truth is displayed in green. Right: Representation of the trained part weight k i as a disk with an area proportional to the value. Note that part detectors which one would assume to be most useful, such as the head, have low values. This is caused by the mismatch between the annotations used to train the 2D detectors and the 3D ground truth.</figDesc><graphic coords="5,66.51,82.91,52.65,112.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two different initialization set-ups for our optimization approach. The default initialization consists of optimizing from an initial hyper-Gaussian centered around the ground. The coarse initialization consists in estimating the 3D location based on a 2D pose estimate and using completely random orientation and latent states for the generative model. In the situations in which the detectors are less noisy both initializations perform roughly the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sample frames from the HumanEva dataset for both the walking (A1) and jogging (A2) actions. The first three columns correspond to successful 2D and 3D pose estimations for the three subjects (S1,S2,S3). The next two columns show typical failures cases of our algorithm. In the fourth column we see that occasionally we suffer from depth errors, where the 3D pose is correct but its depth is not. In the last column we plot other failures, mostly caused by very large errors of the 2D detector, due to mis-classifications or self-occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Results on the HumanEva dataset for the walking (A1) and jogging (A2) actions with all subjects (S1,S2,S3</figDesc><table><row><cell></cell><cell>[29]</cell><cell cols="3">Ideal Detector</cell><cell cols="3">Our Approach</cell></row><row><cell>Err.</cell><cell>2D</cell><cell>2D</cell><cell>3D</cell><cell>Pose</cell><cell>2D</cell><cell>3D</cell><cell>Pose</cell></row><row><cell>All</cell><cell cols="7">21.7 11.0 106.6 51.6 19.5 237.3 55.3</cell></row><row><cell>C1</cell><cell cols="7">19.5 11.1 113.8 52.3 18.9 239.1 55.2</cell></row><row><cell>C2</cell><cell cols="7">22.9 11.1 109.7 51.2 19.6 245.8 55.4</cell></row><row><cell>C3</cell><cell cols="2">22.8 10.8</cell><cell>96.2</cell><cell cols="4">51.2 20.0 227.1 55.4</cell></row><row><cell>S1</cell><cell cols="2">21.8 10.2</cell><cell>96.8</cell><cell cols="4">63.4 19.9 277.2 69.3</cell></row><row><cell>S2</cell><cell cols="7">21.8 10.8 108.0 44.8 18.6 206.6 46.8</cell></row><row><cell>S3</cell><cell cols="7">21.6 12.3 119.0 43.7 20.1 221.4 46.6</cell></row><row><cell>A1</cell><cell cols="7">20.9 10.7 106.0 56.2 19.3 254.4 60.3</cell></row><row><cell>A2</cell><cell cols="7">22.7 11.3 107.2 46.6 19.7 219.0 50.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison against state-of-the-art approaches.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Indeed, each of the part detectors provided in<ref type="bibr" target="#b29">[29]</ref> is formed by several templates and we use their maximum score for each coordinate (u, v, s). For ease of explanation we refer to them as a single template.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work has been partially funded by <rs type="funder">Spanish Ministry of Economy and Competitiveness</rs> under projects PAU+ <rs type="grantNumber">DPI2011-27510</rs>, <rs type="grantNumber">CINNOVA 201150E088</rs> and <rs type="grantNumber">2009SGR155</rs>; by the <rs type="funder">EU</rs> project <rs type="projectName">IntellAct</rs> <rs type="grantNumber">FP7-ICT2009-6-269959</rs> and by the <rs type="funder">ERA-Net</rs> <rs type="projectName">CHISTERA</rs> project <rs type="projectName">VISEN</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Q29y2p9">
					<idno type="grant-number">DPI2011-27510</idno>
				</org>
				<org type="funding" xml:id="_VHp6WcU">
					<idno type="grant-number">CINNOVA 201150E088</idno>
				</org>
				<org type="funded-project" xml:id="_tGMRs35">
					<idno type="grant-number">2009SGR155</idno>
					<orgName type="project" subtype="full">IntellAct</orgName>
				</org>
				<org type="funded-project" xml:id="_JtqUStC">
					<idno type="grant-number">FP7-ICT2009-6-269959</idno>
					<orgName type="project" subtype="full">CHISTERA</orgName>
				</org>
				<org type="funded-project" xml:id="_gUywduE">
					<orgName type="project" subtype="full">VISEN</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3d Human Pose from Monocular Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial Structures Revisited: People Detection and Articulated Pose Estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3D Pose Estimation and Tracking by Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative Appearance Models for Pictorial Structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detailed Human Shape and Pose from Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Twin Gaussian Processes for Structured Prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tracking 3D Human Pose with Large Root Node Uncertainty</title>
		<author>
			<persName><forename type="first">B</forename><surname>Daubney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Discriminatively Trained, Multiscale, Deformable Part Model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pictorial Structures for Object Recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The CMA Evolution Strategy: a Comparing Review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards a new evolutionary computation. Advances on estimation of distribution algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="75" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian Reconstruction of 3D Human Motion from Single-Camera Video</title>
		<author>
			<persName><forename type="first">N</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advanced Dynamic Programming in Semiring and Hypergraph Frameworks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical Gaussian Process Latent Variable Models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic Exploration of Ambiguities for Non-Rigid Shape Recovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="475" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring Ambiguities for Monocular Non-Rigid Shape Estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7056</biblScope>
			<biblScope unit="page">7126</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m">Figure 9: Two sample frames from the TUD Stadtmitte sequence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relevant Feature Selection for Human Pose Estimation and Localization in Cluttered Images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching 3D Geometry to Deformable Part Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reconstructing 3D Human Pose from 2D Image Landmarks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to Parse Images of Articulated Bodies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Randomized Trees for Human Pose Detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining Discriminative and Generative Methods for 3D Deformable Surface and Articulated Pose Reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shared kernel information embedding for discriminative inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single Image 3D Human Pose Estimation from Noisy Observations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Inference with Multiple Heterogeneous Part Detectors for Human Pose Estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative Modeling for Continuous Non-Linearly Embedded Visual Inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast Globally Optimal 2D Human Detection with Loopy Graph Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D People Tracking with Gaussian Process Dynamical Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Articulated Pose Estimation with Flexible Mixtures-of-Parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human Motion Tracking by Temporal-Spatial Local Gaussian Process Experts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Image Proc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1141" to="1151" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
