<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition</title>
				<funder>
					<orgName type="full">National Institute for Health Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-03-24">24 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Enrique</forename><surname>Sanchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mani</forename><surname>Kumar Tellamekala</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
							<email>michel.valstar@nottingham.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>g.tzimiropoulos@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-24">24 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.13372v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or selfattention models to enforce temporal consistency, work on the feature level, ignoring the task-specific temporal dependencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Processes to propose a method for apparent emotion recognition with three key novel components: (a) probabilistic contextual representation with a global latent variable model; (b) temporal context modelling using task-specific predictions in addition to features; and (c) smart temporal context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we address the problem of facial behaviour recognition from video, in particular, the problem of recognising apparent emotions in terms of Valence and Arousal <ref type="bibr" target="#b50">[51]</ref>, and facial expressions in terms of Action Unit intensity <ref type="bibr" target="#b10">[11]</ref>. This is a longstanding problem in video recognition which has been extensively studied by the computer vision community <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref>. Nevertheless, even recent methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b77">77]</ref> struggle to achieve high accuracy on the most difficult datasets including SEWA <ref type="bibr" target="#b33">[34]</ref>, Aff-Wild2 <ref type="bibr" target="#b28">[29]</ref>, BP4D <ref type="bibr" target="#b75">[75]</ref> and DISFA <ref type="bibr" target="#b41">[42]</ref>. Hence, there is still great progress to be made towards solving this challenging problem. In this paper, we show that effective temporal context modelling is a key feature for significantly advancing the state-of-the-art. But what makes emotion (and facial expression) recognition such a difficult problem? There are many, often interrelated, reasons for this including: (a) annotation of emotions is often both subjective and laborious making it hard to annotate it consistently <ref type="bibr" target="#b56">[56]</ref>; (b) there exist only small-and medium-size emotion video datasets and, due to point (a) above, the annotations for these datasets are often noisy; (c) emotions are subtle, acting as unobserved, latent variables that only partially explain facial appearance over time <ref type="bibr" target="#b1">[2]</ref>. They often require temporal as well as multi-modal context in order to be robustly recognised. From these challenges, this paper focuses on solving problem (c). In particular, it proposes a completely unexplored perspective for effective incorporation of temporal context into emotion recognition.</p><p>Previous work in temporal modelling of emotions has primarily focused on modelling facial expression dynamics, i.e., the way that facial expressions evolve over time for recognition purposes. A typical deep learning pipeline for this has been a Convolution Neural Network (CNN) followed by a Recurrent Neural Network (RNN), usually an LSTM or GRU <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b71">71]</ref>. Although these dynamics can be particularly useful in recognising specific facial expressions (e.g. <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b70">70]</ref>), we believe that their importance has been over-emphasised for the problem of emotion recognition.</p><p>We posit that temporal context is more important than facial dynamics for emotion recognition. The cues for inferring a person's apparent emotion from their facial behaviour are often sparsely and non-regularly distributed over a temporal window of interest and collecting such distributed contextual cues is critical to inferring emotions robustly. In addition, due to the person-specific variability of facial expressions but, most importantly, due to their subjective annotation, context must be modelled in a stochastic manner.</p><p>To address the aforementioned challenges, in this work, and for the first time to the best of our knowledge, we build upon the framework of Neural Processes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> to propose a model for emotion recognition with 3 key components:</p><p>(1) stochastic contextual representation with a global latent variable model; <ref type="bibr" target="#b1">(2)</ref> task-aware temporal context modelling not only by using features but also task-specific predictions; and (3) effective temporal context selection.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> depicts an overview of the working flow our method (bottom), RNN-based methods (top), and methods based on self-attention <ref type="bibr" target="#b66">[66]</ref> (middle). Note methods for context modelling based on self-attention do not satisfy any of (1)-(3): They are deterministic and model long-term dependencies not globally, but by using pair-wise feature similarities. Moreover, they do not use task-aware context modelling and do not perform context selection.</p><p>Overall, we make the following 3 contributions:</p><p>1. We propose Affective Processes: the very first model for emotion recognition with three key properties: (a) global stochastic contextual representation; (b) taskaware temporal context modelling; and (c) temporal context selection. We conduct a large number of ablation studies illustrating the contribution of each of our model's key features and components.</p><p>2. We show that our model is more effective in modelling temporal context not only than CNNs+RNNs but also than a strong baseline based on self-attention. Our model outperforms both baselines by large margin.</p><p>3. We validated our approach on the most difficult databases for emotion recognition in terms of Valence and Arousal estimation, namely SEWA <ref type="bibr" target="#b33">[34]</ref> and Af-fWild2 <ref type="bibr" target="#b28">[29]</ref>. We further show that our approach can be effective for the problem of Action Unit intensity estimation on DISFA <ref type="bibr" target="#b41">[42]</ref> and BP4D <ref type="bibr" target="#b75">[75]</ref> datasets. On all datasets used, we show a consistent and often significant improvement over state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews related work from temporal apparent emotion recognition, Action Unit intensity estimation and neural modelling of Stochastic Processes.</p><p>Temporal Modelling in Emotion Recognition: Most existing methods model the temporal dynamics of continuous emotions using deterministic approaches such as Time-Delay Neural Networks <ref type="bibr" target="#b42">[43]</ref>, RNNs, LSTMs and GRUs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">59]</ref>, multi-head attention models <ref type="bibr" target="#b19">[20]</ref>, 3D Convolutions <ref type="bibr" target="#b76">[76,</ref><ref type="bibr" target="#b34">35]</ref>, 3D ConvLSTMs <ref type="bibr" target="#b18">[19]</ref>, and temporal-hourglass CNNs <ref type="bibr" target="#b9">[10]</ref>. While these deterministic models are capable of effectively learning the temporal dynamics, they do not take the inherent stochastic nature of the continuous emotion labels into account. Deviating from this trend, recently, <ref type="bibr" target="#b46">[47]</ref> applied variational RNNs <ref type="bibr" target="#b4">[5]</ref> to the valence estimation problem to add stochasticity to the temporal model of emotion recognition. Similarly, in <ref type="bibr" target="#b47">[48]</ref>, emotion is treated as a dynamical latent system state which is estimated using Gaussian processes coupled with Kalman filters, while Deep Extended Kalman filters are used in <ref type="bibr" target="#b48">[49]</ref>. These probabilistic methods have better latent structurediscovery capabilities than the the deterministic temporal models, however, they focus only on the temporal dynamics of emotions, ignoring the temporal context. In contrast, our method primarily focuses on directly learning the global temporal context in the input sequence using stochastic processes with data-driven 'priors'. AU Intensity Estimation: In order to model the subjectivevariability of the contextual factors in AU intensity estimation, several works advocate for probabilistic models <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b55">55]</ref>. Particularly, Gaussian Processes based models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and variational latent variable models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b68">68]</ref> have been extensively used for this purpose.</p><p>Other probabilistic approaches to facial affect modelling include Conditional Ordinal Random Fields (CORF) <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53]</ref> and Markov Random Fields (MRF) <ref type="bibr" target="#b55">[55]</ref>. To learn the task-specific temporal context, context-aware feature fusion and label fusion modules are proposed in <ref type="bibr" target="#b77">[77]</ref> to model AU intensity temporal dynamics. Although we also propose to learn the task-specific temporal context, our context learning method is completely different to that of <ref type="bibr" target="#b77">[77]</ref>.</p><p>Stochastic Process Modelling <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62]</ref>: Our method primarily builds on the recently proposed stochastic regression frameworks of Neural Processes (NPs). NPs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b37">38]</ref> are a family of predictive stochastic processes that aim to learn distributions over functions by adapting their priors to the data using context observations. Application of NPs in Computer Vision has been limited to basic image completion and super-resolution tasks <ref type="bibr" target="#b22">[23]</ref> as proof-of-concept experiments. To our knowledge, we are the first to show how NPs can be used to solve challenging real-world regression tasks like that of emotion recognition. The NP Encoder is an MLP q φ that computes an individual feature representation r c = q φ (x c , y c ) ∈ R d (d = 128 in our work) for each input context pair, followed by a permutation-invariant aggregation operation. We choose simple summation for aggregation to compute a global deterministic context representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><formula xml:id="formula_0">r C = 1 C c r c ∈ R d ,</formula><p>with C the number of context points. To account for the context stochasticity, r C is used to parameterise a Gaussian distribution from which a global latent variable z can be drawn. To this end, two linear layers are used to compute the mean µ C (r C ) and standard deviation σ C (r C ). The global latent variable z ∼ N (µ C , σ C ) ∈ R d represents a different realisation of the SP conditioned on the context.</p><p>The NP Decoder is an MLP f θ that samples functions f ∼ P through the latent variable z. For a given z, function samples are generated at all target points as y t = f θ (x t , z). Assuming some i.i.d. Gaussian observation noise with learnable variance <ref type="bibr" target="#b21">[22]</ref>, the output of f θ can be that of a mean and standard deviation of a Gaussian distribution:</p><formula xml:id="formula_1">y t ∼ N (f µ θ (x t , z), f σ θ (x t , z)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Affective Processes</head><p>Let us assume that we are given a sequence of images {I t } T t=1 , and that we want to estimate facial affect, in terms of Valence and Arousal, on a per-frame basis y t ∈ R 2<ref type="foot" target="#foot_0">foot_0</ref> . The framework of NPs cannot be applied to emotion recognition as is. Actually, application of NPs in Computer Vision has been limited to basic image completion and superresolution tasks <ref type="bibr" target="#b22">[23]</ref> as proof-of-concept experiments. One of our main contributions is to show how such a framework can be applied for challenging real-world temporal regression tasks like the one of emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Input Features and Pseudo-labels</head><p>The original NP formulation would directly operate on the sequence of images {I t }. Furthermore, it would require ground truth labels for the context points y c both at training and test time. While one can assume the ground-truth labels to be available for training, their need at test time would limit the application of NPs to real-world problems. To alleviate these issues, we firstly propose to train a backbone CNN, comprising a feature extractor Φ(.) and a classifier W, in a standard supervised manner, to provide independent per-frame estimates of Valence and Arousal ŷt ∈ R 2 .</p><p>The purpose of this network is two-fold: Firstly, the feature extractor Φ(.) is used to provide a per-frame feature representation x t = Φ(I t ). Then, we can use the sequence of features x t as input to the NP instead of the image sequence. Secondly, we propose to use the output of the classifier ŷt as pseudo-ground truth labels to be fed as input to the NP. These labels are noisy, and lack temporal consistency, yet they still provide some information regarding the true labels. In this paper, we choose as our backbone an architecture inspired by that of <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b73">73]</ref>. We refer the reader to the Supplementary Material for further details. Frozen Backbone: It is important to remark that Φ(.) and W are kept frozen during the training of our AP. While one could opt for finetuning the backbone while training the AP, the change in Φ(.) and W would lead to changing both the input space and the pseudolabels, leading to poorer performance. When minimising the AP objective (see below), W is no longer learned through the error of the values ŷ, and thus it can lead to produce pseudolabels that lie out of the space Y, i.e. that are not valid. Recall that NPs rely on estimating f (x t ) for the target points, provided some observed values y c = f (x c ) for the context. In this paper the values y c are replaced by the predicted values ŷc provided by W. If these values change over the course of training, and become non plausible values of f , the whole NP framework becomes invalid. We experimentally tried fine-tuning the backbone, and observed a poor performance.</p><p>Overall, to our knowledge, we are the first to show in NP literature that pseudo-ground truth generated from an independent network can be effectively used for defining the labels for the context points used as input to the NP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Context Selection</head><p>Our first and obvious option for context selection is uniform random sampling from the sequence, which would imply the strong assumption that the errors of the backbone are consistent. Considering that the backbone works in a perframe basis, it is likely that some of the input features are more indicative of the context than others. In this paper, we propose to delve into the properties of the NPs to investigate whether we can infer which frames are most optimal for context representation.</p><p>Given that the number of context points can be any number between 1 and the length of the sequence, the choice of r C = r c for a single context point c also results in a valid latent function. Building on this concept and the fact that σ C (r C ) represents the uncertainty of the latent functions, we propose to select the context by first estimating the set of latent functions that each of the individual representations r c would propose, and select those with lowest uncertainty. That is, we propose to first estimate r c and their corresponding σ C (r c ) for all the frames in a given sequence, and select the C with lowest uncertainty. The context set C is then defined as the C frames for which σ c (r c ) is the lowest. Note that this step is used to select the context points, only. Once context points are selected, they are passed through the aggregation function to compute r C in the standard way. This adds a negligible extra complexity to the framework (the values of r c do not need to be computed again). Finally, we emphasize that we apply context selection only at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The Complete Architecture</head><p>The complete architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We use as input the features x t extracted from the pre-trained backbone, as well as the corresponding predictions. The encoder q φ is a three-layer MLP that maps each of the input pairs (x c , ŷc ) into a latent representation r c ∈ R 128 . The input pseudolabels ŷt are firstly upsampled to match the dimension of the feature representation. Once r C has been computed from the average of the selected context points, it is forwarded to a latent encoder, which comprises a common linear layer and two independent linear layers, to compute µ C and σ C , respectively. The decoder f θ is another threelayer MLP that receives the individual features x t and the latent representation z, and produces two outputs ỹµ t and ỹσ t , parameterising the output distribution. During training, z ∼ N (µ C , σ C ), and ỹt ∼ N (ỹ µ t , ỹσ t ). At test time, we use the predictive mean and set z = µ C and ỹt = ỹµ t . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training Losses</head><p>Learning in our model comprises learning the parameters of the encoder q φ and the decoder f θ . For a given set of annotated sequences of varying length, a random subset of frames are used as context, and the learning is the task of reconstructing the labels for the target points. In the NPs, the goal is to maximise the evidence lower-bound (ELBO) of log p(y T |x T , x C , y C ). Because Valence and Arousal take values in [-1, 1], we also consider that the labels y are normally distributed around the origin, and propose a regularisation term for the output distribution as</p><formula xml:id="formula_2">L reg = D KL (N ( ỹµ t , ỹσ t ) N (0, 1)</formula><p>). The full objective to minimise is:</p><formula xml:id="formula_3">L = L nll + λ kl L kl + λ reg L reg ,<label>(1)</label></formula><p>with L nll = -E q φ (z|C) [log p(y T |x T , z)] and L kl = D KL (q φ (z|T ) q φ (z|C)). The optimisation can be efficiently done through the reparametrization trick <ref type="bibr" target="#b24">[25]</ref>. In this paper λ kl and λ reg are set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with other methods</head><p>It is important to remark that our model comes with the following benefits: a) it allows to automatically infer the global context; this adds a new key to existing Neural Processes where context is manually given, b) it allows to model the uncertainty on the estimated context, and c) it allows to draw coherent samples; note that while the decoder works on a per-frame basis, the fact that it is conditioned on a global latent variable whose value can be drawn from a latent distribution, makes all of the samples be dependent on it <ref type="bibr" target="#b6">[7]</ref>.</p><p>Vs. RNN: The task of temporal regression could be approached by using a parametric form for f , by means of an RNN. However, this assumes that the variables y t are drawn from Markov chains, which in practice disregard long-term dependencies needed for temporal context modelling. Furthermore, it can be shown <ref type="bibr" target="#b16">[17]</ref> that the joint distribution p(y T ) is exchangeable, i.e. it is permutation invariant. This offers a new insight into temporal modelling that differentiates from RNNs that condition a current observation on the previous hidden state, i.e. where order matters. We argue that, provided a given static image, the corresponding labels should not be position-dependent. If we shuffle a given sequence and pick up a frame at random, the corresponding annotation is not expected to change. Our approach seeks an orderless modelling of sequences, by looking at them as a whole through the global latent variable z. Vs. Self-attention: Self-attention models <ref type="bibr" target="#b66">[66]</ref> can also model long-term dependencies using pair-wise feature similarities. On the contrary, we choose NPs to estimate f by sampling from a global latent random variable. This allows for a more direct and holistic modelling of the temporal context. Furthermore, unlike self-attention models, in NPs both input signal x c , and labels y c are used to learn context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Performance Metrics</head><p>We validate our approach for emotion recognition on SEWA <ref type="bibr" target="#b33">[34]</ref> and Aff-Wild2 <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b74">74]</ref>. We also validate our approach for the task of Action Unit intensity estimation on BP4D <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b65">65]</ref> and DISFA <ref type="bibr" target="#b41">[42]</ref>.</p><p>SEWA <ref type="bibr" target="#b33">[34]</ref> is a large video dataset for affect estimation inthe-wild. It comprises over 2, 000 minutes of video data annotated with valence and arousal values. It contains subjects from 6 different cultures. We follow the evaluation protocol of <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref>, and apply a 8:1:1 dataset partition We follow a 3-fold cross-validation protocol with 18 subjects used for training and 9 subjects used for testing.</p><p>Performance Metrics: For Valence and Arousal, we follow the AVEC Challenge series <ref type="bibr" target="#b51">[52]</ref> and report the Concordance Correlation Coefficient <ref type="bibr" target="#b35">[36]</ref>. For Action Units, we follow the ranking criteria used in FERA challenges <ref type="bibr" target="#b65">[65]</ref>, and report the Intra Class Correlation (ICC <ref type="bibr" target="#b57">[57]</ref>). A full definition of these metrics can be found in the Supp. Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>All experiments are developed in PyTorch <ref type="bibr" target="#b49">[50]</ref>. First, we pre-train the backbone architecture Φ and W on a per-frame basis for each corresponding dataset (see Supplementary Material). We then use as input the features x t extracted from the pre-trained backbone, as well as the corresponding predictions. The dimension of input features x t is 512.</p><p>Valence and Arousal: To train the AP for Valence and Arousal, we randomly sample sequences between 35 and 70 frames from the corresponding training set. The range 35 -70 can be seen as an augmentation parameter: using a shorter range yielded worse results due to lack of generalisation, whereas no improvement was observed when using a longer range of sequences. The number of context points is randomly chosen from 3 to the length of the given sequence. During training, the target labels are the groundtruth annotations. For the context labels, we use the groundtruth annotations or the predictions from W with probability 0.5. The batch size is 16 videos, and the learning rate and weight decay are set to 0.00025 and 0.0001, respectively. The AP is trained for 25 epochs, each of 1000 iterations, using Adam <ref type="bibr" target="#b23">[24]</ref> with (β 1 , β 2 ) = (0.9, 0.999). We use a Cosine Annealing strategy for the learning rate <ref type="bibr" target="#b39">[40]</ref>.</p><p>At test time, we use a fixed sequence length of 70 frames.</p><p>Action Units: We used the same setting as for Valence and Arousal, with the following differences: the batch size is set to 6 videos, and the learning rate and weight decay are set to 0.0001 and 0.0005, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>We use the validation partition of SEWA to perform our ablation studies. We study the effect of our proposed context selection method, the contribution of the pseudo labels, the effect of the different losses used for the AP training, and the effect of the latent variable. Finally, we explore a different alternative to the AP configuration <ref type="bibr" target="#b36">[37]</ref>. We study in Sec. 6 the complexity of our approach. Effect of context selection: Firstly, we investigate the effect of the number of selected context points and the selection method proposed in Sec. 3.2. For a given trained model, we study the effect of choosing the context points (a) randomly, (b) according to the lowest values of σ c , and (c) according to the largest values of σ c . Fig. <ref type="figure" target="#fig_2">3</ref> shows that the best results are attained when choosing 40 context points according to the lowest value of σ c . We fix for the remaining experiments the number of context points to 40.</p><p>Impact of task-aware context modelling: One of the contributions of our method is that of task-aware context modelling in the form of pseudo-ground truth labels. We conducted an experiment where the AP model is trained without the predictions provided by W. The architecture, training, losses, learning rate and weight decay remain the same as for the main AP. The results, reported in Table <ref type="table" target="#tab_2">1</ref> as AP w/o W , show that the performance drops significantly without the aid of the noisy task-specific predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of global latent random variable:</head><p>We firstly investigate the effect of the latent variable on regressing coherent functions. Recall that our proposed approach consists of a probabilistic regression of coherent functions, i.e. it produces a set of functional proposals. An alternative to this approach is to use a non-probabilistic approach where the latent representation does not parameterise a latent random distribution, but is rather deterministic. The architecture of a deterministic AP is similar to that shown in Fig. <ref type="figure" target="#fig_1">2</ref> although with a single-branch latent encoder and no sampling. With a deterministic AP, one dispenses with the coherent sampling i.e. the dependence among the target outputs is lost, and assumed to be i.i.d. The learning burden is put only into capturing the observation variance ỹσ to account for the output uncertainty (i.e. it is probabilistic in the output, but not in the latent representation). The results of the deterministic AP are shown in Table <ref type="table" target="#tab_2">1</ref> (Deterministic AP). We observe that the performance degrades substantially, in particular regarding the Arousal predictions. This illustrates the importance of the latent variable to model the underlying variability for the joint prediction of Valence and Arousal. Finally, we also investigated the dimensionality of z, observing little effect when having more dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of losses:</head><p>The training of the APs is carried out by minimising the objective shown in Eqn. 1. Recent findings <ref type="bibr" target="#b14">[15]</ref> suggest that training an NP using only a maximum likelihood approach (i.e. by minimising the negative log-likelihood L nll ) can improve performance and avoid the practical issues of using amortised variational inference (i.e. L nll + L kl ). We studied the effect of each case along with our proposed loss. The results are shown in Table <ref type="table" target="#tab_2">1</ref>. We refer with AP nll to the maximum likelihood approach, with AP nll+kl to the one that minimises L nll + L kl , with AP nll+reg to the one that minimises L nll + L reg , and with AP nll+reg+kl to the one that minimises the full loss in Eqn. 1. The numerical results do not shed light into whether one is better than the other. We observe that a combination of the maximum likelihood approach with our proposed regularisation slightly improves the numerical results, although we acknowledge these not to be significant enough. If we use the three losses together we observe a performance degradation. These results align with recent findings that advocate for a further investigation of which of the losses is a better objective for Neural Processes <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different network configurations:</head><p>We also study different network configurations. We first evaluate the perfor- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparison with state-of-the-art</head><p>We compare the results of our models w.r.t. the most recent works in emotion and Action Unit intensity estimation. The bulk of our results are shown in Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_4">3</ref> for Valence and Arousal, and Tables <ref type="table" target="#tab_5">4</ref> and<ref type="table" target="#tab_6">5</ref> for Action Units (MSE is also reported in the Supplementary Material).</p><p>In-house baselines: We trained a set of baselines based on GRU <ref type="bibr" target="#b3">[4]</ref> and Attention <ref type="bibr" target="#b66">[66]</ref>, using, for each case, the same backbone as the one used in AP to provide the input features. We also optimised for each task and model the sequence length (herein denoted by L).  <ref type="table" target="#tab_3">2</ref><ref type="table" target="#tab_4">3</ref><ref type="table" target="#tab_5">4</ref><ref type="table" target="#tab_6">5</ref>indicate that our AP outperforms both recurrent and attentionbased models. We also experimented with using as input both the features and the labels, as well as finetuning the backbone, observing a performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different backbones:</head><p>We validate that our AP can be used with other backbone choices. On SEWA, we first train a ResNet-50, yielding a CCC of 0.567, and then train our AP on top yielding a CCC of 0.655. On AffWild-2, we train our AP using as backbone the publicly available model of <ref type="bibr" target="#b34">[35]</ref>: the backbone of <ref type="bibr" target="#b34">[35]</ref> reports a CCC score on the AffWild-2 test set of 0.43, whereas our AP yields a CCC score of 0.47.</p><p>Analysis: On SEWA, we compare against Mitekova et al. <ref type="bibr" target="#b43">[44]</ref> and Kossaifi et al. <ref type="bibr" target="#b32">[33]</ref>. Notably, our backbone is already on par with the state of the art results of <ref type="bibr" target="#b32">[33]</ref>. We can observe that our method based on temporal context offers the largest improvement. The same behaviour is observed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Valence Arousal Mean Mitekova et al. <ref type="bibr" target="#b43">[44]</ref> 0.44 0.39 0.42 Kossafi et al. <ref type="bibr" target="#b32">[33]</ref> 0  on AffWild2, where our AP significantly improves the Valence CCC. On DISFA, our backbone is again on par with the state of the art methods VGP-AE <ref type="bibr" target="#b11">[12]</ref> and 2DC <ref type="bibr" target="#b38">[39]</ref>. However, our AP brings the largest improvement setting a new state-of-the-art ICC of 0.58, which improves over the Heatmap Regression method of Ntinou et al. <ref type="bibr" target="#b45">[46]</ref>. Finally, we compare our method against the winner and runner-up methods of FERA 2015, ISIR <ref type="bibr" target="#b44">[45]</ref> and CDL <ref type="bibr" target="#b0">[1]</ref>, respectively, as well as against the Heatmap Regression method of <ref type="bibr" target="#b45">[46]</ref>. Our method sets up a new state-of-the-art result with an ICC score of 0.74. We include a set of qualitative results in the Supplementary Material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We proposed a novel temporal context learning approach to facial affect recognition, by modelling the stochastic nature of affect labels. To model uncertainty in the temporal context of the affective signals, our proposed method, called Affective Processes, builds on Neural Processes to learn a distribution of functions. Our method addresses a key limitation of the NPs -that of requiring ground-truth labels at inference time -by proposing the use of pseudo-labels provided by a pre-trained backbone and by using a novel method for automatic context selection. We believe that the key elements of our method pave the way to apply NPs to large-scale supervised learning problems which are inherently stochastic and for which deterministic temporal models like RNNs and self-attention could fall short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Backbone</head><p>This appendix is used to describe the backbone structure and training, left out of the main document due to lack of space. It also includes a formal definition of the main performance metrics used in the paper, as well as the Mean Squared Error reported for the Action Unit intensity estimation task. We first describe the architecture of the backbone (Sec. A.1), and then the training details for each of the databases used in the paper (Sec. A.2). Sec. B and Sec. C are devoted to describing the performance metrics used in the paper and to reporting the MSE results on DISFA and BP4D, respectively.</p><p>A.1. Architecture</p><p>The structure of the backbone for both Valence and Arousal and Action Unit recognition is depicted in Fig. <ref type="figure" target="#fig_6">5</ref>. Note that both are depicted in the same figure for the sake of clarity, although the corresponding subnetworks consisting of the Emotion Head or the Action Units Head are trained independently using the task specific datasets. Both networks share a common module, referred to as Face Alignment Module, which is pre-trained for the task of facial landmark localisation, and kept frozen for the subsequent training steps. For both Valence and Arousal and Action Unit estimation, the backbone is decomposed into three main components, namely a) Face Alignment Module, b) Taskspecific Feature Module, and c) Task-specific Head.</p><p>The Face Alignment Module is a lightweight version of the Face Alignment Network of <ref type="bibr" target="#b2">[3]</ref>. It starts with a 2d convolutional layer (referred to as Conv2d) and a set of 4 convolutional blocks (ConvBlock, depicted in Fig. <ref type="figure">6</ref>) that bring down the resolution of the input image from 256 to 64 and the number of channels from 3 to 128. This set of ConvBlocks is followed by an Hourglass, a four layer set of 128-channel ConvBlocks with skip connections, that aggregate the features at different spatial scales. The Hourglass is followed by another ConvBlock and two Conv2d layers that produce a set of 68 Heatmaps, corresponding to the position of the facial landmarks. In this paper, rather than using the facial landmarks to register the face, we directly concatenate the produced features at both an early and late stage of the network with the Heatmaps. The output is then a 128 + 128 + 68 tensor of 64 × 64, resulting from concatenating the features computed after the fourth Con-vBlock, the features computed after the last ConvBlock, and the produced Heatmaps. This way, the Heatmaps help the subsequent network locally attend to the extracted coarse and fine features <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b73">73]</ref>. The benefits of this approach are twofold: a) it dispenses with the need of registering the faces according to detected landmarks, and b) because of a) we can directly use the features from the Face Alignment Network and have shallower networks in the front-end for the subsequent tasks.</p><p>The Task specific Feature Module consists of a mere set of 4 ConvBlocks, each followed by a max pooling layer, that produce a tensor of 128 × 4 × 4. To form the features x t that will be used as input to our AP network, we further downsample that tensor through an average pooling operation with a 2 × 2 kernel. The 128 × 2 × 2 output is flattened to form the 512-d feature vector x t .</p><p>The Task specific head for Valence and Arousal is composed of four independent Conv2d layers, each with 4 × 4 filters (i.e. equal to the spatial resolution of the input tensor). The first Conv2d layer is the corresponding Valence and Arousal classifier W mentioned in the main document. The output of this layer is a 2-d vector ŷt , corresponding to the values of Valence and Arousal, respectively. In order to boost the performance of the network for the task of predicting the continuous values of Valence and Arousal (ŷ), we approach the backbone training in a Multi-task manner (see below), where the goal is to also classify the basic (discrete) emotion, as well as the bin where both Valence and Arousal would lie in a discretised space. For the basic emotion (happiness, sadness, fear, anger, surprise, disgust and neutral), we include a second Conv2d which outputs the logits corresponding to each of the 7 target classes. For the discretised Valence (v) and Arousal (â), we use two Conv2d layers with 20 outputs each, i.e. we discretise the continuous space in 20 bins, and we treat the task of predicting the corresponding bin as a classification task (see below). Note that these extra heads, as well as the emotion head, are used to reinforce the learning of the regression head tasked with predicting ŷ. Once the network is trained, the heads corresponding to the discrete emotion and the discretised Valence and Arousal are removed from the backbone.</p><p>The Task specific head for Action Unit intensity estimation is also composed of 4 ConvBlocks as for the Valence and Arousal head. The output features, a tensor of 128 × 4 × 4 are also spatially downsampled with average pooling and flattened to form the input features to the AP x t . The Action Units classifier W is a Conv2d with a 4 × 4 filter that maps the 128 × 4 × 4 into either the 5 or 12 target AUs, for BP4D and DISFA, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training</head><p>Data processing The faces are first cropped according to a face bounding box, provided by the off-the-shelf face detector RetinaFace <ref type="bibr" target="#b8">[9]</ref>. Given that the first block of the backbone is a Face Alignment Network that is used to provide the features to the subsequent networks, no face registration step is applied. During training, for image augmentation we applied random cropping (224×224), random horizontal flipping, random rotation(-20 • to +20 • ), color jittering, and random gray scaling operations.  <ref type="figure">6</ref>, our ConvBlocks are of 128 channels, rather than the original 256 used in <ref type="bibr" target="#b2">[3]</ref>. The Face Alignment Network is pre-trained and kept frozen, and returns a set of features resulting from concatenating the output of the last ConvBlock before the Hourglass, the output of the last ConvBlock of the network, and the produced Heatmaps. Then, the Emotion and Action Unit heads follow for each corresponding task. Both have a similar Feature Extraction Module, composed of ×4 ConvBlocks followed by Average Pooling. The output of this module is a 128 × 4 × 4 tensor, which is used as input to the corresponding classifiers, as well as to compute the final feature representation xt that will be used along with ŷt as input to our proposed AP.</p><p>Face Alignment Network The Face Alignment Module was trained on the 300W-LP dataset <ref type="bibr" target="#b2">[3]</ref> using standard Heatmap Regression, and was kept frozen afterwards.</p><p>Valence and Arousal For Valence and Arousal, the network is trained in a Multi-task way. Let ŷ = (ŷ v , ŷa ) be the Valence and Arousal prediction, ê ∈ R 7 be the output of the discrete emotion layer, and v ∈ R 20 and â ∈ R 20 the output of the Valence and Arousal classes, respectively. We denote by y and e the corresponding Valence and Arousal and Emotion ground-truth values. The loss is defined as:</p><formula xml:id="formula_4">L =λ mse L mse + λ ccc L ccc + λ xent-emo L xent-emo + λ xent-va L xent-va<label>(2)</label></formula><p>where L mse = ŷy is the standard MSE loss for Valence and Arousal, L ccc = 1 -CCC(ŷv,yv)+CCC(ŷa,ya) The values of the loss weights are all set to 1 except for the MSE loss that is set to λ mse = 0.5. For both SEWA and AffWild2, the training is performed for 20 epochs, using Adam with learning rate 0.0001, (β 1 , β 2 ) = (0.9, 0.999) and weight decay 0.000001. The learning rate is reduced by a factor of 10 after every 5 epochs. For AffWild2 we used the sequences that were annotated with both discrete emotion and Valence and Arousal. Considering that SEWA has not been annotated with the basic emotions, we train our SEWA backbone by extending it with the sequences of AffWild2 containing such annotations. We backpropagate w.r.t. the emotion head using images from AffWild2, and w.r.t. the remaining heads using only images from SEWA. We apply the same 8:1:1 partition described in the paper, and choose the backbone according to the best validation CCC score.</p><p>Action Units For Action Unit intensity estimation, Mean Squared Error is used as the loss function to train the corresponding models in this work (for BP4D and DISFA). The AU intensities are normalised from -1 to 1 to align with the L reg used in the AP framework described in the main document. Adam optimizer with a learning rate of 0.0003, (β 1 , β 2 ) = (0.9, 0.999), and an L2 weight decay of 0.00001 is used to train the Action Unit head. To tune the initial learning rate, cyclic learning rate scheduler with a cycle length of 2 is used. After 80 epochs, the best model is selected based on the ICC score on the validation set.</p><p>For BP4D, the model is trained using the official train/validation/test partitions. For DISFA, the model is trained using the three-fold cross validation method described in the main document, using exactly the same generated partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Metrics</head><p>For Valence and Arousal, we report the Concordance Correlation Coefficient <ref type="bibr" target="#b35">[36]</ref>, which is used to rank participants in the AVEC Challenge series <ref type="bibr" target="#b51">[52]</ref>. It is a global measure of both correlation and proximity, and is defined as:</p><formula xml:id="formula_5">CCC(y, ŷ) = 2σ y σ ŷρ yŷ σ 2 y + σ 2 ŷ + (µ y -µ ŷ) 2 ,<label>(3)</label></formula><p>where µ, σ, and ρ refer to the mean value, (co-)variance, and Pearson Correlation Coefficient, respectively. For Action Unit intensity, we follow the standard ranking criteria used in FERA challenges <ref type="bibr" target="#b65">[65]</ref>, and we report the Intra Class Correlation (ICC <ref type="bibr" target="#b57">[57]</ref>). For an AU j with ground-truth labels {y j i } N i=1 , and predictions {ỹ j i } N i=1 , the ICC score is defined as ICC j = W j -S j W j +S j , with W j = 1 N i (y j i -ŷj ) 2 + (ỹ j i -ŷj ) 2 , S j = i (y j i -ỹj i ) 2 , and ŷj = 1 2N i (y j i + ỹj i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mean Squared Error Results</head><p>The additional Mean Squared Error results for DISFA and BP4D are reported in Table <ref type="table">6</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Different ways of modelling temporal context in the literature of emotion recognition: Recurrent Models (top), Attentionbased Models (middle), and our proposed approach based on Neural Processes (bottom). Our method proposes a probabilistic modelling of temporal context through a global latent variable z ∼ N (µ, σ), and includes a novel method for automatic context selection. The global latent variable allows to sample temporal functions f θ (., z) that are consistent with the captured context.</figDesc><graphic coords="1,315.61,259.59,222.75,227.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Architecture of AP. The encoder takes as input individual pairs (xc, ŷc), to produce the corresponding feature representations rc. The representations rc for the context are summarised into rC , and passed through a latent encoder that outputs the latent distribution, defined as a Gaussian parameterised by µ(r) and σ(r). The decoder takes a feature representation xt from the target set, as well as a sample from the latent distribution, and produces an output Gaussian distribution parameterised by the outputs (ỹ µ t , ỹσ t ) of the decoder. The numbers represent the dimensions of the linear layers.</figDesc><graphic coords="4,308.86,72.00,237.60,237.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Accuracy on SEWA validation w.r.t. the number of context points and context selection method. The results in red correspond to the accuracy obtained for Valence (V), and those in green for Arousal (A). ↓ (dashed and dotted lines) represents the results obtained by selecting the context frames according to the lowest values of σc , and ↑ (solid lines with diamond markers) represents the results obtained by selecting the frames according to the largest values of σc and, rnd (solid lines with cross markers) represents the results obtained by choosing the frames randomly. The dashed horizontal lines represent the results of the backbone.</figDesc><graphic coords="6,56.86,72.00,222.75,147.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the lowest values of σc . Blue line represents the ground-truth labels. The green line represents the predictions from the backbone. The black dots represent the frames selected as context using σc. The light red curves represent different realisations of f (., z) for 10 different (sampled) z, and the red curve represents the process returned by the predictive mean µC . The variation in the functions corresponds to the global latent uncertainty given the selected context.</figDesc><graphic coords="7,52.59,72.00,490.07,156.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For Valence and Arousal, we used a 4-layer Convolutional GRU with 128-d hidden layers, and L = 30. For Action Units, we trained a 2-layer Bidirectional GRU with 256-d hidden states, with L = 60 frames. For Attention, we used, for both tasks, an architecture based on the Transformer encoder: we used a 2-layer MultiHead Self-Attention encoder with 32 learnable heads, each of 512 dimensions. The sequence lengths are L = 30 and L = 40 frames for Valence and Arousal and Action Units, respectively. The results in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Complexity:</head><label></label><figDesc>Our AP adds very little complexity to the pre-trained backbone. The configuration shown in Fig. 2 comprises only ∼330K parameters, and requires only ∼20 MFLOPs for a given sequence of L = 70 frames 3 . In com-parison, the ConvGRU model has ∼789K parameters, and requires ∼758 MFLOPs for L = 30 frames. The PyTorch built-in 2-layer BiGRU has ∼ 1.2M parameters, and needs ∼71 MFLOPs for L = 60 frames. The Multi-Head attention model has ∼4.2M parameters, and requires ∼125 MFLOPs for L = 30 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion Head and Action Units Head are depicted together, despite these being different networks, trained separately. The grey modules represent 2-d convolutions (Conv2d), whereas the blue blocks represent Convolutional Blocks (ConvBlocks), described in Fig.5. The * 64 inscribed in the first ConvBlock corresponds to a slightly different configuration that uses a skip connection to upsample the number of channels from 64 to 128. The backbone includes a Face Alignment Network, an Hourglass-like architecture that takes the input image I, and produces a set 68 Heatmaps corresponding to the position of the facial landmarks. The Hourglass comprises four layers of ConvBlocks with downsampling, and skip connections (for the sake of clarity we illustrate three layers, where each smaller block corresponds to halving the spatial resolution). As shown in Fig.6, our ConvBlocks are of 128 channels, rather than the original 256 used in<ref type="bibr" target="#b2">[3]</ref>. The Face Alignment Network is pre-trained and kept frozen, and returns a set of features resulting from concatenating the output of the last ConvBlock before the Hourglass, the output of the last ConvBlock of the network, and the produced Heatmaps. Then, the Emotion and Action Unit heads follow for each corresponding task. Both have a similar Feature Extraction Module, composed of ×4 ConvBlocks followed by Average Pooling. The output of this module is a 128 × 4 × 4 tensor, which is used as input to the corresponding classifiers, as well as to compute the final feature representation xt that will be used along with ŷt as input to our proposed AP.</figDesc><graphic coords="13,74.86,72.00,445.51,279.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2</head><label>2</label><figDesc>is the CCC score between the predicted Valence and Arousal values and corresponding ground-truth, L xent-emo is the standard cross entropy loss between the predicted emotion ê and the corresponding ground-truth e. We defineL xent-va = L xent-v + L xent-a ,with L xent-v the cross entropy loss between the 20-d output of the Valence head and the corresponding ground-truth bin, and equivalently L xent-a for Arousal. The ground-truth bin results from uniformly discretising the Valence and Arousal spaces, which lie within the [-1, 1] space, into 20 bins each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>30 Table 6 .Figure 6 .</head><label>3066</label><figDesc>Figure 6. The Convolutional Block (ConvBlock), used in [3]. Instead of using 256 channels, we opt for a lighter version and choose 128 channels instead.</figDesc><graphic coords="14,69.24,211.44,198.00,162.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 . AffWild2<ref type="bibr" target="#b26">[27]</ref> is a large-scale video dataset divided in training, validation, and test partitions. Because the labels for the latter are not made publicly available, we use the training and validation only for our evaluation purposes.BP4D<ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b65">65]</ref> contains videos collected from 41 subjects, performing 8 different tasks. It is the main corpus of FERA 2015 challenge<ref type="bibr" target="#b65">[65]</ref>. We use the official train (168 videos), validation (160 videos), and test (159 videos) partitions, which contain annotations for 5 different AUs. DISFA<ref type="bibr" target="#b41">[42]</ref> contains 27 videos of 27 different subjects, with 4,844 frames each. It contains annotations for 12 AUs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>, Ablation studies on the validation set of SEWA.</figDesc><table><row><cell>Model</cell><cell cols="3">Valence Arousal Mean</cell></row><row><cell>Backbone</cell><cell>0.576</cell><cell>0.632</cell><cell>0.604</cell></row><row><cell>AP nll</cell><cell>0.645</cell><cell>0.676</cell><cell>0.660</cell></row><row><cell>AP nll+kl</cell><cell>0.649</cell><cell>0.669</cell><cell>0.659</cell></row><row><cell>AP nll+reg</cell><cell>0.649</cell><cell>0.676</cell><cell>0.662</cell></row><row><cell>AP nll+reg+kl</cell><cell>0.643</cell><cell>0.628</cell><cell>0.635</cell></row><row><cell>Deterministic AP</cell><cell>0.650</cell><cell>0.576</cell><cell>0.613</cell></row><row><cell>AP + Det.</cell><cell>0.649</cell><cell>0.658</cell><cell>0.652</cell></row><row><cell>AP w/o W</cell><cell>0.600</cell><cell>0.454</cell><cell>0.527</cell></row><row><cell>AP + Det. + Att</cell><cell>0.662</cell><cell>0.672</cell><cell>0.667</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison against state of the art methods on the SEWA test partition. † denotes in-house implementation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="3">Valence Arousal Mean</cell></row><row><cell></cell><cell>.75</cell><cell>0.52</cell><cell>0.64</cell><cell>Ours Backbone</cell><cell>0.398</cell><cell>0.445</cell><cell>0.422</cell></row><row><cell>Ours backbone</cell><cell>0.69</cell><cell>0.61</cell><cell>0.65</cell><cell>ConvGRU [4]  †</cell><cell>0.398</cell><cell>0.503</cell><cell>0.450</cell></row><row><cell>ConvGRU [4]  †</cell><cell>0.72</cell><cell>0.62</cell><cell>0.67</cell><cell>Self-Attn [66]  †</cell><cell>0.419</cell><cell>0.505</cell><cell>0.462</cell></row><row><cell>Self-Attn [66]  †</cell><cell>0.70</cell><cell>0.61</cell><cell>0.65</cell><cell>Ours AP</cell><cell>0.438</cell><cell>0.498</cell><cell>0.468</cell></row><row><cell>Ours AP</cell><cell>0.75</cell><cell>0.64</cell><cell>0.69</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different methods on the validation set of AffWild2. † denotes in-house implementation. † 0.30 0.17 0.78 0.76 0.51 0.64 0.82 0.46 0.59 0.17 0.92 0.68 0.57 Ours AP 0.35 0.19 0.78 0.73 0.52 0.65 0.81 0.49 0.61 0.28 0.92 0.67 0.58</figDesc><table><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell>15</cell><cell>17</cell><cell>20</cell><cell>25</cell><cell>26</cell><cell>Avg.</cell></row><row><cell cols="14">VGP-AE [12] 0.48 0.47 0.62 0.19 0.50 0.42 0.80 0.19 0.36 0.15 0.84 0.53 0.46</cell></row><row><cell>2DC [39]</cell><cell cols="13">0.70 0.55 0.69 0.05 0.59 0.57 0.88 0.32 0.10 0.08 0.90 0.50 0.50</cell></row><row><cell>HR [46]</cell><cell cols="13">0.56 0.52 0.75 0.42 0.51 0.55 0.82 0.55 0.37 0.21 0.93 0.62 0.57</cell></row><row><cell cols="14">Ours backbone 0.25 0.15 0.76 0.77 0.50 0.62 0.82 0.44 0.58 0.22 0.93 0.69 0.56</cell></row><row><cell>BiGRU [4]  †</cell><cell cols="13">0.23 0.13 0.77 0.70 0.53 0.64 0.82 0.42 0.58 0.25 0.92 0.69 0.56</cell></row><row><cell>Self-Attn [66]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison against state of the art methods on the DISFA database (in ICC values). † denotes in-house evaluation</figDesc><table><row><cell>AU</cell><cell>6</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>17</cell><cell>Avg.</cell></row><row><cell>CDL [1]</cell><cell cols="6">0.69 0.73 0.83 0.50 0.37 0.62</cell></row><row><cell>ISIR [45]</cell><cell cols="6">0.79 0.80 0.86 0.71 0.44 0.72</cell></row><row><cell>HR [46]</cell><cell cols="6">0.82 0.82 0.80 0.71 0.50 0.73</cell></row><row><cell cols="7">Ours backbone 0.76 0.74 0.82 0.62 0.39 0.67</cell></row><row><cell>BiGRU [4]  †</cell><cell cols="6">0.78 0.76 0.83 0.62 0.50 0.70</cell></row><row><cell cols="7">Self-Attn [66]  † 0.80 0.78 0.79 0.60 0.42 0.68</cell></row><row><cell>Ours AP</cell><cell cols="6">0.82 0.80 0.86 0.69 0.51 0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Results on the test partition of BP4D dataset (in ICC values).</figDesc><table /><note><p>† denotes in-house evaluation</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>and Table 7. Results on the test partition of BP4D dataset (in MSE values) † denotes in-house evaluation</figDesc><table><row><cell>AU</cell><cell>6</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>17</cell><cell>Avg.</cell></row><row><cell>ISIR [45]</cell><cell cols="6">0.83 0.80 0.62 1.14 0.84 0.85</cell></row><row><cell>HR [46]</cell><cell cols="6">0.68 0.80 0.79 0.98 0.64 0.78</cell></row><row><cell cols="7">Ours backbone 0.80 0.87 0.74 1.23 0.89 0.90</cell></row><row><cell>BiGRU [4]  †</cell><cell cols="6">0.79 0.85 0.76 1.19 0.78 0.87</cell></row><row><cell cols="7">Self-Attn [66]  † 0.82 0.88 0.70 1.22 0.80 0.88</cell></row><row><cell>Ours AP</cell><cell cols="6">0.72 0.84 0.60 1.13 0.57 0.77</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For our experiments, we will also consider the task of Action Unit intensity estimation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The same partitions were kindly provided by the data owners.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We measured the complexity using the Thop library for PyTorch https://github.com/Lyken17/pytorch-OpCounter</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The work by <rs type="person">Michel Valstar</rs> was part-funded by the <rs type="funder">National Institute for Health Research</rs>. The views represented are the views of the authors alone and do not necessarily represent the views of the <rs type="affiliation">Department of Health in England</rs>, <rs type="institution">NHS</rs>, or the <rs type="institution">National Institute for Health Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleix</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">D</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science in the public interest</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pairwise conditional random forests for facial expression recognition</title>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Severine</forename><surname>Dubuisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3783" to="3791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">La prévision: ses lois logiques, ses sources subjectives</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finetti</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;institut Henri Poincaré</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask emotion recognition with incomplete labels</title>
		<author>
			<persName><forename type="first">Didan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="828" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><surname>Retinaface</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Singlestage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatio-temporal encoder-decoder fully convolutional network for video-based dimensional emotion recognition</title>
		<author>
			<persName><forename type="first">Zhengyin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suowei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational gaussian process autoencoder for ordinal prediction of facial action units</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peter Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2, 8, 14</date>
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gaussian process domain experts for modeling of facial affect</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peter Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4697" to="4711" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metalearning stationary stochastic process prediction with convolutional neural processes</title>
		<author>
			<persName><surname>Andrew Yk Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eslami</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural processes</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional conditional neural processes</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew Yk</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end continuous emotion recognition from video using 3d convlstm networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangyan</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6837" to="6841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient modeling of long temporal contexts for continuous emotion recognition</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Affec. Comput. and Intel. Inter</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="191" />
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Appl. of Comput. Vis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal ; In</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentive neural processes</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognition of affect in the wild using deep neural networks</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2005">2017. 1, 2, 5</date>
			<biblScope unit="page" from="1972" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Analysing affective behavior in the first abaw 2020 competition</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Attila</forename><surname>Schulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elnar</forename><surname>Hajiyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11409</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2005">2019. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Aff-wild2: Extending the aff-wild database for affect recognition</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07770</idno>
		<imprint>
			<date type="published" when="2005">2018. 1, 2, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A multi-task learning &amp; generation framework: Valence-arousal, action units &amp; primary expressions</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07771</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04855</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stefanos</surname></persName>
		</author>
		<author>
			<persName><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factorized higher-order cnns with an application to spatio-temporal emotion estimation</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008">June 2020. 1, 5, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sewa db: A rich database for audio-visual emotion and sentiment research in the wild</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedhas</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjoern</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2005">2019. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream auralvisual affect analysis in the wild</title>
		<author>
			<persName><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName><surname>Rumberg</surname></persName>
		</author>
		<author>
			<persName><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A concordance correlation coefficient to evaluate reproducibility</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Empirical evaluation of neural process objectives</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst. Worksh</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bootstrapping neural processes</title>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcoder: Semiparametric variational autoencoders for automatic facial action coding</title>
		<author>
			<persName><forename type="first">Linh</forename><surname>Dieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017">2017. 1, 2, 8, 14</date>
			<biblScope unit="page" from="3190" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial actions: A survey</title>
		<author>
			<persName><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mohammad Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Time-delay neural network for continuous emotional dimension prediction from facial expression sequences</title>
		<author>
			<persName><forename type="first">Hongying</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkuang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cosmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="916" to="929" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Valence and arousal estimation in-the-wild with tensor methods</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Mitenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facial action unit intensity prediction via hard multi-task metric learning for kernel regression</title>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A transfer learning approach to heatmap regression for action unit intensity estimation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ntinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021. 1, 3, 8, 12, 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling emotion in complex stories: the stanford emotional narratives dataset</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Xuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Reddan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Kahhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><surname>Mattek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamil</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Leveraging the bayesian filtering paradigm for vision-based facial affective state estimation</title>
		<author>
			<persName><forename type="first">Meshia</forename><surname>Cédric Oveneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Enescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hichem</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="477" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Leveraging the deep learning paradigm for continuous affect estimation from facial expressions</title>
		<author>
			<persName><forename type="first">Meshia</forename><surname>Cédric Oveneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ercheng</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><forename type="middle">Dìaz</forename><surname>Berenguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hichem</forename><surname>Sahli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affect. Comput</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autodiff workshop -NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Development and psychopathology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">715</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leili</forename><surname>Tavabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Alisamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva-Maria</forename><surname>Messner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop</title>
		<meeting>the 9th International on Audio/Visual Emotion Challenge and Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kernel conditional ordinal random fields for temporal segmentation of facial action units</title>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012. 1, 2</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation</title>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<author>
			<persName><surname>Comput</surname></persName>
		</author>
		<author>
			<persName><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recog</title>
		<imprint>
			<date type="published" when="2012">2012. 1, 2</date>
			<biblScope unit="page" from="2634" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Markov random field structures for facial action unit intensity estimation</title>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Worksh</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Vidhyasaharan</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00360</idno>
		<title level="m">The ambiguous world of emotion representation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Intraclass correlations: uses in assessing rater reliability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sequential neural processes</title>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngsung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10254" to="10264" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporally coherent visual representations for dimensional affect recognition</title>
		<author>
			<persName><forename type="first">Mani</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tellamekala</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Estimation of continuous valence and arousal levels from faces in naturalistic conditions</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Georgios Tzimiropoulos, and Maja Pantic</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep structured mixtures of gaussian processes</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2251" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mixtures of gaussian processes</title>
		<author>
			<persName><forename type="first">Tresp</forename><surname>Volker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="654" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fully automatic facial action unit detection and temporal analysis</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="149" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Timing is everything: A spatiotemporal approach to the analysis of facial actions</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PhD Dissertation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">Timur</forename><surname>Michel F Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017. 2, 5, 7, 8, 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep structured learning for facial action unit intensity estimation</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Variable-state latent conditional random fields for facial expression recognition and action unit detection</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Walecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exact gaussian processes on a million data points</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adv. Neural Inform. Process. Syst.</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14648" to="14659" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A novel dynamic model capturing spatial and temporal patterns for facial expression analysis</title>
		<author>
			<persName><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Shi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019-02">2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Ast-net: An attributebased siamese temporal network for real-time emotion recognition</title>
		<author>
			<persName><forename type="first">Shu-Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiou-Ting</forename><surname>Hsu</surname></persName>
		</author>
		<editor>Brit. Mach. Vis</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fanface: a simple orthogonal improvement to deep face recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Aff-wild: Valence and arousal &apos;in-the-wild&apos;challenge</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Jeffrey F Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2005">2014. 1, 2, 5</date>
		</imprint>
	</monogr>
	<note>Bp</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">M3f: Multi-modal continuous valence-arousal estimation in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. and Workshops on Auto. Face and Gesture Recog</title>
		<imprint>
			<biblScope unit="page" from="617" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Context-aware feature and label fusion for facial action unit intensity estimation with partially labeled data</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
