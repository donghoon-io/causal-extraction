<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Disentangled Semantic Representation for Domain Adaptation</title>
				<funder ref="#_qR6mtuX">
					<orgName type="full">NSFC-Guangdong Joint Found</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Air Force</orgName>
				</funder>
				<funder ref="#_3vsjh37 #_dP9tDYk">
					<orgName type="full">United States Air Force</orgName>
				</funder>
				<funder ref="#_RDUNcPz">
					<orgName type="full">Pearl River S&amp;T Nova Program of Guangzhou</orgName>
				</funder>
				<funder ref="#_MVf2Zqx #_KqPDxkT #_e5RrdzS">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">ADSC</orgName>
				</funder>
				<funder ref="#_agNqQMh #_66sVU6e">
					<orgName type="full">Natural Science Foundation of Guangdong</orgName>
				</funder>
				<funder ref="#_R5FAXa7">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_pTwpZzb">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
							<email>cairuichu@gdut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computers</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computers</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computers</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
							<email>zfhao@gdut.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Mathematics and Big Data</orgName>
								<orgName type="institution">Foshan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Disentangled Semantic Representation for Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation is an important but challenging task. Most of the existing domain adaptation methods struggle to extract the domain-invariant representation on the feature space with entangling domain information and semantic information. Different from previous efforts on the entangled feature space, we aim to extract the domain invariant semantic information in the latent disentangled semantic representation (DSR) of the data. In DSR, we assume the data generation process is controlled by two independent sets of variables, i.e., the semantic latent variables and the domain latent variables. Under the above assumption, we employ a variational auto-encoder to reconstruct the semantic latent variables and domain latent variables behind the data. We further devise a dual adversarial network to disentangle these two sets of reconstructed latent variables. The disentangled semantic latent variables are finally adapted across the domains. Experimental studies testify that our model yields state-of-the-art performance on several domain adaptation benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain adaptation is an important but challenging task. Since the acquisition of a large labeled data is usually either expensive or impractical, how to train on the unlabeled target domain with the help of labeled source domain has become a particular focus. However, this learning scheme suffers from a well-known phenomenon named domain shift, leading an urging motivation in building an adaptive classifier that can efficiently transfer the source labeled data under the domain shift, this problem is also known as unsupervised domain adaptation.</p><p>An essential approach in unsupervised domain adaptation is to understand what the domain-invariant representation across the domains is and how to find it <ref type="bibr" target="#b13">[Zhang et al., 2013;</ref><ref type="bibr" target="#b12">Pan et al., 2011;</ref><ref type="bibr" target="#b8">Gong et al., 2012]</ref>. Typical methodologies explored in the literature include the feature alignment approaches that extract domain-invariant representation by minimizing the discrepancy between the feature distributions inside deep feed-forward architectures <ref type="bibr" target="#b12">[Tzeng et al., 2014;</ref><ref type="bibr" target="#b10">Long et al., 2015;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b10">2017]</ref>, and the adversarial learning approaches that extract the representation by deceiving the domain discriminators <ref type="bibr" target="#b12">[Tzeng et al., 2015;</ref><ref type="bibr">Ganin and Lempitsky, 2015;</ref><ref type="bibr" target="#b8">Ganin et al., 2016;</ref><ref type="bibr" target="#b11">Long et al., 2018]</ref>. Recently, a fine-grained semantic alignment has also been proposed in order to extract domain-invariant representation under the consideration of the semantic information <ref type="bibr" target="#b13">[Xie et al., 2018;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr">Chen et al., 2018]</ref>. However, most of them require target pseudo labels in order to minimize the discrepancies across domains within the same labels, and thus resulting the error accumulation due to the uncertainty of the pseudo-labeling accuracy.</p><p>Due to the complex manifold structures underlying the data distributions, these methods mainly suffer a false alignment problem <ref type="bibr" target="#b12">[Pei et al., 2018;</ref><ref type="bibr" target="#b13">Xie et al., 2018]</ref>. As shown in Figure <ref type="figure">1</ref> <ref type="bibr">(a)</ref>, the data generation process is controlled by the disentangled domain latent variables and semantic latent variables in the latent manifold. The ideal semantic position of the two types of labels -pig and hair drier -should be placed relatively upper and lower on the manifold according to the semantic axis, and the ideal domain position within the same labels should be placed in the left and right according to the domain axis. However, as shown in Figure <ref type="figure">1</ref>(b), once the domain information is not completely removed, samples are distorted on the feature manifold, leading to the false alignment problem, e.g., the Peppa Pig looks like a pink hair-drier and thus the feature of the Peppa Pig might be near to that of the hair-drier in the distorted feature manifold.</p><p>Motivated by the example of Figure <ref type="figure">1</ref>(b), the underlying cause of the false alignment problem is the entanglement of the semantic and domain information. More specifically, samples are controlled by two sets of independent latent variables z y and z d . However, these two sets of latent variables are highly tangled and distorted on the high dimensional feature manifold space. It is very challenging to remove the domain information while preserving the semantic information on a complex tangled feature manifold space.</p><p>In this work, motivated by the disentanglement property of the multiple explanatory factors in the representation learning literature <ref type="bibr">[Bengio et al., 2013;</ref><ref type="bibr" target="#b7">Dinh et al., 2014]</ref>, we propose a Disentangle Semantic Representation learning model (DSR in short) by assuming the independence between the semantic variables z y and the domain variables z d . Our DSR reconstructs the disentangled latent space and simultaneously <ref type="bibr">(a)</ref> Latent manifold of the data generation. uses the semantic variables to predict the target labels. The underlying intuition, as shown in Figure <ref type="figure">1</ref>(c), is that by using the disentangle semantic latent variables that are independent to the domain latent variables, we can easily classify the labels into two categories merely based on the semantic axis z y . We employ a variational auto-encoder to reconstruct the disentangled semantic and domain latent variables, with the help of a dual adversarial network. The extensive experimental studies demonstrate that DSR outperforms state-of-the-art unsupervised domain adaptation methods on standard domain adaptation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep feature learning methods have been shown very effective for unsupervised domain adaptation. The key idea of the deep feature learning is to extract domain-invariant representation by aligning different domains. Some works utilize maximum mean discrepancy(MMD) to realize the domain alignment. <ref type="bibr" target="#b12">[Tzeng et al., 2014]</ref> learns domain-invariant representation by adding an adaptation layer and an additional domain confusion loss; <ref type="bibr" target="#b10">[Long et al., 2015]</ref> reduces the domain discrepancy by using an optimal multi-kernel selection method. <ref type="bibr" target="#b10">[Long et al., 2016]</ref> assumes that the source classifier and target classifier differ by a residual function and enable classifier adaptation by plugging several layers with reference to the target classifier. <ref type="bibr" target="#b10">[Long et al., 2017]</ref> learns domaininvariant representation by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy criterion. Other works introduce a domain adversial layer for the domain alignment. <ref type="bibr">[Ganin and Lempitsky, 2015</ref>] introduces a gradient reversal layer to fool the domain classifier and extracts the domaininvariant representation, <ref type="bibr" target="#b12">[Tzeng et al., 2017]</ref> borrows the idea of generative adversarial network(GAN) <ref type="bibr" target="#b9">[Goodfellow et al., 2014]</ref> and proposes a novel unified framework for adversarial domain adaptation.</p><p>Recent studies also show the benefits of semantic alignment to unsupervised domain adaptation. With the assumption that the distance among the samples with the same label but from different domains should be as small as possible, <ref type="bibr" target="#b13">[Xie et al., 2018]</ref> learns semantic representation by aligning labeled source centroid and pseudo-labeled target centroid. <ref type="bibr">[Chen et al., 2018]</ref> aligns the discriminative features across domains progressively and effectively, via utilizing the intraclass variation in the target domain. <ref type="bibr" target="#b6">[Deng et al., 2018]</ref> proposes a similarity constrained alignment method which enforces a similarity-preserving constraint to maintain classlevel relations among the source and target samples.</p><p>However, most of the semantic alignment methods require target pseudo labels to minimize the variety discrepancies across domains, and thus resulting the error accumulation due to the uncertainty of the pseudo-labeling accuracy. In this work, we employ the concept of variational auto-encoder <ref type="bibr" target="#b9">[Kingma and Welling, 2013]</ref> and adversarial learning to extract the domain invariant semantic representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Disentangled Semantic Representation Model</head><p>In this work, we focus on the unsupervised domain adaptation problem that uses the labeled samples may differ considerably across domains, we induce that the semantic latent variables play an important role in extracting the domain-invariant representation. Let z = {z y , z d }.</p><formula xml:id="formula_0">D S = x S i , y S i n S</formula><p>By further developing the independence property in the latent space, we also assume that z y ⊥ ⊥ z d . Regarding the second point, with the above data generative mechanism, we propose a disentangled semantic representation (DSR) domain adaptation framework by first reconstructing the two independent latent variables via variational auto-encoder and then disentangling them through a dual adversarial training network. The key structure of the proposed framework is given in Figure . 3.</p><p>As shown in the upper part of Figure . 3, the "Reconstruction" architecture, we first obtain a feature via a backbone feature extractor G(.) .e.g ResNet, and then use the VAE-like scheme to reconstruct the feature by first encoding it into the latent variables z, then use the latent variables to reconstruct the feature G(x) from two independent latent variables z y and z d . However, unlike the vanilla VAE, we further design a "Disentanglement" architecture as shown in the green dot box of Figure . 3. In this architecture, two adversarial modules are placed under the semantic latent variables and domain latent variables respectively. For the label adversarial learning module on the left side, it aims to pull all the semantic information into z y and push all the domain information from z d . For the domain adversarial learning module on the right side, it aims to pull all the domain information into z d and push all the semantic information from z y . By doing so, we can obtain those domain-invariant semantic information without the contamination of the domain information.</p><p>We introduce more technical details of our proposed framework in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Latent Variables Reconstruction</head><p>For the reconstruction architecture in DSR framework, we follow the configuration in VAE. We denote q φ (z|x) as the encoder with respect to φ to approximate the intractable true posterior p(z|x). The variational lower bound of the marginal likelihood is given as follow</p><formula xml:id="formula_1">L ELBO (φ, θ r ) = -D KL (q φ (z|x)||P (z)) + E q φ (z|x) [log P θr (x|z)] , (1)</formula><p>where P θr (x|z) denotes the decoder with respect to the parameters θ r and P (z) is the prior distribution. Then, we further decompose the latent variables z into z y and z d . Eq. ( <ref type="formula">1</ref>)  Hy and H d are the encoders for the semantic and domain information respectively. Cy and C d are the classifiers for the label and domain respectively. GRL is a gradient reversal layer that multiplies the gradient by a negative constant. (best view in color) can be derived as follows:</p><formula xml:id="formula_2">L ELBO (φ y , φ d , θ r ) = -D KL (q φy (z y |G(x)) P (z y )) -D KL (q φ d (z d |G(x)) P (z d )) +E q φ y,d (zy,z d |G(x)) [log P θr (G(x)|z y , z d )].</formula><p>(2) Here, we assume that P (z y ), P (z d ) ∼ N (0, I), and φ y and φ d are the parameters of the encoder. Similar to VAE, by applying a reparameterization trick, we use MLP H y (G(x); φ y ) and H d (G(x); φ d ) as the universal approximator of q to encode the data into z y and z d respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Latent Variables Disentanglement</head><p>For the disentanglement architecture of the DSR framework, it consists of two adversarial modules working together following the typical configuration in <ref type="bibr">[Ganin and Lempitsky, 2015]</ref>. On the left of the Figure . 3 under the semantic latent variables z y is the label adversarial learning module that fuses the semantic information and excludes all the domain information. This is done by using a label classifier C y and a domain classifier C d . To exclude the domain information, we use a gradient reversal layer (GRL) for C d . As a result, the parameters φ y in H y are learned by maximizing the loss L d of C d and simultaneously minimizing the loss L y of C y . The parameters θ y,d of C d are learned by L d . The overall objective function of label adversarial learning module is shown as follows:</p><formula xml:id="formula_3">L sem (φ y , θ y,y , θ y,d ) = δ n S x s i ∈D S L y (C y (H y (G(x); φ y ) ; θ y,y ) , y i ) - λ n xi∈(D S ,D T ) L d (C d (H y (G(x); φ y ) ; θ y,d ) , d i ) ,<label>(3)</label></formula><p>where n = n S + n T , λ are a trade-off parameters that balance the two objectives and δ is the parameter that controls the weight of L y . A bigger δ enables the label classifier to learn more semantic information. The default value of δ is 1, and we try different values in order to validate the individual contributions of the domain adversarial learning module in the section 4.</p><p>Similarly, on the right-side adversarial module is the domain adversarial learning module that fuses the domain information to z d and excludes the semantic information from z d . The GRL is placed on the label classifier above in order to absorb all the domain information from z y . However, unlike the semantic module, we do not use cross-entropy as the label loss, because of the unsupervised learning in the target domain. In order to utilize the data in the target domain, we employ maximum entropy loss L e for the label classifier C y . As a result, the parameters φ d in H d are learned by maximizing the loss L e of label classifier C y but minimizing the loss L d of label classifier C d . In addition, the parameters θ d,y of label classifier C y are learned by minimizing its own loss L e . The objective of the domain adversarial learning module is shown as follow</p><formula xml:id="formula_4">L dom (φ d , θ d,d , θ d,y ) = 1 n • xi∈(D S ,D T ) L d (C d (H d (G(x); φ d ) ; θ d,d ) , d i ) - ω n • xi∈(D S ,D T ) L E (C y (H d (G(x); φ d ) ; θ d,y )) ,<label>(4)</label></formula><p>where ω is the trade-off parameter between the two objectives that shapes the feature during.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Summary</head><p>By combining the reconstruction and the disentanglement, we summarize the model as follows.</p><p>The total loss of the proposed disentangled semantic representation learning for domain adaptation model is formulated as:</p><formula xml:id="formula_5">L(φ y , θ y,d , θ y,y , φ d ,θ d,d , θ d,y , θ r ) = L ELOB + βL sem + γL dom ,<label>(5)</label></formula><p>where β and γ are the hyper-parameters that is not very sensitive and we set β=1 and γ=1.</p><p>Under the above objective function our model is trained on the source domain using the following procedure (6)</p><p>The following classifier with the trained optimal parameters is adapted to the target domains. y = C y H y G(x); φy ; θ y,y .</p><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>In this section, we first show that the error will be reduced under the domain-invariant feature space, and second we further develop an upper bound for the target generalization error. Following the instruction in <ref type="bibr" target="#b5">[Ben-David et al., 2007]</ref>. Let R : X → Z denote the representation function, where R H•G follows the definition in Section 2. We denote D S as the source distribution over X and DS as the induced distribution over the feature space Z, i.e., Pr DS [B] def = Pr D S R -1 (B) for any measurable event B. Similarly, we use parallel notation, D T , DT for the target domain. The error on the source domain with a hypothesis h is defined as</p><formula xml:id="formula_6">S (h) =E z∼ DS [E y∼ f (z) [y = h(z)]] =E z∼ DS [C(z) -h(z)],<label>(8)</label></formula><p>where C : Z → [0, 1] is the label classier defined on DS . Theorem 1. Assume that the semantic and the domain factors are independent, i.e., z y ⊥ ⊥ z d . Let z = {z y , z d }, and the error on the disentangled source and target domain with a hypothesis h is</p><formula xml:id="formula_7">y S (h) = S (h) -α S , y T (h) = T (h) -α T , (9) where α S := E z d ∼ DS [C(z d ) -h(z d )] and y S (h) := E zy∼ DS [C(z y ) -h(z y )]</formula><p>denotes the error of DSR with respect to h in the source domain, while y T (h) denotes the error of DSR in target domain.</p><p>Proof. Since z y ⊥ ⊥ z d , we can further derive Eq. ( <ref type="formula" target="#formula_6">8</ref>) as follow,</p><formula xml:id="formula_8">S (h) =E (zy,z d )∼ DS [C(z) -h(z)] = E zy∼ DSy [C(z y ) -h(z y )]+ y S (h) E z d ∼ DS d [C(z d ) -h(z d )] αS = y S (h) + α S .<label>(10)</label></formula><p>In the second equality, based on the independence property between z y and z d , the distribution of DS can be decomposed into two part so as to the error. Similarly, we have y T (h) = T (h) -α T , by decomposing the DT using the independence property.</p><p>Theorem 1 shows that the disentanglement of the representation space is helpful and might also necessary for obtaining less classify error. Then, in the following Theorem 2, we show that the less classify error on the source domain will tighten the error bound at the target domain. Theorem 2. Let R(x) be a fixed representation function from X to Z and H be a hypothesis space. Let h * = arg min h∈H ( T (h), S (h)), and let λ S , λ T be the errors of h * with respect to D S and D T respectively, i.e., λ T := T (h * ) and λ S := S (h * ). We have</p><formula xml:id="formula_9">y T (h) ≤ η + y T (S) + d H DS , DT ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_10">η := y T (h * ) + α * T + y S (h * ) + α * S + α S -α T . Proof. Applying [Ben-David et al., 2007, Theorem 1], we have T (h) ≤ λ T + λ S + S (h) + d H DS , DT ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_11">d H DS , DT = 2 sup h∈H |Pr D S [Z h Z h * ] -Pr D T [Z h Z h * ]| .</formula><p>Then based on Theorem 1, combining with Eq. ( <ref type="formula" target="#formula_10">12</ref>) and Eq. ( <ref type="formula" target="#formula_8">10</ref>), we further obtain</p><formula xml:id="formula_12">y T (h) ≤ λ T + λ S + S (h) + d H DS , DT -α T ≤ λ T + λ S + S (h) + d H DS , DT .<label>(13)</label></formula><p>Note that based on Eq. ( <ref type="formula" target="#formula_8">10</ref>), we denote λ T = T (h * ) = y T (h * )+α * T . and the error upper bound for the target domain can be further derived as follow,</p><formula xml:id="formula_13">y T (h) ≤ λ T + λ S + S (h) + d H DS , DT -α T ≤ y T (h * ) + α * T + y S (h * ) + α * S + y S (h) + α S + d H DS , DT -α T ≤ η + y T (S) + d H DS , DT ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_14">η := y T (h * ) + α * T + y S (h * ) + α * S + α S -α T .</formula><p>4 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Office-31 is a standard benchmark for visual domain adaptation, which contains 4,652 images and 31 categories from three distinct domains: Amazon (A), Webcam (W) and DSLR (D).</p><p>Office-Home is a more challenging domain adaptation dataset than Office-31, which consists of around 15,500 images from 65 categories of everyday objects. This dataset is organized into four domains: Art (Ar), Clipart (Cl), Product (Pr) and Real-world (Rw).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Approaches</head><p>Beside the classical approaches, we also compare our disentangled semantic representation model with some deep transfer learning methods. Two recently proposed semantic enhanced methods, CDAN <ref type="bibr" target="#b11">[Long et al., 2018]</ref> and MSTN <ref type="bibr" target="#b13">[Xie et al., 2018]</ref>, are also compared in the experiment. Note that, CDAN conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions, and MSTN learns semantic representation by aligning labeled source centroid and pseudo-labeled target centroid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Result</head><p>Office-31 Result The classification accuracies on the Office-31 dataset for unsupervised domain adaptation on ResNet-50 are shown in Table 1. Our DSR model significantly outperforms all other baselines on most of the transfer tasks. It is remarkable that our method promotes the classification accuracies substantially on hard transfer tasks, e.g., D→A, W→A, and produces comparable results on the other relatively simple tasks, e.g., A→W, D→W. However, the result of DSR on the W→D and A→D tasks are lower than that of some compared approaches. This is because domain DSLR (D) only has total 498 images for 31 classes and some classes even only have less than 10 samples, it's insufficient for our method to reconstruct the disentangled semantic representation. We conduct the Wilcoxon signed-rank test <ref type="bibr" target="#b12">[Lowry, 2014]</ref> on the reported accuracies, the results are as follows: on the Office-31 data set, our method is comparable with CDAN-M, and significantly outperforms all the other baselines with the p-value threshold 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Office-home Result</head><p>Similar to the results on Office-31, the DSR model also outperforms all other baselines on most of the tasks, as reported in Table <ref type="table">2</ref>. Note that our method achieves a great improvement when the source domain is Art (Ar) or Clipart (Cl) and performs slightly worse than CDAN-M when the source domain is Real World (RW). This is because the Ar and the clipart (Cl) domain contain relatively simpler pictures and more complex scenarios than the other domains and our DSR model can extract such semantic representation easily and thus achieve good performance on this source domain. However, in the Real World (RW) domain, the pictures are taken in real life and there a lot of ambiguous samples, .e.g., pictures with monitor, computer and laptop are tagged with the same label, which implies that the semantic information is difficult to be disentangled and extracted on this domain. We also conduct the Wilcoxon signed-rank test <ref type="bibr" target="#b12">[Lowry, 2014]</ref> on the reported accuracies, our method significantly outperforms the baselines, with the p-value threshold 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Study of the Disentangled Semantic Representation</head><p>To study the effectiveness of the disentangled semantic representation, we compare our methods with two approaches using similarly adversarial learning strategy but different representations on the task Ar→Cl. Such results also can be observed in Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table">2</ref>. For example, DSR achieves remarkably outstanding results on some transfer tasks, e.g. D→A, W→A on the Office-31 dataset and all the tasks except for those using RW as the source domain on the Office-home dataset. These results show a common phenomenon that the samples of the source domain are more complex than that of the target domain, i.e., the source domain has more scenarios than the target domain. Such common phenomenon also shows the advantage of our disentangled semantic representation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mehtods</head><formula xml:id="formula_15">A → W D → W W → D A → D D → A W → A Avg ResNet-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study of the Dual Adversarial Learning</head><p>To study the effectiveness of the dual adversarial learning module, we first train the standard DSR model until convergence, then train the model without the domain adversarial learning module. Such ablated model is named DSR WD in the experiment. The experiment results are shown in Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table">2</ref>. Comparing the result of DSR and DSR WD (δ=1), we find that the performance drops because the semantic information is drained away without disentanglement.</p><p>Even when we use δ = 2, the performance of the ablated model is still worse than the original one. These results verify that the dual adversarial learning module can push the semantic information into the semantic latent variables z y , and simultaneously, push the domain information into the domain latent variables z d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a disentangled semantic representation model for the unsupervised domain adaptation task. Different from previous work , our approach extracts the disentangled semantic representation on the recovered latent space, following the causal model of the data generation process. Our approach is also featured with the variational auto-encoder based latent space recovery and the dual adversarial learning based disentangle of the representation. The success of the proposed approach not only provides an effective solution for the domain adaptation task, but also opens the possibility of disentanglement based learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A toy domain adaption example with "pig" and "hair drier" samples on the "art" and "camera" domain. (a) The data generation process is controlled by the disentangled domain latent variables and semantic latent variables in the latent manifold. (b) The samples are distorted on the feature manifold, and the residual domain information results in the false alignment between the Peppa Pig and the hair drier. (c) The samples are well distributed on the disentangled semantic manifold. (best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>xFigure 2 :</head><label>2</label><figDesc>Figure 2: The causal model of data generation process, which are controlled by the latent variables z d and zy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The framework of the Disentangled Semantic Representation model. In the reconstruction block (marked with the blue dashed lines), the variational auto-encoder is used to recover the semantic latent variables (zy) and the domain latent variables (z d ). In the disentanglement block (marked with the green dashed lines), a dual adversarial network is used to disentangle the latent variables.Hy and H d are the encoders for the semantic and domain information respectively. Cy and C d are the classifiers for the label and domain respectively. GRL is a gradient reversal layer that multiplies the gradient by a negative constant. (best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>φy , θ y,y , φd , θ d,y , θr ) = arg min φy,θy,y,φ d ,θ d,y ,θr L(φ y , θ y,d , θ y,y , φ d , θ d,d , θ d,y , θ r ) ( θ y,d , θ d,d ) = arg max θ y,d ,θ d,d L(φ y , θ y,d , θ y,y , φ d , θ d,d , θ d,y , θ r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure. 4 (a)-(c) show the visualization of the extracted features using t-SNE. As shown in the figure, DSR obtains the best alignment among the three representations. Both DANN and MSTN have a large number of samples are falsely aligned. This result verifies the effectiveness of DSR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The t-SNE visualization of deep features extracted by DANN (a), MSTN (b) and DSR (c). The red points are source domain samples and the blue points are target domain samples.</figDesc><graphic coords="6,56.90,349.73,75.18,74.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on Office-31 for unsupervised domain adaptation (ResNet)MethodAr→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg</figDesc><table><row><cell>50 [He et al., 2016]</cell><cell>68.4</cell><cell>96.7</cell><cell>99.3</cell><cell>68.9</cell><cell>62.5</cell><cell>60.7</cell><cell>76.1</cell></row><row><cell>TCA [Pan et al., 2011]</cell><cell>72.7</cell><cell>96.7</cell><cell>99.6</cell><cell>74.1</cell><cell>61.7</cell><cell>60.9</cell><cell>77.6</cell></row><row><cell>GFK [Gong et al., 2012]</cell><cell>72.8</cell><cell>95.0</cell><cell>98.2</cell><cell>74.5</cell><cell>63.4</cell><cell>61.0</cell><cell>77.5</cell></row><row><cell>DAN [Long et al., 2015]</cell><cell>80.5</cell><cell>97.1</cell><cell>99.6</cell><cell>78.6</cell><cell>63.6</cell><cell>62.8</cell><cell>80.4</cell></row><row><cell>RTN [Long et al., 2016]</cell><cell>84.5</cell><cell>96.8</cell><cell>99.4</cell><cell>77.5</cell><cell>66.2</cell><cell>64.8</cell><cell>81.6</cell></row><row><cell>DANN [Ganin et al., 2016]</cell><cell>82.0</cell><cell>96.9</cell><cell>99.1</cell><cell>79.7</cell><cell>68.2</cell><cell>67.4</cell><cell>82.2</cell></row><row><cell>ADDA [Tzeng et al., 2017]</cell><cell>86.2</cell><cell>96.2</cell><cell>98.4</cell><cell>77.8</cell><cell>69.5</cell><cell>68.9</cell><cell>82.9</cell></row><row><cell>JAN [Long et al., 2017]</cell><cell>85.4</cell><cell>97.4</cell><cell>99.8</cell><cell>84.7</cell><cell>68.6</cell><cell>70.0</cell><cell>84.3</cell></row><row><cell>MSTN [Xie et al., 2018]</cell><cell>86.9</cell><cell>96.7</cell><cell>99.9</cell><cell>87.3</cell><cell>66.9</cell><cell>68.4</cell><cell>84.3</cell></row><row><cell cols="2">CDAN-M [Long et al., 2018] 93.1</cell><cell>98.6</cell><cell>100.0</cell><cell>93.4</cell><cell>71.0</cell><cell>70.3</cell><cell>87.7</cell></row><row><cell>DSR DM(δ = 1)</cell><cell>90.7</cell><cell>97.1</cell><cell>99.2</cell><cell>88.8</cell><cell>71.5</cell><cell>71.2</cell><cell>86.4</cell></row><row><cell>DSR DM(δ = 2)</cell><cell>91.5</cell><cell>97.2</cell><cell>99.5</cell><cell>88.5</cell><cell>71.5</cell><cell>72.1</cell><cell>86.7</cell></row><row><cell>Ours</cell><cell>93.1</cell><cell>98.7</cell><cell>99.8</cell><cell>92.4</cell><cell>73.5</cell><cell>73.9</cell><cell>88.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by <rs type="funder">NSFC-Guangdong Joint Found</rs> (<rs type="grantNumber">U1501254</rs>), <rs type="funder">Natural Science Foundation of China</rs> ( <ref type="formula">61876043</ref>), <rs type="funder">Natural Science Foundation of Guangdong</rs> (<rs type="grantNumber">2014A030306004</rs>, <rs type="grantNumber">2014A030308008</rs>), <rs type="programName">Guangdong High-level Personnel of Special Support Program</rs> (<rs type="grantNumber">2015TQ01X140</rs>) and <rs type="funder">Pearl River S&amp;T Nova Program of Guangzhou</rs> (201610010101). Kun Zhang would like to acknowledge the support by <rs type="funder">National Institutes of Health (NIH)</rs> under Contract No. <rs type="funder">NIH</rs>-<rs type="grantNumber">1R01EB022858-01</rs>, <rs type="grantNumber">FAINR01EB022858</rs>, <rs type="funder">NIH</rs>-<rs type="grantNumber">1R01LM012087</rs>, <rs type="grantNumber">NIH5-5U54HG008540-02</rs>, and <rs type="grantNumber">FAIN-U54HG008540</rs>, by the <rs type="funder">United States Air Force</rs> under Contract No. <rs type="grantNumber">FA8650-17-C-7715</rs>, and by <rs type="funder">National Science Foundation (NSF) EAGER</rs> Grant No. IIS-1829681. The <rs type="funder">NIH</rs>, the <rs type="funder">U.S. Air Force</rs>, and the <rs type="funder">NSF</rs> are not responsible for the views reported here. We would like to thank <rs type="person">Dr Tom Fu</rs> from <rs type="funder">ADSC</rs> and professor <rs type="person">Ke Yiping</rs> from <rs type="affiliation">Nanyang Technological University</rs> for their help and supports on this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qR6mtuX">
					<idno type="grant-number">U1501254</idno>
				</org>
				<org type="funding" xml:id="_pTwpZzb">
					<idno type="grant-number">61876043</idno>
				</org>
				<org type="funding" xml:id="_agNqQMh">
					<idno type="grant-number">2014A030306004</idno>
				</org>
				<org type="funding" xml:id="_66sVU6e">
					<idno type="grant-number">2014A030308008</idno>
					<orgName type="program" subtype="full">Guangdong High-level Personnel of Special Support Program</orgName>
				</org>
				<org type="funding" xml:id="_RDUNcPz">
					<idno type="grant-number">2015TQ01X140</idno>
				</org>
				<org type="funding" xml:id="_MVf2Zqx">
					<idno type="grant-number">1R01EB022858-01</idno>
				</org>
				<org type="funding" xml:id="_KqPDxkT">
					<idno type="grant-number">FAINR01EB022858</idno>
				</org>
				<org type="funding" xml:id="_e5RrdzS">
					<idno type="grant-number">1R01LM012087</idno>
				</org>
				<org type="funding" xml:id="_R5FAXa7">
					<idno type="grant-number">NIH5-5U54HG008540-02</idno>
				</org>
				<org type="funding" xml:id="_3vsjh37">
					<idno type="grant-number">FAIN-U54HG008540</idno>
				</org>
				<org type="funding" xml:id="_dP9tDYk">
					<idno type="grant-number">FA8650-17-C-7715</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dan [</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dann [ganin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mstn [xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cdan-M [</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Accuracy (%) on Office-home for unsupervised domain adaptation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>ResNet</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Ben-David</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08585</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascal</forename><surname>Courville</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vincent</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2013. 2013. 2018. 2018</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Representation learning: A review and new perspectives</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00893</idno>
		<title level="m">Domain alignment with triplets</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ganin and Lempitsky, 2015] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2014">2014. 2014. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Non-linear independent components estimation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2016. 2016. 2012</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
	<note>Domain-adversarial training of neural networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>Kingma and Welling</publisher>
			<date type="published" when="2013">2014. 2014. 2016. 2013. 2013</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
		<respStmt>
			<orgName>JMLR. org</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">Judy</forename><surname>Tzeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Hoffman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Darrell</surname></persName>
		</editor>
		<editor>
			<persName><surname>Saenko</surname></persName>
		</editor>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Kate Saenko, and Trevor Darrell</publisher>
			<date type="published" when="2011">2014. 2014. 2011. 2011. 2018. 2018. 2014. 2014. 2015. 2017. 2017</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-invariant adversarial learning for unsupervised domain adaption</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12751</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><surname>Zhang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2018. 2018. 2013. 2013. 2018. 2018</date>
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning semantic representations for unsupervised domain adaptation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
