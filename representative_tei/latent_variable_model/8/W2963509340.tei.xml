<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning from Noisy Image Labels with Quality Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-11-02">2 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
							<email>jiangchao.yao@student.uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Jiajie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
							<email>ivor.tsang@uts.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
							<email>junsun@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
							<email>chengqi.zhang@uts.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>zhangrui@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Cooperative Medianet Innovation Cen- ter</orgName>
								<orgName type="department" key="dep2">Center for Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">FEIT</orgName>
								<orgName type="department" key="dep4">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution" key="instit1">Shanghai Jiao Tong University</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">are with Center for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">FEIT</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<postCode>2007</postCode>
									<settlement>Ultimo</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Image Communication and Network Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning from Noisy Image Labels with Quality Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-02">2 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1711.00583v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>noisy image labels</term>
					<term>quality embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among the datasets severely degenerates the performance of deep learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a quality embedding model, which explicitly introduces a quality variable to represent the trustworthiness of noisy labels. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the noise effect. At the same time, the highquality labels is still able to be applied for training. To instantiate the model, we further propose a Contrastive-Additive Noise network (CAN), which consists of two important layers: (1) the contrastive layer estimates the quality variable in the embedding space to reduce noise effect; and (2) the additive layer aggregates the prior predictions and noisy labels as the posterior to train the classifier. Moreover, to tackle the optimization difficulty, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We conduct the experimental evaluation of the proposed method over a range of noisy image datasets. Comprehensive results have demonstrated CAN outperforms the state-of-the-art deep learning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W HILE editorially labeled image data is crucial to visual classification [1]- <ref type="bibr" target="#b4">[4]</ref>, weakly supervised detection and segmentation <ref type="bibr" target="#b5">[5]</ref>- <ref type="bibr" target="#b10">[10]</ref>, collecting such datasets in large volume can be prohibitive. Non-editorial means such as social tagging and crowdsourcing, have been explored as efficient alternatives <ref type="bibr">[11]</ref>- <ref type="bibr" target="#b13">[13]</ref>. For example, there are a plethora of images with tags available on the Flickr website, which provides us valuable labeled resources to build image classifiers. However, the challenges lie in the fact that social tags as labels are highly noisy. As a result, deep learning from noisy image labels has attracted the increasing attention <ref type="bibr" target="#b14">[14]</ref>.</p><p>Previous studies have investigated the label noise <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b19">[19]</ref> for non-deep approaches in the machine learning community. For example, Vikas et al. <ref type="bibr" target="#b15">[15]</ref> introduce parameters for annotators to transit latent predictions to noisy labels. For parameter estimation, they resort to an EM optimization algorithm that is also adopted in the contemporaneous works. However, it is not straightforward to apply these studies to deep learning methods due to the computational consuming in the EM optimization.</p><p>With the success of deep learning in computer vision [1]- <ref type="bibr" target="#b4">[4]</ref>, training neural network with noisy image labels has also been explored <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b29">[29]</ref>. These methods can be summarized into two categories, building the robust loss function and modeling the latent label. The former paradigm is heuristic and usually depends on non-trivial hyperparameter selection. For instance, Reed et al. <ref type="bibr" target="#b24">[24]</ref> construct a weighted combination of noisy image labels and predictions to supervise the network training. However, it is unclear that how the weight interacts with the real-world label noise for settings. One popular example of the latter paradigm, Sukhbaatar et al. <ref type="bibr" target="#b14">[14]</ref> model the latent label to handle the label noise. Specifically, the classifier is trained based on latent labels, and thus the label noise will not directly affect the classifier. However, they adapt latent labels to noisy labels with a linear transition layer, which cannot sufficiently model the label corruption. Label noise can still go through this layer to degenerate the performance. The deficiency of above deep learning methods is that they do not explicitly model the trustworthiness of noisy labels. Implicitly considering noise in the loss function or by modeling the latent label may harm the nature of noise, e.g., flip and outlier.</p><p>In this paper, we follow the latter paradigm and propose a quality embedding model. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates our idea as well as its advantage to reduce the noise effect. For example, in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, the latent labels and predictions of the first three cat images must approximately consistent due to their content similarity. However, mismatch will occur between the second prediction and the corresponding annotation by virtue of the label noise. For the fourth image, the prediction induced by the estimation error of the latent label, also has conflict with the fourth annotation. As a result, these two mismatches will mix together for back-propagation. On the other hand, if we explicitly introduce a quality variable to model the trustworthiness of noisy labels like Fig. <ref type="figure" target="#fig_0">1(b)</ref>, label noise can be reduced more effectively. For example, if the quality variable of the second sample is embedded in the non-trustworthy subspace, the latent label can be disturbed accordingly to prevent mismatch error caused by the label noise from back-propagation. While for the fourth sample whose quality variable is estimated in the trustworthy subspace, the latent label still transits to the final With quality embedding as a control from latent labels to predictions, the negative effect of label noise is reduced in the back-propagation.</p><p>prediction causing the mismatch. Then supervision from the correct annotations is normally fed back. Mathematically, we illustrate the corresponding graphical model in Fig. <ref type="figure" target="#fig_1">2</ref>. Different from previous latent-label-based deep learning approaches, a quality variable is specially introduced to model the trustworthiness of noisy labels. By embedding the quality variable into different subspaces, the shortcoming illustrated like Fig. <ref type="figure" target="#fig_0">1</ref>(a) can be solved as Fig. <ref type="figure" target="#fig_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>To instantiate our probabilistic model with deep neural network, we further design a Contrastive-Additive Noise network (CAN) shown in Fig. <ref type="figure">3</ref>. For parameter learning, we optimize an evidence lower bound <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" target="#b32">[32]</ref> plus a variational mutual information regularizer, and deduce an SGD algorithm. The major contribution in this paper can be summarized into four parts in the following. The rest of this paper is organized as follows. Section 2 briefly reviews the related work of learning with noisy labels in deep learning. Then we introduce our quality embedding model, the corresponding instantiation Contrastive-Additive-Noise network as well as its optimization algorithm in Section 3. We validate the efficiency of our method over a range of experiments in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Social websites and crowdsourcing platforms provide us an effective way to gather a large amount of low-cost annotations for images. However, in the visual recognition tasks such as image classification, the noise among labels shall severely degenerate the performance of classification models <ref type="bibr" target="#b33">[33]</ref>. To exploit the great value of noisy labels, several noise-aware deep learning methods have been proposed for the image classification task. Here, we briefly review these related works.</p><p>Robust loss function This line of research aims at designing a robust loss function to alleviate noise effect. For instance, Joulin et al. <ref type="bibr" target="#b34">[34]</ref> weight the cross-entropy loss with the sample number to balance the emphasis of noise in positive and negative instances. Izadinia et al. <ref type="bibr" target="#b23">[23]</ref> estimate a global ratio of positive samples to weaken the supervision in the loss function. Reed et al. <ref type="bibr" target="#b24">[24]</ref> consider the consistency of predictions in similar images and apply bootstrap to the loss function. They substitute the noisy label with a weight combination of the noisy label and the prediction to encourage the consistent output. Recently, Li et al. <ref type="bibr" target="#b28">[28]</ref> re-weight the noisy label with a soft label learned from side information. They train a teacher network with the clean dataset to compute the soft label by leveraging the knowledge graph. The soft label is then combined with the noisy label in the loss function to pilot student model's learning. Andreas et al. <ref type="bibr" target="#b29">[29]</ref> rectify labels in the cross-entropy loss with a label-correction network trained on the extra clean dataset. While these methods are concerned with modifying the labels in the loss function by re-weighting or rectification, our approach also models the auxiliary trustworthiness of noisy image labels to reduce the noise effect on training.</p><p>Modeling the latent labels This paradigm targets at modeling the latent labels to train the classifier, and building a transition for adaption from the latent labels to the noisy labels. With the success of deep learning in image recognition, this kind of idea receives considerable attention. Mnih et al. <ref type="bibr" target="#b20">[20]</ref> first propose a latent variable model on aerial images, which assumes that the noise is symmetric and at random. Based on it, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b27">[27]</ref> use an linear adaptation layer to model the asymmetric label noise, and add the layer on top of a deep neural network. This transition layer can be deemed as the confusion matrix representing label flip probability. However, the matrix only depends on the distribution of labels but ignore the information of image contents. Chen et al. <ref type="bibr" target="#b12">[12]</ref> apply a two-stage approach to model the latent label and learn the translation to the noisy label, in which a clean dataset is used. Different from methods that model label transition in the dataset level, Xiao et al. <ref type="bibr" target="#b26">[26]</ref> propose a probabilistic graphic model that disturbs the label in the image level. However, the model also needs a small part of clean data to learn conditional probability, which may constrains the generalization of the model. To demonstrate the human-centric noisy label exhibits specific structure that can be modeled, Misra et al. <ref type="bibr" target="#b22">[22]</ref> build two parallel classifiers. One classifier deals with image recognition and the other classifier model human's reporting bias. However, it still suffers from the problem mentioned in Fig. <ref type="figure" target="#fig_0">1</ref>(a) since similar images have similar latent variables. Although these methods take advantages of deep neural network to model the latent label, the simple transition cannot sufficiently model the label corruption. We go on by unearthing the annotation quality from training data and further utilize it to guide the learning of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. QUALITY EMBEDDING MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>Consider that we have a noisy image dataset of M items,</p><formula xml:id="formula_0">D : {(x 1 , y 1 ), (x 2 , y 2 ), ..., (x M , y M )} ,</formula><p>where each tuple in the dataset consists of one image x m and its noisy labels y m . Note that x m can be the original image or the feature vector extracted from the image. y m ∈ R K is a K-dimensional binary vector indicating which labels are annotated, and K is the number of categories. However, y m may be corrupted with annotation noise and thus incorrect. We assume the underlying clean label is z m ∈ R K . We introduce s m , a quality variable embedded in D-dimensional Gaussian space, to represent the annotation quality of y m . For ease of reference, we list the notations of this paper in Table <ref type="table" target="#tab_1">I</ref>.</p><p>Formally, it is a multi-label, multi-class classification problem with noise in labels. We target to train a deep classifier from these noisy training samples. There are many other tasks that are consistent with this setting, like weakly supervised object detection and segmentation <ref type="bibr" target="#b5">[5]</ref>- <ref type="bibr" target="#b10">[10]</ref> with web data. B. Quality Embedding Model 1) quality embedding: In this section, we introduce a quality variable in parallel to the latent label, which jointly transit to the noisy image label. Our probabilistic graphical model is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. In the generative process, the latent label vector Z purely depends on the instance X. We model this dependency with P(Z|X). However, the noisy label vector Y is generated based on both the annotation quality S and the latent label Z, which we model with P(Y |S, Z). In the inference process, both the distributions of Z and S are all modeled based on X and Y . We respectively represent these two distributions with q(Z|X,Y ) and q(S|X,Y ), which plays roles of posterior approximation.</p><p>According to the graphical model in Fig. <ref type="figure" target="#fig_1">2</ref>, once given the training set, we have the following log-likelihood.</p><formula xml:id="formula_1">ln P(Y |X) = M ∑ m=1 ln P(y m |x m ) = M ∑ m=1 ln s m ∑ z m P(y m |z m , s m )P(z m |x m )P(s m )ds m = M ∑ m=1 ln E P(z m |x m ),P(s m ) [P(y m |z m , s m )]</formula><p>(1) However, the log-likelihood function is difficult to explicitly compute. We instead choose to optimize an adjustable evidence lower bound (ELBO) <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" target="#b32">[32]</ref>. The ELBO is acquired by introducing two variational distributions q(z m |x m , y m ) and q(s m |x m , y m ) to approximate the true distributions of z m and s m . We illustrate the form of our ELBO in Eq. <ref type="bibr" target="#b2">(2)</ref>.</p><formula xml:id="formula_2">ln P(Y |X) ≥ M ∑ m=1 E q(z m |x m ,y m ),q(s m |x m ,y m ) [ln P(y m |z m , s m )] - M ∑ m=1 D KL [q(z m |x m , y m )||P(z m |x m )] - M ∑ m=1 D KL [q(s m |x m , y m )||P(s m )] (2)</formula><p>Above bound is a good approximation of the marginal likelihood, which provides a basis for selecting a model <ref type="bibr" target="#b32">[32]</ref>. When the gap between marginal likelihood and ELBO becomes zero, the variational distributions approach the true distributions. 2) variational mutual information regularizer: Although Fig. <ref type="figure" target="#fig_1">2</ref> presents the structure prior of our probabilistic model, optimization on ELBO may not converge to the desirable optimal since modeling the distribution with neural network introduces much flexibility. It is a common problem in Bayesian models and a general solution is posterior regularizations <ref type="bibr" target="#b35">[35]</ref>. Posterior regularizations ensure the desirable expectation and simultaneously retain the computational efficiency. Such methods have been applied in clustering <ref type="bibr" target="#b36">[36]</ref>, classification <ref type="bibr" target="#b37">[37]</ref> and image generation <ref type="bibr" target="#b38">[38]</ref>. In this paper, we introduce the regularization for variational distributions of Z and S in the perspective of mutual information maximization. We deduce the regularizers as follows,</p><formula xml:id="formula_3">I((Z, S); (X,Y )) = H(Z, S) -H(Z, S|X,Y ) = E q(X,Y ) E q(Z,S|X,Y ) [ln q(Z, S|X,Y )] + const 1 M M ∑ m=1 E q(z m |x m ,y m ) [ln q(z m |x m , y m )]<label>(3)</label></formula><formula xml:id="formula_4">+ 1 M M ∑ m=1 E q(s m |x m ,y m ) [ln q(s m |x m , y m )] + const,</formula><p>where I(; ) means the mutual information of two distributions and H(•) is the entropy of the variable. As can be seen in Eq.</p><p>(3), maximizing the mutual information is equal to minimizing the entropy of z m and s m . For the latent label z m , such posterior regularization can force the probability q(z m |x m , y m ) close to the extreme points. And for the quality variable s m , it will encourage the distribution q(s m |x m , y m ) to have a low variance.</p><p>3) objective: Combining Eq. ( <ref type="formula">2</ref>) with (3), our objective then becomes the maximization of ELBO along with the mutual information regularizer. Note that, we substitute 1 M in Eq. ( <ref type="formula" target="#formula_3">3</ref>) with a coefficient λ ∈ [0, +∞] to weight the regularization effect in the optimization. Instead of maximization, we rewrite our goal as the following minimization problem for the simplicity sake.</p><formula xml:id="formula_5">min L = - M ∑ m=1 E q(z m |x m ,y m ),q(s m |x m ,y m ) [ln P(y m |z m , s m )] + M ∑ m=1 D KL [q(z m |x m , y m )||P(z m |x m )] + M ∑ m=1 D KL [q(s m |x m , y m )||P(s m )] -λ M ∑ m=1 E q(z m |x m ,y m ) [ln q(z m |x m , y m )] -λ M ∑ m=1 E q(s m |x m ,y m ) [ln q(s m |x m , y m )]<label>(4)</label></formula><p>From Eq. ( <ref type="formula" target="#formula_5">4</ref>), our model mainly differs from previous methods in three aspects. First, P(y m |z m , s m ) indicates that the transition from the latent label to the noisy label is based on both z m and s m while previous methods <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b27">[27]</ref> only depend on z m . Second, previous works <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> use the linear transition P(y m |z m ) while our model applies nonlinear implementation P(y m |z m , s m ). Third, z m and s m are approximated with q(z m |x m , y m ) and q(s m |x m , y m ) in the posterior perspective while previous works <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref> might have to facilitate the extra clean dataset or other label knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Contrastive-Additive Noise Network</head><p>In this section, we instantiate our model with a Contrastive-Additive Noise network (CAN) in Fig. <ref type="figure">3</ref>. Simply, CAN consists of four modules, encoder, sampler, decoder and classifier, which are corresponding to the different parts of our model respectively. In the following, we decribe the design in detail.</p><p>1) architectures: For encoder module, it is used to model the variational distributions, q(s m |x m , y m ) and q(z m |x m , y m ). Concretely, we first forward x m to a neural network to generate a prior label judgement ŷm . Then, according to ŷm and y m , we model the distribution parameters with two elaboratelydesigned layers. The neural network for ŷm can be decided by the type of x m . If x m is the original image, then a convolutional neural network can be applied. While if x m is a feature vector, a fully-connected network can be chosen. In Fig. <ref type="figure">3</ref>, we take the convolutional neural network as an example. The sampler module is the implementation of Monte Carlo sampling for q(s m |x m , y m ) and q(z m |x m , y m ). It receives the output of the encoder module and samples from the Gumbel and Gaussian distributions to generate a sample set of z m and s m . In the next section, we will talk out this part in detail with reparameterization tricks. For the decoder module, it is a neural network for P(y m |z m , s m ), which consists of two group of (linear, ReLU) layers, following with a Sigmoid layer. It takes the sampler output to recover noisy labels. Previous works <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> usually use a linear transition from z m to y m . We consider the nonlinear transition since we have the heterogeneous quality variable s m . The classifier module as our most important target P(z m |x m ), employs a Fig. <ref type="figure">3</ref>: The network consists of four modules, encoder, sampler, decoder and classifier, which are trained end-to-endly. Encoder tries to learn latent labels and evaluate the quality of noisy labels; sampler is used to generate samples from encoder outputs; decoder tries to recover noisy labels from samples. Meanwhile, our classifier is learned based on KL-divergence between q(z) and P(z).</p><p>same network for ŷm in the encoder module. It is trained based on KL-divergence between q(z m |x m , y m ) and P(z m |x m , y m ).</p><p>2) contrastive layer and additive layer: We specially describe these two important layers in the encoder module. Regarding the distribution q(s m |x m , y m ), it is a D-dimensional Gaussian distribution and both mean and variance need to be modeled. We exploits the contrastive layer to implement the estimation. It internally forwards y m and ŷm into a shared fully-connected layer with ReLU (f s (•)) and transforms their difference to µ and log σ 2 with another fully-connected layer (function f t (•)). It is simply represented as follows,</p><formula xml:id="formula_6">(µ(x m , y m ), log σ 2 (x m , y m )) = f t (f s (y m ) -f s ( ŷm )).</formula><p>This contrastive layer is built up based on the assumption that the quality variable s m is related to the difference between y m and ŷm . We evaluate their difference in a latent space with f s (•) and decide which subspace it is embedded with f t (•). This embedding mechanism makes us identify the label quality explicitly and subsequently helps to reduce the noise effect in P(y m |z m , s m ). This idea has never been proposed in previous noise-aware deep learning approaches <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b29">[29]</ref>.</p><p>Regarding the distribution q(z m |x m , y m ), it consists of K Bernoulli distributions and thus K probabilities need to be modeled. We design an additive layer to learn these parameters. It internally uses two non-shared fully-connected layers (f ns1 and f ns2 ) to transform y m and ŷm into a latent space, and then feeds their addition into another fully-connected layer plus a sigmoid function (function f t ), illustrated as follows,</p><formula xml:id="formula_7">q(z m |x m , y m ) = f t (f ns1 (y m ) + f ns2 ( ŷm )).</formula><p>This design learns a posterior label z m from y m and ŷm by a nonlinear combination with neural network. Previous methods in <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b34">[34]</ref> use a weight in their lost function to linearly combine the noisy label y m with the "soft" label from the prediction, the clean dataset or other side information. They usually need non-trivial tuning manually, while we resort to a learning procedure by neural network automatically.</p><p>The whole network can be trained end-to-endly, which will be explained in the next section. In the training, the noise effect is reduced by the branch of the quality variable, and simultaneously the posterior label is estimated by the additive layer to guarantee a more reliable training. We will demonstrate the effectiveness of our network in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization</head><p>In this section, we will analyze the difficulty in optimization and deduce an SGD algorithm with reparameterization tricks.</p><p>1) The reparameterization tricks: The first term in the RHS of Eq. ( <ref type="formula" target="#formula_5">4</ref>) has no closed form when either q(z m |x m , y m ) or q(s m |x m , y m ) is not conjugated with P(y m |z m , s m ). Let alone we model these distributions with deep neural network in the paper. The general way is by the Monte Carlo sampling. However, Paisley et al. <ref type="bibr" target="#b31">[31]</ref> have shown when the derivative is about q(z m |x m , y m ) or q(s m |x m , y m ), the sampling estimation will present high variance. In this case, a large number of samples will be required to have an accurate estimation, which may lead to the high GPU load and the computational burden. Fortunately, reparameterization tricks <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref> are explored to overcome this difficulty in the recent years. They have shown promising efficiency in discrete and continuous representation learning. Simply, the idea behind reparameterization tricks is to decouple the integral variate as one parameter-related part and another parameter-free variate. After integral by substitution, the Monte Carlo sampling on this parameterfree variate will have a small variance. According to this, we apply the reparameterization trick <ref type="bibr" target="#b40">[40]</ref> for discrete z m and the reparameterization trick <ref type="bibr" target="#b39">[39]</ref> for continuous s m as follows, z mk = g(γ mk ) = exp((ln q(z mk =1|x m ,y m )+γ mk1 )/τ) ∑ 1 v=0 exp((ln q(z mk =v|x m ,y m )+γ mkv )/τ)</p><formula xml:id="formula_8">s m = f (ζ m ) = µ(x m , y m ) + σ 2 (x m , y m ) ζ m ,</formula><p>where τ is a temperature to control the discreteness of samples, γ mk ∼ Gumbel(0, 1) 1 and ζ m ∼ N(0, 1) are the parameter-free 1 γ mk1 , γ mk2 are both sampled bylog(-logU), where U ∼ Uniform(0,1)</p><p>variates, q(z m |x m , y m ), µ(x m , y m ) and σ (x m , y m ) are parameterrelated parts. With above reparameterization tricks, we have the following low-variance sampling estimation,</p><formula xml:id="formula_9">E q(z m |x m ,y m ),q(s m |x m ,y m ) [ln P(y m |z m , s m )] 1 N N ∑ n=1 ln P(y m |g(γ (n) m ), f (ζ (n) m )),<label>(5)</label></formula><p>where N is the sample number of γ m and ζ m for the mth image. Based on Eq. ( <ref type="formula" target="#formula_9">5</ref>), the first term in the RHS of Eq. ( <ref type="formula" target="#formula_5">4</ref>) can be efficiently estimated, even though we set the sample number N equal to 1 in the training.</p><p>2) Stochastic variational gradient: The remaining terms in the RHS of Eq. ( <ref type="formula" target="#formula_5">4</ref>) can be explicity computed. We just present their deduction in the appendix. Putting Eq. ( <ref type="formula" target="#formula_9">5</ref>) and ( <ref type="formula" target="#formula_12">8</ref>) (in the appendix) back to Eq. ( <ref type="formula" target="#formula_5">4</ref>), the objective is derivable regarding parameters of all distributions. We can learn the parameter of each distribution with a SGD algorithm, even if they are all modeled with deep neural network. It is important for deep learning especially on the large datasets. Assuming W N , W C , W L and W Q respectively represent the parameters of P(Y |Z, S), P(Z|X), q(Z|X,Y ) and q(S|X,Y ), their gradients can be computed with the following equations with chain rules.</p><formula xml:id="formula_10">∇ W N L = - M ∑ m=1 1 N N ∑ n=1 ∇ W N ln P(y m |g(γ (n) m ), f (ζ (n) m )) ∇ W L L = - M ∑ m=1 1 N N ∑ n=1 ∇ g P(y m |g(γ (n) m ), f (ζ (n) m ))∇ W L g(γ (n) m ) + M ∑ m=1 K ∑ k=1 ln q(z mk1 |x m , y m ) 1-λ (1 -P(z mk1 |x m )) (1 -q(z mk1 |x m , y m )) 1-λ P(z mk1 |x m ) ∇ W L q(z mk1 |x m , y m ) ∇ W Q L = - M ∑ m=1 1 N N ∑ n=1 ∇ f P(y m |g(γ (n) m ), f (ζ (n) m ))∇ W Q f (ζ (n) m ) + M ∑ m=1 1 2 ∇ W Q D ∑ d=1 σ 2 d (x m , y m ) -(1 -λ ) ln σ 2 d (x m , y m ) + M ∑ m=1 1 2 ∇ W Q µ(x m , y m ) T µ(x m , y m ) ∇ W C L = M ∑ m=1 K ∑ k=1 P(z mk1 |x m ) -q(z mk1 |x m , y m ) P(z mk1 |x m )(1 -P(z mk1 |x m )) ∇ W C P(z mk1 |x m )<label>(6)</label></formula><p>where z mk1 is the abbreviation of z mk = 1 for the space sake. Note that, although we have above gradients for CAN, there are two undesirable problems existing in the optimization: (1) It is not easy to precisely decouple the information from backpropagation respectively for z m and s m , i.e., squeeze out the clean label information for z m and leave the quality-related information to s m ; (2) The corresponding label order between z m and y m may be inconsistent in the optimization. For example, the category in first dimension of z m can be corresponding to the category in the second dimension of y m . To avoid these two problems, we can asymmetrically inject auxiliary information to the optimization procedure in an annealing way, that is, substitute ∇ W C L with the following Eq. <ref type="bibr" target="#b7">(7)</ref>.</p><formula xml:id="formula_11">∇ W C Lmod = (1 -ρ(t)) ∇ W C L + ρ(t)∇ W C Ltemp ,<label>(7)</label></formula><p>where ∇ W C Ltemp is gradient regarding the cross-entropy loss between z m and y m , and ρ(t) = exp(-α * t), α &gt; 0 is a time- varying term. In this equation, ∇ W C Lmod is initially decided by ∇ W C Ltemp and then progressively anneals to ∇ W C L with t increasing. It guarantees the decoupling procedure from the back-propagation with asymmetrical constraint to z m and make the label order of z m and y m consistent in the optimization.</p><p>The optimization procedure can be interpreted as a probabilistic auto-encoder <ref type="bibr" target="#b39">[39]</ref>. However, our model is different from the traditional auto-encoder, which is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. A conventional auto-encoder is symmetric, that is, observed knowledge is encoded into latent variables and decoded to itself, for instance in Fig. <ref type="figure" target="#fig_3">4 (a)</ref>, Y is encoded to Z and S, and then Z and S are used to decode to Y . It is usually used in generative models and their corresponding applications like image generation <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>. In Fig. <ref type="figure" target="#fig_3">4</ref> (b), our model uses an auxiliary variables X in the encoding-decoding procedure, that is, X and Y are used to encode Z and S, and then Z and S are only used to decode Y . Simultaneously, a discriminative model will be involved and jointly optimize with our auto-encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we conduct the quantitative and qualitative experiments to show the superiority of CAN in classification. Specifically, we compare CAN with state-of-the-art methods, investigate its performance with varying training sizes, hyperparameter sensitivity and artificial noise. To present a deep insight on how CAN works, we analyze the quality embedding, latent label estimation and noise transition in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We totally have five image datasets used in the experiments. WEB<ref type="foot" target="#foot_0">foot_0</ref> This dataset is a subset of YFCC100M <ref type="bibr" target="#b43">[43]</ref> collected from the social image-sharing website. It is formed by randomly selecting images from YFCC100M, which belong to the 20 categories of the PASCAL VOC <ref type="bibr" target="#b44">[44]</ref>   AMT 3 This dataset is collected by Zhou et al. <ref type="bibr" target="#b18">[18]</ref> from the Amazon Mechanical Turk platform. They submit 4 breeds of dog images from the Stanford Dog dataset <ref type="bibr" target="#b45">[45]</ref> to Turkers and acquire their annotations. To ease the classification, Zhou et al. also provide a 5376-dimensional feature for each image. The statistics of this dataset is illustrated in the right panel of Fig. <ref type="figure" target="#fig_4">5</ref>. There are 7,354 samples in total and the sample number in each category is between 1k and 2k. All images in this dataset belong to one class. Labels in this dataset may contain annotation error.</p><p>V07 4 This dataset is provided for the 20-cateogry classification task in PASCAL VOC Chanllenge 2007 <ref type="bibr" target="#b44">[44]</ref>. It consists of two subsets: trainging (V07TR) and test (V07TE). There are 5,011 samples in V07TR and 4,592 samples in V07TE. All labels in this dataset are clean.</p><p>V12 5 This dataset is provided for the 20-cateogry classification task in PASCAL VOC Chanllenge 2012 <ref type="bibr" target="#b46">[46]</ref>. It consists of two subsets: trainging (V12TR) and test (V12TE). There are 11,540 samples in V12TR and 10,991 samples in V12TE. All labels in this dataset are clean.</p><p>SD4 6 This last dataset consists of 4 categories of dogs (same to <ref type="bibr" target="#b18">[18]</ref>) in the Stanford Dog dataset <ref type="bibr" target="#b45">[45]</ref>. It is a fine-grained categorization dataset and there are 837 samples in total. We randomly partition samples into training (SD4TR) and test (SD4TE) by 3 : 1 to use. All labels in this dataset are clean. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>For WEB, V07 and V12 datasets, a 34-layer residual network <ref type="bibr" target="#b4">[4]</ref> is adopted as the convolutional networks in CAN, and this configuration is also applied to all baselines to be fair. In the training phase, we first resize the short side of each image to 224 and then follow the transformations in the residual network <ref type="foot" target="#foot_1">7</ref> to preprocess images. In the test phase, we average the results of six-crop images as the final prediction. For AMT and SD4 datasets, we directly use the features provided by <ref type="bibr" target="#b18">[18]</ref>. Hence, one 3-layer perception network (5376→1024, ReLU, 1024→30, ReLU, 30→4) is adopted as the substitution of the convolutional networks in CAN. Both the temperature τ in the Gumbel-softmax function and the annealing coefficient ρ in Eq. ( <ref type="formula" target="#formula_11">7</ref>) vary with the formula max 0.5, exp(-3x10 -5 xStep) . N in the sampler is set to 1 following <ref type="bibr" target="#b39">[39]</ref>. The regularizer coefficient λ is empirically set to 0.3. The batch size is set to 50 and the learning rate starting from 0.01 is divided by 10 every 30 epochs. All experiments run 90 epochs. For the evaluation metric, we adopt Average Precision (AP) and mean Average Precision (mAP) like <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b46">[46]</ref>.</p><p>In the following sections of "model comparision", "impact of training size" and "hyperparameter sensitivity", we train all models on WEB and AMT datasets and test them on V07TE, V12TE and SD4TE datasets. Note that, models trained on WEB dataset are evaluated on both V07TE and V12TE datasets since they have same categories. And models trained on AMT dataset are ony evaluated on SD4TE dataset. For the "artificial noise" section, we first quantitatively add noise to V07TR, V12TR and SD4TR datasets, and then train all models. Finally, we test them on V07TE, V12TE and SD4TE datasets.  C. Classification Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training with real-world noisy datasets:</head><p>To demonstrate the effectiveness of the proposed method in classification, we compare CAN with three state-of-the-art approaches, LearnQ <ref type="bibr" target="#b14">[14]</ref>, ICNM <ref type="bibr" target="#b22">[22]</ref> and Bootstrap <ref type="bibr" target="#b24">[24]</ref>. Besides, two baselines Resnet-N and MLP-N are added, which directly train the 34layer residual network and the 3-layer perception network on WEB dataset and AMT dataset. The classification performance for each category on the V07TE, V12TE and SD4TE datasets is reported in Table . II, III, and IV.</p><p>From the results in Table <ref type="table" target="#tab_2">II</ref> and III, we find CAN outperforms all baselines in terms of mAP and show improvement almost in all categories. For example, on V07TE dataset, CAN achieves 83.8% mAP, which outperforms Resnet-N by 4.3% mAP and the best baseline Bootstrap by 1.9% mAP. In the challenging categories such as "bottle", "chair" and "sofa", it also achieves significant improvement. However, although the results of LearnQ, ICNM and Bootstrap are better than those of Resnet-N, the improvement is still limited. Similarly in Table . IV, CAN outperforms the baselines by at least 2.8% mAP while LearnQ, ICNM and Bootstrap only improve about 1.6% mAP compared with MLP-N.</p><p>Based on above experiments, we have the following interpretations. (1) LearnQ and ICNM, which only introduce the latent label to handle the label noise, cannot prevent noise from degenerating the classifier sufficiently. (2) Bootstrap shares the similar idea with CAN in the aspect of estimating the posterior label for training. But its loss function uses the linear combination of predictions and noisy labels, which still cannot prevent the error back-propagation from label noise. (3) Our approach, which one one hand models the trustworthiness of noisy labels to reduce the noise effect, and on the other hand estimates the latent label in the posterior perspective to train the classifier, shows better classification performance.</p><p>2) Impact of training size: To explore the reliability of the proposed method when the training size changes, we compare CAN with other methods on different scales of datasets. We randomly sample different ratios of subsets in WEB and AMT datasets for training, and illustrate results of all the methods on V07TE, V12TE and SD4TE in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>From Fig. <ref type="figure" target="#fig_5">6</ref>, the results of all methods on these datasets decline with the decrease of the training size. However, CAN performs better than other models persistently. For instance, in the left panel of Fig. <ref type="figure" target="#fig_5">6</ref>, when the training size accounts at 20%, CAN achieves 81.0% mAP on the V07TE dataset, while ICNM and LearnQ are even worse than the most simple Resnet-N (79.4% mAP). Similar clues can be found in the middle and right panels. These results demonstrate the reliability of CAN on different scales of datasets.</p><p>In Fig. <ref type="figure" target="#fig_5">6</ref>, we also find the decline trend on SD4TE dataset is more significant than that on V07TE and V12TE datasets. This is because that even if the 20% subset, there are still about 20k samples for training in WEB dataset. But there are only about 1.6k samples remaining in AMT dataset, which may lack enough knowledge to learn the classifier in the training.</p><p>3) hyperparameter sensitivity: To investigate the reliability of CAN with different the regularizer coefficients, we set λ to 0, 0.2, 0.5, 1, 5, 10 to respectively validate its effect. The results are illustrated in Table . V. From this table, we find the performance on all datasets first grows to a peak and then gradually decreases with λ increasing. For example, CAN achieves 85.2% mAP on V12TE dataset when λ =0.2, but significantly decreases to below 76.6% mAP when λ =10. This indicates: (1) the regularizer in the proper degree encourages our model to  (2) too strong regularization may induce the solution to depart from the optimal. Empirically, setting λ between 0 to 1 makes the variational mutual information regularizer collaborate well with KL-divergences. 4) controlled experiments with artificial noise: In previous sections, all models are trained on WEB and AMT with given noise, which does not exhibit the characteristics in different noise levels. To show the superiority of CAN, we quantitatively add noise on V07TR, V12TR and SD4TR datasets for training, and then compare the classification performance of all models on the V07TE, V12TE and SD4TE datasets. The way to add noise to datasets is by setting a corruption probability P noise to randomly decide whether to shuffle elements of each clean label vector or not. We list the model performance in different P noise settings in Table . VI.</p><p>As shown in Table . VI, when the corruption probability P Noise =1.0, the classification results of all models are close to randomness. With P Noise varying from 1.0 to 0, all models show improvement, since there are some clean samples available for training. Specially when P Noise is set to 0.8, 0.6, 0.4, 0.2, CAN robustly outperforms other baselines. However, when the training data becomes purely clean, i.e., P Noise =0, all noise-aware models are worse than Resnet-N and MLP-N. Table . VI indicates: (1) The performance of all existing models is strongly-related to the noise level in the datasets. All noiseaware models perform bad in the heavy noise. (2) When the training data is clean, noise-aware models may be worse than models without considering noise. (3) CAN shows advantages in different noise levels compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Visualization</head><p>To give a deep insight on how CAN works, in this section, we will present the qualitative analysis about quality embedding, latent label estimation and noise transition in CAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) quality embedding:</head><p>The quality variable is estimated in the embedding space by the contrastive layer. To visualize this mechanism, we respectively forward all the training samples into CAN to compute their quality embedding. By comparing the consistency between the prior prediction (thresholded by 0.5) and the noisy label, we then binarize each embedding as trustworthy embedding or non-trustworthy embedding. If we only consider the Gaussian mean of each quality variable plus the embedding type, a low dimensional visualization of quality embedding can be illustrated with t-SNE package <ref type="bibr" target="#b47">[47]</ref>.</p><p>In Fig. <ref type="figure" target="#fig_6">7</ref>, two exemplar categories "aeroplane" and "bike" in WEB dataset, and two exemplar categories "Norfolk Terrier" and "Norwich Terrier" in AMT dataset, are presented. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, the embedding in each category exhibits two distinguishable clusters. It indicates CAN can identify mismatches between latent labels and noisy labels, and selectively embed the quality variable to different subspace based on the training samples. Thus the label noise can be effectively reduced with the auxiliary of the quality variable.</p><p>Besides, we find the embedding for the first two categories are better than that for the last two categories in Fig. <ref type="figure" target="#fig_6">7</ref>. It is because the categories in WEB and AMT datasets are notably different in number and diversity of training samples. For example, there are about 4,200 different images and annotations in the "aeroplane", while there are only about 200 different images and 1,300 annotations in the "Norfolk Terrier". Thus embedding in the first two categories is uniformly distributed but in the last two categories is discretely cluttered.</p><p>2) latent label estimation: The latent label is estimated in the posterior perspective by the additive layer. To visualize this estimation, we forward all the training samples into CAN to compute output of the additive layer. In Fig. <ref type="figure" target="#fig_7">8</ref>   may be totally unrelated to the image content, e.g., "bottle" for the first aeroplane image; (2) In AMT, the Turkers also assign the wrong labels to the fine-grained images. The former error is usually from the batch annotation function provided by the Flickr website. The latter error is usually from the limit domain knowledge of Turkers. Nevertheless, from the estimation, we find our additive layer still successfully rectifies the wrong labels. Thus based on these latent labels for training, CAN achieves the better performance than other baselines.</p><note type="other">Norfolk</note><p>3) noise transition: To explore how the quality embedding intermediates the mismatch between latent labels and noisy labels, we investigate the transition patterns between latent labels and noisy labels. Firstly, we forward all the training samples to CAN to compute quality embeddings and latent labels. Secondly, we utilize K-means to binarize quality embeddings (only consider Gaussian mean) into trustworthy embedding and non-trustworthy embedding. Thirdly, we count transitions from latent labels to noisy labels conditioned on two types of embeddings. In Fig. <ref type="figure" target="#fig_8">9</ref>, we respectively plot two transition patterns with heatmaps for WEB dataset and AMT dataset.</p><p>As shown for WEB dataset in Fig. <ref type="figure" target="#fig_8">9</ref>, the diagonal of the transition pattern conditioned on trustworthy embedding is dominant. In this case, noisy labels are considered to be reliable and thus transition should mainly happen among same labels. However, the transition patterns conditioned on nontrustworthy embedding is diffusing. Because in this case, noisy labels are considered not correct and transition usually happen between different labels. Similarly, transition patterns on AMT dataset in Fig. <ref type="figure" target="#fig_8">9</ref> also have these characteristics. Fig. <ref type="figure" target="#fig_8">9</ref> indicates CAN is based on quality embedding to automatically disturb the latent label to match the noisy label.</p><p>The transition pattern conditioned on non-trustworthy embedding usually reflects the real-world noise. Some interesting patterns can be found. For instance, according to the second panel of Fig. <ref type="figure" target="#fig_8">9</ref>, "plt" class has less transition to other classes while the transition between "prs" and "tv" has high value. It means: (1) people who upload the "pottedplant" images to social websites almost do not annotate it wrong; (2) for "tv" images, some people focus on persons in the TV program, and others may pay attention to TV itself. Similarly in the fourth panel of Fig. <ref type="figure" target="#fig_8">9</ref>, the transition on AMT usually exists in the appear-similar dogs, i.e., "Norfolk Terrier" and "Norwich Terrier", "Irish Wolfhound" and "Scottish Wolfhound". It reflects that it is more difficult to distinguish these two breeds of dogs than other pairs in some sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present a quality embedding model to learn the classifier from noisy image labels, which effectively avoid the error back-propagated from label noise. To instantiate the model, a Contrastive-Additive Noise network is well-designed. Regarding parameter estimation, we deduce an efficient SGD optimization algorithm by applying recent discrete and continuous reparameterization tricks. We demonstrate our model outperform other noise-aware deep learning methods on some noisy training datasets. Simultaneously, detailed visualization on three key parts is presented to give a deep insight on our model. However, we only validate our model in image data in this paper and other types of contents can be further explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A COMPUTATION FOR KL-DIVERGENCES AND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGULARIZERS</head><p>The remaining four terms in the RHS of Eq. ( <ref type="formula" target="#formula_5">4</ref>) can be calculated without sampling. For example, for the latent label z m , both q(z m |x m , y m ) and P(z m |x m ) are two K-dimensional multinomial probabilities. Their KL-divergence term and regularizer can be simplified by enumerating each dimension. For the quality variable s m , it is from the D-dimensional Gaussian space N(µ(x m , y m ), diag(σ 2 (x m , y m ))) whose parameters are implicitly modeled with network of input x m and y m . If we assume its prior P(s m ) is N(0, 1) like <ref type="bibr" target="#b39">[39]</ref>, it is easy to compute their KL-divergence and the regularizer due to the conjugation. In Eq. ( <ref type="formula" target="#formula_12">8</ref>), we give their simplifications bigeminally. </p><p>ACKNOWLEDGMENT</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Analysis about back-propagation in previous methods that model the latent label, as well as our idea to avoid the effect of label noise. (a) All images are forward into the model and the mismatch error caused by both label estimation and label noise are back-propagated. (b)With quality embedding as a control from latent labels to predictions, the negative effect of label noise is reduced in the back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Quality embedding model for noisy image labels. The shaded nodes as the observed variables are image X and its noisy label vector Y. The latent label vector Z and the quality variable vector S are latent variables. Solid lines and dashed lines represent the generative process and the inference process respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Difference between the conventional auto-encoder and our model. Solid lines and dashed lines respectively represent generative (decoding) and inference (encoding) procedures. (a) a conventional auto-encoder is symmetric that observed knowledge is used to encode to latent variables and decoded symmetrically. (b) Our model uses an auxiliary variables X in this encoding-decoding procedure and meanwhile learns a discriminative part (X to Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Left: the instance number of each category in WEB dataset. Right: the instance number of each category in AMT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Classification results with different training sizes. We sample the subsets of WEB and AMT with five different ratios for training, and evaluate all models on V07TE, V12TE and SD4TE datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Quality embedding visualization of two categories in WEB dataset and two categories in AMT dataset. Better distinguishability of clusters indicates better identifiability of mismatches between latent labels and noisy labels. Blue: trustworthy embedding, Green: non-trustworthy embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Exemplars on latent label estimation of WEB dataset (the first two rows) and AMT dataset (the third row). We forward the noisy label (black word in title) and the image into CAN and compute the latent label (red word in title).</figDesc><graphic coords="10,66.03,283.54,58.53,58.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Transition patterns among labels conditioned on trustworthy embedding and non-trustworthy embedding on WEB dataset (the first two panels) and AMT dataset (the last two panels). Transition conditioned on trustworthy embedding requires the consistency between the latent label and the noisy label, and thus concentrates on the diagonal. Transition conditioned on non-trustworthy embedding identifies the mismatch between the latent label and the noisy label, and thus diffuses from the diagonal.</figDesc><graphic coords="10,96.30,423.76,58.58,58.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>D</head><label></label><figDesc>KL [q(z m |x m , y m )||P(z m |x m )]λ E q(z m |x m ,y m ) [ln q(z m |x m , y m )] mk |x m , y m ) ln q(z mk |x m , y m ) 1-λ P(z mk |x m ) D KL [q(s m |x m , y m )||P(s m )]λ E q(s m |x m ,y m ) [ln q(s m |x m , y m )] λ ) ln σ 2 d (x m , y m )σ 2 d (x m , y m ) + 1 2 µ(x m , y m ) T µ(x m , y m ) + const</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Notations and their descriptions frequently used in this paper</figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>M</cell><cell>number of training items</cell></row><row><cell>K</cell><cell>number of categories</cell></row><row><cell>D</cell><cell>dimension of the quality variable</cell></row><row><cell>X</cell><cell>image variable</cell></row><row><cell>Y</cell><cell>noisy label vector variable</cell></row><row><cell>Z</cell><cell>latent label vector variable</cell></row><row><cell>S</cell><cell>quality vector variable</cell></row><row><cell>W C</cell><cell>parameter of classifier network</cell></row><row><cell>W N</cell><cell>parameter of noise network</cell></row><row><cell>W Q</cell><cell>parameter of annotation quality network</cell></row><row><cell>W L</cell><cell>parameter of latent label network</cell></row><row><cell>m</cell><cell>index of an item</cell></row><row><cell>x m</cell><cell>mth observed image</cell></row><row><cell>y m</cell><cell>mth observed noisy label vector</cell></row><row><cell>z m</cell><cell>mth latent label vector</cell></row><row><cell>s m</cell><cell>mth quality vector</cell></row><row><cell>µ</cell><cell>mean of Guassian distribution</cell></row><row><cell>σ</cell><cell>covariance diagonal of Gaussian distribution</cell></row><row><cell>λ</cell><cell>regularizaion cofficient</cell></row><row><cell>γ m</cell><cell>mth sample from Gumbel distribution</cell></row><row><cell>ζ m</cell><cell>mth sample from Gaussian distribution</cell></row><row><cell>τ</cell><cell>temperature in Gumbel-SoftMax</cell></row><row><cell>α</cell><cell>time-varying coefficient</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Classification Results on V07TE</figDesc><table><row><cell>Model</cell><cell>aer</cell><cell>bik</cell><cell>brd</cell><cell>boa</cell><cell>btl</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>cha cow</cell><cell>tbl</cell><cell>dog</cell><cell>hrs mbk prs</cell><cell>plt</cell><cell>shp</cell><cell>sfa</cell><cell>trn</cell><cell>tv</cell><cell>mAP</cell></row><row><cell cols="19">Resnet-N 93.5 85.3 90.1 85.1 51.2 82.3 84.8 91.2 59.3 87.1 72.1 88.7 91.3 88.9 76.1 54.4 87.6 70.0 90.4 61.4 79.5</cell></row><row><cell>LearnQ</cell><cell cols="18">92.8 86.1 91.0 87.8 50.2 84.9 85.1 90.9 59.2 88.3 71.1 90.1 91.2 88.1 78.3 56.6 89.1 73.1 90.7 64.3 80.4</cell></row><row><cell>ICNM</cell><cell cols="18">92.5 86.2 90.5 87.9 47.7 84.0 84.8 90.6 59.8 88.3 72.7 89.8 91.5 87.2 77.0 57.0 88.9 71.5 91.2 65.7 80.3</cell></row><row><cell cols="19">Bootstrap 94.0 88.4 90.3 88.2 51.7 83.8 86.5 91.0 65.4 88.0 77.4 90.4 91.8 90.8 79.8 55.2 92.8 75.2 90.8 66.4 81.9</cell></row><row><cell>CAN</cell><cell cols="18">95.5 87.0 91.4 89.9 60.1 85.5 87.6 92.0 67.2 90.1 77.7 91.8 93.3 90.6 82.1 56.0 93.6 80.7 94.5 70.6 83.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Classification Results on V12TE</figDesc><table><row><cell>Model</cell><cell>aer</cell><cell>bik</cell><cell>brd</cell><cell>boa</cell><cell>btl</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>cha cow</cell><cell>tbl</cell><cell>dog</cell><cell>hrs mbk prs</cell><cell>plt</cell><cell>shp</cell><cell>sfa</cell><cell>trn</cell><cell>tv</cell><cell>mAP</cell></row><row><cell cols="19">Resnet-N 98.4 81.1 92.9 88.7 57.0 87.4 73.2 96.6 63.3 90.0 63.9 94.3 95.0 92.9 76.8 43.8 92.9 67.2 93.1 65.1 80.7</cell></row><row><cell>LearnQ</cell><cell cols="18">98.4 83.8 93.8 88.5 53.5 87.8 73.7 96.5 64.3 90.6 62.6 94.6 96.1 91.6 78.4 46.8 92.8 69.0 94.0 65.4 81.1</cell></row><row><cell>ICNM</cell><cell cols="18">98.1 82.9 93.6 88.9 53.4 87.7 72.3 96.2 64.7 91.2 66.3 94.2 96.2 91.4 78.0 44.0 93.5 69.3 94.4 66.9 81.2</cell></row><row><cell cols="19">Bootstrap 98.6 84.1 93.6 90.9 56.3 89.8 75.5 96.3 69.8 91.6 69.9 94.4 95.8 93.2 82.2 43.2 92.8 70.9 95.4 67.4 82.6</cell></row><row><cell>CAN</cell><cell cols="18">98.8 84.1 95.3 93.2 62.1 90.8 77.0 97.9 72.6 94.4 73.5 96.1 97.7 94.3 82.4 45.5 95.8 71.4 95.8 68.6 84.4</cell></row></table><note><p>category ranges from 4k to 8k. Most of images in this dataset belong to one class and about 10k images have two or more. Labels in this dataset may contain annotation error.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Classification Results on SD4TE</figDesc><table><row><cell>Model</cell><cell>nft</cell><cell>nwt</cell><cell>iwh</cell><cell>swh</cell><cell>mAP</cell></row><row><cell>MLP-N</cell><cell>78.1</cell><cell>73.2</cell><cell>80.9</cell><cell>76.5</cell><cell>77.2</cell></row><row><cell>LearnQ</cell><cell>80.5</cell><cell>73.7</cell><cell>83.0</cell><cell>77.7</cell><cell>78.7</cell></row><row><cell>ICNM</cell><cell>80.5</cell><cell>72.8</cell><cell>83.9</cell><cell>78.3</cell><cell>78.9</cell></row><row><cell cols="2">Bootstrap 80.7</cell><cell>72.5</cell><cell>83.7</cell><cell>78.1</cell><cell>78.8</cell></row><row><cell>CAN</cell><cell>82.0</cell><cell>79.0</cell><cell>81.8</cell><cell>83.8</cell><cell>81.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Classification results with different λ in CAN.</figDesc><table><row><cell>λ</cell><cell>0</cell><cell>0.2</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell cols="2">V07TE 82.9</cell><cell>83.5</cell><cell>84.8</cell><cell>83.6</cell><cell>80.7</cell><cell>78.8</cell><cell>77.0</cell></row><row><cell cols="2">V12TE 84.3</cell><cell>85.2</cell><cell>84.1</cell><cell>83.0</cell><cell>80.8</cell><cell>78.3</cell><cell>76.6</cell></row><row><cell cols="2">SD4TE 78.6</cell><cell>80.7</cell><cell>80.4</cell><cell>79.9</cell><cell>76.4</cell><cell>73.9</cell><cell>71.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Model performance (mAP) with Quantitative noise.</figDesc><table /><note><p>find a good solution;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>plane) motor(bike) cat(bird) cat(boat) table(bottle) chair(bus) person(car) sofa(cat) cow(chair) tv(cow) bottle(table) cat(dog) bike(horse) sheep(motor) train(person) sofa(plant) plane(sheep) bus(sofa) bike(train) chair(tv)</head><label></label><figDesc>, we present 20 examples of WEB dataset and 8 examples of AMT dataset.</figDesc><table /><note><p><p><p>From Fig.</p>8</p>, we observe: (1) the annotations in WEB dataset bottle(</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1"><p>https://github.com/facebook/fb.resnet.torch</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">V12TE SD4TE P noise Resnet-N LearnQ ICNM Bootstrap CAN Resnet-N LearnQ ICNM Bootstrap CAN MLP-N LearnQ ICNM Bootstrap CAN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint segmentation and recognition of categorized objects from noisy web image collection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4070" to="4086" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation for social images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from weak and noisy labels for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="486" to="500" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">447</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from the wisdom of crowds by minimax entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2195" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frenay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep classifiers from image tags in the wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Community-Organized Multimodal Mining: Opportunities for Novel Solutions</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03683</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep networks from noisy labels with dropout regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nokleby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="967" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02391</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational bayesian inference with stochastic search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</editor>
		<meeting>the 29th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1367" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>just-accepted</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of the effect of different types of noise on the precision of supervised learning techniques</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Nettleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orriols-Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="306" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning Visual Features from Large Weakly Supervised Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="775" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1799</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The new data and new challenges in multimedia research</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2007 results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2012 results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
