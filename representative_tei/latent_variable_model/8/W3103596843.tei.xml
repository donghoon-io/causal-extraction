<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Adversarial Neuro-Tensorial Approach for Learning Disentangled Representations</title>
				<funder ref="#_X6Fb5dw">
					<orgName type="full">EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans</orgName>
				</funder>
				<funder>
					<orgName type="full">Partner University Fund</orgName>
				</funder>
				<funder ref="#_qpDe99b #_jTyj7vA">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Imperial College London</orgName>
				</funder>
				<funder>
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Faculty Award</orgName>
				</funder>
				<funder>
					<orgName type="full">Adobe</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-02-16">16 February 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengjiao</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0002-4873-5677</idno>
						</author>
						<author>
							<persName><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
						</author>
						<title level="a" type="main">An Adversarial Neuro-Tensorial Approach for Learning Disentangled Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-16">16 February 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11263-019-01163-7</idno>
					<note type="submission">Received: 23 February 2018 / Accepted: 4 February 2019 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial autoencoder</term>
					<term>Disentangled representation</term>
					<term>Tensor decomposition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several factors contribute to the appearance of an object in a visual scene, including pose, illumination, and deformation, among others. Each factor accounts for a source of variability in the data, while the multiplicative interactions of these factors emulate the entangled variability, giving rise to the rich structure of visual object appearance. Disentangling such unobserved factors from visual data is a challenging task, especially when the data have been captured in uncontrolled recording conditions (also referred to as "in-the-wild") and label information is not available. In this paper, we propose a pseudo-supervised deep learning method for disentangling multiple latent factors of variation in face images captured in-the-wild. To this end, we propose a deep latent variable model, where the multiplicative interactions of multiple latent factors of variation are explicitly modelled by means of multilinear (tensor) structure. We demonstrate that the proposed approach indeed learns disentangled representations of facial expressions and pose, which can be used in various applications, including face editing, as well as 3D face reconstruction and classification of facial expression, identity and pose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The appearance of visual objects is significantly affected by multiple factors of variability such as, for example, pose, illumination, identity, and expression in case of faces. Each factor accounts for a source of variability in the data, while their complex interactions give rise to the observed entangled variability. Discovering the modes of variation, or in other words disentangling the latent factors of variations in visual data, is a very important problem in the intersection of statistics, machine learning, and computer vision.</p><p>Factor analysis <ref type="bibr" target="#b11">(Fabrigar and Wegener 2011)</ref> and the closely related Principal Component Analysis (PCA) <ref type="bibr" target="#b16">(Hotelling 1933</ref>) are probably the most popular statistical methods that find a single mode of variation explaining the data. Nevertheless, visual appearance (e.g., facial appearance) is affected by several modes of variations. Hence, methods such as PCA are not able to identify such multiple factors of variation. For example, when PCA is applied to facial images, the first principal component captures both pose and expressions variations.</p><p>An early approach for learning different modes of variation in the data is TensorFaces <ref type="bibr" target="#b39">(Vasilescu and Terzopoulos 2002)</ref>. In particular, TensorFaces is a strictly supervised method as it not only requires the facial data to be labelled (e.g., in terms of expression, identity, illumination etc.) but the data tensor must also contain all samples in all different variations. This is the primary reason that the use of such tensor decompositions is still limited to databases that have been captured in a strictly controlled environment, such as the Weizmann face database <ref type="bibr" target="#b39">(Vasilescu and Terzopoulos 2002)</ref>.</p><p>Recent unsupervised tensor decompositions methods <ref type="bibr" target="#b33">(Tang et al. 2013;</ref><ref type="bibr">Wang et al. 2017b</ref>) automatically discover the modes of variation in unlabelled data. In particular, the most recent one <ref type="bibr">(Wang et al. 2017b</ref>) assumes that the original visual data have been produced by a hidden multilinear structure and the aim of the unsupervised tensor decomposition is to discover both the underlying multilinear structure, as well as the corresponding weights (coefficients) that best explain Fig. <ref type="figure">1</ref> Given a single in-the-wild image, our network learns disentangled representations for pose, illumination, expression and identity. Using these representations, we are able to manipulate the image and edit the pose or expression the data. Special instances of the unsupervised tensor decomposition are the Shape-from-Shading (SfS) decompositions in Kemelmacher-Shlizerman (2013), <ref type="bibr" target="#b32">Snape et al. (2015)</ref> and the multilinear decompositions for 3D face description in <ref type="bibr">Wang et al. (2017b)</ref>. In <ref type="bibr">Wang et al. (2017b)</ref>, it is shown that the method indeed can be used to learn representations where many modes of variation have been disentangled (e.g., identity, expression and illumination etc.). Nevertheless, the method in <ref type="bibr">Wang et al. (2017b)</ref> is not able to find pose variations and bypasses this problem by applying it to faces which have been frontalised by applying a warping function [e.g., piece-wise affine warping <ref type="bibr" target="#b25">(Matthews and Baker 2004)</ref>].</p><p>Another promising line of research for discovering latent representations is unsupervised Deep Neural Networks (DNNs). Unsupervised DNNs architectures include the Auto-Encoders (AE) <ref type="bibr" target="#b0">(Bengio et al. 2013)</ref>, as well as the Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">(Goodfellow et al. 2014)</ref> or adversarial versions of AE, e.g., the Adversarial Auto-Encoders (AAE) <ref type="bibr" target="#b23">(Makhzani et al. 2015)</ref>. Even though GANs, as well as AAEs, provide very elegant frameworks for discovering powerful low-dimensional embeddings without having to align the faces, due to the complexity of the networks, unavoidably all modes of variation are multiplexed in the latent-representation. Only with the use of labels it is possible to model/learn the manifold over the latent representation, usually as a post-processing step <ref type="bibr" target="#b30">(Shu et al. 2017)</ref>.</p><p>In this paper, we show that it is possible to learn a disentangled representation of the human face captured in arbitrary recording conditions in an pseudo-supervised manner 1 by imposing a multilinear structure on the latent representation of an AAE <ref type="bibr" target="#b30">(Shu et al. 2017)</ref>. To the best of our knowledge, this is the first time that unsupervised tensor decompositions have been combined with DNNs for learning disentangled representations. We demonstrate the power of the proposed approach by showing expression/pose transfer using only the 1 Our methodology uses the information produced by an automatic 3D face fitting procedure <ref type="bibr" target="#b3">(Booth et al. 2017</ref>) but it does not make use of any labels in the training set. latent variable that is related to expression/pose. We also demonstrate that the disentangled low-dimensional embeddings are useful for many other applications, such as facial expression, pose, and identity recognition and clustering. An example of the proposed approach is given in Fig. <ref type="figure">1</ref>. In particular, the left pair of images have been decomposed, using the encoder of the proposed neural network E(•), into many different latent representations including latent representations for pose, illumination, identity and expression. Since our framework has learned a disentangled representation we can easily transfer the expression by only changing the latent variable related to expression and passing the latent vector into the decoder of our neural network D(•). Similarly, we can transfer the pose merely by changing the latent variable related to pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning disentangled representations that explain multiple factors of variation in the data as disjoint latent dimensions is desirable in several machine learning, computer vision, and graphics tasks.</p><p>Indeed, bilinear factor analysis models <ref type="bibr" target="#b34">(Tenenbaum and Freeman 2000)</ref> have been employed for disentangling two factors of variation (e.g., head pose and facial identity) in the data. Identity, expression, pose, and illumination variations are disentangled in <ref type="bibr" target="#b39">Vasilescu and Terzopoulos (2002)</ref> by applying Tucker decomposition [also known as multilinear Singular Value Decomposition (SVD) <ref type="bibr" target="#b8">(De Lathauwer et al. 2000)</ref>] into a carefully constructed tensor through label information. Interestingly, the modes of variation in well aligned images can be recovered via a multilinear matrix factorization <ref type="bibr">(Wang et al. 2017b</ref>) without any supervision. However, inference in <ref type="bibr">Wang et al. (2017b)</ref> might be ill-posed.</p><p>More recently, both supervised and unsupervised deep learning methods have been developed for disentangled representations learning. Transforming auto-encoders <ref type="bibr" target="#b15">(Hinton et al. 2011</ref>) is among the earliest methods for disentangling latent factors by means of auto-encoder capsules. In <ref type="bibr" target="#b10">Desjardins et al. (2012)</ref> hidden factors of variation are disentangled via inference in a variant of the restricted Boltzmann machine. Disentangled representations of input images are obtained by the hidden layers of deep networks in <ref type="bibr" target="#b6">Cheung et al. (2014)</ref> and through a higher-order Boltzmann machine in <ref type="bibr" target="#b27">Reed et al. (2014)</ref>. The Deep Convolutional Inverse Graphics Network <ref type="bibr" target="#b20">(Kulkarni et al. 2015)</ref> learns a representation that is disentangled with respect to transformations such as outof-plane rotations and lighting variations. Methods in <ref type="bibr" target="#b4">Chen et al. (2016)</ref>, <ref type="bibr" target="#b24">Mathieu et al. (2016)</ref>, <ref type="bibr">Wang et al. (2017a)</ref>, <ref type="bibr" target="#b35">Tewari et al. (2017)</ref> and <ref type="bibr" target="#b38">Tran et al. (2017)</ref> extract disentangled and interpretable visual representations by employing adversarial training. Recent works in face modeling <ref type="bibr" target="#b36">(Tewari et al. 2018;</ref><ref type="bibr" target="#b37">Tran and Liu 2018</ref>) also employ self-supervision or pseudo-supervision to learn 3D Morphable Models from images. They rely on the use of a 3D to 2D image rendering layer to separate shape and texture. Contrarily to <ref type="bibr" target="#b36">Tewari et al. (2018)</ref>, <ref type="bibr" target="#b37">Tran and Liu (2018)</ref> the proposed network does not render the 3D shape into a 2D image. Learning the components of a 3D morphable model is an additional advantage of the pseudo-supervision employed. The method in <ref type="bibr" target="#b30">Shu et al. (2017)</ref> disentangles the latent representations of illumination, surface normals, and albedo of face images using an image rendering pipeline. Trained with pseudo-supervision, <ref type="bibr" target="#b30">Shu et al. (2017)</ref> undertakes multiple image editing tasks by manipulating the relevant latent representations. Nonetheless, this editing approach still requires expression labelling, as well as sufficient sampling of a specific expression.</p><p>Here, the proposed network is able to edit the expression of a face image given another single in-the-wild face image of arbitrary expression. Furthermore, we are able to edit the pose of a face in the image which is not possible in <ref type="bibr" target="#b30">Shu et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we will introduce the main multilinear models used to describe three different image modalities, namely texture, 3D shape and 3D surface normals. To this end, we assume that for each different modality there is a different core tensor but all modalities share the same latent representation of weights regarding identity and expression. During training all the core tensors inside the network are randomly initialised and learnt end-to-end. In the following, we assume that we have a set of n facial images (e.g., in the training batch) and their corresponding 3D facial shape, as well as their normals per pixel (the 3D shape and normals have been produced by fitting a 3D model on the 2D image, e.g., <ref type="bibr" target="#b3">Booth et al. 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Facial Texture</head><p>The main assumption here follows from <ref type="bibr">Wang et al. (2017b)</ref>. That is, the rich structure of visual data is a result of multiplicative interactions of hidden (latent) factors and hence the underlying multilinear structure, as well as the corresponding weights (coefficients) that best explain the data can be recovered using the unsupervised tensor decomposition <ref type="bibr">(Wang et al. 2017b)</ref>. Indeed, following <ref type="bibr">(Wang et al. 2017b)</ref>, disentangled representations can be learnt (e.g., identity, expression, and illumination, etc.) from frontalised facial images. The frontalisation process is performed by applying a piecewise affine transform using the sparse shape recovered by a face alignment process. Inevitably, this process suffers from warping artifacts. Therefore, rather than applying any warping process, we perform the multilinear decomposition only on near frontal faces, which can be automatically detected during the 3D face fitting stage. In particular, assuming a near frontal facial image rasterised in a vector x f ∈ R k x ×1 , given a core tensor Q ∈ R k x ×k l ×k ex p ×k id ,<ref type="foot" target="#foot_0">foot_0</ref> this can be decomposed as</p><formula xml:id="formula_0">x f = Q × 2 z l × 3 z ex p × 4 z id ,<label>(1)</label></formula><p>where z l ∈ R k l , z ex p ∈ R k ex p and z id ∈ R k id are the weights that correspond to illumination, expression and identity respectively. The equivalent form in case that we have a number of images in the batch stacked in the columns of a matrix</p><formula xml:id="formula_1">X f ∈ R k x ×n is X f = Q (1) (Z l Z ex p Z id ), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where Q (1) is a mode-1 matricisation of tensor Q and Z l , Z ex p and Z id are the corresponding matrices that gather the weights of the decomposition for all images in the batch. That is, Z ex p ∈ R k ex p ×n stacks the n latent variables of expressions of the images, Z id ∈ R k id ×n stacks the n latent variables of identity and Z l ∈ R k l ×n stacks the n latent variables of illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Facial Shape</head><p>It is quite common to use a bilinear model for disentangling identity and expression in 3D facial shape <ref type="bibr" target="#b2">(Bolkart and Wuhrer 2016)</ref>. Hence, for 3D shape we assume that there is a different core tensor B ∈ R k 3d ×k ex p ×k id and each 3D facial shape x 3d ∈ R k 3d can be decomposed as:</p><formula xml:id="formula_3">x 3d = B × 2 z ex p × 3 z id ,<label>(3)</label></formula><p>where z ex p and z id are exactly the same weights as in the texture decomposition (2). The tensor decomposition for the n images in the batch is therefore written as as</p><formula xml:id="formula_4">X 3d = B (1) (Z ex p Z id ), (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where B (1) is a mode-1 matricization of tensor B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Facial Normals</head><p>The tensor decomposition we opted to use for facial normals was exactly the same as the texture, hence we can use the same core tensor and weights. The difference is that since facial normals do not depend on illumination parameters (assuming a Lambertian illumination model), we just need to replace the illumination weights with a constant.<ref type="foot" target="#foot_1">foot_1</ref> Thus, the decomposition for normals can be written as</p><formula xml:id="formula_6">X N = Q (1) 1 k l 1 Z ex p Z id , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where 1 is a matrix of ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">3D Facial Pose</head><p>Finally, we define another latent variable regarding 3D pose. This latent variable z p ∈ R 9 represents a 3D rotation. We denote by x i ∈ R k x an image at index i. The indexing is denoted in the following by the superscript. The corresponding z i p can be reshaped into a rotation matrix R i ∈ R 3×3 . As proposed in <ref type="bibr" target="#b43">Worrall et al. (2017)</ref>, we apply this rotation to the feature of the image x i created by 2-way synthesis (explained in Sect. 3.5). This feature vector is the i-th column of the feature matrix resulting from the 2-way synthesis (Z ex p Z id ) ∈ R k ex p k id ×n . We denote this feature vector corresponding to a single image as</p><formula xml:id="formula_8">(Z ex p Z id ) i ∈ R k ex p k id . Next (Z ex p Z id ) i is reshaped into a 3 × k ex p k id 3</formula><p>matrix and left-multiplied by R i . After another round of vectorisation, the resulting feature ∈ R k ex p k id becomes the input of the decoders for normal and albedo. This transformation from feature vector (Z ex p Z id ) i to the rotated feature is called rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Architecture</head><p>We incorporate the structure imposed by Eqs. ( <ref type="formula" target="#formula_1">2</ref>), ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_6">5</ref>) into an auto-encoder network, see Fig. <ref type="figure" target="#fig_1">2</ref>. For some matrices</p><formula xml:id="formula_9">Y i ∈ R k yi ×n , we refer to the operation Y 1 Y 2 ∈ R k y1 k y2 ×n as 2-way synthesis and Y 1 Y 2 Y 3 ∈ R k y1 k y2 k y3</formula><p>×n as 3-way synthesis. The multiplication of a feature matrix by B (1) or Q (1) , mode-1 matricisations of tensors B and Q, is referred to as projection and can be represented by an unbiased fully-connected layer.</p><p>Our network follows the architecture of <ref type="bibr" target="#b30">Shu et al. (2017)</ref>. The encoder E receives an input image x and the convolutional encoder stack first encodes it into z i , an intermediate latent variable vector of size 128 × 1. z i is then transformed into latent codes for background z b , mask z m , illumination z l , pose z p , identity z id and expression z ex p via fully-connected layers.</p><formula xml:id="formula_10">E(x) = [z b , z m , z l , z p , z id , z ex p ] T . (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>The decoder D takes in the latent codes as input. z b and z m (128 × 1 vectors) are directly passed into convolutional decoder stacks to estimate background and face mask respectively. The remaining latent variables follow 3 streams:</p><p>1. z ex p (15 × 1 vector) and z id (80 × 1 vector) are joined by 2-way synthesis and projection to estimate facial shape x 3d . 2. The result of 2-way synthesis of z ex p and z id is rotated using z p . The rotated feature is passed into 2 different convolutional decoder stacks: one for normal estimation and another for albedo. Using the estimated normal map, albedo, illumination component z l , mask and background, we render a reconstructed image x. 3. z ex p , z id and z l are combined by a 3-way synthesis and projection to estimate frontal normal map and a frontal reconstruction of the image.</p><p>Streams 1 and 3 drive the disentangling of expression and identity components, while stream 2 focuses on the reconstruction of the image by adding the pose components. The decoder D then outputs the reconstructed image from the latent codes.</p><formula xml:id="formula_12">D(z b , z m , z l , z p , z id , z ex p ) = x.<label>(7)</label></formula><p>Our input images are aligned and cropped facial images from the CelebA database <ref type="bibr" target="#b21">(Liu et al. 2015)</ref> of size 64 × 64, so k x = 3 × 64 × 64. k 3d = 3 × 9375, k l = 9, k id = 80 and k ex p = 15. More details on the network such as the convolutional encoder stacks and decoder stacks can be found in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training</head><p>We use in-the-wild face images for training. Hence, we only have access to the image itself (x) while ground truth labelling for pose, illumination, normal, albedo, expression, identity or 3D shape is unavailable. The main loss function is the reconstruction loss of the image x:</p><formula xml:id="formula_13">E x = E recon + λ adv E adv + λ veri E veri , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where x is the reconstructed image, E recon = x -x 1 is the reconstruction loss, λ adv and λ adv are regularisation weights, E adv represents the adversarial loss and E veri the verification loss. We use the pre-trained verification network V <ref type="bibr" target="#b44">(Wu et al. 2015)</ref> to find face embeddings of our images x and x. As both images are supposed to represent the same person, we minimise the cosine distance between the embeddings:</p><formula xml:id="formula_15">E veri = 1 -cos(V(x), V( x))</formula><p>. Simultaneously, a discriminative network D is trained to distinguish between the generated and real images <ref type="bibr" target="#b13">(Goodfellow et al. 2014)</ref>. We incorporate the discriminative information by following the auto-encoder loss distribution matching approach of Berthelot et al. ( <ref type="formula">2017</ref>). The discriminative network D is itself an auto-encoder trying to reconstruct the input image x so the adversarial loss is</p><formula xml:id="formula_16">E adv = x -D( x) 1 . D is trained to minimise x -D(x) 1 -k t x -D( x) 1 .</formula><p>As fully unsupervised training often results in semantically meaningless latent representations, <ref type="bibr" target="#b30">Shu et al. (2017)</ref> proposed to train with pseudo ground truth values for normals, lighting and 3D facial shape. We adopt here this technique and introduce further pseudo ground truth values for pose x p , expression xexp and identity x id . x p , xexp and</p><p>x id are obtained by fitting coarse face geometry to every image in the training set using a 3D Morphable Model <ref type="bibr" target="#b3">(Booth et al. 2017)</ref>. We incorporated the constraints used in <ref type="bibr" target="#b30">Shu et al. (2017)</ref> for illumination, normals and albedo. Hence, the following new objectives are introduced:</p><formula xml:id="formula_17">E p = z p -x p 2 2 , (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>where x p is a 3D camera rotation matrix.</p><p>123</p><formula xml:id="formula_19">E ex p = f c(z ex p ) -xexp 2 2 , (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>where fc(•) is a fully-connected layer and xexp ∈ R 28 is a pseudo ground truth vector representing 3DMM expression components of the image x.</p><formula xml:id="formula_21">E id = f c(z id ) -x id 2 2 (11)</formula><p>where fc(•) is a fully-connected layer and x id ∈ R 157 is a pseudo ground truth vector representing 3DMM identity components of the image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Multilinear Losses</head><p>Directly applying the above losses as constraints to the latent variables does not result in a well-disentangled representation. To achieve a better performance, we impose a tensor structure on the image using the following losses:</p><formula xml:id="formula_22">E 3d = x 3d -B × 2 z ex p × 3 z id 2 2 , (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where x 3d is the 3D facial shape of the fitted model.</p><formula xml:id="formula_24">E f = x f -Q × 2 z l × 3 z ex p × 4 z id ) 2 2 , (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>where x f is a semi-frontal face image. During training, E f is only applied on near-frontal face images filtered using x p .</p><formula xml:id="formula_26">E n = n f -Q × 2 1 k l 1 × 3 z ex p × 4 z id ) 2 2 (14)</formula><p>where n f is a near frontal normal map. During training, the loss E n is only applied on near frontal normal maps.</p><p>The model is trained end-to-end by applying gradient descent to batches of images, where Eqs. ( <ref type="formula" target="#formula_22">12</ref>), ( <ref type="formula" target="#formula_24">13</ref>) and ( <ref type="formula">14</ref>) are written in the following general form:</p><formula xml:id="formula_27">E = X -B (1) (Z (1) Z (2) • • • Z (M) ) 2 F , (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>where M is the number of modes of variations, X ∈ R k×n is a data matrix, B (1) is the mode-1 matricisation of a tensor B and Z (i) ∈ R k zi ×n are the latent variables matrices.</p><p>The partial derivative of ( <ref type="formula" target="#formula_27">15</ref>) with respect to the latent variable Z (i) are computed as follows: Let x = vec(X) be the vectorised X, ẑ(i) = vec(Z (i) ) be the vectorised Z (i) ,</p><formula xml:id="formula_29">Ẑ(i-1) = Z (1) Z (2) • • • Z (i-1) and Ẑ(i+1) = Z (i+1)</formula><p>• • • Z (M) , then (15) is equivalent with:</p><p>x -</p><formula xml:id="formula_30">(I ⊗ B (1) )vec(Z (1) Z (2) • • • Z (M) ) 2 F = x -(I ⊗ B (1) )(I Ẑ(i-1) ) ⊗ I • I ( Ẑ(i+1) (I ⊗ )) • ẑ (i) 2 2 (16)</formula><p>Consequently the partial derivative of ( <ref type="formula" target="#formula_27">15</ref>) with respect to Z (i) is obtained by matricising the partial derivative of ( <ref type="formula">16</ref>) with respect to Z (i) . The derivation details are in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Derivation Details</head><p>The model is trained end-to-end by applying gradient descent to batches of images, where (12), ( <ref type="formula" target="#formula_24">13</ref>) and ( <ref type="formula">14</ref>) are written in the following general form:</p><formula xml:id="formula_31">E = X -B (1) (Z (1) Z (2) • • • Z (M) ) 2 F , (<label>15</label></formula><formula xml:id="formula_32">)</formula><p>where X ∈ R k×n is a data matrix, B (1) is the mode-1 matricisation of a tensor B and Z (i) ∈ R k zi ×n are the latent variables matrices.</p><p>The partial derivative of ( <ref type="formula" target="#formula_27">15</ref>) with respect to the latent variable Z (i) are computed as follows: Let x = vec(X) be a vectorisation of X , then (15) is equivalent with:</p><formula xml:id="formula_33">X -B (1) (Z (1) Z (2) • • • Z (M) ) 2 F = vec(X -B (1) (Z (1) Z (2) • • • Z (M) )) 2 2 = x -vec(B (1) (Z (1) Z (2) • • • Z (M) )) 2 2 , (<label>17</label></formula><formula xml:id="formula_34">)</formula><p>as both the Frobenius norm and the L 2 norm are the sum of all elements squared.</p><p>x -vec(B (1</p><formula xml:id="formula_35">) (Z (1) Z (2) • • • Z (M) )) 2 2 = x -(I ⊗ B (1) )vec(Z (1) Z (2) • • • Z (M) ) 2 2 , (<label>18</label></formula><formula xml:id="formula_36">)</formula><p>as the property vec(B Z) = (I ⊗ B)vec(Z) <ref type="bibr">holds Neudecker (1969)</ref>.</p><p>Using vec(Z ( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_37">Z (2) ) = (I Z (1) ) ⊗ I • vec(Z (2) ) (Roemer 2012) and let Ẑ(i-1) = Z (1) Z (2) • • • Z (i-1)</formula><p>and ) the following holds:</p><formula xml:id="formula_38">Ẑ (i) = Z (i) • • • Z (M</formula><p>x -</p><formula xml:id="formula_39">(I ⊗ B (1) )vec(Z (1) Z (2) • • • Z (M) ) 2 2 = x -(I ⊗ B (1) )(I Ẑ(i-1) ) ⊗ I • vec( Ẑ (i) ) 2 2 (19) Using vec(Z (1) Z (2) ) = I (Z (2) (I ⊗ )) • vec(Z (1) ) (Roemer 2012) and let Ẑ(i+1) = Z (i+1) • • • Z (M) : x -(I ⊗ B (1) )(I Ẑ(i-1) ) ⊗ I • vec( Ẑ (i) ) 2 2 = x -(I ⊗ B (1) )(I Ẑ(i-1) ) ⊗ I • I ( Ẑ(i+1) (I ⊗ )) • vec(Z (i) ) 2</formula><p>Let ẑ(i) = vec(Z (i) ) be a vectorisation of Z (i) , this becomes:</p><formula xml:id="formula_40">x -(I ⊗ B (1) )(I Ẑ(i-1) ) ⊗ I • I ( Ẑ(i+1) (I ⊗ )) • ẑ (i) 2 2 (16)</formula><p>We then compute the partial derivative of ( <ref type="formula">16</ref>) with respect to ẑ (i) :</p><formula xml:id="formula_41">∂ x -A ẑ (i) 2 2 ∂ ẑ (i) = 2 A T ( A • ẑ (i) -x),<label>(21)</label></formula><p>where</p><formula xml:id="formula_42">A = (I ⊗ B (1) )(I Ẑ(i-1) )⊗ I • I ( Ẑ(i+1) (I ⊗ )).</formula><p>The partial derivative of ( <ref type="formula" target="#formula_27">15</ref>) with respect to Z (i) is obtained by matricising (21).</p><p>To efficiently compute the above mentioned operations, Tensorly <ref type="bibr" target="#b19">(Kossaifi et al. 2016</ref>) has been employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof of Concept Experiments</head><p>We develop a lighter version of our proposed network, a proof-of-concept network (visualised in Fig. <ref type="figure" target="#fig_2">3</ref>), to show that our network is able to learn and disentangle pose, expression and identity.</p><p>In order to showcase the ability of the network, we leverage our newly proposed 4DFAB database <ref type="bibr" target="#b5">(Cheng et al. 2018)</ref>, where subjects were invited to attend four sessions at different times in a span of five years. In each experiment session, the subject was asked to articulate 6 different facial expressions (anger, disgust, fear, happiness, sadness, surprise), and we manually select the most expressive mesh (i.e. the apex frame) for this experiment. In total, 1795 facial meshes from 364 recording sessions (with 170 unique identities) are used. We keep 148 identities for training and leave 22 identities for testing. Note that there are no overlapping of identities between both sets. Within the training set, we synthetically augment each facial mesh by generating new facial meshes with 20 randomly selected expressions. Our training set contains in total 35900 meshes. The test set contains 387 meshes. For each mesh, we have the ground truth facial texture as well as expression and identity components of the 3DMM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disentangling Expression and Identity</head><p>We create frontal images of the facial meshes. Hence there is no illumination or pose variation in this training dataset. We train a lighter version of our network by removing the illumination and pose streams, a proof-of-concept network, visualised in Fig. <ref type="figure" target="#fig_2">3</ref>, on this synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Expression Editing</head><p>We show the disentanglement between expression and identity by transferring the expression of one person to another.</p><p>For this experiment, we work with unseen data (a hold-out set consisting of 22 unseen identities) and no labels. We first encode both input images x i and x j : </p><note type="other">Exp Edit Ground Truth Original Image Expression Our Recon Our Exp Edit Ground Truth</note><p>Fig. <ref type="figure">4</ref> Our network is able to transfer the expression from one face to another by disentangling the expression components of the images. The ground truth has been computed using the ground truth texture with synthetic identity and expression components</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Ground Truth</head><p>Reconstruction Fig. <ref type="figure">5</ref> Given a single image, we infer meaningful expression and identity components to reconstruct a 3D mesh of the face. We compare the reconstruction (last row) against the ground truth (2nd row)</p><formula xml:id="formula_43">E(x i ) = z i ex p , z i id , E(x j ) = z j ex p , z j id , (<label>22</label></formula><formula xml:id="formula_44">)</formula><p>where E(•) is our encoder and z ex p and z id are the latent representations of expression and identity respectively.</p><p>Assuming we want x i to emulate the expression of x j , we decode on:</p><formula xml:id="formula_45">D(z j ex p , z i id ) = x ji , (<label>23</label></formula><formula xml:id="formula_46">)</formula><p>where D(•) is our decoder. The resulting x ji becomes our edited image where x i has the expression of x j . Figure <ref type="figure">4</ref> shows how the network is able to separate expression and identity. The edited images clearly maintain the identity while expression changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">3D Reconstruction and Facial Texture</head><p>The latent variables z ex p and z id that our network learns are extremely meaningful. Not only can they be used to recon-struct the image in 2D, but also they can be mapped into the expression (x ex p ) and identity (x id ) components of a 3DMM model. This mapping is learnt inside the network. By replacing the expression and identity components of a mean face shape with xexp and x id , we are able to reconstruct the 3D mesh of a face given a single input image. We compare these reconstructed meshes against the ground truth 3DMM used to create the input image in Fig. <ref type="figure">5</ref>. At the same time, the network is able to learn a mapping from z id to facial texture. Therefore, we can predict the facial texture given a single input image. We compare the reconstructed facial texture with the ground truth facial texture in Fig. <ref type="figure" target="#fig_3">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Disentangling Pose, Expression and Identity</head><p>Our synthetic training set contains in total 35900 meshes. For each mesh, we have the ground truth facial texture as well as expression and identity components of the 3DMM, from which we create a corresponding image with one of 7 given Fig. <ref type="figure">7</ref> Our network is able to transfer the pose from one face to another by disentangling the pose, expression and identity components of the images. The ground truth has been computed using the ground truth texture with synthetic pose, identity and expression components poses. As there is no illumination variation in this training set, we train a proof-of-concept network by removing the illumination stream, visualised in Fig. <ref type="figure" target="#fig_2">3a</ref>, on this synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pose Editing</head><p>We show the disentanglement between pose, expression and identity by transferring the pose of one person to another.</p><p>Figure <ref type="figure">7</ref> shows how the network is able to separate pose from expression and identity. This experiment highlights the ability of our proposed network to learn large pose variations even from profile to frontal faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments in-the-Wild</head><p>We train our network on in-the-wild data and perform several experiments on unseen data to show that our network is indeed able to disentangle illumination, pose, expression and identity.</p><p>We edit expression or pose by swapping the latent expression/pose component learnt by the encoder E [Eq. ( <ref type="formula" target="#formula_10">6</ref>)] with the latent expression/pose component predicted from another image. We feed the decoder D [Eq. ( <ref type="formula" target="#formula_12">7</ref>)] with the modified latent component to retrieve our edited image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Expression, Pose and Identity Editing in-the-Wild</head><p>Given two in-the-wild images of faces, we are able to transfer the expression, pose of one person to another. We are also able to swap the face of the person from one image to another. Transferring the expression from two different facial images without fitting a 3D model is a very challenging problem. Generally, it is considered in the context of the same person under an elaborate blending framework <ref type="bibr" target="#b45">(Yang et al. 2011)</ref> or by transferring certain classes of expressions <ref type="bibr" target="#b29">(Sagonas et al. 2017)</ref>.</p><p>For this experiment, we work with completely unseen data (a hold-out set of CelebA) and no labels. We first encode both input images x i and x j :</p><formula xml:id="formula_47">E(x i ) = z i ex p , z i id , z i p E(x j ) = z j ex p , z j id , z j p , (<label>24</label></formula><formula xml:id="formula_48">)</formula><p>where E(•) is our encoder and z ex p , z id , z p are the latent representations of expression, identity and pose respectively. Fig. <ref type="figure">8</ref> We compare our expression editing results with <ref type="bibr">Wang et al. (2017b)</ref>. As <ref type="bibr">Wang et al. (2017b)</ref> is not able to disentangle pose, editing expressions from images of different poses returns noisy results</p><p>Assuming we want x i to take on the expression, pose or identity of x j , we then decode on:</p><formula xml:id="formula_49">D(z j ex p , z i id , z i p ) = x jii D(z i ex p , z i id , z j p ) = x ii j D(z i ex p , z j id , z i p ) = x i ji (25)</formula><p>where D(•) is our decoder. The resulting x jii then becomes our result image where x i has the expression of x j . x jii is the edited image where x i changed to the pose of x j . x i ji is the edit where x i 's face changed to the face of x j .</p><p>As there is currently no prior work for this expression editing experiment without fitting an AAM <ref type="bibr" target="#b7">(Cootes et al. 2001)</ref> or 3DMM, we used the image synthesised by the 3DMM fitted models as a baseline, which indeed performs quite well. Compared with our method, other very closely related works <ref type="bibr">(Wang et al. 2017b;</ref><ref type="bibr" target="#b30">Shu et al. 2017)</ref> are not able to disentangle illumination, pose, expression and identity. In particular, <ref type="bibr" target="#b30">Shu et al. (2017)</ref> disentangles illumination of an image while <ref type="bibr">Wang et al. (2017b)</ref> disentangles illumination, expression and identity from "frontalised" images. Hence they are not able to disentangle pose. None of these methods can be applied to the expression/pose editing experiments on a dataset that contains pose variations such as CelebA. If <ref type="bibr">Wang et al. (2017b)</ref> is applied directly on our test images, it would not be able to perform expression editing well, as shown by Fig. <ref type="figure">8</ref>.</p><p>For the 3DMM baseline, we fit a shape model to both images and extract the expression components of the model. This fitting step has high overhead of 20 s per image. We then generate a new face shape using the expression components of one face and the identity components of another face in the same 3DMM setting. This technique has much higher overhead than our proposed method as it requires time-consuming 3DMM fitting of the images. Our expression editing results and the baseline results are shown in Fig. <ref type="figure">9</ref>. Though the baseline is very strong, it does not change the texture of the face which can produce unnatural looking faces shown with original expression. Also, the baseline method can not fill up the inner mouth area. Our editing results show more natural looking faces.</p><p>For pose editing, the background is unknown once the pose has changed, thus, for this experiment, we mainly focus on the face region. Figure <ref type="figure">10</ref> shows our pose editing results. For the baseline method, we fit a 3DMM to both images and estimate the rotation matrix. We then synthesise x i with the rotation of x j . This technique has high overhead as it requires expensive 3DMM fitting of the images.  Figure <ref type="figure">11</ref> shows our results on the task of face swapping where the identity of one image has been swapped with the face of another person from the second image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Quantitative Studies</head><p>We conducted a quantitative measure on the expression editing experiment. We ran a face recognition experiment on 50 pairs of images where only the expression has been transferred. We then passed them to a face recognition network <ref type="bibr" target="#b9">(Deng et al. 2018</ref>) and extracted their respective embeddings. All 50 pairs of embeddings had cosine similarity larger than 0.3. In comparison, We selected 600 pairs of different people from CelebA and computed their average cosine similarity which is 0.062. The histogram of these cosine similarities is visualised in Fig. <ref type="figure" target="#fig_1">12</ref>. This indicates that the expression </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Ablation Studies</head><p>We performed a series of ablation studies. We first trained a network without multilinear losses by simply feeding the concatenated parameters p = [z pose , z ex p , z id ] to the decoder, thus the training of the network is only driven by the reconstruction loss and pseudo-supervision from 3DMM on pose, expression and identity latent variables, i.e., z pose , z ex p and z id . Next, we started to incorporate other losses (i.e., multilinear losses, adversarial loss, verification loss) step by step in the network and trained different models. In this way, we can observe at each step how additional loss may improve the result. In Figs. 13 and 14, we compare the expression and pose editing results. We find that the results without multilinear losses shows some entanglement of the variations in terms of illumination, identity, expression and pose. In particular, the entanglement with illumination is strong, examples can be found in second and ninth row of Fig. <ref type="figure" target="#fig_2">13</ref>. Indeed, by incorporating multilinear losses in the network, the identity   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Discussion on Texture Quality</head><p>It has to be noted that our baseline 3DMM method <ref type="bibr" target="#b3">Booth et al. (2017)</ref> does not change facial texture. It directly samples the original texture and maps it to a 3D face. Hence, the texture quality is exactly the same as that of the original image as no low-dimensional texture representation is used. In terms of texture quality, direct texture mapping has an edge over our proposed method which models the texture using a lowdimensional representation. But direct texture mapping is also prone to artefacts and does not learn the new expression in the texture. Looking at Fig. <ref type="figure">9</ref> column 2, rows 4, 5 and 7, we observe that the texture itself did not change in the baseline result. The eyes and cheeks did not adjust to show a smiling or neutral face. The expression change results from the change in the 3D shape but the texture itself remained the same as in the input. Low-dimensional texture representation does not have this issue and can generate new texture with changed expression.</p><p>Generally methods similar to ours which estimate facial texture is not able to extract the same amount of details as the original image. Figure <ref type="figure" target="#fig_6">15</ref> visualises how our texture reconstruction compares to state-of-the-art works which have been trained on images of higher resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Expression and Identity Interpolation</head><p>We interpolate z i ex p / z i id of the input image x i on the righthand side to the z t ex p / z t id of the target image x t on the left-hand side. The interpolation is linear and at 0.1 interval. For the interpolation we do not modify the background so the background remains that of image x i .</p><p>For expression interpolation, we expect the identity and pose to stay the same as the input image x i and only the expression to change gradually from the expression of the input image to the expression of the target image x t . Figure <ref type="figure" target="#fig_7">16</ref> shows the expression interpolation. We can clearly see the change in expression while pose and identity remain constant.</p><p>For identity interpolation, we expect the expression and pose to stay the same as the input image x i and only the 123 Fig. <ref type="figure">18</ref> Using the illumination and normals estimated by our network, we are able to relight target faces using illumination from the source image. The source ŝsource and target shading ŝtarget are displayed to visualise against the new transferred shading s trans f er . We compare against <ref type="bibr" target="#b30">Shu et al. (2017)</ref> identity to change gradually from the identity of the input image to the identity of the target image x t . Figure <ref type="figure">17</ref> shows the identity interpolation. We can clearly observe the change in identity while other variations remain limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Illumination Editing</head><p>We transfer illumination by estimating the normals n, albedo â and illumination components l of the source (x source ) and target (x target ) images. Then we use ntarget and lsource to compute the transferred shading s trans f er and multiply the new shading by âtarget to create the relighted image result x trans f er . In Fig. <ref type="figure">18</ref> we show the performance of our method and compare against <ref type="bibr" target="#b30">Shu et al. (2017)</ref> on illumination transfer. We observe that our method outperforms <ref type="bibr" target="#b30">Shu et al. (2017)</ref> as we obtain more realistic looking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">3D Reconstruction</head><p>The latent variables z ex p and z id that our network learns are extremely meaningful. Not only can they be used to reconstruct the image in 2D, they can be mapped into the expression (x ex p ) and identity (x id ) components of a 3DMM. This mapping is learnt inside the network. By replacing the expression and identity components of a mean face shape with xexp and x id , we are able to reconstruct the 3D mesh of a face given a single in-the-wild 2D image. We compare these reconstructed meshes against the fitted 3DMM to the input image.  The results of the experiment are visualised in Fig. <ref type="figure" target="#fig_9">19</ref>. We observe that the reconstruction is comparable to other stateof-the-art techniques <ref type="bibr">(Jackson et al. 2017;</ref><ref type="bibr" target="#b12">Feng et al. 2018)</ref>. None of the techniques though capture well the identity of the person in the input image due to a known weakness in 3DMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Normal Estimation</head><p>We evaluate our method on the surface normal estimation task on the Photoface <ref type="bibr" target="#b46">(Zafeiriou et al. 2013</ref>) dataset which has information about illumination. Assuming the normals found using calibrated Photometric Stereo <ref type="bibr" target="#b42">(Woodham 1980)</ref> as "ground truth", we calculate the angular error between our estimated normals and the "ground truth". Figure <ref type="figure" target="#fig_1">20</ref> and Table <ref type="table" target="#tab_3">1</ref> quantitatively evaluates our proposed method against prior works <ref type="bibr">(Wang et al. 2017b;</ref><ref type="bibr" target="#b30">Shu et al. 2017)</ref> in the normal estimation task. We observe that our proposed method performs on par or outperforms previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Quantitative Evaluation of the Latent Space</head><p>We want to test whether our latent space corresponds well to the variation that it is supposed to learn. For our quantitative experiment, we used Multi-PIE <ref type="bibr" target="#b14">(Gross et al. 2010)</ref> as our test dataset. This dataset contains labelled variations in identity, expressions and pose. Disentanglement of variations in Multi-PIE is particularly challenging as its images are captured under laboratory conditions which is quite different from that of our training images. As a matter of fact, the expressions contained in Multi-PIE do not correspond to the 7 basic expressions and can be easily confused. We encoded 10,368 images of the Multi-PIE dataset with 54 identities, 6 expressions and 7 poses and trained a linear SVM classifier using 90% of the identity labels and the latent variables z id . We then test on the remaining 10% z id to check whether they are discriminative for identity classification. We use 10-fold cross-validation to evaluate the accuracy of the learnt classifier. We repeat this experiment for expression with z ex p and pose with z p respectively. Our results in Table <ref type="table" target="#tab_4">2</ref> show that our latent representation is indeed As an ablation study, we test the accuracy of the identity classification of z id from a model trained without the verification. The results in Table <ref type="table" target="#tab_5">3</ref> show that though adding the verification loss improves the performance, the gain is not significant enough to prove that this loss is a substantial contributor of the information.</p><p>In order to quantitatively compare with <ref type="bibr">Wang et al. (2017b)</ref>, we run another experiment on only frontal images of the dataset with 54 identities, 6 expressions and 16 illuminations. The results in Table <ref type="table" target="#tab_6">4</ref> shows how our proposed model outperforms <ref type="bibr">(Wang et al. 2017b</ref>) in these classification tasks. Our latent representation has stronger discriminative power than the one learnt by <ref type="bibr">Wang et al. (2017b)</ref>.</p><p>We visualise, using t-SNE <ref type="bibr" target="#b22">(Maaten and Hinton 2008)</ref>, the latent Z ex p and Z p encoded from Multi-PIE according to their expression and pose label and compare against the latent representation Z 0 learnt by an in-house large-scale adversarial auto-encoder of similar architecture trained with 2 million faces <ref type="bibr" target="#b23">(Makhzani et al. 2015)</ref>. Figures <ref type="figure" target="#fig_10">21</ref> and<ref type="figure" target="#fig_1">22</ref> show that even though our encoder has not seen any images of Multi-PIE, it manages to create informative latent representations that cluster well expression and pose (contrary to the representation learned by the tested auto-encoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Some of our results do still show entanglement in the variations. Sometimes despite only aiming to change expression only, pose or illumination have been modified as well. This happens mainly in very challenging scenarios where for example one of the image shows extreme lighting conditions, is itself black and white or displays large pose variations. Due to the dataset (CelebA) we used, we do struggle with large pose variations. The proof of concept experiments do show that this is possible to be learned with a more balanced dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed the first, to the best our knowledge, attempt to jointly disentangle modes of variation that correspond to expression, identity, illumination and pose using no explicit labels regarding these attributes. More specifically, we proposed the first, as far as we know, approach that combines a powerful Deep Convolutional Neural Network (DCNN) architecture with unsupervised tensor decompositions. We demonstrate the power of our methodology in expression and pose transfer, as well as discovering powerful features for pose and expression classification. For future work, we believe that designing networks with skip connections for better reconstruction quality and which at the same time can learn a representation space where some of the variations are disentangled would be a promising research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Communicated by Dr. Chellappa, Dr. Liu, Dr. Kim, Dr. Torre and Dr. Loy. B Mengjiao Wang m.wang15@imperial.ac.uk 1 Imperial College London, London, UK 2 Stony Brook University, Stony Brook, USA 3 Middlesex University, London, UK</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig.2Our network is an end-to-end trained auto-encoder. The encoder E extracts latent variables corresponding to illumination, pose, expression and identity from the input image x. These latent variables are then fed into the decoder D to reconstruct the image. We impose a multi-</figDesc><graphic coords="5,84.64,56.36,425.32,318.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig.3Our proof-of-concept network is an end-to-end trained autoencoder. The encoder E extracts latent variables corresponding to expression and identity from the input image x. These latent variables are then fed into the decoder D to reconstruct the image. A separate stream also reconstructs facial texture from z id . We impose a multilinear</figDesc><graphic coords="7,107.74,446.99,360.04,197.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Given a single image, we infer the facial texture. We compare the reconstructed facial texture (last row) against the ground truth texture (2nd row) Original Image Pose Our Recon Our Pose Edit</figDesc><graphic coords="9,53.65,252.80,487.21,97.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 Fig. 10 Fig. 11</head><label>91011</label><figDesc>Fig.9Our network is able to transfer the expression from one face to another by disentangling the expression components of the images. We compare our expression editing results with a baseline where a 3DMM has been fit to both input images Original Image Pose Our ReconOur Pose Edit Baseline</figDesc><graphic coords="11,53.65,488.30,487.21,149.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 Fig. 13 Fig. 14</head><label>121314</label><figDesc>Fig. 12 Histogram of cosine similarities on 600 pairs of "non-same" people from CelebA</figDesc><graphic coords="12,308.68,500.51,232.60,133.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15</head><label>15</label><figDesc>Fig. 15 Texture reconstruction compared with Tewari et al. (2018), Tran and Liu (2018). Tewari et al. (2018), Tran and Liu (2018) have been trained with images of higher resolutions of 240 × 240 and 128 × 128 respectively. In comparison our model has only been trained with images of size 64 × 64 pixels</figDesc><graphic coords="14,309.70,489.68,230.44,110.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 16</head><label>16</label><figDesc>Fig. 16 Expression interpolation</figDesc><graphic coords="15,53.65,56.72,487.69,153.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 19</head><label>19</label><figDesc>Fig.19Given a single image, we infer meaningful expression and identity components to reconstruct a 3D mesh of the face. We compare our 3D estimation against recent works(Jackson et al. 2017;<ref type="bibr" target="#b12">Feng et al. 2018)</ref> </figDesc><graphic coords="16,85.15,448.37,424.81,235.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 21</head><label>21</label><figDesc>Fig. 21 Visualisation of our Z ex p and baseline Z 0 t-SNE. Our latent Z ex p clusters better with regards to expression than the latent space Z 0 of an auto-encoder</figDesc><graphic coords="18,203.26,56.66,340.36,154.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,53.65,56.72,487.69,126.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,87.16,227.57,454.12,180.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,54.13,91.10,486.28,246.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Angular error for the various surface normal estimation methods on the Photoface (Zafeiriou et al. 2013) dataset. We also show the proportion of the normals below 35 • and 40 •</figDesc><table><row><cell>Method</cell><cell>Mean ± Std against</cell><cell cols="2">&lt; 35 • (%) &lt; 40 • (%)</cell></row><row><cell></cell><cell>Woodham (1980)</cell><cell></cell><cell></cell></row><row><cell cols="2">Wang et al. (2017b) 33.37 • ± 3.29 •</cell><cell>75.3</cell><cell>96.3</cell></row><row><cell>Shu et al. (2017)</cell><cell>30.09 • ± 4.66 •</cell><cell>84.6</cell><cell>98.1</cell></row><row><cell>Proposed</cell><cell>28.67 • ± 5.79 •</cell><cell>89.1</cell><cell>96.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Classification accuracy results: we try to classify 54 identities using z id , 6 expressions using z ex p and 7 poses using z p . We compare against standard baseline methods such as SIFT and CNN</figDesc><table><row><cell>Features</cell><cell cols="3">Identity (%) Expression (%) Pose (%)</cell></row><row><cell>SIFT and visual bag of</cell><cell>14.60</cell><cell>58.33</cell><cell>55.50</cell></row><row><cell>words, K = 50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT and visual bag of</cell><cell>18.71</cell><cell>59.36</cell><cell>59.46</cell></row><row><cell>words, K = 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Standard CNN model</cell><cell>94.68</cell><cell>96.54</cell><cell>98.78</cell></row><row><cell>Ours (z i denti t y ,</cell><cell>88.29</cell><cell>84.85</cell><cell>95.55</cell></row><row><cell>z ex pressi on , z pose )</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Identity classification accuracy results: we classify 54 identities using z id with and without verification loss</figDesc><table><row><cell>Features</cell><cell>Identity (%)</cell></row><row><cell>Without verification loss</cell><cell>87.94</cell></row><row><cell>Ours (z i denti t y )</cell><cell>88.29</cell></row><row><cell>Without verification loss (frontal only)</cell><cell>99.96</cell></row><row><cell>Ours (z i denti t y , frontal only)</cell><cell>99.98</cell></row><row><cell>Top performing values are given in bold</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Classification accuracy results in comparison withWang et al.   </figDesc><table><row><cell>Identity</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>99.33</cell><cell>19.18</cell></row><row><cell></cell><cell>z ex pressi on (%)</cell><cell>E (Wang et al. 2017b) (%)</cell></row><row><cell>Expression</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>78.92</cell><cell>35.49</cell></row><row><cell></cell><cell>z illuminati on (%)</cell><cell>L (Wang et al. 2017b) (%)</cell></row><row><cell>Illumination</cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>64.11</cell><cell>48.85</cell></row><row><cell cols="2">Top performing values are given in bold</cell><cell></cell></row></table><note><p><p><p><p><p>(2017b): as</p>Wang et al. (2017b)  </p>works on frontal images, we only consider frontal images in this experiment. We try to classify 54 identities using z id versus C, 6 expressions using z ex p versus E and 16 illumination using z ill versus L z i denti t y (%) C</p>(Wang et al. 2017b</p>) (%)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Tensors notation: Tensors (i.e., multidimensional arrays) are and denoted by calligraphic letters, e.g., X . The mode-m matricisation of a tensorX ∈ R I1×I2×•••×I M maps X to a matrix X (m) ∈ R Im × Īm . The mode-m vector product of a tensor X ∈ R I1×I2×...×I M with a vector x ∈ R Im , denoted by X × n x ∈ R I1×I2×•••×In-1×In+1×•••×I N .The Kronecker product is denoted by ⊗ and the Khatri-Rao (i.e., column-wise Kronecker product) product is denoted by . More details on tensors and multilinear operators can be found in<ref type="bibr" target="#b18">Kolda and Bader (2008)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This is also the way that normals are computed inWang et al. (2017b)   up to a scaling factor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>  (20)   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements Mengjiao Wang was supported by an <rs type="funder">EPSRC</rs> DTA from <rs type="funder">Imperial College London</rs>. This work was partially funded by a gift from <rs type="funder">Adobe</rs>, <rs type="funder">NSF</rs> grants <rs type="grantNumber">CNS-1718014</rs> and <rs type="grantNumber">DMS 1737876</rs>, the <rs type="funder">Partner University Fund</rs>, and the <rs type="institution" subtype="infrastructure">SUNY2020 Infrastructure Transportation Security Center</rs> awarded to <rs type="person">Zhixin Shu</rs>, as well as by a <rs type="funder">Google Faculty Award</rs> and <rs type="funder">EPSRC Fellowship DEFORM: Large Scale Shape Analysis of Deformable Models of Humans</rs> (<rs type="grantNumber">EP/S010203/1</rs>) awarded to <rs type="person">Dr. Zafeiriou</rs>. We thank <rs type="institution">Amazon Web Services</rs> for providing computational resources.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qpDe99b">
					<idno type="grant-number">CNS-1718014</idno>
				</org>
				<org type="funding" xml:id="_jTyj7vA">
					<idno type="grant-number">DMS 1737876</idno>
				</org>
				<org type="funding" xml:id="_X6Fb5dw">
					<idno type="grant-number">EP/S010203/1</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">SUNY2020 Infrastructure Transportation Security Center</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecomm ons.org/licenses/by/4.0/</ref>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A robust multilinear model learning framework for 3D faces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4911" to="4919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05360</idno>
		<title level="m">3D face morphable models &quot;in-the-wild</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4DFAB: A large scale 4D database for facial expression analysis and biometric applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5117" to="5126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discovering hidden factors of variation in deep networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Livezey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6583</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1278" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation via generative entangling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5474</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Fabrigar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wegener</surname></persName>
		</author>
		<title level="m">Exploratory factor analysis</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<title level="m">Multi-PIE. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric CNN regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1933">1933. 2017</date>
		</imprint>
	</monogr>
	<note>Analysis of a complex of statistical variables into principal components. International conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Internet based morphable model</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3256" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.1137/07070111X</idno>
		<ptr target="https://doi.org/10.1137/07070111X" />
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tensorly: Tensor learning in python</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-print</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on computer vision (ICCV)</title>
		<meeting>international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders. arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Some theorems on matrix differentiation with special reference to Kronecker matrix products</title>
		<author>
			<persName><forename type="first">H</forename><surname>Neudecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">327</biblScope>
			<biblScope unit="page" from="953" to="963" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of machine learning research, proceedings of the 31st international conference on machine learning</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>machine learning research, proceedings of the 31st international conference on machine learning<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Advanced algebraic concepts for efficient multichannel signal processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Roemer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Ilmenau</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust joint and individual variance explained</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE international conference on computer vision &amp; pattern recognition (CVPR)</title>
		<meeting>IEEE international conference on computer vision &amp; pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural face editing with intrinsic image disentangling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient visual search of videos cast as text retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="606" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic construction of robust spherical harmonic subspaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensor analyzers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015349</idno>
		<ptr target="https://doi.org/10.1162/089976600300015349" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear 3D face morphable model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE computer vision and pattern recognition</title>
		<meeting>eeding of IEEE computer vision and pattern recognition<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilinear analysis of image ensembles: Tensorfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="447" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tag disentangled generative adversarial network for object image re-rendering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/404</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/404" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-sixth international joint conference on artificial intelligence, IJCAI-17</title>
		<meeting>the twenty-sixth international joint conference on artificial intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2901" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning the multilinear structure of visual data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4592" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Photometric method for determining surface orientation from multiple images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02683</idno>
		<title level="m">A lightened CNN for deep face representation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Expression flow for 3D-aware face component transfer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face recognition and verification using photometric stereo: The photoface database and a comprehensive evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIFS.2012.2224109</idno>
		<ptr target="https://doi.org/10.1109/TIFS.2012.2224109" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="135" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Publisher&apos;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
