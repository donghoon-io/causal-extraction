<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidden Markov Pólya trees for high-dimensional distributions</title>
				<funder ref="#_gPuyRNW #_YXVSz2N">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Nakajima Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-12-09">December 9, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Naoki</forename><surname>Awaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistical Science</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<postCode>27708</postCode>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistical Science</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<postCode>27708</postCode>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidden Markov Pólya trees for high-dimensional distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-09">December 9, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2011.03121v3[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Pólya tree (PT) process is a general-purpose Bayesian nonparametric model that has found wide application in a range of inference problems. It has a simple analytic form and the posterior computation boils down to betabinomial conjugate updates along a partition tree over the sample space. Recent development in PT models shows that performance of these models can be substantially improved by (i) allowing the partition tree to adapt to the structure of the underlying distributions and (ii) incorporating latent state variables that characterize local features of the underlying distributions. However, important limitations of the PT remain, including (i) the sensitivity in the posterior inference with respect to the choice of the partition tree, and (ii) the lack of scalability with respect to dimensionality of the sample space. We consider a modeling strategy for PT models that incorporates a flexible prior on the partition tree along with latent states with Markov dependency. We introduce a hybrid algorithm combining sequential Monte Carlo (SMC) and recursive message passing for posterior sampling that can scale up to 100 dimensions. While our description of the algorithm assumes a single computer environment, it has the potential to be implemented on distributed systems to further enhance the scalability. Moreover, we investigate the large sample properties of the tree structures and latent states under the posterior model. We carry out extensive numerical experiments in density estimation and two-group comparison, which show that flexible partitioning can substantially improve the performance of PT models in both inference tasks. We demonstrate an application to a mass cytometry data set with 19 dimensions and over 200,000 observations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Pólya tree (PT) <ref type="bibr" target="#b14">(Freedman, 1963;</ref><ref type="bibr" target="#b12">Ferguson, 1974;</ref><ref type="bibr" target="#b21">Lavine, 1992)</ref> is a stochastic process that generates random probability measures and is introduced as a prior for Bayesian nonparametric inference. While the PT generalizes the Dirichlet process (DP) <ref type="bibr" target="#b11">(Ferguson, 1973)</ref> as it yields the DP under certain hyperparameters <ref type="bibr" target="#b12">(Ferguson, 1974)</ref>, the statistical properties and practical applications of the PT are very different.</p><p>While the DP is most frequently used as a mixing distribution that induces clustering structures, the PT is often adopted for directly modeling probability densities.</p><p>The PT is defined generatively on a recursive partition-or a partition tree-over the sample space through coarse-to-fine sequential probability assignment at each tree split. In a classical (univariate) PT, the tree is dyadic and the conditional probability assigned to the two children nodes at each tree split arises from independent beta priors, which leads to analytic simplicity and ease in computing the posterior. Obtaining the posterior is straightforward from beta-binomial conjugacy and incurs a computational budget that scales only linearly with the sample size, making the PT one of the few nonparametric models applicable to data with massive sample size. Moreover, the posterior computation is embassingly parallelizable across the tree nodes.</p><p>The PT has been applied in various contexts beyond the original application of density estimation. A far-from-exhaustive list includes survival analysis <ref type="bibr" target="#b27">(Muliere and Walker, 1997;</ref><ref type="bibr" target="#b29">Neath, 2003)</ref>, imputing missing values <ref type="bibr" target="#b31">(Paddock, 2002)</ref>, goodness-offit tests <ref type="bibr" target="#b0">(Berger and Guglielmi, 2001)</ref>, two-group comparison <ref type="bibr" target="#b26">(Ma and Wong, 2011;</ref><ref type="bibr" target="#b17">Holmes et al., 2015;</ref><ref type="bibr" target="#b4">Chen and Hanson, 2014;</ref><ref type="bibr" target="#b34">Soriano and Ma, 2017)</ref>, density regression <ref type="bibr" target="#b18">(Jara and Hanson, 2011)</ref>, ANOVA <ref type="bibr" target="#b25">(Ma and Soriano, 2018)</ref>, testing independence <ref type="bibr" target="#b13">(Filippi et al., 2017)</ref>, and hierarchical modeling <ref type="bibr" target="#b6">(Christensen and Ma, 2020)</ref>. The PT has also been utilized in semi-parametric analyses such as in (generalized) linear models <ref type="bibr" target="#b36">(Walker et al., 1999;</ref><ref type="bibr" target="#b15">Hanson and Johnson, 2002;</ref><ref type="bibr" target="#b37">Walker and Mallick, 1997)</ref>.</p><p>Early developments of the PT are based on an a priori fixed partition tree on the sample space. The resulting inference can be sensitive to the choice of the partition points defining the tree. In particular, the resulting process, both a priori and a posteriori, can be jumpy at these points. In hypothesis testing and model choice, this sensitivity is also reflected in the sometimes substantial change in the marginal likelihood/Bayes factor when the partition points are slightly varied. To remedy the issue, <ref type="bibr" target="#b32">Paddock et al. (2003)</ref> modified the PT model so that observations are generated from the PT model with slightly different partition points. <ref type="bibr" target="#b15">Hanson and Johnson (2002)</ref> and <ref type="bibr" target="#b16">Hanson (2006)</ref> proposed a mixture of PTs by defining partition points along quantiles of a parametric model endowed with a hyperprior to allow model averaging on the partition points. This strategy does not allow individual partition points to adapt to local features of the distribution but only the whole set of points to the global structure of the distribution, and is most effective when the underlying density is close to the specified parametric model. Nieto-Barajas and <ref type="bibr" target="#b30">Müller (2012)</ref> took a different approach by modeling the probability assignments within each level of the tree in a correlated manner to smooth out the random measure over the boundaries of partitioning. While these approaches alleviate the sensitivity to partition points in low-dimensional settings, they are not easily applicable (though in principle possible) to problems with even just a handful of dimensions. Moreover, Bayesian inference with these models generally require MCMC, whose effectiveness can (in fact often does) still suffer from the sensitivity with respect to the partition points.</p><p>Another related issue regarding the partitioning scheme of the PT is that in multivariate problems, traditionally the partition tree is constructed by dividing all dimensions of the sample space at each split. For example, for a d-dimensional sample space, each time a tree node is divided, it is split into 2 d children nodes, and probability assignment over these 2 d child nodes is modeled by independent Dirichlet priors. <ref type="bibr" target="#b40">Wong and Ma (2010)</ref> noted that such a "symmetric" partition scheme is undesirable as the dimensionality increases, in which case due to the exponential growth of the partition blocks, the vast majority of the blocks are barely, if at all, populated by data.</p><p>As such they propose to incorporate adaptivity into the partitioning strategy with respect to the structure of the underlying distribution through adopting a Bayesian CART-like prior <ref type="bibr" target="#b5">(Chipman et al., 1998)</ref> on the space of dyadic partition trees.</p><p>However, in order to maintain the analytic simplicity of the posterior and achieving MCMC-free exact Bayesian inference with a linear computational budget, the Bayesian CART-like prior has to be restricted to only divide at the middle point (or otherwise a pre-determined fixed point) on one of the dimensions at each tree split.</p><p>Not only does this hamper the model's ability to adapt to distributional structures, but it makes the model suffer from the same sensitivity with respect to the partition points. Moreover, even with this restriction, the inference algorithm (based on recursive message passing) is only computationally practical for up to about 10 dimensions on continuous sample spaces.</p><p>In a different vein, recent developments have demonstrated that aside from enhancing the partitioning strategy, the PT can also be substantially improved by adopting more flexible priors (as opposed to independent betas) on the probability assignment at each tree split <ref type="bibr" target="#b18">(Jara and Hanson, 2011;</ref><ref type="bibr" target="#b30">Nieto-Barajas and Müller, 2012;</ref><ref type="bibr" target="#b24">Ma, 2017)</ref>.</p><p>One strategy for enriching the PT in this regard is by introducing latent state variables at each tree split and adopt priors on the probability assignment given these states. When the latent states are discrete and modeled with Markov dependency, analytical simplicity is preserved and exact Bayesian inference can proceed through recursive message passing that maintains the linear computational budget <ref type="bibr" target="#b24">(Ma, 2017)</ref>.</p><p>Given these developments, we are motivated by the following questions: Is it possible to incorporate into the PT a very flexible partition tree prior, such as the general Bayesian CART (i.e., without the restriction to partition at middle points), that will (i) enhance its adaptivity to distributional structures in multivariate settings; (ii) resolve its sensitivity to the choice of partition points; and (iii) allow a tractable form of the joint posterior and a posterior inference algorithm that is scalable to moderately high-dimensional problems (e.g., up to 100 dimensions)? Moreover, should such a strategy exist, can the resulting model and inference algorithm be made compatible with incorporating (possibly Markov dependent) latent states on the tree nodes?</p><p>The goals of making the partition tree prior more flexible while enhancing the computational scalability appear at odds with each other. Large tree spaces are well known to be very hard to compute over. In moderate to high dimensional settings exact inference involving flexible tree structures is beyond reach and even the most advanced MCMC approaches tailor-made for trees encounter substantial difficulty due to the pervasive multi-modality of distributions in such spaces. Recent advances in sequential Monte Carlo (SMC) for regression tree models <ref type="bibr" target="#b20">(Lakshminarayanan et al., 2013;</ref><ref type="bibr" target="#b23">Lu et al., 2013)</ref>, however, suggest that efficient inference is possible in moderately high-dimensional settings (up to about 100 dimensions). Moreover, once the partition tree is sampled, the conditional posterior for the rest of the model can be computed analytically through recursive message passing. We will therefore exploit a hybrid strategy that uses a new SMC sampler to efficiently sample from the marginal posterior of the partition tree structure, along with recursive message passing to compute the exact conditional posterior of the latent state variables given the tree.</p><p>Beyond the methodological development, we will also investigate the theoretical properties of the posterior on the partition tree and the latent states. Previous theoretical literature on the PT and related models have mostly focused on establishing the posterior consistency and the contraction rate of the random measure induced under these models <ref type="bibr">(Castillo, 2017a;</ref><ref type="bibr" target="#b3">Castillo and Randrianarisoa, 2021)</ref>. In multivariate settings, however, the partition tree itself is highly informative about the underlying distribution. Moreover, in applications involving model choice and hypothesis testing, it is often the latent states, not the random measures, that are of direct interest. As such, we focus on studying the asymptotic behavior of the marginal posterior on the partition tree and latent states, establishing consistency results on their convergence toward the trees and states that most closely characterize the underlying truth.</p><p>The rest of the paper is organized as follows. In Section 2 we describe a flexible prior on the partition tree structure that relaxes the restriction of "dividing in the middle" on partition points and present a general form of PT models that adopt this prior along with latent states associated with the tree nodes with a Markov dependency structure. In Section 3, we present our hybrid computational strategy that can work effectively up to 100 dimensions consisting of an SMC algorithm for sampling on the marginal posterior of the partition tree and a recursive message passing algorithm for obtaining the exact conditional posterior of the latent states and the random measure given the sampled trees. In Section 4 we investigate the asymptotic properties of the tree structures and latent states identified under the posterior model. In Section 5, we carry out extensive numerical experiments to examine the performance of our method in the context of two important applications of PTs-density estimation and the two-group comparison, followed by an application to a data set from mass cytometry in Section 6. In Section 7 we conclude with a brief discussion. All proofs are provided in Supplementary Materials C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We first review the PT model <ref type="bibr" target="#b11">(Ferguson, 1973;</ref><ref type="bibr" target="#b21">Lavine, 1992)</ref> on a dyadic recursive partition in Section 2.1. The model, while defined on a general multivariate sample space, differs from a traditional multivariate PT which adopts a multi-way symmetric recursive partitioning. Then we introduce a new class of PT models that incorporates both the flexible partition prior and latent states with Markov dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pólya trees defined on recursive dyadic partitions</head><p>Without loss of generality, we consider a continuous sample space represented as a ddimensional rectangle Ω = (0, 1] d . For unbounded sample spaces such as R d , one can transform each margin to [0,1] by applying, say, a cumulative distribution function transform or by standardizing the data based on its observed range of values. We use µ to denote the Lebesgue measure on Ω. A (dyadic) recursive partitioning T on Ω is a sequence of partitions of Ω such that the partition blocks at each level of the partitioning are obtained by dividing each block in the previous level into two children blocks. Formally, we can write T = ∞ k=0 A k , where A k is a partition of Ω in the kth level. More specifically, A 0 = {Ω}, and</p><formula xml:id="formula_0">A ∈ A k (k = 0, 1, 2, . . . ) is divided into A l and A r , which satisfy A l , A r ∈ A k+1 , A l ∪ A r = A, and A l ∩ A r = ∅. (Throughout the</formula><p>paper, a subscript "l" or "r" on a node indicates the left or right child node.) For example, when d = 1 and if the tree is recursively divided at the middle point of each node, then nodes in level k are of the form (l/2 k , (l+1)/2 k ] for some l ∈ {0, . . . , 2 k -1}.</p><p>Another common strategy is to define the tree based on the quantiles of a probability measure</p><formula xml:id="formula_1">F so that A ∈ A k is of the form A = (F -1 ( l 2 k ), F -1 ( l+1 2 k )] for l ∈ 0, . . . , 2 k -1.</formula><p>Given a partition tree T , we can define a random measure Q by putting a prior on the conditional probability θ</p><formula xml:id="formula_2">(A) = Q(A l |A) = 1 -Q(A r |A) at each A ∈ T .</formula><p>Under the PT prior, the parameters θ(A) follow independent beta distributions Beta(α l (A), α r (A)), where α l (A) and α r (A) are hyperparameters. The corresponding posterior, given an i.i.d. sample x 1 , . . . , x n from Q, is again a PT with a simple conjugate update on the conditional proabilities:</p><formula xml:id="formula_3">θ(A) | x 1 , . . . , x n ∼ Beta(α l (A) + n(A l ), α r (A) + n(A r )),</formula><p>where n(A) represents the number of observations in a set A ⊂ Ω. Though the tree needs to be infinitely deep to ensure full support of the PT, for practical purposes, one typically sets a sufficiently large maximum depth (or resolution) of T and compute the posteriors of θ(A)'s defined on this finite tree structure <ref type="bibr" target="#b15">(Hanson and Johnson, 2002)</ref>. We shall refer to a node in the deepest level as a "leaf" or "terminal node".</p><p>On a leaf, the conditional distribution can be set to a baseline F (•|A), such as the uniform distribution µ(•|A). In Section 3 when we present inference algorithms, we shall adopt this practical strategy and assume T is finite and use N (T ) and L(T ) to denote the collection of the non-terminal nodes and the leaf nodes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Incorporating flexible partition points</head><p>We incorporate a Bayesian-CART like prior on T by randomizing both the dimension in which to divide a node and the location to divide. Our prior relaxes the "alwaysdivide-in-the-middle" restriction imposed in <ref type="bibr" target="#b40">Wong and Ma (2010)</ref>. This prior on the partition tree T differs from that in the mixture of PTs of <ref type="bibr" target="#b16">Hanson (2006)</ref>, which does not randomize over the dimension to divide, but generates the boundaries of the tree nodes jointly using quantiles of a parametric family.</p><p>Our prior can be described iteratively as a generative process that recursively divides the sample space. Specifically, suppose we have a node A in the rectanglar</p><formula xml:id="formula_4">form, A = (a 1 , b 1 ]×• • •×(a d , b d ].</formula><p>We divide A into two rectangular children by cutting along a randomly chosen dimension at a random location. The dimension to divide D(A) ∈ {1, 2, . . . , d}, and the (relative) location to divide L(A) ∈ (0, 1) are given independent priors of the following forms:</p><formula xml:id="formula_5">D(A) ∼ Mult(λ 1 (A), . . . , λ d (A)) and L(A) ∼ N L -1 l=1 β l (A)δ l/N L (•),<label>(1)</label></formula><p>where δ x (•) represents the unit point mass at x, and N L -1 is the total number of grid points along (0, 1). Both {λ i (A)} i=1,...,d and {β l (A)} l=1,...,N L -1 sum to 1. In the above, we have adopted a uniform grid over (0, 1) for notational simplicity, but it does not have to be as such. With D(A) = j and L(A) = l/N L , the two children nodes A l and A r are</p><formula xml:id="formula_6">A l = (a 1 , b 1 ] × • • • × (a j , a j + l/N L • (b j -a j )] × • • • × (a d , b d ], A r = (a 1 , b 1 ] × • • • × (a j + l/N L • (b j -a j ), b j ] × • • • × (a d , b d ].</formula><p>In principle one could adopt a continuous prior on the partition location L(A). A discretized prior is helpful, however, because it will substantially simplify posterior computation. In practice, as long as the grid is dense enough, the discrete prior will be practically just as flexible. Indeed we have verified in extensive numerical experiments that when N L is large enough (more than 30 to 50) over a uniform grid, posterior inference no longer improves in any noticeable way.</p><p>For the prior on D(A), we set λ j (A) = 1/d for all nodes A as a default choice.</p><p>When L(A) is given a weak prior widely spread over (0, 1), the resulting inference can be sensitive to the "tail" behavior of the distribution in the node, resulting in high posteriors of L(A) near the extreme values 0 and 1. A detailed discussion on this phenomenon will be provided in Section 5.1.1. This issue can be effectively addressed by making the prior of L(A) depend on the sample size n(A) so that it encourages more balanced divisions at large sample sizes. More specifically, we adopt the following prior with an exponentially decaying tail</p><formula xml:id="formula_7">P (L(A) = l/N L ) = β l ∝ exp [-ηn(A)f (|l/N L -0.5|)] , l = 1, . . . , N L -1, (2)</formula><p>where η ≥ 0 is a hyperparameter and f : [0, ∞) → [0, ∞) is an increasing function with f (0) = 0. In the following, we shall use a function f (x) = x, and so our prior on L(A) is a (discretized) Laplace distribution. We provide theoretical justification for adopting this prior with exponential tails in Section 4.</p><p>Another generalization of the prior on L(A) is to incorporate a spike-and-slab set-up with a spike at the middle point 1/2. In particular, one can adopt a dependent spike prior among the nodes such that once a node A is divided exactly at the middle point, so are its descendants. This generalization will substantially reduce the amount of computation in regions of the sample space where the data are either sparse or lack interesting structure, e.g., close to the uniform distribution. We implement the spike-and-slab in our software but defer the details of this generalization to Supplementary Materials A to avoid distracting the reader from the main ideas.</p><p>Given the tree prior, our PT model now consists of the two components-tree generation and conditional probability assignment. Figures <ref type="figure" target="#fig_14">1(a</ref>   2.3 Hidden Markov Pólya tree models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">General framework</head><p>Next we extend the above model to accommodate two recent developments in the PT literature: (i) incorporating latent state variables along the tree structure and (ii) joint modeling of multiple groups of observations. Incorporating latent variables allow more flexibly characterizing distributional features through adding prior dependency.</p><p>As in recent literature, we consider incorporating discrete state variables that follow a Markov process along the tree structure. Because the description in this section always pertains to the model given the randomly generated partition tree T , for brevity we shall not keep stating "given T ".</p><p>We generalize our notation to allow observing one or more groups of i.i.d. observations. Let G be the number of groups of i.i.d. observations. For the gth group (g = 1, 2, . . . , G), let Q g be the sampling measure for that group. Let Q denote the collection of all G sampling measures. That is, Q = {Q g } G g=1 . Let x g = (x g,1 , . . . , x g,ng )</p><p>denote the observations in the gth group, which are i.i.d. given Q g , where n g , the sample size for the group, is allowed to differ across the groups. We use x = {x g } G g=1</p><p>to denote the collection of all observations from all groups.</p><p>Next we specify a prior on Q in terms of a joint prior on the conditional probability</p><formula xml:id="formula_8">on each A ∈ T , θ g (A) = Q g (A l | A) = 1 -Q g (A r | A).</formula><p>We use latent variable modeling to incorporate prior dependency among the tree nodes. Specifically, let {V (A) : A ∈ T } denote a collection of latent state variables, one for each A, and without loss of generality, assume that V (A) takes discrete values from {1, . . . , I}.</p><p>(In practice, the number of states I can differ among A.) Joint priors of θ g (A) for all g and A are then defined conditionally on these latent states.</p><p>Existing literature has exploited these latent states to characterize both the withingroup structure of each distribution Q g and the between-group relationship among the Q g . An example of within-sample structures is the smoothness of each underlying distribution, which is explored in the context of density estimation <ref type="bibr" target="#b24">(Ma, 2017)</ref>. An example of between-group structures is the difference between two (or more) distributions <ref type="bibr" target="#b34">(Soriano and Ma, 2017)</ref>.</p><p>Dependent modeling of the latent states over the partition tree is desirable as a priori one would expect interesting structures (both within-group and betweengroup) to exhibit themselves in a correlated manner over the sample space. For example, functions tend to have similar smoothness over adjacent locations, and twogroup difference tend to be clustered in space. A computationally efficient strategy for modeling such dependency over the tree is by a hidden Markov process along the tree <ref type="bibr" target="#b8">(Crouse and Baraniuk, 1997)</ref>, which starts from the root node, A = Ω, and sequentially generates the latent states in a coarse-to-fine fashion according to (possibly node-specific) transition matrices ξ(A) whose (i, i )th element is</p><formula xml:id="formula_9">ξ i,i (A) = P (V (A) = i | V (A p ) = i) for i, i ∈ {1, . . . , I},</formula><p>where A p denotes A's parent. (We shall use superscript "p" to indicate the parent of a node in T .) For A = Ω, since Ω has no parent, we can simply let ξ i,i (Ω) be constant over i, representing the initial state probabilities on Ω.</p><p>Given the V (A)'s, {θ g (A)} g=1,...,G can then be modeled as conditionally independent a priori. The specific choices of these conditional priors are problem-dependent.</p><p>We give two examples below. </p><formula xml:id="formula_10">ν(A) | V (A) = i ∼ F i with the (conditional) hyperprior F i given V (A) = i.</formula><p>The hyperpriors F 1 , F 2 , . . . , F I are chosen to be stochastically increasing</p><formula xml:id="formula_11">F 1 ≺ F 2 ≺ • • • ≺ F I . That is, F i ([x, ∞)) ≤ F i+1 ([x, ∞)) for all x &gt; 0.</formula><p>The rate at which the latent states transition into a higher latent state along each subbranch of the partition tree determines the local smoothness of the underlying density.</p><p>Example 2: Two-group comparison</p><p>In two-group comparison, we are interested in testing and identifying differences between two measures Q = {Q g } g=1,2 based on an i.i.d. sample from each. The "global" testing problem can be formulated as testing the following null and alternative hypotheses:</p><formula xml:id="formula_12">H 0 : Q 1 = Q 2 vs H 1 : Q 1 = Q 2 .</formula><p>Noting that two-group differences may exist in parts of the sample space and not others, the coupling OPT <ref type="bibr" target="#b26">(Ma and Wong, 2011)</ref> and the multi-resolution scanning (MRS) model <ref type="bibr" target="#b34">(Soriano and Ma, 2017)</ref> are PT-based models that allow the measures to differ on some nodes A ∈ T and not others. This more "local" perspective on two-group comparison enables these models to not only test for H 0 vs H 1 , but to identify regions on which the two measures differ.</p><p>To achieve this, these models incorporate state variables that characterize whether the conditional probabilities on each A are equal:</p><formula xml:id="formula_13">V (A) = 1 ⇔ Q 1 (A l | A) = Q 2 (A l | A), V (A) = 2 ⇔ Q 1 (A l | A) = Q 2 (A l | A).<label>(3)</label></formula><p>When V (A) = 1, the two corresponding conditional probabilities are given independent beta priors, whereas if V (A) = 2, they are tied and given a single beta prior.</p><p>Markov dependency among the states on different nodes are incorporated to induce the desired spatial correlation of cross-group differences. Additional latent states can be further incorporated to reflect more complex relationships between the distributions. In fact, the MRS also incorporates an additional state V (A) = 3, which introduces the same coupled prior as V (A) = 2, but works as an absorbing state that once V (A) = 3, all descendants of A will remain in that state, corresponding to the case that the conditional distributions Q 1 (•|A), and Q 2 (•|A) are completely equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bayesian inference</head><p>In sum, the models we consider all share a common structure consisting of the following components: (i) the partition tree T defined by the dimension and location variables D's and L's, which follow the priors given in Eq. ( <ref type="formula" target="#formula_5">1</ref>); (ii) the latent state variables V (A) given T which follow a Markov prior; (iii) the conditional probabilities along the given tree T , {θ g (A)} G g=1 , whose joint prior are specified independently across the nodes on T given the latent states; and finally (iv) given the random measures Q g defined by T and θ g (A)'s, we observe an i.i.d. sample x g from each Q g , independently across g. Formally, we have the following full hierarchical model:</p><formula xml:id="formula_14">T | λ, η ∼ p(T | λ, η) {V (A) : A ∈ T } | ξ, T ∼ Markov(ξ) (θ 1 (A), . . . , θ G (A)) | V (A), T ind ∼ p(θ 1 (A), . . . , θ G (A) | V (A)) for A ∈ T x g = (x g,1 , x g,2 , . . . , x g,ng ) | Q g iid ∼ Q g for g = 1, 2, . . . , G.</formula><p>The key to Bayesian inference is the ability to either compute or sample from the joint posterior (T, V, θ) given all data x = (x 1 , . . . , x G ), where V and θ represent the totality of all latent states and conditional probabilities given T respectively. While in some problems such as density estimation one may mainly be interested in just the marginal posterior of the Q g 's, in others such as two-group comparison where one wants to characterize the between-group relationships among the distributions, the latent states (along with T ), which characterizes such relationhips, are often of prime interest. In multivariate and even high-dimensional problems, the tree structure T is also of great interest as it sheds light on the underlying structures in the distributions.</p><p>To this end, we shall take advantage of recent developments in sequential Monte Carlo (SMC) sampling for tree-based models <ref type="bibr" target="#b20">(Lakshminarayanan et al., 2013;</ref><ref type="bibr" target="#b23">Lu et al., 2013)</ref> and advances in message passing algorithms for PT models with Markov dependency <ref type="bibr" target="#b24">(Ma, 2017)</ref>. We introduce a hybrid algorithm that combines these two computational strategies to effectively sample from the joint posterior in highdimensional spaces. Overall, the hybrid algorithm consists of two stages:</p><p>1. Sampling from the marginal posterior of the partition tree of tree structures T 1 , . . . , T M by growing each tree from coarse to fine scales.</p><p>It uses one-step look-ahead message passing to construct proposal distributions for D(A) and L(A), one node at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Computing the conditional posterior given the sampled trees</head><p>Given each tree sampled by the SMC, we analytically compute the exact conditional posteriors of V (A)'s and θ(A)'s using recursive message passing. (The detailed algorithm will be given in Section 3.2.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SMC to sample from tree posterior</head><p>In the SMC stage to sample the trees, each particle stores a realized form of a finite tree structure, and one node of each tree is divided at each step of the SMC sampling.</p><p>Suppose T t is the finite tree obtained after dividing the sample space t times in a particle, and for this tree we define the target distribution</p><formula xml:id="formula_15">π t (T t ) = P (T t | x) ∝ P (T t )P (x | T t ).</formula><p>Here P (T t ) is the joint prior of the variables D(A)'s and L(A)'s for the non-leaf nodes of T t , and P (x | T t ) is the marginal likelihood given the tree T t under the hierarchical model, in which V and θ are integrated out. To sample from this target distribution, we sequentially construct a set of M particles {T m t , W m t } M m=1 , where T m t is a realized tree and W m t is the associated importance weight for the mth particle. Examples of generated trees are given in Figure <ref type="figure">2</ref>, where the sample space has been divided three times, and in the next step, new partition boundaries will be added in the gray nodes.</p><p>Following <ref type="bibr" target="#b20">Lakshminarayanan et al. (2013)</ref>, we adopt in each step of the SMC a breadth-first tree-growth strategy by dividing the "oldest active" leaf node-that is, the one generated in the earliest step and is yet to be terminated in division.</p><p>Further division of a node is terminated once the number of observations in that node is below a pre-set threshold (e.g., 5 in our software implementation) to avoid overfitting. Otherwise a node is bisected along a boundary whose dimension and location are randomly drawn from a proposal distribution. For each particle, a finite tree T t is formed by a sequence of decisions {J s } t s=1 , where J s = (D s , L s ) correspond to all of the variables D(A) and L(A) at the sth step of the SMC. Figure <ref type="figure">2</ref>: An example of realized finite trees in the particle system obtained after the step t = 3. The numbers in the squares indicate in which step the boundaries are drawn. Among the current leaf nodes, the nodes colored gray are the oldest nodes generated in the earliest step, so they are split in the next step.</p><p>As we will see in Proposition 3.1, the target distribution π t (T t ) has a decomposition</p><formula xml:id="formula_16">π t (T t ) = C t π t-1 (T t-1 )π t (J t | T t-1 )w t (T t-1 )</formula><p>, where C t is a constant independent of T t , π t (J t | T t-1 ) a conditional distribution on J t given T t-1 , and w t (T t-1 ) a function of T t-1 . We will choose π t (J t | T t-1 ) as the proposal for J t under which the corresponding importance weight will simply be w t (T t-1 ), independent of J t .</p><p>More specifically, suppose at the current step t, we are to divide A t ∈ T t-1 , into A t,l and A t,r with decision J t . Let M i (A t | J t ) be the marginal likelihood on the node A t under the decision J t evaluated based on the observations in A t . That is,</p><formula xml:id="formula_17">M i (A t | J t ) = G g=1 θ g (A t ) ng(A t,l ) (1 -θ g (A t )) ng(At,r) dP (θ 1 (A t ), . . . , θ G (A t )|V (A t ) = i),<label>(4</label></formula><p>) where n g (A) is the number of observations of the gth group included in A. To avoid cumbersome notation, we suppress in our notation the dependency of M i (A t | J t ) on the observations x. For example, if the {θ g (A t )} G g=1 follow independent beta priors written as Beta(α i l (A t ), α i r (A t )) given V (A t ) = i, then the marginal likelihood has the</p><formula xml:id="formula_18">following expression M i (A t | J t ) = G g=1 B(α i l (A)+ng(A t,l ),α i r (A)+ng(At,r)) B(α i l (At),α i r (At))</formula><p>, where B(•, •) is the beta function. Based on the values of M i (A t | J t ), we can analytically compute the proposal and the importance weight using a general recursive algorithm, as described in the following proposition.</p><p>Proposition 3.1. For every possible decision J t and states i = 1, . . . , I, let ϕ i (A t ) be a function defined recursively:</p><formula xml:id="formula_19">ϕ i (A t ) =        ξ 1,i (Ω)M i (Ω|Jt) I j=1 ξ 1,j (Ω)M j (Ω|Jt) if A t = Ω I j=1 ϕ j (A p t )ξ j,i (At)M i (At|Jt) I k=1 I j=1 ϕ j (A p t )ξ j,k (At)M i (At|Jt) Otherwise,<label>(5)</label></formula><p>where</p><formula xml:id="formula_20">A p t is A t 's parent node. Also, let h(J t | A t ) be a function of J t defined as h(J t | A t ) = I i=1 I j=1 ϕ j (A p t )ξ j,i (A t ) M i (A t | J t ) µ(A t,l ) -n(A t,l ) µ(A t,r ) -n(At,r) µ(A t ) -n(At) ,<label>(6)</label></formula><p>where n(A) denotes the total number of observations included in a node A. Then the target distribution π t (T t ) can be expressed in terms of π t-1 (T t-1 ) as</p><formula xml:id="formula_21">π t (T t ) = C t π t-1 (T t-1 )π t (J t | T t-1 )w t (T t-1 ),</formula><p>where C t is a constant and</p><formula xml:id="formula_22">π t (J t | T t-1 ) = P (J t )h(J t | A t ) jt P (j t )h(j t | A t ) , w t (T t-1 ) = jt P (j t )h(j t | A t )</formula><p>The summation over j t is taken over all possible decisions.</p><p>Corollary 3.1. Let h(J t | A t ) be the function defined in Proposition 3.1. Then the</p><formula xml:id="formula_23">proposal distribution π t (D t | T t-1 ) is given by π t (D t | T t-1 ) = π t (D t | T t-1 )π t (L t | D t , T t-1 ), where 1. π t (D t | T t-1 ) is Mult( λ1 (A t ), . . . , λd (A t )) with λj (A t ) ∝ N L -1 l=1 π t ((j, l/N L ) | T t-1 ) ∝ λ j (A t ) N L -1 l=1 β l (A t )h((j, l/N L ) | A t ). 2. Given D(A t ) = j, the conditional posterior of L(A t ) is π t (L t = l/N L | D t = j, T t-1 ) = N L -1 l=1 βl (A t )δ l/N L (•), for j = 1, 2, . . . , I and l = 1, . . . , N L -1 with βl (A t ) ∝ β(A t )h(j, l/N L | T t-1 ).</formula><p>We also have an analytical expression of the incremental weight:</p><formula xml:id="formula_24">w t (T t-1 ) = d j=1 N L -1 l=1 λ j (A t )β l (A t )h((j, l/N L ) | A t ).</formula><p>Remark: The recursive function ϕ i (A t ) can be computed based on ϕ i (A p t ) with the fixed computational cost. Hence, the optimal proposal π t (J t | T t-1 ) and the incremental weight w t (T t-1 ), which are functions of h(J t | A t ), can be obtained at each step with constant computational cost with complexity O(I 2 N L d n(A t )). As such, our inference algorithm scales linearly in both the dimensionality and the sample size.</p><p>We summarize the algorithm in updating the particle system from</p><formula xml:id="formula_25">{T m t-1 , W m t-1 } M m=1 to {T m t , W m t } M m=1 below.</formula><p>All operations are repeated for m = 1, . . . , M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Choosing the current node</head><p>From T m t-1 , choose the the node generated in the earliest step among the current leaf nodes that have not been terminated, which is denoted by A t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Obtaining the information of the parent node</head><p>Locate A t 's parent node, A p t , and fetch the values of ϕ i (A p t ) for i = 1, . . . , I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Computing the necessary quantities</head><p>For all possible J t = (D t , L t ), compute M i (A | J t ) (i = 1, . . . , I) and h(J t | A t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dividing the current node</head><p>Compute the parameters λj (A t ) for j = 1, . . . , d and sample</p><formula xml:id="formula_26">D m t ∼ Mult( λ1 (A t ), . . . , λd (A t )).</formula><p>Given D m t , compute the parameters βl (A t ) for l = 1, . . . , N L -1 and sample</p><formula xml:id="formula_27">L m t ∼ N L -1 l=1 βl (A t )δ l/N L (•).</formula><p>Divide the current node A t with J m t = (D m t , L m t ) to update the tree T m t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Updating the importance weight</head><p>Compute the incremental weight w t (T m t-1 ) and update the importance weights</p><formula xml:id="formula_28">W m t = W m t-1 w t (T m t-1 ) M m =1 W m t-1 w t (T m t-1 )</formula><p>.</p><p>If the effective sample size 1/ M m=1 (W m t ) 2 is less than some prespecified threshold (e.g., M/10), resample the particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Computing the information on the current node for its descendants</head><p>Given J m t , compute ϕ i (A t ) for i = 1, . . . , I.</p><p>In the algorithm, we stop dividing A t if either the depth of A t is equal to a pre-set maximum resolution K (e.g., 15) or the number of observations in A t is less than a pre-set threshold (e.g., 5). The SMC algorithm terminates when all the nodes of all the particles have been stopped. The maximum resolution K controls the level of local details that the model allows to infer, and larger values of K require more computational time. In a wide range of applications we have found that setting K to beyond 15 to 20 leads to minimal changes in the resulting inference.</p><p>A common technique in SMC is to resample the particles according to the importance weights {W m t } M m=1 when the effective sample size of the particles drops below a level. In sampling trees, however, the importance weights are affected by the choice of nodes to divide in multiple steps, and so the standard resampling scheme can be too "short-sighted" and often results in sacrificing promising trees prematurely. To address this issue we follow the strategy proposed in <ref type="bibr" target="#b23">Lu et al. (2013)</ref> by resampling the particles according to weights a m t ∝ (W m t ) κ for some κ ∈ (0, 1], and compute the new importance weights proportional to W m t /a m t . We generally recommend using a moderate choice of κ such as 0.5, which we have found satisfactory in a variety of numerical experiments, and will be our default choice in all of our later examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Posterior computation given sampled tree structures</head><p>The second stage of our inference strategy is to compute the posterior distributions of the latent states V (A) and the conditional probabilities θ g (A) given each sampled tree. We shall focus on deriving a generic recipe for computing the posterior of the latent states given the tree that works for all models under consideration. Given both the tree and the latent states, the posterior of θ g (A) boils down to the corresponding posterior of standard PT models on a dyadic tree, which is problem-specific as provided in the literature on each such model. Now suppose the SMC algorithm has produced a collection of finite trees {T m } M m=1 along with the importance weights {W m } M m=1 . Given each tree T m , it is possible to analytically calculate the exact posterior of {V (A)} A∈T m with recursive message passing (a form of dynamic programming), which we describe below.</p><p>For A ∈ N (T m ), let φ A (i) be the marginal likelihood on A given that V (A) = i.</p><p>(As before we suppress the dependency of the marginal likelihood on the data to simplify notation.) Specifically,</p><formula xml:id="formula_29">φ A (i) = q(x | A)P (dq | V (A) = i), where q(x | A) = G g=1 z∈xg(A) q g (z | A).<label>(7)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_29">7</ref>), taking the integration with respect to P (dq | V (A) = i) is equivalent to integrating out θ g (A) as well as the θ g (A ) and V (A ) terms for all descendants A of A. Another useful quantity is the marginal likelihood on a node A given the state of its parent node V (A p ) = i, which we denote as Φ A (i) and is given by</p><formula xml:id="formula_30">Φ A (i) =        x∈x(A) µ(x | A) if A is a leaf node, I i =1 ξ i,i (A)φ A (i ) if A is a non-leaf node.<label>(8)</label></formula><p>Note that the Φ A (i) and φ A (i) terms are related by</p><formula xml:id="formula_31">φ A (i) = M i (A | J(A))Φ A l (i)Φ Ar (i),<label>(9)</label></formula><p>where M i is the marginal likelihood defined in (4) given under the decision J(A) = (D(A), L(A)) to divide A into A l and A r . By iteratively computing Eqs. ( <ref type="formula" target="#formula_30">8</ref>) and ( <ref type="formula" target="#formula_31">9</ref>) in a bottom-up fashion (i.e., starting from the leaves all the way to the root), we can compute the pair {(φ A (i), Φ A (i)) : A, i} for all nodes in the tree, and these pairs are the "messages" passed along the tree from leaf to root.</p><p>Given the values of {(φ A (i), Φ A (i)) : A, i}, we can now obtain the posterior Markov transition probability matrices of the latent states given the tree T , ξ(A) = ( ξi,i (A)) I×I where the (i, i )th element ξi,i (A</p><formula xml:id="formula_32">) = P (V (Ω) = i | V (A p ) = i, x, T ) and</formula><p>the posterior marginal probabilities of the latent states given the tree T ,</p><formula xml:id="formula_33">γ(A) = (γ i (A)) I i=1 = (P (V (Ω) = i | x, T )) I i=1 .</formula><p>Specifically, by the Bayes' theorem, ξ(A) = D -1 1 (A)ξ(A)D 2 (A), where D 1 (A) and</p><formula xml:id="formula_34">D 2 (A) are diagonal matrices with D 1 (A) i,i = Φ A (i) and D 2 (A) i,i = φ A (i).</formula><p>After computing these transition matrices, we can compute γ(A) (the feedback "message") in the top-down manner (i.e., starting from the root and down to the leaves) as</p><formula xml:id="formula_35">γ(Ω) = ξ1,• (Ω) and γ(A) = γ(A p ) ξ(A) for A = Ω.</formula><p>We note that Φ Ω (1) computed in the recursive algorithm is the overall marginal likelihood given the tree T , P (x | T ), which can be used to find the maximum a posteriori (MAP) tree among the sampled trees, i.e., the sampled tree T m that maximizes P (T m | x) ∝ P (T m )P (x | T m ). We can use this "representative" tree, along with the conditional posterior of the latent states given this tree, to visualize and summarize the posterior inference in an interpretable way.</p><p>Now that we have completely described our inference algorithm, we provide two specific examples to demonstrate how one may use the output of the algorithmnamely the sampled trees along with the conditional posterior given the trees-to carry out inference. The first example is density estimation which involves learning the within-sample distributional features while the second is two-group comparison whose focus is on learning the between-sample structures. The inference strategies for these quintessential examples are generalizable to a variety of other tasks.</p><p>Example 1: Density estimation</p><p>We consider the problem of estimating an unknown density based on a set of i.i.d.</p><p>observations from that density. This corresponds to G = 1 and so we can drop the subscript g to simplify the notation. We shall use the posterior mean density, also called the predictive density-E[q(•) | x]-as an estimate for the density q = dQ/dµ.</p><p>It can be computed by integrating out the random trees and the latent variables based on the SMC sample. Specifically, for a given tree T , we define for all A ∈ T and i ∈ {1, 2, . . . , I}, the following quantity</p><formula xml:id="formula_36">e A (i) := E[Q(A)I {V (A)=i} | x].</formula><p>All of these e A (i) terms can be computed together by a single top-down (i.e., rootto-leaf) recursion on the tree as given in the following proposition.</p><p>Proposition 3.2. For the root node, e Ω (i) = γ1,i (Ω). For a non-root node A, e A (i) can be computed recursively as</p><formula xml:id="formula_37">e A (i ) = I i=1 ξi,i (A)E[ϑ(A p ) | V (A p ) = i, T m , x] e A p (i), where ϑ(A p ) =        θ(A p ) if A is the left child of A p , 1 -θ(A p ) if A is the right child of A p .</formula><p>conditional predictive measure given the tree T ,</p><formula xml:id="formula_38">E[Q(B) | x, T ] = A∈L(T ) µ(B ∩ A) µ(A) I i=1</formula><p>e A (i) for any Borel set B.</p><p>Now, given an SMC sample of M trees and weights, the posterior predictive density can be computed by</p><formula xml:id="formula_39">E[q(x) | x] ≈ M m=1 W m E[Q(B m (x)) | x, T m ] µ(B m (x)) ,</formula><p>where B m (x) ∈ L(T m ) the leaf node to which x belongs.</p><p>Example 2: Two-group comparison</p><p>If we are interested in comparing two groups of observations using generalizations to the PT models described in Section 2.3.1, we shall compute the posterior probability of the two hypotheses H 0 and H 1 . For example, when V (A) is defined as in Eq (3), the posterior probability of the "global" null hypothesis</p><formula xml:id="formula_40">H 0 : Q 1 = Q 2 is given by P (H 0 | x) = T ∈T P (V (A) = 1 for all A ∈ N (T ) | T, x)P (T | x) ≈ M m=1 W m P (V (A) = 1 for all A ∈ N (T m ) | T m , x),</formula><p>where the sum over T in the first row is over all finite trees with maximum resolution K and the quantity P (V (A) = 1 for all A ∈ N (T m ) | T m , x) again is available analytically by message passing (details given in Supplementary Materials B).</p><p>In addition to testing the existence of any difference between groups, it is usually of interest to detect where and how the underlying distributions differ. To this end, we can compute the "posterior marginal alternative probability" (PMAP) on each node A, along any sampled tree T m :</p><formula xml:id="formula_41">P (θ 1 (A) = θ 2 (A) | T m , x) = P (V (A) = 1 | T m , x) = γi (A).</formula><p>Reporting the PMAPs along a representative tree such as the MAP among the sampled trees can be a particularly useful visualizing tool to help understand the nature of the underlying difference. One can also report on each A the estimated magnitude of the difference using a notion of "effect size" based on the log-odds ratio <ref type="bibr" target="#b34">(Soriano and Ma, 2017)</ref></p><formula xml:id="formula_42">, eff(A) = log θ 1 (A) 1-θ 1 (A) -log θ 2 (A) 1-θ 2 (A)</formula><p>. In particular, one can report the posterior expected effect size E[eff(A) | x, T ], which can be computed using a standard Monte Carlo (not MCMC) sample from the exact posterior given the representative tree. We will demonstrate this using a mass cytometry data set in Section 6</p><p>4 Theoretical Properties</p><p>Next we investigate the theoretical properties of the proposed model. Previous theoretical analysis on the PT had mostly focused on establishing the marginal posterior consistency and contraction of the random measures Q g with respect to an unknown fixed truth <ref type="bibr" target="#b35">(Walker and Hjort, 2001;</ref><ref type="bibr">Castillo, 2017b)</ref>. We shall take a different perspective and instead investigate the asymptotic behavior of the marginal posterior of the partition tree T and the latent states as these are often critical quantities of practical importance in applications. We note that once given the tree and the latent states, our model reduces to standard PTs on a dyadic tree and thus the posterior consistency of the random measures Q g 's will follow from previous results once we establish the posterior consistency of the tree and the latent states. As the sample size increases, the two key theoretical questions of interest here are:</p><p>(1) What tree structures does the marginal posterior of T concentrate around?</p><p>(2) How does the posterior of the latent states given the tree behave?</p><p>These two questions have broad relevance in inference using PT models, and previously several authors have investigated the second question in the two-group comparison context for their variants of the PT model <ref type="bibr" target="#b17">(Holmes et al., 2015;</ref><ref type="bibr" target="#b34">Soriano and Ma, 2017)</ref>. In addressing the second question more generally, we aim to provide results that encompass these previous analyses as special cases. According to our limited knowledge, we are not aware of previous studies on the first question.</p><p>We will address each of the two questions in turn. Throughout this section, we consider finite PTs with maximum depth of the trees set to some value K. We use T K to denote this collection of trees. Also, the asymptotic results are derived under the prior for L(A) provided in Section 2.2 which can depend on the (finite) sample size. The case of an uniform priors on L(A) independent of the sample size is included as a special case where the hyperparameter η = 0. Finally, we consider models that satisfy Assumption 1 and Assumption 2 described below. The models discussed in Section 2.3.1 all meet this requirement.</p><p>Assumption 1. For each group g ∈ {1, ..., G}, let n g be the sample size and P g the true probability measure from which the observations are generated. We assume that (i) There exists ζ g ∈ (0, 1] such that ζ g = lim n→∞ ng n for g ∈ {1, . . . , G}, where n = n 1 + • • • + n G is the total number of observations across all groups.</p><p>(ii) The sampling distribution P g satisfies P g µ, and the density p g = dP g /dµ is positive almost everywhere.</p><p>Additionally, given the tree T and the latent states, the parameters {θ g (A)} G g=1 are given one of the following priors (the model can adopt a mix of these priors for different combinations of A and V (A) values):</p><p>Prior A : θ g (A) independently follow a beta prior.</p><formula xml:id="formula_43">Prior B : θ 1 (A) = • • • = θ G (A)</formula><p>and follow a beta prior.</p><formula xml:id="formula_44">Prior C : θ 1 (A) = • • • = θ G (A) ≡ c(A), some constant in (0, 1).</formula><p>Establishing the theoretical properties also requires a condition on the latent states. In particular, under some states, the support of the prior of the parameters {θ g (A)} G g=1 needs to include the true conditional probabilities. To describe this requirement, given a tree T ∈ T K , let S i (A | T ) be the support of the prior on (θ 1 (A), . . . , θ G (A)) under the state V (A) = i. Then, let τ (A | T ) denote the collection of "feasible states" on A. (A state is "feasible" if the true conditional probabilities are in the support of the corresponding prior given the state.) That is,</p><formula xml:id="formula_45">τ (A | T ) := {i ∈ {1, . . . , I} : (P 1 (A l | A), . . . , P G (A l | A)) ∈ S i (A | T )}.</formula><p>The next assumption states that the prior for the latent states must give positive probability for all the latent states to all simultaneously be feasible.</p><p>Assumption 2. For every</p><formula xml:id="formula_46">T ∈ T K , P (V (A) ∈ τ (A | T ) for all A ∈ T ) &gt; 0.</formula><p>With these assumptions, we next derive asymptotic properties for the marginal posteriors for the tree and the state variables. In the following, we use the notation x n instead of x for the data to indicate the total sample size.</p><p>In order to describe the posterior convergence of the partition trees, we introduce a notion for "tree-based approximation for probability measures". Let T be a finite tree and H a probability measure. Then the "tree-based approximation of H under T ", denoted by H| T , is defined as H| T (B) = A∈L(T ) H(A) µ(B∩A) µ(A) . for any B ∈ B(Ω). The following theorem then characterizes the trees the posterior concentrates on as the sample size grows.</p><p>Theorem 4.1. Let T K M be the collection of trees under which the tree-based approximation of the measures P g minimizes the Kullback-Leibler divergence from the P g 's plus a penalty term on unbalanced splits. That is,</p><formula xml:id="formula_47">T K M = arg min T ∈T K G g=1 ζ g {KL(P g ||P g | T ) + ηB g (T )} ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_48">B g (T ) = A∈N (T ) P g (A)f µ(A l ) µ(A) -0.5 .</formula><p>Then the marginal posterior of T concentrates on T K M . That is, as n → ∞,</p><formula xml:id="formula_49">P (T ∈ T K M | x n ) p - → 1.</formula><p>For the state variables, it is desirable that their posterior distribution concentrates on a collection of feasible states. Moreover, when multiple configurations of the states are feasible, it is desirable that the posterior concentrates around such configurations that provide the most parsimonious representation of the true distributions. For example, if the true conditional distribution on a node is uniform, a model that introduces a possible non-uniform structure on this node is feasible but redundant.</p><p>White and Ghosal (2011) and <ref type="bibr" target="#b22">Li and Ghosal (2014)</ref> showed that, in quite general settings of multi-resolution inference, the posterior probability of such redundant models tends to concentrate its mass on 0. By adapting their techniques, we show that the same property holds in the case of our model.</p><p>To formally describe the results, we need to define the complexity of the model specified by the latent states. Given the state V (A) = i, the complexity of the {θ g (A)} G g=1 , in other words, the number of free parameters of the prior distribution under the ith state is denoted by C i (A). For example, for two-group comparison,</p><formula xml:id="formula_50">C i (A) =        1 if θ 1 (A) = θ 2 (A) 2 if θ 1 (A) = θ 2 (A).</formula><p>Next we introduce the complexity of a combination of states on the tree T . Given a tree T , let V denote a combination of the state variables {V (A)} A∈N (T ) and let v = {v(A)} A∈N (T ) (v(A) ∈ {1, . . . , I}) be one of the possible realizations of V. Then we define the model complexity under v as follows:</p><formula xml:id="formula_51">C(v) = A∈N (T ) C v(A) (A). (<label>11</label></formula><formula xml:id="formula_52">)</formula><p>The next theorem shows that the posterior distribution of the states given the tree will concentrate on those that are feasible and most parsimonious.</p><formula xml:id="formula_53">Theorem 4.2. For T ∈ T K , let V T = {v : v(A) ∈ τ (A | T ) for all A ∈ N (T )}. Then P ({V ∈ V T } ∩ {C(V) = min v∈V T C(v)} | T, x n ) p - → 1.</formula><p>Remark: Consistency results for several existing models are special cases of this theorem. For example, we derive the consistency of the MRS model for two-group comparison as a corollary in Supplementary Materials C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we carry out simulation studies to examine the performance of our model and inference algorithm. In particular, we are interested in (i) understanding how the model with the flexible tree prior compares to those with a "divide-in-themiddle" restriction, and (ii) verifying the linear scalability of our inference algorithm with respect to increasing dimensionality. We again consider the two quintessential examples-(i) density estimation and (ii) the two-group comparison-for inferring within-group and between-group structures respectively.</p><p>We shall consider both low-dimensional settings where the underlying structure is easy to interpret and software for existing PT models are available, and highdimensional settings for which existing implementation of PT models is not applicable and we use our SMC algorithm to carry out inference for both our model and the earlier models with fixed partitioning points (which are special cases of our model).</p><p>Throughout the experiments, the parameters N L and M are fixed to 32 and 1000 respectively. We note that larger N L values can also be adopted at a linear computational cost but did not lead to noticeable change in performance in our examples.</p><p>Details such as the settings of hyper-parameters and simulated data sets are provided in Supplementary Materials F unless explicitly described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Density estimation</head><p>We first consider 2D examples to observe what kind of tree structures are obtained under the flexible model and how prior specification in Eq. ( <ref type="formula">2</ref>) influences the performance. After that, we move to higher dimensional cases to examine the scalability of our new SMC method and the effect of incorporating the flexible partition. For this task we compare our model with the APT model <ref type="bibr" target="#b24">(Ma, 2017)</ref> which also incorporates a prior on the dimension to divide but restricts partitioning at middle points. Its posterior computation is implemented by the apt function in the R package PTT. Supplementary Materials G, which is consistent with the explanation above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Higher-dimensional cases</head><p>We generate d-dimensional i.i.d. observations from a density with independent pairs of margins, i.e., f (x 1 , x 2 , . . . , x d ) = d/2 j=1 f j (x 2j-1 , x 2j ) where </p><formula xml:id="formula_54">f j (x 2j-1 , x 2j ) =p j Beta(x 2j-1 | 0.25, 1) × Beta(x 2j | 0.25, 1)</formula><p>+(1 -p j )Beta(x 2j-1 | 50/j, 50/j) × Beta(x 2j | 50/j, 50/j), with p j = 0.25 + 0.7/j. We consider two different situations: (i) the dimension d = 6, and the sample size n changes from 5,000 to 50,000; and (ii) the sample size n = 10, 000 and the dimensionality changes from 10 to 100. For our method, the maximum depth K is set to 15. For the algorithm of the original APT (implemented by the apt function in the PTT package), in the first case with d = 6, the maximum resolution is fixed to 9 because setting higher values leads to insufficient memory.</p><p>Also, because the apt function does not scale if the dimension is beyond d ≈ 10, in the second case with large d, we used the proposed SMC algorithm to carry out inference for the original APT model, which corresponds to setting N L = 2, namely, fixing all the partition points to the middle. In addition to the original APT, we show the comparison with the classical PT method in which each node is split into 2 d and the maximum depth is 15, the Dirichlet process Gaussian mixture model <ref type="bibr" target="#b10">(Escobar and West, 1995;</ref><ref type="bibr" target="#b28">Müller et al., 1996)</ref> implemented by the PYdensity function in the R package BNPmix <ref type="bibr" target="#b7">(Corradin et al., 2021)</ref>, and for (i) the Gaussian kernel density estimation using the kde function in the R package ks <ref type="bibr" target="#b9">(Duong et al., 2007)</ref>. Figure <ref type="figure" target="#fig_6">4</ref> presents the computational time for five different data sets. To obtain the result, we used a singe-core environment using Intel Xeon Gold 6154 (3.00 GHz) CPU.</p><p>The computational time is linear in both the sample size and the dimensionality. </p><formula xml:id="formula_55">n i=1 log p(x * i | x),<label>(12)</label></formula><p>where p is the posterior mean of the density E[q | x]. We repeat this computation for 50 test/training set pairs and take the average. The results, given in Figure <ref type="figure" target="#fig_7">5</ref>, show that our model substantially outperforms the competitors by this criteria both when d = 6 with varying sample size and when n is fixed with varying dimensionality. We also investigate the performance under small sample sizes, and the results are similar.</p><p>(See Supplementary Materials G for details.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Two-group comparison</head><p>Next we consider the two-group comparison problem, evaluate the performance of our model, and compare it to the original MRS with the "divide-in-the-middle" restriction. We use three scenarios ("Local location shift", "Local dispersion difference", and "Correlation") to generate 50-dimensional data sets. (Details of the scenarios are provided in Supplementary Materials F.2.2.) The first two scenarios involve two-group difference that lies in only parts of the sample space, or "local" differences.</p><p>which will help demonstrate the usefulness of inferring the partition tree in identifying the nature of the differences. The sample size is n 1 = n 2 = 2, 000 in all scenarios.</p><p>The original algorithm for inference under the MRS model by message passing, which is implemented by the mrs function in the R package MRS, is not scalable beyond about 10 dimensions even with fixed partition locations. Hence we compute the posterior for both our model and the original MRS in all scenarios with our SMC and message passing hybrid algorithm. For the two models, the maximum resolution K is fixed to 15. We compare the performance using receiver operating characteristic (ROC) curves computed based on 200 simulated data sets under each scenario. Figure <ref type="figure" target="#fig_8">6</ref> presents the ROC curves. For the location shift and dispersion differences, the model with flexible partitioning results in substantially higher sensitivity. For the correlation scenario, the model with fixed partitioning locations performed slightly better. This is not surprising since in this scenario the difference exists smoothly over entire ranges of the dimensions without natural "optimal" division points, and so the performance gap is the cost for searching over more possible partition locations, none of which improves the model fit than the middle point. It is worth noting again that while the model with fixed partitioning performs well here, it is only with our new computational algorithm that it can be fit to data of such dimensionality.</p><p>To demonstrate the posterior model can help understand the nature of the differences, we present under each scenario the node with the highest PMAP, or</p><formula xml:id="formula_56">P (V (A) = 1 | x) = P (θ 1 (A) = θ 2 (A) | x), in Figure 7.</formula><p>In the location shift and dispersion difference scenarios the boundaries are away from the middle point to characterize the difference, which partly explains the sensitivity gain in adopting the flexible tree prior.</p><p>6 Application to a mass cytometry data set</p><p>Finally, we apply our model for two-group comparison to a mass cytometry data set collected by <ref type="bibr" target="#b19">Kleinsteuber et al. (2016)</ref>. The data set records 19 different mea-surements including physical measurements and biomarkers on single cells in blood samples from a group of HIV patients as well as in reference samples from healthy donors. For demonstration, we compare the sample from an individual patient sample (Patient #1) and to that from a healthy donor to identify differences in immune cell profiles from these samples. The sample sizes are 29, 226 for the health donor and and 228, 498 for the patient, with each observation corresponding to a cell. We set η = 0.1 and the maximum depth K to 25.</p><p>Given the large sample sizes, the posterior probability for the global alternative</p><formula xml:id="formula_57">P (Q 1 = Q 2 | x)</formula><p>is almost 1 and so is of less interest. Our focus is instead on identifying the cell subsets on which the samples differ and on quantifying such differences. To this end, we identify a representative tree and report the "effect size" (i.e., the posterior expected log-odds ratio between the two samples) on each node in a representative tree-the MAP among the sampled trees.</p><p>The estimated eff(A)'s on the MAP tree up to level 9 is visualized in Figure <ref type="figure" target="#fig_10">8</ref>, and the full tree is provided in Supplementary Materials G. We note that the nodes on which there is significant evidence for two-group differences, as well as those with large estimated effect sizes tend to be nested or clustered in subbranches of the tree, which is consistent with our intuition that there is spatial correlation in the two-group differences, and justifies the hidden Markov structure embedded in the MRS model.</p><p>Figure <ref type="figure" target="#fig_21">14</ref> in Supplementary Materials G presents the 20 nodes with the largest values of estimated eff(A). In this figure, many of the nodes are in very deep levels of the tree. We adopted a spike-and-slab with higher spike probability in very deep tree levels to further speed up the computation (details given in Supplementary Materials A) and that explains why many of the very deep, small nodes plotted have partition lines in the middle under the MAP tree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding Remarks</head><p>We have proposed a general framework for the PT model that incorporates a flexible prior on the partition tree and can accommodate latent state variables with Markov dependency along the partition tree. We have proposed a sampling algorithm that combines SMC and recursive message passing that can scale up to moderately highdimensional (∼ 100-dim) problems. Our numerical experiments confirm that our sampling algorithm scales linearly in the sample size and the flexible partitioning tree prior can result in substantial gain in performance in some settings. Though we have mainly used two inference tasks-namely density estimation and two-group comparison-to demonstrate our model and algorithm, our approach can be readily applied to other PT models with a hidden Markov structure.</p><p>One notable limitation of our model-and in fact all CART-like models-is that we only consider trees in which the node boundaries are all parallel to the axes. This could lead to inefficiency in inference. For example, when there is a strong correlation between several variables, drawing boundaries slanted according to the correlation structure would be more effective in characterizing the underlying distribution. Such trees will need to be represented by more than just the D(A) and L(A) used in our model, and how to efficiently compute the posterior distribution is an open problem.</p><p>Finally, our proposed algorithm is currently designed to be run in a single computer environment, so though the computational cost is linear to the sample size n, dealing with huge data with huge n (e.g., &gt; 10 12 ) is not yet realistic. It is of future interest to develop parallelized versions of the algorithm for distributed systems, which could explore either the parallel structure over nodes or parallel SMC algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software</head><p>An R package for our method is available at <ref type="url" target="https://github.com/MaStatLab/SMCMP">https://github.com/MaStatLab/SMCMP</ref>.</p><p>The likelihood has the same form as in the original case without the auxiliary variables R's. Thus we can obtain the following proposition as a generalization of Proposition 3.1.</p><p>Proposition A.1. Let h(J t | A t ) be a function of J t defined as</p><formula xml:id="formula_58">h(J t | A t ) = I i=1 I j=1 ϕ j (A p t )ξ j,i (A t ) M i (A t | J t ) µ(x(A t,l ) | A t,l )µ(x(A t,r ) | A t,r ) µ(x(A t ) | A t ) .</formula><p>Then the target distribution π t (T t , R 1:t ) is expressed with π t (T t-1 , R 1:t-1 ) as</p><formula xml:id="formula_59">π t (T t , R t ) = Cπ t (T t-1 , R 1:t )π t (R t | T t-1 , R p t )π t (J t | T t-1 , R t )w t (T t-1 , R p t ),</formula><p>where C is a constant and</p><formula xml:id="formula_60">π t (R t | T t-1 , R p t ) = P (R t | R p t ) jt P (j t | R t )h(j t | A t ) i=0,1 P (R t = i | R p t ) jt P (j t | R t = i)h(j t | A t ) , π t (J t | T t-1 , R t ) = P (J t | R t )h(J t | A t ) jt P (j t | R t )h(j t | A t ) , w t (T t-1 , R p t ) = i=0,1 P (R t = i | R p t ) jt P (j t | R t = i)h(j t | A t ) .</formula><p>The summation with j t is taken over all possible decisions.</p><p>Its proof is essentially the same as Proposition 3.1 and Corollary 3.1 so it is omitted in this material. The conditional posteriors π t (R t | R p t , T t-1 ) and π t (J t | R t , T t-1 ) are analytically obtained as follows. First, if After sampling (R t , J t ), the incremental weight w t (T t-1 , R p t ) (R p t = 0, 1) is computed as</p><formula xml:id="formula_61">R p t = 0, π t (R t | R p t , T t-1 ) is Bernoulli(r(A t )), where r(A t ) = r(A t ) J j=1 λ j (A t )h((j, 1/2) | A t ) × r(A t ) J j=1 λ j (A t )h((j, 1/2) | A t ) + (1 -r(A t )) J j=1 N L -1 l=1 λ j (A t )β l (A t )h((j, l/N L ) | A t ) -1 , If R p t = 1, then R t is fixed to 1. Second, if R t = 0,</formula><formula xml:id="formula_62">w t (T t-1 , 0) = r(A t ) J j=1 λ j (A t )h((j, 1/2) | A t ) + (1 -r(A t )) J j=1 N L -1 l=1 λ j (A t )β l (A t )h((j, l/N L ) | A t ), w t (T t-1 , 1) = J j=1 λ j (A t )h((j, 1/2) | A t ),</formula><p>with which we update the importance weight</p><formula xml:id="formula_63">W t as W t ∝ W t-1 w t (T t-1 , R p t ).</formula><p>The procedure to update the particle system</p><formula xml:id="formula_64">{T m t-1 , W m t-1 } M m=1 to obtain {T m t , W m t } M m=1</formula><p>is described in the following algorithm. The operations involving the index m is repeated for m = 1, . . . , M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Choosing the current node</head><p>From T m t-1 , choose the oldest note from the leaf nodes, which is denoted by A t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Obtaining the information of the parent node</head><p>Pick up A t 's parent node, which is denoted by A p t , and load the values of ϕ i (A p t )</p><p>for i = 1, . . . , I and R m,p t = R(A p t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Computing the necessary quantities</head><p>If R m,p t = 0, compute M i (A | j, l/N L ) (i = 1, . . . , I) and h(j, l/N L | A t ) for j = 1, . . . , d and l = 1, . . . , N L -1.</p><p>If R m,p t = 1, compute M i (A | j, 1/2) (i = 1, . . . , I) and h(j, 1/2 | A t ) for j = 1, . . . , d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deciding whether to fix the partition or not</head><p>If R m,p t = 0, compute the parameter r(A t ) and draw R m t ∼ Bernoulli(r(A t )).</p><p>If R m,p t = 1, set R m t to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dividing the current node</head><p>Sample J m t = (D m t , L m t ) as follows:</p><p>• If R m t = 0, compute the parameters λj (A t ) for j = 1, . . . , d and sample</p><formula xml:id="formula_65">D m t ∼ Mult( λ1 (A t ), . . . , λd (A t )).</formula><p>Given D m t , compute the parameters βl (A t ) for l = 1, . . . , N L -1 and sample</p><formula xml:id="formula_66">L m t ∼ N L -1 l=1 βl (A t )δ l/N L (•).</formula><p>• If R m t = 1, compute the parameters λj (A t ) for j = 1, . . . , d and sample</p><formula xml:id="formula_67">D m t ∼ Mult( λ1 (A t ), . . . , λd (A t )),</formula><p>and set L m t = 1/2.</p><p>Divide the current node A t with J m t = (D m t , L m t ) to update the tree T m t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Storing the information of the state's posterior</head><p>Given J m t , compute ϕ i (A t ) for i = 1, . . . , I and store them to the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Updating the importance weight</head><p>Compute the incremental weight w t (T m t-1 , R m,p t ) and update the importance weights as</p><formula xml:id="formula_68">W m t = W m t-1 w t (T m t-1 , R m,p t ) M m =1 W m t-1 w t (T m t-1 , R m ,p t</formula><p>) .</p><p>If the effective sample size 1/ M m=1 (W m t ) 2 is less than some prespecified threshold (M/10, say), resample the particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional algorithm for the MRS model</head><p>To describe the algorithm proposed in <ref type="bibr" target="#b34">Soriano and Ma (2017)</ref>, we keep using the same notations in Section 3.2. Given the tree structure T , we compute functions ψ(A) for A ∈ N (T ) in the bottom-up (from the leaf nodes to the root node) manner as follows:</p><formula xml:id="formula_69">ψ(A) =                ξ2,2 (A) + ξ2,3 (A) if A ∈ L(T ), ξ2,2 (A) ψ(A l ) ψ(A r ) + ξ2,3 (A) if A ∈ N (T ) \ {Ω}, ξ1,2 (A) ψ(A l ) ψ(A r ) + ξ1,3 (A) if A = Ω.</formula><p>Recall that only the first row of ξ(Ω) is meaningful as the initial distribution. Then we obtain ψ(Ω) = P (H 0 | T, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs C.1 Posterior computation</head><p>Lemma C.1. For the finite tree T t , let A s be a node whose children nodes are leaf nodes. Then we have</p><formula xml:id="formula_70">π t (V s = i | T t ) = π t (T t , V s = i) π t (T t ) = ϕ i (A s ),</formula><p>where ϕ i (A s ) is defined in (5).</p><p>(Proof) Suppose that A s belongs to the kth layer of T t . Then there is a sub-sequence {ρ(l)} k l=1 such that A ρ(l) belongs to the lth layer and</p><formula xml:id="formula_71">Ω = A ρ(1) ⊃ A ρ(2) ⊃ • • • ⊃ A ρ(k) = A s .</formula><p>By the definition of π t (T t , V 1:t ), for a sequence {v l } k l=1 such that v l ∈ {1, . . . , I}, we obtain the expression of the conditional posterior of {V ρ(l) } k l=1 as</p><formula xml:id="formula_72">π t ({V ρ(l) } k l=1 = {v l } k l=1 | T t ) ∝ P ({V ρ(l) } k l=1 = {v l } k l=1 ) k l=1 M v l (A ρ(l) | J ρ(l) ) = k l=1 ξ v l-1 ,v l (A ρ(l) )M v l (A ρ(l) | J ρ(l) ),</formula><p>where v 0 = 1. We show that for every k = 1, . . . , K</p><formula xml:id="formula_73">π t (V ρ(k) = v k | T t ) ∝ I v 1 =1 • • • I v k-1 =1 k l=1 ξ v l-1 ,v l (A ρ(l) )M v l (A ρ(l) | J ρ(l) ) ∝ ϕ v k (A s )<label>(13)</label></formula><p>holds by induction. First, if k = 1, which is equivalent to s = 1, ρ(1) = 1, and</p><formula xml:id="formula_74">A s = Ω, the posterior of V (Ω) is written as π t (V (Ω) = v 1 | T t ) ∝ ξ 1,v 1 (Ω)M v 1 (A 1 | J 1 ) ∝ ϕ v 1 (Ω).</formula><p>Second, assume that (13) holds for k = k. Then, if k = k + 1, we have</p><formula xml:id="formula_75">π t (Vk = vk, Vk +1 = vk +1 | T t ) ∝ I v 1 =1 • • • I vk -1 =1 π t ({V ρ(l) } k+1 l=1 = {v l } k+1 l=1 | T t ) ∝ I v 1 =1 • • • I vk -1 =1 k l=1 ξ v l-1 ,v l (A ρ(l) )M v l (A ρ(l) | J ρ(l) ) ξ vk,vk +1 (A s )M vk +1 (A s | J s ) ∝ ϕ vk (A ρ( k) )ξ vk,vk +1 (A s )M vk +1 (A s | J s ),</formula><p>from which we obtain</p><formula xml:id="formula_76">π t (Vk +1 = vk +1 | T t ) = I vk=1 π t (Vk = vk, Vk +1 = vk +1 | T t ) ∝ I vk=1 ϕ vk (A ρ( k) )ξ vk,vk +1 (A s )M vk +1 (A s | J s ) ∝ ϕ vk +1 (A s ).</formula><p>Proof of Proposition 3.1</p><p>Let the finite tree T t consist of a sequence of decisions J 1:t = {J s } t s=1 , which sequentially divides nodes A 1:t = {A s } t s=1 . To derive the proposition for the marginal posterior of T t , we first consider the joint posterior of T t and a sequence of the state variables V 1:t which are defined for the nodes A 1:t . From the structure of the model, the joint posterior is written as</p><formula xml:id="formula_77">π t (T t , V 1:t ) = P (J 1:t , V 1:t | x) = 1 Z t P (J 1:t )P (V 1:t ) t s=1,As∈N (Tt) M Vs (A s | J s ) t s=1,As∈L(Tt) µ(x(A s ) | A s ),<label>(14)</label></formula><p>where Z t is the normalizing constant, and A s,l and A s,r are the children nodes of A s .</p><p>For T t-1 and T t , since A t is divided into A t,l and A t,r , we have</p><formula xml:id="formula_78">N (T t ) = N (T t-1 ) ∪ {A t }, L(T t ) = L(T t-1 ) \ {A t } ∪ {A t,l , A t,r }. π t (T t , V 1:t ) = Z t Z t-1 π t-1 (T t-1 , V 1:t-1 )P (J t )P (V t | V 1:t-1 )M Vt (A t | J t ) µ(x(A s,l ) | A t,l )µ(x(A s,r ) | A s,r ) µ(x(A s ) | A s ) .<label>(15)</label></formula><p>Let A p t denote the parent node of A t and V p t = V (A p t ). Then, since the state variables follow the hidden Markov process, P (</p><formula xml:id="formula_79">V t | V 1:t-1 ) = ξ V p t ,Vt (A t ). Integrating out V 1:t-1 \ V p t in (15) gives π t (T t , V p t , V t ) = Z t Z t-1 π t-1 (T t-1 , V p t )P (J t )ξ V p t ,Vt (A t )M Vt (A t | J t ) µ(x(A s,l ) | A t,l )µ(x(A s,r ) | A s,r ) µ(x(A s ) | A s ) .</formula><p>Because A t is a leaf node of T t-1 , by Lemma C.1, we have</p><formula xml:id="formula_80">π t-1 (T t-1 , V p t = j) = π t-1 (T t-1 )π t-1 (V p t = j | T t-1 ) = π t-1 (T t-1 )ϕ j (A p t ).</formula><p>Hence, we obtain the expression of the marginal distribution of T t as</p><formula xml:id="formula_81">π t (T t ) = I i=1 I j=1 π t (T t , V p t = j, V t = i) = Z t Z t-1 π t-1 (T t-1 )P (J t ) I i=1 I j=1 ϕ j (A p t )ξ j,i (A t ) M i (A t | J t ) µ(x(A t,l ) | A t,l )µ(x(A t,r ) | A t,r ) µ(x(A t ) | A t ) ,</formula><p>which completes the proof.</p><p>Proof of Corollary 3.1</p><p>For π t (D t | T t-1 ), by Proposition 3.1, we obtain</p><formula xml:id="formula_82">π t (D t = j | T t-1 ) = N L -1 l=1 π t ((j, l/N L ) | T t-1 ) ∝ N L -1 l=1 P (D t = j, L t = l/N L )h((j, l/N L ) | A t ) ∝ λ j (A t ) N L -1 l=1 β l (A t )h((j, l/N L ) | A t ).</formula><p>On the other hand, the conditional probability of L t is obtained as follows:</p><formula xml:id="formula_83">π t (L t = l/N L | D t = j, T t-1 ) ∝ P (D t = j, L t = l/N L )h((j, l/N L ) | A t ) ∝ β(A t )h(j, l/N L | T t-1 ).</formula><p>The expression of w t (T t-1 ) immediately follows Proposition 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 3.2</head><p>In this discussion, we suppress x and T m in the expectation for simplicity. First, when A = Ω, by the definition e Ω (i) = γ1,i (Ω). Next, if A is not the root node, we can decompose e A (i ) as</p><formula xml:id="formula_84">e A (i ) = I i=1 E[Q(A)I[V (A) = i ]I[V (A p ) = i]].</formula><p>to reflect its dependency on the tree structure.</p><p>Let T ∈ T K and V denote a set of a combination of the states for all of the nonleaf nodes of T . Notice that an element of V does not need to satisfy P (V = v) &gt; 0, where V is the totality of the state variables. In the following proof, for v ∈ V, v(A)</p><p>denotes a state on a node A. Let l(v, T ) denote the log of the joint likelihood function</p><formula xml:id="formula_85">l(v, T ) = log P (x n | T, v) = A∈N (T ) l A (v(A), T ) + A∈L(T ) log µ(x n (A) | A),<label>(18)</label></formula><p>where l A (v(A), T ) = M A (v(A) | T ). By <ref type="bibr" target="#b33">Schwarz (1978)</ref>, this likelihood l A has the following expression</p><formula xml:id="formula_86">l A (v(A), T ) = lA (v(A), T ) - r v(A) 2 log n(A) + O p (1),</formula><p>where lA and r i are defined in ( <ref type="formula">16</ref>). Let v ∈ V be a collection of states such that, for all A ∈ N (T ), θ g (A) is fixed to µ(A l )/µ(A). For v, we have</p><formula xml:id="formula_87">l A (v, T ) = A∈N (T ) n(A l ) log µ(A l ) µ(A) + n(A r ) log µ(A r ) µ(A) + A∈L(T ) log µ(x n (A) | A) = log µ(x) = 0. Hence l(v, T ) is rewritten as l(v, T ) = l(v, T ) -l(v, T ) = A∈N (T ) lA (v(A), T ) -lA (v(A), T ) - C(v) 2 log n + O p (1).</formula><p>For the part inside of the braces, when v is replaced with v T ∈ V T , where V T = {v :</p><p>the KL divergence in ( <ref type="formula">20</ref>) is rewritten as</p><formula xml:id="formula_88">KL(P g | T ||µ) = A∈L(T ) P g (A) log P g (A) µ(A) = p g A∈L(T ) 1 A log P g (A) µ(A) dµ = p g log p g | T dµ = p g (x) log p g (x) µ(x) dµ(x) -p g log p g p g | T dµ = KL(P g ||µ) -KL(P g ||P g | T ).</formula><p>Because KL(P g ||µ) is independent of T , we obtain another expression of T K M in (10)</p><p>as</p><formula xml:id="formula_89">T K M = arg max T ∈T K G g=1 ζ g KL(P g | T ||µ)</formula><p>By Lemma C.4 and (18), for v ∈ V \ V T , we can show that p-lim</p><formula xml:id="formula_90">n→∞ l(v, T ) -l(v T , T ) n = p-lim n→∞ A∈N (T ) l A (v, T ) -l A (v T , T ) n &gt; 0,<label>(21)</label></formula><p>and p-lim n→∞ l(v, T )/n exists. Hence, for T</p><formula xml:id="formula_91">M ∈ T K M , v ∈ V T M , T ∈ T K \ T K M and v ∈ V, we have p-lim n→∞ l(v , T M ) -l(v, T ) n ≥ G g=1 ζ g KL(P g | T M ||µ) - G g=1 ζ g KL(P g | T ||µ) &gt; 0.</formula><p>Hence, for such T M and v , we obtain</p><formula xml:id="formula_92">P (x n | T ) P (x n | T M ) = P (T ) v∈V exp(l(v, T ))P (v) P (T M ) v∈V exp(l(v, T M ))P (v) ≤ v∈V exp(l(v, T ))P (T )P (v) exp(l(v , T M ))P (T M )P (v ) p - → 0. This result implies p(T ∈ T K M | x n ) p - →<label>1</label></formula><p>, which completes the proof of Theorem 4.1.</p><p>To prove Theorem 4.2, we fix T ∈ T K and define a set S T as</p><formula xml:id="formula_93">S T = v ∈ V T | v ∈ arg min v ∈V T C(v ) .</formula><p>Then we want to show</p><formula xml:id="formula_94">P (V ∈ S T | T, x n ) p - → 1. The result (21) implies p(V ∈ V T | T, x n ) p - → 1,</formula><p>so we only need to compare the elements of V T . Let v ∈ V T \ S T and v ∈ S T . For the difference of the log likelihoods, we have</p><formula xml:id="formula_95">l(v , T ) -l(v, T ) = A∈N (T ) lA (v (A), T ) -lA (v(A), T ) + C(v) -C(v ) 2 log n + O p (1),</formula><p>where lA and C is defined in ( <ref type="formula">17</ref>) and ( <ref type="formula" target="#formula_51">11</ref> weakly converges to the χ 2 distribution <ref type="bibr" target="#b39">(Wilks, 1938)</ref>. Hence, we obtain</p><formula xml:id="formula_96">l(v , T ) -l(v, T ) log n p - → C(v) -C(v ) 2 &gt; 0, which implies P (V ∈ S T | T, x n ) p - → 1.</formula><p>Proof of Theorem 4.1 (informative prior)</p><p>This section provides the proofs of Theorem 4.1 for the case of the informative prior (η &gt; 0). In the proof, we often use the results provided in the previous section on the non-informative prior and use the same notations.</p><p>We first discuss the asymptotic behavior of the prior of trees, which is in this case dependent on the sample size n. We only need to focus on the prior of the location variables L since this is only the component that depends on the sample size as given in Eq (2). By the fact that Since given T ∈ T K , the location variables are all independent and the prior of the dimension variables is independent of the sample size n, the log-tree prior l(T ) = log P (T ) has a limit as follows:</p><formula xml:id="formula_97">l(T ) n p - → -η G g=1 ζ g B g (T ),</formula><p>which is the penalty term introduced in Theorem 4.1.</p><p>We next define ψ(T ) as</p><formula xml:id="formula_98">ψ(T ) = G g=1</formula><p>ζ g {KL(P g | T ||µ) -ηB g (T )} .</p><p>By the the proof of Theorem 4.1 for the non-informative case, this is a limit under v ∈ V T (this V T , and V, a collection of possible combination of the state variables on the tree, are defined in the proof for the non-informative case), that is,</p><formula xml:id="formula_99">l(T ) + l(v, T ) n p - → ψ(T )</formula><p>as n → ∞. On the other hand, for v ∈ V \ V T , the limit exists and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Consistency for the MRS model</head><p>To describe the consistency, for a possible node A, we define a variable Z(A) as follows: where T is random.</p><formula xml:id="formula_100">Z(A) =        1 if V (A) = 1, 0 if V (A) ∈ {2, 3}.</formula><p>(Proof) In this case, V T in Theorem 4.2 is written as</p><formula xml:id="formula_101">V T = {v | v(A) = 1 if P 1 (A l | A) = P 2 (A l | A)} .</formula><p>We additionally define ṼT as</p><formula xml:id="formula_102">ṼT = {v | v(A) = 2 if P 1 (A l | A) = P 2 (A l | A)} .</formula><p>Then, under the condition that v ∈ V T , the complexity C(v) is minimized if and only if v ∈ ṼT . Hence, by Theorem 4.2 we obtain</p><formula xml:id="formula_103">P Z(A) = 1 {P 1 (A l |A) =P 2 (A l |A)} for all A ∈ N (T ) | T, x n = P V ∈ V T ∩ ṼT | T, x n p - → 1.</formula><p>We can show the second result by using Theorem 4.1 as follows:</p><formula xml:id="formula_104">P V ∈ V T ∩ ṼT | x n = T ∈T K P V ∈ V T ∩ ṼT | T, x n P (T | x n )</formula><p>≥ arg min</p><formula xml:id="formula_105">T M ∈T K M P V ∈ V T ∩ ṼT | T M , x n P (T ∈ T K M | x n ) p - → 1.</formula><p>analytically, we use the Monte Carlo approximation with 10000 values generated from the true distributions.</p><p>Figure <ref type="figure" target="#fig_15">9</ref> provides the average L 1 distance under the different settings. When the distribution is uniform (a = 0), we can see that the L 1 distance is very small, and this can be understood as an advantage of using the APT model to learn the shrinkage level adaptively. On the other hand, especially when a &lt; 0 and d is large, the distance is larger for a = 0 though the performance is slightly improved if the maximum resolution K is larger. One possible reason to explain this result is that the APT model, with which we learn the smoothness of unknown densities, is not designed to capture such extremely spiky distributions. Thus the performance would be improved by modifying the prior distributions but this discussion is out of this research's scope because our main contribution is proposing the algorithm that works for many types of PT-based models.</p><p>Figure <ref type="figure" target="#fig_16">10</ref> compares the number of nodes included in the MAP trees. From this result, we can see that the number is larger for the larger maximum resolution K = 15 as naturally expected but tends to be smaller when the parameter a is positive. The latter phenomenon is explained by the rule that the SMC algorithm stops splitting nodes when the number of included observations is below the threshold such as 5.</p><p>When a is positive, since the distribution concentrates around the central point, nodes with only a few observations tend to be generated in early levels and thus are no longer split, resulting in a smaller number of nodes on the tree. Hence this numerical result clarifies our proposed algorithm's tendency of "ignoring" nodes with only a few observations "zooming up" nodes including many observations.  where δ j = 0.75 for j = 1, . . . , 5 and δ j = 0 for j = 6, . . . , 25 .</p><p>In the "local location shift" and "local dispersion difference", the parameters are     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) and 1(b) present a graphical model representation for each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Partition tree generation (A 0 : All potential nodes) (b) PT without latent states (c) PT with latent states</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of PT models given the tree T . The hyperparameters are hidden for simplicity.</figDesc><graphic coords="10,258.02,270.99,85.54,55.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 1(c) presents a graphical model representation for the latent state modeling on G probability distributions by PTs given T , which along with our generalized prior on the partition tree T presented in Figure 1(a) forms the most general version of the model we consider in this work.Example 1: Density estimation with adaptive smoothness An example of within-group structures that the latent state V (A) can characterize is the smoothness of the density functions for the random measures. For example, Ma (2017) proposed the adaptive Pólya tree (APT) model which incorporates latent states to allow different levels of local smoothness in the underlying distribution. This is achieved by modeling the θ(A)'s as Beta(m(A)ν(A), (1 -m(A))ν(A)), where m(A) is the prior mean and ν(A) the precision parameter which characterizes the smoothness of the random measure with larger ν(A) corresponding to more smoothness, and then modeling the precision parameters conditional on the latent state V (A) with a hyperprior</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Simulated data are generated from the three scenarios with the densities visualized in the first row of Figure3. (Details on the simulation settings are provided Supplementary Materials F.1.2.) Also presented in Figure3are examples of the posterior mean densities E[q | x] as well as the partition blocks under the MAP tree. Note that the posterior mean is computed by integrating out the unknown tree, and the MAP tree is presented to visualize key distributional features. The results for the first scenario confirms that our more flexible model is much more effective in capturing the discontinuous boundaries of the true density. For the second scenario, our model tends to draw the boundaries that surround the true clusters. In the trees given under the different values of η , however, we can see that fewer nodes were divided inside the clusters when η = 0.01. In contrast, when η = 0.1, the representative tree draws outlines of the clusters and divides regions inside of the clusters at the same time. A similar phenomenon is observed in the third scenario-under our model with flexible partitioning points, partition lines are formed around the region with high density, when η = 0.1 for the boundaries were also drawn within the high probability region. The quantitative comparison based on the KL divergence is provided in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The posterior means of the densities and the representative trees obtained under n = 1000. Each column corresponds to a simulation scenario. The first row shows that true densities, the second row corresponds to the APT model (with fixed partition), and the third and fourth rows correspond to our model with flexible partitioning with parameters η = 0.01 and 0.1 respectively.</figDesc><graphic coords="30,94.41,477.99,139.67,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The wall time under five different data sets. The flexible model with η = 0.01 is used. The black dashed lines indicate the average times.</figDesc><graphic coords="31,180.91,498.27,244.08,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Predictive performance of 6 methods. Each point corresponds to the average of the predictive score in Eq. (12) based on 50 data sets. Each interval is formed by adding and subtracting the standard deviation. In the right plot, the predictive scores of the DPM model for the over 60 dimensional cases are below the displayed range.</figDesc><graphic coords="32,105.76,126.12,191.21,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The ROC curves for the 50-dimensional examples.</figDesc><graphic coords="33,131.16,463.37,339.68,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The node with the highest PMAP P (V (A) = 1 | x) under the three scenarios for the 50-dimensional example, estimated by the MRS with flexible partitioning and η = 0.1. The solid lines mark the boundaries of the nodes and the partition line that divides them into the two children nodes. The red triangle points and the blue circle points are the observations of the two groups included in the node. Gray points indicate the observations outside the node.</figDesc><graphic coords="34,133.16,73.99,335.66,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The MAP tree for the mass cytometry data set visualized up to the 9th level. The size and the color indicate the estimated eff(A), and the number above a node indicates dimension in which it is split. Only the nodes with the sample size larger than 50 are drawn.</figDesc><graphic coords="36,148.38,73.99,309.13,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>), respectively. If v(A) and v (A) introduce the same type of the prior (e.g., Prior A and Prior A), because the corresponding estimators θg (A) have the same form, lA (v (A), T ) -lA (v(A), T ) = 0. On the other hand, if v(A) and v (A) introduce different types of the prior (e.g., Prior A and Prior B), because they are the maximized log-likelihood under the two nested hypotheses, -2[ lA (v (A), T ) -lA (v(A), T )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>.2, we obtain the limit log P (L(A) = µ(A l )/µ(A))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(P g | T ||µ) = arg min T ∈T K G g=1 ζ g KL(P g ||P g | T ), T M ∈ T K M , v ∈ V T M , T ∈ T K \ T K M , and v ∈ V, we have p-lim n→∞ (l(T M ) + l(v , T M )) -(l(T ) + l(v, T )) n ≥ ψ(T M ) -ψ(T ) &gt; 0. P (T, x n ) P (T M , x n ) = P (T )P (x n | T ) P (T M )P (x n | T M ) = P (T ) v∈V P (v) exp(l(v, T )) P (T M ) v∈V P (v) exp(l(v, T M )) ≤ v∈V exp(l(T ) + l(v, T ))P (v) exp(l(T M ) + l(v , T M ))P (v )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Hence, θ 1</head><label>1</label><figDesc>(A) = θ 2 (A) if Z(A) = 0 and θ 1 (A) = θ 2 (A) with probability one if Z(A) = 1. Then we can obtain the following consistency result. Corollary D.1. Let Z = {Z(A)} A∈N (T ) and z = {z(A)} A∈N (T ) be a collection of Z(A) on T ∈ T K and one of its realizations, respectively. If P (Z = z) &gt; 0 for any possible z, thenP Z(A) = 1 {P 1 (A l |A) =P 2 (A l |A)} for all A ∈ N (T ) | T, the indicator function, and P Z(A) = 1 {P 1 (A l |A) =P 2 (A l |A)} for all A ∈ N (T ) | x n p -→ 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The average L 1 distance from the true distribution with error bars that indicate the standard deviation obtained under different sample size, dimensionality, and values of the parameter a.</figDesc><graphic coords="67,98.03,73.98,405.93,396.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The average number of nodes in the MAP trees with error bars that indicate the standard deviation obtained under different sample size, dimensionality, and values of the parameter a.</figDesc><graphic coords="68,98.03,73.98,405.93,396.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>µ 1 = (-2.5, 1.0), µ 2 = (1.0, -2.0), µ 3 = (2.0, 2.5), Σ =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The average KL divergences between the estimated density and the true density.</figDesc><graphic coords="72,100.58,415.80,404.74,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The comparison of the predictive performance. Each point corresponds to the average of the predictive score in Eq. (12) based on 50 data sets. In the right plot, the predictive scores of the DPM model for the over 30 dimensional cases are below the displayed range.</figDesc><graphic coords="73,92.92,89.30,210.33,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The MAP tree for the mass cytometry data set. The size and the color indicate the estimated eff(A). Only the nodes with the sample size larger than 50 are drawn. Since there are a huge number of nodes, the information on the dimension is omitted in this figure.</figDesc><graphic coords="73,93.25,357.89,419.40,255.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: The solid lines delineate the nodes with the highest values of eff(A) and their two children. The red triangle points and the blue circle points are the observations from the two samples in the node. The observations outside the node are in gray. In this figure, the nodes with n g (A) ≥ 10 (g = 1, 2) are chosen.</figDesc><graphic coords="74,86.94,107.60,432.00,491.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the posterior of D t and L t is the same distribution given in Section 3.1. On the other hand, if R t = 1, L t is fixed to 1/2, and the conditional posteriorπ t (J t | L t , R t , T t-1 ) is Mult( λ1 (A t ), . . . , λd (A t )),where λj (A t ) ∝ λ j (A t )h(j, 1/2 | A t ).</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>LM's research is partly supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">DMS-2013930</rs> and <rs type="grantNumber">DMS-1749789</rs>.</p><p>NA is partly supported by a fellowship from the <rs type="funder">Nakajima Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gPuyRNW">
					<idno type="grant-number">DMS-2013930</idno>
				</org>
				<org type="funding" xml:id="_YXVSz2N">
					<idno type="grant-number">DMS-1749789</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Spike-and-slab type prior for L(A)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Introducing an auxiliary variable</head><p>The location variable L(A) follows a spike-and-slab type prior which is expressed with an auxiliary variable R(A) as</p><p>where 1 is the indicator function and the sum of the parameters β l (A) is 1. Under this prior, L(A) follows the prior degenerated at 1/2 if R(A) = 1 and otherwise follows the distribution on grid points other than the middle point. R(A) follows an asymmetric hidden Markov process</p><p>where r(A) ∈ [0, 1]. R(A) = 1 is the absorbing state, so once A is divided at the middle point, L(A ) = 1/2 for every A's descendant node A . In the estimation we especially set the parameters as follows:</p><p>, where {β l (A)} l=1,...,N L is given in (2). Under this setting the prior of L(A) satisfies</p><p>Hence, L(A) follows the same prior as defined in (1) unless A's parent node is divided at the middle point, so the spike-and-slab prior can be seen as a natural extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SMC algorithm</head><p>In the SMC algorithm, we sample values of R(A) in addition to D(A) and L(A).</p><p>If R(A) = 1, which is equivalent to L(A) = 1/2, is sampled, we conclude there is no interesting structure on the node A so fix L to 1/2 for all the subsequent nodes.</p><p>Hence, we need to generalize the SMC algorithm discussed in Section 3.1 to sample from the joint posterior distribution of the finite trees and the auxiliary variables R.</p><p>To describe this joint posterior, let T t denote the finite tree structure, which is determined by the sequence of decisions J 1:t dividing the nodes A 1:t , and let R 1:t be a sequence of the re-fixing variables for A 1:t . Then the target distribution we want to sample from in the SMC is defined as</p><p>The prior P (T t , R 1:t ) = P (J 1:t , R 1:t ) have a Markov chain structure on the tree, and its transition probability is decomposed as</p><p>where J p t = J(A p t ) and R p t = R(A p t ) (A p t is the parent node of A t ). On the other hand, because R 1:t are conditionally independent of the observations given T t , the likelihood only depends on T t as follows:</p><p>For the summand, because θ(A p ) and V (A) are conditionally independent given V (A p ), we obtain</p><p>Therefore, we obtain</p><p>C.2 Asymptotic properties of the tree posteriors Lemma C.2. Let {X i n } n=1,2,... (i = 1, . . . , L) be sequences of random variables that satisfy the following conditions:</p><p>1. X i n &gt; 0 for every i and n.</p><p>2. As n → ∞, the following convergence occurs</p><p>where</p><p>(Proof) We only discuss the case where there exists l such that c l &gt; c l+1 . Let &gt; 0.</p><p>Then, by the first condition,</p><p>as n → ∞. On the other hand,</p><p>where j A is the splitting rule that divides A into A l and A r .</p><p>(Proof) By the result of <ref type="bibr" target="#b33">Schwarz (1978)</ref>, since the parameter θ(A) follow the beta distribution, which belongs to a continuous exponential family, the log of the marginal likelihood is written as</p><p>where the definition of θg (A) (the MLE) and r i (the number of parameters) depend on which type of priors in Assumption 1 is introduced by the state i:</p><p>Notice that, for Prior C, the constant c(A) and the true measures P g must satisfy</p><p>for every g = 1, . . . , G because if this does not hold, the state i is not included in the set of feasible states τ (A | T ).</p><p>Since i ∈ τ (A | T ), the law of large numbers gives θg (A)</p><p>. Hence, we obtain the limit of lA (i, T )/n as lA (i, T ) </p><p>Because Λ g is the KL divergence of the two discrete distributions, Λ g ≥ 0 for all g and Λ g * &gt; 0. By Assumption 1, this result implies that</p><p>Proof of Theorem 4.1 (non-informative prior) and Theorem 4.2</p><p>This section provides the proofs of Theorem 4.1 under the assumption that the prior of the location variables is independent of the sample size, that is, η = 0 in Eq (2).</p><p>The general case is discussed in the next section.</p><p>In this proof, we modify the notation for the marginal likelihood defined in Eq. ( <ref type="formula">4</ref>)</p><p>and use M i (A | T ) to represent the likelihood on A of the tree T under the ith state</p><p>For all A ∈ L(T ), there exists an unique sequence of nodes</p><p>where B A,k ∈ T (k = 0, . . . , K) is a node in the kth level. With this sequence, we obtain the limit of the scaled log-likelihood as</p><p>Because P g | T admits the density function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Numerical evaluation of the density estimators under different complexity of data generating processes</head><p>In this section, we observe the behavior of our proposed HMPT algorithm in density estimation in a case where observed points concentrate around certain points or the boundary of the sample space, which occurs in real data analysis in many cases. We consider the following scenario: the variables X j (j = 1, . . . , d) independently follow the beta distribution</p><p>where the parameter a takes values from R. When a = 0, the data is uniformly distributed in the sample space (0, 1] d , and when a is positive, the distribution concentrates on the central point (0.5, 0.5, ..., 0.5). When a is negative, the distribution concentrates on the vertices of the hyper-cube (0, 1] d . For the cases of a &lt; 0, many values generated from the beta distribution tend to be indistinguishable from 0 and 1, and we found it often made the behavior of our HMPT algorithm, which is designed for continuous distributions without mass points, unstable. As such, for the negative a's, we use an approximative distribution in which the conditional distributions on (0, 10 -5 ] and (0.9 -10 -5 , 1.0] are replaced with the uniform.</p><p>In the estimation, we used the APT model as described in Section 5.1. The number of particles is 1,000, and the maximum resolution is set to 10 or 15. The data is generated under a = (-3, -2, -1, 0, 1, 2, 3), n = 10 2 , 10 3 , 10 </p><p>where β = 0.1 and the number of states I = 5. For the i(&lt; I)th state, given V (A) = i, the precision ν(A) follows the prior</p><p>where a(i) = L + (i -1)(U -L)/(I -1) with L = -1 and U = 4. For the Ith state, F I = 1 ∞ , so ν(A) is fixed to ∞. This is equivalent to stopping the partition and putting the conditional distribution µ(• | A) on A. In the computation, this uniform distribution is approximated by 5 evenly spaced grid points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 Simulation scenarios in the two-dimensional cases</head><p>The data sets are simulated from the following distributions.</p><p>1. "Blocks": In the classical PT method, the Dirichlet prior is set to Dir(0.1k 2 , ..., 0.1k 2 ) (k:</p><p>the depth of the node), and the maximum depth is set to 15. For the Dirichlet process</p><p>Gaussian mixture model, we used the PYdensity function in the R package BNPmix <ref type="bibr" target="#b7">(Corradin et al., 2021)</ref>, in which the strength and discounting parameters are set to 10 and 0, respectively, the covariance matrices are all diagonal (model = "DLS"), and the size of the burn-in period and the sampling is both set to 1000. For the Gaussian kernel density estimation, it is implemented by the kde function in the R package ks.</p><p>We set the bandwidth matrix by using Hpi.diag function which selects the optimal diagonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Two-group comparison F.2.1 Hyper-parameter settings</head><p>For the transition matrix ξ(A), we use the form proposed in <ref type="bibr" target="#b34">Soriano and Ma (2017)</ref> for incorporating multiple testing control</p><p>, where γ ∈ (0, 1), and ρ ∈ (0, 1), and k is the depth of A, and we set (γ, ρ) = (0.3, 0.3) following recommendations in that paper. where δ j = -0.5 for j = 1, . . . , 5 and 0 for j = 6, . . . , 25.</p><p>2. "Local dispersion difference": For j = 1, . . . , 25, (X 1,2(j-1)+1 , X 1,2j ) ∼ 1 3 N (µ 1 , Σ) +</p><p>where ∆ j = -0.4 for j = 1, . . . , 5 and 0 for j = 6, . . . , 25.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian and conditional frequentist testing of a parametric model versus nonparametric alternatives</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guglielmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">453</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pólya tree posterior distributions on densities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;Institut Henri Poincaré, Probabilités et Statistiques</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="2074" to="2102" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Institut Henri Poincaré</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pólya tree posterior distributions on densities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;Institut Henri Poincaré, Probabilités et Statistiques</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="2074" to="2102" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Institut Henri Poincaré</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Randrianarisoa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05265</idno>
		<title level="m">Optional Pólya trees: posterior rates and uncertainty quantification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric k-sample tests for censored and uncensored data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian cart model search</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">443</biblScope>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for related densities by using Pólya trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="153" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bnpmix: an r package for bayesian nonparametric modelling via pitman-yor mixtures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Corradin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Canale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nipoti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contextual hidden markov models for wavelet-domain signal processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ks: Kernel density estimation and kernel discriminant analysis for multivariate data in r</title>
		<author>
			<persName><forename type="first">T</forename><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prior distributions on spaces of probability measures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="629" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Bayesian nonparametric approach to testing for dependence between random variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the asymptotic behavior of bayes&apos; estimates in the discrete case</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="1386" to="1403" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling regression error with a mixture of Polya trees</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1020" to="1033" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inference for mixtures of finite Polya tree models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1548" to="1565" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-sample Bayesian nonparametric hypothesis testing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="320" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A class of mixtures of dependent tail-free processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="553" to="566" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Standardization and quality control for high-dimensional mass cytometry studies of human samples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kleinsteuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corleis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nchinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Medoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry Part A</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="903" to="913" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Top-down particle filtering for Bayesian decision trees</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some aspects of Polya tree distributions for statistical modelling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lavine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1222" to="1235" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian multiscale smoothing of gaussian noised images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="733" to="758" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multivariate density estimation by Bayesian sequential partitioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">504</biblScope>
			<biblScope unit="page" from="1402" to="1410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive shrinkage in Pólya tree type models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="779" to="805" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of distributional variation through graphical multi-scale beta-binomial models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soriano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="541" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupling optional Pólya trees and the two sample problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1553" to="1565" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Bayesian non-parametric approach to survival analysis using Polya trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Muliere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="340" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian curve fitting using multivariate normal mixtures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erkanli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polya tree distributions for statistical modeling of censored data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Neath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Decision Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="186" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rubbery Polya tree</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Nieto-Barajas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="184" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric multiple imputation of partially observed data with ignorable nonresponse</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Paddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="538" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Randomized Polya tree models for nonparametric Bayesian inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Paddock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lavine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="443" to="460" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic multi-resolution scanning for two-sample differences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="547" to="572" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On Bayesian consistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Hjort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="811" to="821" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric inference for random distributions and related functions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Damien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Laud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="485" to="527" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical generalized linear models and frailty models with Bayesian nonparametric mixing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Mallick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="845" to="860" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian smoothing of photon-limited images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="599" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The large-sample distribution of the likelihood ratio for testing composite hypotheses</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="62" />
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optional Pólya tree and Bayesian inference</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1433" to="1459" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
