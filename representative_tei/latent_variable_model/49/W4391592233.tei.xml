<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Bayesian classification for models with scalar and functional covariates</title>
				<funder ref="#_WU3Br4H #_8QS9Mdu #_8Bquv6R">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Ef2WGA2">
					<orgName type="full">United States Department</orgName>
				</funder>
				<funder ref="#_K2qvs56">
					<orgName type="full">FAPESP</orgName>
				</funder>
				<funder ref="#_a63cJBd">
					<orgName type="full">NIMH</orgName>
				</funder>
				<funder ref="#_pXJtaBA">
					<orgName type="full">CNPq</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-02-08">8 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nancy</forename><forename type="middle">L</forename><surname>Garcia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mariana</forename><surname>Rodrigues-Motta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helio</forename><forename type="middle">S</forename><surname>Migon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eva</forename><surname>Petkova</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Population Health</orgName>
								<orgName type="department" key="dep2">Grossman School of Medicine</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Child and Adolescent Psychiatry</orgName>
								<orgName type="department" key="dep2">Grossman School of Medicine</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thaddeus</forename><surname>Tarpey</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Population Health</orgName>
								<orgName type="department" key="dep2">Grossman School of Medicine</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">Todd</forename><surname>Ogden</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julio</forename><forename type="middle">O</forename><surname>Giodano</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">College of Agriculture and Life Sciences</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Matias Perez</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">College of Agriculture and Life Sciences</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Bayesian classification for models with scalar and functional covariates</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-08">8 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.04037v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>latent vector</term>
					<term>functional covariates</term>
					<term>variable selection</term>
					<term>unsupervised clustering</term>
					<term>variational inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider unsupervised classification by means of a latent multinomial variable which categorizes a scalar response into one of L components of a mixture model. This process can be thought as a hierarchical model with first level modelling a scalar response according to a mixture of parametric distributions, the second level models the mixture probabilities by means of a generalised linear model with functional and scalar covariates. The traditional approach of treating functional covariates as vectors not only suffers from the curse of dimensionality since functional covariates can be measured at very small intervals leading to a highly parametrised model but also does not take into account the nature of the data. We use basis expansion to reduce the dimensionality and a Bayesian approach to estimate the parameters while providing predictions of the latent classification vector. By means of a simulation study we investigate the behaviour of our approach considering normal mixture model and zero inflated mixture of Poisson distributions. We also compare the performance of the classical Gibbs sampling approach with Variational Bayes Inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mixture models are popular statistical tools for classification purposes in a broad range of applied fields. The Gaussian mixture model is by far the most used approach for model based cluster analysis (e.g., <ref type="bibr" target="#b7">Day, 1969;</ref><ref type="bibr" target="#b12">Fraley and Raftery, 2006;</ref><ref type="bibr" target="#b33">McNicholas and Murphy, 2010)</ref>. However, classification problems based on mixture models often require non-Gaussian mixture distributions. As an example, consider zero-inflated regression models (e.g., <ref type="bibr" target="#b28">Lambert, 1992;</ref><ref type="bibr" target="#b45">Ridout et al., 1998)</ref>, whose distribution of the count outcome is a mixture of two components and the goal is to classify zero outcomes as coming †Corresponding author: Department of Statistics, University of Campinas, São Paulo, Brazil. e-mail: nancyg@unicamp.br from either a degenerate at zero distribution or zeros generated by means of a count distribution, such as a Poisson or Negative Binomial distribution. Other examples are the zero-augmented models for semi-continuous data (e.g., <ref type="bibr" target="#b47">Rodrigues-Motta et al., 2015)</ref>. Such models fit data using a mixture of two components where one component models the zero by means of a degenerate at zero distribution and the other component models the positive outcome using a continuous positive distribution, as for example gamma or lognormal distributions.</p><p>Mixture probabilities often depend on scalar explanatory variables (e.g., <ref type="bibr" target="#b28">Lambert, 1992;</ref><ref type="bibr" target="#b45">Ridout et al., 1998;</ref><ref type="bibr" target="#b19">Hall, 2000;</ref><ref type="bibr" target="#b20">Hall and Zhang, 2004)</ref>. However, many modern applications routinely have more complex covariates in the form of vectors, matrices, functions, images. The main question of interest is to examine how these complex covariates affect the response. The prevailing approaches in these cases use either a parametric or a nonparametric approach and model the mean of the distribution as a function of the covariates; see for example, <ref type="bibr" target="#b5">Cardot et al. (1999)</ref>; <ref type="bibr" target="#b23">James (2002)</ref>; <ref type="bibr" target="#b41">Ramsay and Silverman (2005)</ref>; <ref type="bibr">Ferraty and</ref><ref type="bibr">Vieu (2006, 2009)</ref>; <ref type="bibr" target="#b16">Goldsmith et al. (2011)</ref>; <ref type="bibr" target="#b32">McLean et al. (2014)</ref> among others. On the other hand, there are other applications where the interest is to study the effect of the covariates on the entire distribution of the response, for example quantile regression <ref type="bibr" target="#b26">(Koenker and Bassett Jr, 1978</ref>) (e.g., <ref type="bibr">Park et al., 2019, and references therein)</ref>.</p><p>In this study, we have a different objective, which is to classify an outcome as accurately as possible using the information on the scalar and functional covariates as explanatory variables for the mixture probability. We model the mixture model in terms of a latent variable. The role of the latent variable is not only to divide a sample of subjects into subgroups according to some similarity measure, but also to provide practitioners interpretable clustering results. Many authors have studied the classification problem, see for example <ref type="bibr" target="#b54">Titterington et al. (1985)</ref>; <ref type="bibr" target="#b9">Everitt and Hand (1981)</ref>; <ref type="bibr" target="#b31">McLachlan and Peel (2004)</ref> and references therein. Moreover, there are R packages (R Core Team, 2020) that perform inference for mixture models such as mixtools <ref type="bibr" target="#b2">(Benaglia et al., 2009)</ref> but these tools cannot implement one or more functional covariates. First of all, it is necessary to reduce the dimensionality of the data. For example, for one of our illustrations, the dataset has 14 functional covariates, each one of which is observed at 45 points. <ref type="bibr" target="#b24">Jiang et al. (2017)</ref> consider these functional covariates as 14 × 45 matrices and propose a variation of principal components analysis built upon a low rank Candecomp/Parafac decomposition applied to the rows and columns of the matrices. Although their method is powerful, it requires all functional covariates to be observed at the same points across observations. Also, it does not take advantage of the functional structure of the data and does not consider how the covariates are ordered in the matrix. In this paper, similarly to <ref type="bibr" target="#b6">Ciarleglio et al. (2018)</ref>, we propose a flexible classification procedure, more general than a Gaussian mixture, that can incorporate functional covariates, either through a linear or non-linear effect, that not only reduces the dimensionality of the problem but also takes advantage of the functional nature of the covariates. Additionally, the method does not restrict the functions to be observed at common points.</p><p>The remainder of this paper is organized as follows. Section 2 introduces the hierarchical mixture model with latent variable and regression of mixture probabilities as function of functional covariates and a Bayesian approach is presented in Section 3. Our method can be applied to regression with a response of continuous, semi-continuous and discrete nature, but in this article we illustrate the method considering a normal mixture model and zero inflated mixture of Poisson (ZIMP) model, as shown in Sections 4 and 5, respectively. An extensive simulation study is given in Section 6, with the primary goal of examining the performance of the normal mixture model and the ZIMP model by considering aspects of sample size and ability of the functional curves to predict the latent variables correctly. The secondary goal of the simulation study is to compare the estimation ability of the MCMC and Variational Bayes (VB) methods <ref type="bibr" target="#b36">(Ormerod and Wand, 2012;</ref><ref type="bibr" target="#b21">Hoffman et al., 2013;</ref><ref type="bibr" target="#b4">Blei et al., 2017)</ref>, as well as their performance with respect to computational time-consuming. Finally, two applications with real data are presented in Section 7, one for the normal mixture model and another for the ZIMP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General model</head><p>For each subject i = 1, . . . , n, we observe: y i the scalar response, z i a vector of scalar covariates and X ij = {t ijs , X ij (t ijs ), 1 ≤ s ≤ S ij , 1 ≤ j ≤ J}, J functional covariates observed at discrete points in closed domains τ 1 , . . . , τ J . In general, these sets are closed intervals on the real line, and although we interpret t ijs as time in this study, our proposed model works for functional covariates observed at points in space or timespace. Each functional covariate X ij is observed at S ij points t ijs , 1 ≤ s ≤ S ij which do need to be the same along the subjects. That is, neither the domains in which we observe the functional covariates nor the observation points need to be the same. We model the distribution of y i hierarchically by means of a latent class model, postulating a mixture distribution for the observed response to classify subjects into L classes. We will assume an L-mixture latent class model with unobserved multinomial random variables indicating class membership {γ i = (γ i1 , . . . , γ iL ), i = 1, . . . , n} where L l=1 γ il = 1 and p il = P (γ il = 1).</p><p>Probability of the random variable γ il is modelled as a function of the scalar and functional covariates by</p><formula xml:id="formula_0">g(p il ) = z i θ l + J j=1 τj F jl (X ij (t), t, φ jl )dt (1)</formula><p>where g is a known link function (e.g., probit or logit), θ l is a vector of parameters that captures the linear additive effect of the scalar covariates, and F jl (•, •) is a bivariate smooth function related to the jth functional covariate in component l which depends on the vector of parameters φ jl . This is termed the functional generalized additive model by <ref type="bibr" target="#b32">McLean et al. (2014)</ref>. Let y = (y 1 , . . . , y n ) , z = (z 1 , . . . , z n ) and X = {X ij , i = 1, . . . , n, j = 1, . . . , J} be vectors and let f l (y i ; λ il ), i = 1, . . . , n be the pdf of the scalar response y i in class l, such as λ il is the vector of parameters in that class. Then the likelihood of the mixture model for y is given by f (y|λ 1 , . . . , λ L , θ, φ, z, X) = n i=1 L l=1 p il f l (y i ; λ il ) , and the complete-data likelihood can be written as f (y, γ|λ 1 , . . . , λ L , θ, φ, z, X)</p><formula xml:id="formula_1">= n i=1 L l=1 f l (y i ; λ il ) γil p il ,<label>(2)</label></formula><p>where the relationship between p ij with θ, φ, z and X is given by(1).</p><p>In general, a nonparametric model is used to represent the effects of the covariates on λ il but not on the mixing probabilities p il , see for example <ref type="bibr" target="#b5">Cardot et al. (1999)</ref>, <ref type="bibr" target="#b23">James (2002)</ref>, <ref type="bibr" target="#b10">Ferraty and Vieu (2006)</ref>, <ref type="bibr" target="#b42">Ramsay and Silverman (2007)</ref>, <ref type="bibr" target="#b32">McLean et al. (2014)</ref> and references therein. In many applications, as will be shown in Section 7, the interest lies in using the covariates solely to classify subjects into L classes. Therefore it is necessary to relate the covariates to the mixing probabilities p il and not to the parameters λ.</p><p>To demonstrate the strength of our method, in our simulations and applications, we will analyse two cases explicitly. The first one is the Mixture of Normal distributions where λ il = (µ l , σ l ) and f corresponds to the normal density with mean µ l and standard deviation σ l . The second one is Zero Inflated Mixture of Poisson distributions (ZIMP), considering f 1 = δ {0} as the distribution for the class with "Pure Zero" and f l (y i ; λ l ) for l = 1, 2 correspond to Poisson distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Functional linear model</head><p>Model (1) can be restricted to be linear by specifying F jl (x(t), t) = w jl (t)x(t), yielding the more common generalized linear functional model</p><formula xml:id="formula_2">g(p il ) = z i θ l + J j=1 τj w jl (t)X ij (t)dt,<label>(3)</label></formula><p>l = 1, . . . , L. To fit model (3), we consider each weight function w jl (.) as a smooth function approximated by a function belonging to the finite-dimensional space spanned by B-splines basis functions. This is not the only possibility, as other bases could be chosen such as Fourier expansion, wavelets, natural splines, etc. <ref type="bibr" target="#b49">(Silverman, 2018)</ref>. Also, we are going to choose the number of knots and knots placement in an ad-hoc manner.</p><p>Although knot determination and placement are important issues, they are not the objective of this work and will not be discussed here. Therefore, for a positive integer K j and a vector of (K j -4) interior knots Υ j ⊂ τ j , we express the weight function as</p><formula xml:id="formula_3">w jl (t) = Kj k=1 φ jlk B (j) k (t),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">{B (j) 1 , . . . , B<label>(j)</label></formula><p>Kj } are cubic B-spline basis functions determined by Υ j .</p><p>Substituting (4) into (3) yields</p><formula xml:id="formula_5">g(p il ) = z i θ l + J j=1 R ij φ jl ,<label>(5)</label></formula><p>where, for each pair (j, l), φ jl = (φ jl1 , . . . , φ jlKj ) and</p><formula xml:id="formula_6">R ij = (R ij1 , . . . , R ijKj ) are vectors of length K j , with R ijk = τj B (j) k (t)X ij (t)dt.</formula><p>The linear case has the advantage of easy interpretability of the weight functions. If the weight w jl (t) is positive (negative) over the interval (t a , t b ), this means that the higher the value of X ij (t) in this interval the higher (lower) the probability of γ il = 1, considering all other explanatory variables fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Functional nonlinear model</head><p>For the more general model (1), we consider each function F jl (•, •) to be a smooth surface which can be well approximated by a family of tensor products of cubic B-splines (see for example, <ref type="bibr" target="#b25">Kim et al. (2018)</ref>). That is, for each function F jl , there exist positive integers K 1j and K 2j and vectors of (K 1j -4) interior knots Υ 1j ⊂ τ j and (K 2j -4) interior knots Υ 2j ⊂ χ j , the image of X ij , such that</p><formula xml:id="formula_7">F jl (s, t) = K1j k1=1 K2j k2=1 φ jlk1k2 B (Υ1j) k1 (t)B (Υ2j) k2 (s),<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">{B (Υ1j) 1 , . . . , B<label>(Υ1j)</label></formula><formula xml:id="formula_9">K1j } and {B (Υ2j) 1 , . . . , B<label>(Υ1j)</label></formula><p>K2j } are B-spline basis determined by Υ 1j and Υ 2j , respectively.</p><p>Substituting (6) into (1) yields</p><formula xml:id="formula_10">g(p il ) = z i θ l + J j=1 R ij φ jl ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">R ij are vectors of dimension (K 1 * K 2 ) × 1 with φ jl (k 1 , k 2 ) = φ jlk1k2 and R ij (k 1 , k 2 ) = χj τj B (Xij) k1 (X ij (s))B (Tij)</formula><p>k2 (t) dt ds properly stacked.</p><p>As we can see, from ( <ref type="formula" target="#formula_5">5</ref>) and ( <ref type="formula" target="#formula_10">7</ref>), using basis expansion for both the linear and nonlinear case, we end up with the same linear structure in terms of parameters θ and φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Bayesian approach to the mixture model regression with functional covariates</head><p>Model (2) specifies the distribution of the response y i depending on which mixture component subject i belongs. The mixture components are parameterized by the vector λ = (λ 1 , . . . , λ L ) whose components are related to the densities f 1 , . . . , f L , respectively, considering category L as the baseline. Therefore, we will denote by Θ = (λ , β ) the vector of unknown parameters where β = (β 1 , . . . , β L-1 ) and β l = (θ l , φ 1l , . . . , φ Jl ) indicate the parameters for the regression coefficients of the model with x i := (z i , R i1 , . . . , R iJ ) as covariates for subject i.</p><p>3.1. Hierarchical structure specification and prior specification A formal Bayesian analysis of a mixture model usually leads to intractable calculations. Data augmentation is an efficient procedure for mixture models that leads to feasible computations using Gibbs sampling <ref type="bibr" target="#b8">(Diebolt and Robert, 1994)</ref>. The joint augmented posterior distribution is the product of (2) and the prior distributions and has no closed form. Therefore, the Gibbs sampling algorithm is suitable to sample from the posterior distribution of γ, λ and β.</p><p>The nature of the application under study dictates the form of f l (y i ; λ l ) which in turn provides knowledge about the nature of parameters in λ l . There is a very rich family of distributions f l (y i ; λ l ) that may characterize the mixture distribution of y i . Instead of focusing on a specific distribution for mixture components, we focus on a general solution for posterior sampling of the latent variables γ and parameters in Θ, which are developed with a general f l (y i ; λ l ) without loss of generality. Therefore, prior distributions for λ are problem specific. In particular, for each of the components of β, we will assume a Student-t prior distribution with mean 0, degrees-of-freedom parameter df , and scale s, with df and s providing minimal prior information to constrain the coefficients to lie in a reasonable range (see Section 2 of <ref type="bibr" target="#b15">Gelman et al., 2008)</ref>. An advantage of the t family is that fat-tailed distributions allow for flexible inference, since it includes both the Gaussian (df = ∞) and the Cauchy (df = 1) distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Posterior computation of parameters</head><p>We sample from the posterior distribution using a Gibbs sampling scheme, and most of the full conditional posterior distribution of the latent variables γ and parameters in Θ are given by standard methods. For the sake of completeness, we describe briefly the conditional posterior distributions for γ and β.</p><p>Full conditional posterior distribution of γ i Let γ -i = (γ 1 , . . . , γ (i-1) , γ (i+1) , . . . , γ n ), i.e., the vector (γ 1 , . . . , γ n ) leaving out the ith element. The full conditional posterior distribution of γ i is given by</p><formula xml:id="formula_12">P (γ il = 1|Θ, y, γ -i ) ∝ f l (y i ; λ l )g -1 l (x i β l ).</formula><p>For example, for the logit link function we have</p><formula xml:id="formula_13">g -1 l x i β l ∝ exp x i β l</formula><p>whereas for the probit link function we have</p><formula xml:id="formula_14">g -1 l x i β l ∝ Φ x i β l for l = 1, . . . , L -1 and g -1 L x i β = 1 - L-1 l=1 g -1 l x i β l .</formula><p>Full conditional posterior distribution of β l The full conditional posterior distribution of β l cannot be computed explicitly except for when we are using the probit link function and thus we can apply the simple latent-variable method of <ref type="bibr" target="#b0">Albert and Chib (1993)</ref>. Other methods for calculating the full conditional have been proposed using data-augmentation or multiple layers of latent variables, see for example <ref type="bibr">Holmes et al. (2006), Frühwirth-Schnatter and</ref><ref type="bibr" target="#b13">Frühwirth (2010)</ref>, <ref type="bibr" target="#b17">Gramacy et al. (2012)</ref> and <ref type="bibr" target="#b39">Polson et al. (2013)</ref>. In our approach, we follow <ref type="bibr" target="#b15">Gelman et al. (2008)</ref> by considering Student t prior distribution for each component of β l , in which the standard logistic regression algorithm proceeds by approximately linearizing the score function, solving using weighted least squares, and then iterating this process, each step evaluating the derivatives at the latest estimate pilk . As in the classical logistic regression, at iteration k, the algorithm determines pseudo-data ψ i given by</p><formula xml:id="formula_15">ψ il = g(p ilk ) + (γ il -pilk )g (p ilk ), i = 1, . . . , n<label>(8)</label></formula><p>and weights</p><formula xml:id="formula_16">W -1 lk = diag g (p ilk ) 2 V ilk (9)</formula><p>where V ilk is the variance function evaluated at p1lk in iteration k <ref type="bibr" target="#b30">(McCullagh and Nelder, 1989)</ref>. We then perform weighted least squares, regressing the working variable ψ l = (ψ 1l , . . . , ψ nl ) on the design matrix x = (x 1 , . . . , x n ) of dimension n × p l with weights W lk to give a new estimate of β l , and the iteration proceeds until approximate convergence.</p><p>We add prior information to the classical logistic regression algorithm given in ( <ref type="formula" target="#formula_15">8</ref>) and ( <ref type="formula">9</ref>) by augmenting the approximate likelihood with the prior distribution β l ∼ N (µ bl , Σ β ), with Σ β = σ 2 l I, l = 1, . . . , L -1. Considering a normal distribution as an approximation to the generalized linear model likelihood, the full conditional posterior density is given by log</p><formula xml:id="formula_17">p(β l |ψ) ∝ exp - 1 2 (ψ l -xβ l ) Σ -1 (ψ l -xβ l ) × exp - 1 2 (β l -µ bl ) Σ β (β l -µ bl ) (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where Σ = W -1 lk , with W -1 lk as in (9) and elements of ψ l given in (8). Rearranging terms in (10), the full conditional posterior density of β l is given by a normal distribution with covariance matrix</p><formula xml:id="formula_19">V β = (x Σ -1 x + Σ -1 β ) -1 and mean V β (x Σ -1 ψ l + Σ -1 β µ bl ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A generic discussion of variational inference</head><p>Modern data analysis often demands computation with complex models and massive datasets. To scale the problem described in the introduction of this paper for large samples and to include more functional covariates, we must resort to approximate posterior inference. Variational Bayes inference (VI) is a machine learning technique that facilitates approximation of the posterior distribution in complex models using massive datasets <ref type="bibr" target="#b4">(Blei et al., 2017;</ref><ref type="bibr" target="#b36">Ormerod and Wand, 2012;</ref><ref type="bibr" target="#b21">Hoffman et al., 2013)</ref>. VB inference provides the main alternative to the Markov Chain Monte Carlo (MCMC) algorithm <ref type="bibr" target="#b46">(Robert and Casella, 2004;</ref><ref type="bibr" target="#b14">Gamerman and Lopes, 2006)</ref>. To fix ideas, let us consider the model described by the DAG (direct acyclical graph) shown in Figure <ref type="figure">1</ref>, where β is a vector of regression parameters, γ i are categorical latent variables and y the observations.</p><formula xml:id="formula_20">Y i γ i p i λ i β z i , x ij (t) i = 1, . . . , n j = 1, . . . , J, = 1, . . . , L</formula><p>Fig. <ref type="figure">1</ref>: Direct Acyclical Graph describing the general model. The circles represent random quantities and rectangles represent deterministic quantities. The variable Y i is the ith individual observation, γ i is a latent variable indicating the class of the ith observation, p i = g -1 (x i β) is the probability of individual i belongs to category , where x i is a vector of known regressors and β the regression parameters.</p><p>VB inference starts by introducing a variational family of distributions, indexed by some variational parameters κ and a criterion function to search for the member q(•|κ) of the family that best approximates the predictive distribution. The optimisation criterion is derived based on the log-marginal posterior distribution of the observed data, a usual model selection criterion, log(p(y|M)). Often this quantity evolves to where it requires the solution to an intractable integral. To avoid this tedious calculation, a lower bound quantity, called ELBO (Evidence Lower Bound) is easily evaluated as: log(p(y|M)) = log p(y, λ, β, γ)dγdβdλ</p><p>= log E q p(y, λ, β, γ) q(λ, β, γ|κ)</p><formula xml:id="formula_21">≥ E q log p(y, λ, β, γ) q(λ, β, γ|κ)<label>(11)</label></formula><p>where γ and (λ, β) represent local and global quantities/parameters, respectively. The inequality in ( <ref type="formula" target="#formula_21">11</ref>) is obtained by Jensen's inequality. It is natural to use this lower bound as a model selection criterion in place of the predictive distribution, avoiding cumbersome high dimensional integration. Therefore, the VI inference objective is to maximize ELBO, which is equivalent to minimizing the Kulback-Leibler divergence up to an additive constant <ref type="bibr" target="#b4">(Blei et al., 2017)</ref>. For the variational family of distributions, q(λ, β, γ|κ), in this paper, we focus on the mean-field inference although many researchers have also studied more complex families (e.g., <ref type="bibr" target="#b21">Hoffman et al., 2013;</ref><ref type="bibr">Ranganath et al., 2016, among others)</ref>.</p><p>Based on the illustrative Figure <ref type="figure">1</ref>, we propose the following partition of the joint distribution of local and global, denominated mean field family <ref type="bibr" target="#b37">(Parisi, 1988)</ref>, q(λ, β, γ|κ) = q(λ|κ)q(β|λ, κ)</p><formula xml:id="formula_22">n i=1 q(γ i |β, κ)<label>(12)</label></formula><p>where κ comprises all the parameters of the variational family. To avoid a cumbersome notation we are using the same notation q for the joint variational distribution of ( <ref type="formula" target="#formula_21">11</ref>) and the conditional distributions in (12). The approximate conditional inference is viewed as an optimisation problem. Given the above setup, the mean field family and the ELBO criterion, one can find the optimal solution via the coordinate ascent variational inference (CAVI) algorithm <ref type="bibr" target="#b3">(Bishop, 2006)</ref>. Each factor of the mean-field variational density is optimised iteratively, while keeping the others fixed, climbing the ELBO to a local optimum.</p><p>Letting ν = (λ, β, γ), we need to compute q * (ν|κ) = q * (λ|κ)q * (β|λ, κ) n i=1 q * (γ i |β, κ) where q * (λ|κ) ∝ exp E (β,γ) log p(λ|β, γ, y, κ) ,</p><p>q * (β|κ) ∝ exp E (λ,γ) log p(β|λ, γ, y, κ) , and (14)</p><formula xml:id="formula_24">q * (γ i |κ) ∝ exp E (γ(-γi),λ,β) log p(γ i |γ(-γ i ), λ, β, y) .<label>(15)</label></formula><p>It is worth it pointing out that the form of the optimal densities involves the full conditional distributions, revealing a link with Gibbs sampling. However, the VB algorithm does not repeatedly simulate from the full conditional distributions as is done by the Gibbs sampler.</p><p>Alternative ways to maximize the ELBO are discussed in <ref type="bibr" target="#b21">Hoffman et al. (2013)</ref> and <ref type="bibr" target="#b43">Ranganath et al. (2014)</ref>. They propose to calculate the ELBO gradient and use one of many alternative gradient ascent algorithms. Much effort has been done to take care of more general settings and developed generic algorithms for conjugate exponentialfamily models <ref type="bibr" target="#b1">(Attias, 1999;</ref><ref type="bibr" target="#b58">Xing et al., 2003)</ref>, leading to the automated variational inference, allowing users to write down a model and immediately use variational inference to approximate its posterior distribution <ref type="bibr" target="#b3">(Bishop, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Normal mixture regression model with functional covariates</head><p>In this section, inspired by the dataset to be analysed in Section 7.1, we deal with the mixture model of two normal distributions with different means but the same variance. Let Y 1 , . . . , Y n , be independent random variables with</p><formula xml:id="formula_25">p(y i |µ 0 , µ 1 , σ 2 , γ i ) = γ i φ(y i ; µ 1 , σ 2 ) + (1 -γ i )φ(y i ; µ 0 , σ 2 ), µ 1 &gt; µ 0 (16)</formula><p>where φ(.; µ, σ 2 ) is the normal density with parameters µ and σ 2 , and γ 1 , . . . , γ n are binary latent random variables, and let</p><formula xml:id="formula_26">p(γ i |β) = p i (β) γi (1 -p i (β)) 1-γi , γ i = 0 or 1</formula><p>where</p><formula xml:id="formula_27">p i (β) = g -1 x i β ,<label>(17)</label></formula><p>g is a link function, and β = (θ, φ) with θ being a vector of parameters associated to scalar effects and φ a vector representing the coefficients of a function written from a B-splines expansion given by ( <ref type="formula" target="#formula_5">5</ref>) or (7).</p><p>For the parameters µ 0 , µ 1 , and σ 2 in the model, we use diffuse priors:</p><formula xml:id="formula_28">• µ 0 ∼ N (0, τ 2 0 ), µ 1 ∼ N (0, τ 2 1 )</formula><p>• σ 2 ∼ inverse gamma(a 0 , b 0 ).</p><p>For each coefficient θ, φ 1 , . . . , φ J , we specify weakly informative t family of prior distributions with mean 0, degrees-of-freedom parameter ν, and scale s, with ν and s providing minimal prior information to constrain the coefficients to lie in a reasonable range (see Section 2 of <ref type="bibr" target="#b15">Gelman et al. (2008)</ref>). The Gaussian distribution is obtained when ν → ∞, whereas the Cauchy distribution corresponds to ν = 1. The observed data likelihood for the hierarchical model is difficult to optimize directly because the unobserved vector γ = {γ i } n i=1 . Denoting by Θ = (µ 0 , µ 1 , σ 2 , β), the complete likelihood is given by -γi) . ( <ref type="formula">18</ref>)</p><formula xml:id="formula_29">f (y, γ|Θ) = n i=1 1 √ 2πσ 2 exp - (y i -µ 0 (1 -γ i ) -µ 1 γ i ) 2 2 σ 2 × [p i (β)] γi [1 -p i (β)] (1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Full conditional posterior distributions</head><p>The joint augmented posterior distribution is proportional to the product of the likelihood given by ( <ref type="formula">18</ref>) and prior distributions specified in the previous section and has no closed form. Therefore, we adapt the Gibbs sampling algorithm to sample from the full conditional posterior distribution of Θ and the latent variables γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Full Conditional posterior distribution of γ i</head><p>Let γ -i be the vector γ = (γ 1 , . . . , γ i-1 , γ i+1 , . . . , γ n ). The full conditional posterior distribution of γ i is given by</p><formula xml:id="formula_30">P (γ i = 1|Θ, y, γ -i ) = φ yi-µ1 σ p i (β) φ yi-µ1 σ p i (β) + φ yi-µ0 σ (1 -p i (β)) since P (γ i = x, Θ, y, γ -i ) = φ yi-µ1x-µ0(1-x) σ p i (β)</formula><p>where p i (β) depends on the covariates through the regression term and it is given by (17).</p><p>4.1.2. Full Conditional posterior distribution of µ 0 and µ 1 We update µ 0 using a normal distribution with mean</p><formula xml:id="formula_31">1 τ 2 0 + 1 σ 2 n i=1 (1 -γ i ) -1 n i=1 y i (1 -γ i )</formula><p>and variance 1</p><formula xml:id="formula_32">τ 2 0 + 1 σ 2 n i=1 (1 -γ i ) -1</formula><p>, while µ 1 is updated, conditionally on µ 0 , with a truncated normal distribution on (µ 0 , ∞), with mean</p><formula xml:id="formula_33">1 τ 2 1 + 1 σ 2 n i=1 γ i -1 n i=1 y i γ i and variance 1 τ 2 1 + 1 σ 2 n i=1 γ i -1</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Full Conditional posterior distribution of σ 2</head><p>We update σ 2 using an inverse-gamma distribution with parameters</p><formula xml:id="formula_34">a 0 + n 2 and b 0 + n i=1 (y i γ i -µ 1 ) 2 + (y i (1 -γ i ) -µ 0 ) 2 . 4.1.4. Full Conditional posterior distribution of β = (θ, φ 1 , . . . , φ J )</formula><p>Here we implement the computation of the full conditional posterior distribution of (θ, φ 1 , . . . , φ J ) simultaneously, following <ref type="bibr" target="#b15">Gelman et al. (2008)</ref>. We sample from the full conditional distribution of β = (θ, φ 1 , . . . , φ J ) by assuming a t prior distribution to each parameter β d in θ and in φ j , j = 1, . . . , J. However, instead of using a t distribution directly, <ref type="bibr" target="#b15">Gelman et al. (2008)</ref> assume</p><formula xml:id="formula_35">β d ∼ N (µ d , σ 2 d ) and σ 2 d ∼ Inv-χ 2 (ν j , s 2 ).</formula><p>The parameters β d 's are treated as missing data and performing the EM algorithm, we estimate σ 2 d 's. The algorithm proceeds by alternating one step of iteratively weighted least squares to calculate the expectation of the logarithm of the full conditional posterior distribution using the estimate βj and one step of the EM algorithm to calculate σ2 d by maximization. Once enough iterations have been performed to reach approximate convergence, we get an estimate for the vector parameter β = (θ, φ 1 , . . . , φ J ). This step is performed inside the Gibbs algorithm to sample from the conditional posterior distribution of (θ, φ 1 , . . . , φ J ). To perform the calculations, we use the bayesglm function implemented in R by <ref type="bibr" target="#b15">Gelman et al. (2008)</ref>. To use the function bayesglm, we specify the link function g as either the probit or logit function, and inform the degrees of freedom ν and scale parameter s as appropriate to consider a Normal, t or Cauchy prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Variational Bayes for normal mixed model</head><p>For the mixture of normal distributions model, the augmented vector of unknown parameters is ν = (λ, β, γ), where λ = (µ 0 , µ 1 , σ 2 ), β = (θ, φ) and γ = (γ 1 , . . . , γ n ). Denote the parameters of the variational distributions as</p><formula xml:id="formula_36">κ = m 0 , s 2 0 , m 1 , s 2 1 , α, A 0 , B 0 , µ * b , V β , and define q * (ν|κ) = q * (µ 0 |κ)q * (µ 1 |κ)q * (σ 2 |κ)q * (β|κ) n i=1 q * (γ i |κ).</formula><p>According to Equations ( <ref type="formula" target="#formula_23">13</ref>), ( <ref type="formula">14</ref>) and ( <ref type="formula" target="#formula_24">15</ref>), we have to calculate the variationals of µ 0 , µ 1 , σ 2 , β, and γ 1 , . . . , γ n .</p><p>In the next sections, to simplify the notation, we will omit the dependence on κ when writing the variational distributions q * . The details of computations can be found in Appendix A.3. 4.2.1. Variational density q * (γ i ) If we consider q * (µ 0 ) and q * (µ 1 ) belonging to the family of independent distributions with means m 0 and m 1 and variances s 2 0 and s 2 1 , respectively, we get that γ i is a Bernoulli random variable with variational parameters for α i given by</p><formula xml:id="formula_37">α i = α i1 α i0 + α i1</formula><p>where</p><formula xml:id="formula_38">α i0 = exp E q * (θ,φ) log [1 -p i (β, φ)] -E q * (σ 2 ) 1 2σ 2 [(y i -m 0 ) 2 + s 2 0 ] .</formula><p>and</p><formula xml:id="formula_39">α i1 = exp E q * (θ,φ) log [p i (β, φ)] -E q * (σ 2 ) 1 2σ 2 [(y i -m 1 ) 2 + s 2 1 ] .</formula><p>4.2.2. Variational densities q * (µ 0 ) and q * (µ 1 ) For k = 0, 1, the variational distribution of µ k is Gaussian with mean m k and variance s 2 k given by</p><formula xml:id="formula_40">m 0 = s 2 0 E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i )y i , s 2 0 = 1 E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i ) + 1/τ 2 0 and m 1 = s 2 1 E q * (σ 2 ) [1/σ 2 ] n i=1 α i y i , s 2 1 = 1 E σ 2 [1/σ 2 ] n i=1 α i + 1/τ 2 1 ,</formula><p>where τ 2 0 and τ 2 1 are the parameters from the prior distribution.</p><formula xml:id="formula_41">4.2.3. Variational density q * (σ 2 )</formula><p>The variational density of σ 2 , considering the likelihood and the prior distribution of σ 2 ∼ IG(a 0 , b 0 ), is given by an inverse gamma with parameters A 0 = a 0 + n/2 and</p><formula xml:id="formula_42">B 0 = b 0 + n i=1 α i 2 (y i -m 1 ) 2 + s 2 1 + n i=1 (1 -α i ) 2 (y i -m 0 ) 2 + s 2 0 . 4.2.4. Variational density q * (β l )</formula><p>We consider the full conditional posterior density of β l , σ as Equation ( <ref type="formula" target="#formula_7">6</ref>) of <ref type="bibr" target="#b15">Gelman et al. (2008)</ref> and derive the variational density of β l and σ as</p><formula xml:id="formula_43">q * (β l , σ) ∝ - 1 2 E q * (γ) (ψ l -x β l ) Σ -1 ψ (ψ l -x β l ) × - 1 2 (β l -µ β ) Σ -1 β (β l -µ l ) + j log(σ j ) -p(σ j |ν j , s j )    (<label>19</label></formula><formula xml:id="formula_44">)</formula><p>where µ l and Σ β are parameters of the prior t distribution of β l in θ or φ j , j = 1, . . . , J, Σ ψ = W -1 l as in ( <ref type="formula">9</ref>), and elements of ψ l are given in (8). The expectation in ( <ref type="formula" target="#formula_43">19</ref>) is taken with respect to ψ il and is derived from</p><formula xml:id="formula_45">E q * (γi) (ψ il ) = g(p ilk ) + (α i -pilk )g (p ilk )<label>(20)</label></formula><p>with</p><formula xml:id="formula_46">α i = E q * (γi) (γ i ) given in Section 4.2.1.</formula><p>There is no closed form for q * (β l , σ) and therefore we can not compute the ELBO. However, when σ is known, the prior distribution of β l becomes a normal distribution. In that case, the variational q * (β l , σ) = q * (β l ) is given by</p><formula xml:id="formula_47">q * (β l ) ∝ - 1 2 E q * (γ) n i=1 (ψ il -x i β l ) Σ -1 ψ (ψ il -x i β l ) × - 1 2 (β l -µ β ) Σ -1 β (β l -µ β ) (21)</formula><p>Rearranging terms in ( <ref type="formula">21</ref>), q * (β l ) is given by a normal distribution with covariance matrix</p><formula xml:id="formula_48">V * β = (x Σ -1 ψ x + Σ -1 β ) -1 and mean µ * β = V * β (x Σ -1 ψ E q * (γ) (ψ l ) + Σ -1 β µ β ), with elements in E q * (γ) (ψ l )</formula><p>given by (20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Calculating the ELBO</head><p>The ELBO is given by</p><formula xml:id="formula_49">ELBO(κ) = n i=1 E q * [log p(y i |γ i , µ 0 , µ 1 , σ 2 , β)] + n i=1 E q * [log p(γ i )] +E q * [log p(µ 0 )] + E q * [log p(µ 1 )] + E q * [log p(σ 2 )] + E q * [log p(β)] - n i=1 (E q * [log q * (γ i )] -E q * [log q * (µ 0 )] -E q * [log q * (µ 1 )] -E q * [log q * (σ 2 )] -E q * [log q * (β)] = E 0 + E 1 + E 2 + E 3 + E 4 + E 5 -F 1 -F 2 -F 3 -F 4 -F 5</formula><p>with the expectation is taken with respect to q * (µ 0 , µ 1 , σ 2 , γ, β|κ), that is</p><formula xml:id="formula_50">E q * := E q * (µ0,µ1,σ 2 ,γ,β|κ) . Recall that q * (µ 0 , µ 1 , σ 2 , γ, β) = q * (µ 0 )q * (µ 1 )q * (σ 2 )q * (γ)q * (β). Let Ψ(•) denote the digamma function. Explicit computations are given in Appendix A.4.</formula><p>Therefore, to compute the ELBO, we need the following pieces:</p><formula xml:id="formula_51">E 0 = - n 2 log 2π - 1 2 A 0 B 0 n i=1 {α i [(y i -m 1 ) 2 + s 2 1 ] + (1 -α i )[(y i -m 0 ) 2 + s 2 0 ]} + n 2 (log(B 0 ) -Ψ(A 0 )) , E 1 = n i=1 α i log g -1 (x i β)q * (β) dβ] + (1 -α i ) log[1 -g -1 (x i β)]q * (β) dβ , E 2 = - 1 2 log(2πτ 2 0 ) - 1 2τ 2 0 (m 2 0 + s 2 0 ), E 3 = - 1 2 log(2πτ 2 1 ) - 1 2τ 2 1 (m 2 1 + s 2 1 ), E 4 =a 0 log b 0 -log(Γ(a 0 )) + (a 0 + 1) log (B 0 -Ψ(a 0 + n/2)) -b 0 A 0 B -1 0 ,<label>and</label></formula><formula xml:id="formula_52">E 5 = - R 2 log 2π - 1 2 log |Σ β | - 1 2 tr(Σ -1 β Σ q * (β) ) - 1 2 [(µ * -µ β ) Σ -1 β (µ * -µ β )]</formula><p>where R is the dimension of the β vector and q * (β) given by ( <ref type="formula">21</ref>). The high dimensional integral in E 1 can be computed efficiently transforming it into a one-dimensional integral as described in Appendix A.5.</p><p>On the other hand,</p><formula xml:id="formula_53">F 1 = n i=1 (α i log α i + (1 -α i ) log(1 -α i )) , F 2 = - 1 2 log 2π - 1 2 - 1 2 log s 2 0 1/2s 2 0 , F 3 = - 1 2 log 2π - 1 2 - 1 2 log s 2 1 (1/2s 2 1 ), F 4 =A 0 log B 0 -log Γ(A 0 ) + (A 0 + 1) (log(B 0 ) -Ψ(A 0 )) -A 0 ,<label>and</label></formula><formula xml:id="formula_54">F 5 = - R 2 log 2π - 1 2 log |Σ q * (β) | - R 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Zero Inflated mixture of Poisson regression model</head><p>In this section we will analyse the case where the observed sample is given by Y 1 , Y 2 , . . . , Y n , independent non-negative integer-valued random variables, such that</p><formula xml:id="formula_55">P (Y i = y|λ 1 , λ 2 , γ i ) = [I(y i = 0)] γi0 1 y i ! e -λ1 λ yi 1 γi1 1 y i ! e -λ2 λ yi 2 γi2 (<label>22</label></formula><formula xml:id="formula_56">)</formula><p>for</p><formula xml:id="formula_57">y i = 0, 1, . . . where γ i = (γ i0 , γ i1 , γ i2 ) are latent multinomial random variables Multinomial(1, p i0 , p i1 , p i2 ) with p i0 = 1 1 + exp(x i β 1 ) + exp(x i β 2 ) , p il = exp(x i β l ) 1 + exp(x i β 1 ) + exp(x i β 2 ) , l = 1, 2.</formula><p>Therefore, the vector of unknowns is ν</p><formula xml:id="formula_58">= (λ 1 , λ 2 , β 1 , β 2 , γ 1 , γ 2 ) = (Θ, γ).</formula><p>For the model parameters we propose using the following priors:</p><formula xml:id="formula_59">• λ 1 ∼ Gamma(a 1 , b 1 ), λ 2 ∼ Gamma(a 2 , b 2 );</formula><p>• β l =(θ l , φ 1l , . . . , φ Jl ) will have weakly informative t family of prior distributions as stated in Section 4.1.4.</p><p>The observed data likelihood for the hierarchical model is difficult to optimize directly because the unobserved vector γ = {γ il , i = 1, . . . , n, l = 0, 1, 2}. Therefore, we consider the complete likelihood given by</p><formula xml:id="formula_60">f (y, γ|Θ) = n i=1 [p i0 I(y i = 0)] γi0 1 y i ! p i1 e -λ1 λ yi 1 γi1 1 y i ! p i2 e λ2 λ yi 2 γi2 . (<label>23</label></formula><formula xml:id="formula_61">)</formula><p>The joint augmented posterior distribution is the product of the likelihood and priors specified above and has no closed form. We adapt the Gibbs sampling algorithm to sample from the posterior distribution of Θ and the latent variables γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Full conditional posterior distributions</head><p>The posterior distribution of parameters is obtained based on a Gibbs sampling scheme. For that, we calculate the full conditional posterior distribution of parameters in Θ, similarly to described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Full Conditional posterior distribution of γ i</head><p>Let γ -i0 , γ -i1 and γ -i2 be the vector γ l = (γ 1l , . . . , γ nl ) without observation γ il , l = 0, 1, 2, respectively. The full conditional posterior distribution of γ i for l = 1, 2 is given by</p><formula xml:id="formula_62">P (γ il = 1|Θ, y, γ -il ) = e -λl λ yi l exp(x i β l ) I(y i = 0) + e -λ1 λ yi 1 exp(x i β 1 ) + e -λ2 λ yi 2 exp(x i β 2 ) ,<label>(24)</label></formula><p>for l = 1, 2 and</p><formula xml:id="formula_63">P (γ i0 = 1|Θ, y, γ -i0 ) = I(y i = 0) I(y i = 0) + e -λ1 λ yi 1 exp(x i β 1 ) + e -λ2 λ yi 2 exp(x i β 2 )</formula><p>. ( <ref type="formula">25</ref>)</p><p>5.1.2. Full Conditional posterior distribution of λ 1 and λ 2 We update λ l , l = 1, 2 using a gamma distribution with parameters</p><formula xml:id="formula_64">a l + n i=1 y i γ il and b l + n i=1 γ il , l = 1, 2. 5.1.3. Full Conditional posterior distribution of β l = (θ l , φ 1l , . . . , φ Jl ), l = 1, 2</formula><p>These computations are exactly the same as the ones described in Section 4.1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Variational Bayes of the ZIMP model</head><p>Analogously to the normal case, we define the variational densities as q * (ν|κ) = q * (λ 1 |κ)q * (λ 2 |κ)q(β 1 |κ)q(β 2 |κ)q(γ|κ)</p><formula xml:id="formula_65">where κ = (α, ψ 1 , ζ 1 , ψ 2 , ζ 2 , µ * β 1 , µ * β 2 , V β 1 , V β 2</formula><p>) is the vector of variational parameters.</p><p>For all the cases, the variational densities q * (β 1 |κ) and q * (β 2 |κ) will have exactly the same computations as in the normal case, see Section 4.2.4. The vector of unknowns is</p><formula xml:id="formula_66">ν = (λ 1 , λ 2 , β 1 , β 2 , γ) = (Θ, γ), where γ = (γ 1 , . . . , γ n ).</formula><p>Again, to simplify the notation, we will omit the dependence on κ when writing the variational distributions q * . The details of computations can be found in Appendix B.2. 5.2.1. Variational density q * (γ i ) If we consider q * (λ 1 ) and q * (λ 2 ) belonging to the gamma family of distributions with parameters (ψ 1 , ζ 1 ) and (ψ 2 , ζ 2 ) respectively, we get γ i is a multinomial random variable Multinomial(1, α i0 , α i1 , α i2 ) with</p><formula xml:id="formula_67">α il = ρ il 2 j=0 ρ ij , l = 0, 1, 2</formula><p>where</p><formula xml:id="formula_68">ρ i0 = I(y i = 0), ρ i1 = exp - ψ1 ζ1 + y i (-log(ζ1) + Ψ(ψ1)) + E q * (β 1 ) [x i β 1 ,<label>and</label></formula><formula xml:id="formula_69">ρ i2 = exp - ψ1 ζ1 + y i (-log(ζ2) + Ψ(ψ2)) + E q * (β 2 ) [x i β 2 .</formula><p>5.2.2. Variational density q * (λ 1 ) and q * (λ 2 )</p><p>The variational distribution of λ l , l = 1, 2 is gamma density with parameters</p><formula xml:id="formula_70">ψ 1 := a 1 + n i=1 α i1 y i and ζ 1 := b 1 + n i=1 α i1<label>(26)</label></formula><p>and</p><formula xml:id="formula_71">ψ 2 := a 2 + n i=1 α i2 y i and ζ 2 := b 2 + n i=1 α i2 ,<label>(27)</label></formula><p>respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Calculating the ELBO</head><p>The ELBO is given by</p><formula xml:id="formula_72">ELBO(κ) = n i=1 E q * [log p(y i |γ i , λ 1 , λ 2 )] + n i=1 E q * [log p(γ i )] + E q * [log p(λ 1 )] + E q * [log p(λ 2 )] + E q * [log p(β 1 )] + E q * [log p(β 2 )] - n i=1 (E q * [log q * (γ i )] -E q * [log q * (λ 1 )] -E q * [log q * (λ 2 )] -E q * [log q * (β 1 )] -E q * [log q * (β 2 )] = E 0 + E 1 + E 2 + E 3 + E 4 + E 5 -F 1 -F 2 -F 3 -F 4 -F 5 ,<label>(28)</label></formula><p>with the expectation taken with respect to q * (λ 1 , λ 2 , γ, β|κ), that is E q * := E q * (λ1,λ2,γ,β|κ) . Therefore, as shown in Appendix B.3 we have</p><formula xml:id="formula_73">E 0 = n i=1 α i0 I(y i = 0) + (-log y i !) + α i1 - ψ 1 ζ 1 + y i (-log(ζ 1 ) + Ψ(ψ 1 )) + α i2 - ψ 2 ζ 2 + y i (-log(ζ 2 ) + Ψ(ψ 2 )) E 1 = n i=1 α i0 I(y i = 0) + α i1 x i µ * β 1 + α i2 x i µ * β 2 -log[1 + exp(x i β 1 ) + exp(x i β 2 ))]q * (β 1 )q * (β 2 ) dβ 1 dβ 2 E 2 = -log(Γ(a 1 )) + a 1 log(b 1 ) + (a 1 -1)(-log(ζ 1 ) + Ψ(ψ 1 )) -b 1 ψ 1 ζ 1 E 3 = -log(Γ(a 2 )) + a 2 log(b 2 ) + (a 2 -1)(-log(ζ 2 ) + Ψ(ψ 2 )) -b 2 ψ 2 ζ 2 , E 4 = - R 2 log 2π - 1 2 log |Σ β 1 | -(1/2)[(µ * β 1 -µ β 1 ) Σ -1 β 1 (µ * β 1 -µ β 1 )] E 5 = - R 2 log 2π - 1 2 log |Σ β 2 | -(1/2)[(µ * β 2 -µ β 2 ) Σ -1 β 2 (µ * β 2 -µ β 2 )]</formula><p>where ψ 1 , ζ 1 , ψ 2 and ζ 2 are given by ( <ref type="formula" target="#formula_70">26</ref>) and ( <ref type="formula" target="#formula_71">27</ref>) and Ψ is the digamma function and R is the dimension of the β 1 (and β 2 ) vector. The high dimensional integral in E 1 can be computed efficiently transforming it into a two-dimensional integral as described in Appendix B.4. On the other hand,</p><formula xml:id="formula_74">F 1 = n i=1 α i0 log α i0 I(y i = 0) + α i1 log α i1 + α i2 log α i2 , F 2 = -log Γ(ψ 1 ) + ψ 1 log(ζ 1 ) + (ψ 1 -1)(-log(ζ 1 ) + Ψ(ψ 1 )) -ψ 1 /ζ 1 , F 3 = -log Γ(ψ 2 ) + ψ 2 log(ζ 2 ) + (ψ 2 -1)(-log(ζ 2 ) + Ψ(ψ 2 )) -ψ 2 /ζ 2 , F 4 = - R 2 log 2π - 1 2 log |Σ q * (β 1 ) | - R 2 , F 5 = - R 2 log 2π - 1 2 log |Σ q * (β 2 ) | - R 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Simulations</head><p>In this section, the primary goal is to examine the performance of the proposed model considering aspects of sample size and discrimination ability of the functional curves by considering the normal mixture model ( <ref type="formula">16</ref>) and the ZIMP model ( <ref type="formula" target="#formula_55">22</ref>), and compare those aspects under the MCMC and VB estimation methods. We consider two simulation studies, given in Sections 6.1 and 6.2, respectively, each of which consists of three steps. In the first step, for each subject i, we generated the functional covariates X i1 with domain T and X i2 with domain in S respectively, and using linear weights w 1 (t), w 2 (t), w 1 (t), w 2 (t), along with a logit model with log(p il /p i1 ), we calculated</p><formula xml:id="formula_75">p i1 = exp t∈T w 1 (t)X i,1 (t) + s∈S w 2 (s)X i,2 (s) 1 + exp t∈T w 1 (t)X i,1 (t) + s∈S w 2 (s)X i,2 (s)<label>(29)</label></formula><p>and</p><formula xml:id="formula_76">p i2 = exp t∈T w 1 (t)X i,1 (t) + s∈S w 2 (s)X i,2 (s) 1 + exp t∈T w 1 (t)X i,1 (t) + s∈S w 2 (s)X i,2<label>(s)</label></formula><p>. ( <ref type="formula">30</ref>)</p><p>Further, we sampled independent γ i ∼ Ber(p i1 ), p i1 = 1 -p i2 to generate the response variable Y i from a normal mixture model as</p><formula xml:id="formula_77">Y i = 9γ i1 + i . (<label>31</label></formula><formula xml:id="formula_78">)</formula><p>To sample from the ZIMP model ( <ref type="formula" target="#formula_55">22</ref>) we sampled independent γ i ∼ multinomial(p i0 , p i1 , p i2 ), with p i0 = 1 -p i1 -p i2 and, given the generated γ i , we generated the response variable Z i as</p><formula xml:id="formula_79">Z i = δ(0)γ i0 + Poisson(λ 1 )γ i1 + Poisson(λ 2 )γ i2<label>(32)</label></formula><p>where δ(0) represents a distribution with point mass at zero. Additionally, 0 &lt; λ 1 &lt; λ 2 . Generated datasets were analysed using the MCMC and VB inference approaches. Later on, we fit models described in Sections 4 and 5 to the generated data described in ( <ref type="formula" target="#formula_77">31</ref>) and (32), respectively. For the MCMC we considered 15,000 iterations with a burn-in of 10,000 and we collected samples every 100th step. Posterior mean parameters are considered as estimates in the MCMC method, while in the VB method posterior mean is replaced by the expectation under the variational distribution with the fitted parameters in place of the posterior.</p><p>To compare the MCMC and VB methods within each class of model, we computed the mean squared error</p><formula xml:id="formula_80">MSE = 1 M n 100 M =1 n i=1 1 L L l=1 (p il -pil ) 2</formula><p>with pil being the estimate of p il under MCMC or VB approach.</p><p>To assess the ability of classification of the mixture normal model, we consider the misclassification rate at the p-percentile</p><formula xml:id="formula_81">MR p = 1 M 100 M =1 1 n n i=1 [I(γ i2 = 1, γip = 0) + I(γ i2 = 0, γip = 1)] ,</formula><p>where γip = I(π i2 &gt; p), p = 0.5, 0.75, 0.9. For the ZIMP model, we weighted the misclassification between the pure zero class and class 3 (largest mean value) twice as large as the misclassification between the pure zero class and class 2). Let γil = I(π il = max πi ) and define L i = 2 =0 I(γ i = 1) and Li = 2 =0 I(γ i = 1) as the true and estimated class labels for subject i. The misclassification rate is computed as</p><formula xml:id="formula_82">MR = 1 M 100 M =1 1 n n i=1 |L i -Li |.</formula><p>The secondary goal of the simulation study was to assess the time performance of the MCMC and VB methods considered in Study 1 and Study 2, described in Sections 6.1 and 6.2, respectively. Simulations were run in laptop using a Intel(R) Core(TM) processor with 8192MB RAM memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Study 1: segregating functional covariates</head><p>In this study, the goal is to assess the ability of cluster discrimination when using segregating functional covariates. We generated 100 datasets {D 500 r , r = 1, . . . , 100} each consisting of n = 500 subjects. From this baseline data, set we constructed subsets of 100 and 300 subjects {(D 100 r , D 300 r ), r = 1, . . . , 100}, sampled by random, in such way that D 100 r ⊂ D 300 r ⊂ D 500 r , for r = 1, . . . , 100. For the baseline dataset, for each subject i = 1, . . . , 500, we simulated the functional covariates X i,1 and X i,2 in a manner such that subjects cluster segregation was mainly due to the segregating characteristic of the curves, as it can be seen on the top panels of Figure <ref type="figure" target="#fig_0">2</ref>. Notice that the shape of the ten sampled subject's functional covariates discriminates subjects between the two classes. It may happens a subject is eventually misclassified, as it is the case of the red curves among the black curves on the top panels of Figure <ref type="figure" target="#fig_0">2</ref>, but it was taken care to be rare event in this Study 1.</p><p>We considered T = [16, 60] and S = [0, 1] and the time points {t j ∈ [16, 60], j = 1, 2, . . . , 45} and {s j ∈ [0, 1], j = 1, 2, . . . , 30}.</p><p>The functional predictors X i,1 and X i,2 along with functional weights w 1 (t), w 1 (t) = 0.5w 1 (t), w 2 (t) and w 2 (s) = -0.25w 2 (s) were used to model mixture probabilities as described in ( <ref type="formula" target="#formula_75">29</ref>) and (30). We simulated from the normal mixture model considering (31) with i ∼ iid N (0, 18), leading to a mixture of a normal distribution with mean µ 0 = 0 and another with mean µ 1 = 9, both having variance σ 2 = 18. Further, we simulated from the ZIMP model considering (32) with λ 1 = 2 and λ 2 = 10.</p><p>For all datasets, we ran the MCMC for 15,000 iterations with a burn-in of 10,000, sampled every 100th step. Using MCMC samples of normal mixed model location parameters, we calculated the average of lower and upper bound of 95% high density posterior (HPD) intervals across the 100 simulated datasets and results are given in Table 1 along with the 2.5% and 95% quantiles calculated from the VB results across the same 100 data, for the same parameters. Results are given according to sample size n. Overall, the intervals indicate a good performance as they include their respective true parameter value. Exceptions are the intervals of µ 0 and µ 1 for n = 300 and n = 500, as the respective MCMC's intervals does not include the true parameter value and neither that of n = 500 for the VB method.</p><p>The MSE results in Table <ref type="table" target="#tab_2">2</ref> show a slight advantage of VB over the MCMC method as the MSE is at most one point smaller in the hundredths place. The MR p for p = 0.75  and p = 0.9 point to this direction also. These results follow for n = 100, 300 and 500. For MR .5 , VB show greater improvement over MCMC for n = 100, 300, 500, with MCMC results being about 1.5 larger than those of VB method. The MSE and MR p results in Table <ref type="table" target="#tab_3">3</ref> show similar results for all sample sizes and methods, with small variation due to method and sample size, with a slight advantage of VB over the MCMC method as the MSE is at most one point smaller in the hundredths place. Elapsed time for obtaining results from each method is given in Table <ref type="table" target="#tab_4">4</ref>. In Study 1, the computational time of the VB method was substantially smaller than the MCMC method for fitting normal mixture and ZIMP models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Study 2: non segregating functional covariates</head><p>In this study the goal is to assess the ability of cluster discrimination when using non segregating functional covariates. Inspired by the example in <ref type="bibr" target="#b34">Mousavi and Sørensen (2018)</ref>, our second scenario was constructed as follows: for the first step, we considered T = [0, 10] and S = [0, 1] and generated 150 functional predictors X i1 and X i2 , i = 1, . . . , 150 using basis expansions on the form</p><formula xml:id="formula_83">X i1 (t) = 13 k=1 C 1ik B (1) k (t) X i2 (s) = 13 k=1 C 2ik B (2) k (s)</formula><p>for i = 1, . . . 150, where B (1) and B (2) are cubic B-splines corresponding to nine equally spaced knots over the intervals [0, 10] and [0, 1] respectively. The coefficients C 1ik and C 2ik are the elements of the matrices C 1 and C 2 which are 13 × 13 matrices simulated as</p><formula xml:id="formula_84">C 1 = Z 1 U 1 and C 2 = Z 2 U 2</formula><p>where Z 1 is a 150 × 13 matrix formed by iid N (0.1, 1) random variables, Z 2 is a 150 × 13 matrix of iid N (0, 1) random variables and U 1 and U 2 are 13 × 13 matrices of iid [0,1]uniform random variables. For each subject, the functional covariates were sampled at 256 equally spaced time points {t j ∈ T, j = 1, 2, . . . , 256} and {s j ∈ S, j = 1, 2, . . . , 256}. We show, in the top panels of Figure <ref type="figure">3</ref>, a sample of 10 curves for X i1 and X i2 . In Study 1, the separation of covariates was clear, but in this Study 2, discriminating the sub-populations is not so clear, see plot on curves X i1 and X i2 on the top panels of Figure <ref type="figure">3</ref>. Fig. <ref type="figure">3</ref>: Functional covariates X i,1 and X i,2 , i = 1, . . . , 10 and weight functions w 1 (•) and w 2 (•), for Scenario 2. Red (traced) lines correspond to γ i = 1, black (solid) lines correspond to γ i = 0.</p><p>Further, we generated 50 datasets with weight functions given by w 1 (t) = -φ(t; 2, 0.5) + φ(t, 7.5, .5) w 2 (s) = 2 sin(10sπ/3) where φ(•; µ, σ) is the normal density with mean µ and standard deviation σ, shown in the bottom panels of Figure <ref type="figure">3</ref>. To calculate the mixture probabilities in ( <ref type="formula" target="#formula_75">29</ref>) and ( <ref type="formula">30</ref>) we considered the functional covariates X i1 (t) and X i2 (s) along with their respective functional weights w 1 (t) and w 2 (s). We then sampled data from mixture normal model in (31) with i ∼ iid N (0, 18), leading to a mixture of a normal distribution with mean µ 0 = 0 and another with mean µ 1 = 9, both variances equal to σ 2 = 18, and the ZIMP model in (32) with λ 1 = 2 and λ 2 = 8.5.</p><p>For all datasets, we ran the MCMC for 15,000 iterations with a burn-in of 10,000, sampled every 100th step. Using MCMC samples of normal mixed model location parameters, the average of lower and upper bound of 95% high density posterior (HPD) intervals across the 100 simulated datasets are given by (-1.35; 1.28) and (7.76; 9.81) for µ 0 and µ 1 , respectively. Considering the VB method for the same datasets we obtain the 2.5% and 95% quantiles across the 100 data sets for µ 0 and µ 1 as (-0.94; 3.36) and (5.31; 9.66), respectively. Results show the true location parameter µ 0 = 0 and µ 1 = 8.5 lie within the lower and upper limits of the respective intervals.</p><p>Although the MSE for the normal mixture model fitted with MCMC is smaller than the VB method as shown, in Table <ref type="table" target="#tab_5">5</ref>, misclassification rate for p = 0.5, 0.75 and 0.9 are smaller under VB method. For the ZIMP model fitted with the MCMC method, the mean of lower and upper bound of 95% high density posterior (HPD) intervals for λ 1 and λ 2 across the 100 datasets are given by (1.62; 2.65) and (7.32; 9.87). Using the VB method for the same datasets we obtain the 2.5% and 95% quantiles for λ 1 and λ 2 as (1.67; 2.41) and (7.5; 9.36), respectively. Results show the true location parameter λ 1 = 2 and λ 2 = 8.5 lie within the lower and upper limits of the respective intervals.</p><p>MCMC and the VB methods produced close MSE and misclassification rate for the ZIMP model -one point of advantage in the hundredths place to MCMC -as shown in Table <ref type="table">6</ref>.</p><p>As shown in Table <ref type="table">7</ref>, the computational time consuming of the VB method is substantially smaller than the MCMC method for normal mixture and ZIMP models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Applications</head><p>7.1. Identification of early responders using EEG data. Placebo responders are those patients whose response is termed "non-specific", e.g., in a drug trial, an improvement in symptoms that is not due to the effect of the active chemicals in the drug. There is an intense debate about how to identify placebo-responders in clinical trials of medications, in particular, for major depressive disorder (MDD) <ref type="bibr" target="#b56">(Walsh et al., 2002)</ref> since, there could be placebo responders among either the control or the treatment group. Furthermore, it is known that there is a high rate of placebo responders among patients in MDD treatment trials and in some experiments with selective serotonin reuptake inhibitors (SSRIs) it was found that some patients can have a better response using placebo <ref type="bibr" target="#b18">(Gueorguieva et al., 2011)</ref>. Identifying such patients using covariates would be an important tool in clinical research. Scalar covariates such as sex and disease severity are typically included in the modeling. On the other hand, there are several studies relating differences in neural processing between placebo and active treatments (see for example, <ref type="bibr" target="#b29">Leuchter et al. (2002)</ref>, <ref type="bibr" target="#b57">Watson et al. (2007)</ref>, <ref type="bibr" target="#b59">Zhang and Luo (2009)</ref>, <ref type="bibr" target="#b55">Wager and Atlas (2015)</ref>, <ref type="bibr" target="#b6">Ciarleglio et al. (2018)</ref>, and references therein). One way of measuring neural processing is through Electroencephalography (EEG). It is a fast, inexpensive and non-invasive procedure that has been used for decades for recording brain activity. One disease for which this is a particularly crucial problem is major depressive disorder (MDD). Recent studies have suggested that less than 40% of MDD patients achieve remission after completing a lengthy course of first-line treatment <ref type="bibr">(McGrath et al., 2013)</ref>. Such a low remission rate may be greatly improved if clinicians are better able to identify patient characteristics that define subgroups of patients who will benefit mostly from a given treatment. Furthermore, placebo response rates can be high in MDD treatment trials and analyses of results from previous trials that have compared placebo to active medications, including a class of commonly used antidepressants know as selective serotonin reuptake inhibitors (SSRIs), have found that some subjects worsen with an antidepressant, i.e., would fare better on placebo <ref type="bibr" target="#b18">(Gueorguieva et al., 2011)</ref>. <ref type="bibr" target="#b24">Jiang et al. (2017)</ref> analysed data from a randomized placebo controlled depression clinical trial of sertraline in order to identify early responders to treatment (which is indicative of a placebo response since it is believed that response to the active treatment is not immediate). The dataset consists of 96 MDD patients, randomized to either a drug or placebo treatment. For each subject, several scalar and categorical covariates are available, as well as their resting state electroencephalography (EEG) under a closed eyes condition. This EEG data contains the current source density amplitude spectrum values (V/m2) <ref type="bibr" target="#b35">(Nunez et al., 2006)</ref>  For EEG location maps see Figure <ref type="figure" target="#fig_5">7</ref> in <ref type="bibr" target="#b48">Rupasov et al. (2012)</ref>. Let y i denote the change in the HAM-D (baseline -week 1) for subject i, i = 1, . . . , 96, where a positive change indicates diminished depression symptom severity. In order to compare our results with Jiang et al. ( <ref type="formula">2017</ref>), we will focus on the same scalar covariates, sex and chronicity, and functional covariates given by data taken from 14 EEG electrodes. Figure <ref type="figure" target="#fig_2">4</ref> shows histograms of the change in HAM-D (baseline -week 1) showing the amount of improvement in depression symptoms after 1 week, a positive change indicates improvement in symptoms. Notice that there is a strong indication of a mixture of two distributions. To explore the data set, we fit a parametric model using the EM algorithm for a mixture of Gaussian distributions (different means and different variances) with no covariates, and the two fitted Gaussian curves are shown in the left panel of Figure <ref type="figure" target="#fig_2">4</ref>, with more than 40% of the subjects are classified as early responders (green curve). That is, this model has low power to discriminate the subjects into two classes. <ref type="bibr" target="#b24">Jiang et al. (2017)</ref> analyzed this dataset using the same hierarchical model given by ( <ref type="formula">16</ref>). The unobserved binary subgroup indicators are modeled via a hierarchical probit model as a function of the baseline EEG measurements and other scalar covariates of interest. In their work, instead of the regression component given by ( <ref type="formula" target="#formula_27">17</ref>), they propose to use the EEG data in the form of a (14 x 45) matrix-valued covariate. However, instead of focusing on estimating the coefficients for the entire matrix, they assume a low-dimensional structure through CP decomposition <ref type="bibr" target="#b27">(Kolda and Bader, 2009)</ref>, reducing the matrix dimension to 9 × 4. One disadvantage of this approach is that it does not take advantage of the functional nature of the data and it lacks direct interpretability for the estimated parameters. Also, the analysis uses a tensor product and so the results will likely depend on how the electrodes are ordered in the matrix.</p><p>Just to get some idea on the behavior of the functional covariates, Figure <ref type="figure">5</ref> presents data taken from 14 EEG electrodes for 9 subjects. Each panel represents a subject and each curve is the EEG for different electrodes. The colors are consistent across all plots. As can be seen, there is a large variability in the range of values for each subject. Therefore, we used as functional covariates the EEG signal for each subject that has been standardized to have zero mean and unit standard deviation. This standardizing will allow us to use the same number of basis functions and same knot location for all functions B (X,j) in ( <ref type="formula" target="#formula_7">6</ref>). To sample from the posterior distribution, we use the Gibbs sampler scheme considering the posterior calculation described in Section 3.2. Table <ref type="table" target="#tab_8">8</ref> presents the posterior estimates of the scalar parameters using logit link function and t, Cauchy and normal priors for the parameters in (θ, φ 1 , . . . , φ J ) and linear vs. non-linear functional models. The value p in each model represents p = 1 96</p><formula xml:id="formula_85">96 i=1 I(p i2 &gt; 1/2).</formula><p>Notice that all results in Table <ref type="table" target="#tab_8">8</ref> are very similar. To choose the best model, we used the posterior predictive checks (see Table <ref type="table" target="#tab_9">9</ref> for the proportion of the sample above a threshold t), h(y) = (1/96)</p><formula xml:id="formula_86">96 i=1 I(y i &gt; t),</formula><p>for t = -5, 0, 5 and 10.</p><p>Although the non-linear model is slightly better than the linear model for t = -5 and t = 10 as shown in ). The effect of sex and chronicity are significant since the intercept (male and low chronicity) has mean value -1.07 (HPD 95% Credible interval: [-1.81;-0.48]), the added effect of being female has mean -1.85 (HPD 95% Credible interval: [-2.73;-1.04]) whereas the added effect of high chronicity has mean -1.79 (HPD 95% Credible interval: <ref type="bibr">[-2.68;-1.11]</ref>). The right panel of Figure <ref type="figure" target="#fig_2">4</ref> shows the fitted distribution using the mean values of the posterior parameters estimated by the linear model with Normal prior and logit link. Comparing left and right panels of Figure <ref type="figure" target="#fig_2">4</ref> and looking at Table <ref type="table" target="#tab_10">10</ref> we can see that the fitted model has a much better ability to discriminate between the two groups. The most likely number of early responders is between 10 (10.4%) and 15 (15.6%) as can be seen from the posterior distribution for 96 i=1 γ i , the total number of early responders, shown in Figure <ref type="figure" target="#fig_4">6</ref>.</p><p>For comparison between MCMC and Variational Bayes results, Table <ref type="table" target="#tab_12">12</ref> shows the expected value estimates for the parameters of the VB distributions for the linear/nonlinear model, logit/probit link and Normal prior (cf. Table <ref type="table" target="#tab_2">2</ref>). We present only the results for the Normal prior since there was very little effect of the prior on the estimate of VB parameters.</p><p>For the linear model, we estimate the weight functions w j , j = 1, . .  the effect to what extent each resting state EEG alpha and theta power in the posterior region of brain under a closed eyes condition could help identify a potential early responders sub-group (which is believed to consist of subjects susceptible to non-specific placebo effects). Figure <ref type="figure" target="#fig_5">7</ref> presents the functional boxplots for the weight functions {w j (t); j = 3, 5 and 6} which were the only ones significant to predict early respondents using the linear model with the logit link and Normal prior and which are related to the EEGs located at P 5 , P 6 and P 7 . The functional boxplots are the equivalent to usual boxplots. They are a graphical method to display five descriptive statistics: the median, the first and third quartiles, and the non-outlying minimum and maximum observations. For a nice review on the subject see <ref type="bibr" target="#b53">Sun and Genton (2011)</ref>.  these animals as cows are at greatest risks of experiencing health disorders during this period. The occurrence of health disorders affects the productivity of cows, and thus is a determining factor of the profitability of dairy herds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Predicting</head><formula xml:id="formula_87">I(E q * (α i ) &gt; 1/2).</formula><p>A potential strategy to identify cows with health disorders in early lactation for treatments and other interventions is through the use of automated monitoring of cow be-havioral, physiological, and performance parameters with automated health monitoring systems based on sensor data <ref type="bibr">(Stangaferro et al., 2016a,c,b)</ref>. This is because it has been demonstrated that multiple sensor parameters, such as for example rumination time, physical activity, resting time, body temperature, milk volume and component yield, are useful for monitoring cow health as they are dramatically altered during episodes of health disorders <ref type="bibr">(Stangaferro et al., 2016a,c,b)</ref> and thus, can be used to predict the health status of cows. Moreover, data from these sensors systems can also be combined with non-sensor data to increase the accuracy of alerts used to identify cows with health disorders. Our dataset for this application consists of data collected in order to train, validate, and test machine-learning algorithms (MLA) models created through a combination of sensor data from Automated Health Monitoring System (AHMS) and non-sensor data available at commercial dairy farms. This project funded by the USDA-NIFA was conducted by the Dairy Cattle Biology and Management Laboratory at Cornell University. We have data on daily clinical examination of cows during the first 40 Days after calving (days in milk) from 258 cows. Information on whether a cow had a health disorder (1) or not (0) was collected by the research team on a daily basis. The data consists of 41 rows (0-40) per cow with sensor and non-sensor data from the previous lactation (i.e., before calving) and after calving. We focused on four sensor parameters related to physical activity for the 30 days prior to calving. The selected parameters were: Activity (Total number of steps in a given day divided by 24), Number of Resting Bouts, Average Rest Time and Total Rest Time per day. Moreover, in this analysis, in addition to the 4 functional covariates as described above, we considered 4 scalar variables related to previous lactation period known to be associated with health outcomes after calving: Z 1 = Age of first calving, Z 2 = Previous lactation days in milk, Z 3 = Previous lactation health event (0 or 1), and Z 4 = Previous lactation number of previous health events.</p><p>The objective of this application example was to model the number of days a cow was sick in the first 40 days after calving. Our main goal was to classify cows into 2 or 3 classes of status by using the scalar and functional covariates related to activity and resting times. There was high variability in the dataset as seen in Table <ref type="table" target="#tab_3">13</ref>. During the observation period, 175 cows were not diagnosed with a health disorder whereas 83 cows with at least one health disorder event with a mean value of 1.60 sick days and a variance of 10.30. If we just consider the values not equal to zero we get a mean value of 4.96 sick days and variance 15.95. As previously noted, we have 68% of zeros in the sample (175/258) and probably a zero inflated distribution to accommodate overdispersion caused by zeros. On the other hand, if we consider only the non-zero observations, we still have a large variance as compared with the mean. Therefore, we fitted a zero inflated mixture of two Poisson distributions.</p><p>In this case, we fitted the model to 208 cows randomly selected as a training set and used the remaining 50 cows to evaluate the prediction using the model. After fitting the model, we obtained the following classification using the posterior mean for the gamma variable. Notice, from Table <ref type="table" target="#tab_13">14</ref> that the classification according to the maximum value of the estimated probability is the same using MCMC and VB estimates. From the 138 cows in the training set that did not get sick during lactation, 136 were classified as belonging to the pure-zero class with high probability while the other 2 cows were classified as Low Incidence. All 43 cows that were sick between 1 and 4 days were classified as Low Sick days 0 1 2 3 4 5 6 7 8 9 10 11 12 13 18 Total Freq 175 22 10 7 7 4 5 4 7 6 2 3 3 2 1 258</p><p>Table <ref type="table" target="#tab_3">13</ref>: Frequency of sick days for lactating cows (training + testing set)</p><p>Incidence whereas only one of the cows diagnosed with a health disorder for 5 days was classified as Low Incidence and the other one was classified as High Incidence. All cows with 6 or more days with a health disorder were classified as High incidence. The posterior mean and standard deviation and quantiles of the posterior distribution as well as the mean for the variational Bayes mean value for these classes are shown in Table <ref type="table" target="#tab_14">15</ref> and Figure <ref type="figure">8</ref>. Notice that the estimates of the mean and standard deviation of the posterior distribution for λ 1 and λ 2 are very similar using VB and MCMC. On the other hand, from Table <ref type="table" target="#tab_16">16</ref> and Figure <ref type="figure">9</ref>, we can see that the VB estimates for the mean of the intercept and scalar coefficients for variables are not so good for the coefficients that are not significant whereas they are similar for the ones which are significant. Direct comparison for the coefficients of the expansion for the weights of the functional covariates are not meaningful and we constructed the functional boxplot for the posterior estimates of the weight functions as shown in Figures <ref type="figure" target="#fig_9">12</ref> and<ref type="figure">13</ref>. Notice that apparently, none of the functional covariates are significant for the regression term for Low Incidence Class whereas only the covariates Activity and, to a lesser extent, Number of Rest Times and Average Rest Times are significant to determine the probabilities of the High Incidence latent class.</p><p>As we noted before, we have 68% of zeros in the sample (175/258), indicative of a zero inflated distribution. On the other hand, if we consider only the non-zero observations, we still have a large variance as compared with the mean. Therefore, we fitted a zero inflated mixture of two Poisson distributions. In this case, we fitted the model to 208 cows and used the remaining 50 cows to check the prediction using the model. After fitting the model, we classified cows health disease state in "Pure zero", "Low Incidence" and "High Incidence" classes using the posterior mean for the gamma variable. Results are given in Table <ref type="table" target="#tab_13">14</ref>. Notice, that the classification according to the maximum value of the estimated probability is the same using MCMC and VB estimates. From the 138 cows that did not get sick during the lactating period, 136 are classified as belonging to the "pure zero" class with high probability, whereas only 2 cows were classified as Low Incidence. All 43 cows that were sick between 1 and 4 days were classified as Low Incidence whereas only one of the cows who got sick for 5 days was classified as Low Incidence where the other one was classified as High Incidence. All cows with 6 or more sick days were classified as High incidence.</p><p>The posterior mean and standard deviation and quantiles of the posterior distribution as well as the mean for the variational Bayes mean value for these classes are shown in Table <ref type="table" target="#tab_14">15</ref> and<ref type="table" target="#tab_8">Figure 8</ref>. Notice that the estimates of the mean and standard deviation of the posterior distribution for λ 1 and λ 2 are very similar using VB and MCMC. On the other hand, from Table <ref type="table" target="#tab_16">16</ref> and Figure <ref type="figure">9</ref>, we can see that the VB estimates for the mean of the Intercept and scalar coefficients for variables are not so good for the coefficients that are not significant whereas they are similar for the ones which are significant.</p><p>Direct comparison for the coefficients of the expansion for the weights of the functional Class Number of sick days 0 1 2 3 4 5 6 7 8 9 11 12 13 18 Pure Zero 136 0 0 0 0 0 0 0 0 0 0 0 0 0 Low Incidence 2 20 10 6 5 1 0 0 0 0 0 0 0 0 High Incidence 0 0 0 0 0 1 5 4 6 5 2 3 1 1 covariates are not meaningful and we constructed the functional boxplot for the posterior estimates of the weight functions as shown in Figures <ref type="figure" target="#fig_9">12</ref> and<ref type="figure">13</ref>. Notice that apparently, none of the functional covariates are significant for the regression term for Low Incidence Class whereas only the covariates "Activity" and, to a lesser extent, "Number of Rest Times" and "Average Rest Times" are significant to determine the probabilities of the High Incidence latent class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>In this paper, we have considered a mixture model driven by latent variables. We used a semi-parametric regression model incorporating functional covariates as predictors for the latent group membership. The main features of our methodology are:</p><p>(a) the non-parametric approach of expanding the unknown functions w j and F j into B-splines basis reduces the dimension of the problem;    (f) the functional covariates do not need to be observed concurrently and they can even be different for each subject; (g) in the case of the linear model, it has the added advantage of interpretability of the weight functions which might naturally incorporate prior information that is available to experts in the field.</p><p>To show the strength of our method, and also to compare the performance of the MCMC with Variational Bayes, we ran several simulation scenarios with different link functions and prior distributions. Also, we analyzed two datasets, one comes from a placebo controlled clinical trial to investigate whether the EEG alpha and theta powers can be used to identify an early placebo responder, and the other dataset comes from the Animal Health Monitoring System (AHMS) -USDA project from the Dairy Cattle Biology and Management Laboratory at Cornell University to study the factors that affect the health of lactating cows.</p><p>of Agriculture(USDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mixture of normal distributions</head><p>A.1. Full Conditional posterior distributions A.1.1. Full Conditional posterior distribution of γ i Let γ -i be the vector γ = (γ 1 , . . . , γ i-1 , γ i+1 , . . . , γ n ). The full conditional posterior distribution of γ i is given by</p><formula xml:id="formula_88">P (γ i = 1|Θ, y, γ -i ) = P (γ i = 1, Θ, y, γ -i ) P (Θ, y, γ -i ) = P (γ i = 1, Θ, y, γ -i ) P (γ i = 1, Θ, y, γ -i ) + P (γ i = 0, Θ, y, γ -i ) = φ yi-µ1 σ g -1 z i θ + J j=1 φ j R ij φ yi-µ1 σ g -1 z i θ + J j=1 φ j R ij + φ yi-µ0 σ 1 -g -1 z i θ + J j=1 φ j R ij since P (γ i = x, Θ, y, γ -i ) = φ yi-µ1x-µ0(1-x) σ g -1 z i θ + J j=1 φ j R ij .</formula><p>Here, g(•) is link function.</p><p>A.1.2. Full Conditional posterior distribution of µ 0 and µ 1 We update µ 0 using a normal distribution with mean</p><formula xml:id="formula_89">1/σ 2 0 + n i=1 I(γ i = 0)/σ 2 -1 n i=1 y i I(γ i = 0)</formula><p>and variance</p><formula xml:id="formula_90">1/σ 2 0 + n i=1 I(γ i = 0)/σ 2 -1</formula><p>.</p><p>On the other hand, conditionally on µ 0 , µ 1 is updated with a truncated normal distribution with mean</p><formula xml:id="formula_91">1/σ 2 0 + n i=1 I(γ i = 1)/σ 2 -1 n i=1 y i I(γ i = 1)</formula><p>and variance</p><formula xml:id="formula_92">1/σ 2 0 + n i=1 I(γ i = 1)/σ 2 -1</formula><p>restricted to (µ 0 , ∞).</p><p>A.1.3. Full Conditional posterior distribution of σ 2 We update σ 2 using an inverse-gamma distribution with parameters</p><formula xml:id="formula_93">a 0 and b 0 + n i=1 (η 0 + η 1 γ i ) 2 ,</formula><p>with a 0 , b 0 , c 0 and d 0 are the parameters of the prior distributions.</p><p>A.2. Full conditional distribution for β</p><formula xml:id="formula_94">Update [δ ij |., v i , x ij ] ∼ N (M -1 (β j [µ v (-j) -v i + µ x (-j) -x ij ] + ∆ -1 µ δ ), M -1 ), with M = β j β j σ 2 x + 1 σ 2 x + ∆ -1 µ v (-j) = z i θ i + j =j β j δ i,j µ x (-j) = j =j β j δ i,j</formula><p>In detail: <ref type="formula" target="#formula_55">22</ref>), ( <ref type="formula" target="#formula_60">23</ref>) and ( <ref type="formula" target="#formula_62">24</ref>) from <ref type="bibr" target="#b4">Blei et al. (2017)</ref>)</p><formula xml:id="formula_95">[δ ij |., v i , x ij ] ∝ φ(v i ; µ v , 1) × φ(x ij ; µ x , σ 2 x ) × φ(δ ij ; µ vδ , ∆) = exp - 1 2 δ ij β j β j δ ij -2δ ij β j (µ v (-j) -v i ) +δ ij (β j β j ) σ 2 x δ ij -2δ ij β j (µ v (-j) -v i ) σ 2 x + δ ij ∆ -1 δ ij -2δ ij ∆ -1 µ δ A.3. Variational distributions A.3.1. Computing q * (γ i ) (cf. Equations (</formula><formula xml:id="formula_96">log q * (γ i ) = = C 1i + {E ν(-γi) [log p(γ i |θ, φ) + log p(y i |γ i , µ)]} = C 1i + E ν(-γi)    γ i log   g -1   z i θ + J j=1 φ j R ij     + (1 -γ i ) log   1 -g -1   z i θ + J j=1 φ j R ij     -γ i 1 2σ 2 (y i -µ 1 ) 2 -(1 -γ i ) 1 2σ 2 (y i -µ 0 ) 2 = C 1i + γ i E ν(-γi)    log   g -1   z i θ + J j=1 φ j R ij     - 1 2σ 2 (y i -µ 1 ) 2    +(1 -γ i )E ν(-γi)    log   1 -g -1   z i θ + J j=1 φ j R ij     - 1 2σ 2 (y i -µ 0 ) 2    = C 1i + γ i    E q * (θ,φ) log   g -1   z i θ + J j=1 φ j R ij     -E q * (σ 2 ) 1 2σ 2 E q * (µ1) (y i -µ 1 ) 2    +(1 -γ i )    E q * (θ,φ) log   1 -g -1   z i θ + J j=1 φ j R ij     -E q * (σ 2 ) 1 2σ 2 E q * (µ0) (y i -µ 0 ) 2    = C 1i + γ i    E q * (θ,φ) log   g -1   z i θ + J j=1 φ j R ij     -E q * (σ 2 ) 1 2σ 2 [(y i -m 1 ) 2 + s 2 1 ]    +(1 -γ i )    E q * (θ,φ) log   1 -g -1   z i θ + J j=1 φ j R ij     -E q * (σ 2 ) 1 2σ 2 [(y i -m 0 ) 2 + s 2 0 ]    where C 1i = C(ν(-γ i ), y i , κ). A.3.2. Computing q * (µ 0 |κ) and q * (µ 1 |κ) For k = 0, we have log q * (µ 0 ) = C 2,k,0 - µ 2 0 2τ 2 0 - n i=1 E q * (σ 2 ,γi) 1 2σ 2 (1 -γ i )(y i -µ 0 ) 2 = C 2,k,0 - µ 2 0 2τ 2 0 - n i=1 E q * (σ 2 ) 1 2σ 2 (1 -α i )(y 2 i -2y i µ 0 + µ 2 0 ). Similarly, for k = 1, log q * (µ 1 ) = C 2,k,1 - µ 2 1 2τ 2 1 - n i=1 E q * (σ 2 ,γi) 1 2σ 2 γ i (y i -µ 1 ) 2 = C 2,k,1 - µ 2 1 2τ 2 1 - n i=1 E q * (σ 2 ) 1 2σ 2 α i (y i -µ 0 ) 2 = C 2,k,1 - µ 2 1 2τ 2 1 - n i=1 E q * (σ 2 ) 1 2σ 2 α i (y 2 i -2y i µ 1 + µ 2 1 ) 2<label>(33)</label></formula><p>This calculation reveals that the coordinate-optimal variational density of µ k , k = 0, 1, are in the exponential family with natural parameters and sufficient statistics given in Table <ref type="table" target="#tab_8">18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural parameters</head><p>Sufficient statistics</p><formula xml:id="formula_97">k = 0 µ 0 E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i )y i µ 2 0 (1/2)(E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i ) + 1/τ 2 0 ) k = 1 µ 1 E q * (σ 2 ) [1/σ 2 ] n i=1 α i y i µ 2 1 (1/2)(E q * (σ 2 ) [1/σ 2 ] n i=1 α i + 1/τ 2 1 )</formula><p>Table <ref type="table" target="#tab_8">18</ref>: Natural parameters and sufficient statistics for distributions q * (µ 0 ) and q * (µ 1 )</p><p>That is, the distribution is Gaussian, expressed in terms of variational mean and variance, the updates for q * (µ k ) are</p><formula xml:id="formula_98">m 0 = E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i )y i E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i ) + 1/τ 2 0 , s 2 0 = 1 E q * (σ 2 ) [1/σ 2 ] n i=1 (1 -α i ) + 1/τ 2 0 and m 1 = E q * (σ 2 ) [1/σ 2 ] n i=1 α i y i E q * (σ 2 ) [1/σ 2 ] n i=1 α i + 1/τ 2 1 , s 2 1 = 1 E σ 2 [1/σ 2 ] n i=1 α i + 1/τ 2 1</formula><p>where τ 2 0 and τ 2 1 are the parameters from the prior distribution.</p><p>A.3.3. Computing q * (σ 2 |κ)</p><p>The variational density of q(σ 2 ) considers the likelihood and the prior distribution of</p><formula xml:id="formula_99">σ 2 ∼ IG(a 0 , b 0 ). Therefore, log q(σ 2 ) = C 3 + E (µ0,µ1,γ) n i=1 γ i log φ(y i ; µ 1 , σ 2 ) + (1 -γ i ) log φ(y i ; µ 0 , σ 2 ) +(a 0 + 1) log(1/σ 2 ) - b 0 σ 2 = C 4 + n 2 log(1/σ 2 ) - n i=1 E (µ1,γ) γ i 2σ 2 (y i -µ 1 ) 2 - n i=1 E (µ1,γ) (1 -γ i ) 2σ 2 (y i -µ 0 ) 2 +(a 0 + 1) log(1/σ 2 ) - b 0 σ 2 = C 4 + n 2 log(1/σ 2 ) - n i=1 α i 2σ 2 E µ1 (y i -µ 1 ) 2 - n i=1 (1 -α i ) 2σ 2 E µ0 (y i -µ 0 ) 2 +(a 0 + 1) log(1/σ 2 ) - b 0 σ 2 = C 4 + n 2 log(1/σ 2 ) - n i=1 α i 2σ 2 [(y i -m 1 ) 2 + s 2 1 ] - n i=1 (1 -α i ) 2σ 2 [(y i -m 0 ) 2 + s 2 0 ] +(a 0 + 1) log(1/σ 2 ) - b 0 σ 2<label>(34)</label></formula><p>which is an inverse gamma with parameters A 0 = a 0 + n/2 and B 0 = b 0 + B 0,q * where B</p><formula xml:id="formula_100">0,q * = n i=1 α i 2 [(y i -m 1 ) 2 + s 2 1 ] + n i=1 (1 -α i ) 2 [(y i -m 0 ) 2 + s 2 0 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Calculating the ELBO</head><p>The ELBO is given by ELBO</p><formula xml:id="formula_101">(κ) = n i=1 (E q * [log p(y i |γ i , µ 0 , µ 1 , σ 2 , β)] + E q * [log p(γ i )] + 1 k=0 E q * [log p(µ k )] + E q * [log p(σ 2 )] + E q * [log p(β)] - n i=1 (E q * [log q * (γ i )] - 1 k=0 E q * [log q * (µ k )] -E q * [log q * (σ 2 )] -E q * [log q * (β)] = E 0 + E 1 + E 2 + E 3 + E 4 + E 5 -F 1 -F 2 -F 3 -F 4 -F 5</formula><p>with the expectation is taken with respect to q * (µ 0 , µ 1 , σ 2 , γ, β|κ), that is E q * := E q * (µ0,µ1,σ 2 ,γ,β|κ) . Recall that q * (µ 0 , µ 1 , σ 2 , γ, β) = q * (µ 0 )q * (µ 1 )q * (σ 2 )q * (γ)q * (β). Let Ψ(•) denote the digamma function.</p><p>Therefore, to compute the ELBO we need to compute the following pieces:</p><formula xml:id="formula_102">E 0 = E q * (log p(y|µ 0 , µ 1 , σ 2 , γ)) = n i=1 E q * [γ i log φ(y i ; µ 1 , σ 2 ) + (1 -γ i ) log φ(y i ; µ 0 , σ 2 )] = - n 2 log 2π - 1 2 E q * (1/σ 2 ) n i=1 {α i [(y i -m 1 ) 2 + s 2 1 ] + (1 -α i )[(y i -m 0 ) 2 + s 2 0 ]} -(n/2)E q * (log σ 2 ) = - n 2 log 2π - 1 2 A 0 B 0 n i=1 {α i [(y i -m 1 ) 2 + s 2 1 ] + (1 -α i )[(y i -m 0 ) 2 + s 2 0 ]} + n 2 (log(B 0 ) -Ψ(A 0 )) .</formula><p>Since,</p><formula xml:id="formula_103">E q * (log p(γ i )) = E q * [γ i log[g -1 (x i β)] + (1 -γ i ) log[1 -g -1 (x i β)] = α i log g -1 i β)q * (β) dβ] + (1 -α i ) log[1 -g -1 (x i β)]q * (β) dβ,</formula><p>we have</p><formula xml:id="formula_104">E 1 = n i=1 α i log g -1 (x i β)q * (β) dβ] + (1 -α i ) log[1 -g -1 (x i β)]q * (β) dβ<label>(35)</label></formula><p>with q * (β) is computed in Section 4.2.4. The high dimensional integral in (35) can be computed efficiently transforming it into a one-dimensional integral as described in Appendix A.5. Also, for k = 0, 1, we have</p><formula xml:id="formula_105">E q * (log p(µ k )) = E q * (log φ(µ k ; 0, σ 2 µ )) = - 1 2 log(2πτ 2 k ) -(1/2τ 2 k )E q * (µ 2 k ) = - 1 2 log(2πτ 2 k ) - 1 2τ 2 k (m 2 k + s 2 k ).</formula><p>Then,</p><formula xml:id="formula_106">E 2 = - 1 2 log(2πτ 2 0 ) - 1 2τ 2 0 (m 2 0 + s 2 0 ) and E 3 = - 1 2 log(2πτ 2 1 ) - 1 2τ 2 1 (m 2 1 + s 2 1 ).</formula><p>Moreover,</p><formula xml:id="formula_107">E 4 = E q * (log p(σ 2 )) = E q * log b a0 0 Γ(a 0 ) (1/σ 2 ) a0+1 exp -b 0 σ 2 = a 0 log b 0 -log(Γ(a 0 )) + (a 0 + 1) log (B 0 -Ψ(a 0 + n/2)) -b 0 A 0 B -1 0 .</formula><p>Finally,</p><formula xml:id="formula_108">E 5 = E q * (log p(β)) = E q * [log φ(β; µ β , Σ β )] = - R 2 log 2π - 1 2 log |Σ β | -(1/2)E q * (β -µ β ) Σ -1 β (β -µ β ) = - R 2 log 2π - 1 2 log |Σ β | - 1 2 tr(Σ -1 β Σ q * (β) ) - 1 2 [(µ * -µ β ) Σ -1 β (µ * -µ β )]</formula><p>where R is the dimension of the β vector.</p><p>On the other hand,</p><formula xml:id="formula_109">F 1 = n i=1 E q * (log q * (γ i )) = n i=1 [α i log α i + (1 -α i ) log(1 -α i )].</formula><p>Also,</p><formula xml:id="formula_110">F 2 = E q * (log q * (µ 0 )) = E q * (log φ(µ 0 ; m 0 , s 2 0 )) = - 1 2 log 2π - 1 2 log s 2 0 - 1 2s 2 0 E q * (µ 0 -m 0 ) 2 = - 1 2 log 2π - 1 2 log s 2 0 (1/2s 2 0 ) - 1 2 = - 1 2 log 2π - 1 2 - 1 2 log s 2 0 (1/2s 2 0 ).</formula><p>Analagously,</p><formula xml:id="formula_111">F 3 = E q * (log q * (µ 1 )) = E q * (log φ(µ k ; m k , s 2 k )) = - 1 2 log 2π - 1 2 log s 2 k - 1 2s 2 k E q * (µ k -m k ) 2 = - 1 2 log 2π - 1 2 log s 2 k (1/2s 2 k ) - 1 2 = - 1 2 log 2π - 1 2 - 1 2 log s 2 1 (1/2s 2 1 )</formula><p>Moreover,</p><formula xml:id="formula_112">F 4 = E q * (log q * (σ 2 )) = E q * log B A0 0 Γ(A 0 ) (1/σ 2 ) A0+1 exp -B 0 σ 2 = E q * A 0 log B 0 -log Γ(A 0 ) + (A 0 + 1) log(1/σ 2 ) - B 0 σ 2 = A 0 log B 0 -log Γ(A 0 ) + (A 0 + 1)E q * log(1/σ 2 ) -B 0 E q * 1/σ 2 = A 0 log B 0 -log Γ(A 0 ) + (A 0 + 1) (log(B 0 ) -Ψ(A 0 )) -A 0 .</formula><p>Finally,</p><formula xml:id="formula_113">F 5 = E q * (log q * (β)) = E q * [log φ(β; µ q * (β) , Σ q * (β ))] = - R 2 log 2π - 1 2 log |Σ q * (β) | -(1/2)E q * (β) [(β -µ q * (β) ) Σ -1 q * (β) (β -µ q * (β ))] = - R 2 log 2π - 1 2 log |Σ q * (β) | - 1 2 tr(Σ -1 q * (β) Σ q * (β) ) = - R 2 log 2π - 1 2 log |Σ q * (β) | - R 2 .</formula><p>A.5. A fast way to compute E * q * (β)[log g -1 (x β)] Let x ∈ R d and F : R d → R d , we want to compute</p><formula xml:id="formula_114">E q * [F (x β)] = R d F (x β) 1 (2π) p/2 det(Σ) exp - 1 2 (β -µ) Σ -1 (β -µ) dβ.</formula><p>Construct an orthonormal matrix S such as the first column of S is x/ x and let Sζ = (β-µ). Since S is orthonormal, the Jacobian of this transformation is 1, S = S -1 , (S Σ -1 S) -1 = (S ΣS) and det(S ΣS) = det Σ. Therefore,</p><formula xml:id="formula_115">E q * [F (x β)] = = R d F (x Sζ + x µ) 1 (2π) p/2 det(Σ) exp - 1 2 ζ S Σ -1 Sζ dζ = R F ( x ζ 1 + x µ) R p-1 1 (2π) p/2 det(Σ) exp - 1 2 ζ S Σ -1 Sζ dζ p . . . dζ 2 dζ 1 = R F ( x ζ 1 + x µ) 1 (2π) 1/2 Σ (1,1) exp - 1 2 ζ 1 (S Σ -1 S) (1,1) ζ 1 dζ 1 .<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Zero Inflated mixture of Poisson distributions</head><p>B.1. Full Conditional posterior distribution of γ i Let γ -i0 , γ -i1 and γ -i2 be the vector γ j = (γ 1j , . . . , γ nj ) without observation γ ij , j = 0, 1, 2, respectively. The full conditional posterior distribution of γ i is given by P (γ i1 = 1|Θ, y, γ -i1 ) = e -λ1 λ yi 1 exp(x i β 1 ) I(y i = 0) + e -λ1 λ yi 1 exp(x i β 1 ) + e -λ2 λ yi 2 exp(x i β 2 ) , P (γ i2 = 1|Θ, y, γ -i2 ) = e -λ2 λ yi 2 exp(x i β 2 ) I(y i = 0) + e -λ1 λ yi 1 exp(x i β 1 ) + e -λ2 λ yi 2 exp(x i β 2 )</p><p>, and P (γ i0 = 1|Θ, y, γ -i0 ) = I(y i = 0) I(y i = 0) + e -λ1 λ yi 1 exp(x i β 1 ) + e -λ2 λ yi 2 exp(x i β 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Variational Bayes</head><p>Analogously to the normal case, we define the variational densities as q * (ν|κ) = q * (λ 1 |κ)q * (λ 2 |κ)q(β 1 |κ)q(β 1 |κ)q(γ|κ) and have to calculate (a) q * (λ 1 |κ) ∝ exp E q * (ν(-λ1)) log p(λ 1 |ν(-λ 2 ), y), κ (b) q * (λ 2 |κ) ∝ exp E q * (ν(-λ2)) log p(λ 2 |ν(-λ 2 ), y), κ (c) q * (β 1 |κ) ∝ exp E q * (ν(-β 1 )|κ) log p(β 1 |ν(-β 1 ), y, κ) (d) q * (β 2 |κ) ∝ exp E q * (ν(-β 2 )|κ) log p(β 2 |ν(-β 2 ), y, κ) (e) q * (γ|κ) ∝ exp E q * (ν(-γ),|κ) log p(γ|ν(-γ), y, κ)</p><p>where κ = (α,</p><formula xml:id="formula_116">ψ 1 , ζ 1 , ψ 2 , ζ 2 , µ * β 1 , µ * β 2 , V β 1 , V β 1</formula><p>) is the vector of variational parameters.</p><p>For all the cases the Variational densities q * (β 1 |κ) and q * (β 2 |κ) will have exactly the same computations as in the normal case, see Section 4.2.4.</p><p>The vector of unknowns is ν = (λ 1 , λ 2 , β 1 , β 2 , γ) = (Θ, γ), where γ = (γ 1 , . . . , γ n ).</p><p>B.2.1. Variational density q * (γ i |κ) If we consider q * (λ 1 |κ) and q * (λ 2 |κ) belonging to the gamma family of distributions with parameters (ψ 1 , ζ 1 ) and (ψ 2 , ζ 2 ) respectively, we get .</p><p>Therefore, we have E q * (ν(-γ i )) [log p(γ i |ν(-γ i ), y, κ)] = = C 1i + γ i0 I[y i = 0] + γ i1 E q * (λ1) [-λ 1 + y i log(λ 1 )] + E q * (β 1 ) [x i β 1 ] +γ i1 E q * (λ2) [-λ 2 + y i log(λ 2 )] +</p><formula xml:id="formula_117">x i E q * (β) [β 1 ] = C 1i + γ i1 - ψ 1 ζ 1 + y i (-log(ζ 1 ) + Ψ(ψ 1 )) + E q * (β 1 ) [x i β 1 ] γ i2 - ψ 2 ζ 2 + y i (-log(ζ 2 ) + Ψ(ψ 2 )) + E q * (β) [x i β 2 ].</formula><p>Therefore, γ i is a multinomial random variable Multinomial(1, α i0 , α i1 , α i2 ) where α i0 ∝ I(y i = 0),</p><formula xml:id="formula_118">α i1 ∝ exp - ψ1 ζ 1 + y i (-log(ζ 1 ) + Ψ(ψ 1 )) + E q * (β 1 ) [x i β 1 ,<label>and</label></formula><formula xml:id="formula_119">α i2 ∝ exp - ψ 1 ζ 1 + y i (-log(ζ 2 ) + Ψ(ψ 2 )) + E q * (β 2 ) [x i β 2 with α i0 + α i1 + α i2 = 1.</formula><p>B.2.2. Variational density q * (λ 1 |κ) and q * (λ 2 |κ)</p><p>For k = 1, 2 we have the prior distributions</p><formula xml:id="formula_120">p(λ k ) = 1 Γ(a k ) b ak k (λ k ) ak-1 e -bkλk .</formula><p>Also, We choose the family of gamma distributions as the variational family for q * (λ 1 |κ) and q * (λ 2 |κ). Therefore, log q * (λ 1 ) = C 2,0 + (a 1 -1) log(λ 1 ) -b 1 λ 1 + where ψ 1 , ζ 1 , ψ 2 and ζ 2 are given by ( <ref type="formula">37</ref>) and ( <ref type="formula">38</ref>) and Ψ is the digamma function.</p><formula xml:id="formula_121">E 1 := n i=1 E q * (log p(γ i )) = = n i=1</formula><p>E q * γ i0 I(y i = 0) + γ i1 x i β 1 ) + γ i2 exp(x i β 2 ) -log[1 + exp(x i β 1 ) + exp(x i β 2 )]</p><formula xml:id="formula_122">= n i=1 α i0 + α i1 x i µ * β 1 + α i2 x i µ * β 2</formula><p>-log[1 + exp(x i β 1 ) + exp(x i β 2 ))]q * (β 1 )q * (β 2 ) dβ 1 dβ 2 (39) with q * (β 1 ) and q * (β 2 ) are obtained similarly to Section 4.2.4. Also,</p><formula xml:id="formula_123">E 4 := E q * (log p(β 1 )) = = E q * [log φ(β 1 ; µ β 1 , Σ β 1 )] = - n 2 log 2π - 1 2 log |Σ β 1 | -(1/2)E q * (β 1 -µ β 1 ) Σ -1 β 1 (β 1 -µ β 1 ) = - R 2 log 2π - 1 2 log |Σ β 1 | -(1/2)[(µ * β 1 -µ β 1 ) Σ -1 β 1 (µ * β 1 -µ β 1 )] (<label>42</label></formula><formula xml:id="formula_124">)</formula><p>where R is the dimension of the β 1 vector. Similarly,</p><formula xml:id="formula_125">E 5 := E q * (log p(β 2 )) = = E q * [log φ(β; µ β , Σ β )] = - R 2 log 2π - 1 2 log |Σ β 2 | -(1/2)[(µ * β 2 -µ β 2 ) Σ -1 β 2 (µ * β 2 -µ β 2 )]<label>(43)</label></formula><p>On the other hand,</p><formula xml:id="formula_126">F 1 := E q * (log q * (γ)) = = n i=1 α i0 log α i0 I(y i = 0) + α i1 log α i1 + α i2 log α i2 ,<label>(44)</label></formula><p>F 2 := E q * (log q * (λ 1 )) = = {-log Γ(ψ 1 ) + ψ 1 log(ζ 1 ) + (ψ 1 -1)E q * [log(λ 1 )] -(ζ 1 )E q * [λ 1 ]} = {-log Γ(ψ 1 ) + ψ 1 log(ζ 1 ) + (ψ 1 -1)(-log(ζ 1 ) + Ψ(ψ 1 )) -</p><formula xml:id="formula_127">ψ 1 /ζ 1 } ,<label>(45)</label></formula><p>and</p><formula xml:id="formula_128">F 3 := E q * (log q * (λ 2 )) = = {-log Γ(ψ 2 ) + ψ 2 log(ζ 2 ) + (ψ 2 -1)E q * [log(λ 2 )] -(ζ 2 )E q * [λ 2 ]} = {-log Γ(ψ 2 ) + ψ 2 log(ζ 2 ) + (ψ 2 -1)(-log(ζ 2 ) + Ψ(ψ 2 )) -ψ 2 /ζ 2 } .<label>(46)</label></formula><p>Finally,</p><p>F 4 := = E q * (log q * (β 1 )) = E q * [log φ(β 1 ; µ q * (β 1 ) , Σ q * (β 1</p><p>))] = -R 2 log 2π -1 2 log |Σ q * (β 1 ) | -(1/2)E q * (β 1 ) [(β 1 -µ q * (β 1 ) ) Σ -1 q * (β 1 )</p><p>(β 1 -µ q * (β 1</p><p>))] = -R 2 log 2π -1 2 log |Σ q * (β 1 ) | -1 2 tr(Σ -1 q * (β 1 )</p><formula xml:id="formula_129">Σ q * (β 1 ) ) = - R 2 log 2π - 1 2 log |Σ q * (β 1 ) | - R 2 . (<label>47</label></formula><formula xml:id="formula_130">)</formula><p>and</p><formula xml:id="formula_131">F 5 := = E q * (log q * (β 2 )) = E q * [log φ(β 2 ; µ q * (β 2 ) , Σ q * (β 2 ))] = - R 2 log 2π - 1 2 log |Σ q * (β 2 ) | -(1/2)E q * (β 2 ) [(β 2 -µ q * (β 2 ) ) Σ -1 q * (β 2 ) (β 2 -µ q * (β 2 ))] = - R 2 log 2π - 1 2 log |Σ q * (β 2 ) | - 1 2 tr(Σ -1 q * (β 2 ) Σ q * (β 2 ) ) = - R 2 log 2π - 1 2 log |Σ q * (β 2 ) | - R 2 .<label>(48)</label></formula><p>B.4. A fast way to compute E q * (β) [log(1 + exp(x β 1 ) + exp(x β 1 )] Similarly to Section A.5 we can construct an orthonormal matrix S such as the first column of S is x/ x . We have that under the variational densities, beta 1 and β 2 are independent d-variate normal random vectors with mean vectors µ * β 1 and µ * β 2 and covariance matrices Σ 1 and Σ 2 , respectively. Letting Sη = (β 1 -µ β 1 ) and Sζ = (β 2 -µ β 2 ) we can write E q * [log(1 + exp(x β 1 ) + exp(x β 2 ))] = = R 2d</p><p>[log(1 + exp(x β 1 ) + exp(x β 2 ))]q * (β 1 )q * (β 2 )</p><formula xml:id="formula_132">dβ 1 dβ 2 = R d F 1 (x β 1 )q * (β 1 )dβ 1 (<label>49</label></formula><formula xml:id="formula_133">)</formula><p>where</p><formula xml:id="formula_134">F 1 (β 1 ) = R d</formula><p>[log(1 + exp(x β 1 ) + exp(x β 2 ))]q * (β 2 ) dβ 2 .</p><p>(50)</p><p>Plugging ( <ref type="formula" target="#formula_115">36</ref>) into (50) we get</p><formula xml:id="formula_135">F 1 (β 1 ) = R log(1 + exp(x β 1 ) + exp( x ζ 1 + x µ β 2 )) × 1 (2π) 1/2 Σ 2,(1,1) exp - 1 2 η 1 (S Σ -1 2 S) (1,1) ζ 1 dζ 1 .<label>(51)</label></formula><p>Now, we can apply ( <ref type="formula" target="#formula_115">36</ref>) and ( <ref type="formula" target="#formula_135">51</ref>) into (49) and we get E q * [log(1 + exp(x β 1 ) + exp(x β 2 ))] = = R R log(1 + exp( x η 1 + x µ β 1 ) + exp(</p><formula xml:id="formula_136">x ζ 1 + x µ β 2 )) × 1 (2π) 1/2 Σ 2,(1,1) exp - 1 2 ζ 1 (S Σ -1 2 S) (1,1) ζ 1 dζ 1 × 1 (2π) 1/2 Σ 1,(1,1)</formula><p>exp -1 2 η 1 (S Σ -1 1 S) (1,1) dη 1 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Functional covariates X i,1 (•) (top left panel) and X i,2 (top right panel) for random sample of 10 subjects, i = 1, . . . , 10, and weight functions w 1 (•) (bottom left panel) and w 2 (•) (bottom right panel), for Study 1. Red (traced) and black (solid) lines correspond to subjects belonging to class 1 and 0, for the normal case, and Class 1 and Class 2 for the ZIMP case, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Histogram of the change in HAM-D (baseline -week 1) showing the amount of improvement in depression symptoms after 1 week for both drug and placebo treated patients. The left panel presents the normal mixture with two components given by the EM algorithm without covariates. The right panel, shows the fit using mixture of two normal distributions 0.84 × N (0.82, 17.08) + 0.16 × N (10.72, 17.08) chosen by the linear model with Normal prior and logit link. Red and green curves are the mixture components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5: EEG for 9 subjects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Posterior distribution for the total number of early responders using linear model with logit link function and Normal prior</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Functional boxplots for the significant weight functions using linear model with logit link function and Normal prior. The magenta area is the 50% central region, red dashed curves are outlier candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :Fig. 9 :Fig. 10 :Fig. 11 :</head><label>891011</label><figDesc>Fig. 8: Histogram for posterior distribution of λ 1 and λ 2 for the ZIMP model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>p(γ i |ν(-γ i ), y, κ) = ∝ I(y i = 0) log(1 + exp(x i β 1 ) + exp(x i β 2 )) γi0 × e -λ1 λ yi 1 e xiβ 1 log(1 + exp(x i β 1 ) + exp(x i β 2 )) γi1 × e -λ2 λ yi 2 e xiβ 2 log(1 + exp(x i β 1 ) + exp(x i β 2 )) γi2 ∝ (I(y i = 0)) γi0 e -λ1 λ yi 1 e xiβ 1 γi1 e -λ2 λ yi 2 e xiβ 2 γi2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E</head><label></label><figDesc>q * (γ i ) γ i1 [-λ 1 + y i log(λ 1 )] = C 2,0 + (a 1 -1) log(λ 1 ) -b 1 λ 1 + n i=1 α i1 [-λ 1 + y i log(λ 1 )] = C 2,0 + n i=1 α i1 y i + a 1 -1 log(λ 1 ) -n i=1 α i1 + b 1 λ 1 E 0 := E q * (log p(y|λ 1 , λ 2 , γ)) = E q * [ n i=1 γ i0 I(y i = 0) + (-log y i !) + γ i1 (-λ 1 + y i log(λ 1 ))] + E q * [γ i2 (-λ 2 + y i log(λ 2 ))] = n i=1 α i0 I(y i = 0) + (-log y i !) + α i1 -ψ 1 ζ 1 + y i (-log(ζ 1 ) + Ψ(ψ 1 )) + α i2 -ψ 2 ζ 2 + y i (-log(ζ 2 ) + Ψ(ψ 2 ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>E 2 :</head><label>2</label><figDesc>= E q * (log p(λ 1 )) = = -log(Γ(a 1 )) + a 1 log(b 1 ) + (a 1 -1)E q * [log(λ 1 )] -b 1 E q * [λ 1 ] = -log(Γ(a 1 )) + a 1 log(b 1 ) + (a 1 -1)(-log(ζ 1 ) + Ψ(ψ 1 )) -= E q * (log p(λ 2 )) = = -log(Γ(a 2 )) + a 2 log(b 2 ) + (a 2 -1)E q * [log(λ 2 )] -b 2 E q * [λ 2 ] = -log(Γ(a 2 )) + a 2 log(b 2 ) + (a 2 -1)(-log(ζ 2 ) + Ψ(ψ 2 )) -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Interval for normal mixed model and ZIMP location parameters, logit link, n = 100, 300, 500, and MCMC and VB inference methods in Study 1.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Normal Mixed model</cell><cell cols="2">ZIMP model</cell></row><row><cell>n</cell><cell>Method</cell><cell>µ 0</cell><cell>µ 1</cell><cell>λ 1</cell><cell>λ 2</cell></row><row><cell>100</cell><cell cols="5">MCMC (-2.34;0.23) VB (-1.82;0.60) (6.95;11.15) (1.44;3.11) (9.00;11.00) (4.30;9.71) (1.33;3.09) (8.81;10.88)</cell></row><row><cell>300</cell><cell cols="5">MCMC (-1.83;-0.40) (6.35;8.72) (1.91;2.80) (9.12;10.26) VB (-1.2;0.08) (7.52;10.32) (1.70;2.66) (9.46;10.50)</cell></row><row><cell>500</cell><cell cols="5">MCMC (-1.44;-0.27) (7.01;8.69) (1.93;2.61) (9.33;10.22) VB (-1.06;-0.11) (7.84,10.14) (1.81;2.42) (9.53;10.33)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>n</head><label></label><figDesc>Method MR .5 MR .75 MR .9 MSE</figDesc><table><row><cell>100</cell><cell>MCMC 0.14 VB 0.10</cell><cell>0.09 0.09</cell><cell>0.08 0.08</cell><cell>0.11 0.10</cell></row><row><cell>300</cell><cell>MCMC 0.13 VB 0.09</cell><cell>0.09 0.07</cell><cell>0.08 0.07</cell><cell>0.11 0.09</cell></row><row><cell>500</cell><cell>MCMC 0.12 VB 0.08</cell><cell>0.08 0.07</cell><cell>0.08 0.07</cell><cell>0.10 0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MSE and misclassification rate at p-percentile (MR p ) for p = 0.5, 0.75, and 0.9 (MR p ) for logit link, normal mixed model, n = 100, 300, 500, VB and MCMC inference methods in Study 1.</figDesc><table><row><cell>n</cell><cell>Method MR MSE</cell></row><row><cell>100</cell><cell>MCMC 0.03 0.16 VB 0.03 0.16</cell></row><row><cell>300</cell><cell>MCMC 0.04 0.16 VB 0.03 0.15</cell></row><row><cell>500</cell><cell>MCMC 0.04 0.16 VB 0.03 0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MSE and misclassification rate (MR) for logit link, ZIMP model, n = 100, 300, 500, VB and MCMC inference methods in Study 1.</figDesc><table><row><cell>n</cell><cell cols="2">Method Normal mixture</cell><cell>ZIMP</cell></row><row><cell>100</cell><cell>MCMC VB</cell><cell>1.43 (0.10) 0.29 (0.14)</cell><cell>6.03 (1.43) 1.04 (0.23)</cell></row><row><cell>300</cell><cell>MCMC VB</cell><cell>2.00 (0.20) 0.41 (0.41)</cell><cell>8.43 (1.81) 2.93 (0.69)</cell></row><row><cell>500</cell><cell>MCMC VB</cell><cell>2.58 (0.18) 0.04 (0.09)</cell><cell>10.51 (2.44) 5.45 (0.66)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean and standard deviation of elapsed time (in minutes) resulted from normal mixture and ZIMP model for fitting data in Study 1 using MCMC and VB methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Misclassification rate (MR p ) MSE</cell></row><row><cell cols="2">Method 0.5 0.75</cell><cell>0.9</cell><cell></cell></row><row><cell cols="2">MCMC 0.09 0.09</cell><cell>0.13</cell><cell>0.02</cell></row><row><cell>VB</cell><cell>0.06 0.07</cell><cell>0.10</cell><cell>0.05</cell></row></table><note><p>MSE and misclassification rate for logit link, normal mixed model, n = 150, VB and MCMC inference methods for Study 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>at a total of 14 electrodes (P 9 , P 10 , P 7 , P 8 , P 5 , P 6 , P O 7 , P O 8 , P O 3 , P O 4 , O 1 , O 2 , P O Z and O Z ) located in occipital and parietal brain regions. Each electrode is measured at 45 frequencies at a 0.25 Hz resolution within the theta (4 -7 Hz) and alpha (7 -15 Hz) frequency bands. The response variable for each subject is the Hamilton Depression Rating Scale (HAM-D), measured before the treatment (baseline) and after one week into the study. It is believed that the active drug treatment can only have an effect on symptoms after two weeks. Therefore, any improvement observed after one week is likely to be due to placebo effect (or spontaneous improvement). For more details, we refer to<ref type="bibr" target="#b24">Jiang et al. (2017)</ref> and references therein.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Table 10 independent of the prior distribution, by the parsimonious Posterior mean (standard deviation) of µ 0 , µ 1 , σ 2 , θ and p for different prior distributions and linear and nonlinear functional model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">logit link</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Nonlinear model</cell><cell></cell><cell></cell><cell>Linear model</cell><cell></cell></row><row><cell>Prior</cell><cell>Student t</cell><cell>Normal</cell><cell>Cauchy</cell><cell>Student t</cell><cell>Normal</cell><cell>Cauchy</cell></row><row><cell>µ0</cell><cell>0.88 (0.48)</cell><cell>0.87 (0.47)</cell><cell>0.89 (0.48)</cell><cell>0.86 (0.61)</cell><cell>0.82 (0.61)</cell><cell>0.89 (0.60)</cell></row><row><cell>µ1</cell><cell>11.52 (1.18)</cell><cell>11.57 (1.18)</cell><cell>11.44 (1.16)</cell><cell>10.81 (1.57)</cell><cell>10.72 (1.52)</cell><cell>10.86 (1.55)</cell></row><row><cell>Intercept</cell><cell>-1.07 (0.34)</cell><cell>-0.008 (0.005)</cell><cell>-1.24 (0.44)</cell><cell>-1.07 (0.34)</cell><cell>-1.07 (0.40)</cell><cell>-1.26 (0.79)</cell></row><row><cell>Sex</cell><cell>-1.85 (0.48)</cell><cell>-0.34 (0.21)</cell><cell>-1.62 (0.42)</cell><cell>-1.85 (0.50)</cell><cell>-1.85 (0.44)</cell><cell>-1.79 (0.54)</cell></row><row><cell>Chronicity</cell><cell>-1.76 (0.45)</cell><cell>-0.44 (0.19)</cell><cell>-1.64 (0.41)</cell><cell>-1.76 (0.44)</cell><cell>-1.79 (0.44)</cell><cell>-1.70 (0.54)</cell></row><row><cell>σ 2</cell><cell>15.82 (2.30)</cell><cell>15.81 (2.34)</cell><cell>15.95 (2.48)</cell><cell>17.21 (3.00)</cell><cell>17.08 (2.85)</cell><cell>17.32 (2.93)</cell></row><row><cell>p</cell><cell>0.18</cell><cell>0.16</cell><cell>0.18</cell><cell>0.15</cell><cell>0.16</cell><cell>0.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">logit link</cell><cell></cell><cell></cell></row><row><cell>t</cell><cell cols="2">Nonlinear model</cell><cell></cell><cell cols="2">Linear model</cell><cell></cell></row><row><cell></cell><cell cols="6">Student t Normal Cauchy Student t Normal Cauchy</cell></row><row><cell>-5</cell><cell>0.51</cell><cell>0.51</cell><cell>0.53</cell><cell>0.44</cell><cell>0.45</cell><cell>0.46</cell></row><row><cell>0</cell><cell>0.61</cell><cell>0.61</cell><cell>0.63</cell><cell>0.62</cell><cell>0.62</cell><cell>0.63</cell></row><row><cell>5</cell><cell>0.34</cell><cell>0.33</cell><cell>0.36</cell><cell>0.47</cell><cell>0.48</cell><cell>0.48</cell></row><row><cell>10</cell><cell>0.44</cell><cell>0.47</cell><cell>0.47</cell><cell>0.39</cell><cell>0.39</cell><cell>0.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Posterior predictive checks as a function of the threshold (t) for different prior distributions and linear and nonlinear functional model.</figDesc><table><row><cell>change in</cell></row></table><note><p><p><p>criterion we chose the linear model fitted with the Normal prior distribution and consider this model to classify subjects as early responders based on the maximum posterior estimate of p(γ i |y), see Table</p>10</p>. Specifically, 15 subjects (15.6%) were classified to the early responder subgroup (posterior mean probability is &gt; 1/2), with the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Frequency table for the posterior mean for the probability of being and early responder.</figDesc><table><row><cell>. , 14, which gives</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>illness for milking cows based on functional covariates measured 30 days before lactation Understanding factors that affect productivity of dairy cattle is essential for optimizing profitability and sustainability of dairy farms. The transition from late pregnancy into early lactation in cows is one of the most important stages of the lactation cycle of Posterior quantiles of normal mixture parameters using MCMC sample for logit link and Normal prior.</figDesc><table><row><cell>Quantile</cell><cell cols="2">2.5% 25%</cell><cell>50%</cell><cell cols="2">75% 97.5%</cell></row><row><cell>µ 0</cell><cell cols="2">-0.40 0.40</cell><cell>0.84</cell><cell>1.22</cell><cell>2.00</cell></row><row><cell>µ 1</cell><cell>8.01</cell><cell cols="4">9.65 10.64 11.74 13.91</cell></row><row><cell>σ 2</cell><cell cols="5">12.20 15.23 16.74 18.82 23.17</cell></row><row><cell>Intercept</cell><cell cols="5">-1.87 -1.27 -1.02 -0.84 -0.51</cell></row><row><cell>Sex</cell><cell cols="5">-2.73 -2.13 -1.83 -1.57 -1.04</cell></row><row><cell cols="6">Chronicity -2.67 -2.13 -1.78 -1.43 -1.08</cell></row><row><cell></cell><cell></cell><cell cols="2">Logit link</cell><cell></cell><cell>Probit link</cell></row><row><cell>Model</cell><cell cols="5">Nonlinear Linear Nonlinear Linear</cell></row><row><cell>E q  *  [µ 0 ]</cell><cell></cell><cell>1.40</cell><cell>1.13</cell><cell></cell><cell>1.15</cell><cell>0.32</cell></row><row><cell>E q  *  [µ 1 ]</cell><cell></cell><cell>13.34</cell><cell>12.38</cell><cell></cell><cell>12.61</cell><cell>9.89</cell></row><row><cell>E q  *  (σ 2 )</cell><cell></cell><cell>18.14</cell><cell>16.68</cell><cell></cell><cell>16.51</cell><cell>14.64</cell></row><row><cell>Intercept (E q  *  [β 0 ])</cell><cell></cell><cell>-0.01</cell><cell>-0.30</cell><cell></cell><cell>-0.01</cell><cell>-0.33</cell></row><row><cell>Female (E q  *  [β 1 ])</cell><cell></cell><cell>-0.14</cell><cell>-0.93</cell><cell></cell><cell>-0.01</cell><cell>-0.89</cell></row><row><cell cols="2">Chronicity (E q  *  [β 2 ])</cell><cell>-0.13</cell><cell>-1.39</cell><cell></cell><cell>-0.17</cell><cell>-1.44</cell></row><row><cell>p</cell><cell></cell><cell>0.10</cell><cell>0.14</cell><cell></cell><cell>0.14</cell><cell>0.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Posterior VB parameters m 0 , m 1 , s 2 0 , s 2 1 , expectation under the variational distribution with VB fitted parameters and p = (1/96) 96 i=1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Classification for the training set according to the class presenting the maximum mean value of the posterior distribution and VB estimation of the latent variable γ</figDesc><table><row><cell>Quantiles -MCMC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>VB Mean and Posterior mean and quantiles for the parameters according to MCMC sample for logit link and Normal prior for Zero Inflated mixture of two Poisson distributions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Mean and standard deviation for the estimated coefficients for the Intercept and scalar variables for MCMC and VB algorithms.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Frequency table f estimated probabilities p0i , p1i and p2i for the Zero Inflated mixture of Poisson using MCMC posterior mean and VB.</figDesc><table><row><cell></cell><cell>Number of Rest Times</cell></row><row><cell>21</cell><cell>1 0.2 0.3</cell></row><row><cell>w</cell><cell></cell></row><row><cell></cell><cell>Days</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This work was partially financed by <rs type="funder">NIMH</rs> grant <rs type="grantNumber">5 R01 MH099003</rs>, <rs type="institution">USDA National Institute of Food and Agriculture Animal Health program</rs> award <rs type="grantNumber">2017-67015-26772</rs> to <rs type="person">Julio Giordano</rs>, <rs type="funder">FAPESP</rs> grants <rs type="grantNumber">2017/15306-9</rs>, <rs type="grantNumber">2018/06811-4</rs> and <rs type="grantNumber">2019/10800-0</rs>, <rs type="funder">CNPq</rs> grants <rs type="grantNumber">302598/2014-6, 442012/2014-4</rs> and <rs type="grantNumber">304148/2020-2</rs>. A special thanks to <rs type="person">Alberto Saa</rs> for helping to solve a computational bottleneck and <rs type="person">Guilherme J.M. Rosa</rs> for fruitful discussions. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the view of the <rs type="institution">National Institute of Food and Agriculture (NIFA)</rs> or the <rs type="funder">United States Department</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_a63cJBd">
					<idno type="grant-number">5 R01 MH099003</idno>
				</org>
				<org type="funding" xml:id="_K2qvs56">
					<idno type="grant-number">2017-67015-26772</idno>
				</org>
				<org type="funding" xml:id="_WU3Br4H">
					<idno type="grant-number">2017/15306-9</idno>
				</org>
				<org type="funding" xml:id="_8QS9Mdu">
					<idno type="grant-number">2018/06811-4</idno>
				</org>
				<org type="funding" xml:id="_pXJtaBA">
					<idno type="grant-number">2019/10800-0</idno>
				</org>
				<org type="funding" xml:id="_8Bquv6R">
					<idno type="grant-number">302598/2014-6, 442012/2014-4</idno>
				</org>
				<org type="funding" xml:id="_Ef2WGA2">
					<idno type="grant-number">304148/2020-2</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Similarly,</p><p>which characterizes q * (λ 1 ) and q * (λ 2 ) as gamma distribution with parameters</p><p>and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Calculating the ELBO</head><p>The ELBO is given by</p><p>with the expectation is taken with respect to q * (λ 1 , λ 2 , γ, β|κ), that is E q * := E q * (λ1,λ2,γ,β|κ) . Therefore, we have to compute</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">422</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring parameters and structure of latent variable models by variational bayes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">mixtools: An R Package for Analyzing Finite Mixture Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chauveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2009-10">2009, October</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Functional linear model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cardot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constructing treatment decision rules based on scalar and functional predictors when moderators of treatment effect are unknown</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ciarleglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tarpey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C, Applied statistics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1331</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimating the components of a mixture of normal distributions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Day</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="474" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of finite mixture distributions through bayesian sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diebolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The title of the workFinite Mixture Distributions. The name of the publisherSpringer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nonparametric functional data analysis: theory and practice</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additive prediction and boosting for functional data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1400" to="1413" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mclust version 3 for r: Normal mixture modeling and model-based clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fraley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data augmentation and mcmc for binary and multinomial logit models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frühwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical modelling and regression structures</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Markov chain Monte Carlo: stochastic simulation for Bayesian inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Lopes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A weakly informative default prior distribution for logistic and other regression models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Pittau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of applied statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1360" to="1383" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Penalized functional regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bobb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Crainiceanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caffo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="830" to="851" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simulation-based regularized logistic regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="567" to="590" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trajectories of depression severity in clinical trials of duloxetine: insights into antidepressant and placebo responses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gueorguieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mallinckrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of general psychiatry</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1227" to="1237" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-inflated poisson and binomial regression with random effects: a case study</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1030" to="1039" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Marginal models for zero inflated clustered data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Modelling</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="180" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian auxiliary variable models for binary and multinomial regression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="168" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized linear models with functional predictors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="432" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent class modeling using matrix covariates with application to identifying early placebo responders based on eeg signals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tarpey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1513" to="1536" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Additive function-on-function regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Staicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="234" to="244" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression quantiles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bassett</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-inflated poisson regression, with an application to defects in manufacturing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Changes in brain function of depressed subjects during treatment with placebo</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Leuchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abrams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychiatry</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelder</surname></persName>
		</author>
		<title level="m">Generalized Linear Models, Second Edition. Chapman and Hall/CRC Monographs on Statistics and Applied Probability Series. Chapman &amp; Hall</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<title level="m">Finite mixture models</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Functional generalized additive models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Staicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scheipl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="269" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model-based clustering of microarray expression data via latent gaussian mixture models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcnicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2705" to="2712" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Functional logistic regression: a comparison of three methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Computation and Simulation</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="250" to="268" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Electric fields of the brain: the neurophysics of EEG</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gaussian variational approximate inference for generalized linear mixed models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ormerod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Parisi</surname></persName>
		</author>
		<title level="m">Statistical field theory</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional analysis for mixed covariates, with application to feed intake of lactating sows</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Mendoza</forename><surname>Benavides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Heugten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Staicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Probability and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian inference for logistic models using pólya-gamma latent variables</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Windle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">504</biblScope>
			<biblScope unit="page" from="1339" to="1349" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<title level="m">R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Principal components analysis for functional data. Functional data analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="147" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Applied functional data analysis: methods and case studies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical variational models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Models for count data with many zeros</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ridout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Demétrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XIXth international biometric conference</title>
		<meeting>the XIXth international biometric conference<address><addrLine>South Africa</addrLine></address></meeting>
		<imprint>
			<publisher>International Biometric Society Invited Papers Cape Town</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="179" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A mixed-effect model for positive responses augmented by zeros</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodrigues-Motta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Galvis Soto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Lachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Baltar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Fisberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Lobo Marchioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1761" to="1778" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Time-dependent statistical and correlation properties of neural signals during handwriting</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Rupasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Erlichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Leiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">43945</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Use of rumination and activity monitoring for the identification of dairy cows with health disorders: Part i. metabolic and digestive disorders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stangaferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Caixeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Abri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7395" to="7410" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Use of rumination and activity monitoring for the identification of dairy cows with health disorders: Part iii. metritis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stangaferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Caixeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Abri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7422" to="7433" />
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Use of rumination and activity monitoring for the identification of dairy cows with health disorders. part ii.mastitis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stangaferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Caixeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Abri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7411" to="7421" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Functional boxplots</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="334" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Statistical analysis of finite mixture distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">E</forename><surname>Makov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The neuroscience of placebo effects: connecting context, learning and health</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="403" to="418" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Placebo response in studies of major depression: variable, substantial, and growing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Seidman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sysko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1840" to="1847" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Placebo analgesia is not due to compliance or habituation: Eeg and behavioural evidence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>El-Deredy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="771" to="775" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A generalized mean field algorithm for variational inference in exponential families</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The transferable placebo effect from pain to emotion: changes in behavior and eeg activity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="634" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
