<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extreme Image Compression Using Fine-tuned VQGANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-15">15 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tinghan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinuo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Libiao</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China † City University of Hong Kong ‡ Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extreme Image Compression Using Fine-tuned VQGANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-15">15 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.08265v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. However, their efficacy and applicability to achieve extreme compression ratios (&lt; 0.05 bpp) remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)-based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields a strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We propose clustering a pre-trained largescale codebook into smaller codebooks through the K-means algorithm, yielding variable bitrates and different levels of reconstruction quality within the coding framework. Furthermore, we introduce a transformer to predict lost indices and restore images in unstable environments. Extensive qualitative and quantitative experiments on various benchmark datasets demonstrate that the proposed framework outperforms state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception at extremely low bitrates (≤ 0.04 bpp). Remarkably, even with the loss of up to 20% of indices, the images can be effectively restored with minimal perceptual loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the ever-increasing amount of visual data being generated at an unprecedented pace, the demand for highly efficient and effective compression algorithms has become increasingly crucial. However, under minimal network bandwidth, the signal-oriented traditional image/video compression codecs (e.g., BPG <ref type="bibr" target="#b0">[1]</ref>, and the latest video coding standard VVC <ref type="bibr" target="#b1">[2]</ref>) inevitably adopt large scalar quantization steps, resulting in a significant loss of texture information with unacceptable blurring and blocking artifacts.</p><p>To bridge the gap of shallow bitrate scenarios, recent image coding methods <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> leverage the power of generative models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to reconstruct the human-favored decoded image/video. There are currently two main perspectives in such generative compression: the first <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> involves using a conditional GAN <ref type="bibr" target="#b14">[15]</ref> as an additional distortion term to optimize deep learning-based end-to-end neural codecs. This category of methods enhances the reconstruction of texture details in the decoded image through adversarial training. However, their effectiveness in achieving high compression ratios, especially below 0.05 bpp, remains limited. Another line <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b15">16]</ref> aims to compress images into compact feature representations at the encoding end and generate decoded images with the aid of GANs, achieving visual pleasing reconstruction even at extremely low compression ratios. However, without additional training, such approaches <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b15">16]</ref> have difficulty reconstructing the original image with large semantic information gaps against the training dataset. For instance, codecs optimized for face images may not perform well on natural scenario images. As such, their practical use in extremely low-bitrate scenarios is hindered by poor generalization ability.</p><p>Recently, vector quantization (VQ)-based generative models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, which utilize discrete image representations, have been well used in image generation tasks. Despite their success in other generation fields, VQ-based generative models have received relatively less attention in the image compression domain. In this work, we reveal a surprising finding: The learned codebook of the VQGAN model <ref type="bibr" target="#b16">[17]</ref>, which has been trained on a large-scale dataset, exhibits a powerful and robust representational capacity. Accordingly, we propose a simple yet effective coding framework by directly integrating VQ-indices compression into the VQGAN model, which yields a significant advancement in the capability of extreme image compression and generalizability across various semantics and resolutions of images. In particular, the VQ-indices map is obtained by identifying the nearest sample in the learned codebook, which is then encoded into a bitstream using arithmetic compression. To enable variable bitrates, we propose clustering a pre-trained large-scale codebook using the K-means algorithm, resulting in a series of smaller codebooks. As such, the image can be represented by various VQ-indices maps, enabling variable bitrates and different levels of reconstruction quality. Moreover, given the potential impact of bitstream loss on the decoding process within an unstable transmission environment, we propose to leverage the second-stage transformer of the VQGAN model to predict missing indices. It can be effectively predicted based on context indices that adhere to the underlying discrete distribution, thereby effectively circumventing image reconstruction failure due to the loss of bitstreams.</p><p>The main contributions in this paper can be summarized as follows:</p><p>• We present a simple yet effective coding framework that utilizes the VQGAN model in developing a novel extreme image compression framework. • We propose a K-means clustering approach to compress the large-scale codebook into a smaller new codebook, which enables variable bitrates and levels of reconstruction quality within our framework. Furthermore, a second-stage transformer is leveraged to predict missing indices, enhancing the resilience of the proposed framework in unstable transmission. • Both qualitative and quantitative results demonstrate that the proposed framework achieves a significant improvement compared to state-of-the-art codecs about both perceptual quality metrics and human-viewed under extremely low bitrates (≤ 0.04bpp). Notably, even in cases where up to 20% of indices are lost, the images can be efficiently restored with minimal perceptual loss, thanks to the collaboration with the generative transformer model. In this work, we aim to compress images at ultra-low bit rates while maintaining the high-perceived quality of the reconstructed images. Our framework is built upon the discrete representations and the generative capacity of VQGAN models. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the entire framework consists of four key components:</p><formula xml:id="formula_0">• The encoder E: extract the input image x ∈ R H×W ×3 into a latent representa- tion z ∈ R H M × W M ×nz . • The codebook e k ∈ R nz , k ∈ 1, 2, • • • , K: map the latent representation z into</formula><p>a sequence of VQ-indices and invert it back to quantized latent representation</p><formula xml:id="formula_1">z q ∈ R H M × W</formula><p>M ×nz through the nearest neighbor lookup. The proposed K-means clustering algorithm method compresses the large-scale codebook into a smaller one, enabling variable bitrates and varying reconstruction quality.</p><p>• The decoder G: synthesize the quantized latent representation z q into a reconstructed image x ∈ R H×W ×3 . • The transformer T : predict the missing index based on the context indices. On the encoding side, VQ-indices are compressed into the final bitstream using lossless compression techniques. On the decoding side, we decode the index sequences from the bitstream and convert the decoded VQ indices back into the quantized latent representation z q by searching the codeword from the codebook. We detail the key components of the proposed framework below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">VQ-indices Compression</head><p>Unlike existing image compression methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that adopt scalar-quantization, we leverage the power of VQ to construct a codebook of representative vectors for latent representations z. In particular, the codebook e k is then used to encode the latent representation by replacing each position of a vector with the index of the closest representative vector by Euclidean distance, which results in a highly compressed 3 version of the latent representation with minimal loss of quality:</p><formula xml:id="formula_2">I ij = argmin k∈1,2,••• ,K ∥z ij -e k ∥ 2 ,<label>(1)</label></formula><p>where i and j denote the position of vector z ij in latent representation z, I ij represents its corresponding index, and K indicates the size of codebook. As demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>(a), an input image x can be efficiently represented by VQ-indices map</p><formula xml:id="formula_3">I ∈ R H M × W</formula><p>M , which significantly reduces the data amount. Then, we adopt the widely used arithmetic coding to compress VQ-indices into bitstreams, further reducing the data size. After decoding the compressed bitstream, the reconstructed latent vectors z q are generated by searching for their corresponding code words based on their indices. Finally, the reconstructed image x is synthesized by the decoder G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adjusting Variable Bitrates</head><p>The quality of the codebook in our proposed framework plays a crucial role in determining the compression performance of the entire system. As such, we develop a rate control strategy that utilizes the K-means algorithm to cluster the pre-trained large-scale trained codebook into the smaller one, thereby enabling the flexible adjustment of compression bitrates by altering the size of the codebook. Subsequently, the newly generated codebook ês derived from the K-means clustering algorithm can serve as a starting point for further fine-tuning, enabling faster convergence during subsequent optimization while ensuring codebook quality. Finally, we can obtain new codebooks of varying sizes, each with a corresponding set of VQ-indices to represent the compressed image, as illustrated in Fig. <ref type="figure" target="#fig_0">1(b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lost VQ-indices Prediction</head><p>In an unreliable transmission environment, lost packets may result in the loss of indices, which can cause incorrect decoding of bitstreams. However, our proposed image codec framework, which utilizes the VQ-indices transformer, can accurately predict lost indices at the decoder end. This approach enhances the robustness of the codec in dealing with unreliable network transmission and ensures that the decoded bitstreams remain accurate. We flatten the VQ-indices map Î, and denote it as [ Îi ] N i=1 , where N indicates the total length. Subsequently, the VQGAN's second-stage transformer is trained to predict the probability distribution of the next possible indices p( Îi | Î&lt;i ). The objective is to maximize the log-likelihood of the data representation, which can be expressed as follows:</p><formula xml:id="formula_4">L T = E x∼p(x) -log p( Î) ,<label>(2)</label></formula><p>where p( Î) = i p( Îi | Î&lt;i ). To simulate the potential loss of indices during transmission, we incorporate a masking procedure. Specifically, we apply a binary mask M = [m i ] N i=1 as follows: For m i = 1, the corresponding index Îi is replaced by a special <ref type="bibr">[mask]</ref> token to indicate that it has been lost. Conversely, if m i = 0, then Îi is left unchanged. The mask process is controlled by a mask ratio (α ∈ [0, 1]),  3 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>DataSets. The proposed framework is trained on the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref>, which contains 1.2 million images distributed across 1, 000 distinct categories. To assess the performance of the proposed model, we evaluate two commonly used datasets in image compression: the Kodak dataset <ref type="bibr" target="#b21">[22]</ref>, which includes 24 natural uncompressed 512 × 768 or 768 × 512 images; and the CLIC2020 dataset <ref type="bibr" target="#b22">[23]</ref>, comprising 250 images that exhibit varying lighting conditions and dynamic range, with resolutions ranging from 320 × 240 to 4032 × 3024.</p><p>Implementation Details. We adopt M = 16 to downsample the image x into the latent representation z. First, the weights of encoder E, decoder G, and the discriminator D are initialized by the officially provided pre-trained VQ-GAN model<ref type="foot" target="#foot_0">foot_0</ref> .  We then fine-tune the entire framework using the default settings and training losses as <ref type="bibr" target="#b16">[17]</ref>, which takes 13 hours on two NVIDIA Tesla-A100 GPUs.</p><p>Evaluation Metrics. Traditional objective quality assessment methods, such as PSNR and SSIM, have been primarily devised for the calculation of pixel-level distortions. However, these methods may not be suitable for assessing perceptual quality. Therefore, we integrate recent approaches i.e. the learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b23">[24]</ref> and the deep image structure and texture similarity (DISTS) <ref type="bibr" target="#b24">[25]</ref> to assess perceptual quality, which better aligns with the way humans perceive images. All of the above metrics are the smaller, the better. Furthermore, we utilize bits per pixel (bpp) to evaluate the rate performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compression Performance Comparison</head><p>Compared Methods. To evaluate the efficacy of our proposed framework, we compare the proposed method with both traditional standard and neural-based typical compression frameworks: First, we compare with classic image codec BPG <ref type="bibr" target="#b0">[1]</ref> and the latest video coding codec VVC <ref type="bibr" target="#b1">[2]</ref>. For the typical end-to-end (E2E) codec based on deep learning, we compare with Cheng et al. <ref type="bibr" target="#b18">[19]</ref> and retrain the model implemented by CompressAI<ref type="foot" target="#foot_1">foot_1</ref> to cover a bitrate range similar to ours. Furthermore, we develop an additional baseline that directly adopts the K-means clustering algorithm without any fine-tuning, denoted as "Ours w/o Fine-tune".</p><p>Qualitative Evaluation. Fig. <ref type="figure" target="#fig_1">2</ref> shows the reconstruction results of various methods on different images on the Kodak dataset <ref type="bibr" target="#b21">[22]</ref>, as well as the corresponding bpp and LPIPS. It is evident that at extremely low bitrates, VVC <ref type="bibr" target="#b1">[2]</ref>, BPG <ref type="bibr" target="#b0">[1]</ref>, and Cheng et al. <ref type="bibr" target="#b18">[19]</ref> exhibit varying degrees of blurring and missing texture details. In contrast, our proposed method reconstructs more texture details, such as trees, ripples, clouds, and feathers, resulting in the lowest LPIPS value. Moreover, models obtained using the K-means clustering algorithm can produce relatively rich details even without fine-tuning training, thanks to the inherent expressive power of the pre-trained codebook. Nevertheless, upon fine-tuning, our proposed method outperforms the former by achieving more color and texture reconstruction consistency.</p><p>Quantitative Evaluation. Our proposed framework is capable of achieving extremely low bitrate compression from 0.01 to 0.04 bpp while maintaining perceptual quality for a wide range of images with diverse semantics and resolutions across various datasets. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, the experimental results demonstrate the proposed approach exhibits remarkable improvements in perceptual-oriented metrics compared to other compression methods. To better evaluate the RD performance improvement of our proposed method, we utilize the Bjontegaard metric <ref type="bibr" target="#b25">[26]</ref>, as demonstrated in Table <ref type="table" target="#tab_1">1</ref>. For instance, with the same reconstruction quality of the LPIPS metric, our approach achieves approximately 97.93% ∼ 99.99%, 93.02% ∼ 99.74% bitrates saving compared to VVC <ref type="bibr" target="#b1">[2]</ref>, BPG <ref type="bibr" target="#b0">[1]</ref> and Cheng et al. <ref type="bibr" target="#b18">[19]</ref> over the two datasets, respectively. These results demonstrate the remarkable advantage of our proposed method achieving high perceptual quality for extremely low bitrate coding.  The performance of models generated by utilizing the K-means clustering algorithm without fine-tuning can yield promising results on the rate-distortion (R-D) curve when the size of the codebook is relatively large. Nonetheless, a significant degradation in performance is observed when the codebook size is reduced to below 64. Fine-tuning can effectively enhance the performance of the model under such conditions, with 21.57% ∼ 25.59% bitrates saving, as shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons with Generative Image Compression Codecs.</head><p>To comprehensively assess the efficiency and applicability of the proposed approach vis-a-vis state-of-the-art generative compression codecs, we perform comparative analyses against three representative codecs, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>. FCC <ref type="bibr" target="#b7">[8]</ref> and HiFiC <ref type="bibr" target="#b6">[7]</ref> are among the first categorical approaches that utilize adversarial loss to optimize the end-to-end framework. Nevertheless, all of them exhibit suboptimal performance in the presence of artifacts resembling colored and circle dots under extreme bitrates (≤ 0.03 bpp). Furthermore, the proposed method achieves bitrate savings ranging from 8.23% to 11.62% when compared to HiFiC <ref type="bibr" target="#b6">[7]</ref>, as indicated in Table <ref type="table" target="#tab_1">1</ref>. As a second line of approach, the SAVD method <ref type="bibr" target="#b15">[16]</ref> encodes images into semantic maps and corresponding texture features. We compare our method with the model trained on the ADE20K outdoor dataset. It can be observed that it suffers from poor generalization ability in the presence of a semantic gap between the training and testing domains, resulting in inadequate texture generation. In contrast, our method exhibits superior performance and generalization in generating reconstructed images with both subjective perception and objective metrics, with much lower bitrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Lost VQ-indices Prediction</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> presents a visualization of the lost indices map ranging from 10% to 40% loss ratios, as well as the corresponding restored images, generated via the utilization of indices predicted by the transformer model. It can be observed that the restored images display a similar degree of fidelity to the reconstructed image without missing indices when the levels of missing data are not particularly significant (≤ 20%). Large ratio loss can also preserve the rough structure of the original image, which verifies the robustness and resilience of the proposed coding framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we propose a novel scheme that utilizes the VQ-indices maps obtained from VQGANs as compact visual data representations to achieve extremely high image compression ratios while maintaining perceptual quality. Our proposed scheme uniquely adjusts the quantization step by varying the size of codebooks through the K-means clustering and recovers missing indices by the transformer, enabling reliable compression with variable bit rates and varying levels of reconstruction quality. Qualitative and quantitative results demonstrate the superiority of our proposed scheme in perceptual quality at extremely low bit rates (≤ 0.04 bpp) compared to state-of-the-art codecs. Even when up to 20% of indices are lost, the images can be successfully restored with minimal perceptual loss. Overall, our work advances image/video coding research by demonstrating the potential of VQ-based generative models for research in ultra-low bitrate compression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed VQGAN-based image coding framework.</figDesc><graphic coords="3,94.32,72.00,423.36,193.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The qualitative comparison results of BPG, VVC, Cheng et al., ours w/o fine-tuning and our method on the Kodak dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The R-D performance of BPG, VVC, Cheng et al., FCC, HiFiC, ours w/o fine-tuning and the proposed method on the Kodak dataset, and the CLIC2020 dataset. Table 1: BD-rate and BD-metric relative to the BPG, VVC, Cheng et al., and ours w/o fine-tuning respectively, where LPIPS is used as the distortion metric in BDmetric.</figDesc><graphic coords="6,94.32,72.00,423.36,92.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative comparisons of three typical generative compression codecs (i.e. FCC, HiFiC, SAVD) and the proposed method over the Kodak dataset.</figDesc><graphic coords="7,435.13,168.45,81.51,81.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Image restoration via lost VQ-indices prediction ranging from 10% to 40%.</figDesc><graphic coords="8,454.28,307.68,61.41,61.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>BPG VVC Origin al Cheng et al. Ours w/o Fine-tune 0.082 / 0.49 0.075 /0.46 BPP / LPIPS↓ 0.060 /0.53 0.027 / 0.25 0.027 / 0.24</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell>BPP / LPIPS↓</cell><cell>0.077/ 0.46</cell><cell>0.080 /0.40</cell><cell>0.081 /0.44</cell><cell>0.039 / 0.17</cell><cell>0.039 / 0.16</cell></row><row><cell>BPP / LPIPS↓</cell><cell>0.082 /0.21</cell><cell>0.063 /0.20</cell><cell>0.045 /0.25</cell><cell>0.023 / 0.16</cell><cell>0.023 / 0.12</cell></row><row><cell>BPP / LPIPS↓</cell><cell>0.059 /0.49</cell><cell>0.051 /0.46</cell><cell>0.052 /0.48</cell><cell>0.023/ 0.27</cell><cell>0.023 / 0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BD-rate and BD-metric relative to the BPG, VVC, Cheng et al., and ours w/o fine-tuning respectively, where LPIPS is used as the distortion metric in BDmetric.</figDesc><table><row><cell>Baseline</cell><cell cols="4">Kodak BD-rate BD-LPIPS BD-rate BD-LPIPS CLIC</cell></row><row><cell>VVC</cell><cell>-98.55%</cell><cell>-0.33</cell><cell>-93.02%</cell><cell>-0.21</cell></row><row><cell>BPG</cell><cell>-99.99%</cell><cell>-0.36</cell><cell>-99.74%</cell><cell>-0.27</cell></row><row><cell>HiFiC</cell><cell>-11.62%</cell><cell>-0.02</cell><cell>-8.23%</cell><cell>-0.01</cell></row><row><cell>Cheng et. al</cell><cell>-97.93%</cell><cell>-0.37</cell><cell>-98.90%</cell><cell>-0.25</cell></row><row><cell cols="2">Ours W/o Fine-tune -21.57%</cell><cell>-0.06</cell><cell>-25.59%</cell><cell>-0.06</cell></row><row><cell cols="5">To enable the proper bitrate range, we perform K-means clustering on the size of 16384</cell></row><row><cell cols="5">codebook of the pre-trained model, reducing the new codebooks into the size, ranging</cell></row><row><cell cols="2">from {2048, 1024, 512, 256, 128, 64, 32, 16, 8}.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/CompVis/taming-transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/InterDigitalInc/CompressAI</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bpg image format</title>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/bpg/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of the versatile video coding (vvc) standard and its applications</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Kui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianle</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens-Rainer</forename><surname>Ohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3736" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time adaptive image compression</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2922" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative compression</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Picture Coding Symposium (PCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="258" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for extreme learned image compression</title>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A training method for image compression networks to improve perceptual quality of reconstructions</title>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taejin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="144" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-fidelity generative image compression</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11913" to="11924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fidelitycontrollable extreme image compression with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Shoma</forename><surname>Iwai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomo</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiro</forename><surname>Sugaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinichiro</forename><surname>Omachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8235" to="8242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Layered conceptual image compression via deep semantic synthesis</title>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="694" to="698" />
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Thousand to one: Semantic prior modeling for conceptual coding</title>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanmin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conceptual compression via deep structure and texture synthesis</title>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanmin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2809" to="2823" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Consistency-contrast learning for conceptual coding</title>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic-aware visual decomposition for image coding</title>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="2333" to="2355" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Taming transformers for highresolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maskgit: Masked generative image transformer</title>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11315" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learned image compression with discretized gaussian mixture likelihoods and attention modules</title>
		<author>
			<persName><forename type="first">Zhengxue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaru</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Katto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7939" to="7948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00395</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kodak photocd dataset</title>
		<author>
			<persName><forename type="first">Eastman</forename><surname>Kodak</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Clic 2020: Challenge on learned image compression</title>
		<author>
			<persName><forename type="first">Toderici</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theis</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnston</forename><surname>Nick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustsson</forename><surname>Eirikur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mentzer</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balle</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Wenzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timofte</forename><surname>Radu</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/datasets/catalog/clic" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: Unifying structure and texture similarity</title>
		<author>
			<persName><forename type="first">Keyan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2567" to="2581" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Calculation of average psnr differences between rd-curves</title>
		<author>
			<persName><forename type="first">Gisle</forename><surname>Bjontegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITU SG16 Doc. VCEG-M</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
