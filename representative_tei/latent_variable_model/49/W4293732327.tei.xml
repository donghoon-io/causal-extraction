<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design</title>
				<funder ref="#_gRYyGDt">
					<orgName type="full">Advanced Research Projects Agency</orgName>
				</funder>
				<funder ref="#_vX8uDmx">
					<orgName type="full">U.S. Department of Energy</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">)</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Akshay</forename><surname>Iyer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering &amp; Management Sciences</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<email>weichen@northwestern.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">)</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Northwestern University</orgName>
								<orgName type="institution" key="instit2">Northwestern University</orgName>
								<orgName type="institution" key="instit3">Northwestern University</orgName>
								<orgName type="institution" key="instit4">Northwestern University</orgName>
								<orgName type="institution" key="instit5">Northwestern University Article</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.21203/rs.3.rs-1987975/v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Posted Date: August 29th, 2022</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-driven design shows the promise of accelerating materials discovery but is challenging due to the prohibitive cost of searching the vast design space of chemistry, structure, and synthesis methods. Bayesian Optimization (BO) employs uncertainty-aware machine learning models to select promising designs to evaluate, hence reducing the cost. However, BO with mixed numerical and categorical variables, which is of particular interest in materials design, has not been well studied.</p><p>In this work, we survey frequentist and Bayesian approaches to uncertainty quantification of machine learning with mixed variables. We then conduct a systematic comparative study of their performances in BO using a popular representative model from each group, the random forest-based Lolo model (frequentist) and the latent variable Gaussian process model (Bayesian).</p><p>We examine the efficacy of the two models in the optimization of mathematical functions, as well as properties of structural and functional materials, where we observe performance differences as related to problem dimensionality and complexity. By investigating the machine learning models' predictive and uncertainty estimation capabilities, we provide interpretations of the observed performance differences. Our results provide practical guidance on choosing between frequentist and Bayesian uncertainty-aware machine learning models for mixed-variable BO in materials design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The goal of materials design is to identify materials with desired property and performance that meet the demands of engineering applications, from among the vast composition-structure design space, which is challenging due to the highly nonlinear underlying physics and the combinatorial nature of the design space. The traditional trial-and-error approach usually involves many experiments or computations for evaluation of materials properties, which can be expensive and time-consuming and thus cannot keep pace with the growing demand. To accelerate materials development with low cost, data-driven adaptive design methods have recently been applied <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> . The adaptive design process starts with small data, selectively adds new samples to guide experimentation/computation, and navigates towards the global optimum.</p><p>The key to adaptive materials design is an efficient policy for searching the chemical/structural design space for the global minimum, such that new samples (material designs) are selected based on existing knowledge. Classical metaheuristic optimization methods, such as simulated annealing (SA) and genetic algorithm (GA), select new design samples based on nature-inspired stochastic rules. In recent years, researchers have built upon these methods by integrating them with statistical physics <ref type="bibr" target="#b4">5</ref> or deep learning <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7</ref> . However, these methods require many design evaluations, and thus lack cost efficiency, which limits their applicability in materials design.</p><p>In contrast, Bayesian Optimization (BO) <ref type="bibr" target="#b7">8</ref> represents a generalizable and more efficient adaptive design approach. Starting from a small set of known designs, with no strict requirement of representation form, BO iteratively fits ML models that predict the performance and quantify the uncertainty associated with unseen designs, and then selects new designs to be evaluated in the next iteration based on an acquisition function. BO methods have demonstrated capabilities in the design optimizations of a diversity of materials, including piezoelectric materials <ref type="bibr" target="#b8">9</ref> , catalysts <ref type="bibr" target="#b9">10</ref> , phase change memories <ref type="bibr" target="#b10">11</ref> , and structural materials <ref type="bibr" target="#b11">12</ref> .</p><p>Through these successful cases, BO has shown its versatility, as well as its high efficiency under a limited budget for design evaluation. Thus it has the potential of being an essential component of data-driven design automation, benefiting materials researchers who are not experts in data science.</p><p>In Bayesian Optimization, the sampling process is guided by acquisition functions. Commonly used acquisition functions, such as expected improvement (EI) <ref type="bibr" target="#b12">13</ref> , take into account both exploitation (pursuing a better objective) and exploration (reducing uncertainty). While exploitation is modulated by the ML model's prediction, exploration relies on the estimation of uncertainty in the predicted response for the unsampled sites. Therefore, uncertainty-aware machine learning (ML) models, i.e., ML models with uncertainty quantification (UQ), play a central role in BO. Various approaches have been developed to equip ML models with the UQ capability, which we will further discuss in the following section.</p><p>However, the mixed-variable problems, i.e., when material design variables include both numerical and categorical ones, pose additional challenges to uncertainty-aware machine learning, and is is ubiquitous in materials design. The design representations of inputs in materials design tasks typically include processing, composition, and structure information. Some design variables such as process type (e.g., hydrothermal or sol-gel), element choice (e.g., Al or Fe), and lattice type (e.g., fcc or bcc) are categorical, while others such as annealing temperature, stoichiometry, and lattice parameters, are numerical. For BO methods to be generally applicable to these diverse design representations, uncertainty-aware ML models must be able to handle mixed-variable inputs.</p><p>In this work, we first examine the methods for quantifying uncertainty in ML models and contrast their fundamental differences from a theoretical perspective. Based on this, we focus on two representative uncertainty-aware mixed-variable ML models that involve frequentist and Bayesian approaches to uncertainty quantification, respectively, and conduct a systematic comparative study of their performances in Bayesian Optimization, with an emphasis on materials design applications. Based on the results, we characterize the relative suitability of frequentist and Bayesian approaches to uncertainty-aware ML as related to problem dimensionality and complexity. Our contribution are two-fold:</p><p>• Outline of the suitability of Bayesian and frequentist uncertainty-aware ML models depending on the characteristics of problems;</p><p>• Identification of key factors that result in the performance difference between Bayesian and frequentist approaches.</p><p>We anticipate this study will assist researchers in physical sciences who use Bayesian Optimization, providing practical guidance in choosing the most appropriate model that suits their purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty-Aware Machine Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty in ML Models</head><p>Uncertainty is ubiquitous in predictive computational models. Even if the underlying physics is deterministic, uncertainty still exists due to the insufficiency of knowledge. Many efforts have been devoted to quantifying uncertainties of physics-based computational models <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> in science and engineering.</p><p>Unlike a physics-based model, the prediction of a data-driven model builds upon observations or previous data. Uncertainty in the prediction arises from (1) lack of data, (2) imperfect fit of model to the data, and (3) intrinsic stochasticity. These collectively form the metamodeling uncertainty <ref type="bibr" target="#b17">18</ref> , which reflects the discrepancy between the data-driven model's prediction and the response given by the physics-based model in unsampled regions. BO's sampling strategy is aimed at reducing the metamodeling uncertainty (exploration) and improve the objective function value (exploitation) by querying certain new samples. To this end, it is desired to have uncertainty-aware ML models, i.e., ML models for which the metamodeling uncertainty can be quantified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequentist and Bayesian Uncertainty Quantification</head><p>Several UQ techniques have been adopted to attain uncertainty-aware ML models. Here, we group them into two broad categories: frequentist and Bayesian. The frequentist approach obtains uncertainty estimation through various forms of resampling: in general, a series of models { fi (x x x)} n i=1 are fitted with different subsets of training data or hyperparameters, then the prediction variability at an unsampled location is estimated from the variance among these models' predictions:</p><formula xml:id="formula_0">ŝ2 (x x x) = Var f1 (x x x), . . . , fn (x x x) ,<label>(1)</label></formula><p>where Var(•) can represent any variance estimate using frequentist statistics, potentially involving noise or bias correction terms. Commonly used resampling techniques include Monte Carlo, Jackknife, Bootstrap, and their variations <ref type="bibr" target="#b18">19</ref> . In particular, uncertainty estimation using ensemble models <ref type="bibr" target="#b19">20</ref> or disagreement/voting of multiple models <ref type="bibr" target="#b20">21</ref> are also examples of the resampling approach.</p><p>The frequentist UQ approach has been adopted in combination with various ML models, including random forests <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref> , boosted trees <ref type="bibr" target="#b23">24</ref> , and deep neural networks <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> . As this approach is generally applicable regardless of the type of ML model, it is frequently coupled with "strong learners", i.e., the models that are capable of accurately fitting highly complex and non-stationary functions.</p><p>Instead of requiring a series of models, the Bayesian UQ approach treats the true model as a random field, and infers its posterior probability from the prior belief and observed data <ref type="bibr" target="#b7">8</ref> to estimate the uncertainty. A prominent example is Gaussian Process (GP) <ref type="bibr" target="#b27">28</ref> . When modeling the data (X X X, y y y), a GP model views the observed response y y y as the true response f f f plus random noise. It assumes that the response f f f at different input locations are jointly Gaussian, i.e., f f f |X X X ∼ N (µ µ µ, K K K). The covariance matrix K K K is inferred from the similarity between inputs using a kernel function. It also takes into account the noise that may be present in observations by assuming y y y| f f f ∼ N ( f f f , σ 2 I I I). For example, the radial basis function (RBF) kernel</p><formula xml:id="formula_1">K y(x x x), y(x x x ′ ) = σ 2 exp -∑ i ω i (x i -x ′ i ) 2<label>(2)</label></formula><p>uses Euclidean distance metric and assigns Gaussian correlations a priori, with global variance σ 2 and correlation parameters ω i , to be learned via maximum likelihood estimation (MLE) in model training. The prediction given by a GP model includes both the mean and the variance, thus providing a measure of metamodeling uncertainty.</p><p>Besides GP, examples of ML methods with Bayesian-style uncertainty estimation include Bayesian linear regression and generalized linear models, Bayesian model averaging <ref type="bibr" target="#b28">29</ref> , and Bayesian neural networks <ref type="bibr" target="#b29">30</ref> . When the posterior is not available in analytical form, estimation of the posterior requires probabilistic sampling techniques such as Markov Chain Monte Carlo (MCMC), whose high computational cost limits its application to various ML models <ref type="bibr" target="#b30">31</ref> . Therefore, Bayesian UQ approach is often adopted in specially designed ML models that have analytical form posterior, such as GP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty Quantification in Mixed-variable ML</head><p>However, for mixed-variable problems, uncertainty quantification of ML models becomes more complicated. In early developed ML methods, categorical variables are mostly handled by ordinal or one-hot encoding <ref type="bibr" target="#b31">32</ref> . Ordinal encoding assigns integer labels for each category; such encoding assumes ordered relation among categories, thus limiting its applicability. One-hot encoding represents a categorical variable t i that takes value from categories (often referred to as levels) {l 1 , l 2 , . . . , l J } with a binary vector</p><formula xml:id="formula_2">c c c i = [c (1) i , . . . , c (J) i ], c ( j) i = ✶ j ,<label>(3)</label></formula><p>where ✶ j is an indicator function, i.e., when t i = l j only the j-th element of c c c i equals 1, and others equal 0. This encoding, however, assumes a symmetry between all categories (the similarity between any two categories are equal <ref type="bibr" target="#b32">33</ref> ), which is generally not true.</p><p>In recent years, some methods have been proposed for uncertainty-aware ML in the mixed variable scenario. Lolo <ref type="bibr" target="#b33">34</ref> , for example, is an extension of the Random Forest (RF) model. As an ensemble of decision trees, RF has native support for mixed-variable problems. Uncertainty is quantified by calculating variance at any sample point from the predictions of the decision trees with bias correction.</p><p>GP models in the original form are uncertainty aware; however, they have problems handling categorical variables. The covariance matrix is inferred from the similarity between inputs characterized by a distance metric. But the aforementioned representations cannot represent the distances between categories.</p><p>The latent variable Gaussian Process (LVGP) model <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> solves this problem by mapping each categorical variable t i into a continuous-variable latent space, where each level l j of t i is represented by a vector z z z i = [z ( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_3">i ( j), . . . , z (q) i ( j)],</formula><p>where q, the dimensionality of latent space, is usually 2. The RBF kernel then becomes</p><formula xml:id="formula_4">K y(x x x,t t t), y(x x x ′ ,t t t ′ ) = σ 2 exp -∑ i ω i (x i -x ′ i ) 2 -∑ i z z z(t i ) -z z z(t ′ i ) 2 2 ,<label>(4)</label></formula><p>where ∥ • ∥ 2 is the L 2 norm. Like other parameters, locations of latent vectors are obtained via MLE during model training.</p><p>With the latent variable representation, the categories are not required to be ordered or symmetric, and their correlations are inherently estimated via distances in the mapped latent space. The latent variable configuration in the latent space also indicates the effects of different levels of a categorical variable on the response, thus making the model interpretable <ref type="bibr" target="#b34">35</ref> . There are extensions of LVGP <ref type="bibr" target="#b36">37</ref> that allow utilising large training data, as well as kernels other than RBF that are suitable for fitting functions with different characteristics. In this work, the vanilla LVGP is used in comparative studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Comparative Studies</head><p>Some related studies have compared the performances of the variety of UQ techniques in materials design applications. For example, Tian et al. <ref type="bibr" target="#b18">19</ref> compared four uncertainty estimators among the frequentist ones in materials property optimization.</p><p>Liang et al. <ref type="bibr" target="#b37">38</ref> conducted a benchmark study of BO for materials design using GP and Random Forest models with different acquisition functions. However, existing studies focus on BO where input variables are numerical, whereas practical materials design problems are often mixed-variable problems. The performance of ML methods using frequentist or Bayesian UQ techniques in BO under different circumstances involving categorical variables is not clear. In particular, the efficacy of two approaches in the materials design context has not yet been examined. We hope to fill the gap in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To examine and compare the performances of Bayesian Optimization using the two ML models (denoted LVGP-BO and Lolo-BO for conciseness), we tested them on both synthetic mathematical functions and materials property optimization problems. Our comparative study is conducted using a modular BO framework (illustrated in Figure <ref type="figure" target="#fig_0">1</ref>), in which the key components such as ML model, acquisition function, and design evaluation can be changed according to the need.</p><p>We use this BO framework to search for the minimum value of any function y(v v v), where the input variables</p><formula xml:id="formula_5">v v v = [x x x,t t t]</formula><p>consists of numerical variables x x x and/or categorical variables t t t, and the response y is always a scalar. The BO performances are compared in two aspects, accuracy and efficiency. Accuracy relates to the ability to find the optimal objective function value.</p><p>We record the complete optimization history for every test case, so that accuracy can be compared by looking at the minimal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Test Functions</head><p>We first present test results of minimizing mixed-variable mathematical functions. The mathematical functions are selected from an online library <ref type="bibr" target="#b38">39</ref> of commonly used test functions, with different dimensionalities and complexities. For mathematical functions that are originally defined on a continuous domain, we convert some of their arguments to be categorical for testing purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-dimensional Simple Functions</head><p>In the first test case, we use the Branin function, which has two input variables and relatively smooth behavior. We modify its definition as follows:</p><formula xml:id="formula_6">f (x,t) = t - 5.1 4π 2 x 2 + 5 π x -6 2 + 10 1 - 1 8π cos(x) + 10,<label>(5)</label></formula><p>where x ∈ [-5, 10] is a numerical variable, and t is categorical, with categories corresponding to values {0, 5, 10, 15}. To provide an intuitive sense of its behavior, we visualize the function in  </p><formula xml:id="formula_7">f (x,t) = sin(x + t) + (x -t) 2 -1.5x + 2.5t + 1,<label>(6)</label></formula><p>where x ∈ [-1.5, 4] is numerical, and t is categorical with 8 categories corresponding to integer values {-3, -2, . . . , 4}. The initial sample size and number of replicates are the same as described above; optimization histories are shown in Figure <ref type="figure" target="#fig_2">2f</ref>.</p><p>From the optimization history plots, we observe that for both test functions, LVGP-BO converges to the global minimum in fewer iterations, thus showing better efficiency. We present in the Supplementary Information additional low-dimensional simple mathematical function test cases, which support this finding as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-dimensional Complex Functions</head><p>We then test LVGP-BO and Lolo-BO in optimizing low-dimensional complex functions, in this case, rugged functions with several local and/or global minimums. The Six-Hump Camel function (visualized in Figure <ref type="figure" target="#fig_2">2c</ref>):</p><formula xml:id="formula_8">f (x,t) = 4 -2.1x 2 + x 4 3 x 2 + xt + (-4 + 4t 2 )t 2 ,<label>(7)</label></formula><p>with numerical x ∈ [-2, 2] and categorical t ∈ {±1, ±0.7126, 0}, is optimized in 30 test runs, each starting from initial samples of size 10. As Figure <ref type="figure" target="#fig_2">2g</ref> shows, both LVGP-BO and Lolo-BO converges to the global minimum, while LVGP leads to faster convergence.</p><p>We also test optimizing the Rastrigin function, which has more local minimums (as shown in Figure <ref type="figure" target="#fig_2">2d</ref>):</p><formula xml:id="formula_9">f (v v v) = 10d + d ∑ i=1 [v 2 i -10 cos(2πv i )],<label>(8)</label></formula><p>where d is the adjustable dimensionality. We set d = 3, with two numerical variables</p><formula xml:id="formula_10">x 1 = v 1 , x 2 = v 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>both in the range</head><p>[-5.12, 5.12], and one categorical variable t = v 3 ∈ {-5, -4, . . . , 5}. Note that we increase the maximum iteration number from 50 to 100, and run 10 replicates because of the long computing time. As Figure <ref type="figure" target="#fig_2">2h</ref> shows, in optimizing this highly multimodal function, LVGP-BO shows more performance superiority: it approaches the global minimum at around 60 iterations and eventually converges to the global minimum, while Lolo-BO does not. We repeated the test on the Rastrigin function with different dimensionalities and initial sample sizes, and tested functions with similar characteristics; the results are in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional Functions</head><p>Moving beyond low-dimensionality, we compare the two BO methods on a series of high-dimensional functions. We are optimizing the Perm function (whose behavior in 2D is shown in Figure <ref type="figure" target="#fig_4">3a</ref>):</p><formula xml:id="formula_11">f (v v v) = d ∑ i=1 d ∑ j=1 j i + 0.5 v j j i -1 2 ,<label>(9)</label></formula><p>the Rosenbrock function (whose behavior in 2D is shown in Figure <ref type="figure" target="#fig_4">3d</ref>):</p><formula xml:id="formula_12">f (v v v) = d ∑ i=1 100 v i+1 -v 2 i 2 + (v i -1) 2 ,<label>(10)</label></formula><p>and a simple quadratic function</p><formula xml:id="formula_13">f (v v v) = d ∑ i=1 v 2 i ,<label>(11)</label></formula><p>where d denotes the dimensionality.</p><p>For the Perm function, we used both low-and high-dimensional settings:  are that, in the 6D test case LVGP-BO and Lolo-BO show close efficiencies (Figure <ref type="figure" target="#fig_4">3b</ref>). Whereas in the 10D case, both BO methods have difficulty optimizing the function and get stuck for more than 20 iterations (Figure <ref type="figure" target="#fig_4">3c</ref>); Lolo-BO displays better convergence rate and final minimum objective value.</p><p>The Perm function is complex because of its non-convexity, and more significantly, its erratic behavior at the domain boundary: the function value is growing nearly exponentially near the boundary. We test BO of the 10D Rosenbrock and quadratic functions to investigate the influence of high dimensionality, without the erratic complexity. The Rosenbrock function is also non-convex, but at the domain boundary it is well-behaved. We define its input variables as following: numerical variables x 1,...,5 = v 1,...,5 ∈ [-5, 10], categorical variables </p><formula xml:id="formula_14">t 1 = v 6 ∈ {-4, 1, 6}, t 2 = v 7 ∈ {-3, 1, 5, 9}, t 3 = v 8 ∈ {-5, 1, 7}, t 4 = v 9 ∈ {-2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials Properties Optimization</head><p>To compare the performances of two ML-BO models in facilitating materials design, we apply the BO framework in Figure <ref type="figure" target="#fig_0">1</ref> to optimize materials' properties using several existing experimental/computational materials datasets. For each optimization task, we use the chemical composition as input variables. We start with a small fraction of samples randomly selected from the dataset; evaluation of a sample is done by querying its corresponding property from the dataset. Model fitting and acquisition function follow the same procedure described in previous sections. Since the materials' properties of interest are desired to be maximized, we use their negative values as the response, so that the optimization task is minimization of the response, consistent with the numerical examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moduli of M 2 AX Compounds</head><p>The M 2 AX compounds <ref type="bibr" target="#b39">40</ref> are a family of materials with a hexagonal crystal structure, in which M, A, and X represent different sites and each site is taken by one of the candidate elements. M 2 AX compounds display high stiffness and lubricity, as well as desirable resistance to oxidation and creeping resistance at high temperatures. These properties make them promising candidates as structural materials in extreme-condition applications such as aerospace engineering. <ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42</ref> Here we design for their elastic properties, which are important for both the capability as a structural material and the manufacturability. From Balachandran et al. <ref type="bibr" target="#b42">43</ref> , we retrieve a dataset that reports Young's, bulk, and shear moduli (E, B, and G, respectively) of 223 M 2 AX compounds within the chemical space M ∈ {Sc, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, W}, A ∈ {Al, Si, P, S, Ga, Ge, As, Cd, In, Sn, Tl, Pb}, X ∈ {C, N}. The input variable v v v is thus three-dimensional, with all inputs being categorical. Since E and G are highly correlated (see SI), we choose E and B as target responses and optimize them separately. Both optimizations start with 30 initial samples and run for 50 iterations, adding one sample per iteration.</p><p>In Figure <ref type="figure" target="#fig_6">4</ref>, we show the distributions and optimization histories for E and B. Both Lolo-BO and LVGP-BO are able to discover the material with optimal modulus; LVGP-BO exhibits marginally higher rates of convergence in both tasks. As observed from a-b, the input-response relations of both E and B are relatively well-behaved, without showing abrupt changes or clusters of values. Hence, the results here are consistent with the findings on numerical test cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bandgap and Stability of Lacunar Spinels</head><p>In another materials design test case, we consider materials having formula AM a M b 3 X 8 and the lacunar spinel crystal structure <ref type="bibr" target="#b43">44</ref> .</p><p>Element candidates for the sites are: A ∈ {Al, Ga, In}, M a ∈ {V, Nb, Ta, Cr, Mo, W}, M b ∈ {V, Nb, Ta, Mo, W}, X ∈ {S, Se, Te}. This is a family of materials that potentially exhibit metal-insulator transitions (MITs) <ref type="bibr" target="#b44">45</ref> , i.e., electrical resistivity changing significantly upon temperature change across a critical temperature. The MIT property is utilisable for encoding and decoding information with lower energy consumption compared to current devices. <ref type="bibr" target="#b45">46</ref> Hence, the AM a M b 3 X 8 materials family are promising materials for next-generation microelectronic devices. For these materials as candidates for MITs, two important properties are: (1) ground state bandgap, as it determines the tunability of a material's resistivity change ratio; and (2) stability, as it reflects a material's synthesizability.</p><p>In a dataset collected by Wang et al. <ref type="bibr" target="#b2">3</ref> , a total of 270 combinations of candidate elements are enumerated, for every compound the ground state bandgap </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and Discussions</head><p>In this section, we seek explanations for LVGP-BO and Lolo-BO's performance differences from two aspects: fitting accuracy and uncertainty estimation quality.</p><p>We select representative mathematical functions among the ones mentioned above, generate training samples following the same procedure for generating the initial samples as described in the earlier comparisons, and fit LVGP/Lolo models to the samples. In Figure <ref type="figure" target="#fig_8">5</ref> we show the behaviors of LVGP and Lolo in fitting the mixed-variable Branin function. With a small training set, LVGP can attain a better fitting for the function compared to Lolo; moreover, its uncertainty quantification assigns low uncertainty in the vicinity of known observations and high uncertainty in the regions where data are sparse. These enable well-directed sampling in the less explored regions, hence promoting the model to "learn" the target function efficiently (in (d), with 30 samples, LVGP fits the function nearly perfectly). Therefore, for low-dimensional simple functions, LVGP's capability We then extend this fitting and UQ comparison to high-dimensional cases. Training samples are generated from the 10-dimensional quadratic surface (Eq. 11) and Perm function (Eq. 9), respectively, then LVGP and Lolo models are tested to accurately fit a model to the samples. At high dimensions, visualization as previously is no longer feasible. Instead, we adopt the relative root-mean-square error (RRMSE)</p><formula xml:id="formula_15">RRMSE = RMSE σ y = ∑ i ( ŷi -y i ) 2 ∑ i (y i -ȳ) 2<label>(12)</label></formula><p>as a metric of fitting quality. For every function, we evaluate the model fitting quality by calculating RRMSE on 1,000 test samples generated independently with the training samples. Figure <ref type="figure" target="#fig_10">7</ref> shows the RRMSEs across different training sample sets to indicate how well the two models fit the mathematical functions. Note that RRMSE is related to another widely used metric, coefficient of determination R 2 , through R 2 = 1 -RRMSE 2 . RRMSE &gt; 1 can happen when the fitted model is no better than constantly predicting ȳ.</p><p>As Figure <ref type="figure" target="#fig_10">7a</ref> shows, in fitting the relatively simpler quadratic function, Lolo attains a higher quality compared to LVGP at a small sample size (50); as the sample size increases to 80, the fitting quality of LVGP improves significantly, whereas the fitting quality of Lolo does not change a lot. We also observe that at a larger training sample size, LVGP's RRMSE has a large variance across different random training sets. With well-directed uncertainty quantification, starting from a small set, LVGP-BO can add samples that improve the fitting, thus leading to efficient convergence.</p><p>However, in fitting the 10D Perm function, both functions fail to attain a good RRMSE; the Lolo model fits slightly better than LVGP, and this comparison is not changed as the training sample size increases. In this case, the dimensionality is too large for the known samples to cover, hence, it is difficult for both ML models to capture the complexity of the Perm function.</p><p>Lolo's slightly better fitting quality of the samples enables it to display higher efficiency in reducing the objective value in Bayesian Optimization of the Perm function.</p><p>In summary, for low-dimensional problems, the ML models are able to fit the input-response function relatively easily; even if the function is highly complex, fitting quality can be improved by adding a small number of well-selected samples. In this case, the quality of UQ becomes the key factor of BO efficiency. Whereas for high-dimensional problems, if the function is complex, it becomes challenging for ML models to fit the function. The number of samples required for covering the input space and improving fitting quality also escalates due to the curse of dimensionality. In this case, better fitting leads to better BO performance, and ML models that are more capable of fitting complex functions (such as Random Forest) have advantages.</p><p>The results and analyses draw a suitability boundary for LVGP-BO and Lolo-BO, and more generally, provide insights for understanding the difference between the two families of uncertainty-aware ML models they represent: • When the design optimization problem is low-dimensional, or high-dimensional but the response is anticipated to be relatively well-behaved, LVGP model is recommended for BO.</p><p>• While for high-dimensional problems with a highly ill-behaved response function, we recommend using a ML model that allow higher model complexity (e.g., Random Forest, Neural Network) with resampling UQ.</p><p>The results constitute a supplement to the previous studies covering BO with all numerical variables and guide the model selection in materials design as well as other mixed-variable BO problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Optimization</head><p>The optimization process starts with initial samples, i.e., an initial set of input variables, and evaluates the responses. A machine learning model is then fitted with the known input-response data, which assigns for any input v v v a mean prediction ŷ(v v v) and associated uncertainty (predicted variance) ŝ2 (v v v). A new sample is selected based on the expected improvement (EI) acquisition function <ref type="bibr" target="#b12">13</ref> :</p><formula xml:id="formula_16">v v v * = arg max v v v EI(v v v), (<label>13</label></formula><formula xml:id="formula_17">) EI(v v v) = E[max{0, ∆(v v v)}] = ŝ(v v v)φ ∆(v v v) ŝ(v v v) + ∆(v v v)Φ ∆(v v v) ŝ(v v v) ,<label>(14)</label></formula><p>where ∆(v v v) = y miny(v v v), the difference between the minimal response value observed so far and the mean prediction of fitted ML model; φ (•) and Φ(•) are the standard normal probability density function (pdf ) and cumulative distribution function (cdf ), respectively. The new sample and corresponding response value are added to the known dataset. This process is repeated iteratively, until the maximum number of iterations or some convergence criterion is reached.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Schematic of Bayesian Optimization framework.</figDesc><graphic coords="7,106.55,63.78,398.89,202.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2a. Plugging Lolo and LVGP into the aforementioned framework, we run Bayesian Optimization to minimize the modified Branin function, starting with 10 initial samples. We repeat this for 30 replicates with different random seeds (i.e., different random initial designs) for each replicate, and the optimization histories across replicates are shown in Figure 2e. To compare the overall performance and robustness of LVGP-BO and Lolo-BO, we show both the median objective value ỹ and the scaled median absolute deviation MAD = median (|y -ỹ|) /0.6745 at every iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. a-d. Visualization Branin, McCormick, Camel, and Rastrigin functions in 2-dimensional (2D) continuous form. e-h. Optimization histories across replicates for Branin, McCormick, Camel, and Rastrigin functions. These plots show the minimal objective function value observed at every iteration. Solid lines represent the median among replicates; shaded areas show plus/minus one scaled median absolute deviation (MAD). The green dashed lines mark the global minimum value of each function.</figDesc><graphic coords="8,56.69,127.80,498.61,219.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 ) 6 -</head><label>16</label><figDesc>dimensional (6D), with one categorical variable t = v 6 ∈ {-4, 1, 6} and numerical variables x 1,...,5 = v 1,...,5 ∈ [-6, 6]. In this case, D f = 7. (2) 10-dimensional (10D), with five categorical variables t 1= v 6 ∈ {-4, 1, 6}, t 2 = v 7 ∈ {-8, -3, 2, 7}, t 3 = v 8 ∈ {-6, 1, 8}, t 4 = v 9 ∈ {±3, ±9},t 5 = v 10 ∈ {0, ±5, ±10}, and numerical variables x 1,...,5 = v 1,...,5 ∈ [-10, 10]. D f = 19 for this function. In view of the computing time, we run 10 replicates for each test, starting from initial samples of size 20 for 6D and 50 for 10D. Observations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. a. Visualization of Perm function in 2D continuous form. b-c. Optimization histories for 6D and 10D Perm functions. d. Visualization of Rosenbrock function. e. Optimization history for 10D Rosenbrock function. f. Optimization history for 10D quadratic function.</figDesc><graphic coords="10,106.55,63.78,398.89,215.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 , 4 ,</head><label>14</label><figDesc>7}, t 5 = v 10 ∈ {±3, ±1, 5}, which make D f = 19. The quadratic function is convex and well-behaved.It takes five categorical variables t 1,...,5 = v 6,...,10 ∈ {0, ±1, ±2} and five numerical variables x 1,...,5 = v 1,...,5 ∈ [-2, 2], with D f = 25. As Figure3e-f shows, both BO methods make progress in descending the function value towards the optimum, while LVGP-BO has a considerably faster convergence rate. Through these, we find that when the dimensionality of the problem is high, convexity influence the comparison between LVGP-BO and Lolo-BO similarly to the low-dimensional situation. For the three 10D functions, Lolo-BO displays consistent behavior of making slow progress. At high dimensions, when the function is not well-behaved near the domain boundary, the efficiency of LVGP-BO decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. a-b. Distributions of E and B values in the M-A space, fixing X = C. c-d. Optimization histories for c. Young's modulus and d. bulk modulus. e-g. Optimization histories of E g with initial sample sizes 10 and 30, and ∆H d with initial sample size 10. h. Scatter plot of bandgap-stability for all samples in the dataset. Note that in c-g, the objective function values are the negative values of the properties, while in h the original values are used.</figDesc><graphic coords="11,56.69,460.08,498.63,210.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>E g and stability (decomposition enthalpy change, ∆H d ) calculated from density functional theory (DFT) are listed. Similar to the previous test case, we use the four-dimensional (categorical) composition as the inputs and optimize two responses E g and ∆H d separately. Figure 4e-g show the test results: starting from 10 initial samples, LVGP-BO and Lolo-BO both discover the compound with optimal ∆H d efficiently, but are relatively slow in optimizing E g ; LVGP-BO shows better efficiency on ∆H d while Lolo-BO shows better efficiency on E g . When we increase the initial sample size to 30, LVGP-BO and Lolo-BO exhibit similar efficiency on E g . We show the different characteristics of the two responses of the dataset by a scatter plot in Figure 4h. Among the 270 E g values in the dataset, 56 are zero and others are positive values. These values form a clustered distribution at 0 and make the target function E g = f (v v v) ill-behaved. Combined with the high-dimensionality, this function becomes challenging for LVGP-BO and Lolo-BO to optimize, as we demonstrated in the high-dimensional numerical examples. In contrast, ∆H d values form a relatively well-behaved target function, hence, LVGP-BO performs better on this task, also in agreement with previous findings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of the behaviors of LVGP and Lolo fitting the Branin function, with varying numbers of training samples. Each panel plots f (x,t) versus x for four fixed values of t, while different colors of curves indicate levels of t. Solid lines represent the true function value, black dots are sample points in the training set, dashed lines are the predicted mean value, and shaded areas show the uncertainty estimation (plus/minus one standard deviation).</figDesc><graphic coords="13,106.55,63.78,398.88,218.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Illustration of the behaviors of LVGP and Lolo fitting the 2D Rastrigin function, with varying numbers of training samples. For clarity, we only show three levels of categorical variable t out of eleven in total.</figDesc><graphic coords="14,106.55,63.78,398.90,220.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Relative RMSE of fitting quadratic function (a) and Perm function (b), using LVGP and Lolo models with varying training sample sizes. Boxplots show results from 10 different randomly selected training sample sets.</figDesc><graphic coords="15,106.55,63.78,398.90,137.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by the <rs type="funder">Advanced Research Projects Agency</rs>-<rs type="programName">Energy (ARPA-E)</rs>, <rs type="funder">U.S. Department of Energy</rs>, under Grant Number <rs type="grantNumber">DE-AR0001209</rs>. The authors thank <rs type="person">Bryan Horn</rs> for conducting some experiments, and <rs type="person">Suraj Yerramilli</rs> for helpful discussions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gRYyGDt">
					<orgName type="program" subtype="full">Energy (ARPA-E)</orgName>
				</org>
				<org type="funding" xml:id="_vX8uDmx">
					<idno type="grant-number">DE-AR0001209</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability</head><p>The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparative Experiments</head><p>To compare the performances of Lolo and LVGP, we substitute them as they "ML Model" into the framework and run BO for a variety of functions, each time keeping the initial designs the same for BO with Lolo and LVGP. The initial designs are generated quasi-randomly following a systematic approach: numerical variables are drawn together from a Sobol sequence <ref type="bibr" target="#b46">47</ref> ; each categorical variable is obtained from shuffling a list where all categories appear equally frequently and at least once. Since the stochasticity of initial samples influences the optimization process, we run multiple replicates of BO with different random seeds for each test problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty-Aware ML models</head><p>In these comparisons, we use the open-source implementation of Lolo <ref type="bibr" target="#b47">48</ref> in Scala language with the Python wrapper lolopy, and a MATLAB implementation of LVGP, which implements the same algorithm as the open-source package coded in R <ref type="bibr" target="#b48">49</ref> . For hyperparameters of both models, we use the default settings: For Lolo, the maximum number of trees is set to the number of data points, the maximum depth of trees is 2 <ref type="bibr" target="#b29">30</ref> , and the minimum number of instances in the leaf is 1. For LVGP, we use 2-dimensional latent variable mapping and the RBF kernel. Detailed settings are listed in the open-source packages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics for Problems Difficulty</head><p>We specify the following metrics to characterize the test problems and BO methods' performance. The dimensionality of inputs is an important criterion of problem difficulty. However, in categorical or mixed-variable cases, dimensionality is more than the number of variables. A more useful index considered in this work is the degrees of freedom, which we define as</p><p>where #levels(t i ) yields the number of levels of t i . This quantity takes into account the number of levels for each categorical variable. In other words, high dimensionality may mean "many levels" in problems with categorical variables. In the following sections, we follow this definition to categorize problems with D f &gt; 15 as high-dimensional, and others as low-dimensional.</p><p>Another criterion of difficulty is the complexity of objective function. In this work, we view the functions that display the following characteristics as ill-behaved:</p><p>• rugged: the response fluctuates a lot, resulting in many local minima;</p><p>• erratic: the response value changes abruptly in certain regions;</p><p>• clustered: the response takes certain values frequently.</p><p>These characteristics make a function challenging for ML and global optimization, hence, we refer to them as "complex" functions. Conversely, other well-behaved functions, including highly nonlinear ones, are referred to as "simple" functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional information</head><p>Competing interests The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Files</head><p>This is a list of supplementary les associated with this preprint. Click to download.</p><p>SupplementaryInformation.pdf</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverse design of solid-state materials via a continuous representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.matt.2019.08.017</idno>
	</analytic>
	<monogr>
		<title level="j">Matter</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1384" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data centric nanocomposites design via mixed-variable bayesian optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<idno type="DOI">10.1039/D0ME00079E</idno>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Des. &amp; Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1376" to="1390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Featureless adaptive optimization accelerates functional electronic materials design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rondinelli</surname></persName>
		</author>
		<idno type="DOI">10.1063/5.0018811</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">41403</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficiently exploiting process-structure-property relationships in material design by multi-information source fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Khatamsaz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actamat.2020.116619</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Materialia</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">116619</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Turning statistical physics models into materials design engines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Miskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>De Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jaeger</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1509316112</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural-network-biased genetic algorithms for materials design: Evolutionary algorithms that learn</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Meenakshisundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Simmons</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscombsci.6b00136</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Comb. Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pollice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1039/D2DD00003B</idno>
	</analytic>
	<monogr>
		<title level="j">Digit. Discov</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2015.2494218</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerated discovery of large electrostrains in batio3-based piezoelectrics using active learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1002/adma.201702884</idno>
	</analytic>
	<monogr>
		<title level="j">Adv. Mater</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active learning across intermetallics to guide discovery of electrocatalysts for co2 reduction and h2 evolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Ulissi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41929-018-0142-1</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Catal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="696" to="703" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On-the-fly closed-loop materials discovery via bayesian active learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Kusne</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-19597-w</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autonomous efficient experiment design for materials discovery with bayesian model averaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Talapatra</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevMaterials.2.113803</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">113803</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient global optimization of expensive black-box functions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1008306431147</idno>
	</analytic>
	<monogr>
		<title level="j">J. Glob. Optim</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="455" to="492" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantification of model uncertainty: Calibration, model discrepancy, and identifiability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1115/1.4007390</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mech. Des</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">100908</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uncertainty prediction for machine learning models of material properties</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Decost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choudhary</surname></persName>
		</author>
		<idno type="DOI">10.1021/acsomega.1c03752</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Omega</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="32431" to="32440" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uncertainty quantification of dft-predicted finite temperature thermodynamic properties within the debye model</title>
		<author>
			<persName><forename type="first">P.-W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Houchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Viswanathan</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.5132332</idno>
	</analytic>
	<monogr>
		<title level="j">The J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">244702</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncertainty quantification and reduction in metal additive manufacturing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41524-020-00444-x</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Concurrent treatment of parametric uncertainty and metamodeling uncertainty in robust design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arendt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00158-012-0805-5</idno>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optim</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="63" to="76" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Role of uncertainty estimation in accelerating materials development via active learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Phys</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">14103</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6405" to="6416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Theory of disagreement-based active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="131" to="309" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aleatoric and epistemic uncertainty with random forests</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="444" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantifying uncertainty in random forests via confidence intervals and hypothesis tests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="841" to="881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty in gradient boosting via ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ustimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of uncertainty quantification in deep learning: Techniques, applications and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2021.05.008</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="243" to="297" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in cnn through the bootstrap of convex neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artif. Intell</title>
		<meeting>AAAI Conf. on Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12078" to="12085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uncertainty quantification using neural networks for molecular property prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hirschfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="3770" to="3780" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A bayesian approach for quantification of model uncertainty</title>
		<author>
			<persName><forename type="first">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Amarchinta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Grandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. &amp; Syst. Saf</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="777" to="785" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uncertainty quantification using bayesian neural networks in classification: Application to biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Paik</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2019.106816</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Analysis</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page">106816</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Challenges in markov chain monte carlo for bayesian neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Papamarkou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hinkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Womble</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.06539</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Overview of Supervised Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="9" to="41" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gryffin: An algorithm for bayesian optimization of categorical variables informed by expert knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Häse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aldeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Roch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1063/5.0048164</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">31406</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-dimensional materials and process optimization using data-driven experimental design with well-calibrated uncertainty estimates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40192-017-0098-z</idno>
	</analytic>
	<monogr>
		<title level="j">Integrating Mater. Manuf. Innov</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="207" to="217" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A latent variable approach to gaussian process modeling with qualitative and quantitative factors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<idno type="DOI">10.1080/00401706.2019.1638834</idno>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="291" to="302" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian optimization for materials design with mixed quantitative and qualitative variables</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-60652-9</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">4924</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable gaussian processes for data-driven design using big data with categorical factors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1115/1.4052221</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mech. Des</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Benchmarking the performance of bayesian optimization across multiple experimental materials science domains</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41524-021-00656-9</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Virtual library of simulation experiments: Test functions and datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Surjanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bingham</surname></persName>
		</author>
		<ptr target="http://www.sfu.ca/~ssurjano" />
		<imprint>
			<date type="published" when="2013">January 24, 2022. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The mn+1axn phases: A new class of solids: Thermodynamically stable nanolaminates</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Barsoum</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0079-6786(00)00006-6</idno>
		<ptr target="https://doi.org/10.1016/S0079-6786(00)00006-6" />
	</analytic>
	<monogr>
		<title level="j">Prog. Solid State Chem</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="201" to="281" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Elastic and electronic properties of select m2ax phases</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Lofland</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.1641177</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Phys. Lett</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="508" to="510" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comprehensive survey of m2ax phase elastic properties</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Warschkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M M</forename><surname>Bilek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mckenzie</surname></persName>
		</author>
		<idno type="DOI">10.1088/0953-8984/21/30/305403</idno>
	</analytic>
	<monogr>
		<title level="j">J. Physics: Condens. Matter</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">305403</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive strategies for materials design using uncertainties</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lookman</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep19660</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016">19660. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling the structural distortion and magnetic ground state of the polar lacunar spinel gav 4 se 8</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Schueller</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevB.100.045131</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">45131</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metal-insulator transitions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Imada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujimori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tokura</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.70.1039</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1039" to="1263" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A steep-slope transistor based on abrupt electronic phase transition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shukla</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms8812</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the distribution of points in a cube and the approximate evaluation of integrals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sobol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Comput. Math. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="86" to="112" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<ptr target="https://github.com/CitrineInformatics/lolo" />
	</analytic>
	<monogr>
		<title level="j">Citrine Informatics. Lolo machine learning library</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lvgp</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=LVGP" />
		<title level="m">Latent variable gaussian process modeling with qualitative and quantitative input variables</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
