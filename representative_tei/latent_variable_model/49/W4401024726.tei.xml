<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wearable Sensor-Based Few-Shot Continual Learning on Hand Gestures for Motor-Impaired Individuals via Latent Embedding Exploitation</title>
				<funder ref="#_vDaNCVX">
					<orgName type="full">Eunice Kennedy Shriver National Institute of Child Health &amp; Human Development of the National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-11">11 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Riyad</forename><forename type="middle">Bin</forename><surname>Rafiq</surname></persName>
							<email>riyadbinrafiq@my.unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weishi</forename><surname>Shi</surname></persName>
							<email>weishi.shi@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">V</forename><surname>Albert</surname></persName>
							<email>mark.albert@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Wearable Sensor-Based Few-Shot Continual Learning on Hand Gestures for Motor-Impaired Individuals via Latent Embedding Exploitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-11">11 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.08969v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand gestures can provide a natural means of human-computer interaction and enable people who cannot speak to communicate efficiently. Existing hand gesture recognition methods heavily depend on pre-defined gestures, however, motorimpaired individuals require new gestures tailored to each individual's gesture motion and style. Gesture samples collected from different persons have distribution shifts due to their health conditions, the severity of the disability, motion patterns of the arms, etc.</p><p>In this paper, we introduce the Latent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot Continual Learning (FSCL) framework that significantly improves the performance of finetuning a model for out-of-distribution data. Our method produces a diversified latent feature space by leveraging a preserved latent embedding known as gesture prior knowledge, along with intragesture divergence derived from two additional embeddings. Thus, the model can capture latent statistical structure in highly variable gestures with limited samples. We conduct an experimental evaluation using the SmartWatch Gesture and the Motion Gesture datasets. The proposed method results in an average test accuracy of 57.0%, 64.6%, and 69.3% by using one, three, and five samples for six different gestures.</p><p>Our method helps motor-impaired persons leverage wearable devices, and their unique styles of movement can be learned and applied in humancomputer interaction and social communication.</p><p>Code is available at: <ref type="url" target="https://github.com/riyadRafiq/">https://github.com/riyadRafiq/</ref> wearable-latent-embedding-exploitation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hand gestures are a flexible and intuitive means of communication for human beings. With the advancement of wearable sensors and machine learning, gesture recognition has become quite popular for communication, smart home appliances, interactive entertainment, etc. <ref type="bibr" target="#b0">[Rafiq et al., 2023;</ref><ref type="bibr" target="#b1">Guo et al., 2021]</ref>. Gesture-based interactions with wearables depend on specific presumptions about users' motor abilities. As a consequence, people with motor impairments face challenges in performing gestures with wearables that are widely adopted for the general public <ref type="bibr" target="#b6">[Siean and Vatavu, 2021]</ref>. The severity of motor impairments leads to a different pattern of motion gestures and creates individual differences among the users <ref type="bibr" target="#b7">[Vatavu and Ungurean, 2022]</ref>. It is social discrimination for this underrepresented population as it deprives them of completely leveraging those wearable devices. As the United Nations Sustainable Development Principle is Leave no one behind, increasing independence and including people with disabilities aid in achieving UN Sustainable Development Goals Good health and well-being and Reduce inequalities <ref type="bibr" target="#b7">[Yu et al., 2023]</ref>.</p><p>To tackle the problem, a large-scale labeled dataset is expected to build a robust hand gesture recognition method. However, this is impractical and cumbersome for motorimpaired individuals to participate in vast data collection. The transfer learning approach has been used to solve the problem. In transfer learning, a model is trained on a source domain and then fine-tuned to a target domain by transferring knowledge from the prior learned task <ref type="bibr">[Zhuang et al., 2020]</ref>. But fine-tuning shows worse performance in a target domain with out-of-distribution samples <ref type="bibr">[Kumar et al., 2022]</ref>. Therefore, applying transfer learning alone cannot solve the problem, as the gesture data from the motorimpaired population are more variable and noisy than the control population (Figure <ref type="figure">1</ref>), and limited data samples might not help the deep learning model capture the diverse patterns among each individual. To utilize limited training data, a unique approach has been proposed <ref type="bibr">[Finn et al., 2017]</ref> and in our case, few-shot transfer learning is an applicable solution.</p><p>In our case, another real-world problem is that all the gesture classes may not be available initially. For example, standard pre-defined gestures can be difficult to perform for individuals lacking fine motor skills. New unseen gestures may become accessible incrementally if motorimpaired individuals want to input their flexible and custom gestures. This context is referred to as a continual learning setting as the model involves learning a disjoint set of classes incrementally <ref type="bibr" target="#b4">[Parisi et al., 2019]</ref>. Continual learning posts two challenges, namely catastrophic forgetting <ref type="bibr" target="#b1">[French, 1999]</ref> when the model's performance drops drastically on old classes and overfitting when the model is not capable of learning generalized features with a few training examples <ref type="bibr">[Gidaris and Komodakis, 2018]</ref>.</p><p>Many continual learning approaches including parameter regularization, functional regularization, replay strategy, etc. have become popular at present [ <ref type="bibr" target="#b7">Van de Ven and Tolias, 2019]</ref>. In this paper, we propose a novel method called Latent Embedding Exploitation (LEE) in our replay-based few-shot continual learning framework that can learn gesture classes incrementally from motor-impaired people. Specifically, in our framework, we utilize three latent embeddings from the feature extractor of a pre-trained model which is trained on the control subjects' gesture samples. The three embeddings are:</p><p>• a preserved latent embedding works as gesture prior knowledge,</p><p>• two additional latent embeddings known as temporary and learned embedding maintain a intra-gesture divergence.</p><p>They jointly aid a pre-trained model to be fine-tuned effectively with a few training examples from a motorimpaired individual. Ideally, the goal of LEE is to navigate the learned feature space toward a rich and diversified feature representation for variable and noisy data. Thus the finetuned model can capture the diverse pattern of unseen gesture classes with a few training examples. As a result, motorimpaired people can take full advantage of wearable devices with our proposed method. The major contributions of this paper are as follows:</p><p>• We explore wearable sensor-based hand gestures from the underrepresented population.</p><p>In addition, we introduce the LEE mechanism in our replay-based fewshot continual learning framework that formulates the diverse gesture samples into a heterogeneous feature representation. Hence, the pre-trained model can be fine-tuned competently with a few training samples for each unseen class in a continual learning setup.</p><p>• We utilize two publicly available gesture datasets to demonstrate the performance of the proposed method.</p><p>Our proposed method achieves competitive performance compared to existing methods. • We experimentally show how latent embeddings can be leveraged to improve the performance of fine-tuning in the limited data of shifted distribution.  <ref type="bibr" target="#b3">[Kunwar et al., 2022;</ref><ref type="bibr" target="#b4">Nguyen-Trong et al., 2021;</ref><ref type="bibr">Li et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The goal of few-shot continual learning is to train new classes incrementally with few data instances. In order to tackle few-shot learning problem, metric-learning [Kaya and Bilge, 2019] , meta-learning <ref type="bibr">[Finn et al., 2017]</ref> and multitask learning [Zhang and Yang, 2021] have been proposed. In the field of hand gesture recognition, camera data <ref type="bibr" target="#b7">[Wu et al., 2012;</ref><ref type="bibr">Stewart et al., 2020] and</ref><ref type="bibr">EMG signals [Rahimian et al., 2021]</ref> have been used for few-shot learning. However, <ref type="bibr" target="#b7">Xu et al. [2022]</ref> proposed a hand gesture customization framework that can learn novel hand gesture classes incrementally with a few training examples. <ref type="bibr" target="#b3">Kimura [2022]</ref> also proposed a self-supervised method for few-shot hand gesture recognition using wearable sensor data. However, they used only control participants' data. In addition, these methods would not work for motor-impaired individuals as the data instances are varying and noisy. Although <ref type="bibr">Malu et al. [2018]</ref> and <ref type="bibr" target="#b2">Kim et al. [2019]</ref> explored the smartwatch interactions for people with upper body motor impairments, they experimented with the touch gestures only. The primary difference between our approach and existing works is that they did not consider the out-of-distribution samples for motion gestures. Directly fine-tuning a pre-trained model may cause a sudden performance decay. The objective of our method is to adapt a model that generates an enhanced feature representation via gesture prior knowledge exploitation and intra-gesture divergence exploration to incrementally learn novel gestures with few training examples from motor-impaired individuals.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>A domain is defined as a joint probability distribution P x,y on X × Y, where X and Y denote the instance space and label space, respectively <ref type="bibr">[Ding and Fu, 2017;</ref><ref type="bibr">Qian et al., 2021]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>Sensor readings from motor-impaired individuals vary substantially due to factors such as health conditions, severity of disability, movement patterns of arms, etc. (Figure <ref type="figure">1b</ref>). Therefore, it is critical for a pre-trained model to adapt to such data samples from sensor readings. Machine learning models without considering out-of-distribution data often result in large performance degradation. For example, a model trained on the data from control participants often fails to capture unique patterns and performs poorly on specific populations such as Parkinson's patients <ref type="bibr">[Bin Rafiq et al., 2020]</ref>. It is difficult to collect a large volume of diverse labeled data from motor-impaired individuals. Moreover, individuals may need their own, custom flexible gestures from time to time in human-computer interaction and social communication.</p><p>In this paper, we propose a novel technique where the model learns new gesture classes incrementally by utilizing multifunctional latent embeddings. Our method considers three latent embeddings instead of a single representation compared to <ref type="bibr" target="#b0">Autoencoders [Bank et al., 2020]</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, a latent embedding from the control population is preserved by leveraging the feature extractor (deep encoder) of a pre-trained model. This preserved latent embedding works as gesture prior knowledge to assist the model to incrementally learn unseen out-of-distribution gesture samples and prevent overfitting. Two additional identical feature extractors are utilized to produce two latent embeddings with available gesture classes from the motorimpaired subjects, and one of them is being updated during training. As a consequence, the learned latent embedding has a strong capability in classifying highly variable data during inference. The total loss of the model in weighted summation is as follows:</p><formula xml:id="formula_0">L = αL ci + βL ii + L cls (1)</formula><p>where L ci is the loss of discrimination between preserved and learned embedding, L ii is the loss of discrimination between temporary and learned embedding and L cls is the classification loss. α and β are trade-off hyper-parameters where α + β = 1. While minimizing the loss in the training stage, the model exhibits complementary learning behavior by adaptively adjusting its focus between exploitative and explorative representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complementary Learning Paradigm</head><p>The complementary learning system plays an important role in the human brain where the hippocampus and the neocortex function in a complementary manner to learn complex behavior <ref type="bibr" target="#b5">[Perrusquía, 2022;</ref><ref type="bibr">Blakeman and Mareschal, 2020]</ref>. Our learning strategy for new classes is inspired by this complementary learning paradigm. The representation space holds the same classes closer under the effect of classification objective while training a deep learning model. However, the model struggles to learn a robust latent space with fewer training examples from out-of-distribution data <ref type="bibr">[Yang et al., 2021]</ref>. Therefore, we model the representation space generation process by utilizing both the classification and the embedding discrimination objectives. In our method, we introduce three multipurpose latent embeddings including preserved control embedding, temporary sample embedding, and learned embedding. The preserved control embedding contains the representation space of the expected pattern of gestures from the source domain. The learned embedding constructs a latent statistical structure with the help of preserved and temporary embedding. Thus, the constructed latent space sufficiently narrows the features of the sameclass data with limited training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Embedding Exploitation</head><p>The learned latent embedding exploits the preserved latent embedding (gesture prior knowledge) to enlarge and diversify the feature space. The network architecture is expected to increase the similarity of the feature space between the preserved and the learned embedding. We utilize the feature extractor, f θ (deep encoder), from the pre-trained model to preserve a latent embedding, z c = f θ (X s ) where X s is the feature space from the source domain i.e. control participants. The input of our model is denoted as x. In our continual learning module, two additional identical feature extractors, f θ and f c θ draw out temporary latent embedding, z i = f θ (x) and learned latent embedding, z c i = f c θ (x) respectively. To this end, we require to expand the similarity between z c and z c i as the following loss:</p><formula xml:id="formula_1">L ci (f c θ ; x, z c ) = 1 -S c (z c , f c θ (x))<label>(2)</label></formula><p>where S c is the cosine similarity between z c and f c θ (x) and it can be defined as follows:</p><formula xml:id="formula_2">S c (z c , z c i ) = z c • z c i ||z c ||||z c i ||<label>(3)</label></formula><p>Latent Embedding Exploration Simultaneously, the learned latent embedding aims to maximize the distance from the identical temporary sample embedding throughout the training which is called intragesture divergence. This action works as a way of exploration for a wide feature space. As a result, the learned latent embedding captures tailored and generalizable feature representation to learn novel gesture classes. To minimize the similarity between z i and z c i is identical as follows:</p><formula xml:id="formula_3">L ii (f θ , f c θ ; x) = S c (f θ (x), f c θ (x))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Objective</head><p>The learning objective of the model is to identify the gesture classes which is a transformation of the input sensor signals to a gesture category. Therefore, we utilize class labels in the final classification layer to guide the learned latent embedding during the training stage. We adopt standard cross-entropy loss for the classification task:</p><formula xml:id="formula_4">L cls (f c θ ; X t , Y t ) = -E (x,y)∈X t ×Y t C c=1 y log δ c (f c θ (x)) (5)</formula><p>where C represents the number of classes, y is the true gesture label, δ c (f c θ (x)) is the predicted probability and δ c is the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The SmartWatch Gesture <ref type="bibr">Dataset [Porzi et al., 2013;</ref><ref type="bibr" target="#b0">Costante et al., 2014]</ref> was built for interacting with mobile applications using arm gestures. This dataset contains 20 distinct gestures from eight different subjects.</p><p>A first-generation Sony smartwatch with a built-in 3-axis accelerometer was worn on the user's right wrist while performing 20 repetitions for each gesture. In total, 3200 sequences were collected and each sequence contains 3-axis acceleration data. We use this dataset as our source domain to build the pre-trained model. The Motion Gesture <ref type="bibr">Dataset [Vatavu and Ungurean, 2022]</ref> was built to understand the gesture articulation of people with upper-body motor impairments. Six different motion gestures were collected by a group of 12 people (six male and six female) with upper-body motor impairments, ranging ages from 27 to 65 years. The participants had a wide range of disabilities including Spinal cord injury, Traumatic brain injury, Multiple sclerosis, Parkinson's disease, etc. A Samsung Gear Fit 2 smartwatch was used by the participants to collect the wrist gesture's accelerometer data. Each participant repeated each gesture eight times. We utilize this dataset in our few-shot continual learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The sequence length of the data samples varies extensively. As a result, we apply a linear interpolation technique to our source and target datasets so that the sequence length (L = 50) of each data sample is constant throughout the experiments. While working with the sensor data, outlier features can negatively influence the results. Therefore, dataset standardization is conducted by removing the mean and scaling it according to the interquartile range for each feature. We select 16 out of 20 gestures from the source domain to build a pre-trained model because we do not want any overlapped gestures between the source domain and the target domain. We follow the leave-one-subject-out strategy to pre-train the model. Throughout all experiments, we used the same subjects for a fair comparison. For our architecture, the feature extractor contains one LSTM layer with 64-dimensional hidden representation, one fully connected layer with 14 units, and one dropout layer with a value of 0.5 between them. A fully connected layer is used as the classification layer. The network architecture remains the same throughout all experiments. The Adam optimizer with learning rate 10 -3 is used for the few-shot setup. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Compared Methods</head><p>We compare our methods with different closely related approaches in the few-shot continual learning setting. Since our method utilizes LSTM in the network architecture, we consider comparing it with an LSTM classifier that learns the gesture classes incrementally with a few training examples. Vanilla-Ft, MAML-Ft <ref type="bibr">[Finn et al., 2017]</ref>, and Prototypical Net-Ft <ref type="bibr" target="#b6">[Snell et al., 2017]</ref> involve fine-tuning the pre-trained models on the few-shot classes. We also compare our method with iCaRL <ref type="bibr" target="#b5">[Rebuffi et al., 2017]</ref>. We assume that the memory buffer exists in all methods for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>We compare the average accuracy of the proposed method with other approaches. Figure <ref type="figure" target="#fig_2">3</ref> shows the test accuracies for three participants including a motor-impaired individual with Spinal cord injury, an individual with Parkinson's disease, and an individual with Multiple sclerosis. In most cases, our LEE method outperforms other techniques. We observe that the iCaRL classifier occasionally shows better accuracy than our method while learning two initial gestures. But</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesture classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM</head><p>Vanilla-Ft MAML-Ft Prototypical Net-Ft iCaRL Ours-LEE Gesture 1 0.44 ± 0.08 0.31 ± 0.07 0.13 ± 0.10 0.24 ± 0.09 0.01 ± 0.02 0.53 ± 0.19 Gesture 2 0.21 ± 0.11 0.16 ± 0.05 0.12 ± 0.08 0.10 ± 0.04 0.04 ± 0.08 0.28 ± 0.13 Gesture 3 0.35 ± 0.15 0.40 ± 0.07 0.12 ± 0.07 0.32 ± 0.05 0.16 ± 0.15 0.58 ± 0.17 Gesture 4 0.34 ± 0.17 0.28 ± 0.10 0.13 ± 0.08 0.22 ± 0.11 0.01 ± 0.02 0.52 ± 0.20 Gesture 5 0.40 ± 0.18 0.28 ± 0.19 0.15 ± 0.03 0.21 ± 0.12 0.30 ± 0.17 0.53 ± 0.24 Gesture 6 0.29 ± 0.14 0.28 ± 0.14 0.10 ± 0.13 0.21 ± 0.13 0.35 ± 0.15 0.43 ± 0.14  Surprisingly, the fine-tuning approach fails compared to the basic LSTM classifier and our LEE.</p><p>In a continual learning setup, it is important to perform well in old classes while trained on a new class. Therefore, in Table <ref type="table" target="#tab_2">1</ref>, we report the class-wise F1 scores after six gestures are trained with five training examples. Our method always provides a higher macro F1 score than other methods. LEE has a 12.3% higher F1 score for all gesture classes. Figure <ref type="figure" target="#fig_4">4</ref> shows performance and forgetting score for Gesture 1 and Gesture 3 after six gestures are trained with five samples. Our LEE method has better performance with less forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Loss Hyperparameter Sensitivity Analysis</head><p>We report the effect of loss hyperparameter sensitivity. We focus only on α as it complements the other hyperparameter (β) in our continual learning module. We choose α to exploit the gesture prior knowledge, selected from α ∈{0.01, 0.05, 0.5, 0.1, 0.9}. According to Figure <ref type="figure" target="#fig_5">5</ref> (left), LEE provides robust accuracy with a wide range of hyperparameters after learning six gestures using five training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Number of Participants and Gestures in Source Domain</head><p>We experiment to produce the preserved latent embedding with a different number of participants from the source domain.</p><p>We achieve higher accuracy with one and three samples using seven different participants (Figure <ref type="figure" target="#fig_5">5</ref> middle). Apart from this, the proposed method is invariant to the number of participants from the source domain. However, it is preferable to utilize a large number of control participants in the source domain to capture a more diversified representation space. We also conduct experiments with a different number of gestures from the source domain to generate the preserved latent embedding (Figure <ref type="figure" target="#fig_5">5</ref> right). Though we get the highest accuracy using 16 gestures, we observe that the accuracy does not change for other different numbers of gestures. Therefore, the observation illustrates that our method is robust to the number of gesture classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Significance of Embeddings</head><p>We conduct experiments to investigate how the preserved latent embedding and the temporary latent embedding contribute to our proposed method. We apply LEE without one of those embeddings, one at a time, and evaluate the performance. Figure <ref type="figure" target="#fig_6">6</ref> (left) and (middle) show that removing either component of interest results in a less tridiagonal-shaped confusion matrix, indicating a drop in model performance and robustness. We further confirm from Figure <ref type="figure" target="#fig_6">6</ref> (right) that both embeddings jointly contribute to robust performance, and thus these embeddings are the foundations for learning unseen gestures incrementally with limited training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Use Cases and Social Impact</head><p>Wearable sensor-based gesture recognition is becoming popular in many areas including communication, controlling home appliances, and interactive entertainment. As most  research doesn't include the motor-impaired population, those individuals face challenges using wearable devices for gesture recognition and communication. We believe our contribution can be impactful for those who need more than a set of pre-defined gestures. Gesture recognition exists for standard movements such as sign language for speechimpaired people but sign language can be difficult to perform for individuals lacking fine motor skills. Our work is part of a fast and flexible gesture-to-speech recognition system that we are developing in collaboration with Shirley Ryan AbilityLab 1 . The need for such solutions is underscored by the prevalence of motor impairments that also impact speech. 12.1% of the population has a motor disability <ref type="bibr">[CDC, 2023]</ref> while 7.6% have a speech disorder <ref type="bibr">[NIDCD, 2024]</ref> through Stroke (795, 000 cases each year), Parkinson's disease (1 million), Multiple sclerosis (727, 000), Spinal cord injury (294, 000) and Cerebral palsy (764, 000) <ref type="bibr" target="#b7">[Wallin et al., 2019;</ref><ref type="bibr">White and Black, 2016]</ref>. The proposed method has the potential to transform the lives of these individuals by providing a more natural and efficient mode of communication, improving quality of life, enhancing interaction, and reducing the burden on caregivers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>Hand gestures are natural and flexible means of communication. Available wearable sensor-based hand gesture solutions are widely adopted for the normal 1 <ref type="url" target="https://www.sralab.org/">https://www.sralab.org/</ref> population and these solutions fail to capture highly variable and inconsistent data samples from motor-impaired people. Moreover, in the real world, motor-impaired individuals face challenges in performing predefined gestures, and many valuable use cases rely on acquiring new gestures. However, a substantial amount of data samples is needed to develop a strong hand gesture recognition method. Therefore, we introduce a novel method called Latent Embedding Exploitation (LEE) to learn novel gesture classes incrementally using a few samples from motor-impaired individuals. We experimentally show that our method outperforms the existing baselines. Our method helps motorimpaired persons leverage wearable devices and their unique movement styles can be learned and applied in humancomputer interaction and social communication. By enabling meaningful interactions with motor-impaired individuals and seamlessly integrating wearable devices into their daily lives, we open the gate to collecting invaluable data from this underrepresented group in real-world scenarios. This data collection paradigm can play a central role in facilitating the advancements in other machine learning research, benefiting not only motor-impaired individuals but also contributing to broader technological innovation. In the future, we will integrate our method with a wearable application and online learning will be explored to assist motor-impaired individuals to input their custom, flexible gestures in realtime. Furthermore, we will survey to collect the opinions of the population and tailor our approach accordingly.      <ref type="table" target="#tab_3">2</ref><ref type="table" target="#tab_4">3</ref><ref type="table" target="#tab_5">4</ref>). We also report Gesture class-wise average macro F1-score for individuals with Parkinson's disease (Table <ref type="table" target="#tab_6">5</ref>) and Multiple sclerosis (Table <ref type="table" target="#tab_7">6</ref>). As our method is implemented in a few-shot continual learning setting, 0.22 ± 0.16 0.17 ± 0.08 0.18 ± 0.05 0.19 ± 0.08 0.12 ± 0.10 0.37 ± 0.11 Gesture 2 0.21 ± 0.05 0.15 ± 0.04 0.17 ± 0.04 0.12 ± 0.10 0.01 ± 0.01 0.22 ± 0.13 Gesture 3 0.26 ± 0.15 0.23 ± 0.09 0.13 ± 0.07 0.19 ± 0.02 0.02 ± 0.15 0.41 ± 0.14 Gesture 4 0.32 ± 0.18 0.24 ± 0.11 0.15 ± 0.05 0.25 ± 0.04 0.02 ± 0.03 0.32 ± 0.12 Gesture 5 0.24 ± 0.20 0.19 ± 0.10 0.12 ± 0.08 0.24 ± 0.05 0.22 ± 0.12 0.47 ± 0.20 Gesture 6 0.21 ± 0.06 0.19 ± 0.05 0.10 ± 0.17 0.16 ± 0.04 0.32 ± 0.18 0.40 ± 0.07 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Pseudocode</head><p>We also provide the algorithm of our LEE mechanism in Algoithm 1.</p><p>Algorithm 1 LEE for Few-Shot Continual Learning Require: f θ , f c θ , α, (X t , Y t ), z c 1: while not done do 2:</p><p>Compute L ci using z c and given samples from X t in Equation (2) 3:</p><p>Compute L ii using given samples from X t in Equation (4) 4:</p><p>Compute L cls using given samples and labels from X t and Y t in Equation (5) 5:</p><p>Compute L using α in Equation (1) 6:</p><p>Update θ ← θ -∇ θ L 7: end while</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: (a) An individual lacking fine motor skills performs hand gestures. (b) Sensor-based gesture samples of two different participants including a control participant (top) and a motorimpaired participant (bottom). Data samples are more variable and noisy for a motor-impaired individual rather than a control participant. Blue, orange, and green lines are acceleration values along the x, y, and z-axis respectively.</figDesc><graphic coords="1,331.81,216.14,90.75,151.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The complete framework containing the LEE mechanism. A latent embedding, zc from the control population is preserved to work as gesture prior knowledge. In addition to it, two latent embeddings, z i and z c i function to maintain intra-gesture divergence. The memory buffer saves the training samples from old gesture classes and provides them while training on a novel class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test accuracies for a motor-impaired individual with Spinal cord injury (top row), an individual with Parkinson's disease (middle row), and a participant with Multiple sclerosis (bottom row) in a few-shot continual learning setting. The accuracy represents the total accuracy over all the gesture classes encountered trained with one, three, and five samples.</figDesc><graphic coords="5,114.42,278.12,126.05,103.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the few-shot continual learning setting, since each gesture class contains very few training examples, the epoch is set to 15 and the mini-batch contains all examples. We run each experiment 10 times with five different orders of the gesture classes. We report the average accuracy with one, three, and five training examples over all the encountered gesture classes. As this is a continual learning setup, we also report the class-wise macro F1 score and forgetting metric to understand each gesture's performance individually. Our memory buffer stores 60× fewer examples compared to traditional replay buffers [Rebuffi et al., 2017].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance-forgetting scaled score for Gesture 1 (left) and Gesture 3 (right) for a motor-impaired individual with Spinal cord injury after six gestures are trained with five training examples.for the rest of the incrementally added classes, LEE always performs better than the iCaRL classifier. The accuracy of the iCaRL classifier significantly drops for new gesture classes because it fails to capture the diverse and highly variable pattern of unseen gestures with few training examples. The performance increases with the sample size for all methods. Surprisingly, the fine-tuning approach fails compared to the basic LSTM classifier and our LEE.In a continual learning setup, it is important to perform well in old classes while trained on a new class. Therefore, in Table1, we report the class-wise F1 scores after six gestures are trained with five training examples. Our method always provides a higher macro F1 score than other methods. LEE has a 12.3% higher F1 score for all gesture classes. Figure4shows performance and forgetting score for Gesture 1 and Gesture 3 after six gestures are trained with five samples. Our LEE method has better performance with less forgetting.</figDesc><graphic coords="6,108.10,186.20,196.65,123.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy for different hyperparameter values (left), number of participants (middle), and number of gestures (right) from the source domain when the preserved latent embedding is produced. We report the accuracy after six gestures are trained with five training examples.</figDesc><graphic coords="7,76.60,53.84,151.27,119.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The confusion matrices for six gesture classes after training with five training examples. left: w/o preserved latent embedding (α = 0); middle: w/o temporary latent embedding (α = 1); right: LEE (α = 0.5).</figDesc><graphic coords="7,76.60,214.96,151.28,128.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Order 1: 'circle', 'double tap', 'rotate fast and slow', 'rotate slow and fast', 'shake', 'tap'. The confusion matrices for six gesture classes after training with five training examples.</figDesc><graphic coords="10,106.60,254.85,126.06,113.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Order 2: 'rotate slow and fast', 'tap', 'rotate fast and slow', 'shake', 'circle', 'double tap'. The confusion matrices for six gesture classes after training with five training examples.</figDesc><graphic coords="10,106.60,535.94,126.06,113.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Order 3: 'double tap', 'shake', 'rotate fast and slow', 'circle', 'tap', 'rotate slow and fast'. The confusion matrices for six gesture classes after training with five training examples.</figDesc><graphic coords="11,106.60,167.97,126.06,113.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Order 4: 'rotate fast and slow', 'tap', 'circle', 'rotate slow and fast', 'double tap', 'shake'.</figDesc><graphic coords="11,106.60,442.81,126.06,113.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Order 5: 'shake', 'double tap', 'rotate slow and fast', 'tap', 'circle', 'rotate fast and slow'.</figDesc><graphic coords="12,106.60,167.97,126.06,113.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. In our setting, we have two domains including source domain,D s = {(x i , y i )}ns i=1 and target domain, D t = {(x i , y i )} nt i=1 where n t &lt;&lt; n s . Each sample, x ∈ R L×3 denotes a signal of L length with three-axis motion values collected from wearable sensors at each timestamp. The two domains have the same feature space (X s = X t ) but different label spaces (Y s ̸ = Y t ). In addition to it, they have different probability distributions i.e. P s (x i , y i ) ̸ = P t (x i , y i ). The data distribution for D t is harder to learn than D s due to highlevel noise e.g. H[P t (x)] &gt;&gt; H[P</figDesc><table /><note><p>s (x)] where H denotes the entropy. Therefore, any statistical learner needs more samples from D t to converge to achieve the same level of accuracy as models trained on D s . In D t , the classes C 1 , C 2 , ..., C n are incrementally accessible at the training time. While accessing a new class, C i , a memory buffer stores training examples from old classes such as C 1 , C 2 , ....., C i-1 . Motorimpaired individuals may face challenges in providing many consistent data samples. As a result, it becomes more difficult to train the model with such limited samples. The goal is to build a pre-trained model in the source domain f : X s → Y s that can be fine-tuned to learn currently available classes from the target domain with few training examples. We design the method in such a way that should reduce the memory usage compared to traditional replay buffers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Gesture class-wise average macro F1 score for a motor-impaired individual with Spinal cord injury in a few-shot continual learning setting (mean±std). We report the scores after six gestures are trained with five training examples. The best macro F1 score is highlighted in bold whereas the second-best score is underlined.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Table7shows the forgetting score for each gesture after training with five training examples. Our method always outperforms other approaches. Test accuracies for a motor-impaired individual with Spinal cord injury in a few-shot continual learning setting (mean±std). The accuracy represents the total accuracy over all the gesture classes encountered trained with one, three, and five samples. The best accuracy is highlighted in bold whereas the second-best accuracy is underlined.</figDesc><table><row><cell># gestures # samples 2 1 3 5 3 1 3 5 4 1 3 5 5 1 3 5 6 1 3 5</cell><cell>Methods MAML-Ft Prototypical Net-Ft 72.0 ± 18.0 61.0 ± 6.4 48.0 ± 6.3 LSTM Vanilla-Ft 67.0 ± 8.8 77.4 ± 19.2 71.0 ± 14.9 58.0 ± 7.0 76.3 ± 10.9 76.1 ± 18.2 78.3 ± 15.9 56.3 ± 4.0 78.1 ± 9.1 65.1 ± 3.6 54.1 ± 7.6 38.1 ± 7.8 58.5 ± 9.5 72.9 ± 4.9 63.8 ± 4.1 39.9 ± 4.6 61.9 ± 5.7 70.7 ± 5.4 66.8 ± 11.1 44.6 ± 4.1 66.0 ± 8.0 51.0 ± 10.2 38.5 ± 7.4 25.0 ± 1.4 44.7 ± 4.7 52.3 ± 2.9 47.7 ± 4.8 27.8 ± 3.0 47.9 ± 2.1 55.8 ± 5.3 54.1 ± 7.6 28.3 ± 2.6 53.3 ± 7.6 45.1 ± 3.1 38.7 ± 5.0 21.9 ± 2.4 32.3 ± 4.4 47.7 ± 2.6 36.8 ± 3.8 21.6 ± 5.2 34.0 ± 3.0 51.0 ± 0.9 43.7 ± 4.2 23.5 ± 4.7 37.0 ± 6.5 35.8 ± 3.2 27.7 ± 2.7 20.7 ± 3.0 26.2 ± 3.1 38.5 ± 2.8 33.5 ± 3.8 21.1 ± 4.1 29.6 ± 3.7 43.1 ± 1.9 35.8 ± 2.1 19.1 ± 5.9 33.1 ± 2.5</cell><cell>iCaRL 77.3 ± 23.2 66.0 ± 8.4 Ours-LEE 83.7 ± 19.3 77.6 ± 18.9 82.4 ± 18.2 80.7 ± 18.9 -68.9 ± 7.3 -74.7 ± 9.5 -81.1 ± 10.3 49.5 ± 6.3 55.9 ± 6.7 44.5 ± 6.0 64.7 ± 6.5 45.8 ± 8.6 67.1 ± 8.4 -52.4 ± 2.4 -59.1 ± 4.4 -64.2 ± 4.9 39.3 ± 6.5 41.7 ± 1.6 35.3 ± 3.8 47.1 ± 3.4 24.1 ± 6.0 53.2 ± 2.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test accuracies for a motor-impaired individual with Parkinson's disease in a few-shot continual learning setting (mean±std). The accuracy represents the total accuracy over all the gesture classes encountered trained with one, three, and five samples. The best accuracy is highlighted in bold whereas the second-best accuracy is underlined.</figDesc><table><row><cell># gestures # samples 2 1 3 5 3 1 3 5 4 1 3 5 5 1 3 5 6 1 3 5</cell><cell>Methods MAML-Ft Prototypical Net-Ft 50.0 ± 5.2 60.6 ± 16.0 72.9 ± 5.7 59.4 ± 15.3 53.3 ± 8.2 LSTM Vanilla-Ft 42.6 ± 14.6 56.3 ± 9.5 71.3 ± 7.7 69.5 ± 12.8 62.3 ± 11.3 54.3 ± 6.0 73.3 ± 4.8 48.0 ± 13.0 50.9 ± 10.8 41.9 ± 3.9 49.1 ± 11.3 59.2 ± 9.1 55.2 ± 8.5 40.1 ± 3.2 51.5 ± 12.7 60.4 ± 6.4 56.1 ± 8.4 41.1 ± 11.0 54.3 ± 5.0 44.5 ± 9.6 39.7 ± 6.7 28.6 ± 3.0 41.3 ± 6.5 51.4 ± 6.1 45.9 ± 11.5 30.0 ± 2.1 38.7 ± 8.2 44.9 ± 3.9 43.5 ± 8.3 28.4 ± 3.5 43.3 ± 2.3 31.5 ± 6.4 28.3 ± 4.7 23.7 ± 2.0 30.0 ± 3.5 35.8 ± 2.1 36.8 ± 4.0 24.6 ± 4.9 35.0 ± 3.0 38.0 ± 4.6 38.4 ± 5.8 22.7 ± 4.4 30.5 ± 2.9 26.5 ± 2.7 26.3 ± 2.1 17.5 ± 2.5 26.1 ± 2.0 31.2 ± 3.4 31.0 ± 3.1 22.3 ± 4.4 27.4 ± 4.0 31.5 ± 4.1 33.1 ± 3.9 20.3 ± 4.1 23.2 ± 3.1</cell><cell>iCaRL 54.3 ± 13.8 65.7 ± 12.9 Ours-LEE 81.2 ± 8.9 74.3 ± 4.5 82.6 ± 11.1 72.9 ± 5.9 -56.3 ± 9.2 -67.1 ± 3.9 -73.1 ± 8.9 42.0 ± 13.4 48.5 ± 9.9 41.5 ± 3.1 60.6 ± 9.2 38.5 ± 7.7 59.5 ± 8.6 -39.5 ± 5.8 -46.2 ± 6.9 -52.0 ± 7.0 21.4 ± 4.7 32.7 ± 2.9 29.1 ± 7.8 40.7 ± 3.8 23.2 ± 7.4 44.3 ± 3.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test accuracies for a motor-impaired individual with Multiple sclerosis in a few-shot continual learning setting (mean±std). The accuracy represents the total accuracy over all the gesture classes encountered trained with one, three, and five samples. The best accuracy is highlighted in bold whereas the second-best accuracy is underlined.</figDesc><table><row><cell>Gesture classes Gesture 1</cell><cell>LSTM</cell><cell>Vanilla-Ft</cell><cell>MAML-Ft</cell><cell>Methods Prototypical Net-Ft</cell><cell>iCaRL</cell><cell>Ours-LEE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Gesture class-wise average macro F1 score for a motor-impaired individual with Parkinson's disease (mean±std). We report the scores after six gestures are trained with five training examples. The best macro F1 score is highlighted in bold whereas the second-best score is underlined. ± 0.14 0.24 ± 0.21 0.18 ± 0.10 0.18 ± 0.10 0.01 ± 0.01 0.39 ± 0.21 Gesture 2 0.13 ± 0.05 0.12 ± 0.03 0.10 ± 0.03 0.15 ± 0.09 0.01 ± 0.01 0.14 ± 0.06 Gesture 3 0.34 ± 0.11 0.49 ± 0.11 0.22 ± 0.16 0.15 ± 0.08 0.09 ± 0.2 0.64 ± 0.13 Gesture 4 0.29 ± 0.18 0.24 ± 0.10 0.13 ± 0.05 0.26 ± 0.15 0.06 ± 0.1 0.45 ± 0.21 Gesture 5 0.19 ± 0.08 0.14 ± 0.06 0.18 ± 0.06 0.10 ± 0.03 0.30 ± 0.17 0.30 ± 0.19 Gesture 6 0.35 ± 0.17 0.25 ± 0.13 0.13 ± 0.05 0.22 ± 0.10 0.24 ± 0.19 0.41 ± 0.26</figDesc><table><row><cell>Gesture classes Gesture 1</cell><cell>LSTM 0.26</cell><cell>Vanilla-Ft</cell><cell>MAML-Ft</cell><cell>Methods Prototypical Net-Ft</cell><cell>iCaRL</cell><cell>Ours-LEE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Gesture class-wise average macro F1 score for a motor-impaired individual with Multiple sclerosis (mean±std). We report the scores after six gestures are trained with five training examples. The best macro F1 score is highlighted in bold whereas the second-best score is underlined.</figDesc><table><row><cell>Methods LSTM Vanila-Ft MAML-Ft Prototypical Net-Ft iCaRL Ours-LEE</cell><cell>forgetting Gesture 1 Gesture 2 Gesture 3 Gesture 4 Gesture 5 Gesture 6 0.33 0.49 0.43 0.12 0.08 -0.47 0.59 0.29 0.22 0.11 -0.36 0.39 0.30 0.04 0.04 -0.55 0.62 0.43 0.28 -0.01 -0.79 0.75 0.53 0.45 --0.25 0.51 0.30 0.13 0.09 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Forgetting score in terms of gesture class-wise macro F1 score for a motor-impaired individual with Spinal cord injury in a few-shot continual learning setting (mean±std). We report the scores after six gestures are trained with five training examples. The best forgetting score is highlighted in bold.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments Research reported in this publication was supported by the <rs type="funder">Eunice Kennedy Shriver National Institute of Child Health &amp; Human Development of the National Institutes of Health</rs> under Award Number <rs type="grantNumber">P2CHD101899</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vDaNCVX">
					<idno type="grant-number">P2CHD101899</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blakeman and Mareschal, 2020] Sam Blakeman and Denis Mareschal. A complementary learning systems approach to temporal difference learning</title>
		<author>
			<persName><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><surname>Bank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05991</idno>
	</analytic>
	<monogr>
		<title level="m">2020 3rd International Conference on Digital Medicine and Image Processing</title>
		<editor>
			<persName><forename type="first">Pieter</forename><surname>Finn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</editor>
		<editor>
			<persName><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2020. 2020. 2020. 2020. 2020. 2015. 2015. 2023. May 13, 2024. 2014. 2017. 2017. 2017</date>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human-machine interaction sensing technology based on hand gesture recognition: A review</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>Mahmut Kaya and Hasan S ¸akir Bilge</publisher>
			<date type="published" when="1999">1999. 1999. 2018. 2018. 2021. 2021. 2018. 2018. 2019. 2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1066</biblScope>
		</imprint>
	</monogr>
	<note>Symmetry</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards a smartwatch application to assist students with disabilities in an iot-enabled campus</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 1st Global Conference on Life Sciences and Technologies (LifeTech)</title>
		<imprint>
			<biblScope unit="page" from="243" to="246" />
			<date type="published" when="2019">2019. 2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10054</idno>
	</analytic>
	<monogr>
		<title level="m">Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2018">2022. 2022. 2022. 2022. 2022. 2022. 2019. 2019. 2019. 2019. 2018. 2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="157422" to="157436" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Mohamed et al., 2021] Noraini Mohamed, Mumtaz Begum Mustafa, and Nazean Jomhari. A review of the hand gesture recognition system: Current progress and future directions. IEEE Access</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gesture recognition using wearable sensors with bi-long short-term memory convolutional neural networks</title>
		<author>
			<persName><surname>Nguyen-Trong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019">2021. 2021. 2024. May 13, 2024. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>Neural networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human-behavior learning: A new complementary learning perspective for optimal decision making controllers</title>
		<author>
			<persName><forename type="first">Adolfo</forename><surname>Perrusquía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Perrusquía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Pramod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pisharady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Saerbeck</surname></persName>
		</author>
		<author>
			<persName><surname>Porzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM international workshop on Interactive multimedia on mobile &amp; portable devices</title>
		<editor>
			<persName><forename type="first">Soheil</forename><surname>Rahimian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Zabihi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Asif</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arash</forename><surname>Farokh Atashzar</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mohammadi</surname></persName>
		</editor>
		<meeting>the 3rd ACM international workshop on Interactive multimedia on mobile &amp; portable devices</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2022. 2022. 2015. 2015. 2013. 2013. 2021. 2023. 2021. 2017</date>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wearable interactions for users with motor impairments: systematic review, inventory, and research implications</title>
		<author>
			<persName><forename type="first">Vatavu ; Alexandru-Ionut</forename><surname>Siean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu-Daniel</forename><surname>Siean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Vatavu</surname></persName>
		</author>
		<author>
			<persName><surname>Snell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<editor>
			<persName><forename type="first">Kenneth</forename><surname>Stewart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumit</forename><surname>Bam Shrestha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emre</forename><surname>Neftci</surname></persName>
		</editor>
		<meeting>the 23rd International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2017">2021. 2021. 2017. 2017. 2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
	<note>Online fewshot gesture learning on a neuromorphic processor</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xie and Cao, 2016] Renqiang Xie and Juncheng Cao. Accelerometer-based hand gesture recognition by neural network and similarity matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Radu-Daniel</forename><surname>Tolias ; Ungurean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ovidiu-Ciprian</forename><surname>Vatavu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ungurean</surname></persName>
		</author>
		<author>
			<persName><surname>Wallin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<idno>arXiv:2304.14623</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<editor>
			<persName><surname>Yu</surname></persName>
		</editor>
		<imprint>
			<publisher>Van de Ven and Tolias</publisher>
			<date type="published" when="2012">2019. 2019. 2022. 2022. 2019. 2019. 2016. 2012. 2012. 2016. 2022. 2022. 2021. 2021. 2023. 2021. 2021. 2021. 2020</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence. Zhuang et al., 2020] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Additional Results We report the confusion matrices for five different orders after six gestures are trained with five training examples. The hand gestures are known as &apos;tap&apos;, &apos;double tap&apos;, &apos;circle&apos;, &apos;rotate fast and slow&apos;, &apos;rotate slow and fast&apos;, and &apos;shake&apos;. Figure 7 -11 illustrates that the LEE method has a more robust performance than other methods. For certain orders, LEE mistakes one gesture for another. Since order is an influential factor</title>
		<imprint/>
	</monogr>
	<note>we intend to explore it further in our future work</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
