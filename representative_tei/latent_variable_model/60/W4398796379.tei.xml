<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2025 A STUDY OF POSTERIOR STABILITY FOR TIME-SERIES LATENT DIFFUSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-02">2 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">LIONS -École Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2025 A STUDY OF POSTERIOR STABILITY FOR TIME-SERIES LATENT DIFFUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-02">2 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.14021v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latent diffusion has demonstrated promising results in image generation and permits efficient sampling. However, this framework might suffer from the problem of posterior collapse when applied to time series. In this paper, we first show that posterior collapse will reduce latent diffusion to a variational autoencoder (VAE), making it less expressive. This highlights the importance of addressing this issue. We then introduce a principled method: dependency measure, that quantifies the sensitivity of a recurrent decoder to input variables. Using this tool, we confirm that posterior collapse significantly affects time-series latent diffusion on real datasets, and a phenomenon termed dependency illusion is also discovered in the case of shuffled time series. Finally, building on our theoretical and empirical studies, we introduce a new framework that extends latent diffusion and has a stable posterior. Extensive experiments on multiple real time-series datasets show that our new framework is free from posterior collapse and significantly outperforms previous baselines in time series synthesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Latent diffusion <ref type="bibr" target="#b31">(Rombach et al., 2022)</ref> has achieved promising performance in image generation and offers significantly higher sampling speeds than standard diffusion models <ref type="bibr" target="#b18">(Ho et al., 2020)</ref>. However, we find that, when applied to time series data, this framework might suffer from posterior collapse <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref>, an important problem that has garnered significant attention in the literature on autoencoders <ref type="bibr" target="#b4">(Baldi, 2012;</ref><ref type="bibr" target="#b28">Lucas et al., 2019)</ref>, where the latent variable contains little information about the data and it tends to be ignored by the decoder during conditional generation. In this paper, we aim to provide a systematic analysis on the impact of posterior collapse on latent diffusion and improve this framework based on our analysis.</p><p>Impact analysis of posterior collapse. We first show that a strictly collapsed posterior reduces the latent diffusion to a variational autoencoder (VAE) <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref>, indicating that this problem makes the framework less expressive, even weaker than a vanilla diffusion model. We then introduce a principled method termed dependency measure, which quantifies the dependencies of an autoregressive decoder on the latent variable and the input partial time series. Through empirical estimation of these measures, we find that the latent variable has an almost exponentially vanishing impact on the recurrent decoder during the generation process. An example (i.e., the green bar chart) is shown in the upper left subfigure of Fig. <ref type="figure">1</ref>. More interestingly, the upper right subfigure illustrates a phenomenon we call dependency illusion: Even when the time series is randomly shuffled and thus lacks structural dependencies, the decoder of latent diffusion still heavily relies on input observations (instead of the latent variable) for prediction.</p><p>New framework to solve the problem. We first point out that the root cause of posterior collapse lies in the improper design of latent diffusion, which leads to avoidable KL-divergence regularization and a lack of mechanisms to address the insensitive decoder <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref>. Building on these findings, we propose a novel framework that extends latent diffusion, allowing the diffusion model and autoencoder to interact more effectively. Specifically, by treating the diffusion process as a form of variational inference, we can eliminate the problematic KL-divergence regularization and Figure <ref type="figure">1</ref>: The global and local dependency measures m t,0 , m t,t-1 (as defined in Sec. 3.2) respectively quantify the impacts of latent variable z and observation x t-1 on predicting the next one x t . We can see that the latent variable z of latent diffusion loses control over the condition generation p gen (X | z), with dependency illusion (as introduced in Sec. 3.3) in the case of shuffled time series. In contrast, our framework has no such symptoms of posterior collapse.</p><p>permit an unlimited prior distribution for latent variables. To let the decoder be more sensitive to the latent variable, we also apply the diffusion process to simulate a collapsed posterior, imposing a significant penalty on the occurrence of dependency illusion. As demonstrated in the lower two subfigures of Fig. <ref type="figure">1</ref>, our framework exhibits no signs of posterior collapse, such as the vanishing impact of latent variables over time.</p><p>In summary, our paper makes the following contributions:</p><p>• We are the first to systematically study posterior collapse in latent diffusion, introducing the technique of dependency measure (Sec. 3.2) for analysis. We show that the problem renders latent diffusion as inexpressive as a simple VAE (Sec. 3.1) and the latent variable also loses control over time series generation in this case (Sec. 3.3); • We present a new framework (Sec. 4.2) that improves upon time-series latent diffusion, which eliminates the risky KL-divergence regularization, permits an expressive prior distribution, and features a decoder that is sensitive to the latent variable; • We have conducted extensive experiments (Sec. 6 and Appendix F) on multiple real timeseries datasets, showing that our framework exhibits no symptoms of posterior collapse (or dependency illusion) and significantly outperforms previous baselines.</p><p>We will publicly release our code upon paper acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: LATENT DIFFUSION</head><p>The architecture of latent diffusion consists of two parts: 1) an autoencoder <ref type="bibr" target="#b4">(Baldi, 2012)</ref> that maps high-dimensional or structured data into low-dimensional latent variables; 2) a diffusion model <ref type="bibr" target="#b35">(Sohl-Dickstein et al., 2015)</ref> that learns the distribution of latent variables.</p><p>Autoencoder. An implementation for the autoencoder is VAE <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref>. Let X and q raw (X) respectively denote the raw data of any form (e.g., pixel matrix) and its distribution.</p><p>The encoder f enc is designed to cast the data X into a low-dimensional vector v = f enc (X). To get latent variable z, a reparameterization trick is performed as</p><formula xml:id="formula_0">µ = W µ v, σ = exp(W σ v), z = µ + diag(σ) • ϵ, ϵ ∼ N (0, I),<label>(1)</label></formula><p>where W µ , W σ are learnable matrices, and diag(•) is an operation that casts a vector into a diagonal matrix. The above procedure, which differentially samples a latent variable z from the posterior <ref type="bibr">et al., 2017)</ref>. The decoder f dec takes latent variable z as the input to recover the real sample X. In VAE, the decoder output f dec (z) is used to parameterize a predefined generation distribution p gen (X | z).</p><formula xml:id="formula_1">q VI (z | X) = N (z; µ, diag(σ 2 )), is called variational inference (Blei</formula><p>For training, VAE is optimized in terms of the evidence lower bound (ELBO), an upper bound of the exact negative log-likelihood:</p><formula xml:id="formula_2">L VAE = E z∼q VI (z|X) [-ln p gen (X | z)] + D KL (q VI (z | X) || p prior (z)),<label>(2)</label></formula><p>where the prior distribution p prior (z) is commonly set as a standard Gaussian N (0, I). The last term of KL divergence leads the prior p prior (z) to be compatible with the decoder f dec for inference, but it is also one cause of posterior collapse <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref>.</p><p>Diffusion model. An implementation for the diffusion model is DDPM <ref type="bibr" target="#b18">(Ho et al., 2020)</ref>. The model consists of two Markov chains of L ∈ N + steps. One of them is the diffusion process, which incrementally applies the forward transition kernel:</p><formula xml:id="formula_3">q forw (z i | z i-1 ) = N (z i ; 1 -β i z i , β i I),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">β i , i ∈ [1, L]</formula><p>is some predefined variance schedule, to the latent variable z 0 := z ∼ q latent (z).</p><p>Here the distribution of latent variable q latent (z) is defined as q VI (z | X)q raw (X)dX. The outcomes of this process are a sequence of new latent variables {z 1 , z 2 , • • • , z L }, with the last one z L approximately following a standard Gaussian N (0, I) for L ≫ 1.</p><p>The other is the reverse process, which iteratively applies the backward transition kernel,</p><formula xml:id="formula_5">p back (z i-1 | z i ) = N (z i-1 ; µ back (z i , i), σ i I), µ back (z i , i) = 1 √ α i z i -β i ϵ back (z i , i) √ 1 -ᾱi ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">α i = 1 -β i , ᾱi = i k=1 α k , ϵ back (•) is a neural network, z</formula><p>L is an initial sample drawn from ∼ N (0, I), and σ i is some backward variance schedule. The outcome of this process is a reversed sequence of latent variables {z L-1 , z L-2 , • • • , z 0 }, where the last variable z 0 is expected to follow the density distribution of real samples: q latent (z 0 ).</p><p>To optimize the diffusion model, common practices adopt a loss function as</p><formula xml:id="formula_7">L DM = E i,z 0 ,ϵ [∥ϵ -ϵ back ( √ ᾱi z 0 + 1 -ᾱi ϵ, i)∥ 2 ],<label>(5)</label></formula><p>where ϵ ∼ N (0, I), z 0 ∼ q latent (z 0 ), and i ∼ U{1, L}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM ANALYSIS</head><p>In this section, we first formulate the problem of posterior collapse in the framework of time-series latent diffusion and show its significance. Then, we define proper measures that quantify the impact of posterior collapse on the models. Finally, we conduct empirical experiments to confirm that time-series diffusion indeed suffers from this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FORMULATION OF POSTERIOR COLLAPSE AND ITS IMPACTS</head><p>Let us focus on time series</p><formula xml:id="formula_8">X = [x 1 , x 2 , • • • , x T ],</formula><p>where every observation x t , t ∈ [1, T ] is a D-dimensional vector and T denotes the number of observations. A potential risk of applying the latent diffusion to time series is the posterior collapse <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref>, which occurs to some autoencoders <ref type="bibr" target="#b3">(Bahdanau et al., 2014)</ref>, especially VAE <ref type="bibr" target="#b28">(Lucas et al., 2019)</ref>. Its mathematical formulation in the framework of latent diffusion is as follows.</p><p>Problem formulation. The posterior of VAE: q VI (z | X), is collapsed if it reduces to the Gaussian prior p prior (z) = N (z; 0, I), irrespective of the time-series conditional X:</p><formula xml:id="formula_9">q VI (z | X) = p prior (z), ∀X ∈ R T D .</formula><p>In this case, the latent variable z contains no information about time series X, otherwise the posterior distribution q VI (z | X) would vary depending on different conditionals. Above is a strict definition. In practice, one is mostly faced with a situation where q VI (z | X) ≈ p prior (z) and it is still appropriate to say that the posterior collapses.</p><p>Implications of posterior collapse. A typical symptom of this problem is that, since the latent variable z carries very limited information of time series X, the trained decoder f dec tends to ignore this input variable z, which is undesired for conditional generation p gen (X | z). Besides this empirical finding from previous works, we find that posterior collapse is also significant in its impact on the expressiveness of latent diffusion. Let us first see the below conclusion. Proposition 3.1 (Gaussian Latent Variables). For standard latent diffusion, suppose its posterior q VI (z | X) is collapsed, then the distribution q latent (z) of latent variable z will shape as a standard Gaussian N (0, I), which is trivial for the diffusion model to approximate.</p><p>Proof. The proof is fully provided in Appendix A.</p><p>In other words, latent variable z is just Gaussian in the case of posterior collapse. The diffusion model, which is known for approximating complex data distributions <ref type="bibr" target="#b11">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr" target="#b26">Li et al., 2024)</ref>, will in fact become a redundant module. Therefore, posterior collapse reduces latent diffusion to a simple VAE, which also samples latent variable z from a standard Gaussian N (0, I).</p><p>We conclude that the problem makes latent diffusion less expressive.</p><p>Takeaway: The problem of posterior collapse not only lets the decode f dec tend to ignore the latent variable z for conditional generation p gen (X | z), but also reduces the framework of latent diffusion to VAE, making it less expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INTRODUCTION OF DEPENDENCY MEASURES</head><p>It is very intuitive from above that the problem of posterior collapse will make the latent variable z lose control of the decoder f dec . To make our claim more solid and confirm that the problem happens to time-series diffusion, we introduce some proper measures that quantify the dependencies of decoder f dec on various inputs (e.g., variable z).</p><p>Autoregressive decoder. Consider that decoder f dec has an autoregressive structure, which conditions on latent variable z and prefix</p><formula xml:id="formula_10">X 1:t-1 = [x 1 , x 2 , • • • , x t-1</formula><p>] to predict the next observation x t . With abuse of notation, we set x 0 = z and formulate the decoder as</p><formula xml:id="formula_11">h t = f dec (X 0:t-1 ), X 0:t-1 = [x 0 , x 1 , x 2 , • • • , x t-1 ]<label>(6)</label></formula><p>where the representation h t , t ≥ 1 is linearly projected to multiple parameters (e.g., mean vector and covariance matrix) that determine the distribution p gen (x t | z, X 1:t-1 ) of some family (e.g., Gaussian). Examples of such a decoder include recurrent neural networks (RNN) <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997)</ref> and Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. We put the formulation details of these example in Appendix B.</p><p>Dependency measure. The symptom of posterior collapse is that the decoder f dec heavily relies on prefix X 1:t-1 (especially the last observation x t-1 ) to compute the representation h t , ignoring the guidance of latent variable x 0 = z. In other words, the variable z loses control of decoder f dec in that situation, which is undesired for conditional generation p gen (X | z).</p><p>Inspired by the technique of integrated gradients <ref type="bibr" target="#b36">(Sundararajan et al., 2017)</ref>, we present a new tool: dependency measure, which quantifies the impacts of latent variable x 0 = z and prefix X 1:t-1 on decoder f dec . Specifically, we first set a baseline input</p><formula xml:id="formula_12">O 0:t-1 as [x 0 = 0, x 1 = 0, • • • , x t-1 = 0]</formula><p>and denote the term f dec (O 0:t-1 ) as h t . Then, we parameterize a straight line γ(s) : [0, 1] → R tD between the actual input X 1:t-1 and the input baseline O 0:t-1 as</p><formula xml:id="formula_13">γ(s) = sX 0:t-1 + (1 -s)O 0:t-1 := [sx 0 , sx 1 , • • • , sx t-1 ].<label>(7)</label></formula><p>Applying the chain rule in differential calculus, we have</p><formula xml:id="formula_14">df dec (γ(s)) ds = t-1 j=0 k=D k=1 df dec (γ j,k (s)) dγ j,k (s) dγ j,k (s) ds = t-1 j=0 k=D k=1 x j,k df dec (γ j,k (s)) dγ j,k (s) ,<label>(8)</label></formula><p>where γ j,k (s) denote the k-th dimension s • x j,k of the j-th vector sx j in point γ(s). With the above elements, we can define the below measures.</p><p>Definition 3.2 (Dependency Measures). For an autoregressive decoder f dec that conditions on both latent variable x 0 = z and the prefix X 1:t-1 to compute representation h t , the dependency measure of every input variable x j , j ∈ [0, t -1] to the decoder is defined as</p><formula xml:id="formula_15">m t,j = 1 ∥h t -h t ∥ 2 h t -h t , D k=1 x j,k 1 0 df dec (γ j,k (s)) dγ j,k (s) ds ,<label>(9)</label></formula><p>where operation &lt; •, • &gt; represents the inner product. In particular, we name m t,0 as the global dependency and m t,t-j , 1 ≤ j &lt; t as the j-th order local dependency.</p><p>We provide the derivation for dependency measure m t,j and detail its relations to integrated gradients in Appendix C. In practice, the integral term can be approximated as</p><formula xml:id="formula_16">1 0 df dec (γ j,k (s)) dγ j,k (s) ds = E s∈U {0,1} df dec (γ j,k (s)) dγ j,k (s) ≈ 1 |S| s∈S df dec (γ j,k (s)) dγ j,k (s) , (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where S is the set of independent samples drawn from uniform distribution U{0, 1}. According to the law of large numbers <ref type="bibr" target="#b33">(Sedor, 2015)</ref>, this approximation is unbiased and gets more accurate for a bigger sample set |S|. Notably, the defined measures have the following properties. Proposition 3.3 (Signed and Normalization Properties). The dependency measure m t,j , ∀j ∈ [0, t-1] is a signed measure and always satisfies</p><formula xml:id="formula_18">t-1 j=0 m t,j = 1.</formula><p>Proof. The proof is fully provided in Appendix D.</p><p>We can see that the measure m t,j can be either positive or negative, with a normalized sum over the subscript j as 1. If m t,j ≥ 0, then we say that vector x j has a positive impact on the decoder f dec for computing representation h j : the bigger is m t,j , the larger is such an impact; Similarly, if m t,j &lt; 0, then the vector x j has a negative impact on the decoder: the smaller is m t,j , the greater is the negative influence. Besides, it is also not hard to understand that there exists a negative impact. For example, the latent variable z ∼ q latent (z) might be an outlier for the decoder f dec , which locates at a low-density region in the prior distribution q prior (z).</p><p>Example Application. Fig. <ref type="figure">1</ref> shows several examples of applying the dependency measures, where each subfigure contains a sample of time series (i.e., blue curve) generated by some model and two types of dependency measures (i.e., red and green bar charts) estimated by Eq. ( <ref type="formula" target="#formula_15">9</ref>). Specifically, every point x t in the time series corresponds to a green bar that indicates the global dependency m t,0 and a red bar that represents the first-order local dependency m t,t-1 . In the upper left subfigure, we can see that the positive impact of latent variable z on the decoder (e.g., m t,0 ) decreases over time and vanishes eventually. From the lower right subfigure, we can even see that some bars (i.e., local dependency m t,t-1 ) are negative, indicating that the variable x t-1 has a negative impact on predicting the next observation x t .</p><p>Takeaway: Dependency measure m t,j , 0 ≤ j &lt; t quantifies the impact of latent variable x 0 = z or observation x j , j ≥ 1 on the decoder f dec . This type of impact can be either positive or negative, which is reflected in the value of measure m t,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EMPIRICAL DEPENDENCY ESTIMATIONS</head><p>We are mainly interested in two types of defined measures: One is the global dependency m t,0 , which estimates the impact of latent variable x 0 = z on the decoder f dec ; The other is the first-order local dependency m t,t-1 , which estimates the dependency of decoder f dec on the last observation x t,t-1 for computing representation h t . In this part, we empirically estimate these measures, with the aims to confirm that posterior collapse occurs and show its impacts.</p><p>Experiment setup. Two time-series datasets: WARDS <ref type="bibr">(Alaa et al., 2017b)</ref> and MIMIC <ref type="bibr" target="#b20">(Johnson et al., 2016</ref>) are adopted. For each dataset, we extract the observations of the first 12 hours, with the top 1 and 5 features that have the highest variances to form univariate and multivariate time Figure <ref type="figure">2</ref>: Dependency measures m t,0 , m t,t-1 averaged over 500 multivariate time series, with 3 standard deviations as the error bars. We can see that the latent variable z of latent diffusion has a vanishing impact on the decoder f dec , a typical symptom of posterior collapse. We also observe a phenomenon of dependency illusion in the case of shuffled time series.</p><p>series. To study the case where time series have no structural dependencies, we also try randomly shuffling the time steps of ordered time series. With the prepared datasets, we respectively train latent diffusion models on them and sample time series from the models.</p><p>Insightful results. The upper two subfigures of Fig. <ref type="figure">1</ref> illustrate the estimated dependencies m t,0 , m t,t-1 for a single time-series sample X, while Fig. <ref type="figure">2</ref> shows the dependency measures averaged over 500 samples. We can see that, for both ordered and shuffled time series, the global dependency m t,0 exponentially converges to 0 with increasing time step t, indicating that latent variable z loses control of the generation process of decoder f dec and the posterior is collapsed.</p><p>More interestingly, as shown in the right part of Fig. <ref type="figure">1</ref>, while there is no dependency between adjacent observations x t-1 , x t in shuffled time series, we still observe that the first-order measure m t,t-1 is significantly different from 0 (e.g., around 0.1 to 0.2). This phenomenon might arise as neural networks overfit and we name it as dependency illusion.</p><p>Takeaway: Time-series latent diffusion exhibits a typical symptom of posterior collapse: latent variable z has an almost exponentially decreasing impact on generation process p gen (X | z).</p><p>More seriously, we observe a phenomenon of dependency illusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROBLEM RETHINKING AND NEW FRAMEWORK</head><p>In this section, we first analyze how the framework design of latent diffusion makes it tend to suffer from posterior collapse. Then, based on our analysis, we propose a new framework, which extends latent diffusion but addresses the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RISKY DESIGN OF LATENT DIFFUSION</head><p>Previous works <ref type="bibr" target="#b34">(Semeniuta et al., 2017;</ref><ref type="bibr" target="#b2">Alemi et al., 2018)</ref> have identified two main causes of the problem: KL-divergence term and strong decoder. For time-series latent diffusion, we will explain as below that those causes indeed exist, but are in fact avoidable.</p><p>Unnecessary regularization. The KL-divergence term D KL (q VI (z | X) || p prior (z)) in Eq. ( <ref type="formula" target="#formula_2">2</ref>) moves the posterior q VI (z | X) towards prior p prior (z), which has the side effect of posterior collapse by definition. In essence, this term is tailored for VAE, such that it is valid to sample latent variable z from the Gaussian prior p prior (z) for inference. However, for latent diffusion, the variable z is sampled from the diffusion model, which can approximate a non-Gaussian prior distribution.</p><p>Hence, the interaction between the VAE and diffusion model is not properly designed, which incurs a limited prior p prior (z) and a risky KL-divergence term D KL (•). Figure 3: In this example, path X → z 1 is the variational inference (which gets rid of KL-divergence regularization) and path X → z 3 shows the collapse simulation (which is to increase the sensitivity of decoder f dec to latent variable z). Compared with time-series latent diffusion, our framework is free from posterior collapse and has a unlimited prior p prior (z).</p><p>Unprepared for recurrence. The strong decoder is also a cause of posterior collapse, which happens to sequence autoencoders <ref type="bibr" target="#b9">(Bowman et al., 2016;</ref><ref type="bibr" target="#b14">Eikema &amp; Aziz, 2019)</ref>. Time series X ∈ R T D have a clear temporal structure, so the corresponding decoder f dec is typically a RNN, which explicitly models the dependency between different observations x i , x j , i ̸ = j. For predicting the observation x j , both latent variable z and previous observation x i , i &lt; j are the inputs to the decoder f dec , so variable z is possible to be ignored.</p><p>Latent diffusion is primarily designed for image generation, with U-net <ref type="bibr" target="#b32">(Ronneberger et al., 2015)</ref> as the backbone, which consists of many layers of feedforward neural networks (FNN) <ref type="bibr" target="#b37">(Svozil et al., 1997)</ref>. For example, convolution neural networks (CNN) <ref type="bibr" target="#b24">(Krizhevsky et al., 2012)</ref>, selfattention <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, and MLP <ref type="bibr" target="#b27">(Lu et al., 2017)</ref>. These FNN layers are highly sensitive to the input variables, so the original design of latent diffusion lacks a mechanism to address the possible insensitivity, which is the case of time-series decoder.</p><p>Takeaway: The improper design of latent diffusion is the root cause of posterior collapse, which results in the avoidable KL-divergence regularization, limits the form of prior distribution p prior (z), and lacks a mechanism to handle the insensitive decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NEW FRAMEWORK</head><p>In light of previous analyses, we propose a new framework that lets the autoencoder interact with the diffusion model more effectively than latent diffusion. With this better framework design, we can eliminate the KL-divergence term, permit a free from of prior distribution p prior (z), and increase the sensitivity of decoder f dec to latent variable z.</p><p>Importantly, we notice a conclusion <ref type="bibr" target="#b18">(Ho et al., 2020)</ref> for the diffusion process (i.e., Eq. ( <ref type="formula" target="#formula_3">3</ref>)):</p><formula xml:id="formula_19">q forw (z i | z 0 ) = N (z i ; √ ᾱi z 0 , (1 -ᾱi )I),<label>(11)</label></formula><p>where the coefficient ᾱi monotonically decreases from 1 to approximately 0 for i ∈ [0, L]. In this sense, suppose the initial variable z 0 is set as v = f enc (X), then we can infer that the random variable z i ∼ q forw (z i | z 0 ) contains ᾱi ×100% information about the vector v, with (1-ᾱi )×100% pure noise. For i → 0, the diffusion process is similar to the variational inference (i.e., Eq. ( <ref type="formula" target="#formula_0">1</ref>)) of VAE, adding slight Gaussian noise to the encoder output v. For i → T , the variable z i simulates the problem of posterior collapse since q forw (z i | z 0 ) ≈ N (z i ; 0, I).</p><p>Diffusion process as variational inference. Considering the above facts, we first treat the starting few iterations of the diffusion process as the variational inference. Specifically, with a fixed small integer N ≪ L, we sample a number i from uniform distribution U{0, N } and let the diffusion process convert the encoder output v = f enc (X) into the latent variable:</p><formula xml:id="formula_20">z = z i ∼ q forw (z i | z 0 ), z 0 = v. (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>In terms of the formerly defined generation distribution p gen (X | z) (parameterized by the decoder f dec ), a negative log-likelihood loss L VI is incurred as</p><formula xml:id="formula_22">L VI = E i∼U {0,N },z 0 [-ᾱ γi ln p gen (X | z = z i )],<label>(13)</label></formula><p>Algorithm 1 Training 1: repeat 2: Sample time series X from the dataset 3: Representation encoding: v = f enc (X) 4:</p><formula xml:id="formula_23">z j ∼ q forw (z j | z 0 = v), j ∼ U{0, N } 5: L VI = -ᾱγj ln p gen (X | z = z j ) 6: i ∼ U{j, L}, ϵ ∼ N (0, I) 7: L DM = ∥ϵ -ϵ back ( √ ᾱi z j + √ •ϵ, i)∥ 2 8: z k ∼ q forw (z k | z 0 = v), k ∼ U{M, L} 9: L CS = (1 - ᾱ⌈ k η ⌉ ) ln p gen (X | z = z k ) 10: Gradient descent with ∇( L VI + L DM + L CS ) 11: until converged</formula><p>Algorithm 2 Sampling 1: zL ∼ p back (zL) = N (0, I) 2: Set stop time: i ∼ U{0, N } 3: for l = L, L -1, . . . , i + 1 do 4: z l-1 ∼ p back (z l-1 | z l ) 5: end for 6: Conditional generation: p gen ( X | z = z i ) 7: return Time series X</p><p>where γ ∈ N + , γN ≤ L is a hyper-parameter, with the aim to reduce the impact of a very noisy latent variable z. As multiplier γ increases, the weight ᾱγi decreases.</p><p>Similar to VAE, the variational inference in our framework also leads the latent variable z to be smooth <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref> in its effect on decoder f dec . However, our framework is free from the KL-divergence term D KL (q VI (z | X) || p prior (z)) of VAE (i.e., one cause of the posterior collapse), since we can facilitate z ∼ q latent (z) at test time through applying the reverse process of the diffusion model (i.e., Eq. ( <ref type="formula" target="#formula_5">4</ref>)) to sample variable</p><formula xml:id="formula_24">z i , i ∈ [0, N ].</formula><p>Diffusion process for collapse simulation. Then, we apply the last few iterations of the diffusion process to simulate posterior collapse, with the purposes of increasing the impact of latent variable z on conditional generation p gen (X | z) and reducing dependency illusion.</p><p>Following our previous variational inference, we set z 0 = f enc (X) and apply the diffusion process to cast the initial variable z 0 into a highly noisy variable z i , i → L. Considering that the variable z i contains little information about the encoder output f enc (X), it is unlikely that the decoder f dec can recover time series X from variable z i , otherwise there is posterior collapse or dependency illusion.</p><p>In this sense, we have the following regularization:</p><formula xml:id="formula_25">L CS = E i∼U {M,L},z i [(1 -ᾱ⌈ i η ⌉ ) ln p gen (X | z = z i )],<label>(14)</label></formula><p>which penalizes the model for having a high conditional density p gen (X | z) for non-informative latent variable z = z i , i ∈ [M, L]. Here M ∈ N + is close to L, ⌈•⌉ is the ceiling function, and η ≥ 1 is set to reduce the impact of informative variable z i .</p><p>For a strong decoder f dec , such as long short-term memory (LSTM), the regularization L CS will impose a heavy penalty if the decoder solely relies on previous observations {x k | k &lt; j} to predict an observation x j . In that situation, a high prediction probability will be assigned to the observation x j even if the latent variable z contains very limited information about the raw data X.</p><p>Training, inference, and running times. Our framework is very different from the latent diffusion in both training and inference. We depict the workflows of our framework in Fig. <ref type="figure">3</ref>, with the training and sampling procedures respectively placed in Algorithm 1 and Algorithm 2. For training, the key points are to compute three loss functions: L VI for likelihood maximization, L DM for training diffusion models, and L CS for collapse regularization. For inference, the main difference is that the stopping time of the backward process is not 0, but a random variable. From these pseudo codes, we can see that our framework is almost as efficient as latent diffusion. We provide an in-depth analysis and empirical experiments about the running times of our framework in Appendix F.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Besides latent diffusion, our paper is also related to previous works that aim to mitigate the problem of posterior collapse for VAE <ref type="bibr" target="#b22">(Kingma &amp; Welling, 2013)</ref>. We collect three main types of such methods and apply them to improve the VAE of latent diffusion, with the corresponding experiment results shown in Table <ref type="table" target="#tab_1">1</ref> of Sec. 6 and Table <ref type="table" target="#tab_4">4</ref> of Appendix F.3. In the following, we briefly introduce those baselines and explain their limitations.</p><p>KL annealing. This method <ref type="bibr" target="#b9">(Bowman et al., 2016)</ref> assigns an adaptive weight to control the effect of KL-divergence term, so that VAE is unlikely to fall into the local optimum of posterior collapse at the initial optimization stage. While this method indeed mitigates the problem, it still cannot fully eliminate the negative impact of the risky KL-divergence regularization.</p><p>Decoder weakening. A representative method in this class is Variable Masking <ref type="bibr" target="#b34">(Semeniuta et al., 2017)</ref>, which randomly masks input observations to the autoregressive decoder, such that the decoder is forced to rely more on the latent variable for predicting the next observation. However, this method will make the model less expressive since the decoder is weakened.</p><p>Skip connections. With the aim to improve the impact of latent variables on the recurrent decoder, this approach (Dieng et al., 2019) directly feeds the latent variable into the decoder at every step, not only at the first step. However, the latent variable in that case acts as a constant input signal at every time step, so the decoder will still tend to ignore this redundant information.</p><p>Compared with the above baselines, our framework can address the problem of posterior collapse and is free from their side effects (e.g., less expressive decoder). The experiment results in Sec. 6 and Appendix F confirm that our framework indeed performs better in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We have conducted extensive experiments to verify that our framework is free from posterior collapse and outperforms latent diffusion (with or without previous baselines) in terms of time series generation. Due to the limited space, some other important empirical studies are put in the appendix, including more time-series datasets and another evaluation metric in Appendix F.3, diverse data modalities (e.g., text) in Appendix F.4, ablation studies in Appendix F.1, and the study of running times in Appendix F.2. The experiment setup is also placed in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">STABLE POSTERIOR OF OUR FRAMEWORK</head><p>To show that our framework has a non-collapsed posterior q VI (z | X), we follow the same experiment setup (e.g., datasets) as Sec. 3.3 and average the dependency measures m t,0 , m subfigures, we can see that, while the global dependency m t,0 still decreases with increasing time step t, it converges into a value around 0.5, which is also a bit higher than the converged first-order local dependency m t,t-1 . These results indicate that latent variable z in our framework maintains its control of decoder f dec during the whole conditional generation process p gen (X | z).</p><p>For shuffled time series in the right two subfigures, we can see that the global dependency m t,0 is always around or above 1, and the local dependency m t,t-1 is negative most of the time. These results indicate that the decoder f dec only relies on latent variable z and the context x t-1 even has a negative impact on conditional generation p gen (X | z), suggesting our framework is without dependency illusion. Based on all our findings, we conclude that: compared with latent diffusion (Fig. <ref type="figure">2</ref>), our framework is free from the effects of posterior collapse (e.g., strong decoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">PERFORMANCES IN TIME SERIES GENERATION</head><p>In this part, we aim to verify that our framework outperforms latent diffusion in terms of time series generation, which is intuitive since our framework is free from posterior collapse. We also include some other methods that are proposed by previous works to mitigate the problem, including KL annealing <ref type="bibr">(Fu et al., 2019a</ref><ref type="bibr">), variable masking (Bowman et al., 2016)</ref>, and skip connections <ref type="bibr" target="#b12">(Dieng et al., 2019)</ref>. We adopt the Wasserstein distances <ref type="bibr" target="#b6">(Bischoff et al., 2024)</ref> as the metric.</p><p>The experiment results on three commonly used time-series datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. From the results, we can see that, regardless of the used dataset and the backbone of autoencoder, our framework significantly outperforms latent diffusion and the baselines, which strongly confirms our intuition. For example, with the backbone of Transformer, our framework achieves 2.53 points lower than latent diffusion w/ KL Annealing on the WARDS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we provide a solid analysis of the negative impacts of posterior collapse on time-series latent diffusion and introduce a new framework that is free from this problem. For our analysis, we begin with a theoretical insight, showing that the problem will reduce latent diffusion to VAE, rendering it less expressive. Then, we introduce a useful tool: dependency measure, which quantifies the impacts of various inputs on an autoregressive decoder. Through empirical dependency estimation, we show that the latent variable has a vanishing impact on the decoder and find that latent diffusion exhibits a phenomenon of dependency illusion. Compared with standard latent diffusion, our framework gets rid of the risky KL-divergence regularization, permits an unlimited prior distribution, and lets the decoder be sensitive to the latent variable. Extensive experiments on multiple real-world time-series datasets show that our framework has no symptoms of posterior collapse and notably outperforms the baselines in terms of time series generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THE IMPACT OF POSTERIOR COLLAPSE</head><p>Under the assumption of posterior collapse, the below equality:</p><formula xml:id="formula_26">q VI (z | X) = p prior (z) = N (z; 0, I),<label>(15)</label></formula><p>holds for any latent variable z ∈ R D and any conditional X ∈ R T D . Then, note that q latent (z) = q VI (z | X)q raw (X)dX = N (z; 0, I)q raw (X)dX = N (z; 0, I) q raw (X)dX = N (z; 0, I),</p><p>which is exactly our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RECURRENT ENCODERS</head><p>We mainly implement the backbone of decoder f dec as LSTM <ref type="bibr" target="#b19">(Hochreiter &amp; Schmidhuber, 1997)</ref> or Transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. In the former case, we apply the latent variable z to initialize LSTM and condition it on prefix X 1:t-1 to compute the representation h t . Formally, the LSTMbased decoder f dec is as</p><formula xml:id="formula_28">s t = LSTM(s t-1 , x t-1 ), ∀t ≥ 1 h t = W 2 f tanh(W 1 f s t ) , (<label>17</label></formula><formula xml:id="formula_29">)</formula><p>where s t is the state vector of LSTM and W 2 f , W 1 f are learnable matrices. In particular, for the corner case t = 1, we fix s 0 , x 0 as zero vectors.</p><p>In the later case, we just treat latent variable z as x 0 . Therefore, we have</p><formula xml:id="formula_30">[s t-1 , s s-2 , • • • , s 0 ] = Transformer(x t-1 , x t-2 , • • • , x 0 ) h t = W 2 f tanh(W 1 f s t-1 ) , (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>where the subscript alignment results from self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DERIVATION OF DEPENDENCY MEASURES</head><p>Integrated gradient <ref type="bibr" target="#b36">(Sundararajan et al., 2017)</ref> is a very effective method of feature attributions. Our proposed dependency measures can be regarded as its extension to the case of sequence data and vector-valued neural networks. In the following, we provide the derivation of dependency measures.</p><p>For the computation h t = f dec (X 0:t-1 ), suppose the output of decoder f dec at origin O 0:t-1 is h t , then we apply the fundamental theorem of calculus as</p><formula xml:id="formula_32">h t -h t = 1 0 df dec (γ(s)) ds ds,<label>(19)</label></formula><p>where γ(s) is a straight line connecting the origin O 0:t-1 and the input X 0:t-1 as γ(s) = sX 0:t-1 + (1 -s)O 0:t-1 . Based on the chain rule, the above equality can be expanded as</p><formula xml:id="formula_33">h t -h t = 1 0 t-1 j=0 k=D k=1 df dec (γ j,k (s)) dγ j,k (s) dγ j,k (s) ds ds = t-1 j=0 1 0 k=D k=1 x j,k df dec (γ j,k (s)) dγ j,k (s) ds ,<label>(20)</label></formula><p>where γ j,k (s) denote the k-th dimension s • x j,k of the j-th vector sx j in point γ(s). Intuitively, every term inside the outer sum operation t-1 j=0 represents the additive contribution of variable x j (to the output difference h t -h t ) along the integral line γ(s). Table <ref type="table">2</ref>: Ablation studies of the hyper-parameters N, M , which are respectively used in the estimations of likelihood loss L VI and collapse penalty L CS . Here LD is short for latent diffusion and the symbolmeans "Not Applicable".</p><p>To simplify the notation, we denote the mentioned term as</p><formula xml:id="formula_34">m t,j = 1 0 k=D k=1 x j,k df dec (γ j,k (s)) dγ j,k (s) ds.<label>(21)</label></formula><p>Since m t,j is a vector, we map the new term to a scalar and re-scale it as</p><formula xml:id="formula_35">m t,j = &lt; m t,j , h t -h t &gt; &lt; h t -h t , h t -h t &gt; ,<label>(22)</label></formula><p>which is exactly our definition of the dependency measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROPERTIES OF OF DEPENDENCY MEASURES</head><p>Firstly, in terms of Eq. ( <ref type="formula" target="#formula_35">22</ref>), it is obvious that the dependency measure m t,j is signed: the measure can be either positive or negative. Then, based on Eq. ( <ref type="formula" target="#formula_33">20</ref>), we have</p><formula xml:id="formula_36">h t -h t = t-1 j=0 m t,j .<label>(23)</label></formula><p>By taking an inner product with the vector h t -h t at both sides, we get</p><formula xml:id="formula_37">&lt; h t -h t , h t -h t &gt;=&lt; t-1 j=0 m t,j , h t -h t &gt;= t-1 j=0 &lt; m t,j , h t -h t &gt; .<label>(24)</label></formula><p>By rearranging the term, we finally arrive at</p><formula xml:id="formula_38">1 = t-1 j=0 &lt; m t,j , h t -h t &gt; &lt; h t -h t , h t -h t &gt; = t-1 j=0 m t,j ,<label>(25)</label></formula><p>which is exactly our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EXPERIMENT DETAILS</head><p>We have adopted three widely used time-series datasets for both analysis and model evaluation, including MIMIC <ref type="bibr" target="#b20">(Johnson et al., 2016)</ref>, WARDS <ref type="bibr">(Alaa et al., 2017a)</ref> We use almost the same model configurations for all experiments. The diffusion models are parameterized by a standard U-net <ref type="bibr" target="#b32">(Ronneberger et al., 2015)</ref>, with L = 1000 diffusion iterations and hidden dimensions {128, 64, 32}. The hidden dimensions of autoencoders and latent variables are fixed as 128. The conditional distribution p gen (X | z) is parameterized as a Gaussian, with learnable mean vector and diagonal covariance matrix functions. For our framework, N, M are respectively selected as 50, 100, with γ = 2 and η = 1. We also apply dropout with a ratio of 0.1 to most layers of neural networks. We adopt Adam algorithm <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref> with the default hyperparameter setting to optimize our model. For Table <ref type="table" target="#tab_1">1</ref> and<ref type="table">Table 2</ref>, every number is averaged over 10 different random seeds, with a standard deviation less than 0.05. For the computing resources, all our models can be trained on 1 NVIDIA Tesla V100 GPU within 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ADDITIONAL EXPERIMENTS</head><p>Due to the limited space of our main text, we put the results of some minor experiments here in the appendix. Notably, we will adopt more datasets and another evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 ABLATION STUDIES</head><p>We have conducted ablation studies to verify that our hyper-parameter selections N = 50, M = 100 are optimal. The experiment results are shown in Table <ref type="table">2</ref>. For both N and M , either increasing or decreasing their values results in worse performance on the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 STUDY ON RUNNING TIMES</head><p>Our framework only incurs a minor increase in training time and enjoys the same inference speed as the latent diffusion. For training, while our framework will run the decoder a second time for collapse simulation L CS , it can be made in parallel with the first run of decoder f dec for likelihood computation L VI . Therefore, the training is still efficient on GPU devices. Our framework also has a different way of variational inference to infer latent variable z from data X. However, it admits a closed-form solution and is thus as efficient as the reparameterization trick of latent diffusion. For inference, our framework has no difference from the latent diffusion: sampling the latent variable z with the reverse diffusion process and running the decoder f dec in one shot.</p><p>To show the running times in practice, we perform an experiment on the MIMIC dataset as shown in Table <ref type="table" target="#tab_3">3</ref>. We can see that our framework indeed only has a minor increase for training. Given that our framework is free from posterior collapse and delivers better generation performances, this slight time investment is well worth it. (MMD) <ref type="bibr" target="#b13">(Dziugaite et al., 2015)</ref>. Lower MMD scores indicate better generative models. From the results shown in Table <ref type="table" target="#tab_4">4</ref>. We can see that our framework still significantly outperforms the baselines in terms of all the new benchmarks. For example, with LSTM as the backbone, our framework achieves a score that is 29.79% lower than Skip Connections on the Energy dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 MORE DATA MODALITIES</head><p>While our paper primarily focused on time series data, our framework is generally applicable to other types of data, including your mentioned text and images.</p><p>Experiment on text data. For text data, considering that natural language sentences exhibits a sequential structure similar to time series, it is intuitive that the posterior of text latent diffusion might also collapse. This intuition is supported by many evidences from previous works <ref type="bibr" target="#b8">(Bowman et al., 2015;</ref><ref type="bibr">Fu et al., 2019b)</ref>. To verify that our framework is capable of improving text latent diffusion, we have conducted an experiment using two publicly available text datasets: ATIS <ref type="bibr" target="#b17">(Hemphill et al., 1990)</ref> and SNIPS <ref type="bibr" target="#b10">(Coucke et al., 2018)</ref>.</p><p>The numbers in this table represent BLEU scores <ref type="bibr" target="#b30">(Papineni et al., 2002)</ref>, a widely used metric for evaluating text generation models. Higher scores indicate better performance. As the results shown in Table <ref type="table" target="#tab_5">5</ref>, we can see that our framework has significantly improved the text latent diffusion and notably outperformed a strong baseline-Skip Connections-across all datasets and backbones. Therefore, our framework also applies to text data.</p><p>Experiment on image data. For image data, we provide a detailed discussion in Sec. 4.1 of our paper: Image latent diffusion is rarely affected by posterior collapse due to its non-autoregressive decoder. To confirm this claim in practice, we conduct an experiment comparing latent diffusion with our framework on the widely used CIFAR-10 dataset <ref type="bibr" target="#b23">(Krizhevsky et al., 2009)</ref>.</p><p>The results are shown in Table <ref type="table" target="#tab_6">6</ref>. The numbers in this table represent FID scores <ref type="bibr" target="#b29">(Naeem et al., 2020)</ref>, a common metric for evaluating image generation models. Lower scores indicate better performance. Our results show that both the baseline model (i.e., KL Annealing) and our framework improve the image latent diffusion to some extent. However, the improvements are minor, suggesting that image models are almost free from posterior collapse.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The results of averaged dependency measures and error bars for our framework, which should be compared with those (e.g., Fig. 2) of latent diffusion, showing that our framework has a stable posterior and is without dependency illusion.</figDesc><graphic coords="9,117.90,81.86,376.15,162.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,127.80,81.86,356.40,175.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,127.80,81.87,356.33,153.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Wasserstein distances of different models on three widely used time-series datasets. The lower the distance metric, the better the generation quality. More results from other time-series datasets, with another evaluation metric, are placed in Table4of Appendix F.3.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="3">MIMIC WARDS Earthquakes</cell></row><row><cell>Latent Diffusion</cell><cell>LSTM</cell><cell>5.19</cell><cell>7.52</cell><cell>5.87</cell></row><row><cell>Latent Diffusion w/ KL Annealing</cell><cell>LSTM</cell><cell>4.28</cell><cell>5.74</cell><cell>3.88</cell></row><row><cell>Latent Diffusion w/ Variable Masking</cell><cell>LSTM</cell><cell>4.73</cell><cell>6.01</cell><cell>4.26</cell></row><row><cell>Latent Diffusion w/ Skip Connections</cell><cell>LSTM</cell><cell>3.91</cell><cell>4.95</cell><cell>3.74</cell></row><row><cell>Our Framework</cell><cell>LSTM</cell><cell>2.29</cell><cell>3.16</cell><cell>2.67</cell></row><row><cell>Latent Diffusion</cell><cell>Transformer</cell><cell>5.02</cell><cell>7.46</cell><cell>5.91</cell></row><row><cell>Latent Diffusion w/ KL Annealing</cell><cell>Transformer</cell><cell>4.31</cell><cell>5.54</cell><cell>3.51</cell></row><row><cell cols="2">Latent Diffusion w/ Variable Masking Transformer</cell><cell>4.42</cell><cell>5.97</cell><cell>4.45</cell></row><row><cell cols="2">Latent Diffusion w/ Skip Connections Transformer</cell><cell>3.75</cell><cell>4.67</cell><cell>3.69</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>2.13</cell><cell>3.01</cell><cell>2.49</cell></row></table><note><p><p><p>t,t-1 over 500 sampled time series. The results are illustrated in Fig.</p>4</p>. For ordered time series in the left two</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ModelBackbone N for L VI M for L CS Diffusion Iterations L MIMIC WARDS</figDesc><table><row><cell>Latent Diffusion</cell><cell>Transformer</cell><cell>-</cell><cell>-</cell><cell>1000</cell><cell>5.02</cell><cell>7.46</cell></row><row><cell cols="2">LD w/ Skip Connections Transformer</cell><cell>-</cell><cell>-</cell><cell>1000</cell><cell>3.75</cell><cell>4.67</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>2.13</cell><cell>3.01</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>50</cell><cell>50</cell><cell>1000</cell><cell>2.59</cell><cell>3.32</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>50</cell><cell>150</cell><cell>1000</cell><cell>2.71</cell><cell>3.46</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>50</cell><cell>200</cell><cell>1000</cell><cell>2.83</cell><cell>3.75</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>2.31</cell><cell>3.16</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>100</cell><cell>100</cell><cell>1000</cell><cell>2.38</cell><cell>3.24</cell></row><row><cell>Our Framework</cell><cell>Transformer</cell><cell>150</cell><cell>100</cell><cell>1000</cell><cell>2.75</cell><cell>3.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>, and Earthquakes (U.S. Geological Survey, 2020). The setup of the first two datasets are introduced in Sec. 3.3. For MIMIC, we specially simplify it into a version of univariate time series for the illustration purpose, which is only used in the experiments shown in Fig.1. All other experiments are about multivariate time series. For the Earthquakes dataset, it is about the location and time of all earthquakes in Japan from 1990 to 2020 with magnitude of at least 2.5 from U.S. Geological Survey (2020). We follow the same preprocessing procedure for this dataset as<ref type="bibr" target="#b25">Li (2023)</ref>. Comparison of Training and Inference Times on the MIMIC dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">Training Time Inference Time</cell></row><row><cell>Latent Diffusion</cell><cell>2hr 10min</cell><cell cols="2">5min 12s</cell></row><row><cell>Our Framework</cell><cell>2hr 50min</cell><cell cols="2">5min 17s</cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell cols="2">Retail Energy</cell></row><row><cell>Latent Diffusion</cell><cell cols="3">Transformer 0.037</cell><cell>0.052</cell></row><row><cell cols="4">Latent Diffusion w/ Skip Connections Transformer 0.033</cell><cell>0.043</cell></row><row><cell>Our Framework</cell><cell cols="4">Transformer 0.025 0.031</cell></row><row><cell>Latent Diffusion</cell><cell></cell><cell>LSTM</cell><cell>0.041</cell><cell>0.057</cell></row><row><cell cols="2">Latent Diffusion w/ Skip Connections</cell><cell>LSTM</cell><cell>0.035</cell><cell>0.047</cell></row><row><cell>Our Framework</cell><cell></cell><cell>LSTM</cell><cell cols="2">0.027 0.033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison on two new time-series datasets, with another metric: MMD.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>F.3 MORE DATASETS AND ANOTHER EVALUATION METRICWe conduct additional experiments on 2 more public UCI time-series datasets<ref type="bibr" target="#b5">(Bay et al., 2000)</ref>: Retail and Energy, with another widely used evaluation metric: maximum mean discrepancy Performance comparison on two text datasets, with BLEU as the metric.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">ATIS SNIPS</cell></row><row><cell>Latent Diffusion</cell><cell cols="2">Transformer 37.12</cell><cell>59.36</cell></row><row><cell cols="3">Latent Diffusion w/ Skip Connections Transformer 40.56</cell><cell>65.41</cell></row><row><cell>Our Framework</cell><cell cols="3">Transformer 51.73 78.12</cell></row><row><cell>Latent Diffusion</cell><cell>LSTM</cell><cell>35.38</cell><cell>55.72</cell></row><row><cell>Latent Diffusion w/ Skip Connections</cell><cell>LSTM</cell><cell>39.27</cell><cell>60.31</cell></row><row><cell>Our Framework</cell><cell>LSTM</cell><cell cols="2">48.46 71.45</cell></row><row><cell>Method</cell><cell cols="3">Backbone CIFAR-10</cell></row><row><cell>Latent Diffusion</cell><cell>U-Net</cell><cell>3.91</cell><cell></cell></row><row><cell>Latent Diffusion w/ KL Annealing</cell><cell>U-Net</cell><cell>3.87</cell><cell></cell></row><row><cell>Our Framework</cell><cell>U-Net</cell><cell>3.85</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on an image dataset, with FID as the metric.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from clinical judgments: Semi-Markov-modulated marked Hawkes processes for risk prognosis</title>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v70/alaa17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personalized risk scoring for critical care prognosis using mixtures of gaussian processes</title>
		<author>
			<persName><forename type="first">Jinsung</forename><surname>Ahmed M Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fixing a broken ELBO</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/alemi18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoencoders, unsupervised learning, and deep architectures</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on unsupervised and transfer learning</title>
		<meeting>ICML workshop on unsupervised and transfer learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The uci kdd archive of large data sets for data mining research and experimentation</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Stephen D Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padhraic</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A practical guide to statistical distances for evaluating generative models in science</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alana</forename><surname>Darcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Deistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Gerken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gloeckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Haxel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaivardhan</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Janne K Lappalainen</surname></persName>
		</author>
		<author>
			<persName><surname>Macke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12636</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">Alp</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
		<ptr target="https://aclanthology.org/K16-1002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=AAWuCvzaVt" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avoiding latent variable collapse with generative skip models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v89/dieng19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting>the Twenty-Second International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2019-04-18">16-18 Apr 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2397" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational neural machine translation</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Eikema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4315</idno>
		<ptr target="https://aclanthology.org/W19-4315" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Burcu</forename><surname>Can</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</editor>
		<meeting>the 4th Workshop on Representation Learning for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="124" to="141" />
		</imprint>
	</monogr>
	<note>RepL4NLP-2019</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cyclical annealing schedule: A simple approach to mitigating KL vanishing</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1021</idno>
		<ptr target="https://aclanthology.org/N19-1021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="240" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10145</idno>
		<title level="m">Cyclical annealing schedule: A simple approach to mitigating kl vanishing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The atis spoken language systems pilot corpus</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Charles T Hemphill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">June 24-27, 1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ts-diffusion: Generating highly complex time series with diffusion models</title>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.03303</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Soft mixture denoising: Beyond the expressive bottleneck of diffusion models</title>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Van Breugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=aaBnFAyW9O" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding posterior collapse in generative latent variable models</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xaVLUYuE" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7176" to="7185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The law of large numbers and its applications</title>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Sedor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Lakehead University; Thunder Bay, ON, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1066</idno>
		<ptr target="https://aclanthology.org/D17-1066" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to multi-layer feed-forward neural networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Svozil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kvasnicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Pospichal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and intelligent laboratory systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="62" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
