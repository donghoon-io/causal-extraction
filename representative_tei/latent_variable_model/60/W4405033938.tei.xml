<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptive Diabetic Retinopathy Grading with Model Absence and Flowing Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-02">2 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenxin</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojing</forename><surname>Yi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Sichuan Eye Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunxiao</forename><surname>Zu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Peking Union Medical College Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptive Diabetic Retinopathy Grading with Model Absence and Flowing Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-02">2 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.01203v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain shift (the difference between source and target domains) poses a significant challenge in clinical applications, e.g., Diabetic Retinopathy (DR) grading. Despite considering certain clinical requirements, like source data privacy, conventional transfer methods are predominantly modelcentered and often struggle to prevent model-targeted attacks. In this paper, we address a challenging Online Model-aGnostic Domain Adaptation (OMG-DA) setting, driven by the demands of clinical environments. This setting is characterized by the absence of the model and the flow of target data. To tackle the new challenge, we propose a novel approach, Generative Unadversarial ExampleS (GUES), which enables adaptation from a data-centric perspective. Specifically, we first theoretically reformulate conventional perturbation optimization in a generative way-learning a perturbation generation function with a latent input variable. During model instantiation, we leverage a Variational AutoEncoder to express this function. The encoder with the reparameterization trick predicts the latent input, whilst the decoder is responsible for the generation. Furthermore, the saliency map is selected as pseudo-perturbation labels.</p><p>Because it not only captures potential lesions but also theoretically provides an upper bound on the function input, enabling the identification of the latent variable. Extensive comparative experiments on DR benchmarks with both frozen pre-trained models and trainable models demonstrate the superiority of GUES, showing robustness even with small batch size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diabetic Retinopathy (DR) is a significant health concern, ranking among the leading causes of blindness and affecting millions of people worldwide <ref type="bibr" target="#b2">[3]</ref>. Early-stage intervention for DR is crucial to preserve vision, highlighting the importance of timely diagnosis <ref type="bibr" target="#b26">[27]</ref>. Although deep learning (DL) has demonstrated promising results in automating the grading of DR <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37]</ref>, deploying DL models in realworld clinical settings remains challenging. For example, DL models often struggle to generalize effectively to complex scenarios, such as variations in imaging equipment, ethnic groups, or temporal factors, leading to different data distributions, a challenge known as domain shift <ref type="bibr" target="#b10">[11]</ref>. This issue significantly hampers the widespread adoption and success of DL-based diagnostic tools in clinical practice <ref type="bibr" target="#b13">[14]</ref>.</p><p>Recently, many adaptation methods for grading diabetic retinopathy (DR) have focused on addressing the issue of domain shift <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref>. The initial focus on classic transfer learning strategies, including Unsupervised Domain Adaptation (UDA) <ref type="bibr" target="#b19">[20]</ref> and Domain Generalization (DG) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, necessitated the availability of well-annotated source data.</p><p>Nevertheless, the growing emphasis on privacy protection has shifted research toward the Source-Free Domain Adaptation (SFDA) framework <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41]</ref>. SFDA involves adapting a source model-pre-trained on the source domain-to the target domain in a self-supervised manner, thereby ensuring the protection of source patient data.</p><p>In recent developments, specific needs have emerged in the clinical field. The introduction of model weight-based techniques for reconstructing training data has created a demand for model privacy <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref>, which goes beyond traditional source data protection. In addition, there is a growing requirement for models capable of handling incoming patient data in a flowing fashion, referred to as a flowing data constraint <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>. Unfortunately, existing SFDA methods cannot effectively address this challenge, as they rely on full access to the model and require offline training on a pre-collected dataset. Fig. <ref type="figure">1</ref> provides an intuitive illustration of this issue.</p><p>In this paper, we consider a clinically motivated setting, called Online Model-aGnostic Domain Adaptation (OMG-DA) and propose a novel Generative Unadversarial ExampleS (GUES) approach for the DR grading problem in this new setting. Specifically, OMG-DA presents an extreme safety scenario: The available target data is unlabeled and arrives in a flowing format, with no prior information about the pre-trained source model and data. Tab. 1 provides a detailed comparison with previous adaptation settings.</p><p>In GUES, we address the absence of source data and pre-trained models by producing generalized unadversarial examples <ref type="bibr" target="#b23">[24]</ref> for unlabeled target data. To this end, we introduce generative unadversarial learning, which theoretically reformulates conventional iterative perturbation optimization. This new method aims to learn a generative function for perturbations and involves addressing two key tasks: <ref type="bibr" target="#b0">(1)</ref> Identifying the latent function input, which is the derivative of initial random noise w.r.t. image data, and (2) selecting a self-supervised property to serve as pseudo-perturbation labels. In practice, we leverage a Variational Autoencoder (VAE) <ref type="bibr" target="#b9">[10]</ref> based approach to facilitate this learning process.</p><p>In terms of function representation, we model the latent input using the encoder along with the reparameterization trick, whilst the decoder accomplishes the generation of individualperturbation. Additionally, we choose the saliency map as the pseudo-perturbation label for two reasons: (1) It helps discover potential lesions, and (2) it aids in identifying the latent input by providing an upper bound.</p><p>Our contributions are summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Adaptation methods for DR grading. Driven by realworld medical requirements, domain adaptation has been an attractive topic in this DR grading issue. For instance, Nguyen et al. <ref type="bibr" target="#b19">[20]</ref> introduce a UDA approach that enables the model to focus on vessel structures that remain invariant to domain shifts via image reconstruction using labeled source domain data. In SFDA, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> propose generating labeled, target-style retinal images to improve the source model's generalization, relying solely on a pre-trained source model and unlabeled target images. Additionally, DG in DR grading has been explored through domain-invariant feature learning approaches <ref type="bibr" target="#b3">[4]</ref> and divergence-based methods <ref type="bibr" target="#b4">[5]</ref>, leveraging labeled source domain data. These methods above rely on labeled data, require full access to the model, and necessitate offline training on a precollected dataset. In real clinical settings, these requirements can be impractical due to constraints around data and model privacy, as well as the need for real-time adaptability without retraining. In contrast, GUES provides an online adaptation solution that operates without requiring labeled data or access to the model, addressing the domain adaptation problem in DR grading.</p><p>Unadversarial learning. Unadversarial learning was initially developed by Salman et al. <ref type="bibr" target="#b23">[24]</ref>, aiming to modify input image distribution to make them more easily recognizable by the model. Current mainstreams achieve this learning process by adding class-specific perturbations to the input images. Here, the perturbations are generated based on the gradient of an objective function w.r.t. image. This approach allows for the design of unadversarial examples without model training. For example, based on <ref type="bibr" target="#b23">[24]</ref>, NSA <ref type="bibr" target="#b24">[25]</ref> introduces a method to generate more natural perturbations using a trainable generator. Similarly, CAT <ref type="bibr" target="#b17">[18]</ref> demonstrates a new distance metric for generating unadversarial examples.</p><p>All existing unadversarial learning methods require access to model parameters, outputs, and labeled data. This dependency invalidates them in our OMG-DA setting which only has access to unlabeled target data. In addition to this, our GUES produces individualized unadversarial examples in a generative manner, which stands out from previous methods that focus on class-specific unadversarial examples.</p><p>Saliency map for medical image. A fine-grained saliency map is a pixel feature generated by calculating the centralsurround differences within images, identifying salient regions without any need for training <ref type="bibr" target="#b18">[19]</ref>. This feature is widely applied in various medical image analysis tasks to extract pathologically important regions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. For instance, in DR grading, studies such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> utilize saliency maps to guide models in focusing on critical features like the optic disc, cup, and vessel structures. Similarly, in brain tumor detection, Tomar et al. <ref type="bibr" target="#b30">[31]</ref> leverage saliency maps to enhance the model's attention on tumor and bone structures. In skin cancer detection, saliency maps help isolate lesion regions with distinctive features, such as lumpiness, which are essential for accurate diagnosis <ref type="bibr" target="#b35">[36]</ref>.</p><p>As stated above, existing works primarily use saliency maps to highlight lesion regions. Unlike the conventional usage of saliency maps, GUES selects saliency maps as pseudo-perturbation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement of OMG-DA</head><p>Given two different but related domains, i.e., source domain S and target domain T , S contains n s labeled samples, while T has n unlabeled data. Both labeled and unlabeled samples share the same C categories. Let X s and Y s be the source samples and the corresponding labels. Similarly, we denote the target samples and their labels by X t = {x i t } n i=1 and Y t = {y i t } n i=1 , respectively, where n signifies the number of samples. The source model θ s is pre-trained on {X s , Y s }.</p><p>OMG-DA is featured in (1) the absence of the source model θ s and domain S and (2) the flowing target data X t the same as the TTA setting <ref type="bibr" target="#b34">[35]</ref>. Unlike previous transfer settings that are model adaptation-centered <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>, OMG-DA considers adaptation from the perspective of data. Specifically, OMG-DA aims to modify the distribution of target data to facilitate downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generative Unadversarial Examples</head><p>The part begins with a brief recap of traditional unadversarial learning <ref type="bibr" target="#b23">[24]</ref>. Unlike adversarial learning <ref type="bibr" target="#b27">[28]</ref> that generates confusing samples to mislead models, unadversarial learning aims to construct generalized samples, tackling promoting out-of-distribution issues. Formally, this learning can be summarized in the following optimization problem.</p><formula xml:id="formula_0">δ = arg min δ L(f θ (x + δ), y), s.t. ||δ|| ≤ ϵ<label>(1)</label></formula><p>where L (•) denotes objective function, e.g., cross-entropy loss for classification tasks, x and y are input image and its label, f θ is a pre-trained model with parameters θ, δ is a perturbation, ϵ is a small threshold. The current scheme solves this problem in an iterative way formulated as</p><formula xml:id="formula_1">δ k+1 = δ k + α • sign (∇xL(f θ (x + δ k ), y)) , k ∈ [0, K -1],<label>(2)</label></formula><p>where α is a trade-off parameter, K is iteration number, δ 0 is an initial random noise. In the inference phase, the optimal perturbation δ is integrated into the input x, forming an unadversarial example x = x + δ, which is easily recognizable by the model f θ . Obviously, the conventional unadversarial paradigm cannot meet our OMG-DA setting due to the absence of f θ , L, and the label y.</p><p>In this paper, we re-consider the iterative optimization process above and obtain the theorem below (The proof is provided in Supplementary).</p><p>Theorem 1 Given the unadversarial learning problem defined in Eq. (9), the iterative process featured by Eq. (10) can be expressed as the following generative form.</p><formula xml:id="formula_2">δ k = δ 0 + V • F Φ ∂δ 0 ∂x ,<label>(3)</label></formula><p>where δ 0 is an initial random noise, V &gt; 0 is a bound constant, F Φ is a generative function, ∂δ0 ∂x is a latent variable.</p><p>Grounded on Theorem 1, we have: When δ k converges to optimal δ, i.e., δ k → δ, function F Φ also evolves to the optimal one, denoted by FΦ , i.e., F Φ → FΦ . This provides an insight: The unadversarial learning problem above can also be solved in a generative fashion. Correspondingly, the generated data are termed generative unadversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Instantiation</head><p>Within this context of generative unadversarial learning, conventional unadversarial learning presented in Eq. ( <ref type="formula" target="#formula_9">9</ref>) is boiled down to learning F Φ ∂δ0 ∂x . We can achieve this by training a generative neural network. To make this solution sense, we have to solve two difficulties as follows. (A) One is the identification of ∂δ0 ∂x when the relationship between them is unknown. (B) The other is selecting property supervision (pseudo-perturbation labels) to drive δ k → δ.</p><p>Solution to problem A. In practice, considering derivative ∂δ0 ∂x is relevant with both random noise δ 0 and input image x, we sample it from a certain Gaussian distribution associated with x. Furthermore, the output size of F Φ is the same as x. Therefore, we employ the VAE model to jointly model ∂δ0 ∂x and F Φ , since VAE is an autoencoder characterized by random sampling. Specifically, as shown in Fig. <ref type="figure">2 (a)</ref>, we approximate ∂δ0 ∂x by latent variable z, which is jointly determined by input x and sampled random signal τ . As for F Φ , it is demonstrated by the decoder module. Suppose D(•) is the decoder, E τ (•) is the encoder with reparameterization trick, our scheme can be formulated as</p><formula xml:id="formula_3">D(Eτ (x)) → FΦ ∂δ0 ∂x , Eτ (x) → ∂δ0 ∂x , D(•) → FΦ(•).<label>(4)</label></formula><p>Solution to problem B. We adopt the fine-grained saliency map as the supervision. Two reasons contribute to our selection. First of all, the empirical results show that, for the specific task of DR grading, the saliency map is an acceptable pseudo-perturbation. Specifically, perturbation in the unadversarial context enhances the regions associated with the category and reduces the prominence of other areas, thereby identifying the lesion zones relevant to DR grading.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>, the saliency map effectively identifies potential lesions, such as hemorrhages, soft exudates, and hard exudates. Furthermore, it includes gradient information, as it highlights regions similar to Grad-CAM (Right side).</p><p>More importantly, we have the theorem below. (The proof is provided in Supplementary)</p><p>Theorem 2 Given the partial derivatives of the initial random noise δ 0 w.r.t image x is ∂δ0 ∂x and x's saliency map is s = G(x) where G(•) is the computation function of the saliency map. We have the following relationship:</p><formula xml:id="formula_4">∂δ 0 ∂x ≤ U • s,<label>(5)</label></formula><p>where U &gt; 0 is a bound constant.</p><p>Theorem 2 suggests that the saliency map provides upper bounds for ∂δ0 ∂x . Namely, s provides relaxed descriptions for the variation ∂δ0 ∂x . This can help guide the learning of ∂δ0 ∂x .</p><p>GUES framework. Based on the analysis above, we instantiate GUES as the framework depicted in Fig. <ref type="figure">2</ref>. As shown in sub-figure (a), our method integrates a VAE model and by-pass connection, achieving the learning of F Φ ∂δ0 ∂x . Specifically, ∂δ0 ∂x is sampled from an input x t -featured Gaussian distribution N (µ, σ), which is jointly learned using the encoder and reparameterization. The decoder, representing F Φ , then transforms ∂δ0 ∂x into a generative perturbation δ t . Finally, the by-pass structure incorporates x t and δ t to produce xt . During the inference phase, as shown in Fig. <ref type="figure">2 (b)</ref>, for a specific testing sample, the trained GUES model outputs the corresponding generative unadversarial examples to the frozen or fine-tuning model that early unseen. Loss function The loss function for GUES training consists of two components. First, we enforce the latent space with mean µ and variance σ satisfy the standard normal distribution N (0, I). Suppose the encoder in VAE models the posterior distribution q(z|x t ) = N (µ(x t ), σ 2 (x t )), this regularization can be formulated as:</p><formula xml:id="formula_5">L KL = D KL (q(z|x t )∥N (0, I)) ,<label>(6)</label></formula><p>where function D KL computes the Kullback-Leibler divergence. The other reconstruction loss between the unadversarial example xt and saliency map g t is presented by the following regression form:</p><formula xml:id="formula_6">L MSE = ∥x t -g t ∥ 2 .<label>(7)</label></formula><p>Formally, combining Eq. ( <ref type="formula" target="#formula_5">6</ref>) and Eq. ( <ref type="formula" target="#formula_6">7</ref>), the final objective of GUES can be summarized as:</p><formula xml:id="formula_7">L GUES = αL KL + βL MSE ,<label>(8)</label></formula><p>where α and β are trade-off parameters. For clarity, we summarize the training procedure of GUES in Algorithm 1. In Eq. ( <ref type="formula" target="#formula_7">8</ref>), the first item L KL ensures the learning of ∂δ0 ∂x . On the one hand, as aforementioned, we use z to present ∂δ0 ∂x . L KL aligns the z space with N (0, I), thereby linking the random noise to ∂δ0 ∂x . On the other hand, q(z|x t ) in L KL is a function of input x t , building relationship x t to ∂δ0 ∂x . Additionally, the reconstruction regulated by the seconded item L MSE encourage δ k → δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We perform evaluation experiments on four existing fundus benchmarks, including APTOS <ref type="bibr" target="#b0">[1]</ref>, DDR <ref type="bibr" target="#b12">[13]</ref>, DeepDR <ref type="bibr" target="#b16">[17]</ref>, and Messidor-2 (termed MD2) <ref type="bibr" target="#b6">[7]</ref>. Those datasets share five grading/classes: no DR, mild DR, moderate DR, severe DR, and proliferative DR. Taking each dataset as a separate domain, we form 12 transfer tasks crossing domains. For example, as APTOS is the source domain while Algorithm 1 The pipeline of proposed GUES Input: Online batch samples B, a trainable VAE θ v consists of an encoder E τ (•) with the reparameterization trick and a decoder D(•). Procedure:</p><formula xml:id="formula_8">1: for x i in B do 2:</formula><p>Approximate the latent function input by E τ (x i ); Create the unadversarial example xi by incorporating δ i and x i through the bypass path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Generate a fine-grained saliency map g i of the x i ; 7:</p><p>Update θ v with Eq.( <ref type="formula" target="#formula_7">8</ref>), taking g t as a supervision. It should be noted that all datasets have a severe class imbalance (e.g., "no DR" class itself takes up to 45.8% of the DDR dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Detail</head><p>Souce model pre-training. We adopt the DeiT-base network <ref type="bibr" target="#b31">[32]</ref> as the backbone of the source pre-trained model, training it in a supervised manner using the source data and corresponding ground truths. During this source training phase, the adopted objective is the classic cross-entropy loss with label smoothing, the same as other methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Variational autoencoder setting. The VAE model is an eight-layer convolutional architecture with a latent space dimension of 10. We do not employ a pre-trained VAE and utilize a VAE without fine-tuning on any other dataset, ensuring that the learning component F Φ ∂δ0 ∂x are unbiased and independent of prior pre-training data. Parameter setting. For the trade-off parameters in Eq. ( <ref type="formula" target="#formula_7">8</ref>), we set α to 1.0, while β is tuned with {0.0001, 0.01, 1} to ensure that the loss values of L KL and L MSE remain on the same scale. Training setting. We adopt the batch size of 64, SGD optimizer with a momentum of 0.9 and a learning rate of 1e-5 on all datasets. All experiments are conducted with PyTorch on a single GPU of RTX A6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison Settings</head><p>Evaluation metrics. To account for unbalanced datasets, in addition to conventional classification accuracy (termed ACC), we adopt the measure of Quadratic Weighted Kappa (termed QWK) <ref type="bibr" target="#b1">[2]</ref> and the average of QWK and ACC (termed AVG). The computation rules of them are provided in Supplementary.</p><p>Competitors. We compare GUES with nine existing stateof-the-art adaptation methods divided into three groups. (1)</p><p>The first group involves applying the source model directly to the target domain. <ref type="bibr" target="#b1">(2)</ref> The second group includes five SFDA methods SHOT <ref type="bibr" target="#b14">[15]</ref>, NRC <ref type="bibr" target="#b38">[39]</ref>, CoWA <ref type="bibr" target="#b11">[12]</ref>, PLUE <ref type="bibr" target="#b15">[16]</ref>, and TPDS <ref type="bibr" target="#b28">[29]</ref>.</p><p>(3) The third group comprises three typical TTA methods: SHOT-IM <ref type="bibr" target="#b14">[15]</ref>, TENT <ref type="bibr" target="#b34">[35]</ref>, and SAR <ref type="bibr" target="#b20">[21]</ref>. %DWFK6L]H $&amp;&amp; 6+27,0 7(17 *8(66+27,0 *8(67 <ref type="bibr" target="#b16">(17</ref> %DWFK6L]H 4:. %DWFK6L]H $9* Corresponding to the comparison protocols above, besides the version GUES corresponding to the case without training, we also introduce GUES+SHOT-IM and GUES+TENT which correspond to the case with training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison Results</head><p>In this part, we present the comparison results following the cases mentioned above. Also, considering batch size a crucial factor for TTA methods, the results as the batch size varies is provided. Results without training. The comparisons are shown in Tab. 2. On average, across the 12 tasks and without training of the source model, GUES achieves improvements of 4.5% in ACC, 2.8% in QWK, and 3.7% in AVG compared to the source model. These results demonstrate that GUES modifies the target data distribution effectively, adapting the  target domain to align with the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results with training.</head><p>As shown in Tab. 2, GUES+SHOT-IM outperforms the previous best SFDA and TTA methods, respectively surpassing TENT in ACC by 2.7%, SHOT in QWK by 1.9%, and SHOT-IM in AVG by 2.6% on average. Meanwhile, compared to SHOT-IM, GUES+SHOT-IM gains over 3.0% in ACC, 2.2% in QWK, and 2.6% in AVG. Similarly, GUES+TENT improves over TENT by 1.7% in ACC, 4.1% in QWK, and 2.9% in AVG. These results highlight the effectiveness of combining GUES with other methods that require training. Results with varying batch size. This part isolates the effect of batch size, which is a crucial factor for TTA methods. Fig. <ref type="figure" target="#fig_2">4</ref> depicts the performance variation as batch size varying from 2 to 64 over the 12 tasks. It is observed that TTA methods SHOT-IM and TENT suffer from severe performance drops when the batch size becomes small. SHOT-IM exhibits a decrease of approximately 16% in ACC when the batch size is reduced from 64 to 2, whilst TENT shows a substantial decline of around 34% in QWK. Oppose to it, the methods with GUES, SHOT-IM+GUES, and TENT+GUES, do not have evident performance decline at the smaller batch size. Moreover, this combination not only mitigates the drop but also shows improvements when the batch size is 64. This indicates that GUES effectively stabilizes the performance of SHOT-IM and TENT, enhancing their robustness to variations in batch size while boosting their overall effectiveness.</p><p>We attribute GUES's excellent robustness to its ability to predict individual perturbations (see Supplementary for more details on the visualization of perturbations) that focus  on single image-specific features rather than global data commonalities. For instance, the conventional unadversarial examples approach refines the class-specific perturbation sensitive to batch size. Furthermore, both SHOT-IM and TENT are entropy-based methods that require large-scale batch size for accurate entropy estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Visualization Analysis</head><p>For a better understanding, Fig. <ref type="figure" target="#fig_3">5</ref> demonstrates whether GUES can help capture pathologically relevant features, such as H (hemorrhages), SE (soft exudates), and EX (hard exudates), determining DR grade. First of all, when comparing the source model with GUES, the source model only captures a limited area of the lesion, while GUES effectively captures most of the DR-related features. Furthermore, combining GUES with SHOT-IM (i.e., GUES+SHOT-IM) expands the focus on DR-related features beyond those captured by SHOT-IM alone. Additionally, when comparing the four models to Oracle, only GUES and GUES+SHOT-IM resemble Oracle, suggesting that GUES effectively directs the model's attention to DR-critical features. Feature distribution. Taking the task DDR→APTOS as a toy experiment, we visualize the feature distribution extracted from the final convolutional layer of the prediction model using a 3D density chart. Considering that APTOS is a class-imbalanced dataset, with the "No DR" class alone comprising up to 49.2% of the dataset, our analysis focuses </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Further Analysis</head><p>Ablation study. In this part, we evaluate the effect of objective loss, as well as the components involved in GUES including the sampling strategy and saliency map-based supervision. To address the first issue, we conduct a progressive experiment. The top four rows of Tab. 3 list the ablation results where the source model's performance is the baseline. Using L KL or L MSE alone yields an average ACC improvement of approximately 1.4% and 1.8%, respectively, over the baseline. As both of them work, the ACC increases 3.5% on average further. The results indicate that all objective components positively affect the final performance.</p><p>To evaluate the impact of the sampling component, we propose a variation method of GUES, GUES w/ AE, where we remove this sampling process by replacing VAE with a conventional Auto-encoder model. In addition, two GUES variations are used to assess the advantage of saliency mapbased supervision. Specifically, GUES w/ Mixup replaces the saliency maps with a Mixup of saliency maps and origi- nal images, whilst GUES w/ Self replaces the saliency maps with the original images. As presented in the Tab. 3, compared with the full version of GUES (the fourth row), GUES w/ AE, GUES w/ Mixup and GUES w/ Self decrease by 3.1% at least on average. Besides, replacing the original images with saliency maps as inputs (GUES w/ Sal) leads to a significant drop of about 15.4%. Above these experiments confirm the effectiveness of our design choices.</p><p>Parameter sensitiveness. Taking the task DDR→APTOS as a toy experiment, we present the GUES performance varying as hyper-parameters 0.5 ≤ α ≤ 1.5 with 0.1 steps, 0.00005 ≤ β ≤ 0.00014 with 0.00001 steps. As depicted in Fig. <ref type="figure" target="#fig_7">8</ref>, the ACC, QWK, and AVG variation surfaces show fluctuations in the tiny performance zone, with approximately 0.2% in ACC, 0.25% in QWK, and 0.4% in AVG. This observation suggests that GUES is insensitive to alterations in α and β.</p><p>Limitation. GUES uses the saliency map to guide the learning of the generative function. This method is effective for DR grading but encounters challenges in natural image scenarios. The natural images contain rich semantics, such as shape, relative structure, and complex background, which are not all relevant to tasks. However, the saliency map blindly highlights all those factors, struggling to capture the task-specific ones. In a theoretical point-view, the richness of these semantics means significant variations, resulting in a super-relaxed bound constant U (see Theorem 2) that undermines the descriptive power of the saliency map for ∂δ0 ∂x . In contrast, fundus images are more monolithic, implying a smaller U that justifies the usage of the saliency map. (The further discussion is provided in Supplementary)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a clinically motivated setting, OMG-DA, where the models are unseen prior to their use, and only target data flows are accessible. This setting ensures both model protection and source data privacy in a data flow scenario. To adapt to the target domain without access to the models, we introduce a GUES approach. Instead of conventional iterative optimization, we generate unadversarial examples for flowing target data by directly predicting individual perturbations. This approach is grounded in the theoretical results of generative unadversarial learning. In practice, we utilize the VAE model to learn the perturbation generation function with a latent input variable. Furthermore, we demostrate that saliency maps can serve as an upper bound for this latent variable. This relationship inspires us to use the saliency maps as pseudo-perturbation labels for model training. Extensive experiments conducted on four DR benchmarks confirm that the proposed method can achieve state-of-the-art results, when it pairs with both frozen pre-trained and fine-tuning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptive Diabetic Retinopathy Grading with Model</head><p>Absence and Flowing Data Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Reproducibility Statement</head><p>The code and data will be made available after the publication of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Proof of Theorem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">A Proof of Theorem 1</head><p>Recalling traditional unadversarial learning. Unadversarial learning aims to develop an image perturbation that enhances the performance on a specific class, which can be succinctly described as follows:</p><formula xml:id="formula_9">δ = arg min δ L(f θ (x + δ), y), s.t. ||δ|| ≤ ϵ<label>(9)</label></formula><p>where L (•) denotes objective function, x and y are input image and its label, f θ is a pre-trained model with parameters θ, δ is a perturbation, ϵ is a small threshold. Solves this problem in an iterative way formulated as</p><formula xml:id="formula_10">δ k+1 = δ k + α • sign (∇xL(f θ (x + δ k ), y)) , k ∈ [0, K -1],<label>(10)</label></formula><p>where α is a trade-off parameter, K is iteration number, δ 0 is an initial random noise. We re-consider the iterative optimization process above and obtain the theorem below.</p><p>Restatement of Theorem 1 Given the unadversarial learning problem defined in Eq. (9), the iterative process featured by Eq. (10) can be expressed as the following generative form.</p><formula xml:id="formula_11">δ k = δ 0 + V • F Φ ∂δ 0 ∂x ,<label>(11)</label></formula><p>where δ 0 is an initial random noise, V is a bound constant, F Φ is a generative function.</p><p>Proof. First, according to the chain principle, we can convert Eq. ( <ref type="formula" target="#formula_10">10</ref>) into</p><formula xml:id="formula_12">δ k+1 = δ k + α • ∂L ∂f θ • ∂f θ ∂x • 1 + ∂δ k ∂x .<label>(12)</label></formula><p>Since that the learning will converge to the unadversarial examples, α • ∂L ∂f θ • ∂f θ ∂x is bounded by a certain constant, denoted by U k &gt; 0, thereby Eq. ( <ref type="formula" target="#formula_12">12</ref>) become</p><formula xml:id="formula_13">δ k+1 ≤ δ k + U k 1 + ∂δ k ∂x .<label>(13)</label></formula><p>We make a further substitution on δ k according to the law presented in Eq. ( <ref type="formula" target="#formula_13">13</ref>), leading to</p><formula xml:id="formula_14">δ k+1 ≤ δ k-1 + U k-1 1 + ∂δ k-1 ∂x + U k 1 + ∂δ k ∂x .<label>(14)</label></formula><p>By continuing this substitution on δ k-1 , • • • , δ 0 in order, we have</p><formula xml:id="formula_15">δ k+1 ≤ δ 0 + U 0 1 + ∂δ 0 ∂x + U 1 1 + ∂δ 1 ∂x + • • • + U i 1 + ∂δ i ∂x + • • • + U k 1 + ∂δ k ∂x ≤ δ 0 + U m k + ∂δ 0 ∂x + ∂δ 1 ∂x + • • • + ∂δ k ∂x ,<label>(15) where</label></formula><formula xml:id="formula_16">U m = max{U 0 , U 1 , • • • , U k }.</formula><p>To obtain generative form, we explore the relationships between { ∂δ1 ∂x , ∂δ2 ∂x , • • • , ∂δ k ∂x } and ∂δ0 ∂x , respectively. To this end, we first investigate the relationship between ∂δ1 ∂x and ∂δ0 ∂x , combining Eq. ( <ref type="formula" target="#formula_13">13</ref>).</p><formula xml:id="formula_17">∂δ 1 ∂x ≤ ∂δ 0 ∂x + U 1 • ∂ ∂x ∂δ 0 ∂x = h 1 ∂δ 0 ∂x<label>(16)</label></formula><p>where, h 1 (•) stands for an equivalent function. For ∂δ2 ∂x , we have the following equation based on Eq. ( <ref type="formula" target="#formula_13">13</ref>) and Eq. ( <ref type="formula" target="#formula_17">16</ref>).</p><formula xml:id="formula_18">∂δ 2 ∂x ≤ ∂δ 1 ∂x + U 2 • ∂ ∂x ∂δ 1 ∂x = h 1 ∂δ 0 ∂x + U 2 • ∂ ∂x h 1 ∂δ 0 ∂x = h 2 ∂δ 0 ∂x<label>(17)</label></formula><p>In the recursion way presented by Eq. ( <ref type="formula" target="#formula_17">16</ref>) and Eq. ( <ref type="formula" target="#formula_18">17</ref>), { ∂δ3 ∂x , • • • , ∂δ k ∂x } can be expressed as</p><formula xml:id="formula_19">∂δ 3 ∂x ≤ h 3 ∂δ 0 ∂x , • • • , ∂δ k ∂x ≤ h k ∂δ 0 ∂x<label>(18)</label></formula><p>Therefore, substituting Eq. ( <ref type="formula" target="#formula_17">16</ref>), ( <ref type="formula" target="#formula_18">17</ref>) and ( <ref type="formula" target="#formula_19">18</ref>) into Eq. ( <ref type="formula" target="#formula_15">15</ref>), we have</p><formula xml:id="formula_20">δ k+1 ≤ δ0 + Um k + ∂δ0 ∂x + h1 ∂δ0 ∂x + • • • + h k ∂δ0 ∂x .<label>(19)</label></formula><p>Let</p><formula xml:id="formula_21">F Φ ∂δ0 ∂x = k + ∂δ0 ∂x + h 1 ∂δ0 ∂x + • • • + h k ∂δ0 ∂x</formula><p>and V be a value that makes the equality relationship hold. Eq. ( <ref type="formula" target="#formula_20">19</ref>) becomes the generative form below.</p><formula xml:id="formula_22">δ k = δ 0 + V • F Φ ∂δ 0 ∂x .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">A Proof of Theorem 2</head><p>Recalling the calculation of the fine-grained saliency map.</p><p>It calculates saliency by measuring central-surround differences within images.</p><formula xml:id="formula_23">G (h, w) = ς max {cen (h, w) -sur (h, w, ς) , 0} , cen (h, w) = I (h, w) , sur (h, w, ς) = h ′ =ς h ′ =-ς w ′ =ς w ′ =-ς I(h + h ′ , w + w ′ ) -I(h, w) (2ς + 1) 2 -1 ,<label>(21)</label></formula><p>where (h, w) is the coordinate of one pixel in grey-scale image (transformed by x t ) with its corresponding value denoted as I(w, h), and ς ∈ {1, 3, 7} denotes surrounding values.</p><p>Restatement of Theorem 2 Given the partial derivatives of the initial random noise δ 0 w.r.t image x is ∂δ0 ∂x and x's saliency map is s = G(x) where G is the computation function of saliency map. We have the following relationship:</p><formula xml:id="formula_24">∂δ 0 ∂x ≤ U • s,<label>(22)</label></formula><p>where U &gt; 0 is a bound constant.</p><p>Proof. we treat s as a middle variable, thus ∂δ0 ∂x can be expressed as the following equation by the chain law.</p><formula xml:id="formula_25">∂δ 0 ∂x = ∂δ 0 ∂s • ∂s ∂x ≤ U • ∂s ∂x ,<label>(23)</label></formula><p>where U &gt; 0 is a bound constant. In Eq. ( <ref type="formula" target="#formula_25">23</ref>), the inequality holds because both the initial noise and the specific saliency map are bounded, resulting in the relative changes between them also being restricted. In addition, according to the definition of derivative, we have</p><formula xml:id="formula_26">∂s ∂x = ∂G(x) ∂x ≈ G(x + △ x ) -G(x) △ x ,<label>(24)</label></formula><p>where △ x is a tiny variation. It is known that the saliency map at (h, w) is only related to itself and its surrounding pixels. Without loss of generality, we build the proof based on the simplest surround case ς = 1 where △ x at (h, w) is presented by Fig. <ref type="figure">9</ref>. According to Eq. ( <ref type="formula" target="#formula_23">21</ref>), we have  Thus, G(x + △ x ) at (h, w) can be expressed as</p><formula xml:id="formula_28">G hw (x + △x) = ς max{ cen (h, w) - 1 8 sur (h, w, ς) - 1 8 sur △ (ς, △x) -I △ , 0}<label>(26)</label></formula><p>Let A 1 = cen (h, w) and A 2 = sur (h, w, ς), B 1 = cen (h, w) - 1  8 sur (h, w, ς), B 2 = 1 8 sur △ (ς, △ x ) -I △ . Eq. ( <ref type="formula" target="#formula_26">24</ref>) has two situations as follows.</p><formula xml:id="formula_29">• S-1. When A 1 &gt; A 2 , B 1 &gt; B 2 or A 1 &lt; A 2 , B 1 &gt; B 2 , G(x + △x) -G(x) △x = I △ -1 8 sur △ (ς, △x) I △ = 1 2 - 4 i=1 I △i I △<label>(27)</label></formula><p>• S-2. When</p><formula xml:id="formula_30">A 1 &gt; A 2 , B 1 &lt; B 2 or A 1 &lt; A 2 , B 1 &lt; B 2 , G(x + △x) -G(x) △x = 0<label>(28)</label></formula><p>The results presented above suggest a insight that ∂s ∂x is proportional to the saliency map s, namely</p><formula xml:id="formula_31">∂s ∂x ∝ s.<label>(29)</label></formula><p>There are two reasons contributing to this conclusion. First, ∂s ∂x ' values confine to a binary situation. More importantly, as shown in Eq. ( <ref type="formula" target="#formula_29">27</ref>), ∂s ∂x describes the relative change relationship between the current pixel and its surrounding pixels. Combing Eq. ( <ref type="formula" target="#formula_25">23</ref>) and Eq. ( <ref type="formula" target="#formula_31">29</ref>), we have</p><formula xml:id="formula_32">∂δ0 ∂x ≤ U • ∂s ∂x ∝ U • s.<label>(30)</label></formula><p>9. Implementation Details    <ref type="table" target="#tab_7">4</ref>. Specifically, in APTOS, the "No DR" class comprises about 49.2% of all samples. In DDR, "No DR" accounts for approximately 45.8%, while in DeepDR, it makes up around 45.7%. In Messidor-2, the "No DR" class represents about 58.2% of the total data. The domain shift of datasets. Each dataset is treated as a distinct domain, with significant variations from factors like country of origin, patient demographics, and differences in imaging equipment used for acquisition. Additionally, analysis of the RGB statistics for proliferative DR (PDR) samples across these datasets/domains reveals distinct fluctuations in each channel (R, G, and B), highlighting the unique visual styles and characteristics of each dataset, as shown in Fig. <ref type="figure" target="#fig_9">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Evaluation metrics.</head><p>The computation rules for accuracy (termed ACC), Quadratic Weighted Kappa (termed QWK), and the average of QWK and ACC (termed AVG) are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACC =</head><p>T P + T N T P + T N + F P + F N ,</p><formula xml:id="formula_33">QW K = 1 - n i=1 n j=1 W (i, j) • O(i, j) n i=1 n j=1 W (i, j) • E(i, j) , Wi,j = (i -j) 2 (C -1) 2 AV G = 1 2 (ACC + QW K) ,<label>(31)</label></formula><p>where T P , T N , F P , and F N represent true positives, true negatives, false positives, and false negatives, respectively. i is a true category, j is a predicted category, C is the number of classes, and n is the total number of samples. O(i, j) is the observed frequency, which represents how many times the true category i was predicted as category j, and E(i, j) is the expected frequency, which indicates how many times category i would be predicted as category j under random guessing, E(i, j) = P (i) × P (j) × n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Supplementary Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1.">Results with Varying Batch Size</head><p>As a supplement to the results with varying batch sizes, Table 5 presents the complete performance of three evaluation metrics across all 12 tasks. TTA methods SHOT-IM and TENT show a performance drop when the batch size is small. Specifically, SHOT-IM decreases by approximately 14.1% in ACC, 3.9% in QWK, and 9.1% when comparing batch sizes of 2 and 64. TENT decreases by approximately 3.0%   . Visualization of a fundus image, a natural image, and their corresponding saliency maps. The fundus image is sampled from APTOS, and the natural image is sampled from Office-Home <ref type="bibr" target="#b33">[34]</ref>. In (e), the amplitude spectrum of these four images is displayed.</p><p>in ACC, 34.1% in QWK, and 18.6% when comparing batch sizes of 2 and 64. However, when these methods are combined with our proposed method, GUES, the decline is not as significant. In SHOT-IM+GUES, the performance shows a decrease of only 2.0% in ACC, 2.0% in QWK, and 2.1% in AVG. In TENT+GUES, the performance shows a decrease of only 0.4% in ACC, 0.8% in QWK, and 0.7% in AVG. These results indicate that our method can prevent declines when the batch size is small, as it predicts individual perturbations that are robust to batch size variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Visualization for Generative Perturbations.</head><p>As depicted in Fig. <ref type="figure">11</ref>, it is evident that different input images exhibit distinct perturbations, as observed directly in the second row. To be more specific, the RGB distribution of the perturbations, illustrated in the third row, further highlights their variability. This analysis demonstrates how GUES dynamically adjusts the perturbations to account for the unique characteristics of each input image, effectively tailoring them to align with the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.">Why are Saliency Maps Unsuitable for Natural Images?</head><p>As we early stated, the proposed method cannot tackle the natural image scenarios well. This part executes a further discussion for this issue using two typical images illustrated in Fig. <ref type="figure" target="#fig_11">12</ref> (a) and (b). There are two key observations to note. First, the fundus image has a simpler background and structure compared to the natural image, which features richer semantics, including diverse shapes, complex relative structures, and intricate backgrounds. This difference is reflected in the amplitude spectrum in Fig. <ref type="figure" target="#fig_11">12</ref>(e), where the fundus image displays a significantly lower frequency band. Second, the saliency maps effectively highlight variations in both fundus and natural images. This is indicated by the fact that the amplitudes of the saliency maps are much larger than the corresponding amplitudes of the images at similar frequencies.</p><p>The effects of this enhancement differ between fundus images and natural images. For simpler fundus images, the noticeable variations are typically related to lesions, making the enhancement useful for highlighting these specific regions (see Fig. <ref type="figure" target="#fig_11">12 (c</ref>)). In contrast, complex natural images exhibit variations that span the entire scene, such as areas of forest, grass, shadows, and a person riding a bike. In this case, the enhancement draws attention to all elements in the image, which can obscure the factors that are relevant to the task at hand. Therefore, we believe that refining a proper self-supervised signal for natural images represents a promising research direction for the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. The instantiation framework of GUES in the OMG-DA setting. (a) For target input xt, the VAE model generates individual perturbation δt = FΦ ∂δ 0 ∂x . After that, the by-pass path incorporates δt and xt to create the generative unadversarial example xt. Treating xt's saliency map gt as reconstruction supervision for model training. (b) At the inference phase, the generated unadversarial example xt is directly provided to the frozen source model or other trainable models.</figDesc><graphic coords="4,141.02,335.52,70.97,70.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 :</head><label>3</label><figDesc>Learn a generative function for perturbation δ i by D; 4: Calculate individual perturbations δ i by D(E τ (x)); 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison results with batch size varying from 2 to 64 over the 12 tasks (The details are provided in Supplementary.). Left, middle, and right report ACC, QWK, and AVG, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Interpretability analysis based on a typical fundus image from "Moderate DR" class in APTOS. Here, H (hemorrhages), SE (soft exudates), and EX (hard exudates) are essential characteristics to judge the DR grade. The gradient CAM-based heatmap of five models visualizes the capture of those lesions. All models are trained on task DDR→APTOS, where Oracle is trained using ground truth in APTOS.</figDesc><graphic coords="7,138.83,219.18,75.60,75.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Feature distribution comparison of 3D density charts on task DDR→APTOS. Oracle is trained on APTOS by ground truth.</figDesc><graphic coords="7,60.73,219.18,75.60,75.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(c) Unadversarial examples (a) Original images (b) Predicted perturbations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Unadversarial examples visualization of two typical target samples from APTOS. The generative perturbations are generated by the GUES model trained on task DDR→APTOS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Parameter sensitiveness study results over α × β based on task DDR→APTOS. From left to right, there are results on ACC, QWK, and AVG, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 i=1= 4 i=1</head><label>44</label><figDesc>cen (h, w, △x) = cen (h, w) + I △ = I hw + I △ . sur (h, w, ς, △x) = (Ii + I △i ) -(I hw + I △ ) 8 , Ii -I hw + 4 i=1 I △i -I △ 8 , = sur(h, w, ς) + sur △ (ς, △x) 8 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Visualize the styles and characteristics of each dataset by analyzing the RGB statistics of proliferative diabetic retinopathy (PDR) samples across APTOS, DDR, DeepDR, and Messidor-2.</figDesc><graphic coords="13,427.93,114.74,96.89,59.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8 Figure 11 .</head><label>811</label><figDesc>Figure 11. Visualization for input images, generative perturbations, and RGB statistic of the corresponding perturbations on transfer task DDR→APTOS.</figDesc><graphic coords="14,140.98,284.21,54.68,54.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12</head><label>12</label><figDesc>Figure 12. Visualization of a fundus image, a natural image, and their corresponding saliency maps. The fundus image is sampled from APTOS, and the natural image is sampled from Office-Home<ref type="bibr" target="#b33">[34]</ref>. In (e), the amplitude spectrum of these four images is displayed.</figDesc><graphic coords="14,437.33,385.53,92.80,84.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between different transfer settings. Notation: source (s), target (t), data x, label y, loss function L (•).</figDesc><table><row><cell>Setting name</cell><cell cols="5">Model availability Data flow Source data privacy Source data Target data</cell><cell>Train loss</cell><cell>Test loss</cell></row><row><cell>Fine-tuning (FT)</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>xt, y t</cell><cell>L (xt, y t )</cell><cell>-</cell></row><row><cell>Domain generalization (DG)</cell><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>xs, y s</cell><cell>-</cell><cell>L (xs, y s )</cell><cell>-</cell></row><row><cell>Unsupervised domain adaptation (UDA)</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>xs,y s</cell><cell>xt</cell><cell>L (xs, y s ) + L (xt)</cell><cell>-</cell></row><row><cell>Source-free domain adaptation (SFDA)</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>xt</cell><cell>L (xt)</cell><cell>-</cell></row><row><cell>Test-time adaptation (TTA)</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>xt</cell><cell>-</cell><cell>L (xt)</cell></row><row><cell>Online model-agnostic domain adaptation (OMG-DA)</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>xt</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>8: end for 9: return The generative unadversarial examples x the others are target domains, we have three transfer tasks APTOS→DDR, APTOS→DeepDR, and APTOS→MD2. The illustration of the label distribution and the domain shift of the four datasets is demonstrated in Supplementary.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The results of Source, SFDA, TTA, OMG-DA, and OMG-DA combination methods on datasets APTOS, DeepDR, DDR, and MD2 are presented. The improvements over baseline methods Source, SHOT-IM, and TENT are highlighted as (+x.x).</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell cols="4">APTOS→DDR APTOS→DeepDR APTOS→MD2 DDR→APTOS DDR→DeepDR DDR→MD2 DeepDR→APTOS ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG</cell></row><row><cell>Source</cell><cell>-</cell><cell cols="4">60.6 59.2 59.9 52.6 71.7 62.1 60.9 48.7 54.8 65.6 72.9 69.3 45.0 60.6 52.8 49.4 34.6 42.0 43.4 71.6 57.5</cell></row><row><cell>GUES</cell><cell>-</cell><cell cols="4">62.0 59.5 60.8 53.0 69.7 61.3 59.8 46.7 53.3 76.0 81.8 78.9 56.4 68.7 62.5 59.1 47.6 53.3 46.5 74.1 60.3</cell></row><row><cell>SHOT [15]</cell><cell cols="5">ICML20 66.9 69.0 67.9 53.6 73.5 63.6 51.7 38.0 44.8 77.0 84.2 80.6 59.2 74.6 66.9 57.1 43.1 50.1 62.3 82.5 72.4</cell></row><row><cell>NRC [39]</cell><cell cols="5">NeurIPS21 61.9 65.2 63.5 51.6 70.9 61.3 54.1 40.3 47.2 60.3 76.3 68.3 52.0 69.1 60.5 50.6 35.2 42.9 52.0 74.3 63.2</cell></row><row><cell>CoWA [12]</cell><cell cols="5">ICML22 59.0 64.9 62.0 51.0 70.7 60.8 53.0 37.3 45.1 57.2 74.5 65.8 50.1 66.8 58.4 53.1 39.6 46.3 50.4 73.0 61.7</cell></row><row><cell>PLUE [16]</cell><cell cols="5">CVPR23 62.0 65.1 63.6 51.3 69.7 60.5 54.5 41.1 47.8 63.4 64.2 63.8 54.3 64.8 59.5 51.6 28.2 39.9 54.6 70.5 62.6</cell></row><row><cell>TPDS [29]</cell><cell>IJCV24</cell><cell cols="4">66.6 67.8 67.2 51.6 71.5 61.5 52.7 40.7 46.7 76.6 83.9 80.2 58.0 73.1 65.6 54.5 41.0 47.8 60.8 80.0 70.4</cell></row><row><cell>SHOT-IM [15]</cell><cell cols="5">ICML20 66.5 69.2 67.9 52.6 73.6 63.1 53.2 36.6 44.9 75.9 82.1 79.0 58.5 73.9 66.2 57.3 43.5 50.4 61.9 84.0 72.9</cell></row><row><cell>TENT [35]</cell><cell>ICLR20</cell><cell cols="4">59.9 50.2 55.1 53.1 70.1 61.6 61.4 48.4 54.9 75.2 82.4 78.8 55.1 68.4 61.7 60.8 50.5 55.6 60.2 79.7 69.9</cell></row><row><cell>SAR [21]</cell><cell>ICLR23</cell><cell cols="4">67.9 63.8 65.8 53.6 73.0 63.3 57.2 44.9 51.1 75.6 83.5 79.5 55.6 71.6 63.6 49.2 37.0 43.1 59.5 79.3 69.4</cell></row><row><cell cols="2">GUES +SHOT-IM -</cell><cell cols="4">68.6 68.5 68.5 53.5 72.8 63.2 55.7 43.8 49.7 77.2 83.1 80.2 60.5 75.1 67.8 61.5 51.2 56.3 62.6 83.2 72.9</cell></row><row><cell>GUES +TENT</cell><cell>-</cell><cell cols="4">61.8 56.3 59.0 53.2 70.0 61.6 61.1 47.2 54.1 75.9 83.0 79.4 58.7 70.8 64.7 63.3 53.8 58.6 54.9 77.6 66.3</cell></row><row><cell>Method</cell><cell>Venue</cell><cell>DeepDR→DDR DeepDR→MD2 MD2→APTOS ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG ACC QWK AVG MD2→DDR MD2→DeepDR</cell><cell>ACC</cell><cell>Avg. QWK</cell><cell>AVG</cell></row><row><cell>Source</cell><cell>-</cell><cell>56.4 66.9 61.7 48.7 50.2 49.4 43.9 70.3 57.1 60.2 56.5 58.3 59.8 58.4 59.1</cell><cell>53.9</cell><cell>60.1</cell><cell>57.0</cell></row><row><cell>GUES</cell><cell>-</cell><cell cols="4">57.3 65.6 61.4 48.3 52.0 50.1 59.3 76.2 67.7 64.7 55.8 60.2 58.6 57.1 57.8 58.4 (+4.5) 62.9 (+2.8) 60.7 (+3.7)</cell></row><row><cell>SHOT [15]</cell><cell cols="2">ICML20 57.4 71.2 64.3 48.8 41.5 45.2 52.7 73.0 62.8 54.6 59.2 56.9 59.6 70.2 64.9</cell><cell>58.4</cell><cell>65.0</cell><cell>61.7</cell></row><row><cell>NRC [39]</cell><cell cols="2">NeurIPS21 44.9 60.9 52.9 49.8 41.8 45.8 48.8 69.1 58.9 52.8 52.8 52.8 58.0 62.8 60.4</cell><cell>53.1</cell><cell>59.9</cell><cell>56.5</cell></row><row><cell>CoWA [12]</cell><cell cols="2">ICML22 48.7 58.4 53.6 49.9 42.7 46.3 51.0 70.1 60.5 49.6 50.9 50.3 57.6 60.6 59.1</cell><cell>56.9</cell><cell>61.9</cell><cell>59.4</cell></row><row><cell>PLUE [16]</cell><cell cols="2">CVPR23 47.2 53.5 50.4 56.4 47.4 51.9 56.0 69.1 62.6 56.5 54.3 55.4 58.8 64.8 61.8</cell><cell>55.5</cell><cell>57.7</cell><cell>56.6</cell></row><row><cell>TPDS [29]</cell><cell>IJCV24</cell><cell>59.3 69.4 64.3 50.5 42.4 46.4 60.3 74.9 67.6 60.0 60.4 60.2 58.9 63.0 60.9</cell><cell>59.2</cell><cell>64.0</cell><cell>61.6</cell></row><row><cell>SHOT-IM [15]</cell><cell cols="2">ICML20 54.6 69.4 62.0 51.2 38.2 44.7 61.6 77.9 69.7 57.0 58.7 57.9 57.5 69.8 63.7</cell><cell>59.0</cell><cell>64.7</cell><cell>61.9</cell></row><row><cell>TENT [35]</cell><cell>ICLR20</cell><cell>58.5 45.4 51.9 58.3 56.5 57.4 55.1 74.1 64.6 55.8 31.7 43.7 58.0 53.6 55.8</cell><cell>59.3</cell><cell>59.2</cell><cell>59.3</cell></row><row><cell>SAR [21]</cell><cell>ICLR23</cell><cell>53.0 66.3 59.6 42.6 33.1 37.9 55.2 73.0 64.1 49.7 48.3 49.0 56.7 65.8 61.3</cell><cell>56.3</cell><cell>61.6</cell><cell>59.0</cell></row></table><note><p>GUES+SHOT-IM -62.3 71.5 66.9 52.8 47.8 50.3 62.8 78.1 70.4 66.0 59.4 62.7 60.6 68.6 64.6 62.0 (+3.0) 66.9 (+2.2) 64.5 (+2.6) GUES+TENT -62.6 61.4 62.0 59.1 57.4 58.2 59.3 75.6 67.5 64.0 50.5 57.2 58.3 56.0 57.1 61.0 (+1.7) 63.3 (+4.1) 62.2 (+2.9)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>ACC results of ablation study (%).</figDesc><table><row><cell>#</cell><cell>LKL</cell><cell>LMSE</cell><cell>APTOS</cell><cell>DDR</cell><cell>DeepDR</cell><cell>MD2</cell><cell>Avg.</cell></row><row><cell>1</cell><cell>✗</cell><cell>✗</cell><cell>51.0</cell><cell>59.1</cell><cell>52.5</cell><cell>53.0</cell><cell>53.9</cell></row><row><cell>2</cell><cell>✓</cell><cell>✗</cell><cell>54.7</cell><cell>58.3</cell><cell>54.9</cell><cell>53.3</cell><cell>55.3</cell></row><row><cell>3</cell><cell>✗</cell><cell>✓</cell><cell>56.9</cell><cell>60.1</cell><cell>52.9</cell><cell>52.9</cell><cell>55.7</cell></row><row><cell>4</cell><cell>✓</cell><cell>✓</cell><cell>60.6</cell><cell>61.3</cell><cell>56.0</cell><cell>55.7</cell><cell>58.4</cell></row><row><cell>5</cell><cell cols="2">GUES w/ AE</cell><cell>53.8</cell><cell>60.2</cell><cell>53.8</cell><cell>53.2</cell><cell>55.3</cell></row><row><cell>6</cell><cell cols="2">GUES w/ Mixup</cell><cell>56.8</cell><cell>60.2</cell><cell>53.0</cell><cell>52.8</cell><cell>55.7</cell></row><row><cell>7</cell><cell cols="2">GUES w/ Self</cell><cell>56.5</cell><cell>60.4</cell><cell>53.7</cell><cell>54.4</cell><cell>56.3</cell></row><row><cell>8</cell><cell cols="2">GUES w/ Sal</cell><cell>46.1</cell><cell>34.3</cell><cell>41.8</cell><cell>45.9</cell><cell>42.0</cell></row><row><cell cols="8">on this crucial property. As shown in Fig. 6, the feature dis-</cell></row><row><cell cols="8">tribution of the source model does not reflect this imbalanced</cell></row><row><cell cols="8">characteristic; instead, it displays a more uniform classifica-</cell></row><row><cell cols="8">tion. Conversely, the feature distribution of GUES exhibits a</cell></row><row><cell cols="8">distinct imbalance, with one expanded high-density region</cell></row><row><cell cols="8">alongside several smaller high-density regions, resembling</cell></row><row><cell cols="6">the distribution pattern seen in Oracle.</cell><cell></cell><cell></cell></row><row><cell cols="8">Visualization of unadversaisal examples. This part visu-</cell></row><row><cell cols="8">alizes unadversarial examples of two typical target samples</cell></row><row><cell cols="8">from APTOS and corresponding generative perturbations,</cell></row></table><note><p><p><p>based on the task DDR→APTOS. Considering the generative perturbations alter the original images that may not be easily visible to the naked eye, we collect RGB statistics to illustrate these changes quantitatively. It is observed that in Fig.</p>7</p>, each channel (R, G, and B) exhibits notable fluctuations, with the RGB statistics of the original images (a) differing significantly from those of the unadversarial examples (c).Additionally, each generative perturbation is unique, meaning that the alterations introduced by these perturbations are individual. These results suggest that the perturbations may help highlight critical DR-related features, refining the model's focus on diagnostically relevant areas.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>)</head><label></label><figDesc>𝐼 2 , 𝐼 𝛥 2 𝐼 1 , 𝐼 𝛥 1</figDesc><table><row><cell>𝐼 4 , 𝐼 𝛥</cell><cell>4</cell></row><row><cell></cell><cell>𝐼 ℎ𝑤 , 𝐼 𝛥</cell></row><row><cell></cell><cell>𝐼 3 , 𝐼 𝛥 3</cell></row><row><cell cols="2">Figure 9. Illustration of x + △x at coordinate (h, w) as we select</cell></row><row><cell cols="2">the simplest surround case ς = 1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Label distribution of the four evaluation datasets: APTOS, DDR, DeepDR, and Messidor-2.</figDesc><table><row><cell>Dataset</cell><cell cols="6">No DR Mild DR Moderate DR Severe DR Proliferative DR Total</cell></row><row><cell>APTOS</cell><cell cols="2">1,805 370</cell><cell>999</cell><cell>193</cell><cell>295</cell><cell>3,662</cell></row><row><cell>DDR</cell><cell cols="2">6,265 630</cell><cell>4,477</cell><cell>236</cell><cell>913</cell><cell>13,673</cell></row><row><cell>DeepDR</cell><cell>914</cell><cell>222</cell><cell>398</cell><cell>354</cell><cell>112</cell><cell>2,000</cell></row><row><cell cols="3">Messidor-2 1,017 270</cell><cell>347</cell><cell>75</cell><cell>35</cell><cell>1,748</cell></row><row><cell cols="7">• APTOS. [1] The dataset originates from Kaggle's APTOS</cell></row><row><cell cols="7">2019 Blindness Detection Contest, organized by the Asia</cell></row><row><cell cols="7">Pacific Tele-Ophthalmology Society (APTOS). It com-</cell></row><row><cell cols="7">prises a total of 5,590 fundus images provided by Aravind</cell></row><row><cell cols="7">Eye Hospital in India. However, only the annotations for</cell></row><row><cell cols="7">the training set (3,662 images) are publicly accessible, and</cell></row><row><cell cols="4">these are used in this study.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">• DDR [13] The DDR dataset comprises 13,673 fundus</cell></row><row><cell cols="7">images collected from 9,598 patients across 23 provinces</cell></row><row><cell cols="7">in China. These images are classified by seven graders</cell></row><row><cell cols="7">based on features such as soft exudates, hard exudates, and</cell></row><row><cell cols="3">hemorrhages.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">• DeepDR [17] The DeepDR dataset comprises 2,000 fun-</cell></row><row><cell cols="7">dus images of both left and right eyes from 500 patients in</cell></row><row><cell cols="3">Shanghai, China.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">• Messidor-2 [7] The Messidor-2 dataset includes 1,748</cell></row><row><cell cols="7">macula-centered eye fundus images. This dataset partially</cell></row><row><cell cols="7">originates from the Messidor program partners, with addi-</cell></row><row><cell cols="7">tional images contributed by Brest University Hospital in</cell></row><row><cell>France.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">The label distribution of datasets. All datasets exhibit</cell></row><row><cell cols="6">imbalanced class distributions, as shown in Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Performance of test time adaptation methods evaluated in ACC, QWK, and AVG across different batch sizes IM [15] 44.9 54.2 58.5 58.0 59.2 59.0 55.6 60.8 60.9 62.0 63.2 64.4 64.7 62.7 52.8 57.5 60.0 60.8 61.8 61.9 59.1 TENT [35] 56.3 57.1 57.8 58.8 59.7 59.3 58.2 25.1 30.2 39.7 47.2 54.1 59.2 42.6 40.7 43.6 48.7 53.0 56.9 59.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>ACC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>QWK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AVG</cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell></cell><cell></cell><cell>53.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Test Time Adaptation Batch Size</cell><cell></cell><cell cols="5">Test Time Adaptation Batch Size</cell><cell></cell><cell cols="5">Test Time Adaptation Batch Size</cell></row><row><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 Avg.</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 Avg.</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 Avg.</cell></row><row><cell>SHOT-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Aptos</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/aptos2019-blindness-detection" />
		<title level="m">Aptos 2019 blindness detection website</title>
		<imprint>
			<date type="published" when="2022-02-20">February 20, 2022</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.Eyepacs.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps" />
		<title level="m">Quadratic weighted kappa</title>
		<imprint>
			<date type="published" when="2005">July 2022. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive diagnosis system for early signs and different diabetic retinopathy grades using fundus retinal images based on pathological changes detection</title>
		<author>
			<persName><forename type="first">Eman</forename><surname>Abdelmaksoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Elmogy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">104039</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Drgen: domain generalization in diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Atwany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Yaqub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards generalizable diabetic retinopathy grading in unseen domains</title>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep learning system for detecting diabetic retinopathy across the disease spectrum</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huating</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3242</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feedback on a publicly distributed image database: the messidor database</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Cazuguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Béatrice</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Trone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Gain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Richard</forename><surname>Ordóñez-Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Massin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Erginay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis &amp; Stereology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cabnet: Category attention block for imbalanced diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">Along</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="153" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ssit: Saliency-guided self-supervised image transformer for diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pujin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to domain adaptation and transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wouter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName><surname>Loog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11806</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Confidence score for source-free unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahuin</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanruo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applications of deep learning in fundus images: A review</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanruo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101971</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020. 3, 5, 6, 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guiding pseudo-labels with uncertainty estimation for source-free unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Litrico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepdrid: Diabetic retinopathy-grading and image quality estimation challenge</title>
		<author>
			<persName><forename type="first">Ruhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Xingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huafeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14922</idno>
		<title level="m">Cat: Collaborative adversarial training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human detection using a mobile platform and novel features derived from a visual saliency mechanism</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Montabone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised domain adaptation for diabetic retinopathy grading using vessel image reconstruction</title>
		<author>
			<persName><surname>Duy Mh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Tt</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Than</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Prange</surname></persName>
		</author>
		<author>
			<persName><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards stable test-time adaptation in dynamic wild world</title>
		<author>
			<persName><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augpaste: A one-shot approach for diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">106489</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Source-free active domain adaptation for diabetic retinopathy grading based on ultra-wide-field fundus images</title>
		<author>
			<persName><forename type="first">Jinye</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">108418. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unadversarial examples: Designing objects for robust vision</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Vemprala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nsa: Naturalistic support artifact to boost network confidence</title>
		<author>
			<persName><forename type="first">Abhijith</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Munz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Screening for diabetic retinopathy</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Daniel E Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">A</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><surname>Schachat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="660" to="671" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Source-free domain adaptation via target prediction distribution searching</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unified source-free domain adaptation</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07601</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A visual attention-based algorithm for brain tumor detection using an on-center saliency map and a superpixel-based framework</title>
		<author>
			<persName><forename type="first">Nishtha</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushmita</forename><surname>Chandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare Analytics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">100323</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On-the-fly test-time adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">Jeya</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Vs Vibashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIDL</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tent: Fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2, 3, 6, 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Saliency-guided and patch-based mixup for long-tailed skin cancer image classification</title>
		<author>
			<persName><forename type="first">Tianyunxi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pujin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10801</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarse-to-fine classification for diabetic retinopathy grading using convolutional neural network</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonglei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gouenou</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101936</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cdtrans: Cross-domain transformer for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Tongkun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Pichao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Exploiting the intrinsic neighborhood structure for source-free domain adaptation</title>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangling</forename><surname>Jui</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In NeurIPS, 2021. 3, 5</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">See through gradients: Image batch recovery via gradinversion</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Diabetic retinopathy grading by a source-free transfer learning approach</title>
		<author>
			<persName><forename type="first">Chenrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">103423</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
