<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion-Based Conditional Image Editing through Optimized Inference with Guidance</title>
				<funder ref="#_ZP4AH2R">
					<orgName type="full">Institute of Information &amp; Communications Technology Planning &amp; Evaluation</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
				<funder ref="#_kV2ugHH">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder ref="#_McKtKwv #_FbE6Qxx">
					<orgName type="full">National Research Foundation</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_H3hUfts #_cw4gaJu">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyunsoo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ECE &amp;</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minsoo</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ECE &amp;</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">ECE &amp;</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IPAI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion-Based Conditional Image Editing through Optimized Inference with Guidance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple but effective training-free approach for text-driven image-to-image translation based on a pretrained text-to-image diffusion model. Our goal is to generate an image that aligns with the target task while preserving the structure and background of a source image. To this end, we derive the representation guidance with a combination of two objectives: maximizing the similarity to the target prompt based on the CLIP score and minimizing the structural distance to the source latent variable. This guidance improves the fidelity of the generated target image to the given target prompt while maintaining the structure integrity of the source image. To incorporate the representation guidance component, we optimize the target latent variable of diffusion model's reverse process with the guidance. Experimental results demonstrate that our method achieves outstanding image-to-image translation performance on various tasks when combined with the pretrained Stable Diffusion model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diffusion-based text-to-image generation models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> have shown superior performance in generating highquality images. These models have also been adapted for text-driven image editing tasks, aiming to translate a given source image into a target domain while preserving its overall structure and background. However, text-driven image-toimage translation remains a challenging task, as it requires selectively modifying specific parts of the image while maintaining the integrity of the background and structure. Moreover, achieving precise control over fine details further adds to the complexity of this task.</p><p>To address these tasks, many text-driven image editing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> rely on additional fine-tuning of pretrained diffusion models. While these methods yield promising results, they are impractical due to the substantial computational and memory overhead required for the fine-tuning process. In contrast, training-free algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> introduce unique inference techniques for diffusion models, achieving their objectives without fine-tuning. However, despite their efficiency, these approaches often struggle to preserve the structure of the source image and tend to produce blurry results.</p><p>We propose a simple yet effective training-free imageto-image translation method built upon pretrained diffusion models. Our approach incorporates representation guidance based on a triplet loss to enhance fidelity to the target task. Specifically, we modify the reverse process of the diffusion model by integrating the representation guidance, which comprises two key components: (a) a CLIP <ref type="bibr" target="#b21">[22]</ref>-based objective that ensures the sampled target latent aligns semantically with the given target prompt, and (b) a structural constraint that enforces similarity between the source and target images' structural information, as extracted from the feature maps of the pretrained diffusion model. Experimental results demonstrate that the proposed reverse process effectively preserves the background and structure of image for both local and global editing tasks. The main contributions of our work are summarized below:</p><p>• We propose a novel representation guidance mechanism motivated by metric learning, leveraging features from the pretrained CLIP and Stable Diffusion for text-driven image-to-image translation.</p><p>• Our approach modifies the denoising process of the pretrained Stable Diffusion model without any additional training procedure.</p><p>• Experimental results on various image-to-image translation tasks using both real and synthetic images verify the outstanding performance of our approach compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text-to-image diffusion models</head><p>Existing methods <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> based on diffusion models have demonstrated remarkable performance in text-to-image gen-arXiv:2412.15798v1 [cs.CV] 20 Dec 2024 eration tasks. For instance, Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> leverages pretrained autoencoders <ref type="bibr" target="#b15">[16]</ref> to project a given image onto a low-dimensional latent space and estimates its distribution within this manifold, rather than modeling the raw data distribution directly. Imagen <ref type="bibr" target="#b24">[25]</ref> employs a large pretrained text encoder to generate text embeddings, which are then used to condition the diffusion model for image synthesis. Similarly, DALL•E 2 <ref type="bibr" target="#b22">[23]</ref> predicts the CLIP image embedding from a text caption and synthesizes an image conditioned on both the estimated CLIP embedding and the textual input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Diffusion-based image manipulation methods</head><p>Text-driven image manipulation aims to preserve the structure and background of the source image while selectively editing the image to align with the target prompt. Several works <ref type="bibr">[6-9, 17, 18, 21, 32]</ref> employ publicly available pretrained text-to-image diffusion models, such as Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>, to deal with the image editing tasks. For example, DiffEdit <ref type="bibr" target="#b5">[6]</ref> adaptively interpolates between the source and target latents at each time step, guided by an estimated object mask. Prompt-to-Prompt <ref type="bibr" target="#b8">[9]</ref> substitutes the self-attention and cross-attention maps of the source latents for those retrieved from the target latents. Plug-and-Play <ref type="bibr" target="#b31">[32]</ref> injects the self-attention and intermediate feature maps obtained from the source latents into the target image generation process. Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref>, on the other hand, optimizes the target latent to align with cross-attention maps extracted from the pretrained diffusion model, based on both the source and target latents. Null-text inversion <ref type="bibr" target="#b19">[20]</ref> first inverts a source image using an optimization-based pivotal tuning procedure to achieve precise reconstruction and then applies the Prompt-to-Prompt technique to generate the target image based on the inverted image. MasaCtrl <ref type="bibr" target="#b4">[5]</ref> modifies traditional self-attention mechanisms in diffusion models, allowing the model to utilize the local features extracted from the source image, hereby ensuring consistency across generated images. Conditional score guidance <ref type="bibr" target="#b16">[17]</ref> derives a score function conditioned on both the source image and source prompt to guide target image generation. Prompt interpolation-based correction <ref type="bibr" target="#b17">[18]</ref> refines the noise prediction for the target latent by progressively interpolating between the source and target prompts in a time-dependent manner. Unlike these approaches, InstructPix2pix <ref type="bibr" target="#b2">[3]</ref> finetunes the pretrained Stable Diffusion using source images and generated pairs of text instructions and target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Diffusion-Based Image-to-Image Translation</head><p>This section describes a simple DDIM-based method tailored for the text-driven image-to-image translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inversion process of source images</head><p>Diffusion models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> generate images through inversion and reverse processes, which correspond to the for-ward and backward processes, respectively. In the inversion process, the original data x 0 is progressively perturbed with Gaussian noise, resulting in the sequence of intermediate latent variables x 1 , x 2 , . . . , x T . For text-driven image-toimage translation, existing algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> often employ the deterministic process of DDIM <ref type="bibr" target="#b27">[28]</ref> using pretrained text-to-image diffusion models. The deterministic DDIM inversion process is defined as follows:</p><formula xml:id="formula_0">x src t+1 = f inv t (x src t ) = √ α t+1 x src t - √ 1 -α t ϵ θ (x src t , t, y src ) √ α t + 1 -α t+1 ϵ θ (x src t , t, y src ),<label>(1)</label></formula><p>where f inv t (•) denotes the inversion process at time step t, ϵ θ (•, •, •) is the noise prediction network, x src t is the noisy source image at time step t, and y src is the CLIP <ref type="bibr" target="#b21">[22]</ref> embedding of the source prompt p src . Note that x src T is obtained by recursively applying Eq. ( <ref type="formula" target="#formula_0">1</ref>) starting from the source image x src 0 , and it is subsequently used in the reverse process to generate the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reverse process of target images</head><p>The target image x tgt 0 is generated from x tgt T , which is set equal to x src T , using the DDIM reverse process given by</p><formula xml:id="formula_1">x tgt t-1 = f rev t (x tgt t ) = √ α t-1 x tgt t - √ 1 -α t ϵ θ (x tgt t , t, y tgt ) √ α t + 1 -α t-1 ϵ θ (x tgt t , t, y tgt ),<label>(2)</label></formula><p>where f rev t (•) is the reverse process function at time step t, and x tgt t and y tgt denote the target image at time step t and the CLIP feature of the target prompt p tgt , respectively. However, recursively applying Eq. ( <ref type="formula" target="#formula_1">2</ref>) to synthesize the target image often fails to preserve the overall structure and background of the source image. Therefore, we modify the reverse process to generate the desired target image in a training-free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>This section elaborates on our sampling strategy, which optimizes the deterministic reverse process of DDIM with respect to the representation guidance term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We propose the optimized objective L dist t , referred to as representation guidance, to address the limitations of the recursive application of Eq. ( <ref type="formula" target="#formula_1">2</ref>), which often fails to preserve the structure of the source image. The key idea is to regulate the reverse process, ensuring that the target image aligns semantically with the target prompt while maintaining the overall structure of the source image through the guidance term. The modified reverse process is given by</p><formula xml:id="formula_2">xtgt t-1 = f rev t (x tgt t ) = α t-1 α t xtgt t - √ 1 -α t γ t ϵ θ (x tgt t , t, y tgt ) -∇ xtgt t L dist t ,<label>(3)</label></formula><p>where f rev t (•) is defined by our modified reverse process at time step t, and γ t is equal to αt-1 αt -1-αt-1 1-αt . We will discuss how L dist t is calculated in Section 4.3. Algorithm 1 summarizes the detailed procedures of the proposed method, referred to as Optimized Inference with Guidance (OIG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Naïve representation guidance</head><p>During the modified reverse process, we simply derive a naïve distance objective, which is defined as </p><formula xml:id="formula_3">L naive-dist t := -Sim f img (x 0 (x tgt t ,</formula><formula xml:id="formula_4">x0 (x t , t, y) = x t - √ 1 -α t ϵ θ (x t , t, y) √ α t .<label>(5)</label></formula><p>Table <ref type="table">1</ref>. Quantitative comparisons of our method with DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref> Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref>, Null-text inversion <ref type="bibr" target="#b19">[20]</ref>, and MasaCtrl <ref type="bibr" target="#b4">[5]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and images sampled from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref>. InstructPix2pix <ref type="bibr" target="#b2">[3]</ref> is included only for reference because it requires extra fine-tuning. For the the drawing → oil painting task, we do not report the BD score since the background is not clearly defined. Black and red bold-faced numbers represent the best and second-best performance in each column. The second term of the right-hand side in Eq. ( <ref type="formula">4</ref>) is inspired by the ability of intermediate feature maps extracted from the noise prediction network to capture the semantic information of the source image, as demonstrated in <ref type="bibr" target="#b31">[32]</ref>. This term promotes the preservation of semantic features from the source image in the generated target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Representation guidance</head><p>Building on the metric learning framework <ref type="bibr" target="#b11">[12]</ref>, we refine the naïve objective in Eq. ( <ref type="formula">4</ref>) by introducing a stricter constraint. Specifically, we guide the generation of the target image to align more closely with the target prompt p tgt than with the source prompt p src . Furthermore, we encourage the generated target latent xtgt t to remain semantically closer to the source latent x src t than xtgt t+1 , thereby effectively preserving critical information from the source image.</p><p>Since the target latent xtgt t+1 at the previous time step inherently contains less semantic information about the source image than the source latent x src t , we propose a stricter constraint than simply aligning xtgt t with x src t . Specifically, we encourage xtgt t to diverge from the target latent xtgt t+1 at the previous timestep. This design is motivated by the intuition that such a strategy better preserves the semantic content of the source image during the reverse process. To enforce this, we impose another constraint such that the distance between M (x tgt t , t, y tgt ) and M (x src t , t, y src ) remains smaller than the distance between M (x tgt t , t, y tgt ) and M (x tgt t+1 , t + 1, y tgt ). Empirical results show that this strategy effectively helps preserve the structure and semantic information of the source image.</p><p>In summary, we define the representation guidance via the effective distance objectives to realize the aforementioned two constraints as follows:</p><formula xml:id="formula_5">L dist t := -λ 1 min(0, Sim(f img (x 0 (x tgt t , t, y tgt )), f txt (p tgt )) -Sim(f img (x 0 (x tgt t , t, y tgt )), f txt (p src )) -β p ) + λ 2 max(0, β f ∥M (x tgt t , t, y tgt ) -M (x src t , t, y src )∥ F -∥M (x tgt t , t, y tgt ) -M (x tgt t+1 , t + 1, y tgt )∥ F ),<label>(6)</label></formula><p>where λ 1 , λ 2 , and β p are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare the proposed algorithm, referred to as OIG, with existing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. Additionally, we present an ablation study to analyze the effects of the proposed components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>We implement the proposed method using PyTorch, based on the publicly available code of Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref>. To speed up the translation of given source images, we reduce the number of denoising timesteps to 50 for all compared algorithms including the proposed method. Additionally, we replace the original captions with those generated by Bootstrapping Language-Image Pre-training (BLIP) <ref type="bibr" target="#b18">[19]</ref>, as the original captions often include languages other than English. The target prompts are constructed from the source prompts to reflect the tasks at hand. For instance, in the cat → dog task, we replace the token most closely related to "cat" in the source prompt with the "dog" token based on the CLIP text encoder. Note that we obtained results from the official codes of DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref>, Nulltext inversion <ref type="bibr" target="#b19">[20]</ref> and MasaCtrl <ref type="bibr" target="#b4">[5]</ref> using the same source and target prompts with the classifier-free guidance <ref type="bibr" target="#b10">[11]</ref> and the pretrained Stable Diffusion v1-4 checkpoint for fair comparisons. For the InstructPix2pix <ref type="bibr" target="#b2">[3]</ref> experiments, we use the official implementation and employ text instructions corresponding to the image-to-image translation tasks, as specified in the InstructPix2pix protocol, instead of using the source and target prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental settings</head><p>Dataset and tasks We select about 250 images for each task from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref>, based on the highest CLIP similarity with the text description corresponding to the source task <ref type="bibr" target="#b1">[2]</ref> to compare our method with stateof-the-art algorithms. For evaluation, we follow the standard experimental protocols of existing methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> for image-to-image translation tasks. Specifically, we focus on   <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> on the data sampled from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. Note that we strictly keep the original aspect ratio of each image during all experiments, and we change the aspect ratio only for visualization. object-centric tasks such as cat → dog, dog → cat, horse → zebra, and zebra → horse. Additionally, we include the dog → crochet dog task, where the goal is to transform the dog into one that resembles a yarn figure. We also test the proposed approach by transforming hand-drawn sketches into oil paintings in the drawing → oil painting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>We evaluate 1) how well the synthesized target image aligns with the target prompt, and 2) how effectively the structure of the source image is preserved after translation. First, we measure the similarity between the target prompt and the generated target image using CLIP <ref type="bibr" target="#b21">[22]</ref>, which we call CLIP Similarity (CS). To assess the overall structural difference between the source and target images, we use the self-similarity map of ViT <ref type="bibr" target="#b30">[31]</ref> extracted from both images. We then compute the squared Euclidean distance between their feature maps, which we call Structure Distance (SD). Additionally, to evaluate how well each algorithm preserves the background, we identify the background components of both the source and target images by removing the object parts using the pretrained segmentation model, Detic <ref type="bibr" target="#b33">[34]</ref>. We measure the squared Euclidean distance between the background regions of the two images, which we denote as Background Distance (BD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative results</head><p>We present quantitative results in Table <ref type="table">1</ref> to compare the proposed method with state-of-the-art training-free apporaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> on various tasks using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and 250 real images sampled from the LAION-5B dataset for each task. As shown in the table, our method outperforms the compared algorithms on most metrics, demonstrating its effectiveness for text-driven image-to-image translation tasks. Additionally, we include the results of InstructPix2pix <ref type="bibr" target="#b2">[3]</ref>, which requires additional fine-tuning of GPT-3 <ref type="bibr" target="#b3">[4]</ref> and Stable Diffusion, unlike the proposed training-free method. However, since the primary Table <ref type="table">2</ref>. Ablation study results to analyze the effect of the proposed component using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref> for various tasks. DDIM employs the reverse process defined in Eq. 2 while Naïve Distance replaces the triplet-based distance objective of the representation guidance in Eq. 6 with the naive distance term in Eq. 4 and Distance utilizes the representation guidance based on the triplet loss. objective of InstructPix2pix-mapping text instructions to image-to-image translations-differs from our goal, its results are reported only for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative results</head><p>We visualize translated results in Figure <ref type="figure" target="#fig_2">2</ref> given by the state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> and the proposed approach, all using the pretrained Stable Diffusion and images sampled from the LAION-5B dataset. As presented in the figure, our method demonstrates superior text-driven image editing performance compared to other methods. Unlike our approach, competing algorithms often struggle to maintain the structural integrity of the source image in the translated outputs.</p><p>To demonstrate the superior performance of the proposed method, we qualitatively compare it with Pix2Pix-Zero using synthesized images generated by Stable Diffusion. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, our method significantly outperforms prior approaches, while Pix2Pix-Zero often fails to preserve structure and introduces noticeable artifacts. We also show additional editing examples of the proposed method on the synthesized images given by the pretrained Stable Diffusion in Figure <ref type="figure" target="#fig_5">4</ref>, which verifies the effectiveness of our method. Notably, our method demonstrates outstanding performance even on multi-subject editing tasks. Additional qualitative comparisons and examples are provided in Appendix E. Furthermore, we introduce coherence guidance, which is an additional component designed to improve the fine details of the target image. Motivated by CycleGAN <ref type="bibr" target="#b34">[35]</ref>, we leverage the concept of cycle-consistency between the source and target domains using a pretrained diffusion model. Empirical results demonstrate that the proposed approach effectively refines minor details in the target image generated through representation guidance, improving overall quality. Comprehensive explanations and qualitative results of the coherence guidance are provided in Appendix B. Note that the results shown in the main paper were obtained without incorporating the coherence guidance.  . Editing examples of the proposed method on synthetic images given by the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. Given the source and target prompts, our method generates a target image while successfully preserving the overall structure of the source image and maintaining the background excluding the parts to be manipulated. . Qualitative reseults of the sensitivity analysis about the hyperparameter λ2 using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation study</head><p>To analyze the impact of each proposed component, we conduct an ablation study on various tasks using synthetic images generated by the pretrained Stable Diffusion and real images sampled from the LAION-5B dataset. As demonstrated in Table <ref type="table">2</ref>, Figure <ref type="figure" target="#fig_7">6</ref>, and Appendix C, the tripletbased distance term in Eq. ( <ref type="formula" target="#formula_5">6</ref>) produces target images with higher fidelity compared to the naïve distance objective described in Eq. ( <ref type="formula">4</ref>). Furthermore, we qualitatively analyze the impact of the hyperparameter λ 2 on the results of the proposed method. As illustrated in Figure <ref type="figure" target="#fig_6">5</ref>, our approach exhibits robustness across a range of λ 2 values from 0.5 to 1.5. Empirical ob-servations reveal that smaller λ 2 values enhance alignment between the target image and the target prompt but may compromise the structural consistency with the source image. In contrast, larger λ 2 values better preserve the structure of the source image in the generated results, albeit at the expense of slightly reduced fidelity to the target prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a simple but effective method for text-driven image-to-image translation tasks based on pretrained text-toimage diffusion models. To enhance the fidelity of the target images to the given prompt, we introduced representation guidance based on a triplet-based distance objective for the reverse process. For the derivation of the distance objec- tive, we encourage the target latents to align closely with the target prompts compared with the source prompts for fidelity while the target latents become closer to the source latents rather than the previous target latents for preserving the structure or background of the source image. To demonstrate the effectiveness of our approach, we conducted extensive experiments on various image manipulation tasks and compared the results with state-of-the-art image-to-image translation methods. Experimental results show that our framework achieves outstanding qualitative and quantitative performance in a variety of scenarios when combined with the pretrained Stable Diffusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis on computational complexity</head><p>In Table <ref type="table">3</ref>, we report the runtime and computational complexity of the proposed method and state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> analyzed on a single NVIDIA A100 GPU. As shown in the table, our method shows comparable computational cost to prior works. Since the proposed method shows superior performance compared to prior works, this demonstrates that our method is a simple but effective approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion on coherence guidance B.1. Revised target generation</head><p>Different from previous frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref> that revise the backward process only, the revised method alternates the estimation of the source and target latents in the order of {x src T -t , xtgt t } t=T -1:0 , where xsrc T -t and xtgt t are source and target latents obtained from our modified forward and backward processes as shown in Figure <ref type="figure">7</ref>. Note that, in the case of t = T , xsrc 0 is equal to x src 0 , which is the source image, while xtgt T is set to x src T , which is given by recursively performing the deterministic DDIM inversion from the source image. The two modified processes are denoted by forward with guidance and backward with guidance. We refer our revised method to Optimized Inference with Guidance + (OIG + ). Algorithm 2 summarizes the detailed procedures of the proposed guidances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Forward with guidance</head><p>We revise the forward process in Eq. ( <ref type="formula" target="#formula_0">1</ref>) by additionally optimizing the proposed efficient version of the cycleconsistency objective L cycle, eff as described in Section B.3 with respect to the source latent xsrc T -t as follows:</p><formula xml:id="formula_6">xsrc T -t+1 = f fwd T -t (x src T -t ) = α T -t+1 α T -t xsrc T -t -1 -α T -t γ ′ T -t ϵ θ (x src T -t , T -t, y src ) -∇ xsrc T -t λ 3 L cycle, eff ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">f fwd T -t (•) denotes the modified forward process at time step T -t, γ ′ T -t is equal to α T -t+1 α T -t -1-α T -t+1 1-α T -t</formula><p>, and λ 3 is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Backward with guidance</head><p>In the backward with guidance process, we improve the backward process in Eq. ( <ref type="formula" target="#formula_1">2</ref>) by optimizing both the distance term L dist t and the cycle-consistency objective L cycle , where the modified backward process is given by</p><formula xml:id="formula_8">xtgt t-1 = f bwd t (x tgt t ) = α t-1 α t xtgt t - √ 1 -α t γ t ϵ θ (x tgt t , t, y tgt ) -∇ xtgt t (L dist t + λ 4 L cycle ),<label>(8)</label></formula><p>where f bwd t (•) is defined by our modified backward process at time step t, γ t is equal to αt-1 αt -1-αt-1 1-αt , and λ 4 is a hyperparameter. L dist t is the distance objective defined in Eq. ( <ref type="formula" target="#formula_5">6</ref>). We will discuss L cycle in Section B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Coherence guidance via cycle-consistency</head><p>The simple DDIM translation, recursively using the backward process defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>) from the final target latent x tgt T , guarantees the cycle-consistency property as verified by <ref type="bibr" target="#b29">[30]</ref>. In other words, after converting the source domain image x src 0 into x tgt 0 in the target domain and then transforming x tgt 0 back to the source domain image denoted by xsrc 0 , the equality x src 0 = xsrc 0 holds. Although the simple DDIM translation guarantees the cycle-consistency, the property fails to hold in OIG because the generation process is modified by incorporating the representation guidance. Hence, we add an objective to enforce the cycle-consistency to further enhance translation results. As described in CycleGAN <ref type="bibr" target="#b34">[35]</ref>, the cycle-consistency term is defined as ∥x src 0 -h(g(x src 0 ))∥ 2,2 in principle, where g(•) is the image-to-image translation operation from the source domain to the target domain, and vise versa for h(•). However, with the assumption that g(•) and h(•) are invertible, we alternatively optimize the following cycle-consistency objective, which is given by</p><formula xml:id="formula_9">L cycle := ∥x tgt 0,f -xtgt 0,b ∥ 2,2 ,<label>(9)</label></formula><p>Table <ref type="table">3</ref>. Runtime and computational complexity analysis of DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref> Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref>, Null-text inversion <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref> and the proposed method. Each algorithm is tested on a single NVIDIA A100 GPU. The proposed method achieves comparable runtime and memory consumption compared to prior works, while outperforming prior works.  Calculate L cycle, eff using Eq. ( <ref type="formula" target="#formula_19">13</ref>)</p><formula xml:id="formula_10">Compute γ ′ T -t ← α T -t+1 α T -t -1-α T -t+1 1-α T -t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculate xsrc</head><p>T -t+1 using Eq. ( <ref type="formula" target="#formula_6">7</ref>) ▷ Forward with guidance Calculate x0 (x tgt t , t, y tgt ) using Eq. ( <ref type="formula" target="#formula_4">5</ref>) Calculate L dist t using Eq. ( <ref type="formula" target="#formula_5">6</ref>) Compute L cycle using Eq. ( <ref type="formula" target="#formula_9">9</ref>)</p><formula xml:id="formula_11">Compute γ t ← αt-1 αt -1-αt-1 1-αt</formula><p>Compute xtgt t-1 using Eq. ( <ref type="formula" target="#formula_8">8</ref>) ▷ Backward with guidance end for</p><formula xml:id="formula_12">x tgt 0 ← xtgt 0 Output: A target image x tgt 0</formula><p>where we denote xtgt 0,f by h -1 (x src 0 ) and xtgt 0,b by g(x src 0 ). The definition of h -1 (•) and g(•) are given by</p><formula xml:id="formula_13">xtgt 0,f = h -1 (x src 0 ) = F bwd ( F fwd (x src 0 )) xtgt 0,b = g(x src 0 ) = F bwd (F fwd (x src 0 )), (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where the auxiliary functions are defined as</p><formula xml:id="formula_15">F fwd (•) = f fwd T -1 • f fwd T -2 • • • • f fwd 0 (•), F bwd (•) = f bwd 1 • f bwd 2 • • • • f bwd T (•) F fwd (•) = f fwd T -1 • f fwd T -2 • • • • f fwd 0 (•) F bwd (•) = f bwd 1 • f bwd 2 • • • • f bwd T (•).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of xtgt</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0,f</head><p>Using the equivalent ordinary differential equation of the simple DDIM forward process in Eq (1), we first approximate xtgt T,f which is equal to xsrc T as</p><formula xml:id="formula_16">xtgt T,f = xsrc T ≈ α T α t0 xsrc t0 + √ 1 -α T - α T (1 -α t0 ) α t0 ϵ θ (x src t0 , t 0 , y src ), (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>where t 0 is an intermediate time step. Although the approximation incurs discretization errors due to the one-step estimation of xsrc T from xsrc t0 , we empirically observe that the proposed method achieves remarkable performance as demonstrated in Section B.4. When we estimate xtgt t-1 from xtgt t during the backward with guidance, xtgt T,f is derived from Eq. ( <ref type="formula" target="#formula_16">11</ref>) by plugging T -t + 1 into t 0 . Finally, xtgt 0,f is obtained by F bwd (x tgt T,f ), where F bwd (•) performs T steps of the recursive backward process in Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of xtgt</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0,b</head><p>To reduce the computational costs for the estimation of xtgt 0,b from xtgt t , we approximate xtgt 0,b using the Tweedie's formula <ref type="bibr" target="#b28">[29]</ref> in Eq. ( <ref type="formula" target="#formula_4">5</ref>) as . Qualitative results of coherence guidance on the data from LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref> and synthetic images using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. Coherence guidance effectively modifies the details of the target image when combined with OIG.</p><formula xml:id="formula_18">xtgt 0,b ≈ x0 (x tgt t , t, y tgt ).<label>(12)</label></formula><p>We eventually calculate L cycle by plugging xtgt 0,f and xtgt 0,b into Eq. ( <ref type="formula" target="#formula_9">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Efficient coherence guidance</head><p>In the case of the forward with guidance, computing the gradient of L cycle in Eq. ( <ref type="formula" target="#formula_9">9</ref>) with respect to the source latent xsrc T -t , denoted as ∇ xsrc T -t L cycle , is memory-intensive and time-consuming since it involves multiple times of backpropagation through the noise prediction network. To tackle this issue, we alternatively derive the following efficient version of the cycle-consistency objective that matches the final target latents instead of the target images as</p><formula xml:id="formula_19">L cycle, eff := ∥x tgt T,f -xtgt T,b ∥ 2,2 .<label>(13)</label></formula><p>In the above equation, xtgt T,f is obtained based on a single forward propagation of the noise prediction network from Eq. ( <ref type="formula" target="#formula_16">11</ref>) by setting t 0 = T -t. We obtain xtgt T,b from F fwd (x tgt 0,b ), where xtgt 0,b is estimated using Eq. ( <ref type="formula" target="#formula_18">12</ref>) and F fwd (•) recursively applies the DDIM inversion process in Eq. ( <ref type="formula" target="#formula_0">1</ref>) for T times.</p><p>Therefore, we can compute the gradient of L cycle, eff with respect to xsrc T -t just by performing a single backpropagation through the noise prediction network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional ablation study</head><p>We report additional ablation study results of the proposed method in Figure <ref type="figure" target="#fig_11">9</ref>. We emphasize that the triplet-based distance term in Eq. ( <ref type="formula" target="#formula_5">6</ref>) enhances the fidelity of the target image and preserves the overall structure well compared to the naïve distance objective in Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results using other pretrained diffusion models</head><p>To demonstrate that the proposed method generalizes well to other pretrained models, we generated target images using our method with pretrained Distilled Stable Diffusion<ref type="foot" target="#foot_0">foot_0</ref> and Latent Diffusion Model (LDM) <ref type="bibr" target="#b23">[24]</ref>. Note that Distilled Stable Diffusion is a lightweight model that has been trained by reducing the parameters of the denoising U-Net. Also, the pipeline of LDM is similar to Stable Diffusion, however the resolution of training data for LDM differ from those of Stable Diffusion.</p><p>As visualized in Figure <ref type="figure" target="#fig_0">10</ref> and 11, the proposed method shows superior performance when combined with Distilled Stable Diffusion and LDM, which demonstrates that our method can generalize well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional qualitative results</head><p>We present additional qualitative results of OIG in Figure <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23</ref>, and 24 to compare with state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> on real images sampled from the LAION-5B dataset <ref type="bibr" target="#b25">[26]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. As visualized in the figures, OIG achieves outstanding results while the previous methods often fail to preserve the structure or background of the source images.</p><p>In order to demonstrate the generalizability of the proposed method, we perform additional experiments using LAION-5B, CelebA-HQ <ref type="bibr" target="#b12">[13]</ref>, and Seasons <ref type="bibr" target="#b0">[1]</ref> datasets. We emphasize that we focus on evaluating image-to-image translation tasks both on object-centric tasks such as cat → cat wearing a scarf task and style transfer tasks like summer → winter task. As visualized in Figure <ref type="figure" target="#fig_8">25</ref>, 26, 27, 28, 29, and 30, the proposed method demonstrates superior textdriven image manipulation performance on real images for various tasks. Figure <ref type="figure" target="#fig_20">31</ref>, 32 and 33 also verify the effectiveness of OIG on the synthesized images given by the pretrained Stable Diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Limitations</head><p>We visualize the failure cases of our method in Figure <ref type="figure" target="#fig_8">12</ref>. These failure cases can be addressed by using OIG + , which combines representation guidance and coherence guidance. OIG + effectively removes the artifacts and resolves inconsistencies in the target image, thereby improving the editing performance of the proposed method.</p><p>In addition, since the DDIM inference process sometimes does not completely reconstruct the original image, our method can struggle to preserve the information about the source image and result in suboptimal image-to-image translation results. Furthermore, the performance of the proposed method is reliant on pretrained text-to-image diffusion models, which may limit its ability to generate target images for complex tasks effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Social impacts</head><p>The proposed method may synthesize undesirable or inappropriate images depending on the pretrained text-to-image generation model <ref type="bibr" target="#b23">[24]</ref>. For example, the incompleteness of the pretrained diffusion model can lead to the generation of images that violate ethical regulations.  <ref type="bibr" target="#b25">[26]</ref> and synthetic images using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref>. Representation guidance significantly improves the details of the target image, such as correcting the structural inconsistencies between source and target images, preserving the structure of foreground region, and enhancing the fidelity. Figure <ref type="figure" target="#fig_3">13</ref>. Additional qualitative results of the proposed method, DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the cat → dog task. Figure <ref type="figure" target="#fig_10">14</ref>. Additional qualitative results of the proposed method, Null-text inversion <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix [3] using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the cat → dog task. </p><note type="other">Figure 15</note><p>. Additional qualitative results of the proposed method, DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the dog → cat task. </p><note type="other">Figure 16</note><p>. Additional qualitative results of the proposed method, Null-text inversion <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix [3] using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the dog → cat task. </p><note type="other">Figure 17</note><p>. Additional qualitative results of the proposed method, DiffEdit <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the dog → crochet dog task. </p><note type="other">Figure 18</note><p>. Additional qualitative results of the proposed method, Null-text inversion <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix [3] using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the dog → crochet dog task.  <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the horse → zebra task.  <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix <ref type="bibr" target="#b2">[3]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the horse → zebra task.  <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the zebra → horse task.  <ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play <ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero <ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the drawing → oil painting task.  <ref type="bibr" target="#b19">[20]</ref>, MasaCtrl <ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix <ref type="bibr" target="#b2">[3]</ref> using the pretrained Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset <ref type="bibr" target="#b25">[26]</ref> on the drawing → oil painting task.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of the proposed method about utilizing the representation guidance.</figDesc><graphic coords="3,257.22,297.45,54.50,54.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.  Qualitative comparisons between the proposed algorithm and state-of-the-art methods<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> on the data sampled from the LAION-5B dataset<ref type="bibr" target="#b25">[26]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref>. Note that we strictly keep the original aspect ratio of each image during all experiments, and we change the aspect ratio only for visualization.</figDesc><graphic coords="5,53.84,373.11,54.96,54.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.  Qualitative comparisons between the proposed algorithm and Pix2Pix-Zero<ref type="bibr" target="#b20">[21]</ref> on synthetic images given by Stable Diffusion<ref type="bibr" target="#b23">[24]</ref>.</figDesc><graphic coords="6,448.10,364.82,71.22,71.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Photo → Van Gogh painting House → Eiffel Tower</figDesc><graphic coords="7,361.31,81.70,58.80,58.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Editing examples of the proposed method on synthetic images given by the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref>. Given the source and target prompts, our method generates a target image while successfully preserving the overall structure of the source image and maintaining the background excluding the parts to be manipulated.</figDesc><graphic coords="7,312.77,328.01,67.39,108.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>Figure 5. Qualitative reseults of the sensitivity analysis about the hyperparameter λ2 using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION-5B dataset<ref type="bibr" target="#b25">[26]</ref>.</figDesc><graphic coords="7,312.61,401.55,67.28,100.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Photo → Watercolor Painting</figDesc><graphic coords="8,376.48,168.89,73.42,73.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2</head><label>2</label><figDesc>Text-Driven Image Editing based on Forward and Backward Guidances Inputs: A source image x src 0 , a source prompt y src , a target prompt y tgt for t ← 0, • • • , T -1 do Compute x src t+1 using Eq. (1) end for xtgt T ← x src T and xsrc 0 ← x src 0 for t ← T, • • • , 1 do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8</head><label>8</label><figDesc>Photo → Watercolor Painting</figDesc><graphic coords="13,32.16,269.21,119.79,79.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>B. 4 .</head><label>4</label><figDesc>Qualitative results of OIG + We visualize the effect of coherence guidance in Figure 8. As shown in the Figure, OIG + enhances the fine details of the target image generated by OIG, such as reducing high-frequency noise or facilitating the alignment of small structural elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Photo → Monet</figDesc><graphic coords="15,382.91,269.22,79.08,79.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .Figure 12 .</head><label>1012</label><figDesc>Figure 10. Qualitative results of the proposed method combined with Distilled Stable Diffusion on real images (1st -4th row) sampled from the LAION-5B dataset<ref type="bibr" target="#b25">[26]</ref> and synthetic images (5th row) given by the pretrained Distilled Stable Diffusion.</figDesc><graphic coords="16,299.39,524.62,76.72,76.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Additional qualitative results of the proposed method, DiffEdit<ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play<ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero<ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the horse → zebra task.</figDesc><graphic coords="24,188.88,571.07,78.23,115.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Additional qualitative results of the proposed method, Null-text inversion<ref type="bibr" target="#b19">[20]</ref>, MasaCtrl<ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix<ref type="bibr" target="#b2">[3]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the horse → zebra task.</figDesc><graphic coords="25,82.47,572.01,78.23,115.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. Additional qualitative results of the proposed method, DiffEdit<ref type="bibr" target="#b5">[6]</ref>, Plug-and-Play<ref type="bibr" target="#b31">[32]</ref>, and Pix2Pix-Zero<ref type="bibr" target="#b20">[21]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the zebra → horse task.</figDesc><graphic coords="26,188.47,569.81,77.82,116.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 22 .Figure 23 .</head><label>2223</label><figDesc>Figure 22. Additional qualitative results of the proposed method, Null-text inversion<ref type="bibr" target="#b19">[20]</ref>, MasaCtrl<ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix<ref type="bibr" target="#b2">[3]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the zebra → horse task.</figDesc><graphic coords="27,188.68,570.58,77.96,116.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Additional qualitative results of the proposed method, Null-text inversion<ref type="bibr" target="#b19">[20]</ref>, MasaCtrl<ref type="bibr" target="#b4">[5]</ref>, and InstructPix2Pix<ref type="bibr" target="#b2">[3]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> and real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the drawing → oil painting task.</figDesc><graphic coords="29,82.90,589.74,78.09,78.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 25 .Figure 26 .Figure 27 .</head><label>252627</label><figDesc>Figure 25. Qualitative results of the proposed method on real images sampled from the LAION 5B dataset<ref type="bibr" target="#b25">[26]</ref> on the cat → cat wearing a scarf and cat → low poly cat task.</figDesc><graphic coords="30,114.00,539.35,87.95,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 28 .Figure 29 .Figure 30 .</head><label>282930</label><figDesc>Figure 28. Qualitative results of the proposed method on real images sampled from the CelebA-HQ dataset<ref type="bibr" target="#b12">[13]</ref> using the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref>.</figDesc><graphic coords="33,89.58,492.23,99.71,99.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 31 .</head><label>31</label><figDesc>Figure 31. Qualitative results of the proposed method with synthetic images given by the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> on style transfer tasks.</figDesc><graphic coords="36,104.59,570.26,90.71,90.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 32 .Figure 33 .</head><label>3233</label><figDesc>Figure 32. Qualitative results of the proposed method with synthetic images given by the pretrained Stable Diffusion<ref type="bibr" target="#b23">[24]</ref> on style transfer tasks.</figDesc><graphic coords="37,79.61,557.01,82.57,82.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>t, y tgt )), f txt (p tgt ) +β f ∥M (x tgt t , t, y tgt ) -M (x src t , t, y src )∥ F ,(4) where f img (•) and f txt (•) are CLIP image and text encoders, respectively, Sim(•, •) represents the cosine similarity between two vectors, ∥ • ∥ F denotes the Frobenius norm,</figDesc><table><row><cell>Algorithm 1 Optimized Inference with Guidance (OIG)</cell></row><row><cell>Inputs: A source image x src 0 , a source prompt y src , a target prompt y tgt</cell></row><row><cell>for t ← 0, • • • , T -1 do</cell></row><row><cell>Compute x src t+1 using Eq. (1)</cell></row><row><cell>end for</cell></row><row><cell>xtgt T ← x src T</cell></row><row><cell>for t ← T, • • • , 1 do</cell></row><row><cell>Compute x0 (x tgt t , t, y tgt ) using Eq. (5). Compute L dist t using Eq. (6).</cell></row><row><cell>Compute γ t ← αt-1 αt -1-αt-1 1-αt .</cell></row><row><cell>Compute xtgt t-1 using Eq. (3).</cell></row><row><cell>end for</cell></row><row><cell>x tgt 0 ← xtgt 0 Output: A target image x tgt 0</cell></row><row><cell>M (•, •, •) extracts an intermediate feature map from the noise</cell></row><row><cell>prediction network ϵ θ (•, •, •), and β f is a hyperparameter.</cell></row><row><cell>Also, x0 (•, •, •) is the estimated image sample based on the</cell></row><row><cell>following Tweedie's formula [29]:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https : / / huggingface . co / docs / diffusers / en / using-diffusers/distilled_sd</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was partly supported by the <rs type="funder">Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP)</rs> [No.<rs type="grantNumber">RS2022-II220959</rs> (No.<rs type="grantNumber">2022-0-00959</rs>), No.<rs type="grantNumber">RS-2021-II211343</rs>, No.<rs type="grantNumber">RS-2021-II212068</rs>] and the <rs type="funder">National Research Foundation (NRF)</rs> [No.<rs type="grantNumber">RS-2021-NR056445</rs> (No.<rs type="grantNumber">2021M3A9E408078222</rs>)] funded by the <rs type="funder">Korea government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZP4AH2R">
					<idno type="grant-number">RS2022-II220959</idno>
				</org>
				<org type="funding" xml:id="_H3hUfts">
					<idno type="grant-number">2022-0-00959</idno>
				</org>
				<org type="funding" xml:id="_cw4gaJu">
					<idno type="grant-number">RS-2021-II211343</idno>
				</org>
				<org type="funding" xml:id="_McKtKwv">
					<idno type="grant-number">RS-2021-II212068</idno>
				</org>
				<org type="funding" xml:id="_FbE6Qxx">
					<idno type="grant-number">RS-2021-NR056445</idno>
				</org>
				<org type="funding" xml:id="_kV2ugHH">
					<idno type="grant-number">2021M3A9E408078222</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the Appendix, we analyze the runtime and computational complexity of our method and compare it with stateof-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> in Section A. Section B introduces additional component, referred to as coherence guidance, which can be combined with the proposed method to enhance the quality of the target image. In section C, we visualize additional ablation study results. Section D demonstrates the qualitative results of our method combined with other pretrained diffusion models other than Stable Diffusion <ref type="bibr" target="#b23">[24]</ref> to highlight the generalizability of the proposed method. Additional qualitative results are provided in Section E. Finally, we discuss the limitations and potential social impacts of the proposed method in Section F and G, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ComboGAN: Unrestrained Scalability for Image Domain Translation</title>
		<author>
			<persName><forename type="first">Asha</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<ptr target="https://github.com/rom1504/clip-retrieval" />
		<title level="m">CLIP Retrieval: Easily Compute CLIP Embeddings and Build a CLIP Retrieval System with Them</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instructpix2pix: Learning to Follow Image Editing Instructions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 4, 5, 6, 14, 19, 21, 23, 25</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing</title>
		<author>
			<persName><forename type="first">Mingdeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">22560-22570, 2023. 2, 4, 5, 6, 11, 12, 14, 19, 21, 23, 25</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 4, 5, 6, 11, 12, 14, 18, 20, 22, 24</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models</title>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyue</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffusion self-guidance for controllable image generation</title>
		<author>
			<persName><forename type="first">Dave</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Holynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="16222" to="16239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prompt-to-Prompt Image Editing with Cross Attention Control</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classifier-Free Diffusion Guidance</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Metric Learning using Triplet Network</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIMBAD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagic: Text-based Real Image Editing with Diffusion Models</title>
		<author>
			<persName><forename type="first">Bahjat</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiran</forename><surname>Zada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffu-sionCLIP: Text-guided Diffusion Models for Robust Image Manipulation</title>
		<author>
			<persName><forename type="first">Gwanghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional Score Guidance for Text-Driven Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusionbased image-to-image translation by noise correction via prompt interpolation</title>
		<author>
			<persName><forename type="first">Junsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Null-text Inversion for Editing Real Images Using Guided Diffusion Models</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 4, 5, 6, 11, 12, 14, 19, 21, 23, 25</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-Shot Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 4, 5, 6, 11, 12, 14, 18, 20, 22, 24</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models from Natural Language Supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2021. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022. 1, 2, 4, 5, 6, 7, 8, 11, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022. 4, 5, 6, 7, 8, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Implicit Models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimation of the Mean of a Multivariate Normal Distribution</title>
		<author>
			<persName><surname>Charles M Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual Diffusion Implicit Bridges for Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Splicing ViT Features for Semantic Appearance Transfer</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Plug-and-play Diffusion Features for Text-Driven Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 4, 5, 6, 11, 12, 14, 18, 20, 22, 24</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unitune: Text-Driven Image Editing by Finetuning an Image Generation Model on a Single Image</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Valevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Leviathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting Twenty-thousand Classes using Image-level Supervision</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Philipp Krähenbühl, and Ishan Misra</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
