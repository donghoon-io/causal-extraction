<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment</title>
				<funder>
					<orgName type="full">Research Grant S of the Tateishi Science and Technology Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">LY Corporation and Saruwatari-Takamichi Laboratory of The University of Tokyo, Japan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-11">11 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Takuto</forename><surname>Igarashi</surname></persName>
							<email>takuto0228@g.ecc.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuki</forename><surname>Saito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kentaro</forename><surname>Seki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinnosuke</forename><surname>Takamichi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keio University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">LY Corp</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kentaro</forename><surname>Tachibana</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">LY Corp</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroshi</forename><surname>Saruwatari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-11">11 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.07280v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>voice conversion</term>
					<term>noise-robust voice conversion</term>
					<term>denoising training</term>
					<term>any-to-any voice conversion</term>
					<term>latent variables</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose noise-robust voice conversion (VC) which takes into account the recording quality and environment of noisy source speech. Conventional denoising training improves the noise robustness of a VC model by learning noisy-to-clean VC process. However, the naturalness of the converted speech is limited when the noise of the source speech is unseen during the training. To this end, our proposed training conditions a VC model on two latent variables representing the recording quality and environment of the source speech. These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner. As a result, the trained VC model can explicitly learn information about speech degradation during the training. Objective and subjective evaluations show that our training improves the quality of the converted speech compared to the conventional training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Voice conversion (VC) is a technology that converts a source speaker's timbre to that of a target speaker while preserving the linguistic content. VC can be widely applied in the real world, such as in movie dubbing <ref type="bibr" target="#b1">[1]</ref>, personalized text-to-speech <ref type="bibr" target="#b2">[2]</ref>, and speaking assistance <ref type="bibr" target="#b3">[3]</ref>. As a result of advancements in deep learning, deep neural network (DNN)-based VC methods have significantly improved the quality of converted speech <ref type="bibr" target="#b4">[4]</ref>.</p><p>Typical DNN-based VC methods train a VC model with a large multi-speaker corpus containing high-quality speech from a variety of speakers. However, actual speech samples recorded in the real world are often degraded by various factors, such as background noise and recording channels. Huang et al. <ref type="bibr" target="#b5">[5]</ref> empirically demonstrated that the recording-quality mismatch of input speech between the training and inference (i.e., clean and noisy) significantly deteriorates VC performance.</p><p>Denoising training (DT) <ref type="bibr" target="#b6">[6]</ref> is one promising approach to achieving noise-robust DNN-based VC. In DT, a VC model is trained using pseudo-noisy (i.e., artificially degraded) speech, with the aim of implicitly denoising input speech during the VC process. Although this training mitigates the distortion of converted speech caused by noisy input speech in inference, the naturalness of the converted speech is still limited when the degradation factor of input speech is unseen during the training. One primary reason is that the trained VC model does not explicitly learn information about speech degradation, such as noise characteristics (e.g., stationary or non-stationary) and noise levels, and does not guarantee generalization performance to various degradation factors.</p><p>In this study, we propose conditional DT (CDT), an improved version of conventional (i.e., unconditional) DT, to improve the noise robustness of VC towards unseen degradation. CDT conditions a DNN-based VC model on two latent variables regarding the degradation of input speech: recording quality and environment. These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner. As a result, the trained VC model can explicitly learn information about speech degradation during the training. We present the CDT framework using NISQA <ref type="bibr">[7]</ref> and PaSST <ref type="bibr" target="#b8">[8]</ref> as representative models to extract latent variables of the recording quality and environment, respectively. In the experimental evaluation, we validate the effectiveness of CDT using S2VC <ref type="bibr" target="#b9">[9]</ref> as the baseline VC model following the conventional DT framework <ref type="bibr" target="#b6">[6]</ref>. Our contributions are summarized as follows.</p><p>• We propose CDT that can improve the noise-robustness of DNN-based VC models by explicitly conditioning the model on speech degradation information. • We present two conditioning strategies with an utterancewise and frame-wise manner so that the trained VC model can take into account not only global but also time-variant characteristics of degradation in input speech. • From the evaluation results, we show that conditioning the VC model on frame-wise latent variables of recording environment is essential for improving the naturalness of the converted speech in the noisy-to-clean VC scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Conventional DT-based noise-robust VC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DT algorithm</head><p>DT is an end-to-end learning method for noisy-to-clean VC, in which data augmentations are utilized to train a noise-robust VC model. In the learning process, pseudo-noisy speech is artificially generated from clean speech by mixing various environmental noises with random signal-to-noise ratios (SNRs) and fed into the VC model as input. The training objective function is computed by comparing the ground-truth clean speech and converted speech, i.e., the VC model's output. Typically, the L1 or L2 loss between mel-spectrograms of ground-truth clean speech and converted speech is used as the objective function. Thus, DT can be interpreted as a learning method in which the VC model acts as a denoising autoencoder <ref type="bibr" target="#b6">[6]</ref>.</p><p>Huang et al. <ref type="bibr" target="#b6">[6]</ref> demonstrated that the noise robustness of VC models trained by DT improved when the VC models were based on an autoencoding process such as AdaIN-VC <ref type="bibr" target="#b10">[10]</ref> and S2VC <ref type="bibr" target="#b9">[9]</ref>, and S2VC with DT was the most effective for VC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Limitations</head><p>Another approach to achieving noise-robust VC without modifying the model structure can be the concatenation of a pretrained speech enhancement model with an existing any-to-any VC model. However, this approach tends to limit the VC performance compared to an end-to-end approach such as DT. This is because the artifacts caused by speech enhancement to suppress unseen noises negatively affects the downstream VC task <ref type="bibr" target="#b11">[11]</ref>.</p><p>Although DT is an end-to-end approach, the naturalness of the converted speech is still limited when the noise of the input speech is unseen during the training. One of the main reasons is that the trained VC model does not explicitly learn information about speech degradation, such as noise characteristics and noise levels, and does not guarantee generalization to various degradation factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed CDT-based noise-robust VC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>One possible solution to the problem presented in Section 2.2 is to enable the DNN-based VC model to explicitly learn information about speech degradation. To this end, we propose CDT, which conditions the model on two latent variables regarding the degradation of input speech: recording quality and environment. For practicality, our CDT algorithms assume that only the source speech samples are degraded by noise. This is because it is more challenging for VC users, who are not always experts in speech technology or high-quality speech recording, to record their own clean source speech than it is to collect target speakers' clean speech. Nevertheless, we believe that our algorithm can be extended to VC where both the source and target speech are degraded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CDT algorithm</head><p>Let x c be any clean speech in the training dataset. The loss function L of our CDT is defined as follows:</p><formula xml:id="formula_0">x s = x c + n,<label>(1)</label></formula><formula xml:id="formula_1">x t = x c ,<label>(2)</label></formula><formula xml:id="formula_2">z s SSL = fSSL (x s ) ,<label>(3)</label></formula><formula xml:id="formula_3">z s rqa = frqa (x s ) ,<label>(4)</label></formula><formula xml:id="formula_4">z s asc = fasc (x s ) ,<label>(5)</label></formula><formula xml:id="formula_5">z t SSL = fSSL x t ,<label>(6)</label></formula><formula xml:id="formula_6">L = f θ z s SSL , z s rqa , z s asc , z t SSL -g mel (x c ) ,<label>(7)</label></formula><p>where x s and x t are the source and target speech, respectively. The pseudo-noisy source speech  <ref type="formula" target="#formula_6">7</ref>).</p><p>We present the CDT framework using NISQA <ref type="bibr">[7]</ref> and PaSST <ref type="bibr" target="#b8">[8]</ref> as representative models to extract latent variables of the recording quality and environment: z s rqa and z s asc , respectively.</p><p>NISQA: NISQA is a method for automatically estimating recording quality scores without reference speech. The opensource model is trained on pseudo-noisy and real-noisy speech, allowing it to robustly predict recording quality values against a variety of noises.</p><p>PaSST: The model trained on AudioSet <ref type="bibr" target="#b12">[12]</ref>, which contains 527 types of tags, achieves significantly higher predictive performance compared to other models <ref type="bibr" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Frame-wise conditioning</head><p>NISQA and PaSST were originally designed to output an utterance-wise prediction: the mean opinion score (MOS) and audio tag, respectively. However, frame-wise features can be obtained as described in the next paragraph. The frame-wise features can be expected to represent the non-stationary characteristics of noise in the noisy source speech.</p><p>Let (zuwNISQA, zuwPaSST) and (z fwNISQA , z fwPaSST ) be latent variables extracted by (NISQA, PaSST) in an utterancewise and frame-wise manner, respectively. Figure <ref type="figure" target="#fig_1">1</ref> shows the feature extraction methods. As shown in Figure <ref type="figure" target="#fig_1">1</ref>(a), NISQA's model segments an input speech, estimates frame-wise features z fwNISQA , computes their weighted average zuwNISQA along with the frame axis, and outputs the final prediction (MOS of input speech). In contrast, PaSST's model outputs audio tags with no dimensions in the frame direction, as shown in the upper part of Figure <ref type="figure" target="#fig_1">1(b)</ref>. However, we can also use the model to extract frame-wise audio tags by segmenting the input speech in advance <ref type="bibr" target="#b13">[13]</ref>. The former extracts zuwPaSST and the latter extracts z fwPaSST .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation 4.1. Experimental conditions</head><p>We used the parallel100 subset from Japanese Versatile Speech (JVS) <ref type="bibr" target="#b14">[14]</ref> and downsampled all speech data to 16 kHz. The subset contains 22 hours of speech data for 100 Japanese speakers (100 utterances per speaker). The numbers of speakers included in the training, validation, and evaluation data were 90 ("jvs001" to "jvs086"), 4 ("jvs087" to "jvs090"), and 10 ("jvs091" to "jvs100"), respectively. The 10 test speakers excluded from the training data were used as unseen speakers for the any-to-any VC evaluation.</p><p>We created the pseudo-noisy speech dataset by adding Here, n was taken from the DEMAND <ref type="bibr" target="#b15">[15]</ref> noise and the WHAM!48kHz <ref type="bibr" target="#b16">[16]</ref> noise for the training and evaluation datasets, respectively. Thus, the noise of speech from the evaluation dataset was unseen in the training process.</p><p>We downsampled the WHAM!48kHz to 16 kHz. Both contained various types of background noises.</p><p>Referring to Huang et al.'s study <ref type="bibr" target="#b6">[6]</ref>, we adopted S2VC <ref type="bibr" target="#b9">[9]</ref> as a backbone VC model with the publicly available implementation on GitHub (robust-vc) 1 . In CDT, we conditioned the VC model on (zuwNISQA or z fwNISQA ) and (zuwPaSST or z fwPaSST ) as shown in Figure <ref type="figure" target="#fig_3">2</ref>. We unified the size of these latent variables to that of zSSL, i.e., 256 × frame size in this study. We concatenated the latent variables along with the feature axis and input them to the source encoder. First, to unify the frame dimension of (zuwNISQA, zuwPaSST) to that of zSSL, we replicated them by a factor of the frame size of zSSL along with the frame axis. Meanwhile, we upsampled (z fwNISQA , z fwPaSST ) to align the frame lengths. Second, to unify the feature dimensions of (zuwNISQA, z fwNISQA ) and (zuwPaSST, z fwPaSST ), we inserted the linear projection layers from 64 to 256 dimensions and from 768 to 256 dimensions, respectively.</p><p>In CDT, the S2VC model was trained based on the loss function shown in Eq. ( <ref type="formula" target="#formula_6">7</ref>) while the parameters of the pretrained models, fSSL(•), frqa(•), and fasc(•), were fixed. Specifically, fSSL(•) is a feature extractor using contrastive predictive coding <ref type="bibr" target="#b17">[17]</ref> as is the case of the study of Huang et al. <ref type="bibr" target="#b6">[6]</ref>. The latent variable extractors, frqa(•) and fasc(•), were based on the official implementations of NISQA 2 and PaSST 3 , respectively. In the implementation of NISQA, the frame length was 150 ms and the frame shift was 40 ms. In the implementation of PaSST, the former was 160 ms and the latter was 50 ms. To generate the converted speech waveform from the predicted log mel-spectrogram, we used the HiFi-GAN vocoder <ref type="bibr" target="#b18">[18]</ref> trained on the JVS training data with a batch size of 16, while all the VC models were trained with a batch size of 6. The optimizer was AdamW <ref type="bibr" target="#b19">[19]</ref> with a learning rate of 5 × 10 -<ref type="foot" target="#foot_1">foot_1</ref> , and β1 = 0.9, β2 = 0.999. Each of the VC models had approximately 33 million parameters which were randomly initialized.</p><p>The training was stopped when the validation loss converged completely and the training time was approximately 60 hours. We trained the VC model using both the conventional DT <ref type="bibr" target="#b6">[6]</ref> and our CDT frameworks and evaluated their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective evaluation</head><p>We randomly selected 250 test pairs of source and target speech samples with different speakers taken from the evaluation dataset (10 JVS speakers). Then, we performed VC on each pair with the VC models trained by the conventional DT 1 <ref type="url" target="https://github.com/cyhuang-tw/robust-vc">https://github.com/cyhuang-tw/robust-vc</ref> 2 <ref type="url" target="https://github.com/gabrielmittag/NISQA">https://github.com/gabrielmittag/NISQA</ref> 3 <ref type="url" target="https://github.com/kkoutini/passt_hear21">https://github.com/kkoutini/passt_hear21</ref>  and CDT, which took less than 80 ms. Our CDT methods are denoted as "uwNISQA-uwPaSST", "uwNISQA-fwPaSST", "fwNISQA-uwPaSST", and "fwNISQA-fwPaSST", depending on whether the model was conditioned on utterance-wise or frame-wise features.</p><p>To evaluate the intelligibility of the converted speech, we used the character error rate (CER) estimated by the automatic speech recognition (ASR) system. The ASR model for calculating CER is a pre-trained ReazonSpeech model available on HuggingFace <ref type="foot" target="#foot_0">4</ref> . The smaller the CER, the more the converted speech preserves the linguistic content of the source speech, and the larger the CER, the more severely distorted the converted speech. Thus, CER is regarded as a measure reflecting the intelligibility of the converted speech. In contrast, to measure the speaker similarity between the converted speech and the target speaker, we used the speaker embedding cosine similarity (SECS). To compute SECS, we generated two fixeddimensional embedding vectors representing the speaker identity of the converted and target speech and computed their cosine similarity. The higher the SECS, the more similar the converted and target speech are in terms of the speaker identity. We adopted x-vector <ref type="bibr" target="#b20">[20]</ref> extracted by using a pre-trained model of WavLM <ref type="bibr" target="#b21">[21]</ref> as the embedding vectors, which is available on HuggingFace 5 .</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the average CER and SECS of the converted speech samples corresponding to the 250 test pairs. As shown, SECS is almost constant across methods, which may be because the VC models trained by CDT received the same information on target speech as the conventional DT when only the source speech was noisy. In contrast, CER differed between the methods. Although NISQA and PaSST were originally designed to output utterance-wise prediction, we can see from Table <ref type="table" target="#tab_0">1</ref> that conditioning the VC model on the utterance-wise features is not very effective for improving CER. In particular, the CER of "uwNISQA-uwPaSST" is lower than that of the conventional DT. This may be because the utterance-wise features consist of the exact same matrices along the frame axis. Although they are quite large, they do not contain much useful information for VC. Thus, they can prevent the VC model from receiving information that is essential for VC. In contrast, conditioning the model on frame-wise features improved CER. This implies that the frame-wise features represent the non-stationary characteristics of noise in the noisy source speech and that the VC model leverages useful information from the conditional latent variables to improve VC performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Subjective evaluation</head><p>We conducted subjective evaluations using crowdsourcing on Lancers <ref type="foot" target="#foot_2">6</ref> regarding the naturalness and speaker similarity of converted speech. We combined every 250 converted speech samples generated in Section 4.2 into a single dataset and used it as the dataset for subjective evaluation. Thus, the evaluation dataset contained 250 × 5 = 1250 converted speech samples. When evaluating their naturalness, evaluators were given a converted utterance randomly sampled from the subjective evaluation dataset. Then, they rated the perceptual quality on a 5-point MOS scale from 1 (very bad) to 5 good). When evaluating speaker similarity, evaluators were given a ground-truth target utterance and the converted utterance sampled from the JVS corpus and the subjective evaluation dataset, respectively. Then, they answered how similar the speakers were who produced the two utterances on a MOS score ranging from 1 (completely different) to 5 (completely same). In both cases, the listening experiment was repeated 20 times per evaluator. There were 100 evaluators for each subjective evaluation on naturalness and speaker similarity.</p><p>As the subjective evaluation results in Table <ref type="table" target="#tab_1">2</ref> show, "uwNISQA-uwPaSST" demonstrated the lowest VC performance in terms of both naturalness and speaker similarity as is the case of the CER results shown in Table <ref type="table" target="#tab_0">1</ref>. On the other hand, "uwNISQA-fwPaSST" and "fwNISQA-fwPaSST" out-performed the conventional DT, although there is no statistical significance between the scores.</p><p>To investigate the differences between "Conventional DT", "uwNISQA-fwPaSST", and "fwNISQA-fwPaSST", we further conducted preference AB tests to compare the naturalness of the converted speech for any pair of these three methods. Fifty listeners took part in each test, and each listener evaluated 10 pairs of converted speech samples using our crowdsourcing-based evaluation platform. The results are shown in Table <ref type="table" target="#tab_2">3</ref>. The proposed methods (i.e. "uwNISQA-fwPaSST" and "fwNISQA-fwPaSST") significantly outperformed the conventional DT (p &lt; 0.05), but there was no significant difference between "uwNISQA-fwPaSST" and "fwNISQA-fwPaSST". This indicates that conditioning the VC model on z fwPaSST is effective for improving the naturalness of the converted speech in the noisy-to-clean VC scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We investigated noise-robust VC from noisy source speech to clean noisy speech in this paper and demonstrated the effectiveness of CDT. We anticipate that CDT should be applicable to VC where the target speech is also noisy. Noisy-to-noisy VC <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, which aims to preserve background noise of source speech during the VC process, is another possible situation in which CDT can be applied.</p><p>We used two latent variables, recording quality and environment, as the conditional features on the DT algorithm. Other variables that characterize input speech, such as speech naturalness (e.g., UTMOS) <ref type="bibr" target="#b24">[24]</ref> and reverberation (e.g., T60 estimator <ref type="bibr" target="#b25">[25]</ref>), can be also introduced to our CDT.</p><p>Regarding the training objective function, we only considered the simple L1 loss between the ground-truth and generated mel-spectrograms. We can introduce other machine learning techniques to improve the noise robustness of the trained VC model, such as adversarial training with feature decoupling <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed conditional denoising training (CDT), which conditions a VC model on two latent variables regarding the recording quality and acoustic environment of noisy source speech. We verified the effectiveness of four CDT methods in the cases where these two latent variables were utterance-wise or framewise. The objective and subjective evaluations showed that conditioning the VC model on frame-wise features can effectively improving VC performance, while conditioning it on utterancewise features does not necessarily improve VC performance.</p><p>In the future, we will consider VC models that take into account various degradations of input speech, including reverberation and bandwidth rejection, as well as the additive noise considered in this study. In addition, towards real-world applications, we will extend our CDT to address noisy speech recorded by actual devices such as smartphones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Latent variables extracted by NISQA and PaSST in an utterance-wise or frame-wise manner.</figDesc><graphic coords="2,82.32,74.17,90.64,136.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x s is calculated by adding noise n to the clean speech x c . g mel (•) is a function which calculates the log mel-spectrogram from input speech. fSSL(•) is a pre-trained SSL model that extracts intermediate feature representations from the source and target speech used for the VC process, i.e., z s SSL and z t SSL . The source speaker's conditional latent variables for recording quality and environment, i.e., z s rqa and z s asc , are extracted by pre-trained DNNs for recording quality assessment frqa(•) and acoustic scene classification fasc(•), respectively. f θ (•) is a DNN-based model parameterized by θ, which predicts the target speaker's log mel-spectrogram from the input features. The model parameter θ is updated to minimize the log mel-spectrogram prediction error shown in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conditioning the VC model on latent variablesnoise n to the clean speech x c with SNR randomly sampled from a uniform distribution U (0,<ref type="bibr" target="#b20">20)</ref> [dB]. Here, n was taken from the DEMAND<ref type="bibr" target="#b15">[15]</ref> noise and the WHAM!48kHz<ref type="bibr" target="#b16">[16]</ref> noise for the training and evaluation datasets, respectively. Thus, the noise of speech from the evaluation dataset was unseen in the training process.We downsampled the WHAM!48kHz to 16 kHz. Both contained various types of background noises.Referring to Huang et al.'s study<ref type="bibr" target="#b6">[6]</ref>, we adopted S2VC<ref type="bibr" target="#b9">[9]</ref> as a backbone VC model with the publicly available implementation on GitHub (robust-vc)1 . In CDT, we conditioned the VC model on (zuwNISQA or z fwNISQA ) and (zuwPaSST or z fwPaSST ) as shown in Figure2. We unified the size of these latent variables to that of zSSL, i.e., 256 × frame size in this study. We concatenated the latent variables along with the feature axis and input them to the source encoder. First, to unify the frame dimension of (zuwNISQA, zuwPaSST) to that of zSSL, we replicated them by a factor of the frame size of zSSL along with the frame axis. Meanwhile, we upsampled (z fwNISQA , z fwPaSST ) to align the frame lengths. Second, to unify the feature dimensions of (zuwNISQA, z fwNISQA ) and (zuwPaSST, z fwPaSST ), we inserted the linear projection layers from 64 to 256 dimensions and from 768 to 256 dimensions, respectively.In CDT, the S2VC model was trained based on the loss function shown in Eq. (7) while the parameters of the pretrained models, fSSL(•), frqa(•), and fasc(•), were fixed. Specifically, fSSL(•) is a feature extractor using contrastive predictive coding<ref type="bibr" target="#b17">[17]</ref> as is the case of the study of Huang et al.<ref type="bibr" target="#b6">[6]</ref>. The latent variable extractors, frqa(•) and fasc(•), were based on the official implementations of NISQA 2 and PaSST 3 , respectively. In the implementation of NISQA, the frame length was 150 ms and the frame shift was 40 ms. In the implementation of PaSST, the former was 160 ms and the latter was 50 ms. To generate the converted speech waveform from the predicted log mel-spectrogram, we used the HiFi-GAN vocoder<ref type="bibr" target="#b18">[18]</ref> trained on the JVS training data with a batch size of 16, while all the VC models were trained with a batch size of 6. The optimizer was AdamW<ref type="bibr" target="#b19">[19]</ref> with a learning rate of 5 × 10 -5 , and β1 = 0.9, β2 = 0.999. Each of the VC models had approximately 33 million parameters which were randomly initialized. The training was stopped when the validation loss converged completely and the training time was approximately 60 hours. We trained the VC model using both the conventional DT<ref type="bibr" target="#b6">[6]</ref> and our CDT frameworks and evaluated their effectiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average CER and SECS of the 250 samples converted by the VC models trained with conditional DT and CDT. The method without any conditioning corresponds to conventional DT. "uw" means utterance-wise and "fw" means frame-wise. Values in the parentheses indicate standard deviations.</figDesc><table><row><cell cols="2">Method NISQA PaSST</cell><cell>CER [%]</cell><cell>SECS</cell></row><row><cell>-</cell><cell>-</cell><cell>26.3</cell><cell>0.935 (±0.034)</cell></row><row><cell>uw</cell><cell>uw</cell><cell>27.2</cell><cell>0.938 (±0.036)</cell></row><row><cell>uw</cell><cell>fw</cell><cell>24.6</cell><cell>0.935 (±0.034)</cell></row><row><cell>fw</cell><cell>uw</cell><cell>24.7</cell><cell>0.931 (±0.036)</cell></row><row><cell>fw</cell><cell>fw</cell><cell>23.3</cell><cell>0.934 (±0.033)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Naturalness and speaker similarity of the speech samples converted by conventional DT and CDT. Values in the parentheses indicate 95% confidential intervals of the scores.Bold scores are the highest among the five compared methods.</figDesc><table><row><cell cols="2">Method NISQA PaSST</cell><cell>Naturalness</cell><cell>Speaker similarity</cell></row><row><cell>-</cell><cell>-</cell><cell>2.75 (±0.085)</cell><cell>2.43 (±0.082)</cell></row><row><cell>uw</cell><cell>uw</cell><cell>2.67 (±0.084)</cell><cell>2.39 (±0.082)</cell></row><row><cell>uw</cell><cell>fw</cell><cell>2.84 (±0.087)</cell><cell>2.50 (±0.082)</cell></row><row><cell>fw</cell><cell>uw</cell><cell>2.74 (±0.086)</cell><cell>2.43 (±0.083)</cell></row><row><cell>fw</cell><cell>fw</cell><cell>2.85 (±0.086)</cell><cell>2.47 (±0.082)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Preference</figDesc><table><row><cell cols="3">AB test results for naturalness of con-</cell></row><row><cell cols="3">verted speech for any pair of (Conventional DT, uwNISQA-</cell></row><row><cell cols="3">fwPaSST, fwNISQA-fwPaSST). "Con. DT" means conventional</cell></row><row><cell cols="3">DT, "(uw, fw)" means uwNISQA-fwPaSST, and "(fw, fw)"</cell></row><row><cell>means fwNISQA-fwPaSST.</cell><cell></cell><cell></cell></row><row><cell>A vs B</cell><cell>Naturalness</cell><cell>p-value</cell></row><row><cell cols="3">Con. DT vs (uw, fw) 0.414 vs 0.586 &lt; 10 -6</cell></row><row><cell cols="3">Con. DT vs (fw, fw) 0.442 vs 0.558 &lt; 10 -3</cell></row><row><cell>(uw, fw) vs (fw, fw)</cell><cell>0.514 vs 0.486</cell><cell>0.38</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://huggingface.co/reazon-research/ reazonspeech-espnet-next</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://huggingface.co/microsoft/ wavlm-base-sv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://www.lancers.jp/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements: This research was conducted as joint research between <rs type="funder">LY Corporation and Saruwatari-Takamichi Laboratory of The University of Tokyo, Japan</rs>. This work was supported by <rs type="funder">Research Grant S of the Tateishi Science and Technology Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Voice conversion for dubbing using linear predictive coding and hidden Markov model</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Mukhneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wijayanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadiyoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Southwest Jiaotong University</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral voice conversion for textto-speech synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Macon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Seatle, U.S.A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="285" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speakingaid systems using gmm-based voice conversion for electrolaryngeal speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="146" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voice Conversion Challenge 2020 -Intra-lingual semi-parallel and cross-lingual voice conversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020 (VCCBC)</title>
		<meeting>Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020 (VCCBC)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
			<biblScope unit="page" from="80" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from robust voice conversion: a survey</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-H. Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT, Virtual Conference</title>
		<meeting>SLT, Virtual Conference</meeting>
		<imprint>
			<date type="published" when="2021-01">Jan 2021</date>
			<biblScope unit="page" from="514" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward degradationrobust voice conversion</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="6777" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NISQA: A deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mittag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chehadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021</date>
			<biblScope unit="page" from="2127" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient training of audio Transformers with patchout</title>
		<author>
			<persName><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH<address><addrLine>Incheon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022</date>
			<biblScope unit="page" from="2753" to="2757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">S2VC: A framework for any-to-any voice conversion with self-supervised pretrained representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021</date>
			<biblScope unit="page" from="836" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot voice conversion by separating speaker and content representations with instance normalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="page" from="664" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denoispeech: Denoising text to speech with framelevel noise modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06">Jun. 2021</date>
			<biblScope unit="page" from="7063" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HEAR: Holistic evaluation of audio representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Steinmetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Velarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcnally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Competitions and Demonstrations Track</title>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
			<biblScope unit="page" from="125" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">JSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takamichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mitsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tanji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustical Science and Technology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="761" to="768" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICA</title>
		<meeting>ICA<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WHAM!: Extending speech separation to noisy environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTER-SPEECH</title>
		<meeting>INTER-SPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020-12">Dec. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR<address><addrLine>New Orleans, U.S.A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WavLM: Large-scale selfsupervised pre-training for full stack speech processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1518" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Direct noisy speech modeling for noisy-to-noisy voice conversion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Tobing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="6787" to="6791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Preserving background sound in noise-robust voice conversion via multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-06">Jun. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UTMOS: UTokyo-SaruLab system for Voice-MOS Challenge 2022</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saeki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koriyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takamichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Incheon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022</date>
			<biblScope unit="page" from="4521" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reverberation-controllable voice conversion using reverberation time estimator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTER-SPEECH</title>
		<meeting>INTER-SPEECH<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-08">Aug. 2023</date>
			<biblScope unit="page" from="2103" to="2107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Noise-robust voice conversion using adversarial training with multi-feature decoupling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
