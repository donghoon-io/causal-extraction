<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifiability Guarantees for Causal Disentanglement from Purely Observational Data</title>
				<funder ref="#_efNuY89">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_zpCGGcF">
					<orgName type="full">DOE</orgName>
				</funder>
				<funder ref="#_gZ4nMf2">
					<orgName type="full">NCCIH/NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Simons Investi-</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-23">23 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Welch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LIDS</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Broad Institute of MIT</orgName>
								<address>
									<settlement>Harvard</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LIDS</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Broad Institute of MIT</orgName>
								<address>
									<settlement>Harvard</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology Broad Institute of MIT</orgName>
								<address>
									<settlement>Harvard</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifiability Guarantees for Causal Disentanglement from Purely Observational Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-23">23 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.23620v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal disentanglement aims to learn about latent causal factors behind data, holding the promise to augment existing representation learning methods in terms of interpretability and extrapolation. Recent advances establish identifiability results assuming that interventions on (single) latent factors are available; however, it remains debatable whether such assumptions are reasonable due to the inherent nature of intervening on latent variables. Accordingly, we reconsider the fundamentals and ask what can be learned using just observational data. We provide a precise characterization of latent factors that can be identified in nonlinear causal models with additive Gaussian noise and linear mixing, without any interventions or graphical restrictions. In particular, we show that the causal variables can be identified up to a layer-wise transformation and that further disentanglement is not possible. We transform these theoretical results into a practical algorithm consisting of solving a quadratic program over the score estimation of the observed data. We provide simulation results to support our theoretical guarantees and demonstrate that our algorithm can derive meaningful causal representations from purely observational data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in representation learning play a pivotal role in the application of machine learning across various fields, including natural language processing, computer vision, and life sciences (c.f., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52]</ref>). The emerging field of causal disentanglement holds the promise to augment such advances by identifying and learning some aspects about the latent causal factors behind data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>. These latent causal factors have been shown to improve the interpretability of high-level concepts behind complex high-dimensional data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b52">53]</ref> and enable extrapolation to predict how novel interventions will affect the data <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In pursuit of causal disentanglement, two critical questions are: <ref type="bibr" target="#b0">(1)</ref> to theoretically understand to what extent the latent causal factors are identifiable, and (2) to algorithmically design efficient methods to learn these factors with finite samples. Despite the recent surge of interest in this area, these questions remain difficult given the inherent challenges of both disentanglement and causal discovery. In the disentanglement literature, the latent factors are assumed to be independent, and it is known that identifying them is not possible without further knowledge on the data-generating process <ref type="bibr" target="#b14">[15]</ref>. Relaxing the independence assumption, causal disentanglement considers potentially related latent factors and aims to discover not only the latent factors but also their latent causal relations. Since this extends disentanglement, the latent factors are unidentifiable without additional information. Furthermore, learning causal relations is notoriously challenging as the number of variables grows: the underlying structure is generally not unique <ref type="bibr" target="#b3">[4]</ref>, and it is computationally and sample inefficient to learn complex causal graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>To overcome these difficulties, a trend in recent works has been to consider having access to interventional data, where a common assumption is to assume that interventions on all single latent factors are available <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b8">9]</ref>. Although a goal of causal disentanglement is to be able to control individual latent factors, it is debatable whether assuming existence of direct interventions on latent factors is reasonable <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3]</ref>, since one can argue that direct interventions on (single) latent factors make these factors non-latent. Furthermore, it might be infeasible to perform interventions on (some of) the factors due to ethical or cost reasons. As a consequence, it is important to understand what can be achieved solely based on observational data.</p><p>In our work, we consider causal disentanglement from purely observational data. The key idea behind our approach is to utilize asymmetries in the joint distribution of the latent factors. In particular, we consider latent factors that are generated by an unknown nonlinear causal model with additive Gaussian noises, from which we obtain observations after an unknown linear mixing. Nonlinear models with additive Gaussian noises have been a popular choice in the causal discovery literature due to their flexibility, intriguing identifiability properties (in the fully observed setting), and benign statistical sample complexities <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b60">61]</ref>. These models imply asymmetric relationships between causes and effects, which can be utilized to distinguish causal directions. Beyond their theoretical properties, these models are commonly chosen to represent real world causal systems, such as gene regulatory networks <ref type="bibr" target="#b15">[16]</ref>, given their ability to fit non-parametric relationships.</p><p>Contributions and Organization. We define nonlinear additive Gaussian noise models in the context of causal disentanglement in Section 2. We provide a precise characterization of latent factors that can be identified in such models, with purely observational data and no graphical restrictions. In particular, we show that the latent variables can be identified up to a layer-wise transformation that is consistent with the underlying causal ordering, and that further disentanglement is not possible. These results are provided in Section 3. We transform these theoretical results into practical algorithms in Section 4 by building upon recent successes of combining score matching and causal discovery <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29]</ref> to devise a method that solves a quadratic program over the score estimation of the observed data. The resulting algorithm enjoys efficiency and flexibility to be combined with any existing off-the-shelf score estimation method. We demonstrate our results empirically with simulations in Section 5, and conclude with a discussion in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Causal disentanglement. Previous works in causal disentanglement have mostly considered varying assumptions on: the available data, the underlying causal model of the latent factors, and the mixing function between latent factors and observed data. Assuming the availability of interventional data, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9]</ref> established results for parametric causal models, whereas <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref> studied non-parametric causal models. Most of these works assume linear mixing (or a special case of polynomial mixing that can be easily reduced to linear functions), with the exception of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>, where stronger assumptions on either the parametric causal model or more interventions are required to compensate for the general mixing functions. Prior to these works, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> established results assuming counterfactual data, which usually leads to stronger identifiability as one can now contrast counterfactual pairs. Few recent works considered identifiability without interventions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b55">56]</ref>. These works typically assume that (parts of) the latent factors can be observed after multiple different mixing functions. In the case where only one observational dataset is available, which is the setting of this paper, previous works have obtained results assuming both parametric models as well as additional structural restrictions on the mixing function <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b20">21]</ref>. Such structural restrictions refer to constraints on the set of latent variables that determine each observed variable, which is distinct from functional restrictions on the mixing function such as linearity. An example of such restrictions is the pure child assumption <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b13">14]</ref>, specifying that each observed variable has only one latent parent. To the best of our knowledge, our work is the first to establish identifiability guarantees of causal disentanglement in the purely observational setting without imposing any structural assumptions over the mixing function. We summarize these comparisons to prior works in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We additionally recognize that identifiability of latent factors from purely observational data has been considered outside of causal disentanglement <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20]</ref>. However, these results do not extend to the setting considered in this work, given the assumed data generating processes to do encapsulate the causal graph.</p><p>Score matching in causal discovery. Since our algorithm builds upon discovery methods using score matching, we briefly review these approaches. Works in this direction have mainly focused on causal discovery when all causal variables are observable in identifiable paramteric causal models such as nonlinear additive Gaussian noise or additive non-Gaussian noise models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28]</ref>. These methods first learn a topological ordering of the causal variables using the second-order derivative of the log-likelihood estimated from score matching. They then apply regression based DAG pruning techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> to retrieve the full causal structure. We note that these works do not inherently extend to causal disentanglement, for they assume direct access to the causal variables that disentanglement intends to learn.</p><p>Expanding these ideas to causal disentanglement is difficult, since we do not observe the latent factors and can only estimate the log-likelihood of the observed variables. Surprisingly, our theory suggests a simple principle to obtain meaningful estimates of the latent factors from the log-likelihood of the observed variables. Moreover, we show that a simple quadratic program can be used to implement this principle, which leads to an efficient algorithm borrowing strength from both nonlinear optimization and machine learning.</p><p>Our principle for identifiability directly expands the main result from <ref type="bibr" target="#b34">[35]</ref>, where variance properties on the diagonal elements of the Jacobian over the score of causal variables are used to derive a topological ordering. While we utilize this result, it is not sufficient for disentanglement given the aforementioned Jacobian can only be determined up to an unknown quadratic form when the causal variables are unobserved. Therefore, our principle additionally relies on properties of the entire Jacobian matrix, which we present in Section 3.2.</p><p>Additionally, we recognize the inherent difficulties of both non-convex optimization and second-order score estimation essential for modern score matching methods, which we discuss further in Section 4.1 and Section 5.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>We now formally define the causal disentanglement problem and introduce relevant definitions. We consider the observed variables X = (X 1 , ..., X d ) ⊤ ∈ R d×1 as generated from the latent causal factors Z = (Z 1 , ..., Z n ) ⊤ ∈ R n×1 via an unknown invertible linear mixing. We do not assume that the latent dimension n is known a priori, but rather can be learned as given by the principle presented in Lemma 1 of <ref type="bibr" target="#b58">[59]</ref>. These latent factors follow a joint distribution p(•), which factorizes according to an unknown directed acyclic graph (DAG) G. We summarize the setup in the following assumption. Assumption 1 (Linear mixing). Our data-generating process can be written as</p><formula xml:id="formula_0">X = H • Z, Z ∼ p(Z) = n i=1 p(Z i |Z pa(i) ),</formula><p>where H ∈ R d×n has full column rank and pa(i) denotes the parents of node i in G.</p><p>We assume linear mixing as it is essential for our theoretical guarantees presented in Section 3.2. However, our results also extend to settings where the true mixing function can be reduced to a linear map, such as in the case of a special class of polynomials <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b58">59]</ref>. For the distribution of Z, we consider nonlinear additive Gaussian noise models as follows.</p><p>Assumption 2 (Nonlinear additive Gaussian noise model.). The factorization term in the joint distribution over Z is specified by</p><formula xml:id="formula_1">Z i = f i (Z pa(i) ) + E i , ∀i ∈ [n],</formula><p>where f = {f i : i ∈ [n]} are twice continuously differentiable, non-linear<ref type="foot" target="#foot_0">foot_0</ref> functions that capture the dependence of Z i on its parents, and E = {E i : i ∈ [n]} denote exogenous noise variables, which are mutually independent and mean-zero Gaussians, i.e., E i ∼ N (0, σ 2 i ).</p><p>We use p X (•) to denote the induced distribution over the observed variables X. We consider causal disentanglement from purely observational data, where we only have access to a dataset consisting of samples from p X (•). Our goal is to learn the most about Z (or equivalently, E, G, and f ) using this dataset. We additionally note that this problem has also been called causal representation learning in literature. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the described setup.  Estimators. We denote generic estimators of Z and E from X by Ẑ(X) : R d → R n and Ê(X) : R d → R n respectively. In our setup, these estimators are constructed by learning the inverse of the unknown mixing matrix H. We denote a valid estimate of this mixing matrix by Ĥ ∈ R d×n , and its Moore-Penrose inverse by Ĥ † = Ĥ⊤ • ( Ĥ Ĥ⊤ ) -1 . To obtain an estimate of Z, we use Ẑ(X) = Ĥ † • X. For simplicity, we denote the transformation from the estimated latent Ẑ(X) to the true latent factors Z by the matrix β ∈ R n×n , where</p><formula xml:id="formula_2">β = H † • Ĥ and Z = β • Ẑ(X).</formula><p>Graph notation. We use ch(i), an(i) and de(i) to denote the children, ancestors and descendants of node i in G, respectively. Node i is called a root node if an(i) = ∅, and a leaf node if de(i) = ∅. We define the k th layer of G, denoted by layer(k), to be the set of all nodes whose longest path to a leaf node is k. Figure <ref type="figure" target="#fig_1">2</ref> illustrates this concept. With a slight abuse of notation, we will interchangeably use Z i ∈ layer(k) to denote i ∈ layer(k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Identifiability Results</head><p>In this section, we present our main theoretical results. We start by providing a precise characterization of latent factors that are identifiable in Section 3.1. We then demonstrate identifiability by providing a constructive proof in Section 3.2. Counterexamples showing that further disentanglement is not possible and our results cannot be strengthened are given in Section 3.3. Detailed proofs are deferred to Appendix A. Throughout this section, we consider the infinite-data regime where enough samples are obtained to exactly determine the observational distribution p X (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Layer-wise Transformations</head><p>For each latent causal factor Z i , we show that its identifiability is dependent on the layer of the corresponding node. Specifically, we show that Z i ∈ layer(k) can be identified up to a linear combination of all variables in layer(k</p><formula xml:id="formula_3">) ∪ layer(k + 1) ∪ • • • ∪ layer(r)</formula><p>, where r denotes the top most layer. The formal definition is as follows.</p><p>Definition 1 (Identifiability up to upstream layers). The latent causal variables Z are identifiable up to upstream layers if it is possible to learn Ẑ(X) from p X (•) such that:</p><formula xml:id="formula_4">Ẑ(X) = P π • C • Z, ∀Z ∈ R n ,</formula><p>where P π ∈ R n×n is a permutation matrix, and C ∈ R n×n is a constant matrix with non-zero diagonal entries and [C] i,j = 0 for all i, j such that i ∈ layer(k) and j∈ ∪ l≤k layer(l).</p><p>This identifiability notion implies that each causal variable can be learned up to a linear combination that does not depend on its descendants. Intuitively, this implies that variables that are more upstream in the underlying causal DAG can more easily be identified. In particular, the root nodes (i.e., the most upstream causal factors) can be identified up to a linear transformation of themselves.</p><p>Beyond this Z-based notion of identifiability, we can further disentangle the exogenous noise variables up to a transformation that depends only on its own layer. The formal definition is as follows.</p><p>Definition 2 (Identifiability up to layers). The exogenous noise variables E are identifiable up to layers if it is possible to learn Ê(X) from p X (•) such that:</p><formula xml:id="formula_5">Ê(X) = P π • C • E, ∀E ∈ R n ,</formula><p>where P π ∈ R n×n is a permutation matrix, and C ∈ R n×n is a constant matrix with non-zero diagonal entries and [C] i,j = 0 for all i, j such that i ∈ layer(k) and j ̸ ∈ layer(k).</p><p>Next, we prove these notions of identifiability in a constructive way using the score function of p X (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identification via Score Functions</head><p>Our analysis will rely on the score function of the observational distribution of X, denoted by</p><formula xml:id="formula_6">s X (x) = ∇ x log p X (x),</formula><p>as well as its Jacobian matrix whose ij th entry is given by</p><formula xml:id="formula_7">[J X (x)] ij = ∇ xi ∇ xj log p X (x).</formula><p>Since X and Z are related through a linear transformation, we can easily write out the closed form for both the score and associated Jacobian of the latent variables Z as follows. Lemma 1. <ref type="foot" target="#foot_1">3</ref> Under Assumption 1, the score functions and associated Jacobian matrices over X and Z are related via the following transformations:</p><formula xml:id="formula_8">s Z (z) = H ⊤ s X (x), J Z (z) = H ⊤ J X (x)H.</formula><p>For an estimator Ẑ(X) = Ĥ • X, we utilize Lemma 1 to obtain the following:</p><formula xml:id="formula_9">J Ẑ (ẑ) = Ĥ⊤ J X (x) Ĥ = β ⊤ J Z (z)β.</formula><p>This shows that we can compute J Ẑ once we estimate Ĥ and J X from p X (•), and that J Ẑ relates to the Jacobian matrix over the true latent variables, J Z , via a quadratic form J Ẑ = β ⊤ J Z β, where β is a product of the unknown H † and Ĥ.</p><p>Under the nonlinear additive Gaussian noise model in Assumption 2, <ref type="bibr" target="#b34">[35]</ref> demonstrated that the i th diagonal element of J Z will have zero variance if and only if node i is a leaf node in G. Building on this result, we can derive a sufficient and necessary condition for when the i th diagonal element of J Ẑ will have zero variance involving the unknown matrix β as follows.</p><p>Lemma 2. The i th diagonal element of [J Ẑ (ẑ)] ii has zero variance, i.e., Var [J Ẑ (ẑ)] ii = 0, if and only if the i th column of β has zero entries in every element corresponding to non-leaf nodes.</p><p>This result provides the intuition that leads to the principle for achieving identifiability. In particular, if we maximize the number of zero-variance terms in the diagonal elements of the estimated Jacobian J Ẑ , then the unknown matrix β must have a maximum number of columns with zeros in all indices corresponding to non-leaf nodes. Since Z = β • Ẑ, we can derive the relation between Ẑ and Z under this maximization, which we summarize in the following lemma. Lemma 3. If we learn Ĥ by solving</p><formula xml:id="formula_10">min Ĥ∈R n Var diag(J Ẑ Ĥ † x) 0 , such that rank( Ĥ) = n,<label>(1)</label></formula><p>then it follows that</p><formula xml:id="formula_11">Ẑi = linear(Z non-leaf ) if Var J Ẑ (ẑ) ii ̸ = 0, linear(Z) if Var J Ẑ (ẑ) ii = 0,</formula><p>where the number of i ∈ [n] such that Var J Ẑ (ẑ) ii = 0 equals to the number of leaf nodes in G.</p><p>It follows from this lemma that we can obtain representations of all non-leaf nodes as linear transformations of all non-leaf latent variables (i.e. layer(1) and above). In other words, we can disentangle the leaf nodes out from the non-leaf nodes. Given this identified linear transformation of the nonleaf nodes, we can iteratively apply Lemma 3 to prune representations of each variable as a linear combination of all variables in its own and upstream layers. This leads to our main theorem. Theorem 1. Under Assumptions 1 and 2, the latent variables Z are identifiable up to their upstream layers from purely observational data.</p><p>Importantly, this result holds without any structural restrictions on the mixing function or the latent causal DAG. It indicates that we can derive representations of latent factors free of all downstream variables, and that it is easier to disentangle the more upstream causal factors.</p><p>Building on Theorem 1, we can show a stronger notion of identifiability for the exogenous noise variables. Consider any layer(i) representation given by a linear combination of all variables in layer(i + 1) ∪ • • • ∪ layer(r), where r denotes the top most layer. Then from the structural equations, it follows that this representation depends nonlinearly on the exogenous noise variables associated with layer(i + 1) ∪ • • • ∪ layer(r) and linearly on the the exogenous noise variables associated with layer(i), which we denote by E layer(i) . Thus, if we regress this representation on all upstream layer representations, e.g., using kernel regression, then the residual terms will equate to a linear combination of E layer(i) . Performing this procedure over all layers i, we can determine layer-wise transformations of E, giving rise to the following theorem. Theorem 2. Under Assumptions 1 and 2, the exogenous noise variables E are identifiable up to their layers from purely observational data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impossibility Results</head><p>Next, we show that further disentanglement is not possible. In particular, we cannot further disentangle the exogenous noise variables within any given layer. The following example illustrates this with two variables. Suppose the exogenous noise variables E 1 and E 2 are identified via two linear combinations denoted by</p><formula xml:id="formula_12">Ê1 = a 1 E 1 + a 2 E 2 , Ê2 = b 1 E 1 + b 2 E 2 .</formula><p>We only know that they are independent mean-zero Gaussian variables. However, any linear coefficients with a</p><formula xml:id="formula_13">1 b 1 σ 2 1 + a 2 b 2 σ 2 2 = 0 satisfy Cov( Ê1 , Ê2 ) = a 1 b 1 σ 2 1 + a 2 b 2 σ 2 2</formula><p>= 0, which means Ê1 and Ê2 are independent. This indicates that Ê1 and Ê2 do not provide enough information to further disentangle E 1 or E 2 . In general, this impossibility result holds for arbitrary graphs. Proposition 1. Under Assumptions 1 and 2, the exogenous noise variables E are generally unidentifiable beyond layer-wise transformation from observational data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm for Layer Recovery</head><p>We now transition to developing practical algorithms to recover the guaranteed causal representations. Our approach consist of two steps: (1) solving for the representations of latent variables up to upstream-layer transformations, and (2) solving for representations of exogenous noise variables up to layer-wise transformations, where step 2 utilizes the output of step 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Step 1: Quadratic Programming on Estimated Scores</head><p>The proof sketch in Section 3.2 provides a simple principle for causal disentanglement. It suggests that we can solve for the estimated mixing function, Ĥ, at each iteration by maximizing the number of zero-variance terms in the Jacobian of the estimated latent score (i.e., Equation ( <ref type="formula" target="#formula_10">1</ref>)). However, this rank-constrained optimization problem is discontinuous and non-convex, leading to an NP-hard problem <ref type="bibr" target="#b45">[46]</ref>. Moreover, the objective function involves the ℓ 0 -norm of a vector of variance terms, which can be hard to optimize.</p><p>To resolve these difficulties, we reduce this optimization problem into a sequence of easier problems. Note that V ar[J Ẑ (z)] ii depends only on the i th column of Ĥ, which we denote as [ Ĥ] i . It follows that we can solve for each column separately by solving for [ Ĥ] i such that V ar[J Ẑ (z)] ii = 0 while not violating the rank constraint. Considering the finite-sample setting where we plug in the sample estimate for V ar[J Ẑ (z)] ii , this problem can be formulated as the following quadratically constrained quadratic program (QCQP):</p><formula xml:id="formula_14">min h∈R n 0 such that h ⊤ JX (x (m) )h = 0, ∀m ∈ [N ], h ⊤ h = 1, h ⊤ [ Ĥ] j = 0, ∀j ∈ [i -1].<label>(2)</label></formula><p>Here, we use estimated zero-centered Jacobians JX of observed samples x (m) , given by JX (x (m) ) ≜ ĴX (x (m) ) -JX (X) with JX (X) ≜ 1 /N N m=1 ĴX (x (m) ). The constraint h ⊤ JX (x (m) )h = 0 is equivalent to enforcing the sample estimate of V ar[J Ẑ (z)] ii to be zero. The additional constraints h ⊤ h = 1 and h ⊤ [ Ĥ] j = 0 ensure that we do not violate the rank constraint. A formal derivation of equivalence is given in Appendix B.</p><p>Breaking the problem in Equation ( <ref type="formula" target="#formula_10">1</ref>) into a series of problems in Equation ( <ref type="formula" target="#formula_14">2</ref>) allows us to operate over a lower dimensional space and use any off-the-shelf solvers for QCQP. In practice, we use the cutting plane method for mixed integer programming <ref type="bibr" target="#b24">[25]</ref>. Algorithm 1 summarizes the overall approach, where we construct the layer(k) representations iteratively by solving a series of QCQPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Step 2: Layer-wise Nonlinear Regression</head><p>Given the learned representation Ẑ from Algorithm 1, we now proceed to disentangle the exogenous noise variables, which fully determine the randomness of the observations. Following the proof of Theorem 2, we can recover a representation of the exogenous noise variables E layer(k) by non-linearly regressing the layer(k) representation of Z on all upstream representations and taking the residual terms. This procedure is summarized in Algorithm 2.</p><p>Algorithm 1 Recovering Z up to upstream layers.</p><p>1: Input: N samples of X in the observational distribution. 2: Estimate JX (x (m) ), ∀m ∈ [N ] using any off-the-shelf score estimation method (see Section 5). 3: Initialize Ẑ = 0 n×N , X = (x (1) , . . . , x (N ) ) ∈ R d×N , and k = n. 4: while k &gt; 0 do 5:</p><p>Initialize Ĥ = 0 k×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for i = 1, . . . , d do 7:</p><p>Set [ Ĥ] i to be the solution of Equation ( <ref type="formula" target="#formula_14">2</ref>). Break when no feasible solution is found. Fill in all-zero columns of Ĥ with random vectors to remain full column rank. Set X = 0 0×N .</p><p>12: Fit nonlinear regression on Ẑlayer(k) using Êlayer(k+1) , . . . , Êlayer(K) .</p><formula xml:id="formula_15">for i = 1, ..., k do 13: if V ar[J Ẑ (ẑ)] i,i = 0 then 14: Set [ Ẑ] n-k = [ Z] i and let k ← k -1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Set Êlayer(k) as the residual terms. 6: end for 7: Return: Ê.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Results</head><p>We test our proposed algorithms using simulations <ref type="foot" target="#foot_3">4</ref> . Algorithm 1 requires estimating the score of the observational distribution and its performance relies on the quality of this estimation. To evaluate this, we conduct two sets of experiments. In Section 5.1, we use perfect score oracles, which compute the Jacobian matrices exactly using the ground-truth data-generating process. This serves as a verification of our theoretical results. In Section 5.2, we estimate the score functions from the samples using two popular score-estimation methods. Details of the experiments can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Score Oracle Simulations: Validation of Theoretical Results</head><p>To further validate our theoretical results, we run Algorithms 1 and 2 to learn the latent causal factors and the exogenous noise variables. We consider the following causal graphs with 4 nodes: (1) a line graph represented as</p><formula xml:id="formula_16">Z 1 → Z 2 → Z 3 → Z 4 , and (2) a Y-structure represented as Z 1 → Z 2 → Z 3 , Z 2 → Z 4 .</formula><p>For each case, we generate 2000 observational samples and compute the corresponding scores using the ground-truth link functions.</p><p>We present the results of our estimation in Figure <ref type="figure" target="#fig_4">3</ref>. The scatter plots depict the relationships between the ground-truth Z i and the estimated Ẑj , where we color the dots with the values of Z 1 . The heatmaps show the mean absolute correlations (MAC) between the ground-truth E i and the estimated Êj . For the estimated latent causal factors Ẑ, we see trends that are consistent with Theorem 1 in both cases, where the root node is perfectly identified and Z 2 is estimated with some mixing of Z 1 .</p><p>For the estimated exogenous noise variables Ê, the results validate Theorem 2. In the line graph, our algorithm perfectly disentangles all variables; in the Y-structure, we can perfectly disentangle E 1 and E 2 , while E 3 and E 4 are mixed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results using Score Estimation</head><p>In this set of experiments, we aim to mimic real-world settings where the score functions are estimated from samples. We use two popular methods to generate point-wise estimates of the Jacobians of the scores: the second-order Stein estimator <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref> and the sliced score matching with variance reduction (SSM-VR) estimator <ref type="bibr" target="#b41">[42]</ref>. We then plug these estimators into Algorithms 1 and 2.</p><p>We use the same sampling procedure on the four-node line graph as described in the previous section with varying sample sizes. Here we evaluate the mean absolute correlation (MAC) between the true and estimated exogenous noise variables. We adjust the tolerance of our QCQP solver to account for noisy estimates (see Appendix C). Table <ref type="table">2</ref> reports the results averaged across 10 repeated runs. With noisy score estimates, we can still learn these variables although, as expected, accuracy decreases as compared to the results using oracle score estimates, where we can recover the exogenous noise almost perfectly.</p><p>As reliable higher-order score estimation is an active area of research <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref>, we seek to evaluate how the accuracy of our algorithm can increase under improved score estimation. Specifically, we consider how the MAC of exogenous noise estimates behaves under varying levels of noise in the plug-in Jacobians. We perturb the true Jacobian matrices with noise and plot the returned MAC with respect to the signal-to-error ratio (SER) in Figure <ref type="figure">4</ref>. This shows that the accuracy improves with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we derive partial identifabilty guarantees of causal disentanglement from purely observational data and linear mixing without any structural restrictions. In particular, we utilize asymmetries in nonlinear causal models with additive Gaussian noise. We provide a precise characterization of identifiability in this setting, where the latent causal factors can be identified up to upstream layers and the exogenous noise variables can be identified up to their layers. We show that further disentanglement is not possible without additional assumptions or alternative datasets.</p><p>These theoretical analyses indicate a simple but hard to optimize principle for deriving efficient algorithms. We show that this optimization problem can be solved via a series of simpler quadratically constrained quadratic programs. This leads to a flexible algorithm that allows us to use any off-theshelf QCQP solvers and score estimation methods. We demonstrate its correctness and efficiency using simulations.</p><p>While we view this work as having a primarily theoretical contribution, we additionally believe that the notion of layer-wise identifiability has many practical implications. In particular, our methods can be used to identify hierarchical topics at various layers of a causal system. For instance, when applied to a latent genealogical tree, each layer representation would contain all prior ancestral information used to determine the traits of a given generation.</p><p>In future work, it would be interesting to extend our results to latent causal models with other asymmetries. In particular, we believe our result could be extended to learn upstream layer representations of nonlinear additive models with generic noise as an extension of <ref type="bibr" target="#b27">[28]</ref>, by modifying the principle to achieve identifiability in Equation <ref type="bibr" target="#b0">(1)</ref>. It would also be interesting to understand how our identifability results in the purely observational setting could aid when additional external data, such as interventions or multi-modal data, are available. Additionally, since our work shows that causal disentanglement can be solved orthogonally to score estimation, extending and testing our proposed approaches to applications where there exist pretrained score estimators would be another interesting avenue to pursue. Furthermore, given further disentanglement beyond layer-wise identifiability is not possible with purely observational data, it remains unclear what minimum faithfulness assumptions are required to achieve stronger identifiability guarantees, which we view as an import question.</p><p>Broader impact. Our work advances the field of causal representation learning, where it was commonly thought that without interventional data causal variables could only be discovered up to linear combinations of all variables without structural assumptions. While our work has many potential applications, we feel that no particular societal consequence needs to be highlighted.</p><p>gator Award to Caroline Uhler. R.W. was supported by a fellowship by the Eric and Wendy Schmidt Center at the Broad Institute and the Advanced Undergraduate Research Opportunities Program at MIT. J.Z. was partially supported by an Apple AI/ML PhD Fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs for Identifiability</head><p>A.1 Proof of Lemma 1</p><p>Proof. Given the linear relation X = H • Z, we relate the probability density functions p(•) of Z and p X (•) via p X (x) = p(H † x)| det(H † )|. Furthermore, we write the gradient of the log density of p X (x) with respect to X as</p><formula xml:id="formula_17">∇ X log p X (x) = ∇ X p X (x) p X (x) = ∇ X p(H † x)| det(H † )| p(H † x)| det(H † )| = ∇ X p(H † x) p(H † x) = ∇ X log p(H † x) = (H † ) ⊤ ∇ Z log p(z). Thus, it follows that ∇ Z log p(z) = H ⊤ ∇ X log p X (x), or s Z (z) = H ⊤ s X (x) as desired.</formula><p>Differentiating ∇ X log p X (x) with respect to X, we get</p><formula xml:id="formula_18">∇ 2 X log p X (x) = ∇ X (H † ) ⊤ ∇ Z log p(z) = (H † ) ⊤ ∇ 2 Z log p(z)(H † ). Thus, it additionally follows that ∇ 2 Z log p(z) = H ⊤ (∇ 2 X log p X (x))H, or J Z (z) = H ⊤ J X (x)H.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 2</head><p>Before proceeding to the proof of Lemma 2, we must prove the following supplementary lemma.</p><p>Lemma 4. For any two distinct leaf nodes k and l, it follows that ∂s k (z) ∂z l = 0.</p><p>Proof. From <ref type="bibr" target="#b34">[35]</ref>, we denote the score of the latent variable Z k evaluated at z as</p><formula xml:id="formula_19">s k (z) = - z k -f k (z pa(k) ) σ 2 k + i∈ch(k) ∂f i (z pa(i) ) ∂z k • z i -f i (z pa(i) ) σ 2 i .</formula><p>We further derive the following expression:</p><formula xml:id="formula_20">∂s k (z) ∂z l = 1 σ 2 k ∂f k (z pa(k) ) ∂z l + i∈ch(k) ∂ 2 f i (z pa(i) ) ∂z k ∂z l z i -f i (z pa(i) ) σ 2 i + 1 σ 2 i ∂f i (z pa(i) ) ∂z k ∂z i ∂z l - ∂f i (z pa(i) ) ∂z l = 1 σ 2 k ∂f k (z pa(k) ) ∂z l + i∈ch(k) ∂ 2 f i (z pa(i) ) ∂z k ∂z l z i -f i (z pa(i) ) σ 2 i + 1 σ 2 i ∂f i (z pa(i) ) ∂z k 1 {l=i} - ∂f i (z pa(i) ) ∂z l = 1 σ 2 k ∂f k (z pa(k) ) ∂z l + 1 {l∈ch(k)} 1 σ 2 l ∂f l (z pa(l) ) ∂z k + i∈ch(k)∩ch(l) 1 σ 2 i [∇ k ∇ l f i (z pa(i) ) • z i -∇ k ∇ l f i (z pa(i) ) • f i (z pa(i) ) -∇ k f i (z pa(i) ) • ∇ l f i (z pa(i) )].</formula><p>When k and l are distinct leaf nodes, ch(k) = ch(l) = ∅, and our expression simplifies to</p><formula xml:id="formula_21">∂s k (z) ∂z l = 1 σ 2 k ∂f k (z pa(k) ) ∂z l = 0,</formula><p>since l ̸ ∈ pa(k), which completes our proof.</p><p>We now proceed to the proof of Lemma 2.</p><p>Proof. (Lemma 2) We first prove the backward direction. Given J Ẑ (ẑ) = β ⊤ J Z (z)β, we express J Ẑ (ẑ) ii as</p><formula xml:id="formula_22">J Ẑ (ẑ) ii = n j=1 n k=1 β ji β ki ∂s j (z) ∂z k .</formula><p>Assuming that β ji = 0 for all j / ∈ layer(0), the above expression simplifies to</p><formula xml:id="formula_23">J Ẑ (ẑ) ii = n j,k∈layer<label>(0)</label></formula><p>β ji β ki ∂s j (z) ∂z k .</p><p>Utilizing Lemma 4 and Lemma 1 from <ref type="bibr" target="#b34">[35]</ref>, which states that Var ∂s k (z)</p><formula xml:id="formula_24">∂z k = 0 for all k ∈ layer(0), it follows that Var J Ẑ (ẑ) ii = 0.</formula><p>Now, we prove the forward direction. Denote C i := {j : β ji ̸ = 0} as the set of all indices in the i th column of β that are non-zero. It suffices to show that if there exists some k ∈ C i such that k / ∈ layer(0), then Var J Ẑ (ẑ) ii ̸ = 0.</p><p>Let i c be the most downstream node in ch(C i ) := {ch(j) : j ∈ C i } such that i c / ∈ pa(i) for any i ∈ ch(C i ). Such a node must exist under the assumption that some non-leaf node is contained in C i . We express J Ẑ (ẑ) ii as follows</p><formula xml:id="formula_25">J Ẑ (ẑ) ii = k,l∈Ci β ki β li ∂s k (z) ∂z l = k∈Ci β 2 ki ∂s k (z) ∂z k + k,l∈Ci k̸ =l β ki β li ∂s k (z) ∂z l = k∈Ci β 2 ki   - 1 σ 2 k + i∈ch(k) 1 σ 2 i ∇ 2 k f i (z pa(i) ) • z i -∇ 2 k f i (z pa(i) ) • f i (z pa(i) ) -(∇ k f i (z pa(i) )) 2 )   + k,l∈Ci k̸ =l β ki β li 1 σ 2 k ∂f k (z pa(k) ) ∂z l + 1 {l∈ch(k)} 1 σ 2 l ∂f l (z pa(l) ) ∂z k + i∈ch(k)∩ch(l) 1 σ 2 i ∇ k ∇ l f i (z pa(i) ) • z i -∇ k ∇ l f i (z pa(i) ) • f i (z pa(i) ) -∇ k f i (z pa(i) ) • ∇ l f i (z pa(i) ) = k∈Ci β 2 ki   - 1 σ 2 k + i∈ch(k) 1 σ 2 i ∇ 2 k f i (z pa(i) ) • E i -(∇ k f i (z pa(i) )) 2 )   + k,l∈Ci k̸ =l β ki β li 1 σ 2 k ∂f k (z pa(k) ) ∂z l + 1 {l∈ch(k)} 1 σ 2 l ∂f l (z pa(l) ) ∂z k + i∈ch(k)∩ch(l) 1 σ 2 i ∇ k ∇ l f i (z pa(i) ) • E i -∇ k f i (z pa(i) ) • ∇ l f i (z pa(i) ) .</formula><p>Now, by separating the terms containing i c , we get</p><formula xml:id="formula_26">J Ẑ (ẑ) ii = k∈Ci k∈pa(ic) β 2 ki 1 σ 2 ic ∇ 2 k f ic (z pa(ic) ) (E ic ) +<label>(3)</label></formula><p>k,l∈Ci k̸ =l k,l∈pa(ic)</p><formula xml:id="formula_27">(β ki β li ) 1 σ 2 ic ∇ k ∇ l f ic (z pa(ic) ) (E ic ) -<label>(4)</label></formula><p>k∈Ci k∈pa(ic)</p><formula xml:id="formula_28">β 2 ki 1 σ 2 ic ∇ k f ic (z pa(ic) ) 2 -<label>(5)</label></formula><p>k,l∈Ci k̸ =l k,l∈pa(ic)</p><formula xml:id="formula_29">(β ki β li ) 1 σ 2 ic ∇ k f ic (z pa(ic) ) ∇ l ∇ l f ic (z pa(ic) ) + (6) k∈Ci β 2 ki     - 1 σ 2 k + i∈ch(k) i̸ =ic 1 σ 2 i ∇ 2 k f i (z pa(i) ) • E i -(∇ k f i (z pa(i) )) 2 )     +<label>(7)</label></formula><p>k∈Ci k̸ =l</p><formula xml:id="formula_30">β ki β li 1 σ 2 k ∇ l f k (z pa(k) ) + 1 {l∈ch(k)} 1 σ 2 l ∇ k f l (z pa(l) ) + (8) i∈ch(k)∩ch(l) i̸ =ic 1 σ 2 i ∇ k ∇ l f i (z pa(i) ) • E i -∇ k f i (z pa(i) ) • ∇ l f i (z pa(i) ) .<label>(9)</label></formula><p>Let g(z_ ic ) = (5) + ( <ref type="formula">6</ref>) + ( <ref type="formula" target="#formula_29">7</ref>) + (8) + <ref type="bibr" target="#b8">(9)</ref>. Given that (5), ( <ref type="formula">6</ref>), ( <ref type="formula" target="#formula_29">7</ref>), ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_30">9</ref>) are functions of only variables upstream of Z ic , we have that g(z _ic ) ⊥ ⊥ E ic . Furthermore, let ic + E[h(z_ ic )] 2 σ 2 ic + V ar(g). Therefore, the variance of J Ẑ (ẑ) ii is positive if and only if V ar[h(z_ ic )] &gt; 0, E[h(z_ ic )] ̸ = 0, or V ar(g(z_ ic )) &gt; 0. We will now show that V ar[h(z_ ic )] &gt; 0 or E[h(z_ ic )] ̸ = 0.</p><p>Let us rewrite h(z_ ic ) in matrix form as h(z_ ic ) ∝ (β ′ i ) ⊤ ∂ 2 z pa(ic ) ,z pa(ic ) f ic (z pa(ic) )(</p><formula xml:id="formula_31">β ′ i ) = ∂ 2 β ′ i ,β ′ i f ic (z pa(ic) )</formula><p>, where β ′ i ∈ R |pa(ic)| is a vector formed by finding the entries that correspond to pa(i c ) in β. Note that by our assumption pa(i c ) ∩ C i ̸ = ∅, we thus have β ′ i ̸ = 0. By the directional non-linear assumption in Assumption 2, we know that ∂ 2</p><formula xml:id="formula_32">β ′ i ,β ′ i f ic (z pa(ic)</formula><p>) cannot be 0 for all realizations of z pa(ic) . This implies that P[h(z_ ic ) = 0] ̸ = 1, which further implies that either V ar[h(z_ ic )] &gt; 0 or E[h(z_ ic )] ̸ = 0 as desired.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The considered data-generating process. The latent variables Z follow a nonlinear causal model with additive Gaussian noises. We observe them after an unknown linear mixing (gray edges).</figDesc><graphic coords="4,108.00,378.65,205.92,115.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layers of the causal DAG. A latent variable is contained in layer(k) if its longest path to a leaf node is k.</figDesc><graphic coords="4,350.04,391.70,124.74,100.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>10 :</head><label>10</label><figDesc>ComputeZ = Ĥ † X and J Z ( Z(m) ) = Ĥ⊤ JX (x (m) ) Ĥ, ∀m ∈ [N ].11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Score oracle simulations. (A) Estimated versus true latent variables on the line graph. (B) Estimated versus true latent variables on the Y-structure. (C) Estimated versus true exogenous variables on the line graph. (D) Estimated versus true exogenous variables on the Y-structure.</figDesc><graphic coords="9,172.37,347.71,75.67,75.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 2 :Figure 4 :</head><label>24</label><figDesc>Figure 4: Mean absolute correlation (MAC) of E estimations v.s. Signal-to-error ratio (SER) of Jacobian matrices.</figDesc><graphic coords="10,377.28,72.00,126.72,83.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>hσ 2 ic∇</head><label>2</label><figDesc>(z_ ic ) = k,l∈Ci k,l∈pa(ic) (β ki β li ) 1 k ∇ l f ic (z pa(ic) ) ,which similarly contains only variables upstream of z ic . Thus it holds that h(z_ ic ) ⊥ ⊥ E ic . Now we writeJ Ẑ (ẑ) ii = h(z_ ic ) • E ic + g(z_ ic ),and it suffices to show that V ar[h(z_ ic ) • E ic + g(z_ ic )] ̸ = 0. Expanding this expression, we getV ar[h(z_ ic ) • E ic + g(z_ ic )] = V ar[h(z_ ic ) • E ic ] + V ar[g(z_ ic )] + 2Cov[h(z_ ic ) • E ic , g(z_ ic )] = E[h(z_ ic ) 2 E 2 ic ] -E[[h(z_ ic )E ic ] 2 + V ar(g) + 2E[h(z_ ic )E ic g(z_ ic )] -2E[h(z_ ic )E ic ]E[g(z_ ic )] = E[h(z_ ic ) 2 ]E[E 2 ic ] + V ar(g) = V ar[h(z_ ic )]σ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our results to prior works on causal disentanglement. For the latent model, L stands for linear mechanisms whereas NL stands for nonlinear mechanisms; G stands for Gaussian noise whereas NG stands for non-Gaussian noise; Discrete refers to discrete causal variables. Here, we summarize the identifiability results in terms of latent causal graph identification.</figDesc><table><row><cell></cell><cell>Data</cell><cell cols="3">Latent Model Structural Mixing Identifiability Results</cell></row><row><cell>[1, 8]</cell><cell>Counterfact.</cell><cell>General</cell><cell>No</cell><cell>Fully identifiable.</cell></row><row><cell>[43, 9]</cell><cell>Hard Interv.</cell><cell>LG</cell><cell>No</cell><cell>Fully identifiable.</cell></row><row><cell cols="3">[2, 49, 48, 50] Hard Interv(s). General</cell><cell>No</cell><cell>Fully identifiable.</cell></row><row><cell>[49, 59]</cell><cell>Soft Interv.</cell><cell>General</cell><cell>No</cell><cell>Up to transitive closure.</cell></row><row><cell>[45]</cell><cell>Multi-view</cell><cell>LG</cell><cell>No</cell><cell>Block-wise identifiable.</cell></row><row><cell>[58]</cell><cell>Multi-view</cell><cell>NL</cell><cell>No</cell><cell>Block-wise identifiable.</cell></row><row><cell>[14, 19]</cell><cell>Purely Obs.</cell><cell>Discrete</cell><cell>Yes</cell><cell>Up to Markov equivalence.</cell></row><row><cell>[11, 54, 55]</cell><cell>Purely Obs.</cell><cell>LNG</cell><cell>Yes</cell><cell>Fully identifiable.</cell></row><row><cell>[21]</cell><cell>Purely Obs.</cell><cell>NL</cell><cell>Yes</cell><cell>Fully identifiable.</cell></row><row><cell>This work</cell><cell>Purely Obs.</cell><cell>NLG</cell><cell>No</cell><cell>Up to causal layers.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>To ensure that fi are non-degenerate, we assume that they are "directional non-linear", i.e., there does not exist β ∈ R |pa(i)| with ∥β∥2 = 1 such that ∂ 2 β,β fi(z) = 0 for all z ∈ R |pa(i)| .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Similar results have been used in[48,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p><ref type="bibr" target="#b48">49]</ref>, where<ref type="bibr" target="#b48">[49]</ref> provided formulas for general mixings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Code is publicly available at: https://github.com/uhlerlab/observational-crl</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Chandler Squires</rs> for helpful discussions, as well as <rs type="person">Karthikeyan Shanmugan</rs> and the anonymous reviewers for their valuable feedback. This work was partially supported by <rs type="funder">ONR</rs> (<rs type="grantNumber">N00014-22-1-2116</rs>), <rs type="funder">NCCIH/NIH</rs> (<rs type="grantNumber">1DP2AT012345</rs>), <rs type="funder">DOE</rs> (<rs type="grantNumber">DE-SC0023187</rs>), and a <rs type="funder">Simons Investi-</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_efNuY89">
					<idno type="grant-number">N00014-22-1-2116</idno>
				</org>
				<org type="funding" xml:id="_gZ4nMf2">
					<idno type="grant-number">1DP2AT012345</idno>
				</org>
				<org type="funding" xml:id="_zpCGGcF">
					<idno type="grant-number">DE-SC0023187</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma 3</head><p>Proof. Without loss of generality, assume that layer(0) corresponds to the the last l indexed latent variables. We claim there must be exactly l such columns of β containing zero entries in every element corresponding to non-leaf nodes, or Ĥ could not be an optimal solution of Equation <ref type="bibr" target="#b0">(1)</ref>. We write β in block form as</p><p>We note that the columns of β need not be ordered in this way to achieve our result and that we assume this structure to simplify notation. We derive the inverse of β via taking the Schur complement as follows</p><p>where A -1 and C -1 are full column rank. Now taking, Ẑ = β -1 Z, we derive the desired equalities</p><p>thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 1</head><p>Proof. Assume without loss of generality that the latent variables Z are reverse layerly ordered such that layer(0) = {Z 0 , ..., Z l0 }, layer(1) = {Z l0+1 , ..., Z l1 }, and so on. Solving for Ĥ according to the optimization problem framed in Equation ( <ref type="formula">1</ref>), it follows from Lemma 3 that</p><p>where { Ẑi : V ar[J Ẑ ( Ẑ(X))]] ii = 0} denotes the representations of layer(0) variables up to upstream layers. Now, if we denote X ′ as the set of vectors in { Ẑi : V ar[J Ẑ ( Ẑ(X))]] ii ̸ = 0}, it follows that</p><p>where H ′ is a full column rank matrix. Thus, viewing X ′ as our observations of the true non-leaf latent variables, we can utilize Lemma 3 to show that we can recover the layer(1) up to its upstream layer representation. Continuing this method of pruning at each iteration, it is clear to see that we can derive the upstream layer representation for all variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Theorem 2</head><p>Proof. Assume there are n distinct layers of G. From Definition 1, it follows that Êlayer(n) = Ẑlayer(n) , given pa(i) = ∅ for all i ∈ layer(n). Now, considering the next layer, namely layer(n -1), we can express Ẑlayer(n-1) as follows given the principle in Assumption 2,</p><p>where NONLINEAR( Ẑlayer(n) ) denotes the nonlinear relationship between Ẑlayer(n-1) and Z pa(layer(n-1)) ∈ Z layer(n) specified by the linear combination of nonlinear functions {f i : i ∈ layer(n -1)}. However, given Êlayer(n) = Ẑlayer(n) , NONLINEAR( Ẑlayer(n) ) = NONLINEAR( Êlayer(n) ) can be determined. Thus, it follows we can learn Êlayer(n-1) as the residual of the nonlinear regression of Ẑlayer(n-1) on Êlayer(n) as Êlayer(n-1) = Ẑlayer(n-1) -NONLINEAR( Êlayer(n) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now generalizing to layer n -i, we can express Ẑlayer(n-i) as</head><p>Ẑlayer(n-i) = Êlayer(n-i) + NONLINEAR( Êlayer(n-i+1) , ..., Êlayer(n) ).</p><p>Therefore, by the same principle, we can determine Êlayer(n-i) as the residual of the nonlinear regression of Ẑlayer(n-i) on Êlayer(n-i+1) , ..., Êlayer(n) as</p><p>Therefore, we can determine Êlayer(n-i) for all i = 0, ..., n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation of Algorithms</head><p>We show that the optimization problem in Equation ( <ref type="formula">1</ref>) can equivalently be solved by solving the QCQP in Equation ( <ref type="formula">2</ref>) for each column sequentially. Given that the i th element of diag(H ⊤ J X (X)H) can be expressed as</p><p>we can naturally break our optimization problem into sub-problems of solving for the optimal column vector [H i ] that results in zero variance for the term above. We will now determine an equivalent expression for the variance of this term in terms of a given vector v ∈ R d :</p><p>With this reformulation, the following implication clearly follows:</p><p>This indicates that the first constraint in the QCQP from Equation ( <ref type="formula">2</ref>) solves for a column vector that results in a zero variance term in the diagonal of the estimated Jacobian matrix, as desired. The additional constraints ensure that all of the column vectors added to Ĥ are linearly independent, fulfilling the full column rank constraint from Equation (1). Thus, continuously solving this QCQP is equivalent to solving the rank-constrained optimization problem from Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Experiments C.1 Synthetic Data Sampling Procedure</head><p>We establish the causal relationships between all nodes and their parents to follow the parametric function f i (X) = ||X|| 2 + E i , where each E i is independently sampled from a mean-zero Gaussian distribution with variance uniformly distributed over [0.1, 1]. For each experiment, we generate random samples of the exogenous noise terms and produce the latent variables via the data generating procedure from Assumption 2. We perform min-max scaling such that every variable is within the range [0, 1]. We perform this scaling, since <ref type="bibr" target="#b32">[33]</ref> warns of the fact that valid causal orderings can often be recovered by order of the variables' variance in synthetically generated data, and we wish to show our algorithm performs as desired without this seeming advantage. We randomly sampled n × n full rank matrices and took the linear transformation of the latent variables on this matrix to derive the corresponding observational samples.</p><p>C.2 Implementation of QCQP Solver</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Perfect Score Estimation</head><p>With perfect score estimation, we solve each QCQP to global optimality efficiently using Gurobi optimization solvers <ref type="bibr" target="#b12">[13]</ref> on an Apple M2 CPU with 8 cores. We continuously solve the QCQP indicated by Equation ( <ref type="formula">2</ref>) with feasibility tolerance set to 0.001, until no feasible solutions remain. We continue by iteratively appending linearly independent column vectors of unit magnitude until Ĥ is of the desired dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Score Estimation</head><p>When using data-driven score estimation methods, we are not guaranteed to find any column vector that solves the specific QCQP indicated by Equation ( <ref type="formula">2</ref>) perfectly. To combat this challenge, we first prune the top 25% of de-meaned Jacobian estimates, JX (x), by Frobenius norm to remove outliers. We then solve for the minimum value t such that |v ⊤ JX (x (m) )v| ≤ t optimizing over all v ∈ R d . Then, we use the same procedure as in the perfect score estimation case with feasibility tolerance set to t + 0.001 to solve for the estimated matrix Ĥ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Implementation of Score Estimation</head><p>Stein Estimator. We implemented the second-order Stein estimator introduced in <ref type="bibr" target="#b4">[5]</ref> to generate the point-wise estimates of the score's Jacobian matrices. We use RBF kernels with bandwidth value selected as the median of pairwise distances between points in X. Our implementation is adapted from <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b34">[35]</ref>.</p><p>SSM-VR Estimator. We implemented the sliced score matching with variance reduction (SSM-VR) model developed by <ref type="bibr" target="#b41">[42]</ref> to generate functional score estimators. We then estimated the Jacobian of the score estimator using automatic differentiation. Our implementation is adapted from <ref type="bibr" target="#b47">[48]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised representation learning with sparse perturbations</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15516" to="15528" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interventional causal representation learning</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="372" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-domain causal representation learning via weak distributional invariances</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="865" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A characterization of markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Steen A Andersson</surname></persName>
		</author>
		<author>
			<persName><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><surname>Michael D Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Second-order stein: Sure for sure and other applications in high-dimensional inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cun-Hui</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1864" to="1903" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Identifying linearly-mixed causal representations from multi-node interventions</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmi</forename><surname>Ninad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.02695</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38319" to="38331" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning linear causal representations from interventions under general nonlinear mixing</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CAM: causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03299</idno>
		<title level="m">Anchored discrete factor analysis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petteri</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Estimation of genetic networks and functional structures between genes by using bayesian networks and nonparametric regression</title>
		<author>
			<persName><forename type="first">Seiya</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoru</forename><surname>Miyano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
			<publisher>World Scientific</publisher>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning nonparametric latent causal graphs with unknown interventions</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Causal machine learning: A survey and open problems</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aengus</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15475</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning latent causal graphs via mixture oracles</title>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18087" to="18101" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identifiability of deep generative models without auxiliary information</title>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15687" to="15701" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identification of nonlinear latent hierarchical models</title>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dodiscover: Causal discovery algorithms in Python</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaron</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Montagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Trevino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ness</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Citris: Causal identifiability from temporal intervened sequences</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Gavves</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13557" to="13603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.14153</idno>
		<title level="m">Anton van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifying weight-variant latent causal models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cutting planes in integer and mixed integer programming</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Weismantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="397" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical variable selection for generalized additive models</title>
		<author>
			<persName><forename type="first">Giampiero</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2372" to="2387" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating high order gradients of the data distribution by denoising</title>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25359" to="25369" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causal discovery with score matching on additive models with arbitrary noise</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Montagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicoletta</forename><surname>Noceti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="726" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Montagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicoletta</forename><surname>Noceti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03382</idno>
		<title level="m">Scalable causal discovery with score matching</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient learning of generative models via finite-difference score matching</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19175" to="19188" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Identifiability of causal graphs using functional models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3757</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning interpretable concepts: Unifying causal representation learning and foundation models</title>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09236</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Christof</forename><surname>Alexander G Reisach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><surname>Weichwald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13647</idno>
		<title level="m">Beware of the simulated dag! varsortability in additive noise models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Jacobian-based causal discovery with nonlinear ica</title>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Reizinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Score matching enables causal discovery of nonlinear additive noise models</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthäus</forename><surname>Kleindessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18741" to="18753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Identifying representations for intervention extrapolation</title>
		<author>
			<persName><forename type="first">Sorawit</forename><surname>Saengkyongam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04295</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Q O'</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotirios</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06201</idno>
		<title level="m">Diffusion models for causal discovery via topological ordering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6471</idno>
		<title level="m">On causal and anticausal learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Linear causal disentanglement via interventions</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Seigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Salil S Bhate</surname></persName>
		</author>
		<author>
			<persName><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="32540" to="32560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A bound for the error in the normal approximation to the distribution of a sum of dependent random variables</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Sixth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="583" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain causal representation learning</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Sturma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rank-constrained optimization and its applications</title>
		<author>
			<persName><forename type="first">Chuangchuang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="128" to="136" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Scorebased causal representation learning with interventions</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Acarturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08230</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">General identifiability and achievability for causal representation learning</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2314" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Score-based causal representation learning: Linear and general transformations</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varıcı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00849</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Nonparametric identifiability of causal representations from unknown interventions</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Wendong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On the sample complexity of causal discovery and the value of domain expertise</title>
		<author>
			<persName><forename type="first">Samir</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03274</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning in biomedicine</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wainberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Merico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="829" to="838" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Causal component analysis</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wendong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating latent variable causal graphs</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14891" to="14902" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Identification of linear non-gaussian latent hierarchical structure</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Danru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingling</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><surname>Magliacane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08335</idno>
		<title level="m">A sparsity principle for partially observable causal representation learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Causalvae: Disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9593" to="9602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Multi-view causal representation learning with partial observability</title>
		<author>
			<persName><forename type="first">Dingling</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04056</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Identifiability guarantees for causal disentanglement from soft interventions</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Meng Fang, and Mykola Pechenizkiy. Interpretable reward redistribution in reinforcement learning: A causal approach</title>
		<author>
			<persName><forename type="first">Yudi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sample complexity bounds for scorematching: Causal discovery and generative modeling</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
