<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time-Domain Audio Source Separation Based on Gaussian Processes with Deep Kernel Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aditya</forename><forename type="middle">Arie</forename><surname>Nugraha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><forename type="middle">Di</forename><surname>Carlo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshiaki</forename><surname>Bando</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology (AIST)</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Fontaine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>Télécom Paris</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazuyoshi</forename><surname>Yoshii</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Advanced Intelligence Project (AIP)</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazuyoshi</forename><forename type="middle">Yoshii</forename><surname>Time</surname></persName>
						</author>
						<title level="a" type="main">Time-Domain Audio Source Separation Based on Gaussian Processes with Deep Kernel Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Submitted on 28 Jul 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time-domain audio source separation</term>
					<term>Gaussian processes</term>
					<term>deep kernel learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio source separation extracts audio signals of interest or removes unwanted signals from recordings <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. It is invaluable in many applications, including speech enhancement <ref type="bibr" target="#b3">[3]</ref>, automatic speech recognition <ref type="bibr" target="#b4">[4]</ref>, music separation <ref type="bibr" target="#b5">[5]</ref>, and sound event detection <ref type="bibr">[6]</ref>.</p><p>One of the most modern approaches to single-channel audio source separation is to use a deep neural network (DNN) that works in the time domain, e.g., TasNet <ref type="bibr" target="#b7">[7]</ref>, Conv-TasNet <ref type="bibr" target="#b8">[8]</ref>, SuDoRM-RF <ref type="bibr" target="#b9">[9]</ref>, SuDoRM-RF++ <ref type="bibr" target="#b10">[10]</ref>, SepFormer <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, MossFormer <ref type="bibr" target="#b13">[13]</ref>, and WaveFormer <ref type="bibr" target="#b14">[14]</ref>. Most such time-domain separation methods are based on the encoder-separator-decoder architecture that performs mask-based source separation in a pseudo-time-frequency (pseudo-TF) space. More specifically, the encoder transforms a timedomain mixture signal into a spectrogram-like mixture representa-This work was supported by ANR Project SAROUMANE (ANR-22-CE23-0011), JST PRESTO no. JPMJPR20CB, and JSPS KAKENHI nos. 20H00602, 21H03572, 23K16912, and 23K16913.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture Source latent variable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gram matrix construction</head><p>Source covariance mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiener filtering</head><p>Source estimates tion, the separator estimates the spectrogram-like source representations, and the decoder recovers the time-domain source signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference model</head><note type="other">Encoder Separator Decoder</note><p>All the networks are trained jointly in a fully data-driven manner, encouraging the learned pseudo-TF space to be optimal for source separation. To further improve the performance, various architectures for the separator <ref type="bibr">[7-11, 13, 14]</ref> and various filterbanks for the encoder and decoder <ref type="bibr" target="#b15">[15]</ref> have been investigated.</p><p>In this paper, we revisit the classical approach to audio source separation based on the Gaussian process (GP) <ref type="bibr" target="#b16">[16]</ref>. If source signals follow independent GPs meaning that any finite set of sampled points is normally distributed, the mixture signal given as the sum of these signals follows a GP whose covariance function is given as the sum of the source covariance functions. Source separation is then recast as posterior inference of the latent sources from the observed mixture with Wiener filtering <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. Although this model is strictly formulated in the continuous time domain, most conventional methods perform Wiener filtering frame-wise, assuming the local stationarity and the inter-frame independence <ref type="bibr" target="#b19">[19]</ref>. Specifically, the magnitude spectrogram of the mixture signal is decomposed while keeping the phase spectrogram untouched <ref type="bibr" target="#b1">[1]</ref>. The performance of this strategy, however, is affected by the time and frequency resolutions. In addition, the possibly incompatible phase information causes unpleasant artifacts in reconstructed time-domain signals. To the best of our knowledge, a method based on variational sparse GPs <ref type="bibr" target="#b20">[20]</ref> is the only method that operates entirely in the time domain.</p><p>Recent studies on GPs have focused on integrating deep learning methods into the probabilistic framework <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>. One can use the covariance function of a GP for estimating the uncertainty to improve the robustness and interpretability, as in Bayesian neural networks <ref type="bibr" target="#b23">[23]</ref>. Alternatively, one can parameterize the covariance function of a GP with a DNN in the framework of deep kernel learning (DKL) <ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref>. For example, a DNN was used for learning a lowdimensional representation of the input on which the kernel function works as a similarity measure <ref type="bibr" target="#b24">[24]</ref>. Despite the theoretical support, these models tend to be hard to train in practice <ref type="bibr" target="#b22">[22]</ref>. To mitigate this difficulty, heuristics (e.g., pre-training) need to be considered <ref type="bibr" target="#b24">[24]</ref>.</p><p>©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p><p>In this paper, we propose a GP-based audio source separation method that uses a time-domain Wiener filter parameterized by a DNN for inferring source signals from a mixture signal at once (Figure <ref type="figure" target="#fig_0">1</ref>). We assume that the non-stationarity of each source signal is governed by a time-domain sequence of latent variables. The Gram matrix of the source GP over all sampled points is computed with a kernel function over the latent variables, which are estimated from the mixture signal with an encoder-separator-decoder network, e.g., Conv-TasNet <ref type="bibr" target="#b8">[8]</ref> and SepFormer <ref type="bibr" target="#b11">[11]</ref>. The kernel function (GP-based generative model) and the network (DKL-based inference model) are trained jointly in the maximum likelihood framework. This is the first attempt to combine DKL with full-covariance Wiener filtering for GP-based source separation. We show that the proposed approach achieved good speech separation performances under clean, noisy, and noisy-reverberant conditions with the popular WSJ0-2mix <ref type="bibr" target="#b27">[27]</ref>, WHAM! <ref type="bibr" target="#b28">[28]</ref>, and WHAMR! <ref type="bibr" target="#b29">[29]</ref> benchmark datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Let x ≜ [x1, . . . , xT ] T ∈ R T be a series of T sampled points from a mixture signal and s k ≜ [s k1 , . . . , s kT ] T ∈ R T be that from the signal of source k ∈ {1, . . . , K}, where K is the number of sources. The goal of source separation is to recover source signals {s k } K k=1 given the observed mixture signal x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Neural time-domain audio source separation</head><p>Time-domain audio source separation methods are capable of estimating the source signal s k given the mixture signal x. The main advantage of this approach is that it can circumvent the phase estimation, which has still been a challenging problem for separation methods that estimate masks for magnitude spectrograms or phaseaware masks for complex-valued spectrograms <ref type="bibr" target="#b1">[1]</ref>.</p><p>For this purpose, deep learning techniques have been used. In the encoder-separator-decoder approach <ref type="bibr">[7-11, 13, 14]</ref>, the encoder transforms x into a pseudo-TF representation from which source representations are extracted by the separator and transformed to s k using the decoder. The separation can be generally expressed as</p><formula xml:id="formula_0">{ŝ k } K k=1 ← Dec (Sep (Enc (x))) .<label>(1)</label></formula><p>Strictly speaking, the separation is not done in the time domain but in a learned pseudo-TF space that encapsulates the signal features with phase information and is optimized for the training data. The optimizable filterbanks <ref type="bibr" target="#b15">[15]</ref> used in the encoder and decoder are trained jointly with the separator that incorporates a powerful DNN, e.g., the long short-term memory network <ref type="bibr" target="#b7">[7]</ref>, the temporal convolutional network <ref type="bibr" target="#b8">[8]</ref>, the multi-resolution convolutional network <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>, and the transformer network <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gaussian process regression with deep kernel learning</head><p>Let f be a continuous function over a space Y. If f follows a Gaussian process (GP) <ref type="bibr" target="#b16">[16]</ref>, any finite set of T sampled points denoted by f ≜ {f (yt)} T t=1 given Y ≜ {yt ∈ Y} T t=1 follows a multivariate Gaussian distribution. Its probability density function is given by pΘ (f |Y) = N (µ, VΘ(Y)), where µ ∈ R T is a mean vector and VΘ(Y) ∈ R T ×T is a covariance matrix whose elements are computed with a kernel function kΘ(•, •) parameterized by Θ as</p><formula xml:id="formula_1">{VΘ(Y)} tt ′ = kΘ(yt, y t ′ ),<label>(2)</label></formula><p>where, in this case, t, t ′ ∈ {1, . . . , T } serve as the row and column indices, respectively. Given another set of T * sampled points denoted by Y * , GP regression <ref type="bibr" target="#b16">[16]</ref> estimates the corresponding f * via the posterior distribution of f * given f , Y, and Y * .</p><p>Deep kernel learning (DKL) <ref type="bibr" target="#b24">[24]</ref> exploits the expressive power of DNNs in constructing {VΘ(Y)} tt ′ via a non-linear mapping gΦ(•), where Φ denotes DNN parameters, given a kernel kΘ(•, •) as {VΘ(Y)} tt ′ = kΘ(gΦ(yt), gΦ(y t ′ )).</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>This section describes the proposed method based on a latent variable model (Figure <ref type="figure" target="#fig_0">1</ref>). It performs source separation in a pseudo-TF domain (latent variable estimation) with an inference model, computes the Gram matrices of source GPs, and finally estimates source signals with a Wiener filter. Let Z k = [z k1 , . . . , z kT ] T ∈ R T ×Q be a series of latent variables of source k ∈ {1, . . . , K}, where z kt ∈ R Q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Source separation based on Gaussian processes</head><p>We assume that each source signal is drawn from a GP in the continuous time domain and thus any finite set of T sampled points, s k , follows a real multivariate Gaussian distribution with a zero-mean vector and a covariance matrix governed by a source-specific kernel function VΘ k with parameters Θ k as follows:</p><formula xml:id="formula_2">pΘ k s k Z k = N s k 0, VΘ k (Z k ) .<label>(4)</label></formula><p>We also consider the presence of white noise. Let ϵ ∈ R T be a series of T sampled points from white noise, which is expressed as follows:</p><formula xml:id="formula_3">p(ϵ) = N ϵ 0, λI ,<label>(5)</label></formula><p>where λ ∈ R+ is the noise variance (regularization parameter).</p><p>Assuming the signal additivity, the mixture x = K k=1 s k + ϵ can be said to follow a multivariate Gaussian distribution as follows:</p><formula xml:id="formula_4">p Θ,λ x Z = N x 0, VΘ(Z) + λI ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_5">VΘ(Z) ≜ K k=1 VΘ k (Z k ) with Θ ≜ {Θ k } K k=1 and Z ≜ {Z k } K k=1 .</formula><p>The posterior distribution of s k given Z and x is given by p</p><formula xml:id="formula_6">Θ,λ s k Z, x = N s k µ s k , Σ s k ,<label>(7)</label></formula><formula xml:id="formula_7">µ s k = VΘ k (Z k ) (VΘ(Z) + λI) -1 x,<label>(8)</label></formula><formula xml:id="formula_8">Σ s k = VΘ k (Z k ) -VΘ k (Z k ) (VΘ(Z) + λI) -1 VΘ k (Z k ),<label>(9)</label></formula><p>where the posterior mean is considered as the estimated source, i.e., ŝk ≜ µ s k . Equation ( <ref type="formula" target="#formula_7">8</ref>) is also known as Wiener filtering whose filter VΘ k (Z k ) (VΘ(Z) + λI) -1 requires estimation of source latent variables Z to obtain the covariance matrices {VΘ k (Z k )} K k=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Estimation of latent variables and covariance matrices</head><p>We estimate Z given x using a DNN-based inference model with an encoder-separator-decoder architecture. While the decoder was originally used for estimating time-domain source signals from the pseudo-TF source representations obtained by the separator (Section 2.1), our decoder estimates the time-domain latent variables Z from the frame-varying pseudo-TF representations as follows:</p><formula xml:id="formula_9">Z ← InferΦ (x) = Dec ϕ D Sep ϕ S Enc ϕ E (x) ,<label>(10)</label></formula><p>where Φ ≜ {ϕ D , ϕ S , ϕ E } denotes the inference model parameters that gather the encoder parameters ϕ E , the separator parameters ϕ S , and the source-specific decoder parameters ϕ D ≜ {ϕ D k } K k=1 . The decoder has a 1-dimensional convolutional layer as in most vanilla decoders except that it returns a matrix of R T ×Q , not a vector of R T .</p><p>We then compute a covariance matrix VΘ k (Z k ) using a nonstationary, non-degenerate composite kernel resulting from multiplying the linear kernel and the squared-exponential kernel <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_10">VΘ k (Z k ) tt ′ = ω k0 δ tt ′ + ω k1 z T kt z kt ′ e -θ k ∥zkt-z kt ′ ∥ 2 ,<label>(11)</label></formula><p>where Θ k ≜ {ω k0 , ω k1 , θ k } ∈ R 3 + gathers the kernel parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter optimization</head><p>The inference model Φ, the kernel parameters Θ, and the noise parameter λ are optimized, in principle, such that the log-likelihood of the sources given the mixture and latent variables, ln p Θ,λ (s|Z, x), is maximized. This is performed by gradient descent with backpropagation using permutation-invariant training (PIT) <ref type="bibr" target="#b30">[30]</ref>. For better optimization, we opt for a two-stage training approach.</p><p>In the first stage, we train a vanilla encoder-separator-decoder model, e.g., Conv-TasNet <ref type="bibr" target="#b8">[8]</ref>, by minimizing the negative scaleinvariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b31">[31]</ref> given by</p><formula xml:id="formula_11">L SI-SDR ≜ - K k=1 10 log 10 α 2 s T k s k (αs k -µ s k ) T (αs k -µ s k ) ,<label>(12)</label></formula><p>where, in this stage, the estimated sources {µ s k } K k=1 are the output of the vanilla decoder and α = (</p><formula xml:id="formula_12">µ s k ) T s k s T k s k -1</formula><p>is the optimal scaling factor that minimizes (αs k -µ s k ) T (αs k -µ s k ). In the second stage, we construct an inference model by combining the encoder and separator of the pre-trained vanilla model with our decoder. Although fine-tuning the parameters ϕ E and ϕ S may provide some improvement, it is not easy to effectively configure the fine-tuning procedure. As a preliminary attempt, we opt to freeze ϕ E and ϕ S in this paper and only optimize ϕ D together with Θ and λ by minimizing the negative log-likelihood (NLL) given by</p><formula xml:id="formula_13">L NLL ≜ -ln p Θ,λ (s|Z, x) = - K k=1 ln p Θ,λ (s k |Z, x) = 1 2 K k=1 L Σ k -1 (s k -µ s k ) T L Σ k -1 (s k -µ s k ) + K k=1 T t=1 ln L Σ k tt + KT 2 ln 2π,<label>(13)</label></formula><p>where L Σ k is obtained by the Cholesky decomposition for regularized</p><formula xml:id="formula_14">Σ s k , i.e., Σ s k + ε L Tr Σ s k I = L Σ k L Σ k</formula><p>T with ε working to ensure the positive definiteness of the left-hand side. In this paper, we set the default value to ε = 10 -6 , but when it is needed, we let it be larger to allow a successful decomposition of a particular Σ s k . Instead of L NLL , L SI-SDR can also be used in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dimensionality reduction</head><p>Our model performs source separation based on the covariance matrix VΘ k (Z k ) ∈ R T ×T , which is computationally prohibitive in practice. For example, a 1-s signal sampled at 8 kHz has a covariance matrix of size (8000 × 8000). To deal with it, we perform partitioning during both the training phase and the test phase.</p><p>In the training phase, the latent variables</p><formula xml:id="formula_15">{Z k ∈ R T ×Q } K k=1 are estimated once given x ∈ R T . Each source latent variable Z k is partitioned into non-overlapping L segments {Z kl ∈ R T ′ ×Q } L l=1</formula><p>, where T ′ ≪ T and l is the segment index. In this paper, we set T ′ = 1600 (200 ms at 8 kHz). The covariance matrix construction and source separation are then performed given these segmented latent variables. L NLL is computed segment-wise, whereas L SI-SDR is calculated after concatenating the separated source segments.</p><p>In the test phase, the covariance matrix construction and source separation are performed given possibly overlapping latent variable segments. The final separated sources are obtained by simple concatenation of separated source segments (for non-overlapping seg-ments) or by the overlap-add (OLA) technique <ref type="bibr" target="#b32">[32]</ref> with a Hann weighting window (for overlapping segments). In this paper, we use segments with an overlap of size T ′ /2 = 800 (i.e., 100 ms at 8 kHz).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>We considered single-channel separations of two speech signals from mixtures under clean (ideal), noisy, and noisy-reverberant conditions. We used the 'min' variants of the WSJ0-2mix dataset <ref type="bibr" target="#b27">[27]</ref> for the clean condition, the WHAM! dataset <ref type="bibr" target="#b28">[28]</ref> for noisy conditions, and the WHAMR! dataset <ref type="bibr" target="#b29">[29]</ref> for noisy-reverberant conditions. Each dataset has training, validation, and test sets of 20 000, 5000, and 3000 mixtures, respectively. All data are sampled at 8 kHz.</p><p>For clean speech mixtures, we set all separation models to output the two speech signals (K = 2). For noisy or noisy-reverberant mixtures, all models were set to additionally output the residual signal, corresponding to the noise component or the noise-and-reverberation component, respectively (K = 3). Nonetheless, the performance assessment took into account only the estimated speech signals.</p><p>Speech separation performance was assessed in terms of improvements of the SI-SDR (SI-SDRi), the perceptual evaluation of speech quality (PESQi), and the short-time objective intelligibility (STOIi) <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>. A permutation solver that maximizes SI-SDR (as in PIT <ref type="bibr" target="#b30">[30]</ref>) was used to decide the best source ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Configurations</head><p>We considered a non-causal Conv-TasNet <ref type="bibr" target="#b8">[8]</ref> based on the Asteroid library <ref type="bibr" target="#b35">[35]</ref> or a non-causal SepFormer <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> based on the Speech-Brain library <ref type="bibr" target="#b36">[36]</ref> as the baseline model. <ref type="foot" target="#foot_0">1</ref> We trained a baseline model with a batch of 16 (Conv-TasNet) or 4 (SepFormer) 4-s segments (T = 32000) for 300 epochs on the original training dataset (without data augmentation as in, e.g., <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>). The learning rate of the Adam optimizer <ref type="bibr" target="#b37">[37]</ref> was initially set to 10 -3 (Conv-TasNet) or 10 -4 (SepFormer) and halved when the validation loss did not improve after 5 consecutive epochs. A norm-based gradient clipping <ref type="bibr" target="#b38">[38]</ref> with a threshold of 5 was applied.</p><p>Our GP-based model, Conv-TasNet+GP or SepFormer+GP, was built utilizing the parameters of encoder ϕ E and separator ϕ S of Conv-TasNet or SepFormer, respectively, trained for 200 epochs, at which the SI-SDRi scores have stopped improving, and were virtually the same after 300 epochs (cf. Table <ref type="table">1</ref>). We substituted the original decoder with our decoder that outputs 8-dimensional source latent variables (Q = 8). The parameters of our decoder ϕ D were initialized using random semi-orthogonal matrices <ref type="bibr" target="#b39">[39]</ref>, while the other parameters were initialized as ω k0 = 10 -2 , ω k1 ← N (1, 10 -4 ), θ k ← N (1, 10 -4 ), and λ = 10 -2 . The training configuration of a GPbased model was the same as that of the vanilla model, except that it was trained with a batch of 16 4-s segments for 100 epochs. If we take into account the pre-training of the vanilla model, the GP-based model was trained for 300 epochs in total. Based on a grid-searchbased hyperparameter tuning, we found that the generally-optimal initial learning rate for L NLL was 2×10 -5 , while that for L SI-SDR was 5×10 -5 (ConvTasNet+GP) or 5×10 -6 (SepFormer+GP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and discussion</head><p>Table <ref type="table">1</ref> compares the performance of different models in terms of SI-SDRi, PESQi, and STOIi scores. These scores demonstrate that Table <ref type="table">1</ref>: Average speech separation performance scores of the different models on the test set. Higher is better for all metrics. OLA denotes the overlap-add operation required for separation with overlapping segments. Boldface numbers show the top performances taking into account the 95% confidence interval over the best performances (indicated by ⋆ ) in each group separated based on the baseline model (shown in italics).  the proposed GP-based models outperformed the vanilla models, especially under the more challenging noisy (WHAM!) and noisyreverberant (WHAMR!) conditions. 2 Although the SI-SDRi scores of SepFormer+GPs are not significantly different from those of Sep-Former under these two conditions, the PESQi and STOIi scores imply that the pure time-domain modeling in the GP-based models, including SepFormer+GPs, improves perceptual quality, likely due to better phase consistency. These results also suggest that separation could benefit from the estimation of the residual covariance structure during the Wiener filter computation. Separation with overlapping segments (denoted by OLA) is shown to be useful probably by eliminating the boundary effect that causes the discontinued waveform in separation with non-overlapping segments. The GP-based models trained with L SI-SDR generally performed better speech separation 2 Our baseline performance is generally higher than that in the literature. The reported SI-SDRi scores on WSJ0-2mix, WHAM!, and WHAMR! for Conv-TasNet are 15.3 dB, 12.7 dB, and 8.3 dB <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b13">13]</ref>, while those for SepFormer are 20.4 dB, 14.7 dB, and 11.4 dB <ref type="bibr" target="#b12">[12]</ref>. This could be attributed to differences in the details of the network and training configurations. than those trained with L NLL . It may indicate that constraining the covariance matrices as in L NLL makes optimization more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Figure <ref type="figure" target="#fig_1">2</ref> provides insight using examples of source latent variables, covariance matrices, and signals estimated using Sep-Former+GPs. The estimated signals and the SI-SDR scores (shown within the figures) look similar. Although we may notice differences in the details, the estimated covariance matrices also show a similar pattern reflecting the temporal structure of the time domain signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper proposes a novel time-domain audio source separation based on Gaussian processes with deep kernel learning that effectively combines the expressive power of a DNN with the rigorous full-covariance Wiener filtering. With comparable numbers of parameters, the proposed GP-based models outperformed the corresponding vanilla models in speech separations under challenging noisy and noisy-reverberant conditions. Future work includes extensive ablation studies to investigate the effective training procedures and interpreting our approach within the variational framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed Gaussian process-based single-channel audio source separation method based on time-domain Wiener filtering with deep kernel learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Separation examples of a mixture segment under a noisy-reverberant condition (of WHAMR! dataset) using SepFormer+GPs. For each subfigure, the columns from left to right show the first speech signal, the second speech signal, and the residual. The rows from top to bottom show latent variable estimates (Q = 8), covariance matrix estimates, time-domain waveform estimates, and time-domain waveform targets.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The total number of parameters for WSJ0-2mix (K =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2) was 5.05 M (Conv-TasNet), 5.17 M (Conv-TasNet+GP), 25.68 M (SepFormer), or 25.81 M (SepFormer+GP), whereas the total number of parameters for WHAM! or WHAMR! (K = 3) was 5.12 M (Conv-TasNet), 5.31 M (Conv-TasNet+GP), 25.75 M (SepFormer), or 25.94 M (SepFormer+GP).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>October 22-25, 2023, New Paltz, NY</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Audio Source Separation and Speech Enhancement</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<title level="m">Audio Source Separation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ICASSP 2022 Deep Noise Suppression Challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9271" to="9275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CHiME-6 Challenge: Tackling multispeaker speech recognition for unsegmented recordings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Music Demixing Challenge 2021</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">808395</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sound event detection and separation: A benchmark on DESED synthetic soundscapes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="840" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TasNet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SuDoRM-RF: Efficient networks for universal audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE MLSP</title>
		<meeting>IEEE MLSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compute and memory efficient universal sound source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Signal Process. Syst</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="259" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring self-attention mechanisms for speech separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2169" to="2180" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MossFormer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time target sound extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Veluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6364" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio source separation with a single sensor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="199" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaussian processes for underdetermined source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3155" to="3167" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic modeling paradigms for audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Audition: Principles, Algorithms and Systems</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse Gaussian process audio source separation using spectrum priors in the time-domain</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="995" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bridging deep and multiple kernel learning: A review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The promises and pitfalls of deep kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Ober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI, 2021</title>
		<meeting>UAI, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="1206" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair</title>
		<author>
			<persName><forename type="first">E</forename><surname>Goan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
			<biblScope unit="page" from="45" to="87" />
		</imprint>
	</monogr>
	<note>Bayesian neural networks: An introduction and survey</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Manifold Gaussian processes for regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3338" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial examples, uncertainty, and transfer testing robustness in Gaussian process hybrid deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02476v1</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WHAM!: Extending speech separation to noisy environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multitalker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SDR -half-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Signal estimation from modified shorttime fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ): A new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asteroid: The PyTorch-based audio source separation toolkit for researchers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2637" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">SpeechBrain: A general-purpose speech toolkit</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624v1</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
