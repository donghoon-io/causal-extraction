<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DOLDA: a regularized supervised topic model for high-dimensional multi-class regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-06-12">12 June 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">O</forename><surname>R I G I N A L P A P E R</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Måns</forename><surname>Magnusson</surname></persName>
							<email>mans.magnusson@liu.se</email>
							<idno type="ORCID">0000-0002-0296-2719</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<postCode>581 83</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<addrLine>Konemiehentie 2</addrLine>
									<postCode>02150</postCode>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leif</forename><surname>Jonsson</surname></persName>
							<email>leif.jonsson@ericsson.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Ericsson AB</orgName>
								<address>
									<postCode>164 80</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mattias</forename><surname>Villani</surname></persName>
							<email>mattias.villani@liu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<postCode>581 83</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<postCode>114 19</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DOLDA: a regularized supervised topic model for high-dimensional multi-class regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-12">12 June 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s00180-019-00891-1</idno>
					<note type="submission">Received: 13 June 2017 / Accepted: 10 April 2019 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text classification</term>
					<term>Latent Dirichlet Allocation</term>
					<term>Horseshoe prior</term>
					<term>Diagonal Orthant probit model</term>
					<term>Interpretable models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating user interpretable multi-class predictions in data-rich environments with many classes and explanatory covariates is a daunting task. We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised topic model for multiclass classification that can handle many classes as well as many covariates. To handle many classes we use the recently proposed Diagonal Orthant probit model (Johndrow et al., in: Proceedings of the sixteenth international conference on artificial intelligence and statistics, 2013) together with an efficient Horseshoe prior for variable selection/shrinkage (Carvalho et al. in Biometrika 97:465-480, 2010). We propose a computationally efficient parallel Gibbs sampler for the new model. An important advantage of DOLDA is that learned topics are directly connected to individual classes without the need for a reference class. We evaluate the model's predictive accuracy and scalability, and demonstrate DOLDA's advantage in interpreting the generated predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During recent decades more and more textual data has become available, creating a growing need to statistically analyze large amounts of textual data. The popular Latent Dirichlet Allocation (LDA) model introduced by <ref type="bibr" target="#b2">Blei et al. (2003)</ref> is a generative probabilistic model in which each document is summarized by a set of latent semantic themes, often called topics. Formally, a topic is a probability distribution over the vocabulary. An estimated LDA model is, therefore, a compressed latent representation of the documents where each document is a mixture of topics and where each word (token) in a document belongs to a single topic. Most probabilistic topic models, such as LDA, are unsupervised, i.e. the topics are learned solely from the words in the documents without access to other document meta-data.</p><p>In many situations, though, there is other information we would like to incorporate in modeling a corpus of documents. A common example is when we have labeled documents, such as ratings of movies together with a movie description, illness categories in medical journals, or the locations of identified bugs together with bug reports in software engineering applications. The simplest approach would be to use a standard topic model and then use the estimated topic distributions per document in another model, such as a logistic regression model. This two-step approach would result in topics that are not produced for the purpose of explaining the dependent variable of interest. Alternatively, one could use a supervised topic model to find the semantic topic structure in the documents that are related to the class of interest. The difference between a supervised topic model and a two-step approach is similar to the difference between principal component regression (PCR) and partial least squares (PLS). In PCR, the principal components are first computed and then a regression model is estimated based on the estimated components. In PLS the components are estimated together with the regression model with the purpose of estimating components that have good predictive performance for the dependent variable of interest <ref type="bibr" target="#b8">(Geladi and Kowalski 1986)</ref>.</p><p>Most of the proposed supervised topic models have been designed with the objective of by trying to find good text classification models, and the focus has naturally been on the predictive performance. However, the predictive performance of most supervised topic models is similar to that of using a Support Vector Machine (SVM) with covariates based on word frequencies <ref type="bibr" target="#b11">(Jameel et al. 2015)</ref>. While predictive performance is certainly important, the real attraction of supervised topic models comes from their ability to learn semantically relevant topics and to use those topics to produce accurate interpretable predictions of documents or other textual data. The interpretability of a model is an often-neglected feature, but it is crucial in real-world applications. As an example, <ref type="bibr" target="#b23">Parnin and Orso (2011)</ref> show that bug fault localization systems are quickly disregarded when the users cannot understand how the system has reached its predictive conclusion. Compared to other text classification systems, topic models are very well suited for interpretable predictions since topics are abstract entities that humans can easily grasp. The problems of interpretability in multi-class supervised topic models can be divided into three main areas.</p><p>First, most supervised topic models use a logit or probit approach, where the model is identified by the use of a reference category to which the effect of any covariate is compared. This defeats one of the main purposes of supervised topic models since it complicates the interpretability of the models. Instead of interpreting the effect of a topic on a class, we need to interpret it as the effect on a class compared to the reference category.</p><p>Second, to handle multi-class categorization a topic should be able to affect multiple classes, and some topics may not influence any class at all. In most supervised topic modeling approaches (such as <ref type="bibr" target="#b12">Jiang et al. 2012;</ref><ref type="bibr" target="#b33">Zhu et al. 2013;</ref><ref type="bibr" target="#b11">Jameel et al. 2015)</ref> the multi-class problem is solved using binary classifiers in a "one-vs-all" classification approach. This approach works well in the situation of evenly distributed classes, but may not work well for skewed class distributions <ref type="bibr" target="#b27">(Rubin et al. 2012)</ref>. A one-vs-all approach also makes it more difficult to interpret the model. Estimating one model per class makes it impossible to see which classes are affected by the same topic and which topics do not predict any label. In these situations, we would like to have one topic model to interpret. The approach of one-vs-all is also costly from an estimation point of view since we need to estimate one model per class <ref type="bibr" target="#b31">(Zheng et al. 2015)</ref>, something that can be difficult in a situation with hundreds of classes.</p><p>Third, there can be situations with hundreds of classes and hundreds of topics [see <ref type="bibr" target="#b14">Jonsson et al. (2016)</ref> for an example]. Without regularization or variable selection we would end up with a model with too many parameters to interpret and uncertain parameter estimates. In a good predictive supervised topic model, one would like to find a small set of topics that are strong determinants of a single document class label. This is especially relevant when the numbers of observations in different classes are skewed, which is a common problem in real-world situations <ref type="bibr" target="#b27">(Rubin et al. 2012</ref>). In the more rare classes, we would like to induce more shrinkage compared to more common classes.</p><p>Multi-class regression is a non-trivial problem in Bayesian modeling. Historically, the multinomial probit model has been preferred due to the data augmentation approach proposed by <ref type="bibr" target="#b1">Albert and Chib (1993)</ref>. Augmenting the sampler using latent variables lead to straightforward Gibbs sampling with conditionally conjugate updates of the regression coefficients. The Albert-Chib sampler often tend to mix slowly, and the same holds for improved samplers such as the parameter expansion approach in <ref type="bibr" target="#b10">Imai and van Dyk (2005)</ref>. Recently, <ref type="bibr" target="#b25">Polson et al. (2013)</ref> have proposed a similar data augmentation approach using Polya-gamma variables for the Bayesian logistic regression model. This approach preserves conditional conjugacy in the case of a Normal prior for the regression coefficients and was the foundation for the supervised topic model in <ref type="bibr" target="#b33">Zhu et al. (2013)</ref>.</p><p>In addition to the issue of interpretability of models, scalability of topic models is of crucial importance. MCMC algorithms are generally considered to be computationally costly. In the case of probabilistic topic models, this is even more prominent, since we often sample at least one parameter per word for the whole corpus. Modern corpora can be very large, with millions of documents, making efficient and parallel sampling a crucial component.</p><p>In this paper we explore a new approach to supervised topic models that produces accurate multi-class predictions from semantically interpretable topics using a fully Bayesian approach, hence solving all three of the above-mentioned problems. The model combines LDA with the recently proposed Diagonal Orthant (DO) probit 123 model <ref type="bibr" target="#b13">(Johndrow et al. 2013)</ref> for multi-class classification, with an efficient Horseshoe prior that achieves sparsity and interpretation by aggressive shrinkage <ref type="bibr" target="#b3">(Carvalho et al. 2010)</ref>. The new Diagonal Orthant Latent Dirichlet Allocation (DOLDA)<ref type="foot" target="#foot_0">foot_0</ref> model has been demonstrated to have competitive predictive performance, while still producing interpretable multi-class predictions from semantically relevant topics. In addition, we also derive an efficient and parallel MCMC sampler that can be used to scale up model inference to larger corpora.</p><p>The paper is organized as follows. In Sect. 2, we describe the new proposed model and in Sect. 3 we then describe the proposed scalable MCMC sampler for the proposed model. In Sect. 4, the experimental results are presented and in Sect. 5 we conclude the paper. In the "Appendix", a full derivation of the sampler is supplied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Incorporating supervised information or meta-data in the estimation of topic models has been done in a large number of papers, such as Rosen-Zvi et al. ( <ref type="formula">2004</ref>), <ref type="bibr" target="#b9">Griffiths et al. (2005)</ref> and <ref type="bibr" target="#b6">Chemudugunta et al. (2007)</ref> that incorporate author information, syntax and background structures, to give a few early examples. How topic models incorporate the labeled information, such as classes, can broadly be classified into two groups, downstream supervised models and upstream supervised models. In upstream topic models, loosely defined, the topics are conditioned on the labeled information, while in downstream topic models, the labeled information is conditioned on the topics. Examples of upstream topic models are topics conditioned on authorship <ref type="bibr" target="#b26">(Rosen-Zvi et al. 2004</ref>), topical perspectives <ref type="bibr" target="#b0">(Ahmed and Xing 2010</ref>) and more general supervised information <ref type="bibr" target="#b18">(Mimno and McCallum 2012)</ref>.</p><p>In downstream topic models, of which our proposed model is an example, the label information is instead conditioned on the topics, similar to conditioning on covariates in a linear regression model or a logistic regression model. One of the first approaches was proposed by <ref type="bibr" target="#b17">McAuliffe and Blei (2008)</ref> were the authors propose a supervised topic model based on the generalized linear model framework, thereby making it possible to link binary, count, and continuous response variables to topics that are inferred jointly with the regression/classification effects. This idea has been elaborated further, especially in the case of classification, in a series of paper, all closely connected to this work. The three most related approaches are <ref type="bibr" target="#b12">Jiang et al. (2012)</ref>, that propose a downstream supervised topic model using a max-margin classification, <ref type="bibr" target="#b33">Zhu et al. (2013)</ref> that propose a logistic supervised topic model using data augmentation with Polya-gamma variates and <ref type="bibr" target="#b24">Perotte et al. (2011)</ref> that use a hierarchical binary probit approach to model a hierarchical label structure in the form of a binary tree. All of these models are downstream supervised topic models using MCMC for inference and different forms of data augmentation approaches to model classes. <ref type="bibr" target="#b33">Zhu et al. (2013)</ref> and <ref type="bibr" target="#b24">Perotte et al. (2011)</ref> in combine a data augmentation strategy together with a linear model, just our proposed model. Compared to these earlier work we differ in two major ways. First, we use the diagonal orthant data augmentation scheme that is computationally attractive for many classes, essentially addressing the issue of scalability in the number of classes. In addition, unlike the work of <ref type="bibr" target="#b33">Zhu et al. (2013)</ref>, <ref type="bibr" target="#b24">Perotte et al. (2011)</ref> and <ref type="bibr" target="#b12">Jiang et al. (2012)</ref>, we focus on interpretability by using the horseshoe prior <ref type="bibr" target="#b3">(Carvalho et al. 2010)</ref>, something not done previously to ours. We also, just like <ref type="bibr" target="#b31">(Zheng et al. 2015)</ref>, focus on scalability in supervised topic models, but unlike <ref type="bibr" target="#b31">(Zheng et al. 2015)</ref>, we do not use Metropolis-Hastings to improve computational complexity. Instead, our approach focuses on the use of exchangeability to enable parallelism and analytically updates to make computations more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diagonal Orthant Latent Dirichlet Allocation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Handling the challenges of high-dimensional interpretable supervised topic models</head><p>To solve the first and second challenges identified in the Introduction, i.e., reference classes and multi-class models, we propose the use of the Diagonal Orthant (DO) probit model in <ref type="bibr" target="#b13">Johndrow et al. (2013)</ref> as an alternative to the multinomial probit and logit models. <ref type="bibr" target="#b13">Johndrow et al. (2013)</ref> propose a Gibbs sampler for the Diagonal Orthant model and show that it mixes well. One of the benefits of the DO model is that all classes can be independently modeled using binary probit models when conditioning on the latent variable, thereby removing the need for a reference class.</p><p>The parameters of the model can be interpreted as the effect of the covariate on the marginal probability of a specific class, which makes this model especially attractive when it comes to interpreting the inferred topics. This model also includes multiple classes in an efficient way that makes it possible to estimate a multi-class linear model in parallel over the classes.</p><p>The third problem of modeling supervised topic models is that the semantic meanings of all topics do not necessarily have an effect on our label of interest; one topic may have an effect on one or more classes, and some topics may just be noise that we do not want to use in the supervision. In cases where there are many topics and many classes, we will also have a very large number of parameters to analyze. The Horseshoe prior in <ref type="bibr" target="#b3">Carvalho et al. (2010)</ref> was specifically designed to filter out signals from massive noise. This prior uses a local-global shrinkage approach to shrink some (or most) coefficients to zero while allowing for sparse signals to be estimated without any shrinkage. This approach has shown good performance in linear regression-type situations <ref type="bibr" target="#b4">(Castillo et al. 2015)</ref>, with many predictors <ref type="bibr" target="#b21">(Nalenz and Villani 2018)</ref>, which makes it straightforward to incorporate other covariates into our model, something that is rarely done in the area of supervised topic models. Different global shrinkage parameters are used for the different classes to handle the problem with an unbalanced number of observations in different classes. This makes it possible to shrink more when there is less data for a given class and shrink less in classes with more observations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative model</head><p>The DOLDA generative model is described below. See also a graphical description of the model in Fig. <ref type="figure" target="#fig_0">1</ref>. A summary of the notation is given in Table <ref type="table" target="#tab_0">1</ref>.</p><p>(1) For each topic k = 1, . . . , K (a) Draw a distribution over words φ k ∼ Dir V (β)</p><p>(2) For each label l ∈ L (a) Draw a global shrinkage parameter τ l ∼ C + (0, 1) (b) For each covariate and topic p = 1, . . . , <ref type="bibr" target="#b13">(Johndrow et al. 2013)</ref>.  The matrix with word-topic probabilities :</p><formula xml:id="formula_0">K + P (i) Draw local shrinkage parameter λ l, p ∼ C + (0, 1) (ii) Draw coefficients 2 η l, p ∼ N (0, τ 2 l λ 2 l, p ) (3) For each observation/document d = 1, . . . , D (a) Draw topic proportions θ d ∼ Dir K (α) (b) For each token n = 1, . . . , N d (i) Draw topic assignment z n,d |θ d ∼ Categorical(θ d ) (ii) Draw word w n,d |z n,d , φ z n,d ∼ Categorical(φ z n,d ) (c) y d ∼ Categorical(p d ) where p d = L l cd f N ,l (z, x) d η l• -1 F N (z, x) d η 1• , . . . , F N (z, x) d η L• and F N (•) is the univariate normal CDF</formula><formula xml:id="formula_1">K × V X Covariate/feature matrix (including intercept): D × P φ k</formula><p>The word probabilities for topic k:</p><formula xml:id="formula_2">1 × V x d</formula><p>Covariate/features for document d 123 4 Inference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The MCMC algorithm</head><p>Markov Chain Monte Carlo (MCMC) is used to estimate the model parameters. We use different global shrinkage parameters τ l for each class, based on the fact that the different classes can have a different number of observations. This gives the following sampler for inference in DOLDA, see "Appendix A" for details.</p><p>(1) Sample the latent variables a d,l ∼ N + ((x z) T d η l , 1) for l = y d and a d,l ∼ N -((x z) T d η l , 1) for l = y d , where N + and N -are the positive and negative truncated normal distribution, truncated at 0.</p><p>(2) Sample all of the regression coefficients as in an ordinary Bayesian linear regression per class label l where η l ∼ MVN μ l , ((X Z) T (X Z) + τ 2 l l ) -1 and l is a diagonal matrix with the local shrinkage parameters λ i per parameter in η l and μ l = ((X z) T (X z) + τ 2 l l ) -1 (X z) T a l (3) Sample the global shrinkage parameters τ l at iteration j using the following two step slice sampling:</p><formula xml:id="formula_3">u ∼ U 0, 1 + 1 τ l,( j-1) -1 1 τ 2 l, j ∼ G ⎛ ⎝ ( p + 1)/2, 1 2 K +P p=1 η l, p λ l, p 2 ⎞ ⎠ I 1 τ 2 l,( j-1) &lt; (1 -u)/u</formula><p>where I indicates the truncation region for the truncated gamma. (4) Sample each local shrinkage parameter λ i,l at iteration j as</p><formula xml:id="formula_4">u ∼ U 0, 1 + 1 λ 2 p,l,( j-1) -1 1 λ 2 p,l, j ∼ Exp 1 2 η l, p τ l 2 I 1 λ 2 p,l,( j-1) &lt; (1 -u)/u</formula><p>where I indicates the truncation region for the truncated exponential distribution.</p><p>(5) Sample the topic indicators z</p><formula xml:id="formula_5">p(z i,d = k|w i , z ¬i , η, a) ∝ φ v,k • M ¬i d,k + α × exp - 1 2 L l -2 η l,k N d a d,l -(z ¬i d x d )η l + η l,k N d 2</formula><p>where M is a D × K count matrix containing the sufficient statistics for and M ¬i is this matrix with topic indicator z i,d removed from M.</p><p>(6) Sample the topic-vocabulary distributions</p><formula xml:id="formula_6">φ k ∼ Dir(β + N k,• )</formula><p>where N is a K × V count matrix containing the sufficient statistics for .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficient parallel sampling of z</head><p>To improve the speed of the sampler we cache the calculations done in the supervised part of the topic indicator sampler and parallelize the sampler. Very large text corpora are increasingly common, so efficient sampling of the z is absolutely crucial in practice. The basic sampler for z can be slow due to the serial nature of the collapsed sampler and the fact that the supervised part of p(z i,d ) involves a dot product. A naive implementation would result in a complexity of O((</p><formula xml:id="formula_7">K + P) • L • K ) to sample just one topic indicator z i .</formula><p>The supervised part of document d can be expressed as g ¬i d,k where</p><formula xml:id="formula_8">g ¬i d,k = exp - 1 2 L l -2 η l,k N d a d,l -(z ¬i d x d )η l + η l,k N d 2 .</formula><p>By realizing that sampling a topic indicator z i,d will only change this part a little, we can derive the relationship</p><formula xml:id="formula_9">g ¬i d,k = g ¬(i-1) d,k + 1 N 2 d L l η l,k η l,z i,d - L l η l,k η l,z i-1,d ,</formula><p>where g</p><formula xml:id="formula_10">¬(i-1) d,k</formula><p>is the supervised effect computed for the previous token and where the expression L l η l,k η l,z i,d can be calculated once per iteration in η and can be stored in a two-dimensional array of size K 2 . We can use the above relationship to update the supervision after sampling each topic indicator by calculating g ¬i d,k "on the fly" based on the previous supervised contribution. This means that we only need to compute g ¬i d,k once per document, and then we just need to update these values. This approach to computing g ¬i d,k increases the speed by an order of magnitude for a model with 100 topics and reduces the computational complexity of sampling one z i,d from O((K + P) • L • K ) to O(K ) for all but the first token per document. For details, see "Appendix B".</p><p>To further improve the performance we parallelize the sampler and use the fact that documents are conditionally independent given . By sampling instead of marginalizing it out we will gain from parallelization with the additional cost of sampling . This approach to parallelizing topic models give us a sampler that correctly samples the posterior using an ergodic Markov chain, unlike other parallel approaches such as AD-LDA <ref type="bibr" target="#b16">(Magnusson et al. 2018;</ref><ref type="bibr" target="#b22">Newman et al. 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>In summary, we propose a sampler that samples the z i,d in parallel over the documents, the elements in sampled in parallel over topics, and sampling η can be in parallel over classes. The code is publicly available at <ref type="url" target="https://github.com/lejon/DiagonalOrthantLDA">https://github.com/lejon/ DiagonalOrthantLDA</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational complexity</head><p>The computational complexity of the sampler depends on the different parameters sampled. Below we analyze the different parts of the sampler for one iteration. Sampling the regression coefficients has three parts, (1 Finally, the total complexity of the sampler, with regard to the number of classes (L), the number of topics (K ), the number of documents (D), the mean document size ( N ), and the number of covariates (P) is O(L</p><formula xml:id="formula_11">) computing post = ((X z) T (X z) + τ 2 l l ) is of complexity O(L • D • (K + P) 2 ), (2) inverting post for all classes is of com- plexity O(L • (K + P) 3 ),</formula><formula xml:id="formula_12">•(K + P) 3 + L • D •(K + P) 2 + K • D • N )</formula><p>where N = N /D. From this analysis, we can see that as the corpus grows (D → ∞) we see that sampling η and the topic indicators z will dominate the computations. But we would also expect the number of topics to grow as the number of documents grows. In this situation, the main cost of the algorithm would be sampling the ηs and the first topic indicator of each document.</p><p>Due to the similarity of the DOLDA sampler to that of the MedLDA sampler, it is straightforward to use the cyclical Metropolis-Hastings proposals in <ref type="bibr" target="#b31">Zheng et al. (2015)</ref> for inference in DOLDA. But, as shown in <ref type="bibr" target="#b16">Magnusson et al. (2018)</ref>, it is not obvious that the reduction in sampling complexity will result in a faster sampler when MCMC efficiency is taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of convergence and prediction</head><p>We evaluate the convergence of the MCMC algorithm by monitoring the unnormalized log-likelihood over the iterations:</p><formula xml:id="formula_13">log L(w, y|z, η, X, α, β) = log p(y|z, η, X) + log p(w|z, α, β) ∝ D d log ⎡ ⎣ (1 -F N (-(z d x d )η j )) l = j F N (-(z d x d )η l ) ⎤ ⎦ - D d log ⎡ ⎣ J j=1 (1 -F N (-(z d x d )η j )) l =s F N (-(z d x d )η l ) ⎤ ⎦ +K log V β -K V log (β) + K V log N k,v + β - K log V N k,v + β +D log K α -DK log (α) + D K log M d,k + α - D log K M d,k + α ,</formula><p>where F N is the univariate normal distribution and the last part is the same computations commonly used in evaluating the standard LDA model.</p><p>To make predictions for a new document d we first need to sample the topic indicators of the given document from</p><formula xml:id="formula_14">p(z i,d = k|w , ) ∝ φk,v • M ¬i d,k + α ,</formula><p>where φk,v is the mean of the last part of the posterior draws of . We use the posterior mean based on the last iterations instead of integrating out to avoid potential problems with label switching. However, we have not seen any indications of label switching after convergence in our experiment, probably because the data sets used for document predictions are usually quite large. The topic indicators are sampled for the predicted document using the fast PC-LDA sampler in <ref type="bibr" target="#b16">Magnusson et al. (2018)</ref>.</p><p>The mean of the sampled topic indicator vector for the predicted document, z , is then used for class predictions:</p><formula xml:id="formula_15">y = arg max (z , x ) η .</formula><p>This is a maximum a posteriori estimate, but it is straightforward to calculate the whole predictive distribution for the label. the scalability of the sampler. The experiments are performed on 2 sockets with 8-core Intel Xeon E5-2660 Sandy Bridge processors at 2.2GHz at the National Supercomputer Center (NSC) at Linköping University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Corpora and priors</head><p>To study the different aspects of the DOLDA model we use multiple corpora. We collected a corpus containing the 10,810 highest-rated movies at IMDb.com. We use both the textual description and information about producers and directors to classify a given movie into a genre. We also analyze the classical 20 Newsgroup corpus.</p><p>In addition, we also include two corpora based on the New York Times Annotated Corpus <ref type="bibr" target="#b28">(Sandhaus 2008)</ref> for our experiments. To label the documents we use the classification in the "online section". Thus, we only use articles from 2001 and later, when the "online section" was added. From these documents, we extract labels that we call "Top level" and "Hierarchical". These labels are used as the class of the documents. An example of an "online section" is Arts; Dining and Wine; Education; Books A document with the above example online-section would get the top label "Arts" and the hierarchical (2 level) label "Arts; Dining and Wine". For the hierarchical classification, we extracted only the articles which had at least two levels ("Arts" being the first level and "Dining and Wine" the second level in the example above). After extracting the classes for the documents we create four subsets from the corpora described above. These subsets contain 90%, 60%, 20%, and 10% of documents sampled from the above corpus. None of the documents in the 10% subset exists in the other subsets. The purpose of the NYT corpora is to show how the sampler scales, both with regard to documents (∼ 600 K) and with a large number of classes (240).</p><p>Our companion paper <ref type="bibr" target="#b14">(Jonsson et al. 2016</ref>) applies the DOLDA model developed here to bug localization in a large-scale software engineering context using a corpus with 15,000 bug reports, each belonging to one of 118 classes.</p><p>We include the corpora for different purposes. IMDb is a smaller corpus, but contains additional covariates. The 20 Newsgroups corpus is included to enable accuracy performance with other comparable supervised topic models. The New York Times corpus is included to show the scalability of the MCMC sampler with regard to the number of classes as well as the number of documents.</p><p>The corpora are tokenized and a standard stop list of English words are removed, as well as the rarest word types that make up 1% of the total tokens, or in some experiments, the words that occur less than 10 times. In the IMDb corpus, we only include genres with at least 10 movies (Table <ref type="table" target="#tab_1">2</ref>).</p><p>In all experiments, we use a relatively vague prior setting α = β = 0.01 for the LDA part of the model and c = 100 for the prior variance of the η coefficients in the normal model prior and for the intercept coefficient when using the Horseshoe prior. The accuracy experiment for IMDb uses 5-fold cross-validation and the 20 Newsgroups corpus uses the same training and test set as in <ref type="bibr" target="#b32">Zhu et al. (2012)</ref> to enable  direct comparisons of accuracy. In the interpretability analysis of the IMDb corpus we use the whole corpus, without cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Classification accuracy</head><p>Figure <ref type="figure" target="#fig_3">2</ref> shows the accuracy on the hold-out test set for the 20 Newsgroups corpus for different numbers of topics. The accuracy of our model is slightly lower than MedLDA and SVM using only textual features, but higher than both the classical supervised multi-class LDA and the ordinary LDA together with an SVM approach.</p><p>We can also see from Fig. <ref type="figure" target="#fig_3">2</ref> that the accuracy of using the DOLDA model with the topics jointly estimated with the supervision part outperforms a two-step approach of first estimating LDA and then using the DO probit model with the pre-estimated mean topic indicators as covariates. This is true for both the Horseshoe prior and the normal prior, but the difference with regard to accuracy is just a few percentage points.</p><p>The advantage of DOLDA is that it produces interpretable predictions with semantically relevant topics. Therefore, it is reassuring that DOLDA can compete in accuracy with other less interpretable models such as the SVM, even when the model is dramatically simplified by aggressive Horseshoe shrinkage for interpretational purposes. Our Fig. <ref type="figure">3</ref> Accuracy for DOLDA on the IMDb data with normal and Horseshoe prior and using a two step approach with the Horseshoe prior next analysis illustrates the interpretational strength of DOLDA. See also our companion paper <ref type="bibr" target="#b14">(Jonsson et al. 2016)</ref> in the software engineering literature for further demonstrations of DOLDA's ability to produce interpretable predictions in industrial applications.</p><p>Figure <ref type="figure">3</ref> displays the accuracy on the IMDb corpus as a function of the number of topics. The estimated DOLDA model also contains several other discrete covariates, such as the film's director and producer. The accuracy of the more aggressive Horseshoe prior is better than the normal prior for all topic sizes. A supervised approach with topics and supervision inferred jointly again outperforms a two-step approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Model interpretability</head><p>To illustrate the interpretability of DOLDA, we fit a new model to the IMDb corpus using only topics as covariates. Note first in Fig. <ref type="figure">4</ref> how the Horseshoe prior is able to distinguish between so-called signal topics and noise topics; the Horseshoe prior aggressively shrinks a large fraction of the regression coefficient toward zero, making it much easier to interpret how different latent aspects of the documents affect the class label. This is achieved without the need of setting any additional hyper-parameters in the model.</p><p>The Horseshoe shrinkage makes it easy to identify the topics that affect a given class. This is illustrated for the Romance genre in the IMDb corpus in Fig. <ref type="figure">5</ref>. This genre consists of relatively few observations (only 39 movies), and the Horseshoe prior, therefore, shrinks most coefficients to zero, keeping only one large signal topic that happens to have a negative effect on the Romance genre. The normal prior, on the other, hand gives a much denser, and therefore a much less interpretable solution.</p><p>For a further analysis of what triggers a Romance genre label, Table <ref type="table" target="#tab_2">3</ref> shows the 10 top words for Topic 39. From this table, it is clear that the signal topic identified using the Horseshoe prior is some sort of "crime" topic that is negatively related to the Romance genre, which makes intuitive sense. The crime topic is clearly expected to be positively related to the Crime genre, and Fig. <ref type="figure" target="#fig_5">6</ref> shows that this is indeed the case.  We can also see from Fig. <ref type="figure" target="#fig_5">6</ref> that Topic 33 has a strong negative effect on the Crime genre. In Table <ref type="table" target="#tab_2">3</ref> we can see that Topic 33 seems to be some sort of Sci-Fi topic. This topic has, in turn, the largest positive relationship with the Sci-Fi movie genre.</p><p>This illustrates an example how the aggressive shrinkage of the Horseshoe prior not only increases the prediction accuracy, but also simplifies interpretations since a much smaller number of topics is estimated to affect a given label -making it easier to focus on the topics that actually have an effect in the analysis. This is much more difficult in the Normal prior situation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Topic quality and effect of supervision</head><p>Even though the accuracy improves using a supervised approach, this raises the question of the effect of the supervision on the quality of the individual topics. How are the topics affected by the supervision and by shrinkage priors?</p><p>To study the effect on the topics, we focus on two measurements of topic quality. First, we study the effect of topic coherence using the measure proposed by <ref type="bibr" target="#b19">Mimno et al. (2011)</ref>. This measure has been shown in experiments to be a good approximation of topical coherence, estimated using manual annotations, such as topic intrusion <ref type="bibr" target="#b5">(Chang et al. 2009)</ref>.</p><p>We also study the document entropy of the topics. This measure gives us an indication of how the topics are distributed over documents. Are topics evenly distributed over documents (high entropy) or more sparsely distributed over documents (low entropy). This is an indication of the effect that supervision has on the topics. Is the supervised information making the distribution over documents more or less sparse?</p><p>In Fig. <ref type="figure" target="#fig_6">7</ref> we can see that, in general, there is no large difference in coherence or document entropy between the different models and priors, which is also true for the other corpora (not shown). This indicates that the effect of the supervision on the inferred topics is small; document entropy and topic coherence remains more or less the same with and without supervision.</p><p>To study the effect of the supervision in more detail, we focus instead on those topics that are actually affected by the supervision in the model. Since we have L number of coefficients that affect each topic, we choose to study the supervised effect on topic k, called r k , by looking at the sum of the absolute values of the η coefficients, i.e.</p><formula xml:id="formula_16">r k = L l=1 |η l,k |.</formula><p>This is a rough estimate of the overall supervised effect on the individual topics. We also studied negative and positive regression coefficients separately, but the results Figure <ref type="figure" target="#fig_7">8</ref> show the effect of the supervision. We can see the effect of the horseshoe prior in that the regression supervised effects are lower in general, due to the shrinkage imposed by the prior. With regard to topic coherence and document entropy, the supervision has no clear effect. Instead, it seems like the effect that the supervision has on the topics is corpus-specific. In the 20 Newsgroups corpus, we can see a small positive relationship between coherence and supervision effect, something that is not shared by the other corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>These results indicate that the effect of the supervision on individual topics depends on the corpora (and the label). This makes sense in that different types of labels will relate to different aspects of the text and the underlying topics. For the 20 Newsgroups, we can see a slight positive correlation between coherence and the supervised effects, and this is also a corpus with higher prediction accuracy. But overall, the results seem to indicate that there is no clear effect of the supervision on topic quality, as measured by document entropy and coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Scaling and parallelism performance</head><p>One of the contributions of this model is its ability to scale to larger corpora using a parallel and efficient MCMC sampler. To study the scalability of the model we ran Fig. <ref type="figure">9</ref> Scaling performance (left) and parallel performance (right). scaling experiments were run for 5000 iterations and the parallel performance experiments were run for 1000 iterations each. All were run with 3 different random seeds and the average runtime was computed. In the parallel experiment, the 20% NYT Hierarchical data was used and 2, 4, 8, and 16 cores experiments on the runtime effects on the different aspects identified in the complexity analysis, the number of classes, the number of documents and the number of topics.</p><p>Figure <ref type="figure">9</ref> show the results of the scaling and parallel performance experiments. From the results, we can see that the scaling in size of the corpus is more or less linear, as we expect. More interestingly, the runtime of the two NYT corpora is very similar. The hierarchical NYT corpus has roughly ten times more classes (240 vs. 31) than the top-level NYT corpus while being roughly one third in size. Hence we can conclude that, empirically, the largest effect on runtime is the number of documents, or tokens, rather than the number of classes for a standard setting with 100 topics.</p><p>Figure <ref type="figure">9</ref> also shows the parallel performance of the sampler. Unlike most other large-scale MCMC samplers for topic models, this sampler is both parallel and samples using an ergodic Markov chain with the posterior as the target. This still gives good parallel performance, even on a smaller corpus, such as the IMDb corpus. It is also obvious that the concurrency with regard to the different classes is also of importance. For the hierarchical NYT corpus, the increased number of classes affects the overall sampling time, but the parallel performance is not affected. We can also see that the parallel performance is needed mainly when the number of topics is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Several supervised topic models have been proposed with the purpose of identifying topics that can be used successfully to classify documents. We have proposed DOLDA, a supervised topic model with special emphasis on generating semantically interpretable predictions together with an efficient and scalable MCMC sampler for inference. An important component of the model to ease interpretation is the DO-probit model without a reference class. By coupling the DO-probit model with an aggressive Horseshoe prior with a shrinkage that is allowed to vary over the different classes, it is possible to create an interpretable classification model that automatically identifies the most interesting "signal" topics. At the same time, the DOLDA model comes with very few hyperparameters -only the standard LDA parameters α and β are needed, which has been extensively studied in <ref type="bibr" target="#b30">Wallach et al. (2009)</ref>. The fact that there are so parameters is different from most other supervised topic models <ref type="bibr" target="#b12">(Jiang et al. 2012;</ref><ref type="bibr" target="#b32">Zhu et al. 2012;</ref><ref type="bibr" target="#b15">Li et al. 2015)</ref>.</p><p>Our experiments show that the gain in interpretation from using DOLDA comes with only a small reduction in prediction accuracy compared to the state-of-the-art supervised topic models; moreover, DOLDA outperforms other fully Bayesian models such as the original supervised LDA model. We have also shown that learning the topics jointly with the classification part of the model gives more accurate predictions than a two-step approach, where a topic model is first estimated and a classifier is then trained on the learned topics, showing a general benefit of supervised topic modeling.</p><p>The horseshoe prior has also shown benefits in supervised topic models, leading to a much more clear picture of the important topics for a given label with similar, or better, prediction accuracy. The computational cost of the horseshoe is small compared to the other parts of the sampler, making it an attractive prior for use in other supervised models as well.</p><p>The supervision effect on the topics is generally small and in line with previous results. The supervision, in general, does not seem to affect the topic interpretability much, but there seems to be an indication that this is corpus (and label) dependent. In the 20 Newsgroups corpus, where the accuracy is higher, the relationship between topic coherence and supervision effects are slightly positive.</p><p>Finally, we show that the DOLDA model scales well for large corpora and many classes. Still, further improvement in scalability can be achieved with regard to the number of topics K . The ideas of <ref type="bibr" target="#b31">Zheng et al. (2015)</ref> can probably improve the scalability with respect to K , the number of topics, but this is something we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling the latent variables a</head><p>This is achieved the same approach as in <ref type="bibr">Johndrow et al. (2013, p. 34)</ref>, using truncated Normal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling the regression coefficients Á</head><p>The prior for η l is given by</p><formula xml:id="formula_17">η,l = Λ -1 η,l = τ 2 l ⎛ ⎜ ⎜ ⎜ ⎝ c/τ 2 l 0 • • • 0 0 λ 2 l,1 0 . . . . . . . . . 0 0 • • • λ 2 l, p ⎞ ⎟ ⎟ ⎟ ⎠ ,</formula><p>where η 1 is the intercept of the model and p is the total number of η for class l. Conditioned on a, we sample updates of each η l as in ordinary Bayesian linear regression.</p><formula xml:id="formula_18">η l ∼ N μ n,l , -1 post,l ,</formula><p>where</p><formula xml:id="formula_19">μ n = ((X Z) T (X Z) + η ) -1 (X Z) T a,</formula><p>and post,l = (X z) T (XN z) + η,l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling of category global shrinkage parameter l</head><p>The derivations are done for a given class l so we suppress the index l in the derivations. We start by deriving the unnormalized posterior distribution of τ .</p><formula xml:id="formula_20">p(τ |λ, η) ∝ p(η|λ, τ ) • p(τ ) = 1 (2π) p cτ 2 p λ 2 1 • • • λ 2 p exp - 1 2 η T η η • 2 π • 1 1 + τ 2 ∝ 1 τ p exp - 1 2τ 2 i η 2 i λ 2 i • 1 1 + τ 2</formula><p>We use slice sampling as presented by <ref type="bibr">Scott (2010, p. 6f.)</ref>. We set γ = 1 τ 2 that implies τ = γ -1 2 and let μ = </p><formula xml:id="formula_21">p(γ |λ, η) ∝ γ p exp - 1 2 i η 2 i λ 2 i γ • 1 1 + γ -1 d dγ γ -1 2 ∝ exp - 1 2 μγ γ p-1 2 1 γ + 1 .</formula><p>To sample τ we use the slice sampling algorithm of <ref type="bibr">Damlen et al. (1999, Section 3.</ref>2) by setting</p><formula xml:id="formula_22">l(γ ) = 1 1 + γ π(γ ) = exp - 1 2 μγ γ p-1 2 .</formula><p>We can see that π(γ ) is the density of a Gamma distribution with α = ( p + 1)/2 and β = 1 2 μ. We can hence sample γ in two steps:</p><formula xml:id="formula_23">u ∼ U (0, (1 + γ ) -1 ) γ ∼ G ( p + 1)/2, 1 2 μ2 I (γ &lt; (1 -u)/u)</formula><p>where I (•) indicates the truncation region. After sampling we transform back to τ by setting τ = γ -1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling of local shrinkage parameter l per category</head><p>As with the global shrinkage parameter, the category index l is suppressed. The computations follow that of τ in large parts. We have that for each λ</p><formula xml:id="formula_24">i,l p(η|λ i , τ )p(λ i ) ∝ 1 λ i exp - η 2 i 2τ 2 λ 2 i • 1 1 + λ 2 i , and then we set γ i = 1 λ 2 i with λ i = γ -1 2 i and hence p(γ i ) ∝ γ 1 2 i exp - η 2 i 2τ 2 γ -1 i • 1 1 + γ -1 i d dγ γ -1 2 i ∝ exp - η 2 i 2τ 2 γ i • 1 γ i + 1 .</formula><p>To sample λ i we use a similar algorithm that of the slice sampling approach used for τ :</p><formula xml:id="formula_25">l(γ i ) = 1 1 + γ i π(γ i ) = exp - 1 2 η i τ 2 γ i ,</formula><p>and, hence, we sample</p><formula xml:id="formula_26">u i ∼ U (0, (1 + γ i ) -1 ) γ i ∼ Exp 1 2 η i τ 2 I (γ &lt; (1 -u i )/u i ),</formula><p>where I (•) indicates the truncation region and Exp is the exponential distribution.</p><p>After sampling γ i we convert back to λ i with λ</p><formula xml:id="formula_27">i = γ -1 2 i .</formula><p>Sampling the topic indicators z|8, Á, z ¬i , X</p><p>The final step of the sampler is to derive the conditionals for the topic indicators z. We first remove all parameters that do not depend on z from the joint posterior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Efficient updating of supervision effects</head><p>One of the more important aspects of the sampler is that we need to update the supervised addition to full conditional posterior of z i,d , g ¬i d,k . Observe that this should be computed before starting to sample each topic indicator, per document, and for all k. where z i,d is the topic indicator at position i, z (i-1),d is the previous topic indicator and g</p><formula xml:id="formula_28">¬(i-1) d,k</formula><p>is the supervised effect for the previous topic indicator. In addition, the expression L l η l,k η l,z i,d can be pre-calculated during each iteration further reducing the (amortized) complexity of the sampler.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1The Diagonal Orthant probit supervised topic model (DOLDA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and (3) sampling from the multivariate Gaussian distribution of each η l is also of complexity O(L • (K + P) 3 ). Sampling the topic indicators has complexity O((K + P) • L • K • D) for the first topic indicator in each document, a complexity dominated by O(L • D • (K + P) 2 ). If we use the method for increased efficiency proposed above, all other topic indicators can be sampled with complexity O(K • N ). Sampling the latent variables a is of complexity (K + P) • D • L and is hence dominated by the sampling of η. Similarly, sampling τ and λ is of complexity (K + P) • L and is also dominated by the sampling of η. Sampling is of complexity O(K • V ), something that is generally dominated by sampling the topic indicators O(K • N ), since topically N &gt;&gt; V (Magnusson et al. 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Accuracy of MedLDA, taken from Zhu et al. (2012) (left) and accuracy of DOLDA for the 20 Newsgroup test set (right)</figDesc><graphic coords="13,225.03,186.32,160.12,126.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 Fig. 5</head><label>45</label><figDesc>Fig. 4 Coefficients for the IMDb corpus with 80 topics using the normal prior (left) and the Horseshoe prior (right)</figDesc><graphic coords="15,53.40,56.75,331.72,77.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Regression coefficients for the class Crime for the IMDb corpus with 80 topics using the Horseshoe prior (upper) and a normal prior (below)</figDesc><graphic coords="16,53.40,123.32,331.72,336.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Document entropy (left) and coherence (right) for the IMDb corpus</figDesc><graphic coords="17,53.40,55.91,331.72,129.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Coherence and document entropy by supervised effect with 50 topics</figDesc><graphic coords="18,53.40,56.03,331.72,245.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>p(z i,d = k| , η, z ¬i , x d ) ∝ p(w d |z d , ) • p(z d |θ d , ) • p(a d |η, zd , x d ), where d∈D [ p(w d |z d , )] • p(z| , ) is the standard LDA posterior. Due to the conditional conjugacy of the Dirichlet prior for the multinomial distribution, we can integrate out either or both and . Integrating out only results in either the partially collapsed sampler for parallel sampling over documentsp(z i,d = k| , η, z ¬i , X) ∝ φ k,v • M ¬i d,k + α • p(a d |η, z¬i d,k , x d ),where we also need to sample . This can be done in parallel over topics asφ k ∼ Dir(β + N k ).Alternatively, we can integrate out both and and use the sequential collapsed samplerp(z i,d = k| , η, z ¬i , k, j + β • M ¬i d,k + α • p(a d |η, z¬i d,k , x d ),Since the latent variables of a Diagonal Orthant probit model are conditionally independent given the regression parametersJohndrow et al. (2013, Section 1.3)  we have that p(a d |η, z ¬i d , x d ) = L l=1 p(a d,l |η l , z¬i d,k , x ), where p(a d,l |η l , z¬i d,k , x d ) is the density of the N ((z ¬i d x d,k )η l , 1) distribution, so p(a d,l |η l , z¬i d,k , x d ) ∝ exp -z d x d )η a d + (z d x d )η η(z d x d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>,.</head><label></label><figDesc>for all k = 1, . . . K , d = 1, . . . D and i = 1, . . . N d .To sample a topic indicator we first need to compute the supervised effect for each k when the topic indicator z i has been removed asg ¬i d,k = exp -To draw a new topic indicator we then usep(z i,d = k|•) ∝ φ k,v • M ¬i d,k + α • g ¬i d,k .Once we have calculated g d,k , we would like to efficiently add and withdraw a topic indicator from these values, since sampling the topic indicators is done once per token and iteration of the sampler, and the number of tokens can be very large. In the following way we can calculate the relation between g d,k and g ¬i d,k .g ¬i d,k = expl -(z ¬i d x d )η l + η l -[(z d x d )] η l + η i,d /N d + η lk η l,z i,d ,htt ps : //www.overlea f .com/ project/5a2c2856e2804c1b13b23cd2 and thereforeg k,d = g ¬i d,k • expk η l,z i,d ,where z i,d is the topic indicator at position i. As can be seen, to update g d we need to loop over the g d vector and update it element-wise when adding and removing a topic indicator z i,d . As is shown below, it is possible to update this g d vector on the fly when sampling each new token, based ong ¬i d,k = g k,d k η l,z i,d -L l η l,k η l,z (i-1),d ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>DOLDA model notation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Corpora used in experiment, by the number of classes (L), the number of documents (D), the vocabulary size (V ), and the total number of tokens (N )</figDesc><table><row><cell>Corpus</cell><cell>L</cell><cell>D</cell><cell>V</cell><cell>N</cell></row><row><cell>IMDb</cell><cell>20</cell><cell>10,810</cell><cell>47,371</cell><cell>967,255</cell></row><row><cell>20 Newsgroups</cell><cell>20</cell><cell>18,846</cell><cell>187,321</cell><cell>4,913,292</cell></row><row><cell>New York Times (hiearchical)</cell><cell>240</cell><cell>183,751</cell><cell>1,540,464</cell><cell>129,151,602</cell></row><row><cell>New York Times (top level)</cell><cell>31</cell><cell>595,635</cell><cell>3,326,778</cell><cell>339,298,734</cell></row></table><note><p><p>Statistics have been computed using the word tokenizer in the tokenizers R package with default settings</p><ref type="bibr" target="#b20">(Mullen 2016)</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>words in topics using the Horseshoe prior Topic 33Earth space planet alien human future years world time mission Topic 39 Police murder detective killer case investigation crime crimes solve murdered</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>DOLDA is Swedish for hidden or latent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The intercept is assigned a normal prior.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>ExperimentsWe study model performance in four different ways: the classification accuracy, the interpretability of the model, the topic quality and supervision effects on topics, and</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements Open access funding provided by <rs type="institution">Aalto University</rs>.</p><p>Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ref>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Derivation of the MCMC sampler</head><p>Here we will derive the sampler presented in Sect. 4. The full joint posterior distribution is</p><p>with notation summarized in Table <ref type="table">1</ref>.</p><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1140" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Stat Assoc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">422</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The horseshoe estimator for sparse signals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="465" to="480" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian linear regression with sparse priors</title>
		<author>
			<persName><forename type="first">I</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt-Hieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1986" to="2018" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading tea leaves: how humans interpret topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling general and specific aspects of documents with a probabilistic topic model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gibbs sampling for Bayesian non-conjugate and hierarchical models by using auxiliary variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Damlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wakefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J R Stat Soc Ser B (Stat Methodol)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="344" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Partial least-squares regression: a tutorial</title>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal Chim Acta</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of the multinomial probit model using marginal data augmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Dyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Econom</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="334" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised topic models with word order structure for document classification and retrieval learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Retr J</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="330" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for maximum margin supervised topic models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1592" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diagonal orthant multinomial probit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johndrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth international conference on artificial intelligence and statistics</title>
		<meeting>the sixteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic localization of bugs to faulty components in large scale software systems using Bayesian classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Broman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sandahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eldh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on software quality, reliability and security</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised labeled latent Dirichlet allocation for document categorization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="593" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse partially collapsed mcmc for parallel inference in topic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Broman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Graph Stat</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="463" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Topic models conditioned on arbitrary features with Dirichlet-multinomial regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3278</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 conference on empirical methods in natural language processing</title>
		<meeting>the 2011 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>association for computational linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">tokenizers: a consistent interface to tokenize natural language text. R package version 0</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mullen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tree ensembles with rule structured horseshoe regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nalenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Appl Stat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2379" to="2408" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed algorithms for topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuncion</forename><forename type="middle">A</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009-08">2009. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are automated debugging techniques actually helping programmers?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Parnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 international symposium on software testing and analysis</title>
		<meeting>the 2011 international symposium on software testing and analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchically supervised latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2609" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian inference for logistic models using Pólya-gamma latent variables</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Windle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Stat Assoc</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">504</biblScope>
			<biblScope unit="page" from="1339" to="1349" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th conference on uncertainty in artificial intelligence</title>
		<meeting>the 20th conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical topic models for multi-label document classification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="157" to="208" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times annotated corpus LDC2008T19. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Parameter expansion in local-shrinkage models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.5265</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking LDA: why priors matter</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linear time samplers for supervised topic models using compositional proposals</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MedLDA: maximum margin supervised topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2237" to="2278" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved Bayesian logistic supervised topic models with data augmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the association for computational linguistics</title>
		<meeting>the 51st annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
