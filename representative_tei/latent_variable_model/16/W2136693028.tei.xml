<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging relational autocorrelation with latent group models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
							<email>jneville|jensen@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Jensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging relational autocorrelation with latent group models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Statistical relational learning</term>
					<term>probabilistic relational models</term>
					<term>latent variable models</term>
					<term>autocorrelation</term>
					<term>collective inference Dagstuhl Seminar Proceedings 05051 Probabilistic</term>
					<term>Logical and Relational Learning -Towards a Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The presence of autocorrelation provides strong motivation for using relational techniques for learning and inference. Autocorrelation is a statistical dependency between the values of the same variable on related entities and is a nearly ubiquitous characteristic of relational data sets. Recent research has explored the use of collective inference techniques to exploit this phenomenon. These techniques achieve significant performance gains by modeling observed correlations among class labels of related instances, but the models fail to capture a frequent cause of autocorrelation-the presence of underlying groups that influence the attributes on a set of entities. We propose a latent group model (LGM) for relational data, which discovers and exploits the hidden structures responsible for the observed autocorrelation among class labels. Modeling the latent group structure improves model performance, increases inference efficiency, and enhances our understanding of the datasets. We evaluate performance on three relational classification tasks and show that LGM outperforms models that ignore latent group structure when there is little known information with which to seed inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autocorrelation is a statistical dependency between the values of the same variable on related entities, which is a nearly ubiquitous characteristic of relational datasets. For example, hyperlinked web pages are more likely to share the same topic than randomly selected pages <ref type="bibr" target="#b0">[1]</ref>, and movies made by the same studio are more likely to have similar box-office returns than randomly selected movies <ref type="bibr" target="#b1">[2]</ref>. More formally, autocorrelation is defined with respect to a set of related instance pairs (z i , z j ) ∈ Z; it is the correlation between the values of a variable X on the instance pairs (z i .x, z j .x).</p><p>Despite the challenges to learning, the presence of autocorrelation offers a unique opportunity to improve model performance, as autocorrelation enables inferences about one object to be used to improve inferences about related objects. Indeed, recent work in relational domains has shown that collective inference over an entire dataset results in more accurate predictions than conditional inference over each instance independently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> and that the gains over conditional models widen as autocorrelation increases <ref type="bibr" target="#b3">[4]</ref>.</p><p>Collective inference techniques exploit autocorrelation by reasoning with collective models that represent the dependencies among class labels of related instances. Collective models have lower variance than models that represent autocorrelation dependencies indirectly through dependencies on other attributes of related instances <ref type="bibr" target="#b3">[4]</ref>. Collective inference techniques use these low variance models to propagate information throughout the dataset and improve the overall set of predictions.</p><p>Modeling the correlations among class labels directly, however, fails to capture a frequent cause of autocorrelation-the presence of underlying groups, conditions, or events that influence the attributes on a set of entities. For example, in the cinematic domain, it is likely that studios cause the observed autocorrelation among movie returns. Movie-goers are unlikely to choose movies based on the success of other movies from the same studio. It is more likely that movie success is influenced by some unobserved properties of the studio (e.g., advertising budget). In this case, the class labels of movies are conditionally independent given studio type (e.g., high-budget studio). Latent-variable models that represent the correlation of unobserved group properties (e.g., studio type) with attribute values (e.g., movie returns) may be able to express density functions more accurately and compactly than approaches that directly model the observed autocorrelation.</p><p>When the groups are observable (e.g., movies made by the same studio), we can model the latent structure with a relatively simple application of the expectation-maximization (EM) algorithm. A similar approach is used in information retrieval where latent-unigram models represent each document as a group of word occurrences 1 that are conditionally independent given the topic of the document <ref type="bibr" target="#b4">[5]</ref>. In many relational datasets, however, group membership is unobserved and must be inferred from the relations and attributes. For example, the World Wide Web contains communities-groups of hyperlinked pages with similar topics. Although we can not directly observe which community a web page belongs to, intra-community citations are more frequent than inter-community citations so hyperlinks are evidence of the underlying community structure. To continue the document retrieval metaphor, it is as if we have word occurrence information (attribute values) and noisy indicators of word co-occurrence within documents (link information) but we do not know the document boundaries or the topic distributions. In these situations, a joint model of attributes and links is needed to recover group membership and infer latent group properties.</p><p>In this work, we propose a latent group model (LGM) for relational data, which is a joint model of links, attributes, and groups for unipartite relational graphs. The model addresses a number of weaknesses of current collective models. First, collective models, which model autocorrelation dependencies directly, generally require computationally-complex, approximate inference techniques because inference is over a large, cyclic graphical model. If, however, the instances are conditionally independent given the underlying group structure, then exact inference is not only feasible but much more efficient. Second, collective models typically restrict their representation to the dependencies among instances that are directly linked in the data. Linkage is generally sparse<ref type="foot" target="#foot_0">foot_0</ref> , so this restriction constrains the space of possible dependencies to a reasonable size and prevents useful information from being drowned out by noise. However, modeling the dependencies among neighboring but unlinked instances (e.g., transitive relationships) allows information to propagate in a more elaborate manner during inference. Group models are a natural way to extend the representation to improve model performance without fully representing the O(N 2 ) dependencies between all pairs of instances. Finally, latent-variable models recover the underlying groups and identify their associated density functions. This attempt to model the true cause of autocorrelation will improve domain understanding and motivate development of additional modeling techniques.</p><p>Our initial evaluation of LGMs are on out-of-sample classification tasks. More specifically, we aim to learn LGMs of datasets where the attributes and links are fully observed, and group structure is unobserved, and then apply the model to classify instances in new datasets, where the attributes are partially observed, links are fully observed and the groups are again unobserved. This approach is suited for domains with large, nearly disconnected graph structures. For example, in gene prediction tasks, models of proteins and how they interact to perform certain functions in the cell can be learned in one genome and then applied to classify the proteins in new genomes. In addition, this approach is suited for dynamic network domains, where groups emerge and and disband over time. For example, fraud detection efforts usually analyze a single dataset that is evolving over time. The data contain demographic information about individuals and transactional links (e.g., bank deposits, telephone calls) that can indicate the underlying organizations. In these domains, LGMs could be used to detect group formation and use a few hand-labeled examples to seed inference about the classifications of new group members.</p><p>In the remainder of the paper, we outline LGM, our initial algorithms for learning and inference, and related work in statistical relational learning. We present empirical evaluation on three classification tasks to demonstrate the capabilities of the model, showing that LGMs perform better than models that ignore latent groups when there is little known information with which to seed inference. Finally, we conclude with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent Group Models</head><p>Latent group models specify a generative probabilistic model for the attributes and link structure of a relational dataset. The model posits groups of objects in the data of various type. Membership in these groups influences the observed attributes of objects, as well as the existence of relationships (links) among objects.</p><p>For this initial investigation of LGMs, we make several simplifying assumptions about the data. More specifically, we assume a unipartite relational data graph (single object type) with binary, undirected links, and at most one link between any pair of objects. We also assume the number of objects, groups, and group types are fixed and known. However, it is relatively straightforward to extend the model to accommodate deviations from these assumptions.</p><p>The model assumes the following generative process for a dataset with N O objects and N G groups: (a) For each object j, i &lt; j ≤ N O : i. Choose a value for e ij from p(E|G i = G j , T i , T j ), a Bernoulli probability conditioned on the two objects' groups types and whether they are in the same group.</p><formula xml:id="formula_0">1. For each group g, 1 ≤ g ≤ N G : (a)</formula><p>This generative model specifies that attribute values and link existence are conditionally independent given group membership and type information. More specifically, the class labels of objects are conditionally independent.</p><p>The joint distribution of the dataset D, with groups N G , objects N O , and links L, is thus given by:</p><formula xml:id="formula_1">p(D) = g∈N G p(t g ) A∈A o∈N O p(a o |g o , t go ) • lij ∈L p(e ij =1|g i =g j , t gi , t gj ) lij / ∈L p(e ij =0|g i =g j , t gi , t gj )</formula><p>See figure <ref type="figure">1</ref> for a graphical representation of LGM. The model is similar to hierarchical Bayesian models but extended to a relational domain where the generative process is responsible for generating both attributes and links. More specifically the model is a form of probabilistic relational model (PRM) <ref type="bibr" target="#b5">[6]</ref> that combines a directed relational Bayesian network, link existence uncertainty, and hierarchical latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning</head><p>Learning an LGM consists of learning the parameters of the distribution and inferring the latent variables on both objects (group membership) and groups</p><formula xml:id="formula_2">A G N O E N O (N O -1)/2 T N G M 2:1 Fig. 1.</formula><p>LGM graphical model representation. The plates represent replicates: N G groups; N O objects, each with M attributes; and N O (N O -1)/2 possible binary links. The shaded nodes are observed variables: A are object attributes, E is a binary variable indicating link existence. The unshaded nodes are unobserved variables: T is the group's type, G is the object's group membership. The dashed line indicates the underlying link relationships in the data, which each involve two objects; consequently each E will be influenced by two G and two T variables in the unrolled Bayes net.</p><p>(group type). Ideally, we could learn the model using a straightforward application of the EM algorithm-iterating between inferring the latent variables (E-step) and estimating the parameters (M-step). Unfortunately, there are difficulties with this approach. First, there are N O latent group variables with N G possible values and N G latent type variables with k possible values. When the average group size is small (N G = O(N O )), we expect that EM will be very sensitive to the start state. Furthermore, the E-step requires that we run inference over a large, complex, rolled-out Bayes net with 2N O |A M | + 4N O (N O -1)/2 edges, where objects' group memberships are all interdependent (given the link observations). Exact inference in this situation is impractical, although approximate inference techniques such as loopy belief propagation or variational methods may allow accurate inference. However, given the number of latent variables and their dependency on sparse link information (L N O (N O -1)/2), the likelihood function will have many local (suboptimal) maxima and we expect that EM will not converge to a reasonable solution.</p><p>Because collective models propagate information only on existing links, we expect the autocorrelated groups will have more intra-group links than intergroups links. We exploit this characteristic to decouple the group discovery from the remainder of the estimation process and propose the following approximate learning algorithm:</p><p>1. Hypothesize group membership for objects based on the observed link structure alone. 2. Use EM to infer group types and estimate the remaining parameters of the model.</p><p>A hard clustering approach, which assigns each object to a single group, greatly simplifies the estimation problem-we only need to estimate the latent group type variables and parameters of p(T ) and p(A|G, T ). To this end, we employ a recursive spectral decompostion algorithm with a norm-cut objective function <ref type="bibr" target="#b6">[7]</ref> to cluster the objects into groups with high inter-group and low intragroup linkage. Our approach appears to work well in practice, but refinements that iterate the clustering and EM steps, or incorporate soft clusterings, may improve results even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference</head><p>There are two ways to apply LGMs to relational data. First, given a dataset with observed attributes and links, we can use the model to cluster objects into groups with similar attribute values and patterns of linkage. Second, we can apply the model to a new unseen dataset with partially observed attributes and/or links to infer both the group memberships and the unobserved attributes/links. This approach exploits the underlying group structure to improve predictions by jointly inferring the latent groups, their types, and the unknown attributes/links. Our initial investigation of LGMs has focused primarily on this latter task to enable an objective comparison of LGMs with current, alternative techniques.</p><p>In the experiments reported below, we learn the model on a dataset with fully observed attributes and links, then we apply the model to a new dataset with partially observed attributes and fully observed links to jointly infer the group memberships and unknown attributes. We designed our learning algorithm with this out-of-sample classification task in mind. When classifying a new dataset with partially observed attributes, we can cluster the objects into groups using the observed links and then use the learned model to jointly infer the group types and the unobserved attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>There are two types of statistical relational models related to LGMs: models that represent joint distributions of attributes conditioned on link structure, and models that cluster objects into groups based on link and attribute structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint models</head><p>The first category consists of models that represent a joint distribution of the attributes of set of instances. Relational Markov networks (RMNs) <ref type="bibr" target="#b0">[1]</ref> and relational dependency networks (RDNs) <ref type="bibr" target="#b7">[8]</ref> model autocorrelation in a procedural fashion. RMNs use clique templates to model the pairwise correlations among class labels of related instances, whereas RDNs use aggregated features. These techniques model the autocorrelation dependencies at a global level-the autocorrelation dependencies are assumed to be uniform across each link in the data and parameters of features are tied across the entire dataset. As such, these models will not be able to distinguish among regions with varying levels of autocorrelation.</p><p>Probabilistic relational models (PRMs) <ref type="bibr" target="#b5">[6]</ref> are also able to model autocorrelation relationships, but only if the autocorrelation can be structured to be acyclic (e.g., with temporal constraints). The use of latent variables in PRMs has been explored in limited settings where the groups are known and represented as objects in the data <ref type="bibr" target="#b8">[9]</ref>. For example, in the cinematic domain we could use a PRM with a latent variable on studios to model the autocorrelation of movie returns. However, latent groups are not posited by the model and group variables are not conditioned on the observed link structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cluster models</head><p>The second category consists of models that cluster relational data into groups. Popescul and Ungar <ref type="bibr" target="#b9">[10]</ref> cluster relational database tables and use the cluster IDs as features in individual classification models that reason about each instance independently. This approach has been shown to improve classification performance, but it can only be employed in situations where the test set instances link into the clusters used during training because the features use the identity of the clusters rather than generalizing over the properties of the groups.</p><p>Kubica, Moore, Schneider and Yang <ref type="bibr" target="#b10">[11]</ref> use a latent variable model that is a special case our proposed model to cluster objects into groups based on their attribute values and link structure. Their approach is geared toward clustering data with multiple transactional links (e.g., phone calls, email and meetings) where the links patterns are homogeneous with respect to the groups. In other words, it is assumed that all groups have the same distribution of intra-and intergroup linkage. A situation where the patterns of linkage differ among groups is, however, easy to imagine. For example, consider machine learning papers: Reinforcement learning papers tend to cite papers in optimization, operations research, and theory, but genetic algorithm papers cite primarily other genetic algorithm papers. Allowing the link probabilities to vary among groups will be important for modeling group structures in large heterogeneous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Consider the case where there are k group types, C class values, and each object has a latent variable. There is a spectrum of group models ranging from k = C to k = N G . RMNs can reason at one end of the spectrum (k = C) by tying the parameters across all links and creating a feature for each class. Techniques that cluster the data for features to use in conditional models (e.g., <ref type="bibr" target="#b9">[10]</ref>), can reason at the other end of the spectrum (k = N G ) by using the identity of each cluster. The approach of <ref type="bibr" target="#b10">[11]</ref> uses k = 1 in the sense that it ties the parameters of intraand inter-group link probabilities across all groups.</p><p>When group size is large there may be enough data to reason about each group independently, setting k = N G . For example, in the cinematic domain, once a studio has made a sufficient number of movies, we can reason about the likely returns of its next movie independent of the rest of the data. However, when group size is small, assuming that all groups are drawn from the same distribution, and setting k = C, will offset the limited data available for each group. A model that can smoothly vary k can be thought of as a backoff model, where we smooth to the background signal when we don't have enough data to estimate about a group's type in isolation. LGMs offer a principled framework within which to explore this spectrum.</p><p>One of the primary advantages of LGMs is that influence can propagate between pairs of objects that are not directly linked but are close in graph space (e.g., in the same group). In RMNs and RDNs, the features of an object specify its Markov blanket. This limits influence propagation because features are generally constructed over the attributes of objects one or at most two links away in the data. Influence can only propagate farther by influencing the probability estimates of attribute values on each object in a path sequentially. An obvious way to address this issue is to model the O(N 2 O ) dependencies among all pairs of objects in the data, but dataset size and sparse link information makes this approach infeasible for most datasets. PRMs with existence uncertainty <ref type="bibr" target="#b11">[12]</ref> are the only current models that consider the full range of dependencies and their influence on observed attributes.</p><p>LGMs are a natural way to expand current representations while limiting the number of dependencies to model. LGMs can aggregate influence over a local neighborhood, instead of only passing on autocorrelation information through changes to the probability distributions of each object in a path sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The experiments in this section demonstrate the utility of latent group models in relational domains. Using three classification tasks, we evaluate whether the models can leverage autocorrelation to improve model accuracy and illustrate the conditions under which the models will perform well.</p><p>We present results for two variations of LGM. The first variation, LGM-k, sets the number of group types to the number of class label values, k = C (e.g., for binary tasks, k = 2); the second variation, LGM-2k, sets k = 2C. We compare the LGM to four alternative models. The first two are individual classification models that reason about each instance independently and do not use the class labels of related instances: the relational probability tree (RPT) model <ref type="bibr" target="#b12">[13]</ref> is a decision tree model and the relational Bayesian classifier (RBC) model <ref type="bibr" target="#b13">[14]</ref> is a naive Bayes model. The third model is a relational dependency network (RDN) <ref type="bibr" target="#b7">[8]</ref> that reasons about networks of instances collectively. The fourth model (RDN-ceil) is a probabilistic ceiling for the RDN model, where we allow the true labels of related instances to be used during inference. This model shows the level of performance possible if the RDN model could infer the true labels of related instances with perfect accuracy.</p><p>To limit the confounding effects of feature construction and model selection, we consider the restricted task of predicting class labels using only the class la-bels of related instances and/or the group membership. For the RPT and RBC models, we clustered the training and test sets together and used cluster ID as the sole attribute in the model. The performance of these baseline models illustrates the baseline utility of clustering without typing the groups and serves as a comparison to previous work <ref type="bibr" target="#b9">[10]</ref>, which clusters the data to generate additional features for classification. For the LGM, RDN and RDN-ceil, we used the class label of related instances as the sole attribute available for modeling. When possible, we used exact inference but RDNs require approximate inference techniques. In the RDN experiments, we used Gibbs with chains of length 500 and burn-in of 100. (At this length, acccuracy and AUC had converged.) During inference we varied the number of known class labels available to seed the inference process. We expect performance to be similar when other information serves to seed the inference process-either when some labels can be inferred from intrinsic attributes, or when weak predictions about many related instances serve to constrain the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Tasks</head><p>The first data set is drawn from the Internet Movie Database (www.imdb.com). We used a sample of 1,382 movies released in the U.S. between 1996 and 2001. The binary classification task was to predict movie opening weekend returns (&gt;$2million). Based on past work that showed movie receipts to be autocorrelated through studios <ref type="bibr" target="#b1">[2]</ref>, we considered a unipartite graph of movies, where links indicate movies that are made by a common studio.</p><p>The second data set is drawn from Cora, a database of computer science research papers extracted automatically from the Web using machine learning techniques <ref type="bibr" target="#b14">[15]</ref>. We considered the unipartite co-citation graph of 4,330 machinelearning papers. The classification task was to predict paper topic. There are seven topics including Neural Networks and Reinforcement Learning.</p><p>The third data set was collected by the WebKB Project <ref type="bibr" target="#b15">[16]</ref>. The data consist of a set of 3,877 web pages from four computer science departments, manually labeled with the categories: course, faculty, staff, student, research project, or other. We considered the unipartite co-citation web graph. The classification task was to predict page category. As in previous work on this dataset, we do not try to predict the category Other ; we remove them from the data after creating the co-citation graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Figure <ref type="figure" target="#fig_0">2a-c</ref> shows area under the ROC curve (AUC) results for each of the models on the three classification tasks 3 . The graph shows AUC for the most prevalent class, averaged over 4-5 training/test splits. For IMDb and Cora, we used temporal samples where we learned the model on one year and applied the model to the subsequent year. For WebKB, we used cross-validation by department, learning on three departments and testing on the fourth. For each training/test split we ran 10 trials at each level of labeling, except the RDNs where we ran 5 trials due to relative inefficiency of RDN inference. The error bars indicate the standard error of the AUC estimates for a single training/test split, averaged across the training/test splits. This illustrates the variability of performance within a particular sample, given the random initial labeling. The results show that LGM performance quickly reaches performance levels comparable to RDN-ceil. (Note that RDNs and LGMs cannot be expected to do better than random at 0% labeled.) On IMDb and WebKB, LGM performance asymptotes at less than 40% known labels, indicating that the model is able to exploit group structure when there is enough information to accurately infer the group type. The RDN doesn't converge as quickly to the ceiling level of performance. There are two explanations for this effect. First, when there are few constraints on the labeling space (e.g., fewer known labels), RDN inference may not be able to fully explore the space of labelings. Although we saw performance plateau for Gibbs chains of length 500-2000, it is possible that longer chains, or alternative inference techniques, could further improve RDN performance. The second explanation is that joint models are disadvantaged by the data's sparse linkage. When there are few labeled instances, influence may not be able to propagate to distant objects over the existing links in the data. A group model that allow influences to propagate in more elaborate ways may be able to exploit the seed information more successfully. Future work will attempt to quantify the amount of error due to each of these sources.</p><p>LGM performance does not reach that of RDN-ceil in Cora. Although the LGM outperforms the RDN when there is little know information, eventually the RDN takes over as it converges to RDN-ceil performance. We conjecture that this effect is due to the quality of the clusters recovered in Cora. The distribution of cluster sizes has high variance-ranging from large clusters with more than 100 papers to small clusters with 2-3 papers.</p><p>LGM is not able to model papers in the large clusters as accurately as the RDN because the distribution of class labels within these groups is too uniform. The clustering technique has likely conflated several groups and returned them as one cluster when they should be partitioned. A technique the iterates over clustering and model estimation may be more robust in this situation.</p><p>RPT performance is near random on all three datasets. This is because the RPT algorithm uses feature selection and there is significant correlation between only a few cluster IDs and the class label. This indicates that there is little evidence to support generalization about cluster identities themselves. The RBC, on the other hand, does not do any feature selection, it simply uses cluster IDs without regard to their support in data. On IMDb and Cora, the RBC significantly outperforms all other models. However, these are the two samples where the test set instances link into the training set. In the third dataset, where the training and test sets are nearly disjoint, the RBC does no better than random. To further explore this effect, we partitioned the IMDb data into five disjoint training/test splits using snowball sampling of movies through studios. Figure <ref type="figure" target="#fig_0">2d</ref> graphs performance on this view of the IMDb. RBC performance drops to random when the training and test sets are nearly disjoint.</p><p>For a subjective evaluation of the clustering abilities of LGM, table 1 lists the studios associated with the IMDb clusters. We group the clusters by their (inferred) type values and present a sample of the associated studios and the estimated probability distribution of movie returns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>This paper presents a latent group model that reasons jointly about attribute information and link structure to improve reasoning in relational domains. To date, work on statistical relational models has focused on models of attributes conditioned on the link structure (e.g., <ref type="bibr" target="#b0">[1]</ref>), or on models of link structure conditioned on the attributes (e.g., <ref type="bibr" target="#b11">[12]</ref>). These restrictions to the model space make learning and inference more tractable but limit the manner in which influence can propagate in the data. However, as our initial investigation has shown, modeling the interaction among links and attributes promises to improve model generalization and interpretability.</p><p>Latent group models are a natural means to model the attribute and link structure simultaneously. The groups decouple the link and attribute structure, thereby offering a way to learn joint models tractably. Preliminary investigations of latent variable models in the social networks community for link prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, and in the relational learning community for clustering <ref type="bibr" target="#b10">[11]</ref> and augmentation of classification models <ref type="bibr" target="#b8">[9]</ref> show promise. However, the power and range of applicability of these models is yet to be fully explored. Our analysis has shown that group models outperform collective models when there is little information to seed inference. This is likely because a smaller amount of information in needed to infer group type than is needed to propagate information throughout sparse relational graphs. This suggests active inference as an interesting new research direction-where techniques choose which instances to label based on estimated improvement to the collective predictions.</p><p>Latent group models extend the manner in which collective models exploit autocorrelation to improve model performance. One of the reasons collective inference approaches work is that the class labels are at the "right" level of abstraction-they summarize the attribute information that is relevant to related objects. Group models also summarize the information but at higher level of abstraction (e.g., group membership and type). Positing the existence of groups decouples the search space into a set of biased abstractions and could be considered a form of predicate invention <ref type="bibr" target="#b18">[19]</ref>. This allows the model to consider a wider range of dependencies to reduce bias while limiting potential increases in variance and promises to unleash the full power of statistical relational models. Indeed, the results we report for LGMs use only the class labels and the link information but they achieve nearly the same level of performance reported by relational models in the recent literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Model performance as the proportion of labeled instances during inference is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Studios associated with IMDb groups</figDesc><table><row><cell>Group type 1 (6 clusters): P (returns &gt; $2mil|G) = 0.90</cell></row><row><cell>Paramount Pictures, Universal Pictures, Buena Vista Pic-</cell></row><row><cell>tures, New Line Cinema, DreamWorks, MGM</cell></row><row><cell>Group type 2 (24 clusters): P (returns &gt; $2mil|G) = 0.59</cell></row><row><cell>Columbia Pictures, Warner Bros., 20th Century Fox, Desti-</cell></row><row><cell>nation Films, Lot 47 Films, Margin Films</cell></row><row><cell>Group type 3 (7 clusters): P (returns &gt; $2mil|G) = 0.30</cell></row><row><cell>Artisan Entertainment, Miramax, Gramercy Pictures, Sony</cell></row><row><cell>Pictures, RCV Film Distribution, United Artists, Trimark</cell></row><row><cell>Pictures</cell></row><row><cell>Group type 4 (32 clusters): P (returns &gt; $2mil|G) = 1.5e-4</cell></row><row><cell>Seventh Art Releasing, Strand Releasing, Zeitgeist Films,</cell></row><row><cell>October Films, The Shooting Gallery, Curb Entertainment</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In the datasets we have analyzed, existing links account for 1-10% of the O(N 2 ) possible dependencies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Accuracy results, over all classes, show similar behavior.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative probabilistic models for relational data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 18th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linkage and autocorrelation cause feature selection bias in relational learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning</title>
		<meeting>the 19th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced hypertext categorization using hyperlinks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why collective inference improves relational classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="593" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Data Mining</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dependency networks for relational data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th IEEE International Conference on Data Mining</title>
		<meeting>the 4th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic classification and clustering in relational data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 17th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-based concept invention for statistical relational learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="665" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic link and group detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI/IAAI</title>
		<meeting>AAAI/IAAI</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="798" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning probabilistic models of link structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="679" to="707" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning relational probability trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="625" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple estimators for relational bayesian classifers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE International Conference on Data Mining</title>
		<meeting>the 3rd IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A machine learning approach to building domain-specific search engines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 16th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="662" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to extract symbolic knowledge from the world wide web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dipasquo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th National Conference on Artificial Intelligence</title>
		<meeting>the 15th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockstructures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1077" to="1087" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicate invention in inductive logic programming</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Inductive Logic Programming</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="34" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
