<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Variational Autoencoding Approach for Inducing Cross-lingual Word Embeddings</title>
				<funder ref="#_8CNxCVm">
					<orgName type="full">National High Technology Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liangchen</forename><surname>Wei</surname></persName>
							<email>liangchen.wei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
							<email>zhdeng@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Variational Autoencoding Approach for Inducing Cross-lingual Word Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-language learning allows one to use training data from one language to build models for another language. Many traditional approaches require word-level alignment sentences from parallel corpora, in this paper we define a general bilingual training objective function requiring sentence level parallel corpus only. We propose a variational autoencoding approach for training bilingual word embeddings. The variational model introduces a continuous latent variable to explicitly model the underlying semantics of the parallel sentence pairs and to guide the generation of the sentence pairs. Our model restricts the bilingual word embeddings to represent words in exactly the same continuous vector space. Empirical results on the task of cross lingual document classification has shown that our method is effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations have become an increasingly important tool in machine learning. Typically, word embeddings are trained to represent words in a continuous space in an unsupervised way, which characterizes the lexicosemantic relations among words. In many NLP tasks, they prove to be high-quality features, in contrast to hand-crafted, and thus expensive features. Successful applications of distributed representations include sentence modeling <ref type="bibr" target="#b2">[Bengio et al., 2003]</ref>, sentiment analysis <ref type="bibr" target="#b11">[Socher et al., 2011]</ref> and document classification <ref type="bibr" target="#b7">[Klementiev et al., 2012]</ref>.</p><p>The use of distributed representations is motivated by the idea that they capture semantics and syntax as well as encoding similarity between words. Like words have synonyms in the same language, word pairs across language also share resembling semantics. Mikolov <ref type="bibr" target="#b11">[Mikolov et al., 2013]</ref> observed a strong similarity between vector spaces across languages and suggested a linear cross-lingual mapping between the two vector spaces is technically possible.</p><p>Recently, it is becoming more and more desirable to have unsupervised techniques which can learn useful syntactic and semantic features that are invariant to tasks or languages ‚á§ Corresponding Author which we are interested in. With accurate joint-space embeddings of both language, one can develop abundant textual resources from language A to build models for language B. This is especially useful for transferring limited label information from high-resources to low-resources languages, and has been demonstrated effective for document classification by <ref type="bibr" target="#b7">Klementiev[Klementiev et al., 2012]</ref>, whose model outperforms a strong phrase based machine translation baseline.</p><p>Defining a joint space objective function is crucial at the core of cross-lingual learning. Several models for training cross-lingual embeddings have been proposed, usually starting from a monolingual objective following cross-lingual objective as constraints. <ref type="bibr" target="#b15">[Zou et al., 2013]</ref> learned word embeddings of different languages in separate spaces with monolingual corpus and projected the embeddings into a joint space. <ref type="bibr" target="#b11">[Mikolov et al., 2013]</ref> learned a linear projection from one embedding space to another. It was extended by <ref type="bibr">[Faruqui and Dyer, 2014]</ref>, who simultanteously projected source and target language embeddings into a joint space using canonical correlation analysis. Bilingual autoencoder for bags-of-words (BAE) <ref type="bibr" target="#b1">[AP et al., 2014]</ref> reconstructed the bag-of-words representation of semantic equivalent sentence pairs. BilBOWA <ref type="bibr">[Gouws et al., ]</ref> extended CBOW and skipgram models with minimizing differences between parallel sentence pair representations. <ref type="bibr">[ ≈†uster et al., 2016]</ref> learned multi sense word embeddings with discrete autoencoders. Many of these models can be viewed as instances of a more general framework for inducing cross-lingual word embeddings, which integrates monolingual objectives(similar words in each language have similar embeddings) and cross-lingual objectives(similar words across languages have similar representations).</p><p>Inspired by the recent advances <ref type="bibr" target="#b6">[Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014]</ref> in neural variational inference, we propose a variational autoencoding model(BiVAE) to cross-lingual learning. Unlike the framework mentioned above, we explicitly model the underlying semantics of bilingual sentence pairs(see Figure1). Similar to <ref type="bibr" target="#b14">[Suzuki et al., 2016]</ref>, we make two assumptions about BiVAE:</p><p>‚Ä¢ There exists a continuous latent variable z from this underlying semantic space.</p><p>‚Ä¢ This variable z, guides the generative process of the equivalent sentence pairs x and y independently, i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùëß ùë• ùë¶ ùúÉ ùúë</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùê∑</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(x, y</head><formula xml:id="formula_0">| z) = p(x | z)p(y | z).</formula><p>With this assumption, the following formulation characterizes our probabilistic language model:</p><formula xml:id="formula_1">p(x, y) = Z z p(x, y | z)p(z)d z (1)</formula><p>This brings the benefits that the latent variable z serves as a global variable capturing underlying semantics between parallel sentence pairs, thus forcing the words from different language to be embedded in a unified vector space without the help of extra alignment constraints. However the incorporation of it into the probabilistic language model brings one challenge: the posterior inference in this model is intractable. In order to address this issue, we propose the variational encoder-decoder model to cross-lingual learning, motivated by recent success of variational neural inference <ref type="bibr" target="#b6">[Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014]</ref>. We employ deep neural networks to model the posterior distribution of the latent hidden variable as they are capable of learning highly nonlinear functions thus making the inference tractable. In order to train model parameters efficiently, we apply a reparameterization trick <ref type="bibr" target="#b6">[Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014]</ref> on the variational lower bound which enables us using stochastic gradient optimization during training. Specifically, BiVAE has two essential components:</p><p>‚Ä¢ A variational inference network infers the posterior distribution of z according to the encoded representation of parallel sentence pairs(i.e. q (z | x, y)). ‚Ä¢ A decoder network reconstructs the observable sentences conditioned on the inferred distribution of z(i.e.</p><formula xml:id="formula_2">p ‚úì (x | z), p ‚úì (y | z))</formula><p>. Model details will be introduced in section 3. We train the cross-lingual word embeddings with the proposed model and apply these embeddings to a standard document classification task and show that training with parallel corpus only, our model performs comparably with previous reported state-ofthe-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Variational Autoencoder</head><p>In this section, we briefly review the VAE <ref type="bibr" target="#b6">[Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014]</ref>. The VAE is a generative model which is based on a regularized version of the standard autoencoder. It modifies the autoencoder architecture by replacing the deterministic function enc with a learnable posterior recognition model, q(z | x). The VAE imposes a prior distribution on the hidden variable z which enforces a regular geometry over the hidden representation and makes it possible to draw proper samples from the model using ancestral sampling. Intuitively, the VAE learns codes not as single points, but as soft ellipsoidal regions in latent space, forcing the codes to fill the space rather than memorizing the training data as isolated codes.</p><p>Given an observed variable x, the VAE introduces a latent variable z, and assumes that x is generated from z which can be characterized by the following formula:</p><formula xml:id="formula_3">p(x, z) = p ‚úì (x | z)p(z)<label>(2)</label></formula><p>where ‚úì denotes the generative parameter of the model and p(z) denotes the prior distribution of the latent variable z, e.g Gaussian distribution. p ‚úì (x | z) is the conditional distribution that models the generative process of x conditioned on the hidden variable z, typically estimated via a non-linear deep neural network.</p><p>The integration of z brings a challenge on the posterior inference and VAE adopts two techniques to tackle this problem: variational neural inference and reparameterization.</p><p>Variational neural inference employs deep neural network to approximate the posterior distribution of latent variable z, which is parameterized by a diagonal Gaussian distribution:</p><formula xml:id="formula_4">q (z | x) = N (¬µ(x), diag( 2 (x)))<label>(3)</label></formula><p>where mean ¬µ(x) and variance diag( 2 (x)) are both nonlinear functions of x parameterized with deep neural networks.</p><p>Reparameterization reparameterizes z as a function of ¬µ and instead of using the standard sampling method. VAE introduces a standard Gaussian noise variable ‚úè for generating samples from q (z | x), namely the reprarameterization trick: z = ¬µ + ‚úè (4) VAE uses an objective which encourages the model to keep its posterior distribution of z close to its prior distribution. And this objective is a valid lower bound estimation on the true log likelihood of the data, making VAE a generative model. The objective takes the following form:</p><formula xml:id="formula_5">L V AE (‚úì, ; x) = KL(q (z | x) || p(z))+ E q (z|x) ([logp ‚úì (x | z)]) Ô£ø logp(x)<label>(5)</label></formula><p>Maximizing the objective function is equivalent to maximizing the reconstruction likelihood of observable variable</p><p>x and minimizing the Kullback-Leibler divergence between the approximated posterior and the prior distribution of latent variable z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual Variational Autoencoder</head><p>In this section, we introduce our proposed model in detail. Formally, given the definition in Eq.1, the variational lower bound of BiVAE can be formulated as follows:  where x and y denote observable semantic equivalent sentences from each language respectively, p(z) is the prior distribution of latent variable z, which is Gaussian distribution here. q (z | x, y) is our posterior approximator,</p><formula xml:id="formula_6">L BiV AE (‚úì, ; x, y) = KL(q (z | x, y) || p(z)) +E q (z|x,y) [logp ‚úì (x | z) + logp ‚úì (y | z)] Ô£ø logp(x, y)<label>(6)</label></formula><formula xml:id="formula_7">p ‚úì (x | z) and p ‚úì (y | z)</formula><p>represent the conditional distribution of x and y conditioned on z. ‚úì and are parameters of generative and inference neural networks respectively. Our goal is to train high quality bilingual word features with the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Inference Network Neural sentence encoder</head><p>The neural encoders(see Figure2) aim at encoding the parallel sentence pairs into distributed representations. Following <ref type="bibr" target="#b3">[Bowman et al., 2015]</ref>'s approach, we adopt RNN to encode the parallel sentence pairs. But different from Bowman, who encodes the monolingual sentences using vanilla RNN, we adopt bidirectional LSTM as sentences are better summarized with context both forwards and backwards. Given an instance of parallel sequence pairs</p><formula xml:id="formula_8">[w A 1 , w A 2 , ..., w A T A ] and [w B 1 , w B 2 , ..., w B T B ]</formula><p>, the forward LSTM reads the sequence from left to right and the backward LSTM in the opposite direction:</p><formula xml:id="formula_9">! h i A = LST M Cell( ! h i 1 A , W i A ) h i A = LST M Cell( h i+1 A , W i A ) ! h i B = LST M Cell( ! h i 1 B , W i B ) h i B = LST M Cell( h i+1 B , W i B )<label>(7)</label></formula><p>where are hidden representations at position i generated in two directions. We call W A and W B lookup matrix as they learn word features in separate vector spaces, we use them only for sentence summarization. Finally, we concatenate the hidden states at the last time step of each LSTM encoder to represent the sentences:</p><formula xml:id="formula_10">W A 2 R |V A |‚á•d A and W B 2 R</formula><formula xml:id="formula_11">h A = [ ! h T A A ; h 1 A ] h B = [ ! h T B B ; h 1 B ]<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational neural inferer</head><p>Exactly modeling the posterior p(z | x, y) is intractable. Conventional models typically employ the mean field approaches. However, due to its oversimplified assumption, it fails to capture the true posterior distribution of z. Following <ref type="bibr" target="#b6">[Kingma and Welling, 2013]</ref>, we employ neural networks to approximate the posterior distribution of z to get a tighter lower bound and assume the approximator has the following form:</p><formula xml:id="formula_12">q (z | x, y) = N (z; ¬µ(f (h A , h B )), 2 (f (h A , h B ))I) (9)</formula><p>where mean ¬µ and standard variance are outputs of neural networks based on sentence representations h A and h B . Function f projects the separate representations of parallel sentence pairs onto our concerned latent semantic space:</p><formula xml:id="formula_13">h = f (h A , h B ) = g(W z [h A ; h B ] + b z )<label>(10)</label></formula><p>where W z 2 R dz‚á•(d h A +d h B ) and b z 2 R dz are connection matrix and bias respectively, d z is the dimensionality of the latent space, d h A and d h B are dimensionality of outputs of variational encoders. g(‚Ä¢) is elementwise activation function which we set Relu throughout our experiment.</p><p>We obtain the diagonal Gaussian distribution parameter ¬µ and 2 through linear regression:</p><formula xml:id="formula_14">¬µ = W ¬µ h + b ¬µ , log 2 = W h + b (11)</formula><p>where ¬µ, log 2 are both d z dimension vectors. To obtain representations of z, we employ the same technique as VAE and reparameterize it as:</p><formula xml:id="formula_15">z 0 = ¬µ + ‚úè, ‚úè ‚á† N (0, I)<label>(12)</label></formula><p>During encoding, we reconstruct the parallel sentence pairs based on sampled z 0 from q (z | x, y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder Network</head><p>Given the latent variable z, decoder network defines the probability over the observable variables x and y. By sampling z 0 from the posterior approximator q (z | x, y), we are able to reconstruct x and y and estimate the expectation likelihood term. We represent the reconstructed parallel sentence pairs using one-hot BoW representations for the following reasons:</p><p>‚Ä¢ By projecting the common distributed representations of parallel sentence pairs, namely z, to each language's vocabulary, where we introduce two connection matrix E</p><p>A and E B which can be viewed as word embeddings, we obtain cross lingual representations of words embedded in exactly the same vector space.</p><p>‚Ä¢ Acceleration. Large scale learning requires efficient approaches. As our goal is training cross lingual word embeddings instead of machine translation, we employ a softmax decoder by independently generating the words(z ! [{x i }, {y i }]). ‚Ä¢ Vanilla LSTM decoder is sensitive to subtle variation in the hidden states, thus making it hard to train the neural sentence encoder. Inspired by <ref type="bibr" target="#b10">[Miao et al., 2016]</ref>, the conditional probability over observable variables x and y is modelled by multinomial logistic regression with parameter shared across sentence pairs:</p><formula xml:id="formula_16">p ‚úì (x, y | z) =p ‚úì (x | z)p ‚úì (y | z) = |T A | Y i=1 exp{zE A x i + b i A } P |V A | j=1 exp{zE A x j + b i A } ‚Ä¢ |T B | Y i=1 exp{zE B y i + b i B } P |V B | j=1 exp{zE B y j + b i B }<label>(13)</label></formula><p>where</p><formula xml:id="formula_17">E A 2 R dz‚á•|V A | and E B 2 R dz‚á•|V B | learns joint- space semantic</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>word embeddings and b</head><p>A , b B represent the bias terms.</p><p>We use Monte Carlo method to estimate the expectation term over the posterior in Eq.6 which typically has less variance than the generic estimator. The training objective for an instance of parallel sentence pair (x, y) is defined as follows:</p><formula xml:id="formula_18">LBiV AE ( , ‚úì, x, y) = D KL (q (z | x, y) || p(z)) + 1 L L X l=1 (logp ‚úì (x | z 0 l ) + logp ‚úì (y | z 0 l ))<label>(14)</label></formula><p>where</p><formula xml:id="formula_19">z 0 l = ¬µ + ‚úè l , ‚úè l ‚á† N (0, I). L is the number of samples.</formula><p>The first term is the Kullback-Leibler divergence between the posterior and prior distribution of z, which can be integrated analytically <ref type="bibr" target="#b6">[Kingma and Welling, 2013]</ref>. The KLdivergence term can be interpreted as regularizing , encouraging the approximate posterior to be close to the prior p(z).</p><p>And the second term can be interpreted as the negative expected reconstruction error of x and y. Since the objective in Eq.14 is differentiable, we can jointly optimize the model parameters ‚úì and using stochastic gradient optimization. The model is implemented using Tensorflow <ref type="bibr" target="#b0">[Abadi et al., 2016]</ref>, the model parameters of both generative process and posterior estimator are trained jointly using ADAM <ref type="bibr" target="#b5">[Kingma and Ba, 2014]</ref>.</p><p>We also apply dropout and batch normalization <ref type="bibr" target="#b12">[Srivastava et al., 2014;</ref><ref type="bibr">Ioffe and Szegedy, 2015]</ref> to the neural networks introduced in our model to reduce overfitting. We use 200 units for LSTM memory cell and 40 units for latent variable z, consequently 40 units for the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section we present experiments which evaluate the quality of induced cross-lingual representations. We evaluate the embeddings in a standard cross-lingual document classification task which tests semantic transfer of information across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross Lingual Document Classification</head><p>We use an exact replication of the cross-lingual document classification(CLDC) setup introduced by <ref type="bibr" target="#b7">[Klementiev et al., 2012]</ref> to evaluate the embeddings. The CLDC task setup is as follows: A labeled data set of documents in some language A is available to train a classifier, however we are interested in classifying documents in another language B at test time. As Klementiev noted, the aim of this task is not to provide a state-of-the-art document classifier but rather to examine the validity of joint semantic space model. Specifically, we train an averaged perceptron classifier <ref type="bibr" target="#b4">[Freund and Schapire, 1999]</ref> on the labelled training data in the source language and then attempt to apply the classifier to the target data. Documents are represented as the tf-idf weighted sum of the embedding vectors of the words that appear in the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset and Setup</head><p>As our joint space model utilizes parallel corpus only, we train the bilingual embeddings for the English-German language pair using Europarl v7 parallel corpus <ref type="bibr" target="#b8">[Koehn, 2005]</ref>, and use the induced representations to classify a subset of the English and German sections of the Reuters RCV1/RCV2 multilingual corpora <ref type="bibr" target="#b9">[Lewis et al., 2004]</ref> that are assigned to only one of four categories: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets).</p><p>We preprocess the corpus by lowercasing all the words, removing all the punctuations and replacing all the digits with 0. We select the top words by term frequency appearing in the corpus and keep the vocabulary size as | V EN |=40000 and | V DE |= 50000.</p><p>For the classification experiment, 15000 documents(for each language) were selected randomly by <ref type="bibr" target="#b7">Klementiev[Klementiev et al., 2012]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table1 summarizes results on the task of CLDC. We compare the performance of our model with some baselines and previous work. The Majority Class is a system where we simply classify the test documents as the class with the most training examples. The MT is a phrase-based machine translation system where test documents were translated into the same language as training documents. We also summarize some of the previous work. [ <ref type="bibr" target="#b7">Klementiev et al., 2012]</ref> proposed to train two neural network languages models simultaneously along with a regularization term that encourages pairs of frequently aligned words to have similar word embeddings.</p><p>BiCVM <ref type="bibr">[Hermann and Blunsom, 2013]</ref> learned word embeddings via minimizing the compositional representations between parallel sentence pairs. BAE <ref type="bibr" target="#b1">[AP et al., 2014]</ref> reconstructed the bag-of-words representation of semantic equivalent sentence pairs to learn word embeddings. Bi-Skip <ref type="bibr" target="#b10">[Luong et al., 2015]</ref>   tures can not be enhanced with monolingual corpus due to the structure of its probabilistic language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Examples and Visualization</head><p>We find, for each English word, a list of top several English and German words closest to it based on Euclidean distance in a learned joint bilingual vector space. Our list of words includes { january, oil, man, economy, microsoft, president, market, great, communication, law}. Table <ref type="table" target="#tab_4">2</ref> illustrates the properties captured within and across languages. Bilingually, our embeddings succeed in selecting the 1-best translations for all words in the list. Monolingually, our embeddings possess a clearly good clustering structure, which reveals topic nature of the semantic vector sapce. Figure <ref type="figure" target="#fig_2">3</ref> gives a visualization of the joint vector space using t-SNE <ref type="bibr" target="#b10">[Maaten and Hinton, 2008]</ref>. The English and German words which are translations of each other are represented by almost the same point in the vector space, revealing the semantic validity of the joint vector space.</p><p>5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cross-lingual Learning</head><p>Overall, approaches for training bilingual word embeddings can be categorized into several classes: offline mapping, monolingual adaption and parallel training.</p><p>In offline mapping, word representations are first trained on each language independently and a mapping is then learned to transform representations from one language to another. The advantage of this approach is its speed as no further training of word representations is required given monolingual word embeddings. Word embeddings trained in this approach include <ref type="bibr" target="#b11">[Mikolov et al., 2013]</ref>  to learn linear mapping.</p><p>Monolingual adaption jointly optimize the monolingual objectives of each language, with cross-lingual objective enforced as cross-lingual regularizer. One advantage of this approach is that it utilizes monolingual corpus in addition to parallel corpus to enhance the learned features.</p><p>Unlike previous schemes fix representations on either one or both sides, Parallel training leverage sentence aligned parallel corpus only and train a model to learn similar representations for aligned sentences. <ref type="bibr">[Hermann and Blunsom, 2013;</ref><ref type="bibr" target="#b1">AP et al., 2014]</ref> and the model proposed in this paper follow this approach. One advantage of this approach is that models can be designed to train word representations embedded in exactly the same continuous vector space, avoiding explicit alignment.</p><p>As introduced in section 1, cross-lingual word embeddings can be applied to various NLP tasks, including semantic applications such as cross-lingual dictionary induction <ref type="bibr">[Vulic and Moens, 2013a;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013]</ref> and CLDC <ref type="bibr" target="#b7">[Klementiev et al., 2012]</ref> as well as syntactic applications such as cross-lingual syntactic dependency parsing <ref type="bibr" target="#b14">[T√§ckstr√∂m et al., 2012]</ref> and lexicon extraction <ref type="bibr">[Vulic and Moens, 2013b]</ref>. We prove our embeddings effective in CLDC task but its application remains to be explored in other cross-lingual NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variational Neural Inference</head><p>In order to perform efficient inference and learning in generative probabilistic models on large-scale dataset, variational autoencoder was recently proposed by <ref type="bibr" target="#b6">[Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014]</ref>. Different from conventional mean field approximation, VAE employs neural network to approximate the posterior distribution of latent variable and optimize the model parameters with a reparameterized variational lower bound using the stochastic gradient optimization technique. Following Kingma, semi-supervised VAE has been proposed to model labeled dataset. Variational RNN has been proposed to deal with sequential data, which have been proved successful in speech modeling.</p><p>Variational neural inference has also shown strong performance in text processing. <ref type="bibr" target="#b10">[Miao et al., 2016]</ref> proposes a generic variational inference framework for generative and conditional models of text. <ref type="bibr" target="#b3">[Bowman et al., 2015]</ref> imposes a prior distribution on the hidden states of a standard RNN language model, helping generating sentences from the latent semantic space. <ref type="bibr" target="#b15">[Zhang et al., 2016]</ref> introduces a latent variable z to a standard neural machine translation framework to guide the generation of target translations. To the best of our knowledge, we are the first to introduce this technique to learn cross-lingual word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a variational encoder-decoder framework for cross-lingual learning. By introducing the hidden variable z, we learn cross-lingual word embeddings in exactly the same continuous vector space instead of projecting them from separate spaces. We also conduct a standard CLDC task to evaluate BiVAE. Experiment results show that BiVAE performs comparably with the previous reported state-of-the-art model.</p><p>For future work, we are interested in modifying the variational neural decoder, e.g. LSTM decoder, to generate plausible parallel sentence pairs from the latent semantic space. With limited training corpus, we can train BiVAE to help generate more semantic equivalent sentence pairs to enrich the corpus in an unsupervised way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of BiVAE as a directed graph model. x and y denote the observable semantic equivalent sentence pairs. We use solid lines to denote generative model p ‚úì (x | z), p ‚úì (y | z) and dashed lines to denote the variational inference approximation q (z | x, y). Both ‚úì and are trainable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of core structure of BiVAE. Variational inference network approximates the posterior distribution of z based on representation h encoded by bidirectional LSTM encoder. Decoder network projects the common hidden codes to BoW representation of observable variables x and y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of joint semantic space using t-SNE. We represent 10 high frequency En-De word translation pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>from RCV1/RCV2 corpus. One third Accuracy of cross lingual document classification. We compare BiVAE embeddings to the best models from past work. Data denotes the corpus utilized for inducing bilingual word embeddings. Numbers in boldface highlight the best scores per metric of the selected documents(5000) were used as test sets and a varying size between 100 and 10000 of the remainder were used as training set. Another 1000 documents were kept as development set for hyper-parameter tuning. A multi class document classifier was trained for 10 epoch with English documents and is used to classify German documents and vice versa.</figDesc><table><row><cell>Model Majority Baseline</cell><cell>Data -</cell><cell cols="2">en ! de de ! en 46.8 46.8</cell></row><row><cell>MT Baseline</cell><cell>Europarl</cell><cell>68.1</cell><cell>67.4</cell></row><row><cell cols="2">Klementiev et al. Europarl+RCV</cell><cell>77.6</cell><cell>71.1</cell></row><row><cell>BiCVM</cell><cell>Europarl</cell><cell>83.7</cell><cell>71.4</cell></row><row><cell>BAE</cell><cell>Europarl+RCV</cell><cell>91.8</cell><cell>74.2</cell></row><row><cell>BilBOWA</cell><cell>Europarl+RCV</cell><cell>86.5</cell><cell>75.0</cell></row><row><cell>BiSkip CLSim BiVAE</cell><cell>Europarl Europarl+RCV Europarl</cell><cell>90.7 92.7 91.0</cell><cell>80.3 80.1 80.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>extended skip-gram model to bilingual circumstances where separate context of aligned word pairs were jointly predicted.<ref type="bibr" target="#b11">[Shi et al., 2015]</ref> proposed a matrix cofactorization framework, CLSim, for learning cross lingual embeddings.From Table1 we conclude that our proposed model outperforms most previous work and performs comparably with the previous state-of-the-art model, CLSim, with BiVAE tops the accuracy of submission de ! en. But note that our model is trained with Europarl corpus only while CLSim induce the embeddings using both Europarl and RCV corpus.Different from those approaches which start from monolingual objective learning embeddings in separate vector space following cross lingual objective as constraints, BiVAE learns word representations embedded in exactly the same vector space without the help of explicit alignments. While on the other hand, one drawback of BiVAE is that the learned fea-</figDesc><table><row><cell>Word</cell><cell>Language</cell><cell>Nearest neighbours</cell></row><row><cell>law</cell><cell>En De</cell><cell>law, rule gesetz, rechtsstaat</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Example English words with nearest words in English (En) and German (De) measured by Euclidean distance.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>january En january, february, march De januar, j√§nner, april oil En oil, crude, slick De √ñl, erd√∂l, roh√∂l man En man, woman, person De mann, mensch, mannes economy En economy, economics De wirtschaft, weltwirtschaft microsoft En microsoft, google, intel De microsoft, google, intel president En president, mr De pr√§sident, herr market En market, internal De markt, binnenmarkt great En great, deal, huge De gro√ües, gro√üen, enorme communication En communication, feedback De kommunikation, mitteilung</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is partially supported by the <rs type="funder">National High Technology Research and Development Program of China</rs> (Grant No. <rs type="grantNumber">2015AA015403</rs>). We would also like to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8CNxCVm">
					<idno type="grant-number">2015AA015403</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName><surname>Ap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">2003. Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Faruqui and Dyer, 2014] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2014">2015. 2015. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Generating sentences from a continuous space. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName><forename type="first">Schapire</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Gouws</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2455</idno>
		<idno>arXiv:1502.03167</idno>
	</analytic>
	<monogr>
		<title level="m">Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<publisher>Ioffe and Szegedy</publisher>
			<date type="published" when="1999">1999. 1999. 2014. 2013. 2013. 2015. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="277" to="296" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bilbowa: fast bilingual distributed representations without word alignments</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ba</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Welling</forename><forename type="middle">;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName><surname>Klementiev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04">2004. Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne</title>
		<author>
			<persName><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008-11">2015. 2015. 2008. Nov. 2008. 2016. 2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
	<note>Proc. ICML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<idno>arXiv:1401.4082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2011">2013. 2013. 2014. 2015. 2015. 2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ACL. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bilingual learning of multi-sense embeddings with discrete autoencoders</title>
		<author>
			<persName><surname>≈†uster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09128</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vulic and Moens, 2013a] Ivan Vulic and Marie-Francine Moens. Cross-lingual semantic similarity of words as the similarity of their semantic word responses</title>
		<author>
			<persName><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01891</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</title>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>T√§ckstr√∂m</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><surname>Uszkoreit</surname></persName>
		</editor>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012">2016. 2016. 2012. 2012. 2013. 2013. 2013</date>
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07869</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2016. 2016. 2013. 2013</date>
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Variational neural machine translation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
