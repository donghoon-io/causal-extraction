<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian model and dimension reduction for uncertainty propagation: applications in random media *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-03-28">28 Mar 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Constantin</forename><surname>Grigo</surname></persName>
							<email>constantin.grigo@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phaedon-Stelios</forename><surname>Koutsourelakis</surname></persName>
							<email>p.s.koutsourelakis@tum.de.1</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian model and dimension reduction for uncertainty propagation: applications in random media *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-28">28 Mar 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1711.02475v2[stat.ML]</idno>
					<note type="submission">* Submitted to the editors March 29, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian</term>
					<term>model-order reduction</term>
					<term>dimensionality reduction</term>
					<term>Stochastic Variational inference</term>
					<term>sparsity</term>
					<term>random media AMS subject classifications. 62P30</term>
					<term>62C10</term>
					<term>78M34</term>
					<term>65C20</term>
					<term>35R60</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Well-established methods for the solution of stochastic partial differential equations (SPDEs) typically struggle in problems with high-dimensional inputs/outputs. Such difficulties are only amplified in large-scale applications where even a few tens of full-order model runs are impracticable. While dimensionality reduction can alleviate some of these issues, it is not known which and how many features of the (high-dimensional) input are actually predictive of the (high-dimensional) output. In this paper, we advocate a Bayesian formulation that is capable of performing simultaneous dimension and model-order reduction. It consists of a component that encodes the high-dimensional input into a low-dimensional set of feature functions by employing sparsity-inducing priors and a decoding component that makes use of the solution of a coarse-grained model in order to reconstruct that of the full-order model. Both components are represented with latent variables in a probabilistic graphical model and are simultaneously trained using Stochastic Variational Inference methods. The model is capable of quantifying the predictive uncertainty due to the information loss that unavoidably takes place in any model-order/dimension reduction as well as the uncertainty arising from finite-sized training datasets. We demonstrate its capabilities in the context of random media where fine-scale fluctuations can give rise to random inputs with tens of thousands of variables. With a few tens of full-order model simulations, the proposed model is capable of identifying salient physical features and produce sharp predictions under different boundary conditions of the full output which itself consists of thousands of components.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. One of the most difficult obstacles in the application of uncertainty quantification methods in large-scale engineering problems pertains to the poor scalability of uncertainty propagation tools in high dimensions. The golden standard for such problems i.e. Monte Carlo, exhibits convergence rates that are independent of the dimension of the random input (and output). Nevertheless, for computationally intensive models for which only 10 or 100 runs can be practicably performed, it is of paramount importance to decrease as much as possible the number of simulations needed. This can only be achieved if one can extract sufficient knowledge from the few simulations that can be carried out in order to infer the quantities of interest <ref type="bibr" target="#b63">[64]</ref>.</p><p>One obvious strategy in overcoming these limitations is the use of surrogates or emulators that are trained on a limited number of runs and can subsequently substitute the forward model. Amongst existing methods for uncertainty propagation, those based on (generalized) polynomial chaos expansions (gPC, <ref type="bibr" target="#b80">[81]</ref>) have grown into prominence in recent years with the development of non-intrusive, stochastic collocation approaches <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b42">43]</ref>. More recent efforts have employed Gaussian Processes (GPs, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>) or multivariate regression schemes <ref type="bibr" target="#b6">[7]</ref>. While all these tools are highly expressive and can potentially approximate sufficiently well the sought input-output map, they exhibit significant limitations in high input dimensions (e.g. in the hundreds), an instantiation of the well-documented curse of dimensionality <ref type="bibr" target="#b14">[15]</ref>. One could argue that employing larger, more flexible emulators, e.g. as those arising in the context of Deep Neural Networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, could overcome such problems. We emphasize though that uncertainty propagation problems in computational physics and engineering are not Big Data problems <ref type="bibr" target="#b37">[38]</ref> and minimizing the number of training data generated by running the full-order simulator is the primary objective.</p><p>A more recent trend to the problem has been based on the use of less-expensive, lowerfidelity models in order to provide accurate estimates of the higher-fidelity quantities of interest <ref type="bibr" target="#b30">[31]</ref>. When combined with statistical learning procedures, such formulations can also yield quantitative estimates of the confidence in the predictions produced <ref type="bibr" target="#b36">[37]</ref>. One of the strengths of such tools stems from the use of lower-fidelity models that retain some of the underlying physics and as such produce outputs that are strongly correlated/dependent with the high-fidelity ones <ref type="bibr" target="#b60">[61]</ref>. The systematic construction of such lower-fidelity or, more generally, reduced-order models, has also received a lot of attention. A prominent role in these efforts, at least in the context of PDE-based models, is held by reduced-basis techniques <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b28">29]</ref> which are based on the identification of a low-dimensional linear subspace in the solution vector space on which a Galerkin projection of the governing equations is attempted <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b15">16]</ref>. Naturally such an assumption ceases to hold as higher-dimensional inputs are considered and various strategies have been adopted to address this limitation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The potential of dimensionality-reduction methods in overcoming the curse of dimensionality has also been demonstrated by employing data-driven, nonlinear, manifold learning techniques (e.g. <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b65">66]</ref>) that have been developed in the context of statistics and machine learning applications, in truly high-dimensional problems in computational physics <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84]</ref>. One set of applications which really pushes the limits of existing uncertainty propagation techniques, as well as being of significant engineering interest, involves random heterogeneous media <ref type="bibr" target="#b76">[77]</ref>. The macroscale properties of composites (e.g. fiber-reinforced) or polycrystalline materials (e.g. alloys) depend strongly on the underlying microstructure. The latter is characterized by significant randomness which invariably implies gigantic numbers of random variables <ref type="bibr" target="#b35">[36]</ref> and must be propagated across different length scales <ref type="bibr" target="#b59">[60]</ref> in the context of simulation-based analysis and design <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b85">86]</ref>. Despite recent significant progress in the development of hierarchical <ref type="bibr" target="#b51">[52]</ref> and concurrent <ref type="bibr" target="#b49">[50]</ref> deterministic multiscale methodologies, most formulations rely on scale separation arguments and the existence of Representative Volume Elements (RVE). However their size, the boundary conditions that must be employed on the RVE in order to extract effective properties are not necessarily uniquely determined nor is their effect in the macroscale response <ref type="bibr" target="#b57">[58]</ref>. Furthermore, only a small portion of this work has been directed to stochastic/probabilistic multiscale problems <ref type="bibr" target="#b13">[14]</ref> and even less, to strategies that would be applicable to high-dimensional, non-Gaussian uncertainties encountered in materials problems <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In this paper we propose a Bayesian formulation for the construction of reduced-order descriptions for PDE-based models, capable of dealing with high-dimensional stochastic inputs in the coefficients as is the case for example in random media or problems which are characterized by stochastic spatial variability. It consists of two basic ingredients: a) a (latent) coarse-grained version of the full-order PDE, and b) a (latent) coarse-to-fine map that relates the outputs of the two models. We note that coarse-grained models serve as a stencil for the construction of the reduced description that retain a priori the salient physical features of the full-order description. They are parametrized by a lower-dimensional set of variables which provide localized, predictive summaries of the underlying high-dimensional random input. Such a model unavoidably compromises the informational content of the stochastic full-order model and is in general incapable of providing perfect predictions. To that end, it is complemented by a probabilistic map that relates the outputs of the coarse-grained model to the desired outputs of the full-order one. In contrast to existing techniques that perform the dimensionality reduction of the input and the construction of the emulator to the output in two separate steps <ref type="bibr" target="#b43">[44]</ref>, both of these components are trained simultaneously in the framework advocated. As a result it is ensured that only low-dimensional features of the input that are predictive of the response (and not of the input itself) are learned and retained.</p><p>We employ a Stochastic Variational Inference scheme <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b29">30]</ref> in order to train the proposed model. This is combined with appropriate prior specifications that promote the discovery of a sparse set of features that maximally compress the random input <ref type="bibr" target="#b20">[21]</ref>. The hierarchical nature of the model allows it to learn from a limited number of full-order runs (in the examples performed these range from 10 to 100). Its Bayesian nature yields probabilistic predictions of the full-order outputs (independently of their dimension) that reflect not only the unavoidable information loss mentioned earlier, but also the effect of learning from a finite (and small) dataset.</p><p>The remainder of the paper is organized as follows: In Section 2, we present the essential ingredients and provide algorithmic details for the inference and learning processes. In Section 3, we present numerical illustrations in the context of high-dimensional elliptic, stochastic PDEs and conclude in Section 4 with some possible extensions involving adaptive refinement and the use of multiphysics models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology.</head><p>In general, we use the subscript 'f ' to denote quantities pertaining to the (high-dimensional) full-order model and the subscript 'c' for quantities associated with the (lower-dimensional) coarsened/reduced-order description. We begin with the presentation of the full-order model (FOM) and subsequently explain the essential ingredients of the proposed formulation.</p><p>2.1. SPDE's with random coefficients and the full-order model. In the modeling of physical systems, material properties such as electrical or thermal conductivity, elastic moduli or fluid permeability are only known up to a stochastic level. We denote by λ(x) a scalar (without loss of generality), random field describing any of these properties where x is the spatial variable in the problem domain D and consider a governing PDE of the form (1)</p><p>A(x, λ(x))u(x, λ(x)) = 0, for x ∈ D</p><p>where A(x, λ(x)) is some differential operator (to be specialized in Section 3) and u(x; λ(x)) is the sought solution field. Since the method proposed is data-driven, we will not be concerned with the particulars of the solution of the governing equations which are generally complemented with appropriate boundary conditions. We simply make use of the discretized versions λ f ∈ R N el,f and u f ∈ R N dof,f of the coefficient random field λ(x) and the solution u(x), respectively. We also denote by u f (λ f ) the deterministic map implied by the solution of the discretized PDE which gives the solution vector for each λ f . We note that the scale of spatial variability of λ(x) in many random media necessitates sufficiently fine discretizations of the governing PDE in order to accurately represent the solution. As a consequence, the resulting algebraic system of equations is high-dimensional and cumbersome to solve repeatedly. In the cases considered, both the dimensions of the random input and solution vectors λ f , u f are thus assumed high, i.e. N el,f , N dof,f &gt;&gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.</head><p>A Bayesian reduced-order model. Any attempt to construct an emulator of the input-output map u f (λ f ) on the basis of a finite set D = λ</p><formula xml:id="formula_0">(n) f , u (n) f N n=1</formula><p>of FOM evaluations is faced with the following difficulties:</p><p>• the high input dimension N el,f = dim(λ f ) corresponding to the fine scale discretization of the coefficient random field λ(x) in relation to the available data N . This is known as the "large p, small N " paradigm in statistics <ref type="bibr" target="#b79">[80]</ref> where p refers to N el,f = dim(λ f ); • the prohibitive cost of enlarging the data set size N ; and • the high dimension N dof,f = dim(u f ) of the discretized solution/output vector u f . It is therefore imperative to employ emulators that encode as much as possible a priori information from the FOM which, as such, do not require data to be learned. Secondly, it is essential to identify a low-dimensional set of features of the input λ f that are nevertheless predictive of the output <ref type="bibr" target="#b81">[82]</ref> and can be learned from the few data available. In the context of deterministic materials' microstructures for example, several upscaling tools have been developed which substitute the high-dimensional microstructures by a low-dimensional set of effective properties <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. Thirdly, it is important to enable effective dimensionality reductions of the output u f that are seamlessly incorporated with the previous two aspects.</p><p>We propose a three-component reduced-order model (ROM) that encapsulates the aforementioned desiderata and consists of the following steps (Figure <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b24">[25]</ref>):</p><p>• a probabilistic mapping from the high-dimensional λ f to a lower-dimensional, coarsegrained representation λ c (dim(λ c ) dim(λ f )). This mapping is mediated by the density p c (λ c |λ f , θ c ) parametrized by θ c ; • a coarser discretization of the original PDE where u c is the solution vector (dim(u c ) dim(u f )). We denote by u c (λ c ) the deterministic input-output mapping implied by this model; and • a probabilistic coarse-to-fine mapping from the output u c of the coarse model to the output of the FOM u f . We denote this with the density p cf (u f |u c , θ cf ) which is parametrized by θ cf . The combination of these three components yields the following conditional density: Next, the PDE is solved using a (much) coarser discretization. Finally, the FOM solution vector u f is reconstructed from the coarse one, u c → u f . where we used the fact that p cm (u c |λ c ) = δ(u c -u c (λ c )). The combination of the latent (unobserved) variables λ c , u c with the model parameters θ c , θ cf yield a probabilistic graphical model <ref type="bibr" target="#b34">[35]</ref> which is formally depicted in Figure <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_1">p(u f |λ f , θ cf , θ c ) = p cf (u f |u c , θ cf ) decoder p cm (u c |λ c ) coarse model p c (λ c |λ f , θ c ) encoder du c dλ c = p cf (u f |u c (λ c ), θ cf )p c (λ c |λ f , θ c )dλ c ,<label>(2)</label></formula><p>The latent variables λ c can be interpreted as a probabilistic filter (encoder) on the FOM input λ f . By solving the coarse model, these are inexpensively transformed to u c which are finally decoded to predict the FOM output u f . It is important to note that in order for p(u f |λ f , θ cf , θ c ) to approximate well the reference density p ref (u f |λ f ) = δ(u f -u f (λ f )), it is irrelevant if the latent variables λ c provide a high-fidelity encoding of λ f in the sense of being able to reconstruct λ f . Rather, λ c must be predictive (through u c ) of the FOM response u f . Hence the λ c implied in our model might be very different from the reduced coordinates identified by a (non)linear dimensionality reduction tool applied directly on λ f (or samples thereof ) <ref type="bibr" target="#b75">[76]</ref>.</p><p>Furthermore, we remark that, in general, and if no redundancies in λ f are present, the coarse-graining process effected in the proposed model will unavoidably result in some information loss, i.e. for dim(λ c ) &lt;&lt; dim(λ f ) there is an upper bound on the mutual information I(λ c , λ f ) ≤ I 0 . Consequently, there will be uncertainty in the predictions produced by the ROM which we attempt to capture with the aforementioned densities. We note that this source of uncertainty is independent of the uncertainty arising from the finite dataset which we account for in a Bayesian formulation as discussed in the sequel.</p><p>The decoding density p cf (u f |u c (λ c ), θ cf ) maps the coarse response vector u c to its finescale counterpart u f , where dim(u c ) dim(u f ). As a result, p cf plays the role of a generative model for dimensionality reduction <ref type="bibr" target="#b74">[75]</ref> of the FOM output. While many other possibilities exist, given the spatial character of the problems considered, one would expect that this component plays the role of an interpolant, i.e. it attempts to reconstruct each u f,i associated with point x i by employing the coarse-model outputs u c,j , potentially associated with points x j in the vicinity of x i .</p><p>We finally note that the coarse model u c (λ c ) is used as the central building block of the reduced-order model constructed. Its form determines to a large extent the meaning of the latent variables λ c employed and their association with λ f through p c . Apart from the necessary requirement that it is much less expensive to evaluate than the FOM, one could envisage in its place models accounting for different physics than the FOM, or parametrized models as in the case of reduced-basis techniques (where these parameters would need to be trained in conjunction with θ c , θ cf ) or even stochastic models (in which case the full p cm would need to be employed in Equation <ref type="formula" target="#formula_1">2</ref>).</p><p>In the sequel we discuss the specifics of the building components and of the densities p c , p cf in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.</head><p>The coarse-graining distribution p c . We denote by k the index of each macro-cell or macro-element in the discretization of the coarse model (see Figure <ref type="figure" target="#fig_0">1</ref>). We postulate a relationship of the form 1</p><formula xml:id="formula_2">(3) λ c,k = N features j=1 θc,jk ϕ jk (λ f ) + σ c,k Z k , Z k ∼ N (0, 1), where ϕ k (λ f ) = {ϕ jk (λ f )} N features j=1</formula><p>is a set of predefined feature functions which attempt to filter relevant information of λ f in order to find a λ c which is most predictive for the reconstruction of u f . These are combined with weights θc,k = { θc,jk } N features j=1 and a residual 1 Often, there are physical bounds of type λ &gt; 0 or λ lo ≤ λ ≤ λ hi on the random field λ = λ(x, ξ(x)). This should be reflected in the regression model on λc and can be realized with a link function λ c,k = χ(z k ) where χ : R → D λ with D λ the admissible domain for λ. In such a case all instances of λ c,k in the subsequent equations should be substituted by z k .</p><p>noise with variance σ c,k which represents the uncertainty in λ c,k . The resulting p c is (4)</p><formula xml:id="formula_3">p c (λ c |λ f , θ c ) = dim(λc) k=1 N (λ c,k | θT c,k ϕ k (λ f ), σ 2 c,k ), hence θ c = { θc,k , σ 2 c,k } dim(λc)<label>k=1</label></formula><p><ref type="foot" target="#foot_0">foot_0</ref> . Naturally, different numbers of feature functions N features can be employed for each k. Using suitable features is a crucial aspect of the expressivity of the model. We provide a detailed list in Appendix A and note that these consist of various statistical descriptors. Some of these convey physical information of the problem, i.e. they should include topological descriptors <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b40">41]</ref> as well as homogenization-based quantities <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b76">77]</ref>. Others however are based on image recognition tools <ref type="bibr" target="#b67">[68]</ref> or even autoencoder representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b71">72]</ref>. We finally note that employing large numbers of feature functions (as we do in this study) poses important model selection issues which we discuss in Section 2.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.</head><p>The coarse-to-fine map p cf . This provides a generative interpretation of highdimensional output u f by employing the (latent) coarse model output u c as shown schematically in the third step of Figure <ref type="figure" target="#fig_0">1</ref>. In this study, we employ a linear model of the form ( <ref type="formula">5</ref>)</p><formula xml:id="formula_4">p cf (u f |u c (λ c ), θ cf ) = N (u f |W u c + b, S),</formula><p>where we denote the model parameters uc) is a projection matrix and S the covariance. To ensure that the number of unknown parameters scales linearly with the dimension of the FOM output u f , we employ a diagonal S. Furthermore, and in order to reduce the amount of data needed, we exploit the spatial characteristics of the problem in order to restrict the number of free parameters in W , b as discussed in Section 3.</p><formula xml:id="formula_5">θ cf = {W , b, S}. We note that b ∈ R dim(u f ) is a bias vector, W ∈ R dim(u f )×dim(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Model training.</head><p>Given the aforementioned components of the proposed model, we discuss the calibrations of the model parameters θ = {θ cf , θ c } on the basis of a set of</p><formula xml:id="formula_6">N FOM observations D = λ (n) f , u (n) f N n=1</formula><p>. Following the Bayesian paradigm, the plausibility for a certain parameter value θ is given by the posterior <ref type="bibr" target="#b5">(6)</ref> p(θ|D) ∝ L(D|θ)p(θ),</p><p>where p(θ) is a model prior to be specified and ( <ref type="formula">7</ref>)</p><formula xml:id="formula_7">L(D|θ) = N n=1 p(u (n) f |λ (n) f , θ)</formula><p>is the likelihood function. We note that maximizing the log-likelihood with respect to θ is equivalent to minimizing the Kullback-Leibler divergence <ref type="bibr" target="#b8">[9]</ref> between the reference density</p><formula xml:id="formula_8">p ref (u f |λ f ) = δ(u f -u f (λ f )) and the model-implied density p(u f |λ f , θ cf , θ c ) in (2)</formula><p>. The latter however implies an integration w.r.t. λ c which despite the form of p c and p cf is analytically intractable due to the dependence on the coarse model output u c (λ c ). Furthermore, due to the dimensionality of the model parameters (particularly θ cf ) we adopt a hybrid strategy which is based on the computation of the Maximum a Posteriori estimate θ MAP of θ,</p><formula xml:id="formula_9">(8) θ MAP = arg max θ p(θ|D)</formula><p>and the use of Laplace approximations to approach the true posterior <ref type="bibr" target="#b45">[46]</ref>. Hence, in Section 2.5.1 we put forth a Variational Expectation-Maximization scheme <ref type="bibr" target="#b1">[2]</ref> for the efficient computation of θ MAP . Particular aspects that pertain to the prior specifications are presented in Section 2.5.2 and in Section 2.6 the use of the trained model in producing probabilistic predictive estimates is discussed.</p><p>2.5.1. Maximizing the posterior. Equations ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula">7</ref>) lead to</p><formula xml:id="formula_10">(9) p(θ|D) ∝ p(θ) • N n=1 p cf (u (n) f |u c (λ (n) c ), θ cf )p c (λ (n) c |λ (n) f , θ c )dλ (n) c ,</formula><p>where p(θ) denotes the prior on the model parameters θ = {θ cf , θ c }. In order to carry out the maximization of the intractable objective we resort to the Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b16">[17]</ref>. Based on Jensen's inequality, we can lower-bound the log likelihood</p><formula xml:id="formula_11">L(D|θ cf , θ c ) (7) as log L(D|θ cf , θ c ) = N n=1 log p cf (u (n) f |u c (λ (n) c ), θ cf )p c (λ (n) c |λ (n) f , θ c )dλ (n) c ≥ N n=1 q n (λ (n) c ) log   p cf (u (n) f |u c (λ (n) c ), θ cf )p c (λ (n) c |λ (n) f , θ c ) q n (λ (n) c )   dλ (n) c = N n=1 F (n) (q n (λ (n) c ); θ) = F( q n (λ (n) c ) N n=1 ; θ),<label>(10)</label></formula><p>where q n (λ</p><formula xml:id="formula_12">(n)</formula><p>c ) are arbitrary probability densities. Consequently, the log posterior (9) has the lower bound <ref type="bibr" target="#b10">(11)</ref> log p(θ|D</p><formula xml:id="formula_13">) ≥ F( q n (λ (n) c ) N n=1 ; θ) + log p(θ).</formula><p>The basic idea behind the EM-algorithm is to maximize iteratively the lower-bound <ref type="bibr" target="#b10">(11)</ref> with respect to parameters θ and the auxiliary distributions q n (λ</p><formula xml:id="formula_14">(n) c ) N n=1</formula><p>. One can readily verify that for a given value of θ = θ (t) , the optimal q n 's are given by the posterior of each λ</p><formula xml:id="formula_15">(n) c , i.e. (<label>12</label></formula><formula xml:id="formula_16">) q opt n (λ (n) c ) = p n (λ (n) c |θ (t) ) ∝ p cf (u (n) f |u c (λ (n) c ), θ (t) cf )p c (λ (n) c |λ (n) f , θ (t) c ).</formula><p>In this case the lower-bound becomes tight and the inequality in <ref type="bibr" target="#b10">(11)</ref> turns into an equality. The previous suggests the following maximization process whereby at each iteration t one alternates between: E-step: Given the current parameter values θ (t) , find the q</p><formula xml:id="formula_17">(t+1) n (λ (n) c ) that maximize F( q n (λ (n) c ) N n=1</formula><p>; θ (t) ) (see Equation <ref type="formula" target="#formula_15">12</ref>). M-step: Given the current expected values . q (t+1) n , maximize the posterior lower bound ( <ref type="formula">13</ref>)</p><formula xml:id="formula_18">θ (t+1) = arg max θ F( q (t+1) n (λ (n) c ) N n=1 ; θ) + log p(θ)</formula><p>to find the next best estimates θ (t+1) .</p><p>The iterations are repeated until a suitable convergence criterion on the parameters θ is met. Partial or incomplete updates can readily be performed and could potentially lead to computational benefits <ref type="bibr" target="#b53">[54]</ref>.</p><p>Stochastic Variational Inference during the E-step. We emphasize that no further FOM runs (apart from those performed to generate the training data D) are needed in any of the steps above but note that the E-step is analytically intractable due to the dependence on the coarse model outputs u c (λ</p><formula xml:id="formula_19">(n) c</formula><p>). In order to avoid employing Monte Carlo sampling schemes (e.g. MCMC, SMC) which, despite the unbiased estimates they produce, are not as efficient in terms of the number of times u c (λ c ) needs to be evaluated, we propose employing an approximate inference scheme that relies on Stochastic Variational Inference (SVI) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b29">30]</ref>. These yield suboptimal approximations to the densities needed in the E-step which are nevertheless shown to be sufficient for accurate estimation of θ MAP <ref type="bibr" target="#b10">[11]</ref>. To that end, we employ a family of densities q n,ξ n (λ (n) c ) parametrized by ξ n and seek their optimal values in terms of maximizing the variational lower-bound F (n) . In particular, at each iteration (i.e. given θ (t) ) and for each n, we seek 3 <ref type="bibr" target="#b13">(14)</ref> ξ n = arg max</p><formula xml:id="formula_20">ξ n F (n) V I (ξ n )</formula><p>where</p><formula xml:id="formula_21">F (n) V I (ξ n ) = F (n) (q n,ξ n (λ (n) c ); θ) = q n,ξ n (λ (n) c ) log   p cf (u (n) f |u c (λ (n) c ), θ cf )p c (λ (n) c |λ (n) f , θ c ) q n,ξ n (λ (n) c )   dλ (n) c = log p cf (u (n) f |u c (λ (n) c ) q n,ξ n (λ (n) c ) + log p c (λ (n) c |λ (n) f , θ c ) q n,ξ n (λ (n) c ) + H(q n,ξ n (λ (n) c ))<label>(15)</label></formula><p>where . q n,ξ n (λ (n) c ) imply expectations with respect to q n,ξ n (λ</p><formula xml:id="formula_22">(n) c ) and H(q n,ξ n (λ (n) c</formula><p>) is the corresponding Shannon entropy. Since the derivatives of the objective above with respect to ξ n involve expectations with respect to q n,ξ n (λ</p><formula xml:id="formula_23">(n) c</formula><p>) and in order to minimize the variance 3 It can be shown that the optimization problem in Equation 15 is equivalent to minimizing the Kullback-Leibler divergence between q n,ξ n (λ</p><formula xml:id="formula_24">(n) c ) and q opt n (λ (n) c</formula><p>) given in Equation <ref type="formula" target="#formula_15">12</ref>.</p><p>in these estimates, we employ the reparametrization trick <ref type="bibr" target="#b32">[33]</ref>. In particular, for the family of multivariate Gaussians q n,ξ n (λ</p><formula xml:id="formula_25">(n) c ) = N (λ (n) c |µ (n) V I , Σ (n) V I ) where ξ n = {µ (n) V I , Σ<label>(n)</label></formula><p>V I }<ref type="foot" target="#foot_1">foot_1</ref> , the reparametrization trick consists of expressing λ</p><formula xml:id="formula_26">(n) = µ (n) V I + Σ (n) V I (n) where (n) ∼ N (0, I).</formula><p>Upon substitution in the objective of <ref type="bibr" target="#b14">(15)</ref>, we obtain:</p><formula xml:id="formula_27">F (n) V I (ξ n ) = log p cf (u (n) f |u c (µ (n) V I + Σ (n) V I (n) )) N ( (n) |0,I) + log p c (µ (n) V I + Σ (n) V I (n) |λ (n) f , θ c ) N ( (n) |0,I) + H(q n,ξ n (λ (n) c )). (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>Given that (up to a constant) H(q n,ξ n (λ</p><formula xml:id="formula_29">(n) c )) = 1 2 log |Σ (n)</formula><p>V I | and after application of the chain rule we obtain the following derivatives:</p><formula xml:id="formula_30">∂F (n) V I ∂µ (n) V I = ∂ log p cf ∂u c ∂u c ∂λ c N ( (n) |0,I) + ∂ log p c ∂λ c N ( (n) |0,I) ∂F (n) V I ∂ Σ (n) V I = ∂ log p cf ∂u c ∂u c ∂λ c ( (n) ) T N ( (n) |0,I) + ∂ log p c ∂λ c ( (n) ) T N ( (n) |0,I) + (Σ (n) V I ) -1 .<label>(17)</label></formula><p>If not given in closed form, the expectations above with respect to (n) are estimated with Monte Carlo and the (noisy) derivatives are used to update ξ n in conjunction with the ADAM stochastic optimization method <ref type="bibr" target="#b31">[32]</ref>. We note finally that the gradients above involve derivatives of the coarse model's output w.r.t. the coefficients λ c , ∂uc ∂λc . These can efficiently be obtained given the size of the model by solving the adjoint equations (see e.g. <ref type="bibr" target="#b27">[28]</ref>).</p><p>M-step: model parameter updates. For maximization of the posterior lower bound, we use gradients of F from Equation <ref type="formula" target="#formula_11">10</ref>,</p><formula xml:id="formula_31">∇ θ cf F( q (t+1) n (λ (n) c ) N n=1 ; θ cf , θ c ) = N n=1 ∇ θ cf log p cf (u (n) f |u c (λ (n) c ), θ cf ) q (t+1) n ,<label>(18)</label></formula><formula xml:id="formula_32">∇ θc F( q (t+1) n (λ (n) c ) N n=1 ; θ cf , θ c ) = N n=1 ∇ θc log p c (λ (n) c |λ (n) f , θ c ) q (t+1) n . (<label>19</label></formula><formula xml:id="formula_33">)</formula><p>Given the model densities p cf (u f |u c (λ c ), θ cf ) (Equation ( <ref type="formula">5</ref>)), p c (λ c |λ f , θ c ) (Equation ( <ref type="formula">4</ref>)), we obtain:</p><formula xml:id="formula_34">∇ W F = S -1 N n=1 (u (n) f -b) u T c (λ (n) c ) q (t+1) n -W u c (λ (n) c )u T c (λ (n) c ) q (t+1) n ,<label>(20)</label></formula><formula xml:id="formula_35">∇ b F = S -1 N n=1 u (n) f -W u c (λ (n) c ) q (t+1) n -N b ,<label>(21)</label></formula><formula xml:id="formula_36">∇ S F = S -1 2 N n=1 (u (n) f -b -W u (n) c )(u (n) f -b -W u (n) c ) T q (t+1) n S -1 -N ,<label>(22)</label></formula><formula xml:id="formula_37">∇ θc F = N n=1 Φ T (λ (n) f )Σ -1 c λ (n) c q (t+1) n -Φ T (λ (n) f )Σ -1 c Φ(λ (n) f ) θc ,<label>(23)</label></formula><formula xml:id="formula_38">∇ Σc F = Σ -1 c 2 N n=1 (λ (n) c -Φ(λ (n) f ) θc )(λ (n) c -Φ(λ (n) f ) θc ) T q (t+1) n Σ -1 c -N . (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>In a maximum likelihood setting, i.e. with uniform priors p(θ) ∝ const., we observe that the update equations given by ∇ θ F = 0 are linear in all parameters θ = {W , b, S, θc , Σ c ) and closed-form updates can be carried out. We provide these update equations in Section 3 where priors are specified. In general, the gradients above can also be used together with the log prior gradients in any iterative (stochastic) optimization scheme. A complexity analysis of training and prediction stages is given in Section 2.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Prior specification.</head><p>A key point of the proposed model is the discovery of predictive features of the high-dimensional input λ f during the coarse-graining process λ f → λ c . This dimensionality reduction process takes place in the linear model for p c (Equation ( <ref type="formula">3</ref>)) and depends on the vocabulary of feature functions ϕ(λ f ) employed. One strategy is to sequentially add features from a parametric <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b61">62]</ref> or predefined <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref> set of feature functions ϕ(λ f ) upon optimization of a suitable predictive performance measure. Another way to proceed is to start with a large dictionary of features ϕ, which can potentially produce an excessively complex model that overfits and is hampered by non-unique optima.</p><p>In order to regularize the problem, we employ a prior on the feature function coefficients θc that favors sparse solutions where only a few components assume non-zero values. Apart from computational advantages (pruned out features do not need to be evaluated for predictions), such a prior reveals the features that are most predictive for λ c and thus may provide further insight to the underlying physics of the problem. Several sparsity enforcing approaches were tested in this work, including the Laplacian prior (or LASSO regression <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b25">26]</ref>) as well as Student-t type prior models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b21">22]</ref>. We achieved best experimental results using a slightly modified version of the Relevance Vector Machine (RVM) <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b9">10]</ref>  </p><formula xml:id="formula_40">W (0) , b (0) , S (0) , θ(0) c , Σ<label>(0)</label></formula><p>c , γ (0) {Initialization} 1: Set t ← 0 2: while (not converged) do 3:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-step:</head><p>{Completely parallelizable in n} 4:</p><p>for n = 1 to N do 5:</p><p>Update q</p><formula xml:id="formula_41">(t+1) n (λ<label>(n)</label></formula><p>c ) according to <ref type="bibr" target="#b11">(12)</ref> 6:</p><p>Estimate λ</p><formula xml:id="formula_42">(n) c q (t+1) n , λ<label>(n)</label></formula><p>c (λ</p><formula xml:id="formula_43">(n) c ) T q (t+1) n</formula><p>, and u c (λ</p><formula xml:id="formula_44">(n) c ) q (t+1) n , u c (λ (n) c )u T c (λ (n) c ) q (t+1) n 7:</formula><p>end for 8:</p><p>M-step:</p><formula xml:id="formula_45">9: Find W (t+1) , b (t+1) , S (t+1) , θ(t+1) c , Σ<label>(t+1) c</label></formula><p>by maximization of F using ( <ref type="formula" target="#formula_34">20</ref>)-( <ref type="formula" target="#formula_38">24</ref>)</p><p>10:</p><p>Inner E-step:</p><p>11:</p><p>Given the posterior q (t+1) θc ( θc ) in ( <ref type="formula">29</ref>), estimate θ2 c,i q (t+1) θc using Laplace approximation</p><p>12:</p><p>Inner M-step:</p><p>13:</p><p>Maximize the evidence lower bound G(q (t+1) θc ( θc ); γ) given in (31) using update equation <ref type="bibr" target="#b35">(36)</ref> 14:</p><p>t ← t + 1 15: end while 16: return W MAP , b MAP , S MAP , θc,MAP , Σ c,MAP , γ * <ref type="bibr" target="#b54">[55]</ref>. Given the likelihood ( <ref type="formula">7</ref>)</p><formula xml:id="formula_46">L( θc ) = N n=1 p cf (u (n) f |u c (λ (n) c ), θ cf )p c (λ (n) c |λ (n) f , θc , Σ c )dλ (n) c , the marginal w.r.t. θc is (25) P(γ) = L( θc )p( θc |γ)d θc .</formula><p>We determine the value of the hyperparameters as ≥ q θc ( θc ) log L( θc )p( θc |γ) q θc ( θc ) d θc = G(q θc ( θc ); γ), <ref type="bibr" target="#b27">(28)</ref> where q θc ( θc ) is an arbitrary auxiliary distribution. The q (t+1) θc ( θc ) that maximizes (28) for a given γ (t) (as the inequality becomes an equality) is <ref type="bibr" target="#b28">(29)</ref> q</p><formula xml:id="formula_47">(t+1) θc ( θc ) ∝ L( θc )p( θc |γ (t) ).</formula><p>Using the fact that (30) log p( θc |γ) ∝ -1 2</p><formula xml:id="formula_48">N features i=1 log γ i - 1 2 N features i=1 θ2 c,i γ i ,</formula><p>and keeping only terms that depend on γ, we get</p><formula xml:id="formula_49">(31) G(q (t+1) θc ( θc ); γ) ∝ - 1 2 N features i=1 log γ i - 1 2 N features i=1 γ -1 i θ2 c,i q (t+1) θc</formula><p>.</p><p>Setting the derivatives ∂ ∂γ j G(q (t+1) θc ( θc ); γ) to 0 yields the update equations ( <ref type="formula">32</ref>)</p><formula xml:id="formula_50">γ (t+1) j = θ2 c,j q (t+1) θc .</formula><p>To estimate the expected value θ2 c,j q (t+1) θc</p><p>, we perform Laplace approximation ( <ref type="formula">33</ref>)</p><formula xml:id="formula_51">q (t+1) θc ( θc ) ≈ N ( θc | θ(t) c , Σ<label>(t) ), where (34) θ(t</label></formula><formula xml:id="formula_52">) c = arg max θc L( θc )p( θc |γ (t) )</formula><p>is the maximum of q (t+1) θc ( θc ) in ( <ref type="formula">29</ref>) for a given γ (t) , which we find using EM as described in 2.5.1. According to Laplace approximation, the covariance Σ(t) is given by</p><formula xml:id="formula_53">(35) ( Σ(t) ) -1 = -∇ θc ∇ θc log q (t+1) θc ( θc ) θc= θ(t) c = N n=1 Φ T (λ (n) f )Σ -1 c Φ(λ (n) f ) + (diag[γ (t) ]) -1 ,</formula><p>where ∇ θc ∇ θc log q (t+1) θc</p><formula xml:id="formula_54">( θc ) θc= θ(t) c</formula><p>denotes the Hessian of log q (t+1) θc</p><p>( θc ) at θc = θ(t) c . We finally get <ref type="bibr" target="#b35">(36)</ref> γ</p><formula xml:id="formula_55">(t+1) j = θ2 c,j q (t+1) θc ≈ ( θ(t) c,j ) 2 + Σ(t) jj ,</formula><p>where the '≈' accounts for the Laplace approximation. After convergence of γ (t) , θ(t) c , Σ(t) to γ * , θc,MAP , ΣMAP , the posterior on θc is approximated by <ref type="bibr" target="#b36">(37)</ref> p( θc |D) ≈ N ( θc | θc,MAP , ΣMAP ).</p><p>It can be shown <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b73">74]</ref> that many of the prior variance parameters γ i converge to 0 such that the corresponding features ϕ i are effectively deactivated. A summary of the optimization scheme is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Model predictions.</head><p>A key feature of the proposed model is the ability to produce probabilistic predictions that reflect the various sources of uncertainty enumerated previously. Given the posterior p(θ|D) on the model parameters θ which in the case of MAP estimates can be substituted by δ(θ -θ MAP ) and for a new input λ f , the Bayesian reduced-order model formulated yields a predictive posterior density p pred (u f |λ f , D) for the FOM output u f of the form</p><formula xml:id="formula_56">p pred (u f |λ f , D) = p(u f , θ|λ f , D) dθ = p(u f |λ f , θ) Equation (2) p(θ|D) dθ = p cf (u f |u c (λ c ), θ cf )p c (λ c |λ f , θ c )dλ c p(θ|D) dθ.<label>(38)</label></formula><p>While the aforementioned density is analytically intractable, samples can inexpensively be generated by following the steps, see also Figure <ref type="figure" target="#fig_4">3:</ref> • drawing a sample θ = {θ c , θ cf } from the posterior p(θ|D);</p><p>• drawing a sample λ c ∼ p c (λ c |λ f , θ c );</p><p>• solving the coarse model to obtain u c (λ c );</p><formula xml:id="formula_57">• drawing a sample u f ∼ p cf (u f |u c (λ c ), θ cf ).</formula><p>In the examples presented in Section 3, we use the approximate posterior <ref type="bibr" target="#b36">(37)</ref> for θc and MAP estimates for all other model parameters, which are denoted with a 'MAP' subscript in the following. Since p cf (u f |u c (λ c ), θ cf ) is Gaussian, we can estimate the predictive posterior mean µ pred = u f p pred as <ref type="bibr" target="#b38">(39)</ref> </p><formula xml:id="formula_58">µ pred (λ f ) = 1 M M m=1 u f p cf (u f |u c (λ (m) c ), θ cf )du f = W MAP 1 M M m=1 u c (λ (m) c ) + b MAP where λ (m) c</formula><p>∼ p c (λ c |λ f , θ c )p(θ c |D). In the equation above, M denotes the number of Monte Carlo samples needed to produce an accurate estimate of this quantity. As each of these samples requires solely a solution of the coarse FE model, the cost is negligible. Similarly, the predictive posterior variance σ 2 pred,i of each component u f,i can be estimated as:</p><formula xml:id="formula_59">(40) σ 2 pred,i (λ f ) = 1 M M m=1 (u f,i -µ pred,i ) 2 p cf (u f |u c (λ (m) c ), θ cf )du f .</formula><p>2.6.1. Model testing. In order to assess the predictive performance of the model in the ensuing examples, we introduce the following error measures </p><formula xml:id="formula_60">e(λ f ) = 1 N dof,f N dof,f i=1 (µ pred,i (λ f ) -u f,i (λ f )) 2 var(u f,i ) ,<label>(41)</label></formula><formula xml:id="formula_61">L(λ f ) = - 1 N dof,f N dof,f i=1 log N (u (n) f,i | µ pred,i (λ f ), σ 2 pred,i (λ f )), (<label>42</label></formula><formula xml:id="formula_62">)</formula><formula xml:id="formula_63">Training/offline stage Generate training data D λ (n) f ∼ p(λ f ) u (n) f = u f (λ (n) f ) D = λ (n) f , u (n) f N n=1 Evaluate features ϕ jk (λ (n) f ),</formula><p>Solve coarse model u</p><formula xml:id="formula_65">(m) c = uc(λ (m) c ) Sample FOM solution u (m) f ∼ p cf (u f |uc(λ (m) c ), θ cf )</formula><p>Repeat and update µ pred , σ pred eq. ( <ref type="formula">39</ref>), ( <ref type="formula">40</ref>) where the u f,i (λ f ) are the true FOM outputs. The error measure e(λ f ) is normalized by the true variance var(u f,i ) of u f,i (estimated by Monte Carlo). Hence, if we would naively use the training data mean as the predictive mean estimate for all test cases, the expected value e(λ f ) would be 1. The second quantity L(λ f ) represents an approximate predictive loglikelihood under the assumption that the predictive density can be sufficiently approximated by independent Gaussians. In contrast to e(λ f ) which captures the deviation of the predictive mean from the truth, L(λ f ) reflects also the predictive uncertainty. To obtain a reference value for L(λ f ), we use the means µ data,i and variances σ 2 data,i of the training data in place of µ pred,i and σ 2 pred,i respectively. In the ensuing examples we compute average values of the aforementioned error measures over multiple samples λ f generated from the same density as the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Numerical complexity analysis.</head><p>In the complexity analysis of the proposed approach, it is essential to distinguish between training and prediction (Figure <ref type="figure" target="#fig_4">3</ref>). As it can be seen in the inner for-loop of Algorithm 1, training complexity grows linearly with the number of training samples N due to the variational densities q n associated with each data point. However, as a result of the factorial form of the likelihood function in equation ( <ref type="formula">7</ref>) and the resulting mutual independence of the q n 's, this step may be fully parallelized in N . We did not observe any dependence of the required number of EM epochs w.r.t. N .</p><p>For training and prediction purposes, one must solve the coarse FE model. The cost of each of these solves depends on the dimension of u c i.e. N dof,c = dim(u c ) which is by construction much smaller than that of N dof,f = dim(u f ). Also, prediction complexity is completely independent of the number of training data N .</p><p>The scaling w.r.t. N dof,f = dim(u f ) ≈ dim(λ f ) depends on the particular form of p cf . For the one adopted in this study (Equation ( <ref type="formula">5</ref>)) the scaling of the update equations in the training phase is linear with respect to N dof,f . We note also that the values of the feature functions ϕ(λ f ) can be pre-computed and stored for each of the training samples λ (n) f . 3. Numerical experiments. As a numerical test case for the method presented in the previous section, we consider the following linear elliptic PDE</p><formula xml:id="formula_66">∇ x • (-λ(x)∇ x u) = 0 for x ∈ D = [0, 1] d , u = û for x ∈ Γ u , (<label>43</label></formula><formula xml:id="formula_67">) -λ(x)(∇ x u) • n = ĥ • n for x ∈ Γ h ,</formula><p>where n is the outward, unit normal vector on Γ h , λ(x) &gt; 0 is a random diffusivity and u = u(x, λ(x)) the solution field. The primary goal of the first example is to demonstrate the ability of the proposed model to identify salient, predictive features of the random input λ f . In the second example, the capability of the model to deal with very high-dimensional inputs (the cases considered involve dim(λ f ) = 256 × 256 = 65536 and dim(λ c ) ≤ 64) is evidenced as well as its resilience in providing accurate predictive estimates with limited training data (N ≈ 10 . . . 100) or in cases where predictions are sought under different boundary conditions than the ones used in training.</p><p>3.1. One-dimensional example. In the first example, we consider the SPDE in <ref type="bibr" target="#b42">(43)</ref> in one spatial dimension d = 1 where there exists a closed-form solution for homogenized diffusion coefficients λ c . We use this closed-form solution as a feature function ϕ(λ f ) in combination with 99 other functions, some of which provide similar information.</p><p>We use the boundary conditions û(x) = 0 for x ∈ Γ u = {0} and ĥ = -100 for x ∈ Γ h = {1}. The FOM is given by a Galerkin discretization with N el,f = 128 linear finite elements (i.e. dim(u f ) = 129). In each such element, we assume constant diffusivity λ f,i ∈ {λ lo , λ hi }, where λ lo = 1, λ hi = 10. Samples of λ f are generated by using a level-cut Gaussian process <ref type="bibr" target="#b43">(44)</ref> f (x) ∼ GP (0, k(x -x ))</p><p>with squared exponential covariance kernel k</p><formula xml:id="formula_68">(x -x ) = exp -(x-x ) 2 l 2</formula><p>and length scale parameter l = 0.01. We consider the values of f (x) at the center points of each element which constitute a 128-dimensional Gaussian random vector f . For each element i, we assign the value λ lo = 1 if f i &lt; f cut and λ hi = 10 otherwise. The cutoff parameter f cut is related to the expected volume fraction of the two phases. For each training datum λ (n) f , we also randomize f cut such that the resulting dataset contains volume fractions uniformly distributed in (0, 1). θc,j ϕ j (λ</p><formula xml:id="formula_69">[k] f ) + σ c,k Z k , Z k ∼ N (0, 1),</formula><p>where with λ</p><p>[k]</p><p>f we denote the subset of λ f which is part of coarse element k i.e. for the first coarse element λ f corresponds to the first 8 entries of λ f and so on. We employ the same feature functions in all coarse elements, i.e. ϕ jk = ϕ j . Furthermore, we assume that the same coefficients can be used in each of those regressions, i.e. θc,jk = θc,j . As a result, we obtain closed-form updates for the model parameters θ c which, according to Equations ( <ref type="formula" target="#formula_37">23</ref>), <ref type="bibr" target="#b23">(24)</ref> will take the form</p><formula xml:id="formula_70">θ(t+1) c = N n=1 Φ T (λ n f )(Σ (t) c ) -1 Φ(λ (n) f ) + (diag[γ (t) ]) -1 -1 N n =1 Φ T (λ (n ) f )(Σ (t) c ) -1 z (n ) q (t+1) n ,<label>(46)</label></formula><formula xml:id="formula_71">Σ (t+1) c = 1 N N n=1 diag[(z (n) -Φ(λ (n) f ) θc )(z (n) -Φ(λ (n) f ) θc ) T ] q (t+1) n ,<label>(47)</label></formula><p>where Φ kj (λ f ) = ϕ j (λ</p><formula xml:id="formula_72">[k]</formula><p>f ) and γ can be updated according to <ref type="bibr" target="#b35">(36)</ref>. We assume that p(Σ c |D) = δ(Σ c -Σ c,MAP ) and that p( θc |D) is given by the Laplace approximation in <ref type="bibr" target="#b36">(37)</ref>.</p><p>Feature functions. It is known <ref type="bibr" target="#b76">[77]</ref> that the effective diffusion coefficient for 1-dimensional problems such as the one considered, corresponds to the harmonic mean. We therefore use it as a feature function in conjunction with 99 other ones which include generalized means, lineal path function <ref type="bibr" target="#b41">[42]</ref>, 2-point correlation function, effective medium approximations <ref type="bibr" target="#b76">[77]</ref> and distance transforms <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>3.1.2. The coarse-to-fine map p cf . For the coarse-to-fine map p cf (u f |u c (λ c ), θ cf ), we employ the model given in Equation 5 and set the bias parameter b = 0. We further determine the projection matrix W ∈ R 129×9 by linearly interpolating between coarse and fine grids. In particular <ref type="bibr" target="#b47">(48)</ref> W</p><formula xml:id="formula_73">ij =        x i -X j-1 X j -X j-1 for X j-1 ≤ x i ≤ X j , x i -X j+1 X j -X j+1 for X j ≤ x i ≤ X j+1 ,<label>0 else,</label></formula><p>where X j = j-1 8 , j = 1, . . . , dim(u c ) = 9 are the coordinates of the nodes of the coarse model (i.e. the spatial locations to which the outputs u c correspond to) and x i = i-1</p><p>128 , i = 1, . . . , dim(u f ) = 129 are the coordinates of the nodes of the FOM (i.e. the spatial locations to which the outputs u f correspond to). The covariance matrix S (Equation <ref type="formula">5</ref>, which is assumed to be diagonal) is treated as free parameter and its MAP estimate is computed. In the absence of a prior, according to <ref type="bibr" target="#b21">(22)</ref>, the updates for S are closed-form, ( <ref type="formula">49</ref>) c with respect to EM iterations t. The blue curve corresponds to the harmonic-mean feature function and quickly converges to 1. All remaining 99 coefficients become 0 (only a subset is depicted).</p><formula xml:id="formula_74">S (t+1) = 1 N N n=1 diag (u (n) f -W u c (λ (n) c ))(u (n) f -W u c (λ (n) c )) T q (t+1) n .</formula><p>3.1.3. Results. In Figure <ref type="figure" target="#fig_7">4</ref> results obtained with N = 16 training data are depicted. On the right-hand side, we observe the evolution of the coefficients θc with respect to the Expectation-Maximization iterations. One observes that θc,1 , which corresponds to the harmonic mean feature function, quickly converges to 1 whereas all remaining θc 's become 0, i.e. all remaining features are deactivated. Hence the sparsity-inducing prior is shown capable of distinguishing the most predictive feature function(s), despite the large number of such features and the small number of training data. On the left hand-side, we depict predictions of the FOM output u f obtained using the trained model for an indicative test case λ f . While the posterior mean does not coincide with the reference solution, the model's predictive posterior is able to envelop it. One can also visually inspect the predictive posterior means of the coarse model properties λ c in relation with the underlying FOM diffusivity λ f .</p><p>In order to assess the overall predictive ability of the model we computed average values of the error metric e(λ f ) (Equation <ref type="formula" target="#formula_60">41</ref>) in Section 2.6 over N test = 1024 test samples. We obtain the value of e(λ f ) = 0.027(3) which is approximately 30 times smaller than the reference value of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-dimensional examples.</head><p>In this section, we examine the SPDE in <ref type="bibr" target="#b42">(43)</ref> in the two dimensional unit square where there is no closed form solution for the effective diffusion coefficients λ c . For the FOM, we discretize with a uniform square mesh of size 256 × 256 (dim(u f ) = 66049) and assume constant diffusivity within each element (i.e. dim(λ f ) = 65536). We consider two-phase random media, i.e. λ f,i ∈ {λ lo , λ hi } and eval- uate the performance of the method proposed for various contrasts c = λ hi λ lo <ref type="foot" target="#foot_3">6</ref> . It is noted that the more pronounced the contrast in the properties of the two phases is, the more the (random) topology and higher-order statistical descriptors affect the macroscopic response ( <ref type="bibr" target="#b76">[77]</ref>). We consider a distribution on λ f defined implicitly through a level-cut Gaussian pro-</p><formula xml:id="formula_75">cess f (x) ∼ GP (0, k(x -x )) with k(x -x ) = exp -|x-x | 2 l 2</formula><p>and l = 0.01. We generate samples of the random vector associated with the center points of each of the 65536 elements (e.g. <ref type="bibr" target="#b66">[67]</ref>) and assign values λ lo or λ hi based on a threshold f cut , as we did in Section 3.1. We again randomize this threshold so as the resulting samples have a range of expected volume fractions between 0 and 1. Indicative samples λ f are depicted in Figure <ref type="figure" target="#fig_8">5</ref> together with the corresponding FOM outputs u f (λ f ).</p><p>We consider boundary conditions of the form</p><formula xml:id="formula_76">û(x) = a 0 + a 1 x 1 + a 2 x 2 + a 3 x 1 x 2 , x ∈ Γ u , ĥ(x) = -∇ x û(x), x ∈ Γ h .<label>(50)</label></formula><p>Furthermore, we use Γ u = {0} and Γ h = ∂D\ {0}, i.e. Neumann boundary conditions of the form above almost everywhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Model distributions.</head><p>For the coarse-to-fine map p cf (u f |u c (λ c ), θ cf ), we again fix the bias vector b = 0 and the coarse-to-fine projection matrix W ∈ R 66049×dim(uc) so that it corresponds to a bilinear interpolation of the fine and coarse model grid points (as we did in the one-dimensional example). The covariance S of the residual noise in Equation 5 is treated as a free parameter and the MAP estimate is obtained using the same updates as in Equation <ref type="formula">49</ref>. θc,j ϕ j (λ</p><formula xml:id="formula_77">[k] f ) + σ c,k Z k , Z k ∼ N (0, 1),</formula><p>(as in 3.1.1) with a set of 100 feature functions adapted to the 2d case (see Appendix A for a summary). We employed the same coefficients θc,j for all macro-elements k and will discuss a more flexible version in Section 3.2.5. The update equations for θc , Σ c are equivalent to those given in <ref type="bibr" target="#b45">(46)</ref>, <ref type="bibr" target="#b46">(47)</ref> and we employ p(Σ c |D) = δ(Σ c -Σ c,MAP ) and p( θc |D) as computed by the Laplace approximation in (37).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Predictive performance.</head><p>In order to assess the predictive performance of the proposed model, we use the error measures e and L as defined in <ref type="bibr" target="#b40">(41)</ref> and <ref type="bibr" target="#b41">(42)</ref>, respectively and average over multiple test cases. Both measures are plotted in Figure <ref type="figure" target="#fig_9">6</ref> against the number of training samples N for the three different coarse model sizes with N el,c = 2 × 2, 4 × 4, 8 × 8 and for a contrast c = λ hi λ lo = 2. The test data are generated with boundary conditions as in Equation 50 with a = (0, 800, 1200, -2000) T . We observe that in all three cases, the reduced-order models constructed are able to reach their asymptotic values with less than N = 16 training samples. The coarsest of these models (i.e. with N el,c = 2 × 2) converges the fastest due to the fewer free parameters but attains error values that are not as low as the finer models. In the top row of Figure <ref type="figure" target="#fig_10">7</ref>, three indicative test samples λ f , u f are depicted and compared with the posterior predictive estimates µ pred (Equation <ref type="formula">39</ref>) and σ pred (Equation <ref type="formula">40</ref>), whereas the bottom row shows the L 2 -distance of the predictive mean to the true reference. The latter are computed with N = 128 training samples and for a coarse model of size N el,c = 8 × 8. We observe that in all cases and despite the unavoidable predictive uncertainty, the probabilistic predictions obtained tightly envelop the truth.  .</p><p>Figure <ref type="figure" target="#fig_13">8</ref> provides further insight on the trained model as it depicts the corresponding predictive posterior means of the coarse-model's properties λ c for various test instances λ f . The predictive uncertainty σ pred (Equation <ref type="formula">40</ref>) is in part due to the residual uncertainty in p cf captured S = diag(s 2 j ) as well as the uncertainty in p c modeled by σ 2 c,k . The corresponding standard deviations σ c,k , s j for each of the coarse element k and FOM nodes j are depicted in Figure <ref type="figure" target="#fig_14">9</ref>. We observe that the σ c,k is generally larger way from the boundary of the problem domain D. The opposite behavior is observed for the s j 's which tend to be larger closer to the boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Activated features for different contrasts.</head><p>In order to gain further insight of the feature functions that are activated, we train the coarse model of size N el,c = 4 × 4 for five different contrast values c = 2, 10, 100, 500 and 1000. We generate N = 1024 in which we also randomize the boundary conditions employed by drawing a ∼ N (0, σ 2 a ) in Equation <ref type="formula" target="#formula_76">50</ref>with σ 2 a = ( 0, 10 6 , 10 6 , 10 6 ) T . The MAP estimates of the coefficients θc are shown in figure <ref type="figure" target="#fig_15">10</ref>. We generally observe that for higher contrast values c, the magnitude of the nonzero θc 's as well as the number of activated feature functions increase. Furthermore feature functions taking into account the whole microstructural vector λ f become activated. This could be attributed to the fact that the higher the contrast the more prominent becomes the role of the microstructure and its higher-order statistics in predicting the system's response. Apart from generalized means, other features that play a role correspond to effective medium approximations such as the self-consistent approximation (SCA) or Bruggeman formula <ref type="bibr" target="#b11">[12]</ref> (a) Coarse-grained, effective property λ c pc for the three test samples shown in Figure <ref type="figure" target="#fig_10">7</ref>     as well as the differential effective-medium approximation (DEM) <ref type="bibr" target="#b11">[12]</ref>. Statistical features such as "Gaussian linear filters" (Figure <ref type="figure" target="#fig_5">11</ref>) and the first principal component (computed by PCA on 4096 samples of λ f ) also seem to be important.   <ref type="formula" target="#formula_76">50</ref>) a = (0, 800, 1200, -2000) T and a = (0, 500, -1500, 1000) T . We use N = 1024 training samples in order to avoid the effects of small datasets. The predictive error measures e and L (Section 2.6) are averaged over multiple test instances and the results are shown in Table <ref type="table" target="#tab_1">1</ref>. We observe only slight deterioration for predictions on differ-  ent boundary conditions than those used for training which implies that the model is able to incorporate salient information about the physical behavior of the random medium. In Figure <ref type="figure" target="#fig_19">12a</ref> a few indicative test cases are depicted, one for each of the four possible combinations of training/testing boundary conditions. In Figure <ref type="figure" target="#fig_19">12b</ref> we show 4 test cases where the model is trained with FOM data obtained on boundary conditions a and predictions are computed for randomly sampled boundary conditions according to ã ∼ N (0, σ 2 a ) with σ 2 a = ( 0, 10 6 , 10 6 , 10 6 ) T . In all the aforementioned cases, accurate predictions were obtained which envelop the ground truth.</p><p>3.2.5. Predictive performance improvement by local/global θc 's. We consider in this section a more flexible model for p c and examine its potential in terms of the accuracy of the predictions produced. In contrast to Equation <ref type="formula">51</ref>, we consider relations between λ c and λ f of the form ( <ref type="formula">52</ref>)</p><formula xml:id="formula_78">z k = N features j=1 θc,jk ϕ j (λ [k] f ) + σ c,k Z k , Z k ∼ N (0, 1),</formula><p>where the coefficients θc,jk are now explicitly dependent on each macro-cell k in the problem domain. While the same feature functions ϕ j are employed for each k, the model can assign  <ref type="formula" target="#formula_60">41</ref>) for a model with θc,jk = θc,j for all macro-cells k (Equation <ref type="formula">51</ref>) and for the model θc,jk = θc,jk (Equation <ref type="formula">52</ref>) but identical hyperparameters γ jk = γ j . Predictions for N = 16 and N = 1024 training data are reported.</p><p>different coefficients θc,jk at each k, and therefore can potentially account for local features in the coarse-graining process. This increases the number of model parameters and in order to provide proper regularization as well as to enhance the interpretability of the results, we employ the same hyperparameters γ j for all θc,jk associated with the same feature function j, independently of the macro-cell k. In this manner information can be shared across macrocells and feature functions will either be active or inactive over the whole domain. Predictive errors for the original and this enhanced model are compared in Table <ref type="table" target="#tab_2">2</ref> under a low number of training samples N = 16 as well as for N = 1024. In the latter case, we observe that using different θc,jk 's for different macro-cells k, leads to improvements in predictive performance.</p><p>For N = 16 however, the simpler model where θc,jk = θc,j exhibits superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions.</head><p>We have introduced a Bayesian formulation that performs simultaneous model-order and dimensionality reduction for problems characterized by high-dimensional inputs/outputs as those arising in PDEs for random heterogeneous media. At the core of the proposed architecture lies a coarsened version of the original description with a latent closure model (constitutive law). The latter serves as a filter of the FOM high-dimensional input. The outputs of the coarsened model are decoded in order to yield predictions of the FOM high-dimensional output. All three components are modeled with parametrized densities which are trained simultaneously using FOM simulation data. We have demonstrated that this can be achieved with only a few tens of such samples and that the resulting reduced-order model can extract essential information that allow it to produce crisp predictions even under different boundary conditions from those used in training. The probabilistic nature of the model enables it to quantify uncertainties arising from the information loss that unavoidably takes place in all coarse-graining processes as well as those due to the use of finite-sized datasets. An essential feature of the model is the use of sparsity-inducing priors that promote the discovery of a low-dimensional set of features of the input which are most predictive of the FOM response. The training process involves Bayesian inference which is carried out using Stochastic Variational Inference tools that require repeated computations only of the coarse model and its parametric derivatives. Apart from uncertainty propagation, the resulting Bayesian reduced-order model can be readily used for other computationally intensive tasks such as optimization or the solution of inverse problems.</p><p>Several extensions can be envisaged with respect to all three building blocks. With regards to the coarse-graining density p c an important enhancement would involve the automatic discovery of the feature functions using semi-supervised models <ref type="bibr" target="#b38">[39]</ref> rather than employing a predefined vocabulary. This would enable better predictive results as well as lead to further physical insight on the statistical descriptors of the underlying random medium that are predictive of its response. Several improvements are possible for the coarse model employed. The immediate one is the development of an adaptive refinement scheme on the basis of probabilistic predictive metrics which would focus computational resources and statistical learning on the most informative parts of the problem domain (i.e. subsets of the random input vector). The use of different physical models is also possible and especially in multiscale problems, it might be necessary to employ a different description than the FOM. Finally, with regards to the coarse-to-fine map p cf , a possible enhancement could involve nonlinear maps between the coarse and FOM outputs that would promote further dimensionality reductions in this component. Table <ref type="table">3</ref> shows a list of the 100 feature functions used in the 2d numerical examples of Section 3.2. Features 1-88 take the subset λ</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic representation of the model defined by Equation (2). Starting from the top left: In the first step, an effective representation of the FOM input λ f → λ c is found.Next, the PDE is solved using a (much) coarser discretization. Finally, the FOM solution vector u f is reconstructed from the coarse one, u c → u f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of the three-component Bayesian network implied by p(u f |λ f , θ cf , θ c ) in Equation 2. The internal vertices λ c , u c are latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc>adjusted for use in latent variable models. The basic prior model is of the form p( θc |γ) = N ( θc |0, diag[γ]) where γ ∈ R N features + is a vector of non-negative hyperparameters describing the prior variance of each feature component. These hyperparameters are estimated by first integrating out the model parameters θc and then performing what is known as type-II maximum likelihood or evidence maximization Algorithm Posterior maximization Require:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>in an inner loop of Expectation-Maximization (EM). To that end, we use the log evidence lower bound log P(γ) = log L( θc )p( θc |γ)d θc<ref type="bibr" target="#b26">(27)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model workflow for the training phase (left) and the prediction phase (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3. 1 . 1 .</head><label>11</label><figDesc>The coarse-graining distribution p c . For the coarse model, we employ a discretization consisting of 8 linear elements with the same boundary conditions as the FOM and assume that the diffusivity is constant within each element. Hence, dim(λ c ) = 8 and dim(u c ) = 9. For the coarse-graining distribution p c (λ c |λ f , θ c ), we adopt the model 5 discussed in Section 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: One-dimensional example. For a test input λ f , the blue line corresponds to true FOM output u f , the black line is the predictive mean µ pred (Equation 39) enveloped by ±2 predictive standard deviations σ pred (Equation 40). The bars underneath depict the FOM input λ f (dim(λ f ) = 128) (top) and the predictive posterior mean λ c pc with the model parameters learned from the training data. (Right) Evolution of θ(t) c with respect to EM iterations t. The blue curve corresponds to the harmonic-mean feature function and quickly converges to 1. All remaining 99 coefficients become 0 (only a subset is depicted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two-dimensional example. Samples λ f , with dim(λ f ) = 256 × 256 and the corresponding PDE outputs u f (dim(u f ) = 257 × 257) for contrast λ hi λ lo = 100. The same boundary conditions are employed.</figDesc><graphic coords="19,83.07,96.36,420.71,127.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Averaged error measures e and L as defined in equations (41), (42) for contrast c = λ hi λ lo = 2 and different coarse model sizes N el,c versus the number of training data samples N . The error bars are due to randomization of training data. The green line on the right corresponds to L data , see Section 2.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top row: Predictive mean µ pred (blue) ±σ pred (transparent grey) and true response u f (colored) for three test samples for c = λ hi λ lo = 2, N el,c = 8 × 8 and N = 128 training data samples. Bottom row: Normalized error between ground truth u f and posterior predictive mean µ pred i.e.</figDesc><graphic coords="21,83.07,96.36,420.72,205.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>|u f,i -µ pred,i | |u f,i |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>with N = 128, N el,c = 8 × 8. (b) Lower right corner macro-cells and mean effective properties λ c,k of the microstructures shown in 8a (c) Effective properties of randomly chosen macro-cells of the microstructures shown in 8a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Predictive posterior mean λ c .</figDesc><graphic coords="22,171.65,516.72,243.58,78.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MAP estimates of σ c,k (left) and s j (right) as computed for c = 2, N = 128 and N el,c = 8 × 8.</figDesc><graphic coords="23,116.29,96.36,354.30,127.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: MAP estimates of the coefficients θc for 5 different contrast ratios c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Left: The "Gaussian linear filter" with variance Σ = 8LI (marked as Gauss 8 in Figure 10) where L is the length of the macro-cell. Right: The first PCA component (marked as PCA 1 in 10) obtained performing PCA on all macro-cells of 4096 unsupervised samples of λ [k]f . The feature function outputs are computed as the inner product of the above images with every λ [k] f .</figDesc><graphic coords="24,94.15,96.36,398.59,148.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Predictive mean µ pred (blue) ±σ pred (transparent grey) and true response u f (colored). The top left and the bottom right plots are predictions using identical boundary conditions as have been used for training. The top right and the bottom left are trained using a but predict on a and vice versa. (b) Predictive mean µ pred (blue) ±σ pred (transparent grey) and true response u f (colored). The boundary conditions of the four test cases are randomly selected as explained in the text whereas training was performed using boundary conditions a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Prediction examples for N = 1024, N el,c = 4×4, l = 0.01 and c = 10 using different boundary conditions. No substantial deterioration in predictive performance is observed if predictions are performed on a test set with different boundary conditions than the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>2 50</head><label>2</label><figDesc>α+ α 2 +4λ hi λ lo 2 , α = λ lo (2v lo -1) + λ hi (2v hiparameters a, b parameters of a • e -b•d fit to lineal path 17-18 Number of distinct high/low conducting blobs 19-22 Number of high/low conducting pixels to cross from left to right/up to down 23-26 Max. extent of high/low conducting blob in x/y-direction mean/variance of convex area of high/low conducting blobs 38-41 Inv. distance of connected path through high/low cond. phase in x/y-direction, 0 if no connected path existent 42-43 Specific surface -4 ∂ ∂d S 2 (d)| d=0 , with 2-point correlation S 2 (r) 44-48 "Gaussian linear filter" compute w i = N (x i |µ center , aI) where µ center is the macro-element center and x i are fine-scale element locations. Compute ϕ j = w T λ [k] f 49 Standard deviation ϕ j = (λ f,i -λ f,i ) Log standard deviation ϕ j = log( (λ f,i -λ f,i ) 2 ) 51 Ising energy Energy of a 2d Ising system with coupling J = 1 and no external field /maximum of distance transforms under different distance metrics 82-88 Local PCA loadings Perform PCA using every macro-cell λ [k] f . Compute projections onto loadings ϕ j = w T λ [k] f 89-92 Max. extent of high/low conducting blob in x/y-direction of whole microstructure λ f 93-97 SCA, Maxwell-Garnett, Differential Effective Medium on whole microstructure λ f 98-100 Global PCA loadings Perform PCA using whole microstructures λ f . Compute projections onto loadings ϕ j = w T λ fTable 3: Set of 100 feature functions ϕ applied in the 2d numerical examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Averaged error measures e and L as defined in Equation 41 and Equation 42. In the off-diagonal cells, we test on data with boundary conditions a whilst having trained using a and vice versa. 3.2.4. Predictions under different boundary conditions. The goal of this section is to examine the ability of the proposed model to produce accurate predictions of FOM outputs under certain boundary conditions when it has been trained with data involving FOM runs under different boundary conditions. To investigate this, we train the model with a coarse model size N el,c = 4 × 4 and FOM data obtained under the two boundary conditions specified by (Equation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Averaged error measure e as defined in (</figDesc><table><row><cell></cell><cell></cell><cell>N = 16</cell><cell></cell><cell>N = 1024</cell></row><row><cell></cell><cell>c</cell><cell>θc,jk = θc,j</cell><cell>γ jk = γ jk</cell><cell cols="2">θc,jk = θc,j γ jk = γ j</cell></row><row><cell>Error measure e</cell><cell>2</cell><cell cols="2">0.0215 ± 0.002 0.0715 ± 0.036</cell><cell>0.00643</cell><cell>0.00412</cell></row><row><cell></cell><cell>10</cell><cell cols="2">0.0277 ± 0.027 0.0479 ± 0.009</cell><cell>0.00948</cell><cell>0.00593</cell></row><row><cell></cell><cell cols="3">100 0.0317 ± 0.005 0.0589 ± 0.015</cell><cell>0.0166</cell><cell>0.0103</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We also denote by Σc = diag(σ 2 c ) whenever this is more convenient.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We use diagonal covariances Σ (n) V I .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Since λ f is bounded by λ lo , λ hi , we seek λ c,k that also take values in [λ lo , λ hi ]. To enforce this constraint, we apply the sigmoid link function λ c,k = χ(z k ) = λ hi -λ lo 1+e -z k + λ lo and perform the linear regression in z-space. We note that more rigorous bounds<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b76">77]</ref> exist in homogenization theory but are not applied here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>We always used λ lo = 1 and set λ hi = c. While the coercivity constant of the PDE depends on λ lo , the data-driven model proposed was found to be insensitive to this.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>[k]f as input, features 89-100 use the whole vector λ f .</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subgrid Upscaling and Mixed Multiscale Finite Elements</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arbogast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1150" to="1171" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Variational Bayesian EM Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Learning Deep Architectures for AI, Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Free energy computations by minimization of Kullback-Leibler divergence: An efficient adaptive biasing potential method for sparse representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="page" from="3849" to="3870" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-output local Gaussian process regression: Applications to uncertainty quantification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="page" from="5718" to="5746" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional Adaptive Relevance Vector Machines for Uncertainty Quantification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="881" to="B908" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-output separable Gaussian process: Towards an efficient, fully Bayesian paradigm for uncertainty quantification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Konomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="page" from="212" to="239" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York, 1</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>st ed. 2006. corr. 2nd printing 2011 ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational Relevance Vector Machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="652" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Berechnung verschiedener physikalischer konstanten von heterogenen substanzen. i. dielektrizittskonstanten und leitfhigkeiten der mischkrper aus isotropen substanzen</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A G</forename><surname>Bruggeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annalen der Physik</title>
		<imprint>
			<biblScope unit="volume">416</biblScope>
			<biblScope unit="page" from="636" to="664" />
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reduced Basis Methods for Uncertainty Quantification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM/ASA Journal on Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="813" to="869" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Chernatynskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Phillpot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty Quantification in Multiscale Simulation of Materials: A Prospective</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="157" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Constantine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google-Books-ID: TOJ9BwAAQBAJ</title>
		<imprint>
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven model reduction for the Bayesian solution of inverse problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Numerical Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="966" to="990" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiscale finite element methods for porous media flows and their applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Efendiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APPLIED NUMERICAL MATHEMATICS</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="577" to="596" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Least angle regression</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reduced Basis Collocation Methods for Partial Differential Equations with Random Coefficients</title>
		<author>
			<persName><forename type="first">H</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM/ASA Journal on Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="192" to="217" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of Sparse Bayesian Learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="383" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive sparseness for supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1150" to="1159" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">non-linear dimension reduction methodology for generating data-driven stochastic input models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ganapathysubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="6612" to="6637" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient reduced-basis treatment of nonaffine and nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Grepl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Maday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Patera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESAIM: Mathematical Modelling and Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="575" to="605" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic Reduced-Order Modeling for Stochastic Partial Differential Equations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Grigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Uncertainty Quantification in Computational Sciences and Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="111" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian lasso regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="835" to="845" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A variational approach to the theory of the elastic behaviour of multiphase materials</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shtrikman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Mechanics and Physics of Solids</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Numerical solution of implicitly constrained optimization problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinkenschloss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
		<respStmt>
			<orgName>Rice University Department of Computational and .</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Certified Reduced Basis Methods for Parametrized Partial Differential Equations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hesthaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Briefs in Mathematics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic Variational Inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting the output from a complex computer code when fast approximations are available</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIOMETRIKA</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Relevance</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic characterization and simulation of multi-phase random media</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probabilistic Engineering Mechanics</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accurate Uncertainty Quantification Using Inaccurate Computational Models</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3274" to="3300" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Special Issue: Big data and predictive computational modeling</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="1252" to="1254" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning via Gaussian Processes</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">December 13-18, 2004. 2004</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Characterization of Porous Solids and Powders: Surface Area</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thommes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pore Size and Density</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lineal-path function for random heterogeneous materials</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Torquato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="922" to="929" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An adaptive hierarchical sparse grid collocation algorithm for the solution of stochastic differential equations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="page" from="3084" to="3113" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis for stochastic input model generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page" from="7311" to="7331" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bayesian Methods for Backpropagation Networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="211" to="254" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">Information Theory, Inference and Learning Algorithms</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003-09">Sept. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A Generalized Empirical Interpolation Method: Application of Reduced Basis Techniques to Data Assimilation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Maday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mula</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="221" to="235" />
			<pubPlace>Milan, Milano</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of predictive nonlinear theories for multiscale modeling of heterogeneous materials</title>
		<author>
			<persName><forename type="first">K</forename><surname>Matou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G D</forename><surname>Geers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="192" to="220" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Concurrent design of hierarchical materials and structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Modeling and Simulation SMNS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="207" to="240" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Effective properties of composite materials with periodic microstructure: a computational approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moulinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="109" to="143" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Homogenization of inelastic solid materials at finite strains based on incremental minimization principles. Application to the texture analysis of polycrystals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Miehe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lambrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Mechanics and Physics of Solids</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2123" to="2167" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Branch and Bound Algorithm for Feature Subset Selection</title>
		<author>
			<persName><forename type="first">Fukunaga</forename><surname>Narendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="917" to="922" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Reduced basis technique for nonlinear analysis of structures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Noor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA J</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Designing a New Material World</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page" from="993" to="998" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ostoja-Starzewski</surname></persName>
		</author>
		<title level="m">Microstructural Randomness and Scaling in Mechanics of Materials</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Variational Bayesian inference with stochastic search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</editor>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Key computational modeling issues in Integrated Computational Materials Engineering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Panchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kalidindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Mcdowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Design</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4" to="25" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Model inversion via multi-fidelity Bayesian optimization: a new paradigm for parameter estimation in haemodynamics, and beyond</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Society Interface</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Negri</surname></persName>
		</author>
		<title level="m">Reduced Basis Methods for Partial Differential Equations: An Introduction, La Matematica per il 3+2</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monte</forename><surname>Bayesian</surname></persName>
		</author>
		<author>
			<persName><surname>Carlo</surname></persName>
		</author>
		<title level="m">Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02</title>
		<meeting>the 15th International Conference on Neural Information Processing Systems, NIPS&apos;02<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sequential Operations in Digital Picture Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Pfaltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="471" to="494" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dimensionality Reduction by Locally Linear Embedding, Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simulation of multi-dimensional stochastic processes by spectral representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shinozuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deodatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASME Applied Mechanics Reviews</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="29" to="53" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
		<title level="m">Morphological Image Analysis: Principles and Applications</title>
		<meeting><address><addrLine>Berlin Heidelberg, Berlin, DE</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="89" to="125" />
		</imprint>
	</monogr>
	<note>Opening and Closing</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A multi-length scale sensitivity analysis for the control of texture-dependent properties in deformation processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sundararaghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Plasticity</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1581" to="1605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">2319</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Probabilistic visualisation of high-dimensional binary data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="592" to="598" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The relevance vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sparse Bayesian Learning and the Relevance Vector Machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Probabilistic Principal Component Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The Information Bottleneck Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing</title>
		<meeting>of the 37-th Annual Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Random Heterogeneous Materials</title>
		<author>
			<persName><forename type="first">S</forename><surname>Torquato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Microstructure of twophase random media. I. The n-point probability functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Torquato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="2071" to="2077" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Certified real-time solution of the parametrized steady incompressible Navier-Stokes equations; Rigorous reduced-basis a posteriori error bounds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Patera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Numerical Methods in Fluids</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="773" to="788" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
		<title level="m">Bayesian Factor Regression Models in the &quot;Large p, Small n&quot; Paradigm, Bayesian Statistics</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Homogeneous Chaos</title>
		<imprint>
			<date type="published" when="1938">1938</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="897" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning for basis selection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="2153" to="2164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Reduced dimensional Gaussian process emulators of parametrized partial differential equations based on Isomap</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">471</biblScope>
			<biblScope unit="page">20140697</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Manifold learning for the emulation of spatial fields from computational models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Triantafyllidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="666" to="690" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">High-Order Collocation Methods for Differential Equations with Random Inputs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hesthaven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1118" to="1139" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yip</surname></persName>
		</author>
		<title level="m">Handbook of Materials Modeling</title>
		<meeting><address><addrLine>Dordrecht; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005-06">2005. June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
