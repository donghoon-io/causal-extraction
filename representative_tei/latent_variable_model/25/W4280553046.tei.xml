<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Multi-Form Bayesian Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-23">23 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Guo</surname></persName>
							<email>ericzhendong@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Haitao</forename><surname>Liu</surname></persName>
							<email>htliu@dlut.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Yew-Soon</forename><surname>Ong</surname></persName>
							<email>oyew-soon@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xinghua</forename><surname>Qu</surname></persName>
							<email>xinghua001@e.ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Yuzhe</forename><surname>Zhang</surname></persName>
							<email>zhang.yz@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
							<email>asjmzheng@ntu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">†</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">§</forename><forename type="middle">X</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Science and Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineer- ing</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Energy and Power Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<region>Dalian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Computational Intelligence Lab</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Multi-Form Bayesian Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-23">23 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2501.13337v1[cs.CE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Model based Optimization</term>
					<term>Multi-Form Optimization</term>
					<term>Transfer Optimization</term>
					<term>Bayesian Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real-world problems, such as airfoil design, involve optimizing a black-box expensive objective function over complex structured input space (e.g., discrete space or non-Euclidean space). By mapping the complex structured input space into a latent space of dozens of variables, a two-stage procedure labeled as generative model based optimization (GMO) in this paper, shows promise in solving such problems. However, the latent dimension of GMO is hard to determine, which may trigger the conflicting issue between desirable solution accuracy and convergence rate. To address the above issue, we propose a multi-form GMO approach, namely generative multi-form optimization (GMFoO), which conducts optimization over multiple latent spaces simultaneously to complement each other. More specifically, we devise a generative model which promotes positive correlation between latent spaces to facilitate effective knowledge transfer in GMFoO. And further, by using Bayesian optimization (BO) as the optimizer, we propose two strategies to exchange information between these latent spaces continuously. Experimental results are presented on airfoil and corbel design problems and an area maximization problem as well to demonstrate that our proposed GMFoO converges to better designs on a limited computational budget.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many real-world problems, such as airfoil <ref type="bibr" target="#b0">[1]</ref> or hull-form shape design <ref type="bibr" target="#b1">[2]</ref> involve optimizing a blackbox expensive objective function over complex structured input space. According to <ref type="bibr" target="#b2">[3]</ref>, the structured input space refers to discrete space <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> or non-Euclidean space <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which has the following characteristics, i.e., <ref type="bibr" target="#b0">(1)</ref> the number of variables in the structured input space are 100+, and (2) the variables in the original structured input space can be highly interacted. Hence, solving such kind of problems is challenging.</p><p>One promising way to tackle the above challenging problems is a two-stage procedure that has emerged over the past few years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which is labeled as generative model based optimization (GMO) in this paper. The general process of GMO is as follows. First, a generative model g : Z → X that maps the structured input space X into a continuous latent space Z with dozens of variables, is built. Then, optimization is carried out over this learnt latent space Z. Following this way, the original challenging problem becomes more accessible.</p><p>Though a series of successful stories has been reported <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, the effectiveness of GMO can be influenced by the latent dimension of generative model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. That is, when the latent dimension is set high, the convergence rate of GMO can drop dramatically with the increase of latent dimension, which is also known as the issue of "curse of dimensionality" <ref type="bibr" target="#b13">[14]</ref>. On the contrary, if the latent dimension is set low, the solution accuracy in such low-dimensional latent space can be limited, resulting in sub-optimal final solutions <ref type="bibr" target="#b3">[4]</ref>. In other words, there is a balance between desirable solution accuracy and convergence rate when selecting the dimension of latent space for GMO.</p><p>To address the above balance issue, we propose a multi-form optimization approach, namely generative multi-form optimization (GMFoO). In particular, in contrast to the existing efforts that consider single latent space, we propose to include multiple latent spaces in GMFoO. Moreover, we propose to carry out optimizations over multiple latent spaces at the same time so as to benefit from each other. The above idea is inspired by the emerging topic known as transfer optimization (TO) <ref type="bibr" target="#b14">[15]</ref>, which attempts to gain knowledge from the related tasks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> or alternate formulations <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> to achieve better solutions with even fewer computational costs. In particular for our proposed GMFoO, the quicker optimization progress of the low-dimensional latent space can help to accelerate the process of the highdimensional latent space. In turn, the better solutions in the high-dimensional latent space may help to resolve the solution accuracy issue in low-dimensional latent space. Thereby, GMFoO can take a balance in between desirable solution accuracy and convergence rate to achieve the optimal solutions efficiently.</p><p>Also note that, various algorithms such as evolutionary algorithms (EA) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, particle swarm optimization (PSO) algorithms <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> can be used in GMO for the optimization over latent space. Among them, Bayesian optimization (BO) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> gains particular attentions in the GMO paradigm <ref type="bibr" target="#b27">[28]</ref>, since it is well-known to be sample-efficient for solving expensive black-box problems <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Therefore, we study GMFoO with BO in this paper.</p><p>The main contribution of this paper can be summarized as follows:</p><p>• We propose the novel GMFoO framework, for solving black-box problems that involve complex structured input space. Instead of optimizing in single latent space as most GMO studies do, the GMFoO conducts optimization over multiple latent spaces simultaneously to benefit from each other, therefore achieving a balance between desirable solution accuracy and convergence rate.</p><p>• Particularly, we devise a generative model that promotes positive correlations among latent spaces in the training stage to facilitate effective knowledge transfer in GMFoO. And further, we instantiates the proposed GMFoO with BO, where two strategies are proposed to exchange information between the latent spaces continuously.</p><p>• Through tests on the airfoil and corbel design problems and an area maximization problem, the effectiveness of GMFoO has been well demonstrated.</p><p>The remainder of this paper is organized as follows. Section II presents the preliminaries. And then, Section III illustrates the details of GMFoO framework and the related algorithm. After that, Section IV shows the experimental studies. Finally, we draw conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In order to outline the contributions of this work more clearly, we provide preliminaries on some related topics in this section, such as generative model based optimization, multi-form optimization and Bayesian optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Model-based Optimization</head><p>The GMO paradigm can be formulated as:</p><formula xml:id="formula_0">min x∈X f (x) = min z∈Z f (g(z)) ≈ min z∈Z y(z) (1)</formula><p>where, X is the structured input space, Z is the latent space learnt by using a generative model g, and y is the latent objective model to approximate the objective function f . While various generative models such as variational autoencoder (VAE) <ref type="bibr" target="#b30">[31]</ref> can be employed, here the generative adversarial net (GAN) <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> is used to illustrate the process of mapping the structured input space into latent space. As shown in Fig. <ref type="figure">1</ref>, GAN is consisted of two components as the generator and the discriminator. With the input of latent vector z, the generator is trained to synthesize the structured object, the discriminator is trained to distinguish the synthesized objects x ′ (also denoted by G(z) in Eq.( <ref type="formula">2</ref>)) from the real data x, with the following adversarial training loss: where, D(x) and D(G(z)) are the probabilities that the samples coming from the real dataset X or the generator, respectively. After training, the generator of GAN can synthesize various x by varying z.</p><formula xml:id="formula_1">min G max D V (D, G) = E x∼P data [log D (x)] + E z∼Pz [log (1 -D (G (z)))] (2)</formula><p>Then, optimization can be carried out over this learnt latent space with latent vector z. Correspondingly, the related pseudo code of standard GMO procedure is summarized in Algorithm 1. Thus far, many successful applications of GMO has been reported <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, further work is required to improve the performance of GMO. For instance, as discussed in the Introduction part, the selection of latent dimension can greatly influence the GMO performance, which may trigger the conflicting issue between desirable solution accuracy and convergence rate. To this end, the key contribution of this work is that, instead of optimizing in single latent space as most GMO approaches do, we propose a multi-form approach, which conducts optimization in multiple latent spaces at the same time to address the above issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transfer Optimization and Multi-Form Optimization</head><p>Instead of starting the optimization search from scratch as most traditional approaches do, transfer optimization (TO) proposes to gain knowledge from the related tasks to achieve better solutions with even fewer computational costs <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Recently, several comprehensive and insightful reviews of TO have been reported <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. According to <ref type="bibr" target="#b14">[15]</ref>, TO can be classified into three categories, i.e., sequential transfer optimization (STO) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>, multitasking optimization (MTO) <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> and multiform optimization (MFoO) <ref type="bibr" target="#b20">[21]</ref>. While STO and MTO deal with distinct optimization tasks that are presumed to be related, MFoO exploits alternate formulations of a single target task.</p><p>More formally, in hope to improve the efficiency of an search algorithm, MFoO proposes joint optimization of several alternate formulations, with continuous knowledge transfer between them:</p><formula xml:id="formula_2">T = min x∈X f (x)<label>(3)</label></formula><formula xml:id="formula_3">Q M F oO t (T |T 1 , • • • , T K , M (t)) -Q t (T ) ≥ 0 (4)</formula><p>where, T denotes the target optimization task, Q t is a measure quantifying the quality of solution(s) obtained at the t th iteration, T 1 , • • • , T K are alternate formulations of T , and M (t) is the knowledge base to facilitate knowledge transfer between the alternate formulations.</p><p>The previous efforts of MFoO in building alternate formation T k can be mainly categorized into two groups. The first group of studies propose to build T k by reformulating the objective function as f k :</p><formula xml:id="formula_4">T k = min x∈X f k (x, M (t))<label>(5)</label></formula><p>For instance, some works attempt to multiobjectivize a single-objective problem of interest to remove the local optima <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, some others propose to augment the expensive high-fidelity objective evaluations by a large number of low-fidelity but cheap samples to accelerate the optimization progress <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The second group of MFoO studies propose to build T k by constructing alternate searching space X k :</p><formula xml:id="formula_5">T k = min x∈X k f k (x k , M (t))<label>(6)</label></formula><p>For example, some work proposes to carry out multi-form optimization with both constrained and unconstrained formulations <ref type="bibr" target="#b45">[46]</ref>. Alternatively, some studies proposes to divide the full design space into subset spaces. Then, optimizations are conducted over subset spaces by fixing the remaining variables as constant as those of the current best solution <ref type="bibr" target="#b46">[47]</ref>. The work in this paper belongs to the second group of MFoO shown in Eq.( <ref type="formula" target="#formula_5">6</ref>). Compared to the existing studies, the distinction of this work is that, we use a generative model to promote positive correlation between the alternate searching spaces in our work. And accordingly, the optimizations in these correlated searching spaces can complement each other to achieve the optimal solution efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bayesian Optimization (BO)</head><p>Due to its sample efficiency <ref type="bibr" target="#b47">[48]</ref>, BO is most frequently used as the optimizer in GMO <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The general procedure of BO is shown in Algorithm 2. First, it uses Gaussian process (GP) <ref type="bibr" target="#b48">[49]</ref> to build a surrogate of the objective function. Second, it employs an acquisition function which incorporates the GP surrogate to select the next promising solution candidates to sample, and simulations are conducted to evaluate the new sample candidate. Third, the new sample is added to the training set for the next iteration.</p><p>As a key component of BO, GP is often used to approximate the input-output relation, which formulates the function prediction Y (x) as a Gaussian random variable:</p><formula xml:id="formula_6">Y (x) ∼ GP(m(x), k(x, x ′ ))<label>(7)</label></formula><p>where m(x) denotes the mean function, and k(x, x ′ ) is the covariance function between samples x and x ′ . For standard GP with samples from a single source, the posterior estimates of the mean function prediction (ŷ) and the corresponding mean squared error (σ 2 ) at a point x are expressed as:</p><formula xml:id="formula_7">ŷ(x) = k(x, X) K + σ 2 n I -1 y σ2 (x) = k(x, x) -k(x, X) K + σ 2 n I -1 k(X, x) + σ 2 n (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where, σ 2 n is the Gaussian noise, K is the covariance matrix over the input features of samples X, and k is the cross-correlation vector between x and X.</p><p>As another key component of BO, the acquisition function acts as a surrogate that determines which sample to be evaluated next. Among various acquisition functions such as probability of improvement (PI) <ref type="bibr" target="#b47">[48]</ref>, upper-bound confidence (UCB) <ref type="bibr" target="#b49">[50]</ref>, etc., expected improvement (EI) <ref type="bibr" target="#b50">[51]</ref> is most frequently used. Assuming that the function prediction Y (x) comes from a GP with the posterior estimate as Y (x) ∼ N (ŷ(x), σ 2 (x)), the basic idea of EI is to measure how much objective improvement can be attained with respect to the current best solution f min , which is formulated as:</p><formula xml:id="formula_9">EI(x) = (f min -ŷ(x))Φ (u) + σ(x)ϕ (u) u = (fmin -ŷ(x))/σ(x)<label>(9)</label></formula><p>where, Φ (•) and ϕ (•) denote the standard normal distribution function and density function. In each optimization cycle, we select the next point to sample by maximizing the EI: Note that most BO algorithms work well for optimization over domains of moderate dimensions <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. And accordingly, the effectiveness of GMO can become questionable when the dimension of latent space is set too large (e.g., larger than 15). From this end, the highlight of this work is that, we propose a multi-form approach that makes use of multiple searching spaces to help resolve the limitations of BO.</p><formula xml:id="formula_10">x * = arg max x EI(x)<label>(10)</label></formula><p>With the above, it is not hard to comprehend that this work aims to solve the black-box problems that involve complex structured input space, by proposing a novel GMO approach. In particular, instead of optimizing in single latent space as most GMO studies do, we propose a multi-form GMO approach, which conducts optimization in multiple latent spaces at the same time to achieve a balance between convergence rate and desirable solution accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first show the general framework of GMFoO, then we instantiates GMFoO with BO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GMFoO framework</head><p>Given that a set of latent spaces are learnt by using a generative model, the GMFoO framework can be formulated as: where, X is the structured input space of a black-box problem, T 0 , T 1 , • • • , T m are the "sub-tasks" that conduct optimization in one of the learnt latent space, and M (t) is the knowledge base to facilitate information exchange among the "sub-tasks". Furthermore, in order to achieve a balance between desirable solution accuracy and convergence rate, we propose to include one high-dimensional latent space Z and one or several low-dimensional latent spaces C 1 , • • • , C m in the generative model g. And accordingly, the "sub-tasks" in GMFoO can be expressed as:</p><formula xml:id="formula_11">min x∈X f (x) = min{T 0 , T 1 , • • • , T m |M (t)}<label>(</label></formula><formula xml:id="formula_12">T 0 = min f (g(z)) ≈ min z∈Z y 0 (z) T i = min f (g(c i )) ≈ min ci∈Ci y i (c i ), i = 1, • • • , m<label>(12)</label></formula><p>Three questions may arise when instantiating the GMFoO framework. That is, (1) how to train Z and C 1 , • • • , C m to be correlated so as to promote effective knowledge transfer; (2) how to exchange samples between Z and C 1 , • • • , C m ; and (3) how to build M (t) to help the "sub-tasks" to benefit from each other. The questions (1) and ( <ref type="formula">2</ref>) mainly concerns how to build the generative model, which will be discussed in this subsection. And (3) is related to the proposed optimization algorithm, which will be discussed in the next subsection.</p><p>Figure <ref type="figure">2</ref> shows the GMFoO framework we proposed to answer the questions ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>), where the generative model is labeled as MFoO-GAN, i.e., GAN for generative multi-form optimization. More specifically, to promote positive correlations among Z and C i , we follow the idea of Info-GAN <ref type="bibr" target="#b51">[52]</ref> by imposing a regularization term of maximizing the mutual information I (c i , G(z)) in the training loss:</p><formula xml:id="formula_13">min G max D V M (D, G) = V (D, G) - m i=1 λ i I (c i , G (z))<label>(13)</label></formula><p>where, λ i is the weight of a related regularization term. For simplicity, we use c and c ′ to denote c i and c ′ i , respectively, in the following paragraphs. Let P (c|x) denote the real probabilistic distribution of the inverse inference of x, and Q(c|x) a factored Gaussian distribution that used to approximate P (c|x), the mutual information I (c, G(z)) can be derived as:</p><formula xml:id="formula_14">I(c; G(z)) = H(c) -H(c|G(z)) = E x∼G(z) E c∼P(c|x) [log P (c|x)] + H(c) = E x∼G(z) [DKL (P (•|x)||Q(•|x))] ≥0 + E c∼P (c|x) [log Q(c|x)] + H(c) ≥ E x∼G(z) E c∼P (c|x) [log Q(c|x)] + H(c) = - |c| j=1 (cj -µj) 2 2σ 2 j + log σj √ 2π + H(c) ≡ LI (c, G(z))<label>(14)</label></formula><p>where, H(•) is the information entropy, and µ j and σ 2 j are the mean and standard derivation of a Gaussian distribution for the j th component of the latent variable c. Additionally, L I (c, G(z)) is the lower bound of I (c, G(z)).</p><p>Since I (c i , G(z)) is only active when training the generator, we can pick out the regularization term from Eq. <ref type="bibr" target="#b12">(13)</ref>, with denoting µ j = c ′ j and fixing H(c) as a constant, as below:</p><formula xml:id="formula_15">min -LI (c, G(z)) ≃ min |c| j=1 (cj -cj ′ ) 2 2σ 2 j + log σj √ 2π<label>(15)</label></formula><p>Then, by minimizing the gap between c and the inverse inference c ′ through Eq.( <ref type="formula" target="#formula_15">15</ref>), the major feature of the the object G(z) is captured by c, and the low-dimensional latent space C is trained to be correlated with the high-dimensional latent space Z.</p><p>For the sample exchange from the low-dimensional latent space C to the high-dimensional latent space Z, we propose to fix the remaining variables (denoted by z * ) of z = [c, z * ] as constants, e.g., fixing z * as zeros as shown in Fig. <ref type="figure">3(a)</ref>. The benefits of doing so can be as follows. First, fixing z * helps to prioritize the optimization search in directions of major variability of the object x, and thus helping to accelerate the overall optimization progress. Second, by fixing z * instead of assigning it as a random noise, the samples in the latent space C can be reused in the next iteration, without worrying the negative influence caused by the randomness of z * . Additionally, fixing z * makes it easy to transform samples from the low-dimensional latent space to the high-dimensional latent space.</p><p>Conversely, for the sample exchange from the high-dimensional latent space Z to the low-dimensional latent space C, we propose to use the inverse inference structure of the discriminator, as shown in Fig. <ref type="figure">3(b)</ref>. More specifically, c</p><p>′ is the mean of the posterior inverse inference of x with the learnt probability Q(c ′ |x).</p><p>Compared to c as a prior component of z = [c, z * ] to generate the object x, c ′ also takes the variability of</p><p>x caused by z * into account via σ 2 j (see Eq.( <ref type="formula" target="#formula_14">14</ref>)) in the training process. In other words, c ′ can capture the major features of x with better robustness when doing the inverse inference. Therefore, we propose to use c ′ with the process shown in Fig. <ref type="figure">3</ref>(b) to transform samples from the high-dimensional latent space to the low-dimensional latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An instantiation of GMFoO with BO</head><p>While different versions of GMFoO can be proposed by building different knowledge base M (t), one possible instantiation is presented in this subsection. With the focus on discussing how to build knowledge base M (t) to help the "sub-tasks" to benefit from each other, only one high-dimensional and one lowdimensional latent spaces are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Enhanced Local Exploitation</head><p>As a subset of the high-dimensional latent space Z, the low-dimensional latent space C of MFoO-GAN can capture the major variability of the target object. It means, the current best solution of C, i.e., c min , can be located in the neighborhood of the real optimal solution of Z. In other words, c min can be used as an indicator to enhance the local exploitation of the promising areas of Z. Taking the cue, we build knowledge base M (t) as shown in Fig. <ref type="figure">4</ref>, where we use c min to narrow the searching space of Z. Considering that the neighborhood of the optimal solution of Z and C may not be exactly overlapped, an additional variable ∆ is used to control the size of the narrowed space. The influence of changing ∆ will be discussed in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GP-based Multi-Fidelity Optimization</head><p>Note that the sample efficiency of BO is highly related to the accuracy of the GP surrogate. One direct way to improve the GP accuracy is to increase the training samples. However, as discussed with Fig. <ref type="figure">3</ref>, there is accuracy loss when transforming the samples from the high-dimensional latent space Z to the low-dimensional latent space C. In the meantime, directly injecting the samples of C into Z may further enhance the exploitation of these major directions, but ignoring exploration along the remaining directions (i.e., z * from z = [c, z * ]). In view of the above, we propose to treat the samples transformed from both Z and C as low-fidelity samples. And accordingly, we propose to build multi-fidelity GP (MFGP), which utilizes the covariance matrix as knowledge base M (t) to extract information from the low-fidelity samples to improve the surrogate accuracy.</p><p>More specifically, we propose to build two MFGP surrogates to carry out optimization in GMFoO. Let C and Z denote the high-fidelity samples of C and Z, respectively, and C</p><p>′ and Z ′ the low-fidelity samples transformed from Z and C, respectively. In the meantime, y(C) and y(Z) are the vectors of objective function values of related samples. Then, the correlation vector and covariance matrix of MFGP built in the low-dimensional latent space are formulated as:</p><formula xml:id="formula_16">kM (c, C * ) = ρ11k(c, C), ρ12k(c, C ′ ) KM (C * ) = ρ11K (C) ρ12K (C, C ′ ) ρ21K (C ′ , C) ρ22K (C ′ ) + D<label>(16)</label></formula><p>where, k and K are the correlation vector and matrix formulation of input variables (see Eq. </p><formula xml:id="formula_17">ŷ (c) = k M (c, C * ) K -1 M (C * ) y (C) y (Z) σ 2 (c) = k M (c) -k M (c, C * ) K -1 M k M (C * , c) + σ 2 n,1<label>(17)</label></formula><p>Similarly, the MFGP built in the high-dimensional latent space can be formulated as:</p><formula xml:id="formula_18">kM (z, Z * ) = ρ11k(z, Z), ρ12k(z, Z ′ ) KM (Z * ) = ρ11K(Z) ρ12K(Z, Z ′ ) ρ21K(Z ′ , Z) ρ11(Z ′ ) + D<label>(18) ŷ</label></formula><formula xml:id="formula_19">(z) = k M (z, Z * ) K -1 M (Z * ) y (Z) y (C) σ 2 (z) = k M (z) -k M (z, Z * ) K -1 M k M (Z * , z) + σ 2 n,2<label>(19)</label></formula><p>Correspondingly, the EI acquisition function for the sample search in the low-and high-dimensional latent space can be formulated as:</p><formula xml:id="formula_20">EIc(c) = (f min -ŷ(c))Φ (uc) + σ(c)ϕ (uc) uc = (fmin -ŷc(c))/σc(c)<label>(20)</label></formula><formula xml:id="formula_21">EIz(z) = (f min -ŷ(z))Φ (uz) + σ(z)ϕ (uz) uz = (fmin -ŷz(z))/σz(z)<label>(21)</label></formula><p>With the above, Fig. <ref type="figure" target="#fig_4">6</ref> shows the flowchart of our proposed GMFoO algorithm, which is also summarized in Algorithm 3. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>, when conducting optimizations over the high-and lowdimensional latent spaces simultaneously, knowledge transfer is carried out in each iteration to benefit from each other. Therefore, our proposed GMFoO can be expected to take a good balance between solution accuracy and convergence rate for solving black-box problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, tests are carried out on airfoil and corbel design problems and an area maximization problem as well to show the efficacy of our proposed GMFoO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines for comparison</head><p>To examine the performance of GMFoO, we compare it against four kinds of algorithms as shown below:</p><p>(1) To show the advantage of multi-form optimization in GMFoO, we compare it against the conventional GMOs using single latent space. More specifically, the conventional GMOs are carried out with the high-and low-dimensional latent space of MFoO-GAN, respectively, and the standard BO algorithm is used as the optimizer. The related GMO procedures are labeled as GMO-High and GMO-Low, respectively. (2) To illustrate the advantage of GMFoO over traditional MFoO method, we compare it with a GMO which uses a multi-form BO algorithm (labeled as NashEGO <ref type="bibr" target="#b46">[47]</ref>) to optimize the high-dimensional latent space of MFoO-GAN. The related procedure is labeled as GMO-NashEGO. The main difference between GMO-NashEGO and GMFoO is as follows. The alternate searching spaces of GMO-NashEGO are generated randomly. Differently, we promote positive correlation between the alternate searching spaces with MFoO-GAN, and thereby facilitating effective information exchange in multi-form optimization process.</p><p>(3) To show the advantage of GMFoO over non-generative method, the dimension-reduction (DR) method is also selected for the test. In particular, we follow the procedure in <ref type="bibr" target="#b52">[53]</ref>, the singular value decomposition (SVD) approach is used to map the structured input space into a continuous latent space. Then, optimization is carried out in the related latent space with the standard BO algorithm, which is denoted by SVD-BO.</p><p>(4) To inspect the influence of optimization algorithms on the performance of GMO, the state of the art genetic algorithm CMA-ES <ref type="bibr" target="#b53">[54]</ref> and a recently proposed kriging-assisted evolutionary algorithm labeled as IKEA <ref type="bibr" target="#b54">[55]</ref> are used for the optimization over high-dimensional latent space of MFoO-GAN. Accordingly, the related GMO procedures are labeled as GMO-CMAES and GMO-IKEA, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementing Details</head><p>Tensorflow and PyTorch packages are used to train the MFoO-GAN, and Matlab is used to build the optimization algorithms. Then, a PERL script is programmed to build the connections between Python and Matlab codes. More specifically, the GPML toolbox <ref type="bibr" target="#b48">[49]</ref> is used to build the BO algorithms such as the standard BO, NashEGO and the optimizer of GMFoO. Meanwhile, the code of IKEA published by the authors in Github is used, and the code of CMA-ES is downloaded from the Mathworks<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>The Latin hypercube sampling <ref type="bibr" target="#b55">[56]</ref> is used for the design of experiment (DoE) for the BO algorithms and IKEA. The number of initial training samples for the standard BO and IKEA are set as 11 times of the dimension of the low-dimensional latent space of MFoO-GAN (i.e., 11d L ). For the optimizer of GMFoO, the initial training samples are split into halves for the optimization in high-and low-dimensional latent space, respectively. Note that the NashEGO and CMA-ES algorithms start the optimization search with a random sample. This starting point is set as the best solution of the DoEs of the other compared algorithms. Meanwhile, the population size of CMA-ES and IKEA are set to be 11d L .</p><p>Additionally, two parameters can be tuned in GMFoO, which are the size parameter ∆ of narrowed high-dimensional latent space and the dimension of low-dimensional latent space d L . By default, we set Build or Update Multi-Fidelity GP: Build multi-fidelity GP surrogate in latent space Z i (see Eqs.( <ref type="formula" target="#formula_17">17</ref>) and ( <ref type="formula" target="#formula_19">19</ref>)) Exploitation in the high-dimensional latent space:</p><p>a. Update the narrowed space of Z (see Fig. <ref type="figure">4</ref>) b. Sample search in the narrowed space of Z with EI c. Samples exchange between Z and C (see Fig. <ref type="figure">3</ref>) 10: end while ∆ = 0.15, and d L is varied in the range of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> in the following tests. After that, sensitivity analysis are carried out to discuss the influence of ∆ and d L on GMFoO performance in subsection F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tests on Airfoil Design Problems</head><p>The airfoil shape optimization has been extensively used as a benchmark to showcase the effectiveness of newly proposed algorithms <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Figure <ref type="figure">7</ref> shows a brief introduction of the airfoil design. The forces that acting on the airfoil can be decomposed into two components, i.e., the drag D and Lift L, which are the functions of airfoil contours. Meanwhile, the airfoil contour of the best design performance may vary according to the working condition that defined by the Mach number (M a ∞ ), Reynolds number (Re) and the angle of attack (AoA).</p><p>For the airfoil design optimization in this work, 192 points are used to describe the airfoil contour. Accordingly, there are 384 design variables in this structured space. More importantly, these design variables are highly interacted in order to generate meaningful airfoil contours. Instead of carrying out optimization in such high-dimensional structured space directly, we use MFoO-GAN to map it into continuous latent spaces with dozens of variables. The UIUC dataset<ref type="foot" target="#foot_1">foot_1</ref> is used as the training set. As a comparison baseline, the non-generative model SVD is also used to train the latent space with the UIUC dataset. After that, the performance of GMFoO is examined through the optimization of airfoils that working under low-speed and subsonic conditions, respectively. More detailed descriptions are given as below:</p><p>1. Design Optimization for a Low-Speed Airfoil The low-speed airfoils have been widely used for unmanned aerial vehicles (UAVs) <ref type="bibr" target="#b57">[58]</ref>. Following the settings in <ref type="bibr" target="#b7">[8]</ref>, the design objective is to maximize the lift to drag ratio, L/D, with the working condition set as Re = 1.8 × 10 6 , Mach number M a ∞ = 0.01, and AoA = 0 deg. The XFOIL <ref type="bibr" target="#b58">[59]</ref> is used to compute L/D. Meanwhile, the dimension of high-and low-dimensional latent spaces of MFoO-GAN are set to be 13 and 3, respectively. The dimension of latent space of SVD is also set to be 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Design Optimization for a Subsonic Airfoil</head><p>Similar to the low-speed airfoil design, we still use L/D to optimize the subsonic airfoil with XFOIL. By refering to <ref type="bibr" target="#b0">[1]</ref>, the working condition of the the target airfoil is set as follows, i.e., Re = 1.8 × 10 6 , M a ∞ = 0.45 and AoA = 0 deg. And further, to test the effectiveness of GMFoO in different settings, the dimension of high-and low-dimensional latent spaces of MFoO-GAN are set to be 23 and 4, respectively. For SVD, the dimension of latent space is set to be 23.  As shown in Figs. <ref type="figure">8(a</ref>) and (b), both the convergence rates and final optimal solutions of GMFoO are better than those of SVD-BO, which are consistent with the observations in <ref type="bibr" target="#b3">[4]</ref>. The reason behind is as follows. That is, SVD is essentially a linear dimension reduction model. Hence, the solution accuracy of the related latent space of SVD can be worse than those of nonlinear deep learning models such as MFoO-GAN, resulting in the poor performance of SVD-BO.</p><p>Among GMFoO, GMO-High and GMO-Low, GMO-Low converges most quickly at its optimal solution, while GMO-High always achieves better final solutions than those of GMO-Low. In the meantime, the convergence rate of GMO-High (in 23-dimensional design space) is observed to be slower than that of GMO-Low (in 4-dimensional design space), as shown in Fig. <ref type="figure" target="#fig_7">9(b)</ref>. Differently, by carrying out optimizations in both high-and low-dimensional latent spaces simultaneously to complement each other, our proposed GMFoO takes a good balance in between solution accuracy and convergence rate, which always achieves the best solutions with even faster convergence rates.</p><p>Though carrying out multi-form optimization with alternate subspaces, the performance of GMO-NashEGO looks poorer than those optimizing over single latent space, such as GMO-High and GMO-Low. The reason behind can be explained as follows. Firstly, unlike GMFoO which promotes positive correlations between alternate searching spaces to facilitate effective knowledge transfer, the subspaces are generated randomly in GMO-NashEGO. And accordingly, the knowledge transfer between alternate formulations in GMO-NashEGO may be not that efficient. Hence, the performance of GMO-NashEGO are poor as show in our testing cases.</p><p>Among GMFoO, GMO-IKEA, GMO-CMAES and GMO-High, the convergence rates of EA-based GMOs, i.e., GMO-IKEA and GMO-CMAES, are slower than those of BO-based GMOs, i.e., GMO-High and GMFoO. This can be easy to be understood, as the BO algorithms are usually more efficient than the EA algorithms when solving problems in moderate dimension. In the meantime, owing to the excellent global search performance of EA algorithms, the final solutions of IKEA looks slightly better than those of GMO-High and GMFoO, when optimizing over the 13-dimensional latent space for the low-speed airfoil (see Fig. <ref type="figure">8(b)</ref>). However, when increasing the latent dimension from 13 to 23, as shown in Fig. <ref type="figure" target="#fig_7">9</ref>(b), GMO-High and GMFoO achieve better solutions than those of GMO-IKEA within sample budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">on Decorative Corbel Design Problems</head><p>Corbel is a common category of decorative architectural geometry, as shown in Fig. <ref type="figure">10(a)</ref>. When designing corbels, we need to meet both aesthetics and mechanical requirements, i.e., the decorative corbel needs to look beautiful while maintaining good mechanical performance <ref type="bibr" target="#b59">[60]</ref>. Without loss of generality, we present a simplified mechanical model for the mainbody of the corbel as shown in Fig. <ref type="figure">10(b</ref>). There are two forces, i.e., the gravity mg and a force F acting on the mainbody. The centroid and the center of gravity of the mainbody are denoted by C and G, respectively.</p><p>The corbel weight and the locations of C and G are functions of the corbel curve. On one hand, to prevent the mainbody from falling off the base, the weight and the moment of gravity should be optimized to be as small as possible, making the related optimal curve be very flat. On the other hand, the corbel with too flat curve may not look good. To meet both the mechanical and aesthetic requirements, we formulate the design problem of the corbel curve as follows:</p><formula xml:id="formula_22">min f (x) = w 1 mgd G + w 2 (d C -d * C ) 2<label>(22)</label></formula><p>where, d G and d C are distances of G and C to the fixed wall surface, respectively, and d * C is the idealized centroid distance that meets the aesthetics requirement. And w 1 and w 2 are the weights of the mechanical and aesthetics objectives.</p><p>Similar to the airfoil design optimization, we use 192 points to describe the corbel curve. And accordingly, such structured input space for the corbel curve design has 384 design variables, which are highly interacted in order to generate meaningful corbel curves. Therefore, instead of optimizing over such high-dimensional and discrete structured space directly, we train MFoO-GAN and SVD to map it into continuous latent spaces with dozens of variables. The dataset from <ref type="bibr" target="#b59">[60]</ref> is used for the training of MFoO-GAN and SVD. The dimension of high-and low-dimensional latent spaces of MFoO-GAN are set    <ref type="formula" target="#formula_22">22</ref>). Figure <ref type="figure">11</ref> shows the optimization results of the corbel design over 10 runs. Within sample budget, the BO-based GMOs such as GMO-High and GMFoO, achieve better solutions with faster convergence rates than those of EA-based GMOs. When comparing between GMO-Low and GMO-High, the convergence rate of GMO-Low is faster while the final solutions of GMO-High are better. By carrying out optimization in both high-and low-dimensional latent spaces simultaneously, our proposed GMFoO achieves a good balance in between solution accuracy and convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Test on Area Maximization Problem</head><p>By following the idea in <ref type="bibr" target="#b2">[3]</ref>, we further test our proposed GMFoO for finding the 28 × 28 binary image that has the largest total area (i.e. the largest number of pixels with value 1) in the MNIST dataset. Note that the original design space for the above optimization problem is a 784-dimensional structured space, where the design variables can be only valued as 0 and 1. More importantly, the design variables in the structured input are highly interacted in order to generate meaningful images.</p><p>To solve the above problem efficiently, the first step is to embed the structured input space into a continuous latent space that can generate high-quality images. Figure <ref type="figure" target="#fig_0">12</ref> shows the images generated by MFoO-GAN and SVD, respectively. Obviously, both the high-and low-dimensional latent spaces of MFoO-GAN can generate various hand-writing digits clearly. However, SVD cannot generate any meaningful images. In other words, the SVD-based approach (labeled as SVD-BO in this paper) fails to solve the area maximization task of binary images. Therefore, only the GMO approaches are compared for solving this challenging toy problem.</p><p>Figure <ref type="figure">13</ref> shows the optimization results of the area maximization problem. Similar to the optimization results shown in airfoil and corbel design tasks, our proposed GMFoO achieves the best solutions with even faster convergence rate. With the above, the effectiveness of our proposed GMFoO has been well demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussions of Algorithm Performance</head><p>In this subsection, the reason behind the superior performance of GMFoO is discussed by analyzing the relations between the high-and low-dimensional latent spaces of MFoO-GAN. And further, the effects of algorithm parameters on GMFoO performance are also investigated. Since the performance of multi-form transfer optimization is sensitive to the degree of underlying inter-task similarities <ref type="bibr" target="#b14">[15]</ref>, the functional correlation between the high-and low-dimensional latent spaces of MFoO-GAN and the distributions of optimal solutions of latent spaces are analyzed.</p><p>Figure <ref type="figure">14</ref> shows the functional correlation between the high-and low-dimensional latent spaces. Specifically, recalling Section III.B, the sample set {c ′ , y(z)}, that transformed from the high-dimensional sample set {z, y(z)}, are used as low-fidelity samples to build the multi-fidelity GP. Therefore, to analyze the correlation between the high-and low-dimensional latent space, the real function value of c ′ , i.e., y(c ′ ), is calculated (see the dashed path in the upper of Fig. <ref type="figure">14</ref>). In particular, 1000 sample pairs as {y(z), y(c ′ )} are calculated to plot the scatterplot. Obviously, the sample pairs are closely distributed along the dashed line, and the Pearson's coefficient of sample pairs is 0.8. It indicates that the high-and low-dimensional latent space are well correlated, and therefore accurate multi-fidelity GP surrogate can be built <ref type="bibr" target="#b43">[44]</ref> to facilitate positive knowledge transfer in GMFoO.</p><p>Figure <ref type="figure" target="#fig_3">15</ref> shows the distributions of optimal solutions over 10 runs, by using the parallel coordinates. The intersections of a polyline and vertical axes in the parallel coordinates exhibit the component parameter values of a optimal solution. As shown in the shaded area of Figs. <ref type="figure" target="#fig_3">15(a</ref>) and (b), the distribution ranges of c 1 , c 2 and c 3 of the optimal solutions of the high-and low-dimensional latent spaces are almost overlapped. In the meantime, the function values of optimal solutions of the low-dimensional latent space are approaching towards those of the high-dimensional latent space. It confirms that the optimization in the low-dimensional latent space of MFoO-GAN does help to prioritize the search in the directions of major variability of the target object. Then, by leveraging the optimal solution of low-dimensional latent space through the enhanced local exploitation strategy (see the upper in Fig. <ref type="figure" target="#fig_3">15</ref> and Section III.B), the optimizer of the high-dimensional latent space can arrive at the neighborhood of the real optimum more quickly. With the above, it is not hard to comprehend why GMFoO can always achieve better solutions than the compared algorithms as shown in Subsection D.</p><p>2. Effects of the size parameter ∆ Note that a multi-fidelity GP based optimization and an local exploitation strategy are proposed in GMFoO, their effectiveness are analyzed by varying the size parameter ∆. Specifically, ∆ controls the area of local exploitation (see Fig. <ref type="figure">4</ref>). When ∆ = 0, only the multi-fidelity GP based optimization is conducted in GMFoO, which is denoted by GMFoO 0. Further, to show the combined effects and the sensitivity of ∆ on GMFoO performance, ∆ is also set to be 0.1, 0.15 and 0.2, and the related GMFoO Figure <ref type="figure" target="#fig_12">16</ref> shows the testing results of low-speed and subsonic airfoil optimizations, respectively. Compared to GMO-High, GMFoO 0 consistently achieves better results, and therefore the effectiveness of multi-fidelity GP based optimization is demonstrated. In the meantime, the medians of GMFoO 10, GMFoO 15 and GMFoO 20 are better than those of GMFoO 0, indicating that the proposed local exploitation strategy can also help to improve the GMFoO performance.</p><p>Among GMFoO 10, GMFoO 15 and GMFoO 20, the performance of GMFoO 15 is most robust, which achieves the second best and the best results for the low-speed and subsonic airfoil optimization, respectively. The reason behind can be explained as follows. The neighborhood of optimal solutions of the high-and low-dimensional latent space cannot be exactly overlapped. Hence, too small ∆ may lead the exploitation area get out of the vicinity of the real optimal of the high-dimensional latent space. On the other hand, too large ∆ may result in worse sample efficiency and thus worse results within budget. In other words, there should be a balance when selecting ∆ to achieve the global optimal solution efficiently. Hence, GMFoO with ∆ = 0.15 (i.e., <ref type="bibr">GMFoO 15)</ref> achieves the best solutions in our tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Effects of the dimension of low-dimensional space</head><p>To inspect the effects of the dimension of low-dimensional latent space d L on GMFoO performance, we change d L from 3 to 5 in the following tests. According to the convergence histories of GMFoO for low-speed and subsonic airfoil optimizations shown in Figs. <ref type="figure">8</ref> and<ref type="figure" target="#fig_7">9</ref>, the sample budget for the low-speed and subsonic airfoil optimization is set as 120 and 300, respectively.</p><p>As the MFoO-GANs with different d L are trained separately, the landscape of related latent spaces can be different. Hence, the convergence histories of GMO-High look different for different cases. However, GMFoO always achieves better solutions than those of GMO-High and GMO-Low.</p><p>When comparing GMO-Low procedures with different d L , the convergence rate of GMO-Low with d L = 3 is the fastest for both low-speed and subsonic airfoil optimizations. In the meantime, the overall performance of GMFoO with d L = 4 is the best, which achieves the second best and the best optimal solutions for the low-speed and subsonic airfoil optimizations, respectively. The reasons behind can be explained as follows. With the increase of d L , the solution accuracy of low-dimensional latent space becomes better. However, due to the "curse of dimensionality", much more samples are required to attain  the optimal solution of the corresponding low-dimensional space. As a compromise, the GMO-Low with d L = 4 performs best within sample budget. And accordingly, the information exchange between the low-dimensional latent space with d L = 4 and the high-dimensional latent space can be most efficient, making the GMFoO with d L = 4 attains the overall best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a novel generative model based optimization (GMO) framework, namely GMFoO, to more efficiently solve the black-box problems that involve complex structured input space. We showed that, instead of mapping the structured input space into single latent space as most GMO studies do, generating multiple latent spaces and carrying out optimizations over these latent spaces simultaneously, can help to achieve a good balance in between desirable solution accuracy and convergence rate. By instantiating GMFoO with Bayesian optimization, the effectiveness of our proposed approach has been well demonstrated through testing on airfoil and corbel design problems and an area maximization problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 2 :</head><label>12</label><figDesc>Figure 1: Standard process of generative model based optimization (GMO)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2 3 : 4 : 5 :…Figure 2 :</head><label>23452</label><figDesc>Figure 2: GMFoO framework, where the generative model is labeled as MFoO-GAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Sample synthesis and exchange process between the high-and lower-dimensional latent space, (a) in the low-dimensional latent space, (b) in the high-dimensional latent space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Leveraging knowledge through the multi-fidelity modeling, (a) in the high-dimensional latent space, (b) in the low-dimensional latent space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Flowchart of GMFoO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3 1 : 2 :</head><label>312</label><figDesc>Pseudo code of GMFoO Require: Target optimization problem, dataset X with structured input space Ensure: Optimal solution of the target problem Training of Multiple Latent Spaces: Train MFoO-GAN to obtain high-dimensional latent space Z and one or more highly correlated low-dimensional latent spaces C for X DoEs in Multiple Latent Spaces: Use space-filling technique to generate the initial samples and evaluate related objective function to obtain the paired training sets {Z, Y (Z)}, {C, Y (C)} 3: while the termination condition is not satisfied do 4:for i, enumerate Zi ∈ {Z, C} do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>WingFigure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Introduction of (a) the airfoil contour and the forces acting on it, and (b) airfoils in UIUC database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Testing results of subsonic airfoil, where the dimension of high-and low-dimensional latent spaces of MFoO-GAN are set to be 23 and 4, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Introduction to the decorative corbel, (a) decorative corbel and its components, (b) forces acting on the mainbody of corbel, (c) profiles of corbel mainbody in the database</figDesc><graphic coords="14,190.77,237.11,213.74,67.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Generated hand-writing digits by (a) the 20-dimensional latent space and (b) the 4dimensional latent space of MFoO-GAN, and (c) the 20-dimensional latent space of SVD</figDesc><graphic coords="15,200.45,230.64,194.38,60.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 .Figure 13 :</head><label>113</label><figDesc>Figure 13: Testing results of area maximization problem based on the MNIST dataset, where the dimension of high-and low-dimensional latent spaces of MFoO-GAN are set to be 20 and 4, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Experimental results with different ∆ in GMFoO, (a) low-speed airfoil optimization, (b) subsonic airfoil optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Effects of d L on GMFoO performance by testing on low-speed airfoil design, (a) d L = 3, (b) d L = 4, (c) d L = 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Effects of d L on GMFoO performance by testing on subsonic airfoil design, (a) d L = 3, (b) d L = 4, (c) d L = 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>6 :</head><label>6</label><figDesc>Sample Search with EI: Use the EI acquisition function to find the next sample to query in Z i , i.e., z * Update the Training Set: Evaluate the label y * of the new sample z * i and update the training set of Z i</figDesc><table><row><cell></cell><cell>i = arg max z i</cell><cell>EI(zi)</cell></row><row><cell>7:</cell><cell></cell></row><row><cell>8:</cell><cell>end for</cell></row><row><cell>9:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://yarpiz.com/235/ypea108-cma-es</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://m-selig.ae.illinois.edu/ads.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient aerodynamic shape optimization with deeplearning-based geometric filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hull-form stochastic optimization via computational-cost reduction methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Serani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering with Computers</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sample-efficient optimization in the latent space of deep generative models via weighted retraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09191</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Airfoil design parameterization and optimization using b\&apos;ezier generative adversarial networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fuge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12496</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A b-spline-based generative adversarial network model for fast interactive airfoil aerodynamic optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA Scitech 2020 Forum</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Constrained bayesian optimization for automatic chemical design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05501</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aerodynamic design optimization and shape exploration using generative adversarial networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA Scitech 2019 Forum</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2351</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured variationally auto-encoded optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3267" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3387" to="3395" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microstructural materials design via deep adversarial learning methodology</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Catherine</forename><surname>Brinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mechanical Design</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Survey of modeling and optimization strategies to solve high-dimensional design problems with computationally-expensive black-box functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural and multidisciplinary optimization</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="219" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Insights on transfer optimization: Because experience is the best teacher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curbing negative influences online for seamless transfer evolutionary optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4365" to="4378" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward adaptive knowledge transfer in multifactorial evolutionary computation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolutionary multitasking for multiobjective optimization with subspace alignment and adaptive differential evolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A meta-knowledge transfer-based differential evolution for multitask optimization</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolutionary multitasking across single and multiobjective formulations for improved problem solving</title>
		<author>
			<persName><forename type="first">B</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on multiform multi-objective evolutionary optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Memetic Computing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal optimization enhanced cooperative coevolution for large-scale optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3507" to="3520" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient generalized surrogate-assisted evolutionary algorithm for highdimensional expensive problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="379" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A social learning particle swarm optimization algorithm for scalable optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page" from="43" to="60" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Committee-based active learning for surrogate-assisted particle swarm optimization of expensive problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2664" to="2677" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A tutorial on bayesian optimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Frazier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02811</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Funneled bayesian optimization for design, tuning and control of autonomous systems</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Martinez-Cantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1489" to="1500" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Good practices for bayesian optimization of high dimensional structured spaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Siivola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paleyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15471</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Calibrated and recalibrated expected improvements for bayesian optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural and Multidisciplinary Optimization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalizing transfer bayesian optimization to source-target heterogeneity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: An overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge transfer through machine learning in aircraft design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sagarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient transfer learning method for automatic hyperparameter tuning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evolutionary transfer optimization-a new frontier in evolutionary computation research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A review on evolutionary multi-task optimization: Trends and challenges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A flexible transfer learning framework for bayesian optimization with convergence guarantee</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N T</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="656" to="672" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-objective multi-tasking optimization based on incremental learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2004" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reducing local optima in single-objective problems by multi-objectivization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Corne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Evolutionary Multi-criterion Optimization</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiobjectivization by decomposition of scalar cost functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Handl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Problem Solving from Nature-ppsn X, International Conference Dortmund</title>
		<imprint>
			<publisher>Germany</publisher>
			<date type="published" when="2008-09">September, 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parallel multi-fidelity expected improvement method for efficient global optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural and Multidisciplinary Optimization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analysis of dataset selection for multi-fidelity surrogates for a turbine problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Haftka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural and Multidisciplinary Optimization</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2127" to="2142" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-fidelity bayesian optimization via deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Theoretical and numerical constraint-handling techniques used with evolutionary algorithms: a survey of the state of the art</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="1245" to="1287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nash game based efficient global optimization for large-scale design problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="175" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian processes for machine learning (gpml) toolbox</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3011" to="3015" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Parallel gaussian process optimization with upper confidence bound and pure exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Contal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vayatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="225" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient global optimization of expensive black-box functions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global optimization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="492" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Metric-based mathematical derivation of efficient airfoil design variables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Journal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1349" to="1361" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A fast kriging-assisted evolutionary algorithm based on incremental learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Large sample properties of simulations using latin hypercube sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="151" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hierarchical surrogate-assisted evolutionary multi-scenario airfoil shape optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Congress on Evolutionary Computation (CEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Summary of low-speed airfoil data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mcgranahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Broughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Deters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Selig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>University of Illinois, Champaign, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Xfoil: An analysis and design system for low reynolds number airfoils</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Low Reynolds number aerodynamics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Generative design of decorative architectural parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
