<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Hierarchical Bayesian Model of Multi-View Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Wyoming</orgName>
								<address>
									<region>WY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chao</forename><surname>Lan</surname></persName>
							<email>clan@uwyo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Wyoming</orgName>
								<address>
									<region>WY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Hierarchical Bayesian Model of Multi-View Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional anomaly detectors examine a single view of instances and cannot discover multi-view anomalies, i.e., instances that exhibit inconsistent behaviors across different views. To tackle the problem, several multi-view anomaly detectors have been developed recently, but they are all transductive and unsupervised thus may suffer some challenges. In this paper, we propose a novel inductive semi-supervised Bayesian multi-view anomaly detector. Specifically, we first present a generative model for normal data. Then, we build a hierarchical Bayesian model, by first assigning priors to all parameters and latent variables, and then assigning priors over the priors. Finally, we employ variational inference to approximate the posterior of the model and evaluate anomalous scores of multi-view instances. In the experiment, we show the proposed Bayesian detector consistently outperforms state-of-the-art counterparts across several public data sets and three well-known types of multi-view anomalies. In theory, we prove the inferred Bayesian estimator is consistent and derive a proximate sample complexity for the proposed anomaly detector.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly Detection (AD) is a fundamental task with broad applications, such as in clinical diagnosis, fraud transaction detection and cybersecurity <ref type="bibr" target="#b3">[Chandola et al., 2009]</ref>. Traditional detectors only examine a single view<ref type="foot" target="#foot_0">foot_0</ref> of instances and cannot discover multi-view anomalies, i.e., instances that exhibit inconsistent behaviors across different views. One example is in web image analysis, an image can be described its category such as car or animal (view 1) and its web page such as cars.com or animals.com (view 2). If an image is assigned to the animal group in view 1 but car group in view 2, then it is natural to consider this image anomalous [Marcos <ref type="bibr" target="#b9">Alvarez et al., 2013]</ref>. Other examples can be found in digit recognition <ref type="bibr">[Li et al., 2018b]</ref> and movie recommendation on MovieLens dataset <ref type="bibr" target="#b5">[Gao et al., 2011]</ref>. How to effectively leverage multiple views to detect anomaly is an interesting and significant topic, often referred to as multi-view anomaly detection.</p><p>In the literature, a number of multi-view anomaly detectors have been developed. Some of them try to find samples that have inconsistent cross-view cluster membership. HOrizontal Anomaly Detection (HOAD) <ref type="bibr" target="#b5">[Gao et al., 2011]</ref> pioneers this branch of methods. In HOAD, the author first constructs a combined similarity graph based on the similarity matrices, and computes the key eigenvectors of the graph Laplacian of the combined matrix. Then anomalies are identified by computing cosine distance between the components of these eigenvectors. This idea is further studied by <ref type="bibr" target="#b9">[Marcos Alvarez et al., 2013]</ref> and <ref type="bibr" target="#b8">[Liu and Lam, 2012]</ref> for different application tasks. Another successful group of methods is developed from a perspective of data representation <ref type="bibr" target="#b6">[Li et al., 2015;</ref><ref type="bibr" target="#b15">Zhao and Fu, 2015;</ref><ref type="bibr">Li et al., 2018a]</ref>. The intuition in these works is that a normal sample usually serves as a good contributor in representing the other normal samples while the outliers do not. Low-rank matrix recovery is the technique which can exploit the intrinsic structure of data and explore the representation relationship of samples. Therefore, by calculating the representation coefficients in low-rank matrix recovery, the multi-view outliers can be identified. In addition, <ref type="bibr" target="#b6">[Iwata and Yamada, 2016]</ref> utilizes a sophisticated statistical machine learning algorithm to detect anomalies. They design a probabilistic latent variable model to infer the consistent or inconsistent characteristics of multiple views for each object.</p><p>We note that above detection methods are unsupervised and transductive. In some applications, however, one can often get plenty of labeled normal data (e.g., one can collect or simulate normal network traffic data for a certain period of time). In these cases, it is natural to hypothesize these data will enable the detector to better capture normal behavior than unlabeled data. In addition, when an unseen testing instance arrives, above methods have to add it to the existing training set and rerun the detection algorithm (e.g., clustering or matrix factorization), which often causes inefficiency.</p><p>To lift above limitations, in this paper we propose a novel Bayesian model for semi-supervised multi-view anomaly detection. To be specific, we first present a generative model for normal data, assuming that different views of a normal instance are generated from a single latent factor through different projection matrices; and the views are independent conditioned on the factor <ref type="bibr" target="#b2">[Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b4">Dasgupta et al., 2002]</ref>. Then we build a hierarchical Bayesian model, by first assigning priors on model parameters and then assigning priors over the priors. In particular, we assign the automatic relevance determination (ARD) prior <ref type="bibr" target="#b11">[Neal, 2012]</ref> on the projection matrices to sparsify their columns for automatically determining the dimension of latent factor; we also place Student's t distributions on the latent factor prior and the likelihood to improve robustness of the estimator <ref type="bibr" target="#b1">[Archambeau et al., 2006;</ref><ref type="bibr" target="#b4">Gai et al., 2008]</ref>. Finally, we employ variational inference to derive an analytical approximation to the posterior probability (of unobserved variables and parameters) of model. To detect multi-view anomalies, we propose to measure the outlier score by calculating the value of log marginal distribution of multiple observed views.</p><p>The contributions of this paper are summarized as: 1) To the best of our knowledge, this paper is the first attempt to detect multi-view outliers under semi-supervised scenario via a Bayesian model of inductive learning. 2) In theory, we proves the proposed estimator approaches the true model at a rate of</p><formula xml:id="formula_0">O v d v m0 log( v d v N ) N</formula><p>, and under mild conditions we derive a first sample complexity of O 1 2 γ 2 V 2 lnV +ln 1 δ for a multi-view anomaly detector to achieve detection rate .</p><p>3) we experimentally evaluates the proposed method on both synthetic and real-life multi-view data. The competing results demonstrate the effectiveness of our model. The rest of this paper is organized as follows. In Section II, we introduce basic notations; in Section III, we present the proposed Bayesian multi-view anomaly detection model; in Section IV, we theoretically analyze the model; in Section V, we show experimental results and discussions; in Section VI, we conclude the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Problem Setup</head><p>Before going further, we explain some preliminary knowledge and notational conventions used throughout the paper.</p><p>To clarify, in anomaly detection applications, the term semi-supervised detection has been widely used to describe the scenario in which AD methods only incorporate the use of labeled normal samples to learn a model that compactly characterizes the "normal" class <ref type="bibr">[Chalapathy and Chawla, 2019;</ref><ref type="bibr" target="#b0">Akcay et al., 2018;</ref><ref type="bibr" target="#b3">Chandola et al., 2009;</ref><ref type="bibr">Blanchard et al., 2010;</ref><ref type="bibr" target="#b10">Muñoz-Marí et al., 2010;</ref><ref type="bibr" target="#b13">Song et al., 2017]</ref>. However, there are a few works <ref type="bibr" target="#b4">[Das et al., 2016;</ref><ref type="bibr" target="#b12">Siddiqui et al., 2018;</ref><ref type="bibr">Görnitz et al., 2013]</ref> having investigated the general semisupervised setting where one also utilizes unlabelled data. In this work, we stick to the first AD setting.</p><p>Following the definition used in <ref type="bibr">[Li et al., 2018a]</ref>, there are three kinds of outliers in multi-view setting. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, Class-outlier is an outlier that exhibits inconsistent characteristics (e.g. cluster membership) across different views. Attribute-outlier is an outlier that exhibits consistent abnormal behaviours in each view. Class-Attribute-outlier is an outlier that exhibits class outlier characteristics in some views while shows attribute outlier properties in the other views. Suppose we are given a data set D which consists of N instances, denoted by n = 1, 2, ..., N , described by V views with each view v = 1, 2, ..., V . The feature representation of instance n under view v is</p><formula xml:id="formula_1">x v n ∈ R d v , where d v is the dimensionality of view v. X v = [x v 1 , x v 2 , ..., x v N ] ∈ R d v ×N is sample set observed in view v.</formula><p>In this way, the whole data set is denoted as D = {X 1 , X 2 , ..., X V }. Then, the multiview anomaly detector computes an anomaly score for each instance and compares it to a threshold τζ for finding the outlier in multi-view setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bayesian Muli-View Anomaly Detector</head><p>In this section, we illustrate the proposed probabilistic model together with its estimation and the outlier score calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Process</head><p>To link the multiple views x 1 , x 2 , ..., and x V together, we introduce a common latent variable z. The intuition here is that: generally, an normal instance can be sufficiently described by a single view for learning tasks. Therefore, it is reasonable to suppose that these different views share some common features or latent structure, then the problem is how to build a framework to learn these common structure or the correspondence between observed views and the unobserved space.</p><p>To explore it, we proposed a probabilistic model which describes the generative process of multi-view instances whose views are linked via a single, reduced-dimensionality latent variable space. Specifically, We assume x 1 , x 2 , ..., and x V are generated from z by first choosing a value for the latent variable z and then sampling observed variables conditioned on this latent value. The</p><formula xml:id="formula_2">d 1 , d 2 , ..., d V -dimensional observed vectors x 1 n , x 2 n , ..., x V</formula><p>n are defined by a linear transformation governed by the matrix</p><formula xml:id="formula_3">W v ∈ R d v ×m of the latent vector z ∈ R m plus a projection noise v ∈ R d v</formula><p>, so that</p><formula xml:id="formula_4">x v n = W v z n + µ v + v v = 1, 2, ..., V<label>(1)</label></formula><p>where</p><formula xml:id="formula_5">µ v ∈ R d v</formula><p>is the data offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Specification</head><p>In the following, we introduce the probabilistic formulation, assign prior distribution on latent variables and parameters in Eq. ( <ref type="formula" target="#formula_4">1</ref>), and assign priors on the priors. Specifically, we define Student's-t prior distribution over the latent variable In Eq. ( <ref type="formula" target="#formula_6">2</ref>), we adopt Student's-t distribution's equivalent form according to <ref type="bibr" target="#b8">[Liu and Rubin, 1995;</ref><ref type="bibr" target="#b1">Archambeau et al., 2006]</ref>. u &gt; 0 is a latent scale variable. Its Gamma prior p(u) and the associated Gaussian condition p(z|u) are defined as:</p><formula xml:id="formula_6">z z ∼ S(z | 0, I m , ν) = + 0 p(z|u)p(u)du<label>(2)</label></formula><formula xml:id="formula_7">u ∼ G u | ν 2 , ν 2 , z | u ∼ N z | 0, u -1 I m (3)</formula><p>Similarly, noise v is a d v dimensional zero-mean Student's-t variable with precision Ψ v and degree of freedom</p><formula xml:id="formula_8">ν v ∼ S( v | 0, Ψ v , ν)<label>(4)</label></formula><p>By the property of affine transformation of random variable, combining (1), ( <ref type="formula" target="#formula_6">2</ref>), ( <ref type="formula">3</ref>) and ( <ref type="formula" target="#formula_8">4</ref>) gives the conditional distributions of observed variables</p><formula xml:id="formula_9">x v x v | z ∼ S(x v | W v z + µ v , Ψ v , ν)<label>(5)</label></formula><formula xml:id="formula_10">x v | z, u ∼ N x v | W v z + µ v , (uΨ v ) -1</formula><p>(6) Next, we place priors on parameters W v , µ v and Ψ v . Let w vi ∈ R m be the i th row of W v , we first employ ARD prior on W v to automatically sparsify its columns</p><formula xml:id="formula_11">W v | α v ∼ d v i=1 N w vi | 0, diag(α v ) -1 (7)</formula><p>Then we parameterize the distributions over µ v and Ψ v by defining</p><formula xml:id="formula_12">µ v ∼ N (µ v | 0, β -1 v I d v ), Ψ v ∼ W(Ψ v | K -1 v , ν v ) (8)</formula><p>where</p><formula xml:id="formula_13">W(Ψ v |K -1 v , ν v ) ∝ |Ψ v | (νv-d v -1) 2 exp -1 2 T r(K v Ψ v ) denotes Wishart distribution.</formula><p>Finally, we complete the specification of priors (ν,α v ) over prior distributions of variables u and w vi respectively</p><formula xml:id="formula_14">ν ∼ G(ν | a ν , b ν ), α v ∼ m j=1 G(α vj | a α , b α ) (9)</formula><p>where α v controls the magnitude of W v . If certain α vj is large, the jth column of W v will tend to take value zero and become little importance.</p><p>The graphical representation of Bayesian model over a data set of N instances is illustrated by Figure <ref type="figure" target="#fig_1">2</ref> in which arrows represent conditional dependencies between random variables. Since we have no further knowledge about the hyperparameters of priors, we choose broad ones by setting</p><formula xml:id="formula_15">a α = b α = β v = 10 -3 , K v = 10 -3 I d v , ν v = d v + 1, a ν = 2 and b ν = 0.1, m = min{d v -1; v = 1, . . . , V }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Inference</head><p>The goal of model inference is to learn posterior distributions of latent variables and parameters. Based on Figure <ref type="figure" target="#fig_1">2</ref>, the joint probability of data set D, latent components Z = {z 1 , . . . , z N }, U = {u 1 , . . . , u N }, and parameters</p><formula xml:id="formula_16">Θ = {{W v , α v , µ v , Ψ v } V v=1 , ν} can be written as p(X 1 , . . . , X V , Z, U, Θ) = p(ν)× V v=1 p(W v |α v )p(α v )p(µ v )p(Ψ v )× N n=1 V v=1 p(x v n |z n ,u n ,W v ,µ v ,Ψ v )p(z n |u n )p(u n |ν)<label>(10)</label></formula><p>It is analytically intractable to derive the posterior distribution p(Z, U, Θ|D) from Eq. ( <ref type="formula" target="#formula_16">10</ref>) directly. Therefore, we adopt variational inference for approxiamting the posterior by a facterized distribution</p><formula xml:id="formula_17">q(Z, U, Θ) = N n=1 q(z n ) N n=1 q(u n )× q(ν) V v=1 q(Ψ v )q(µ v ) m j=1 q(α vj ) d v i=1 q(w vi ) (11)</formula><p>The distribution q is found by maximizing the lower bound</p><formula xml:id="formula_18">L q(Z,U,Θ) = log p(D)-KL(q(Z,U,Θ)||p(Z,U,Θ|D)) = q(Z,U,Θ) log p(D,Z,U,Θ) q(Z,U,Θ) dZdU dΘ<label>(12)</label></formula><p>Since log p(D) is constant, maximizing the low bound is equivalent to minimizing the KL divergence between q(Z,U,Θ) and p(Z,U,Θ|D). By substituting factor distributions in Eq. ( <ref type="formula">11</ref>) into ( <ref type="formula" target="#formula_18">12</ref>) and dissecting out the dependence on one of the factors q l (Ω l ), we have following result</p><formula xml:id="formula_19">log q l (Ω l ) = E q k (Ω k ),k =l [log p(D, Z, U, Θ)] + const (13)</formula><p>where</p><formula xml:id="formula_20">Ω = {{z n , u n } N n=1 , {W v , {α vj } m j=1 , µ v , Ψ v } V v=1</formula><p>, ν} refers all latent components and parameters of model, E • [•] represents an expectation w.r.t. distribution q k (Ω k ) for all k = l. Combining (13), (10) with the distributions defined in section 3.2, we obtain the following factor distributions</p><formula xml:id="formula_21">q(ν) = G(ν|â ν , bν ), q(z n ) = N (z n |µ zn , Σ zn ) (14) q(u n ) = G(u n |α un , β un ), q(Ψ v ) = W(Ψ v | K-1 v , νv ) (15) q(µ v ) = N (µ v |µ µv , Σ µv ), q(α vj ) = G(α vj |â α , bα ) (16) q(w vi ) = N (w vi |µ wvi , Σ wvi ) i = 1, . . . , d v (17) where n = 1, . . . , N , v = 1, . . . , V , j = 1, . . . , m, âν = a ν + N 2 , bν = b ν - 1 2 N+ n log u n -u n (<label>18</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">µ zn = Σ zn v W T v u n Ψ v x v n -µ v Σ zn = v W T v u n Ψ v W v + u n I m -1 (19) âα = a α + d v 2 , bα = b α + ||W v,:j || 2 2 (<label>20</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">β un = ν + z T n z n 2 + 1 2 v x v n T Ψ v x v n -2x v n T Ψ v µ v -2x v n T Ψ v W v z n +2 z T n W T v Ψ v µ v + µ T v Ψ v µ v + T r[ W v z n z T n W T v Ψ v ] α un = 1 2 ( ν + m) + v d v 2</formula><p>(21)</p><formula xml:id="formula_26">Kv = K v + n x v n x v n T + µ v µ T v -µ v x v n T -x v n µ T v -W v z n x v n T -x v n z T n W T v + W v z n µ T v + diag [T r(Σ wv1 z n z T n ), . . . , T r(Σ w vd v z n z T n )] + µ v z T n W T v + W v z n z T n W T v u n νv = ν v + N (22) µ µv = Σ µv n u n Ψ v x v n -W v z n Σ µv = n u n Ψ v + β v I d v -1<label>(23)</label></formula><formula xml:id="formula_27">Σ wvi = diag α v + n z n z T n u n Ψ v ii -1 µ wvi = Σ wvi n z n u n Ψ v ,:i T (x v n -µ v ) -z n z T n d v i =1,i =i u n Ψ v i i w vi (24)</formula><p>where M ,:i is the i th column of matrix M , • denotes the expectation, and we use Stirling's approximation Γ(x) ∼ √ 2π exp(-x)x x-1 2 for log Γ(ν/2) when deriving the factor q(ν). The equations (14-24) represent a set of consistency conditions for the maximum of the lower bound subject to the factorization constraint. We can find an optimal solution by first initializing all q k (Ω k ) properly and then cycling through the factors and re-estimating each distribution in turn using the updated moments of other factors. We monitor the convergence of optimization by evaluating the lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Outlier Score Measurement</head><p>The distribution p(x 1 , ..., x V ) of V -view observed variable (x 1 , ..., x V ) is expressed, from the sum and product rules of probability, in the form</p><formula xml:id="formula_28">p(x 1 , ..., x V ) = V v=1 p(x v |z, u)p(z|u)p(u)dzdu = S(x 1 , ..., x V |M (µ) , Λ (W,Ψ) , ν) (25)</formula><p>by integrating out z and u, it gives a Student's-t distribution for p(x 1 , ..., x V ), where</p><formula xml:id="formula_29">M (µ) = [µ 1 ; µ 2 ; . . . ; µ V ]<label>(26)</label></formula><p>and</p><formula xml:id="formula_30">Λ (W,Ψ) is a v d v -by-v d v precision matrix Λ (W,Ψ) vv = W v W T v + Ψ -1 v if v = v W v W T v if v = v<label>(27)</label></formula><p>From Eq. ( <ref type="formula" target="#formula_18">12</ref>), we know that the log marginal likelihood can be approximated by the evidence lower bound (ELBO).</p><p>Through maximizing the ELBO, we find an optimum estimate of the data distribution. According to theorem 1 in section 4.1, this estimated distribution is close to the real model distribution with theoretical guarantee. Since all parameters of estimated data distribution are learned on the normal sample, thus it reasonably concludes that normal instance will have bigger value in Eq. ( <ref type="formula">25</ref>). By this insight, we formulate the outlier score s(x i ) of an instance</p><formula xml:id="formula_31">x i = [x 1 i ; x 2 i ; . . . ; x V i ] as the negative unscaled Student's-t density s(x i ) := -1+ (x i -M (µ) ) T Λ (W,Ψ) (x i -M (µ) ) ν - ν+ v d v 2</formula><p>(28) Conceptually, a negative outlier score measures the probability that the sample is generated from the multi-view distribution defined by normal data, therefore bigger value in Eq. ( <ref type="formula">28</ref>) means this sample is less likely to be the normal class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we show theoretical analysis for the proposed model (Due to space limitation, detailed proofs are omitted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Consistency of the Bayesian Estimator</head><formula xml:id="formula_32">Let X N V := {{x v n } V v=1 } N n=1 be a sample of i.i.d multi-view random variables collected from distribution p 0 . We consider statistical models M m = {p θm |θ m ∈ Θ m } with the count- able collection M m |1 ≤ m ≤ min{d v ; v = 1, . . . , V } ,</formula><p>where Θ m is the parameter set associated with m latent components model. Let F + (Θ m ) be the set of all possible distributions over Θ m . Now we assume that there exists a true model M m0 that contains the true data distribution p θ * m 0 (i.e., there exists m 0 and θ * m0 ∈ Θ m0 satisfying p 0 = p θ * m 0 ). Assumption 1. There exists g(N ) for which there is a distribution ρ m0,N,V ∈ F + (Θ m0 ) such that</p><formula xml:id="formula_33">KL(p θ * m 0 , p θm 0 )ρ m0,N,V dθ m0 ≤ g(N ),<label>(29)</label></formula><formula xml:id="formula_34">KL(ρ m0,N,V , π m0 (θ m0 )) ≤ N • g(N )<label>(30</label></formula><p>) where π m0 (•) on θ m0 ∈ Θ m0 is a prior over model M m0 . Theorem 1. Given assumption 1, for any α ∈ (0, 1), if there exists a true model M m0 such that p 0 = p W * m 0 and the coefficients of</p><formula xml:id="formula_35">W * m0 = [W * 1,m0 ; W * 2,m0 ; . . . ; W * V,m0 ] ∈ R d v ×m0 are bounded, then E D α (p Wm , p W * m 0 ) • πm α,N,V (W m |X N V ) dW m = O V v=1 d v m 0 log( V v=1 d v N ) N (31) where πm α,N,V (W m |X N V )</formula><p>is the approximate posterior distribution derived by variational inference.</p><p>From Eq. 31, we see that, in expectation w.r.t the random variables X N V under distribution p W * m 0 , the average α-Renyi loss (D α ) <ref type="bibr" target="#b14">[Van Erven and Harremos, 2014;</ref><ref type="bibr">Chérief-Abdellatif, 2018]</ref> between a distribution in the selected model and the true distribution over πm α,N,V (W m |X N V ) goes to zero as n → +∞. This shows the Bayesian estimator of our model is consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Compute Optimal Threshold</head><p>Input: Data {X , X }, Swapping Rate γ, Detection Rate ζ Output: Detection Threshold τζ 1: Generate mixture set X γ via swapping views randomly. 2: Compute anomaly scores for all points in X and X γ via Eq. ( <ref type="formula">28</ref>), and denote them as S and S γ respectively. 3: Calculate empirical CDF Fa . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sample Complexity of Multi-View Detector</head><p>Let {X , X } be two "clean" nominal sets (both containing k i.i.d. multi-view draws from p 0 ). We take X as training input. For X , given a swapping rate γ, we use it to generate a 'mixture' dataset X γ . In this case, the mixture data X γ can be approximately treated as k points drawn from a mixture distribution p γ , which generates a multi-view outlier and nominal point with probability 2γ and 1-2γ respectively. Our multi-view semi-supervised anomaly detector is trained on X and assigns anomaly scores to all data points in X and X γ . Intuitively, an ideal detector would rate all alien data points higher than all nominals (higher score means more anomalous). The key challenge in practice is to select a threshold for anomaly score that gives the guarantee on achieving the desired outlier detection rate. Motivated by <ref type="bibr" target="#b8">[Liu et al., 2018]</ref>, our approach to obtaining a theoretical guarantee is based on considering the cumulative distribution function (CDF) over anomaly scores.</p><p>Let F and Fγ be the empirical CDFs of anomaly scores of samples from p 0 and p γ respectively. The empirical CDF for an abnormal sample can be derived as Fa (s(x)) = Fγ (s(x))-(1-2γ) F (s(x)) /2γ. With sufficient data and knowledge of γ, empirical CDFs F , Fγ and Fa will convergence to the ground truth F, F γ and F a . After deriving F a , a detector can achieve an outlier detection rate of ζ by selecting an anomaly score τ ζ that is the 1 -ζ quantile of F a and raises an alarm on the testing point whose anomaly score is greater than τ ζ . Alg. 1 summarizes the steps for finding a reasonable threshold achieving the desired outlier detection. Theorem 2. Let X and X be the nominal data sets containing k i.i.d V-view instances drawn from distribution p 0 . Given a swapping rate γ, let X γ be the mixture set generated from X over the randomness of view selecting and view swapping.</p><p>For any ∈ (0, ζ) and δ ∈ (0, 1),</p><formula xml:id="formula_36">if k &gt; (1 -γ) 2 2 2 γ 2 ln 2 1 -1 -g(δ, V )<label>(32)</label></formula><p>where</p><formula xml:id="formula_37">1 g(δ,V ) = 1 δ V Ṽ =2 V ! Ṽ !(V -Ṽ )! Ṽ ! e ,</formula><p>• is the floor function, and e is Euler's Number, then with probability at least 1 -δ, Algorithm 1 will output a threshold τζ that achieves an multi-view outlier detection rate of at least 1 -η, where η = 1 -ζ + .</p><p>Theorem 2 provides a value for the sample size k that guarantees at least ζfraction of outliers in the test points will be detected (an additional error term is introduced here because of the finite sample size). By using Stirling's formula for approximating factorials (e.g. Ṽ !, (V -Ṽ )!), above guarantee is approximately polynomial since k grows as O 1<ref type="foot" target="#foot_3">foot_3</ref> γ 2 V 2 lnV +ln 1 δ . We believe theorem 2 is the first PAC-style guarantee for multi-view anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluations</head><p>We now show the effectiveness of proposed method on public Outlier Detection Datasets (ODDS) 2 , WebKB dataset<ref type="foot" target="#foot_4">foot_4</ref> and MovieLens dataset<ref type="foot" target="#foot_5">foot_5</ref> . We compared the proposed model with representative and cutting edge multi-view anomaly detectors: HOrizontal Anomaly Detection (HOAD) <ref type="bibr" target="#b5">[Gao et al., 2011]</ref>, Affinity Propagation (AP) <ref type="bibr" target="#b9">[Marcos Alvarez et al., 2013]</ref>, Probabilistic Latent Variable Model (PLVM) <ref type="bibr" target="#b6">[Iwata and Yamada, 2016]</ref> and Latent Discriminant Subspace Representation (LDSR) <ref type="bibr">[Li et al., 2018a]</ref>. For AD problems, the most widely used performance evaluation metrics are ROC curve and AUC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on Synthetic Multi-View Settings</head><p>We employ 9 data sets, namely thyroid, annthyroid, forestcover, vowels, pima, vertebral, lympho, wine and glass, which are obtained from the ODDS library <ref type="bibr" target="#b12">[Rayana, 2016]</ref>. We generate multiple views by randomly splitting the features, where each feature can belong to only one view. To generate three types of multi-view outliers, we follow the strategy in previous works (e.g. <ref type="bibr">[Li et al., 2018a]</ref>) for fair comparison. After the outlier generation stage, we equivalently split all normal instances into two parts, and use one of them as the training set to train the proposed model. Then we verify the outlier detection performance on the test set which consists of the remaining normal data and generated outliers. On each dataset, we repeat the random outlier generation procedure 20 times and at each time, we perturb 2.5% of the data in that procedure. We average their performance and report AUC results (mean ± standard deviation) in Table <ref type="table" target="#tab_0">1</ref>.</p><p>From table 1, we can observe that the proposed method consistently outperforms all competing counterparts on almost nine data sets for all kinds of multi-view outliers. The superiority of proposed method is expected, because it uses the semi-supervised anomaly detection technique, which can maximally capture the nature and property of normal instances. This, in turn, can help the learned model to better distinguish whether the test instance is normal or not, thus improving the detection performance.</p><p>To investigate how the number of outliers affects the performance of different models, we experiment on data corrupted by progressively higher percentages of outliers. The Figure <ref type="figure" target="#fig_3">3</ref> shows the variation of AUCs on data set pima with outlier ratio of 2%, 5%, 10%, 15%, 20%, 25% and 30% for three types of outliers. We see that, in general, as the anomaly rate increases, the performance decreases. And the proposed method is comparatively robust and outperforms other compared ones with all outlier ratio settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Real World Multi-View Data</head><p>Further, we compare them on the WebKB dataset <ref type="bibr" target="#b2">[Blum and Mitchell, 1998</ref>] which has been widely used for evaluating multi-view learning algorithms <ref type="bibr" target="#b6">[Guo, 2013;</ref><ref type="bibr" target="#b6">Li et al., 2014]</ref>. We use its Cornell subset in our experiment. It contains 195 webpages over 5 labels. Each webpage is described by four views: content, inbound link, outbound link and cites. Figure <ref type="figure" target="#fig_4">4</ref> shows the ROC curves of all compared methods on the We-bKB dataset with outlier ratio of 5% (left) and 10% (right). We can observe that clearly, our approach achieves higher AUC than its competitors, which demonstrates the strength of our Bayesian detector.</p><p>To present the qualitative analysis of Bayesian model in detecting inconsistency between users' rating behavior and movie genre, we apply the proposed model to MovieLens small dataset which contains 100,836 ratings over 9,742  (PLVM method misses here, because it fails to execute on high dimensional dataset due to exponent overflow of Eq. 10 in their paper.)</p><p>movies by 610 users. We sample 1000 movies, and perform our model to calculate anomalous scores for them. Table 2 lists some movies high and low anomalous scores. Movie 'spirited away' is categorized into animation and fantasy genre, but it receives most of ratings from users that watch and tag action-thriller movies. In other words, it exhibits inconsistent behavior between genre view and rating view, and thus has a high anomalous score. Contrarily, low anomalous score movies, e.g. sacrifice, do not show view inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we offer a novel hierarchical Bayesian model to find multi-view outliers under a semi-supervised detection scenario via inductive learning. We prove our Bayesian estimator is consistent and derive a sample complexity for the detector. In experiment, we show the proposed model consistently outperforms state-of-the-art multi-view anomaly detectors across both synthetic and real-world multi-view data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of three types of outliers in multi-view setting.</figDesc><graphic coords="2,79.74,54.00,191.53,135.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed hierarchical Bayesian model for a data set of N observations (For the concision of graph, here we omit the dependence of x v n on Wv, Ψv, µv).</figDesc><graphic coords="3,54.54,54.00,241.91,116.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4: Optimize threshold by τζ = max s(x) ∈ {S, S γ }| Fa (s(x)) ≤ 1 -ζ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The variation curves of AUC W.R.T outlier ratio.</figDesc><graphic coords="5,377.28,162.74,118.44,80.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROC curves of compared methods on WebKB dataset.(PLVM method misses here, because it fails to execute on high dimensional dataset due to exponent overflow of Eq. 10 in their paper.)</figDesc><graphic coords="6,320.58,363.64,115.92,79.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>5078±.0724 .6801±.0866 .8540±.0691 .5921±.0768 .8338±.0972 .5714±.1648 .6503±.1574 .7083±.1410 AP .6737±.1164 .5747±.0669 .6774±.0739 .7062±.1125 .9376±.0293 .8586±.0604 .5369±.1539 .6947±.1078 .7497±.1117 PLVM .8989±.0091 .8904±.0363 .4870±.0126 .5481±.0067 .9086±.0083 .7564±.0061 .5413±.0251 .4058±.0481 .4087±.0246 LDSR .9751±.0074 .9876±.0022 .9983±.0005 .9181±.0153 .9858±.0057 .9793±.0200 .9362±.0053 .9932±.0009 .9940±.0040 Our .9877±.0056 .9979±.0011 .9995±.0027 .9875±.0071 .9877±.0044 .9958±.0074 .9225±.0177 .9417±.0450 .9530±.0292 AUC values (mean ± std) on nine UCI datasets with outlier ratio 5%.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell>Thyroid</cell><cell>Annthyroid ForestCover</cell><cell>Vowels</cell><cell>Pima</cell><cell>Vertebral</cell><cell>Lympho</cell><cell>Wine</cell><cell>Glass</cell></row><row><cell cols="10">Attribute HOAD .5202±.0864 .Class Outlier Outlier HOAD .5393±.0303 .5849±.0348 .6872±.0337 .3818±.0384 .5557±.0310 .5209±.0812 .6058±.0715 .7124±.0638 .4277±.0932 AP .5847±.0227 .5265±.0350 .7906±.0332 .7520±.0513 .5659±.0365 .5272±.0449 .7402±.0498 .5629±.0933 .5576±.0518 PLVM .5676±.0093 .4087±.0176 .6035±.0044 .5479±.0282 .5425±.0138 .4444±.0416 .5254±.0061 .4860±.0040 .5433±.0104 LDSR .8631±.0217 .7128±.0418 .7551±.0293 .9245±.0173 .5924±.0543 .6070±.0568 .8228±.0762 .5889±.0916 .7098±.0498 Our .8744±.0205 .7383±.0450 .8672±.0197 .9360±.0158 .6354±.0400 .8891±.1171 .8825±.0410 .8373±.0424 .7613±.0570</cell></row><row><cell>Class-Attribute</cell><cell>Outlier</cell><cell cols="8">HOAD .4934±.0270 .4976±.0311 .4342±.0468 .5994±.1342 .4181±.0260 .7386±.0700 .7085±.0609 .5798±.0615 .5598±.0652 AP .6380±.0723 .5647±.0819 .8054±.0373 .8511±.0713 .7916±.0555 .7277±.0524 .5481±.0918 .5481±.1173 .7308±.0676 PLVM .7122±.0191 .8933±.0134 .8184±.0087 .6390±.0223 .8249±.0063 .6913±.0261 .6120±.0195 .7094±.0145 .9555±.0092 LDSR .9344±.0179 .9122±.0220 .9845±.0049 .9642±.0064 .9315±.0146 .9185±.0371 .9765±.0135 1±0 .9900±.0026 Our .9863 ±.0075 .9842±.0076 .9857±.0095 .9757±.0082 .9510±.0169 .9836±.0198 .9571±.0536 .9201±.0470 .9984±.0023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dance of Reality 0.962 Winslow Boy 0.092 The Dark Knight 0.956 Sacrifice 0.084 High and low anomalous score movies</figDesc><table><row><cell>Movie Title</cell><cell>Score Movie Title</cell><cell>Score</cell></row><row><cell>Spirited Away</cell><cell cols="2">0.982 The Rebound 0.162</cell></row><row><cell>Quiz Show</cell><cell>0.966 Scooties</cell><cell>0.150</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A view is a set of features that often have similar semantics, e.g., a webpage can be described by one view of its content and another view of its hyperlinks[Xu et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2013].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>http://odds.cs.stonybrook.edu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>http://lig-membres.imag.fr/grimal/data.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>https://grouplens.org/datasets/movielens/latest Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ganomaly: Semisupervised anomaly detection via adversarial training</title>
		<author>
			<persName><surname>Akcay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cédric Archambeau, Nicolas Delannay, and Michel Verleysen</title>
		<author>
			<persName><forename type="first">Archambeau</forename><surname>Springer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>
			<persName><surname>Blanchard</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-11">2018. 2006. 2006. Nov. 2010</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2973" to="3009" />
		</imprint>
	</monogr>
	<note>Semi-supervised novelty detection</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chalapathy and Chawla, 2019] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">;</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="1998">1998. 1998. 2019</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
	<note>Combining labeled and unlabeled data with co-training</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chérief-Abdellatif, 2018] Badr-Eddine Chérief-Abdellatif. Consistency of elbo maximization for model selection</title>
		<author>
			<persName><surname>Chandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009. 2009. 2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Anomaly detection: A survey</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust bayesian pca with student&apos;s t-distribution: the variational inference approach</title>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>
			<persName><forename type="first">Jiading</forename><surname>Gai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Stevenson</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2016. 2016. 2002. 2008. 2008</date>
			<biblScope unit="page" from="1340" to="1343" />
		</imprint>
	</monogr>
	<note>ICIP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A spectral framework for detecting inconsistency across multi-source object relationships</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<editor>
			<persName><forename type="first">Marius</forename><surname>Görnitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konrad</forename><surname>Kloft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ulf</forename><surname>Rieck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Brefeld</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011. 2011. 2013</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="235" to="262" />
		</imprint>
	</monogr>
	<note>JAIR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tomoharu Iwata and Makoto Yamada. Multi-view anomaly detection via robust probabilistic latent variable models</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>
			<persName><surname>Li</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2016. 2016. 2014. 2014. 2015. 2015. 2018</date>
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiview low-rank analysis with applications to outlier detection</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ml estimation of the t distribution using em and its extensions, ecm and ecme</title>
		<author>
			<persName><forename type="first">Lam ;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanhai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald B Rubin ; Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Garrepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Thomas G Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPW</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="1995">2012. 2012. 1995. 1995. 2018</date>
			<biblScope unit="page" from="19" to="39" />
		</imprint>
	</monogr>
	<note>Open category detection with pac guarantees</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering-based anomaly detection in multi-view data</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1545" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semisupervised one-class support vector machines for classification of remote sensing data</title>
		<author>
			<persName><surname>Muñoz-Marí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3188" to="3197" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">;</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feedback-guided anomaly discovery via online optimization</title>
		<author>
			<persName><forename type="first">;</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName><surname>Siddiqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shebuti Rayana. ODDS library</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016. 2018. 2018</date>
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
	<note>SIGKDD</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hybrid semi-supervised anomaly detection model for high-dimensional data</title>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tim Van Erven and Peter Harremos. Rényi divergence and kullback-leibler divergence</title>
		<author>
			<persName><forename type="first">Harremos</forename><surname>Van Erven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3797" to="3820" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dualregularized multi-view outlier detection</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2015</date>
		</imprint>
	</monogr>
	<note>A survey on multi-view learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
