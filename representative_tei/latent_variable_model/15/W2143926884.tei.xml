<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Human Behavior Analysis: Learning Correlation and Interaction Across Modalities</title>
				<funder>
					<orgName type="full">U.S. Army RDE-COM</orgName>
				</funder>
				<funder ref="#_YnTuQvA #_MRQhYX7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_SZJ945a">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
							<email>yalesong@csail.mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<email>morency@ict.usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Randall</forename><surname>Davis</surname></persName>
							<email>davis@csail.mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL Cambridge</orgName>
								<orgName type="institution">USC ICT Los Angeles</orgName>
								<address>
									<postCode>02139 90094</postCode>
									<region>MA CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Human Behavior Analysis: Learning Correlation and Interaction Across Modalities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2388676.2388684</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.5.4 [Pattern Recognition]: Applications-Signal processing Multimodal signal processing</term>
					<term>multi-view latent variable discriminative models</term>
					<term>canonical correlation analysis</term>
					<term>kernel methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal human behavior analysis is a challenging task due to the presence of complex nonlinear correlations and interactions across modalities. We present a novel approach to this problem based on Kernel Canonical Correlation Analysis (KCCA) and Multi-view Hidden Conditional Random Fields (MV-HCRF). Our approach uses a nonlinear kernel to map multimodal data to a high-dimensional feature space and finds a new projection of the data that maximizes the correlation across modalities. We use a multi-chain structured graphical model with disjoint sets of latent variables, one set per modality, to jointly learn both view-shared and view-specific substructures of the projected data, capturing interaction across modalities explicitly. We evaluate our approach on a task of agreement and disagreement recognition from nonverbal audio-visual cues using the Canal 9 dataset. Experimental results show that KCCA makes capturing nonlinear hidden dynamics easier and MV-HCRF helps learning interaction across modalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Human communication is often accompanied by multimodal nonverbal cues, such as gestures, eye gaze, and facial expressions. These nonverbal cues play an important role in the way we communicate with others and can convey as much information as spoken language. They complement or substitute for spoken language, help to illustrate or emphasize main points, and provide a rich source of predictive information for understanding the intentions of the others. Automatic analysis of human behavior can thus benefit from harnessing multiple modalities.</p><p>From the machine learning point of view, multimodal human behavior analysis continues to be a challenging task, in part because Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ICMI <ref type="bibr">'12, October 22-26, 2012</ref>, Santa Monica, California, USA. Copyright 2012 ACM 978-1-4503-1467-1/12/10 ...$15.00. learning the complex relationship across modalities is non-trivial. Figure <ref type="figure" target="#fig_0">1</ref>(a) shows a pair of time-aligned sequences with audio and visual features (from <ref type="bibr" target="#b11">[11]</ref>; details can be found in Section 4.1). When learning with this type of data, it is important to consider the correlation and interaction across modalities: An underlying correlation structure between modalities may make the amplitude of the signal from one modality different in relation to the signal from another modality, e.g., loud voice with exaggerated gestures. Also, the interaction between modalities may have certain patterns that change the direction in which each sequence may evolve over time.</p><p>In this paper, we investigate the hypothesis that transforming the original data to be maximally correlated across modalities, in a statistical sense, and capturing the interaction across modalities explicitly from the transformed data improves recognition performance on human behavior analysis. To this end, we present a novel approach to multimodal data learning that captures complex nonlinear correlations and interactions across modalities, based on KCCA <ref type="bibr" target="#b3">[3]</ref> and MV-HCRF <ref type="bibr" target="#b10">[10]</ref>. Our approach uses a nonlinear kernel to map multimodal data to a high-dimensional feature space and finds a new projection of the data that maximizes the correlation across modalities. Figure <ref type="figure" target="#fig_0">1</ref>(b) shows the projected signals found by KCCA, where the relative importance of gestures 'head shake' and 'shoulder shrug' have been emphasized to make the statistical relevance between the audio and visual signals become as clear as possible. We then capture the interaction across modalities using a multi-chain structured latent variable discriminative model. The model uses disjoint sets of latent variables, one set per view, and jointly learns both view-shared and view-specific sub-structures of the projected data.</p><p>We evaluated our approach using the Canal 9 dataset <ref type="bibr" target="#b11">[11]</ref>, where the task is to recognize agreement and disagreement from nonverbal audio-visual cues. We report that using KCCA with MV-HCRF to learn correlation and interaction across modalities successfully improves recognition performance compared to baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Due to its theoretical and practical importance, multimodal human behavior analysis has been a popular research topic. While audio-visual speech recognition is probably the most well known and successful example <ref type="bibr" target="#b6">[6]</ref>, multimodal affect recognition has recently been getting considerable attention <ref type="bibr" target="#b12">[12]</ref>. Bousmalis et al. <ref type="bibr" target="#b1">[1]</ref> proposed a system for spontaneous agreement and disagreement recognition based only on prosodic and gesture cues, as we did here. They used an HCRF to capture the hidden dynamics of the multimodal cues. However, their approach did not consider the correlation and interaction across modalities explicitly.</p><p>Canonical correlation analysis (CCA) has been successfully applied to multimedia content analysis <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b13">13]</ref>. Hardoon et al. used kernel CCA (KCCA) for learning the semantic representation of images and their associated text. However, their approach did not consider capturing hidden dynamics in the data. Latent variable discriminative models, e.g., HCRF <ref type="bibr">[7]</ref>, have shown promising results in human behavior analysis tasks, for their ability to capture the hidden dynamics (e.g., spatio-temporal dynamics). Recently, the multi-view counterpart <ref type="bibr" target="#b10">[10]</ref> showed a significant improvement over single-view methods in recognizing human actions. However, their work did not learn nonlinear correlation across modalities. We extend this body of work, enabling it to modeling multimodal human behavior analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR APPROACH</head><p>Consider a labeled sequential dataset D = {(xi, yi)} N i=1 where xi is a multivariate observation sequence and yi ∈ Y is a sequence label from a finite label set Y. Since we have audio-visual data, we use the notation xi = (ai, vi) where ai ∈ R na×T and vi ∈ R nv ×T are audio and visual sequences of length T , respectively.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of our approach. We first find a new projection of the original observation sequence x = (a , v ) using KCCA <ref type="bibr" target="#b3">[3]</ref> such that the correlation between a and v is maximized in the projected space (Section 3.1). Then we use this projected data as an input to MV-HCRF <ref type="bibr" target="#b10">[10]</ref> to capture hidden dynamics and interaction between audio and visual data (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KCCA</head><formula xml:id="formula_0">Given a set of paired samples {(ai, vi)} N i=1 , A = [a1, • • • , aN ] and V = [v1, • • • , vN ], Canonical Correlation Analysis (CCA)</formula><p>aims to find a pair of transformations wa and wv such that the correlation between the corresponding projections ρ(w a A, w v V) is maximized. However, since CCA finds wa and wv that are linear in the vector space, it may not reveal nonlinear relationships in the data <ref type="bibr" target="#b9">[9]</ref>.</p><p>Kernel CCA (KCCA) <ref type="bibr" target="#b3">[3]</ref> uses the kernel trick <ref type="bibr" target="#b9">[9]</ref> to overcome this limitation by projecting the original data onto a high dimensional feature space before running CCA. A kernel is a function</p><formula xml:id="formula_1">K(xi, xj) that, for all xi, xj ∈ R, K(xi, xj ) = Φ(xi), Φ(xj)</formula><p>where •, • denotes an inner product, and Φ is a nonlinear mapping function to a Hilbert space F, Φ :</p><formula xml:id="formula_2">x ∈ R → Φ(x) ∈ F.</formula><p>To apply the kernel trick, the standard KCCA then rewrites wa (and wv) as an inner product of the data A (and V) with a direction α (and β),</p><formula xml:id="formula_3">wa = A α, wv = V β (1)</formula><p>If we assume that a's and v's are centered (i.e., mean zero), the goal is to maximize the correlation coefficient </p><formula xml:id="formula_4">ρ(•, •) = max</formula><formula xml:id="formula_5">= max α,β αAA VV β αAA AA α • βVV VV β = max α,β α KaKvβ α K 2 a α • β K 2 v β . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where Ka = K(A, A) and Kv = K(V, V) are kernel matrices. Since Equation 2 is scale invariant with respect to α and β (they cancel out), the optimization problem is equivalent to:</p><formula xml:id="formula_7">max α,β α KaKvβ subject to α K 2 a α = β K 2 v β = 1 (3)</formula><p>The corresponding Lagrangian dual form is</p><formula xml:id="formula_8">L(α, β, θ) = α KaKvβ - θα 2 (α K 2 a α -1) - θ β 2 (β K 2 v β -1)<label>(4)</label></formula><p>The solution to Equation 3 is found by taking derivatives of Equation 4 with respect to α and β, and solving a standard eigenvalue problem <ref type="bibr" target="#b5">[5]</ref>. However, when Ka and Kv are non-invertible, as is common in practice with large datasets, problems can arise such as computational issues or degeneracy. This problem is solved by applying the partial Gram-Schmidt orthogonalization (PGSO) with a precision parameter η to reduce the dimensionality of the kernel matrices and approximate the correlation.</p><p>After we find α and β, we plug the solution back in to Equation <ref type="formula">1</ref>to obtain wa and wv, and finally obtain new projections:</p><formula xml:id="formula_9">A = [ wa, a1 , • • • , wa, aN ] (5) V = [ wv, v1 , • • • , wv, vN ] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-view HCRF</head><p>Given the new projection's audio-visual features A and V (Equation 5), the next step is to learn the hidden dynamics and interaction across modalities (see Figure <ref type="figure" target="#fig_0">1</ref> (b) and (c)).</p><p>A Multi-View Hidden Conditional Random Field <ref type="bibr" target="#b10">[10]</ref> (MV-HCRF) is a conditional probability distribution that factorizes according to a multi-chain structured undirected graph G = (V, E ), where each chain is a discrete representation of each view. We use disjoint sets of latent variables h a ∈ H a for audio and h v ∈ H v for visual channel to learn both view-shared and view-specific sub-structures in audio-visual data. An MV-HCRF defines p(y | a , v ) as</p><formula xml:id="formula_10">p(y | a , v ) = h exp{Λ Φ(y, h, a , v )} y ,h exp{Λ Φ(y , h, a , v )} where h = {h a , h v } and Λ = [λ, ω] is a model parameter vector. The function Λ Φ(y, h, a , v ) is factorized with feature functions f k (•) and g k (•) as Λ Φ(y, h, a , v ) = s∈V k λ k f k (y, hs, a , v ) + (s,t)∈E k ω k g k (y, hs, ht, a , v ).</formula><p>Following <ref type="bibr" target="#b10">[10]</ref>, we define three types of feature functions. The label feature function f k (y, hs) encodes the relationship between a latent variable hs and a label y. The observation feature function f k (hs, a , v ) encodes the relationship between a latent variable hs and observations x. The edge feature function g k (y, hs, ht) encodes the transition between two latent variables hs and ht. We use the linked topology from <ref type="bibr" target="#b10">[10]</ref> to define the edge set E (shown in Figure <ref type="figure" target="#fig_0">1(c</ref>)), which models contemporaneous connections between audio and visual observations, i.e., the concurrent latent states in the audio and visual channel mutually affect each other. Note that the f k (•) are modeled under the assumption that views are conditionally independent given respective sets of latent variables, and thus encode the view-specific sub-structures. The feature function g k (•) encodes both view-shared and view-specific sub-structures.</p><p>The optimal parameter set Λ * is found by minimizing the negative conditional log-likelihood</p><formula xml:id="formula_11">min Λ L(Λ) = 1 2σ 2 Λ 2 - N i=1 log p(yi | a i , v i ; Λ) (6)</formula><p>where the first term is the Gaussian prior over Λ that works as an L2-norm regularization. We find the optimal parameters Λ * using gradient descent with a quasi-newton optimization method, the limited-memory BFGS algorithm <ref type="bibr" target="#b5">[5]</ref>. The marginal probability of each node is obtained by performing an inference task using the Junction Tree algorithm <ref type="bibr" target="#b2">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT</head><p>In this section, we describe the dataset, detail our experimental methodology, and discuss our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluated our approach using the Canal9 dataset <ref type="bibr" target="#b11">[11]</ref>, where the task is to recognize agreement and disagreement from nonverbal audio-visual cues during spontaneous political debates. The Canal9 dataset is a collection of 72 political debates recorded by the Canal 9 TV station in Switzerland, with a total of roughly 42 hours of recordings. In each debate there is a moderator and two groups of participants who argue about a controversial political question. The dataset contains highly spontaneous verbal and non-verbal multimodal human behavior data.</p><p>In order to facilitate comparison, we used the same part of the Canal9 dataset with nonverbal audio-visual features as was selected for use in Bousmalis et al. <ref type="bibr" target="#b1">[1]</ref>. This consisted of 53 episodes of agreements and 94 episodes of disagreement collected over 11 debates. Bousmalis et al. selected the episodes based on two criteria: (a) the verbal content clearly indicates agreement/disagreement, which ensures that the ground truth label for the episode is known; (b) the episode includes only one person, with a close-up shot of the speaker. The audio channel was encoded with 2-dimensional prosodic features, including the fundamental frequency (F0) and the energy. The visual channel was encoded with 8 gestures: 'Head Nod', 'Head Shake', 'Forefinger Raise', 'Forefinger Raise-Like', 'Forefinger Wag', 'Hand Wag', 'Hands Scissor', and 'Shoulder Shrug', where the presence/absence of the actions in each frame was manually annotated with binary values. We downsampled the data from the original sampling rate of 25 kHz to 12.5 kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methodology</head><p>The first step in our approach is to run KCCA to obtain a new projection of the data. We used a Gaussian RBF kernel as our kernel function K(xi, xj) = exp -xi -xj /2γ 2 because of its empirical success in the literature <ref type="bibr" target="#b9">[9]</ref>. We validated the kernel width γ = 10 k , k = [-1, 0, 1] and the PGSO precision parameter η = [1 : 6] using grid search. The optimal parameter values were chosen based on the maximum correlation coefficient value.</p><p>Our experiments followed a leave-two-debates-out cross-validation approach, where we selected 2 debates of the 11 debates as the test split, 3 debates for the validation split, and the remaining 6 debates for the training split. This was repeated five times on the 11 debates. The F1 scores were averaged to get the final result. We chose four baseline models: Hidden Markov Models (HMM) <ref type="bibr" target="#b8">[8]</ref>, Conditional Random Fields (CRF) <ref type="bibr" target="#b4">[4]</ref>, Hidden Conditional Random Fields (HCRF) <ref type="bibr">[7]</ref>, and Multi-view HCRF (MV-HCRF) <ref type="bibr" target="#b10">[10]</ref>. We compared this to our KCCA with MV-HCRF approach. Note that HMM and CRF perform per-frame classification, while HCRF and MV-HCRF perform per-sequence classification. The classification results of each model in turn were measured accordingly.</p><p>We automatically validated the hyper-parameters of all models. For all CRF-family models, we varied the L2-norm regularization factor σ = 10 k , k = [0, 1, 2] (see Equation <ref type="formula">6</ref>). For HMM and HCRF, the number of hidden states were varied |H| = [2 : 6]; for MV-HCRF, they were <ref type="bibr">[2 : 4]</ref>). Since the optimization problems in HMM, HCRF and MV-HCRF are nonconvex, we performed two random initializations of each model; the best model was selected based on the F1 score on the validation split. The L-BFGS solver was set to terminate after 500 iterations.</p><formula xml:id="formula_12">|H A |, |H V | = ([2 : 4],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Result and Discussion</head><p>We first compared our approach to existing methods: HMM <ref type="bibr" target="#b8">[8]</ref>, CRF <ref type="bibr" target="#b4">[4]</ref>, HCRF <ref type="bibr">[7]</ref>, and MV-HCRF <ref type="bibr" target="#b10">[10]</ref>. Figure <ref type="figure">2</ref> shows a bar plot of mean F1 scores and their standard deviations. We can see that our approach clearly outperforms all other models.</p><p>For further analysis, we investigated whether learning nonlinear correlation was important, comparing KCCA to CCA and the original data. Table <ref type="table" target="#tab_1">1</ref> shows that models trained with KCCA always outperformed the others, suggesting that learning nonlinear correlation in the data was important. Figure <ref type="figure" target="#fig_0">1</ref> a new space found by KCCA, where the 'head shake' and 'shoulder shrug' gestures were relatively emphasized compared to 'head nod', which maximized the correlation between the audio and visual signals. We believe that this made our data more descriptive, allowing the learning algorithm to capture the hidden dynamics and interactions between modalities more effectively. We also investigated whether our approach captures interaction between audio-visual signals successfully. We compared the models trained with a unimodal feature (audio or visual) to the models trained with audio-visual features. Table <ref type="table" target="#tab_2">2</ref> shows means and standard deviations of the F1 scores. In the three single-chain models, HMM, CRF, and HCRF, there was an improvement when both audio and visual features were used, confirming that using a combination of audio and visual features for our task is indeed important. Also, MV-HCRF outperformed HMM, CRF, HCRF, showing empirically that learning interaction between audio-visual signals explicitly improved the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We presented a novel approach to multimodal human behavior analysis using KCCA and MV-HCRF, and evaluated it on a task of recognizing agreement and disagreement from nonverbal audiovisual cues using the Canal9 dataset. On this dataset, we showed that preprocessing multimodal data with KCCA, by projecting the data to a new space where the correlation across modalities is maximized, helps capture complex nonlinear relationship in the data. We also showed that KCCA with MV-HCRF, which jointly learns view-shared and view-specific interactions explicitly, improves the recognition performance, showing that our approach successfully captured complex nonlinear interaction across modalities. We look forward to applying our technique to other applications in multimodal human behavior analysis for further analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our approach. (a) An audio-visual observation sequence from the Canal 9 dataset [11]. KCCA uses a nonlinear kernel to map the original data to a high-dimensional feature space, and finds a new projection of the data in the feature space that maximizes the correlation between audio and visual channels. (b) The projected data shows that emphasizing the amplitude of the 'head shake' and 'shoulder shrug' gestures maximized the correlation between audio and visual channels. (c) multiview HCRF for jointly learning both view-shared and view-specific sub-structures of the projected data. at and vt are observation variables from audio and visual channels, and h a t and h v t are hidden variables for audio and visual channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a av wv] E[w a aa wa] E[w v vv wv] = max wa,wv w a AV wv w a AA waw v VV wv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,113.50,16.20,385.00,140.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>bar plot of mean F1 scores with error bars showing standard deviations. This shows empirically that our approach successfully learned correlations and interactions between au- dio and visual features using KCCA and MV-HCRF.</head><label></label><figDesc>(b)  shows the data projected in</figDesc><table><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean F1 Score</cell><cell>0.6 0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>.59</cell><cell>.61</cell><cell>.64</cell><cell>.68</cell><cell>.72</cell></row><row><cell></cell><cell></cell><cell>HMM</cell><cell>CRF</cell><cell cols="2">HCRF MV-HCRF</cell><cell>Our Approach</cell></row><row><cell cols="3">Figure 2: A Models</cell><cell cols="2">Original Data</cell><cell>CCA</cell><cell>KCCA</cell></row><row><cell></cell><cell cols="2">HMM</cell><cell cols="2">.59 (.09)</cell><cell cols="2">.59 (.12) .61 (.13)</cell></row><row><cell></cell><cell>CRF</cell><cell></cell><cell cols="2">.61 (.04)</cell><cell cols="2">.63 (.03) .67 (.08)</cell></row><row><cell></cell><cell cols="2">HCRF</cell><cell cols="2">.64 (.13)</cell><cell cols="2">.65 (.07) .69 (.06)</cell></row><row><cell></cell><cell cols="2">MV-HCRF</cell><cell cols="2">.68 (.13)</cell><cell cols="2">.71 (.07) .72 (.07)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Experimental results (means and standard deviations of F1 scores) comparing KCCA to CCA and the original data. The results show that learning nonlinear correlation in the data was important in our task.</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Experimental results (means and standard deviations of F1 scores) comparing unimodal (audio or video) features to the audio-visual features. The results confirms that using both audio and visual features are important in our task.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Audio</cell><cell>Video</cell><cell>Audio+Video</cell></row><row><cell>HMM</cell><cell cols="2">.54 (.08) .58 (.11)</cell><cell>.59 (.09)</cell></row><row><cell>CRF</cell><cell cols="2">.48 (.05) .58 (.15)</cell><cell>.61 (.04)</cell></row><row><cell>HCRF</cell><cell cols="2">.52 (.09) .60 (.09)</cell><cell>.64 (.13)</cell></row><row><cell>MV-HCRF</cell><cell>•</cell><cell>•</cell><cell>.68 (.13)</cell></row><row><cell>KCCA + MV-HCRF</cell><cell>•</cell><cell>•</cell><cell>.72 (.07)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>This work was funded by <rs type="funder">ONR</rs> grant #<rs type="grantNumber">N000140910625</rs>, <rs type="funder">NSF</rs> grant #<rs type="grantNumber">IIS-1118018</rs>, <rs type="funder">NSF</rs> grant #<rs type="grantNumber">IIS-1018055</rs>, and <rs type="funder">U.S. Army RDE-COM</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SZJ945a">
					<idno type="grant-number">N000140910625</idno>
				</org>
				<org type="funding" xml:id="_YnTuQvA">
					<idno type="grant-number">IIS-1118018</idno>
				</org>
				<org type="funding" xml:id="_MRQhYX7">
					<idno type="grant-number">IIS-1018055</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Probabilistic Networks and Expert Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Audio-Visual Automatic Speech Recognition: An Overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden conditional random fields</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view latent variable discriminative models for action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Canal9: A database of political debates for analysis of social interactions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dielmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salamin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">ACII</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-modal correlation learning for clustering on image-audio dataset</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
