<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Sparse Partial Least Squares</title>
				<funder ref="#_NMaZGZt">
					<orgName type="full">Spanish Ministry of Science and Innovation (MINECO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Diego</forename><surname>Vidaurre</surname></persName>
							<email>diego.vidaurre@ohba.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Marcel</forename><forename type="middle">A J</forename><surname>Van Gerven</surname></persName>
							<email>m.vangerven@donders.ru.nl</email>
						</author>
						<author>
							<persName><forename type="first">Concha</forename><surname>Bielza</surname></persName>
							<email>mcbielza@fi.upm.es</email>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Larra</surname></persName>
							<email>pedro.larranaga@fi.upm.es</email>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
							<email>t.heskes@science.ru.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Oxford Centre for Human Brain Activity</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<postCode>OX3 7JX</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Donders Institute for Brain, Cognition and Behavior</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postCode>6525H</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computational Intelligence Group</orgName>
								<orgName type="institution">Universidad Politécnia Madrid</orgName>
								<address>
									<postCode>28660</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institute for Computing and Information Science</orgName>
								<orgName type="department" key="dep2">Intelligent Systems</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postCode>6525H</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Sparse Partial Least Squares</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1162/NECO_a_00524</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Partial least squares (PLS) is a class of methods that makes use of a set of latent or unobserved variables to model the relation between (typically) two sets of input and output variables, respectively. Several flavors, depending on how the latent variables or components are computed, have been developed over the last years. In this letter, we propose a Bayesian formulation of PLS along with some extensions. In a nutshell, we provide sparsity at the input space level and an automatic estimation of the optimal number of latent components. We follow the variational approach to infer the parameter distributions. We have successfully tested the proposed methods on a synthetic data benchmark and on electrocorticogram data associated with several motor outputs in monkeys.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Partial least squares (PLS) <ref type="bibr" target="#b27">(Wold, Sj östr öm, &amp; Eriksson, 2001</ref>) is a family of techniques originally devised for modeling two sets of observed variables, which we shall call input and output components, by means of some (typically low-dimensional) set of latent or unobserved components. This model can also be extended to deal with more than two sets of components <ref type="bibr" target="#b25">(Wangen &amp; Kowalsky, 1989)</ref>. It is commonly used for regression but is also applicable to classification <ref type="bibr" target="#b2">(Barker &amp; Rayens, 2003)</ref>. In this letter, we focus on the regression paradigm. Latent components are generated by some linear transformation of the input components, while the output components are assumed to be generated by some linear transformation of the latent components.</p><p>The difference between PLS and related techniques lies in how the latent components are estimated <ref type="bibr" target="#b13">(Hastie, Tibshirani, &amp; Friedman, 2008;</ref><ref type="bibr" target="#b19">Rosipal &amp; Krämer, 2006)</ref>. Unlike PLS, principal components regression (PCR), for example, does not consider the output when constructing the latent components. Also, PLS differs from canonical correlation analysis (CCA) in that CCA treats the input and output spaces symmetrically <ref type="bibr" target="#b12">(Hardoon, Szedmak, &amp; Shawe-Taylor, 2004)</ref>. A complete comparison between PLS, PCR and classical shrinkage regression from a statistical perspective is given by <ref type="bibr" target="#b8">Frank and Friedman (1993)</ref>, and further insight into the shrinkage properties of PLS can be found, for instance, by <ref type="bibr" target="#b11">Goutis (1996)</ref>. There exist Bayesian formulations of some latent component models in the literature, such as PCA <ref type="bibr" target="#b4">(Bishop, 1998;</ref><ref type="bibr" target="#b17">Nakajima, Sugiyama, &amp; Babacan, 2011)</ref>, CCA <ref type="bibr" target="#b9">(Fujiwara, Miyawaki, &amp; Kamitani, 2009;</ref><ref type="bibr" target="#b23">Virtanen, Klami, &amp; Kaski, 2011;</ref><ref type="bibr" target="#b24">Wang, 2007)</ref> and mixtures of factor analyzers <ref type="bibr" target="#b3">(Beal, 2003;</ref><ref type="bibr" target="#b10">Ghahramani &amp; Beal, 2000)</ref>. To our knowledge, however, a Bayesian version of PLS has not yet been proposed.</p><p>Different varieties of PLS regression arise by the way they extract latent components <ref type="bibr" target="#b19">(Rosipal &amp; Krämer, 2006)</ref>. In its classic form, PLS aims to maximize the covariance among the latent components, which are constrained to be orthogonal, using the nonlinear iterative partial least squares (NIPALS) algorithm <ref type="bibr" target="#b26">(Wold, 1975)</ref>. This is more an algorithmic than a traditional statistical approach, and, hence, the analysis of its properties is less obvious. A more rigorous approach (from a statistical perspective) is taken by <ref type="bibr" target="#b7">de Jong (1993)</ref>, who directly formulates the latent space as a linear projection of the input space and solves the resulting optimization problem by the so-called SIMPLS algorithm. The SIMPLS algorithm is equivalent to NIPALS only when the output space is unidimensional. Sparsifying accounts of PLS are proposed by <ref type="bibr" target="#b22">van Gerven, Chao, and Heskes (2012)</ref> and <ref type="bibr" target="#b6">Chun and Keles (2010)</ref>. A kernelized approach has been introduced by Lindgren, Geladi, and <ref type="bibr" target="#b16">Wold (1993)</ref> and <ref type="bibr" target="#b20">Rosipal and Trejo (2001)</ref>.</p><p>The main goal of this letter is to develop a Bayesian approach for PLS regression. We use variational inference <ref type="bibr" target="#b15">(Jaakkola, 2001)</ref> for estimating the parameters. Let X be an N × p input matrix and Y be an N × q output matrix, with elements x ni and y nj and rows x n and y n . Assuming centered data, we follow the definition of PLS given by where P and Q are, respectively, p × k and k × q loading matrices, Z is the N × k latent score matrix, with elements z il and rows z n , and Z and Y are the matrices of residuals. We use an intermediate k-dimensional latent space, k typically being lower than p and q. We consider a Bayesian hierarchy defined through several normal Wishart distributions for the latent and output variables, as well as for the loading matrices:</p><formula xml:id="formula_0">Z = XP + Z , Y = ZQ + Y ,</formula><formula xml:id="formula_1">z ∼ N (x P, ), -1 ∼ W (A, ι), p l ∼ N (0, l ), -1 l ∼ W (B l , ν l ), y ∼ N (z Q, ), -1 ∼ W (C, κ ), q j ∼ N (0, j ), -1 j ∼ W (D j , ς j ), (1.1)</formula><p>with l = 1, . . . , k and j = 1, . . . , q, and where p l is the lth column of P and q j is the jth column of Q. A, B l , C, D j are the scale matrix hyperparameters of the Wishart prior distributions, and ι, ν l , κ, ς j are the corresponding degrees of freedom. (See Figure <ref type="figure" target="#fig_0">1</ref> for a graphical representation using plate notation.) In the remainder, we suppress the hyperparameters in our notation when it is clear from the context. By imposing separate gaussian priors for each column of P (and Q), we are allowing different input (and latent) variable couplings for each component l = 1, . . . , k (and j = 1, . . . , q). An obvious simplification, which we take in the rest of the letter, is to let l = (and j = ), so that information is borrowed among the latent components and responses and the number of parameters is reduced. The derivation of the general case is straightforward.</p><p>It might appear that for large p scenarios, a large number of parameters is associated with the full precision matrix -1 . However, as we will show, the estimation of these matrices is low rank, so that the effective number of parameters is kept reasonably low.</p><p>Note that E[y |x ] = x PQ. Since PQ has at most rank k, this formulation is related to reduced rank methods <ref type="bibr" target="#b14">(Izenman, 1975)</ref> and approaches that penalize the nuclear norm of the coefficient matrix <ref type="bibr" target="#b28">(Yuan, Ekici, Lu, &amp; Monteiro, 2007)</ref>. There is also a connection with the multivariate group Lasso <ref type="bibr" target="#b18">(Obozinski, Wainwright, &amp; Jordan, 2011)</ref>, which imposes an L 1 /L 2penalty on the coefficient matrix so that a common sparsity pattern is shared by all responses. However, the multivariate group Lasso does not account for correlated errors. The sparse multivariate regression with covariance estimation approach <ref type="bibr" target="#b21">(Rothman, Levina, &amp; Zhu, 2010)</ref>, on the other hand, does consider correlation between the responses by simultaneously estimating the coefficient matrix and the (full) inverse covariance matrix of the response variables. The coefficient matrix is L 1 -regularized, and then the sparsity pattern can vary for each response. Our approach is also somewhat related to the multitask feature learning problem, where each task has a different set of inputs and the goal is to find some shared structural parameterization that is beneficial for the individual tasks. For example, the method proposed by <ref type="bibr" target="#b1">Argyriou, Evgeniou, and Pontil (2006)</ref> seeks a lowrank linear transformation such that the outputs are encouraged to share a common input sparsity pattern. The method introduced by <ref type="bibr" target="#b0">Ando and Zhang (2005)</ref> is formulated so that unlabeled data can be used for learning common underlying predictive functional structures. However, these approaches do not build on a generative, model and it is not possible to express a Bayesian formulation that leads to the same estimator.</p><p>The rest of the letter is organized as follows. Section 2 introduces the variational approximation in the basic setting. Section 3 describes how to achieve a sparse solution. Section 4 proposes an improved model that aims to estimate the optimal number of latent components and increase the accuracy. Section 5 presents a simulation study with comparisons to other methods. Section 6 provides some results for real neural signal decoding. Finally, section 7 provides conclusions and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variational Parameter Inference</head><p>We are interested in the posterior distribution Pr(P, Q | X, Y ), given by</p><formula xml:id="formula_2">P(P, Q, Z, -1 , -1 , -1 , -1 | X, Y ) dZ d -1 d -1 d -1 d -1 .</formula><p>For computational reasons, we approximate the posterior distribution of the parameters given Y by a variational distribution with the following factorization:</p><formula xml:id="formula_3">P(P, Q, Z, -1 , -1 , -1 , -1 | X , Y ) ≈ F(P, Q, Z, -1 , -1 , -1 , -1 ) = F(Z)F(P, -1 , -1 , Q, -1 , -1 ).</formula><p>The variational approximation automatically (i.e., without the need for further assumptions) factorizes into F(Z)F(P, -1 , -1 )F(Q, -1 , -1 ).</p><p>This can be easily verified by inspecting the functional F(P, -1 , -1 , Q, -1 , -1 ), defined as the log of the joint distribution when we take the expectation with respect to Z <ref type="bibr" target="#b3">(Beal, 2003)</ref>. Since Z separates P, -1 , -1 from Q, -1 , -1 in the Bayesian hierarchy, the resulting expression for F(P, -1 , -1 , Q, -1 , -1 ) does not have interaction terms between the two groups of variables.</p><p>Also on computational grounds, we assume F(P, -1 , -1 ) = F(P) F( -1 , -1 ) and, analogously, F(Q, -1 , -1 ) = F(Q)F( -1 , -1 ). From this, we have an automatic factorization between -1 and -1 ( -1 and -1 ). Finally, F(Z) automatically factorizes into N n=1 F(z n ), so that up to a constant, we have</p><formula xml:id="formula_4">F(z n ) = E P,Q, -1 , -1 [log P(z n |X , P, -1 ) + log P(y|z n , Q, -1 )] = - 1 2 z n (E[ -1 ]+E[Q -1 Q ])z n +(x n μ P E[ -1 ]+y n E[ -1 ]μ Q )z n ,</formula><p>where μ P and μ Q are the expectations of, respectively, P and Q. Expectations are with regard to the variational distribution. Completing the square, we have</p><formula xml:id="formula_5">F(z n ) = N (z n ; μ z n , S z n ) (2.1) with S z n = (E[ -1 ] + E[Q -1 Q ]) -1 and μ z n = S z n (E[ -1 ]μ P x n + μ Q E[ -1 ] y n ). We can compute E[Q -1 Q ] = μ Q E[ -1 ]μ Q + q j 1 =1 q j 2 =1 E[ -1 j 1 j 2 ]S Q j 1 j 2 ,</formula><p>where S Q j 1 j 2 denotes the k × k cross-covariance matrix relative to the j 1 th and j 2 th columns of the loading matrix Q.</p><p>For -1 , we have, up to a constant,</p><formula xml:id="formula_6">log F( -1 ) = E Z,P [log P(Z|X, P, -1 ) + log P( -1 )] = ι -k -N -1 2 log | | - 1 2 Tr((E[Z Z] + E[P X XP] -μ Z X μ P -μ P X μ Z + A -1 ) -1 ),</formula><p>where we have used standard properties of the trace operator. Here, we can identify a Wishart distribution,</p><formula xml:id="formula_7">F( -1 ) = W ( -1 ; Ã-1 ,ι), (2.2) with ι = ι + N and Ã-1 = (E[Z Z] + E[P X XP] -μ Z X μ P -μ P X μ Z + A -1 ) -1 , where E[Z Z] = N n=1 E[z n z n ] = N n=1 (S z n + μ z n μ z n ).</formula><p>If we set -1 to be diagonal, then the variational distribution F(P) factorizes over columns and we get a gamma distribution for each element</p><formula xml:id="formula_8">-1 ll : F( -1 ll ) = G( -1 ll ;ι, Ã-1 ll ) (2.3) with ι = ι + N 2 and Ã-1 ll = 1 2 (E[Z •l Z •l ] + E[p l X X p l ] -2 μ Z •l X μ p l ) + A -1 ll , where Z •l denotes the lth column of Z.</formula><p>If we do not factorize F(P), that is, if -1 is not chosen to be diagonal, then we have, up to a constant, log F(P)</p><formula xml:id="formula_9">= E Z, -1 , -1 log P(Z|X, P, -1 ) + k l=1 log P(p l | ) = - N n=1 1 2 x n PE[ -1 ]P x n -x n PE[ -1 ]μ z n - 1 2 k l=1 p l E[ -1 ]p l .</formula><p>We define p as the concatenation of the rows of P and p * as the concatenation of the rows of the p × k least-squares solution (X X ) -1 X μ Z , so that after some algebra, we can identify a pk-dimensional gaussian distribution,</p><formula xml:id="formula_10">F(P) = N (P; μ P , S P ) (2.4) with S p = (E[ -1 ] ⊗ I k + X X ⊗ E[ -1 ]) -1 and μ p = S p X X ⊗ E[ -1 ] p * ,</formula><p>where I k is the k × k identity matrix and ⊗ denotes the Kronecker product.</p><p>From this expression, we can reconstruct μ P and S P .</p><p>When -1 is diagonal, we can simplify</p><formula xml:id="formula_11">F(P) = k l=1 F(p l ).</formula><p>For each factor, we have</p><formula xml:id="formula_12">F(p l ) = N (p l ; μ p l , S p l ) (2.5) with S p l = (E[ -1 ] + E[ -1 ll ]X X ) -1 and μ p l = E[ -1 ll ]S p l X μ Z •l .</formula><p>For -1 , we have, up to a constant,</p><formula xml:id="formula_13">log F( -1 ) = E p l log P( -1 ) + k l=1 log P(p l | -1 ) = ν l -p -k -1 2 log | -1 | - 1 2 Tr((E[PP ] + B -1 l ) -1 ),</formula><p>where we can identify a Wishart distribution:</p><formula xml:id="formula_14">F( -1 ) = W ( -1 ; B-1 , ν), (2.6) with B-1 = (E[PP ] + B -1</formula><p>) -1 and ν = ν + k. Note that the matrix E[PP ] is not full rank as far as p &gt; k, which is typically the case. It has, in fact, rank k. Then the effective number of parameters of this matrix is not p(p -1)/2, but, at most, pk + 1k(k -1)/2. When p is high relative to N, it becomes necessary to borrow information between the components l = 1, . . . , k, suggesting the choice -1 l = -1 . Calculations for Q and dependent distributions are similar to those of P and are given in appendix A. <ref type="bibr" target="#b5">Brown &amp; Zidek (1980)</ref> theoretically showed that an adaptive joint estimation dominates an independent estimation for each of the outputs separately when the number of inputs is considerably larger than the number of outputs, but this domination breaks down when the number of outputs approaches the number of inputs. In our situation, when estimating Q, the number of outputs q typically even exceeds the number of hidden units k. This suggests that the factorization F(Q) = q j=1 F(q j ), mimicking independent estimation, is the sensible choice for q &gt; k, which is often the case.</p><p>In short, the proposed approach proceeds as follows:</p><p>1. Initialize Z to the k first principal components of Y . 2. Compute the distributions of -1 , P and -1 using equations 2.2, 2.4, and 2.6. 3. Compute the distributions of -1 , Q and -1 using equations A.1, A.3, and A.5. 4. Compute the distribution of Z using equation 2.1. 5. Repeat steps 2 to 4 until convergence. This grouping of the updates is motivated by the structure of the Bayesian hierarchy and the variational factorization in equation 2.1. A variant of the basic scheme, by assuming diagonality of -1 and -1 , arises by substituting equation 2.2 by 2.3, 2.4 by 2.5, A.1 by A.2, and A.3 by A.4. A variational lower bound of the evidence is given in appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparsity in P and Q</head><p>For achieving sparsity on the input variables, we may impose a groupsparsifying prior on P, so that the groups are the k-dimensional rows of P. By setting an individual regularization parameter on each group and integrating out P, the maximum likelihood value of such regularization parameters will effectively discard some groups. This is an example of groupwise automatic relevance determination (see, for example, <ref type="bibr" target="#b23">Virtanen et al., 2011)</ref>.</p><p>To achieve this objective, we set priors</p><formula xml:id="formula_15">P i• ∼ N (0, σ 2 i I k ),</formula><p>where P i• is the ith row of P. This way, we will effectively drop the useless inputs (rows of P). We define the precisions σ -2 i to be gamma distributed. The variational approximation of the posterior of p l becomes a gaussian distribution with parameters</p><formula xml:id="formula_16">S p l = (diag(E[σ -2 ]) + E[ -1 ll ]X X ) -1 and μ p l = E[ -1 ll ]S p l X μ Z •l .</formula><p>The derivation for nonfactorized P is straightforward. For σ -2 i , we have</p><formula xml:id="formula_17">E[σ -2 i ] = 2ν + k E[P i• P i• ] + 2B -1 ii = 2ν + k k l=1 (S P il,il + μ 2 P il ) + 2B -1 ii .</formula><p>Also, we impose a similar groupwise prior on Q:</p><formula xml:id="formula_18">Q l• ∼ N (0, γ 2 l I q ),</formula><p>with the precision γ -2 l being gamma distributed. The idea is to obtain a data-driven estimation of the importance of each latent component when estimating Q. The variational approximation of q j is a gaussian distribution with parameters S q j</p><formula xml:id="formula_19">= (diag(E[γ -2 ]) + E[ -1 j j ]E[Z Z]) -1 and μ q j = E[ -1 j j ]S q j μ Z Y • j .</formula><p>Again, we follow a variational approximation to obtain</p><formula xml:id="formula_20">E[γ -2 l ] = 2ς + q E[Q l• Q l• ] + 2D -1 ll = 2ς + q q j=1 (S Q l j,l j + μ 2 Q l j ) + 2D -1 ll .</formula><p>Then a value σ -2 i (γ -2 l ) close to zero means that the corresponding input (latent) variable can be considered irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive P and Automatic Selection of k</head><p>The proposed estimation of P disregards Y and uses only the current state of Z. Put differently, equal attention is paid to all latent components when estimating P, no matter the contribution of each latent component to the prediction of Y . A possible improvement would be to focus on those latent components that are more useful for modeling Y , regularizing more the latent components that are relatively useless-those whose coefficients in Q are lower.</p><p>Here, we impose groupwise priors on P for both rows and columns so as to achieve two goals. First, we provide a more adaptive estimation of P, which we hope will reduce the bias. Second, starting with a reasonably high value of k, we will be better able to obtain an estimate of the adequate dimension for the latent space. Hence, the purpose of this section is to improve the model performance for a given number of latent components. A proper model selection scheme, which would compare the model evidence (see appendix B) for different values of k, would be an alternative (or complementary) way to go.</p><p>We consider the variational approximation with P and Q factorized over columns. We let γ -2 also influence P so that it indeed controls the latent space for both projection matrices. We balance the regularization effect of γ -2 with an additional variable φ -2 , so that the relative contributions of σ -2 and γ -2 are adaptively estimated from data. We choose φ -2 to be gamma distributed for keeping conjugacy. Specifically, we propose</p><formula xml:id="formula_21">P il ∼ N (0, (σ -2 i + φ -2 γ -2 l ) -1 ), φ -2 ∼ G(e, ϕ).</formula><p>The variational distribution of p l is a gaussian distribution whose parameters are</p><formula xml:id="formula_22">S p l = (diag(E[σ -2 ]) + E[φ -2 ]E[γ -2 l ]I p + E[ -1 ll ]X X ) -1 and μ p l = E[ -1 ll ]S p l X μ Z •l .</formula><p>For γ -2 , we have</p><formula xml:id="formula_23">E[γ -2 l ] = 2ς + p + q φ -2 p i=1 (S P il,il + μ 2 P il ) + q j=1 (S Q l j,l j + μ 2 Q l j ) + 2d -1 l ,</formula><p>and, for φ -2 ,</p><formula xml:id="formula_24">E[φ -2 ] = 2ϕ + p k p i=1 k l=1 γ -2 l (S P il,il + μ 2 P il ) + 2e -1 .</formula><p>Then, by automatic relevance determination, the number of coordinates of γ -2 significantly greater than zero at convergence is an estimation of the optimal number of latent variables k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Synthetic Experiments</head><p>In order to test in practice the performance of the algorithms, we now present a synthetic simulation study where we compare the sparse approach proposed in section 3 (SPLS for short) with other methods. For simplicity, we do not present results for the nonfactorized estimation of P and Q or for full matrices -1 and -1 .</p><p>We have tested some alternative PLS methods: the NIPALS and SIMPLS algorithms and the frequentist sparse PLS from Chun and Keles (2010) (sSIMPLS). All the PLS methods have been provided with four latent components. Also, we have tested a number of non-PLS univariate and multivariate regression techniques. Univariate methods are separately applied for each response; these include ordinary least squares regression (OLS), ridge regression, and the Lasso <ref type="bibr" target="#b13">(Hastie et al., 2008)</ref>. From the multivariate side, we have tested the multivariate group lasso (MGL) <ref type="bibr" target="#b18">(Obozinski et al., 2011)</ref> and the sparse multivariate regression with covariance estimation approach (MRCE) <ref type="bibr" target="#b21">(Rothman et al., 2010)</ref>. The regularization parameters for the non-Bayesian methods have been chosen by cross-validation.</p><p>The design of the simulation study is as follows. In all experiments, the number of input variables is p = 50, and the number of responses is q = 8. We have covered several different situations, varying the number of hidden components, which we denote as k 0 , and the amount of training data (N = 100, 500). We have set k 0 = 1, 2, 4, 8. Within each situation, we generated 100 random replications. For each of them, we have sampled 1000 testing data points. Then, for each situation and each model, reports are shown over 100 × 1000 × 8 = 800,000 residuals.</p><p>For each random replication, the input matrix was generated according to a multivariate gaussian distribution with zero mean and covariance matrix M, whose elements were set as</p><formula xml:id="formula_25">M i 1 i 2 = r |i 1 -i 2 | 1</formula><p>, and r 1 was sampled from the uniform distribution with support [0, 1]. Hence, depending on r 1 , the degree of collinearity in the input matrix is different among the replications. Of course, the testing input matrix was sampled using the same covariance matrix M.</p><p>When k 0 &lt; 8, the latent components were generated as Z = XP + Z . Sparsity was enforced by setting each row of P to zero with probability 0.8 (ensuring at least two relevant input variables). The rest of the rows were sampled from a standard normal distribution. The noise Z was generated from a gaussian distribution with zero mean and diagonal covariance matrix , with ll = r 2 sd(X p l ), where sd(•) denotes the sample standard deviation and the value r 2 was sampled from the uniform distribution with support [0.01, 0.1] (separately for each each diagonal element).</p><p>We generated the responses as Y = ZQ + Y . We did not consider sparsity in Q, whose elements were sampled from a standard normal distribution. The noise Y was sampled from a gaussian distribution with zero mean and diagonal covariance matrix , with diagonal elements j j = r 3 sd(Zq j ), where r 3 was sampled from the uniform distribution with support [0.25, 0.5].</p><p>When k 0 = 8, the response was computed as Y = XF + Y , where F has rank q. Since q &gt; k, F has a higher number of effective parameters than PQ in the factorized (k 0 &lt; 8) case. F was sampled from a normal standard distribution, considering sparsity as before.</p><p>Figures <ref type="figure">2,</ref><ref type="figure">3</ref>, 4, and 5 show box plots with the performance of the different methods for all situations. Results are in terms of the explained variance R 2 . In short, these graphs illustrate how the methods behave for different When k 0 = 8, the response function does not factorize and has the highest number of parameters. In this case, the PLS methods have fewer parameters than the actual true response function and tend to underfit. This is in particular the case for N = 500, when there are sufficient data for a reliable estimation of all the parameters. Interestingly, SPLS is the only PLS method that does not perform much worse than ridge regression, the Lasso, MGL, and MRCE for N = 100 and also for N = 500.</p><p>The k 0 = 1 case is the opposite extreme. Whereas the full-rank methods aim to estimate pq = 400 parameters, the PLS methods, which are set to use four latent components, estimate 4p + 4q = 232 parameters. Still, this is much more than the true number of parameters (p + q = 58). For this reason, the PLS methods are not much better than the others. Indeed, SPLS and the Lasso appear to be the more effective in controlling the complexity of the model. Note that the differences between the methods are very subtle for N = 500.</p><p>When k 0 = 2, the true response function has 116 parameters. In this case, for N = 100, SPLS and the Lasso clearly outperform the other methods. SPLS is slightly better than the Lasso. Surprisingly, SIMPLS and NIPALS do not perform better than OLS. For N = 500, SPLS, OLS, ridge regression, the Lasso, and MGL are almost indistinguishable and better than the others.</p><p>Finally, when k 0 = 4, the true number of parameters (232) matches that of the PLS methods. Surprisingly, for N = 100, SIMPLS, sSIMPLS, and NIPALS are again less accurate than any of the univariate regression methods, MGL and MRCE. On the contrary, SPLS, followed by the Lasso, are the better methods. For N = 500, SPLS, the univariate regression methods (including OLS) and MGL have the same performance, which is not far from the optimal Bayes error.</p><p>It is noticeable that SPLS is the only PLS method that works as well as the Lasso, and even beats it in some situations (N = 100 and k 0 = 1, 2). The good performance of the univariate regression techniques (in particular, ridge regression and the Lasso) is very likely because is diagonal, even when there is a coupling due to . The poor performance of MRCE probably comes from the estimation of a full inverse covariance matrix of the response variables.</p><p>In these experiments, the performance of APLS (not shown) is not very different from that of SPLS. This is not surprising, because the synthetic data sets were generated according to equations 1.1, which correspond to the SPLS model. In the next section, we shall see an example where the adaptive version is clearly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Electrocorticogram Data Decoding</head><p>In this section we describe some experimental results on a neuroscientific data set. In particular, we aim to decode the motor output from electrocorticogram (ECoG) signals collected in monkeys. ECoG signals were recorded at a sampling rate of 1 kHz per channel, for 32 electrodes implanted in the right hemisphere, and filtered. Afterward, the signals were bandpass-filtered from 0.3 to 500 Hz. A time-frequency representation of the ECoG signals containing p = 1600 variables (32 electrodes, 10 frequency bins, and 5 time lags) was used as the input for the algorithms. The monkey's movements were captured at a sampling rate of 120 Hz. Seven degrees of freedom of the monkey's movements (q = 7) are to be modeled and predicted. From the available data, we have used N = 5982 time points for training and 5982 more for testing. More details about the data can be found in <ref type="bibr" target="#b22">van Gerven et al. (2012)</ref>.</p><p>Given the high dimensionality of the data, we have run the simplest version of the algorithms; we have factorized P and Q, and -1 and -1 are taken to be diagonal.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> illustrates the performance (explained variance R 2 over testing data) of the proposed approaches compared to SIMPLS and sSIMPLS. Although not included in the plot, the results of NIPALS are almost indistinguishable from SIMPLS. It is remarkable that APLS performs better than the other methods for most of the responses and is always better than the nonadaptive algorithm. Note that APLS appears to need fewer latent components to give a reasonable estimation. Moreover, APLS is more robust to the choice of k than the other algorithms (including SPLS). Only for the wrist abduction response is the APLS accuracy clearly decreased when k &gt; 5. On the other hand, SPLS performs worse in general for high values of k. Figure <ref type="figure">7a</ref> shows, for SPLS and APLS, a histogram with the values of q j=1 |P i• q j | as a measure of the importance of each input variable for the output prediction. (Note that σ -2 reflects the importance of each input for predicting the latent variables and hence is not a good measure of importance of each input variable for the output prediction.) It is worth noting that APLS yields sparser solutions than SPLS in this sense.</p><p>Figure <ref type="figure">7b</ref> shows the values of γ -2 and the variance of the latent components for six executions of APLS, each with a different number of latent components k = 2, . . . , 7. Each line, labeled with a different type of symbol, corresponds to a different value k. Note that latent variables that have a high value γ -2 l (left graph) or exhibit a low variance (right graph) are given less importance. From the graph, we can conclude that the first two latent components are the most important for the prediction. For example, the line with the × symbol corresponds to k = 5 and has five components (symbols). Each component of this line corresponds to a latent component in the model. The lowest value of γ -2 l (or the highest variance of Z l ) for this model pertains to the first two components. Hence, for this model, the two relevant components are l = 1, 2. For the models with k = 6, 7 components (whose lines have the and symbols), however, the last components have the lowest values for γ -2 . The accuracy in these cases is worse than the accuracy that can be obtained with lower k values (see Figure <ref type="figure" target="#fig_3">6</ref>). In summary, for all runs, there are two predominant latent components (either the first two or the last two), which indicates that k = 2 is a reasonable estimate of the optimal value of latent components.</p><p>Finally, Figure <ref type="figure">8</ref> demonstrates the trajectories decoded by SIMPLS and APLS. We have used k = 7 for SIMPLS and k = 2 for APLS. The two Notes: Each row corresponds to a motor output: shoulder abduction (SA), shoulder flexion (SF), pronation (P), wrist abduction (WA), shoulder rotation (SR), elbow flexion (EF), and wrist flexion (WF). The best method is highlighted in bold. components from γ -2 . Unlike other PLS formulations, the adaptive model appears to be robust to the choice of k.</p><p>For automatically selecting k, we can run the algorithm with several values of k and then select the one that reaches the highest model evidence (see appendix B). A more sophisticated solution is to move toward a nonparametric method, where a proper prior on Z would automatically select the optimal number of latent components. However, the model would probably lose its conjugacy, so that variational inference would no longer be practicable, and we would have to resort to sampling methods of inference.</p><p>Note that up to permutation of the latent components and sign flips, the model is identifiable thanks to the priors over P and Q, even when neither Q nor the latent components are forced to be orthonormal. Furthermore, although the model is unidentifiable with respect to permutations of the latent components, due to the initialization of Z (step 1 of the algorithm in section 2), the method will always produce the same order in the latent components across different runs.</p><p>Future developments can involve a Markovian consideration of the time dynamics. By means of this extension, a connection between PLS and an input-output linear dynamical system <ref type="bibr" target="#b3">(Beal, 2003)</ref> can be established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A</head><p>We now formulate the variational update equations for -1 , Q, -1 . For a full matrix -1 , we have a Wishart distribution, = -q 2 ln(2π ) + 1 2</p><formula xml:id="formula_26">q j=1 ψ κ + 1 -j 2 + q ln 2 + ln C-1 - κ 2 μ z n μ Q C-1 μ Q + q j=1 C-1 j j S q j μ z n - κ 2 Tr S z n μ Q C-1 μ Q + q j=1 C-1 j j S q j - κ 2 y n C-1 y n + κy n C-1 μ Q μ z n ,</formula><p>where G(•) and ψ (•) are the gamma and digamma functions. The expressions for E[ln P( -1 )], E[ln P(q j | -1 )] and E[ln P( -1 )] are analogous to The variational lower bound for other variations of the method can be easily computed following the same line of argument.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Bayesian hierarchy of the proposed Bayesian PLS model, where Z lies in the latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Box plot of the R 2 values for k 0 = 8 and N = 100, 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Box plot of the R 2 values for k 0 = 2 and N = 100, 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: R 2 value for each response for a different number of latent components k = 2, . . . , 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 7: (a) Histogram of values q j=1 |P i• q j | for SPLS and APLS. (b) Vector γ -2 and vector of individual latent component variances for APLS. Each line, labeled with a different type of symbol, represents a different run with a different maximum number of latent components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>( -1 ) = W ( -1 ; C-1 , κ ), (A.1) with C-1 = (Y Y + E[Q Z ZQ]-Y μ Z μ Q -μ Q μ Z Y + C -1 ) -1 and κ = κ + N. + E[ln F( -1 , -1 )] P(y n |z n , Q, -1 )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>E[ln P( -1 )], E[ln P(p l | -1 )], and E[ln P( -1 )], respectively, and are not shown.The rest of the terms in equation B.1 correspond to the negative entropies of the F(•) distributions-for example:E[ln F(p l )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean Absolute Error (and Standard Deviations)  for APLS (k = 2), OLS, Univariate Ridge, Univariate Lasso, Multivariate Group Lasso (MGL), and Multivariate Regression with Covariance Estimation (MRCE).</figDesc><table><row><cell></cell><cell>APLS</cell><cell>OLS</cell><cell>Ridge</cell><cell>Lasso</cell><cell>MGL</cell><cell>MRCE</cell></row><row><cell cols="7">SA 0.59(±0.003) 0.92(±0.004) 0.70(±0.003) 0.77(±0.003) 0.90(±0.004) 0.60(±0.003)</cell></row><row><cell cols="7">SF 0.64(±0.003) 0.92(±0.004) 0.72(±0.003) 0.79(±0.003) 0.91(±0.004) 0.72(±0.002)</cell></row><row><cell>P</cell><cell cols="6">0.51(±0.002) 0.70(±0.003) 0.56(±0.002) 0.60(±0.002) 0.70(±0.003) 0.51(±0.002)</cell></row><row><cell cols="7">WA 0.57(±0.002) 0.84(±0.004) 0.62(±0.003) 0.74(±0.003) 0.84(±0.003) 0.68(±0.002)</cell></row><row><cell cols="7">SR 0.55(±0.002) 0.81(±0.003) 0.61(±0.002) 0.70(±0.003) 0.80(±0.003) 0.57(±0.002)</cell></row><row><cell cols="7">EF 0.53(±0.002) 0.84(±0.004) 0.70(±0.003) 0.68(±0.003) 0.81(±0.003) 0.66(±0.002)</cell></row><row><cell cols="7">WF 0.54(±0.002) 0.75(±0.003) 0.58(±0.002) 0.65(±0.002) 0.74(±0.003) 0.66(±0.002)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Neural Computation 25, 3318-3339 (2013) c 2013 Massachusetts Institute of Technology doi:10.1162/NECO_a_00524</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by projects <rs type="grantNumber">TIN2010-20900-C04-04</rs> and <rs type="person">Cajal Blue Brain</rs> of the <rs type="funder">Spanish Ministry of Science and Innovation (MINECO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NMaZGZt">
					<idno type="grant-number">TIN2010-20900-C04-04</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shoulder abduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shoulder flexion</head><p>algorithms turn out to do well. APLS is a bit smoother (less noisy) than SIMPLS. Again, the outcome for NIPALS is very similar to SIMPLS.</p><p>Table <ref type="table">1</ref> illustrates the results of APLS versus OLS, ridge regression, the Lasso, MGL, and MRCE. Interestingly, the performance of APLS is higher than the other methods for all responses. Most differences are statistically significant according to a t-test. MGL does worse than ridge regression and the Lasso. MRCE, however, is also quite competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We have proposed a Bayesian formulation of PLS, with extensions for sparsity, adaptive modeling of P, and automatic determination of k, and we empirically showed that they perform well on ECoG data decoding.</p><p>The proposed approximation relies on the Bayesian paradigm, and, hence, regularization is performed in a data-driven fashion with low risk of overfitting. An advantage is interpretability of the model: using diagonal matrices -1 = diag(σ -2 ) and -1 = diag(γ -2 ), automatic relevance determination provides a measure of the relevance of each input and latent component. If, in addition, we use the adaptive extension proposed in section 4, we can obtain a reasonable estimate of the optimal number k of latent For diagonal -1 , we have the diagonal components to be gamma distributed:</p><p>For Q, we have, in the general case, a kq-dimensional gaussian distribution,</p><p>whose parameters can be reconstructed from</p><p>, where q is defined like p and q * is the concatenation of the rows of (μ Z X ) -1 μ Z Y . If we choose to factorize F(Q), which follows from taking -1 to be diagonal, we have F(q j ) = N (q j ; μ q j , S q j ), (A.4)</p><p>With regard to -1 j , we have</p><p>) -1 and ς = ς + q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head><p>In this appendix, we derive a variational lower bound of the evidence from model 1. This can be used to monitor the inference process and check convergence. The lower bound is defined as </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffmann</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial least squares for discrimination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rayens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="166" to="173" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational algorithms for approximate Bayesian inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished doctoral dissertation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian principal components</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Keams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive multivariate ridge regression</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Zidek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="74" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse partial least squares regression for simultaneous dimension reduction and variable selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename><surname>Keles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SIMPLS: An alternative approach to partial least squares regression</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A statistical view of some chemometrics regression tools</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="109" to="135" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating image bases for visual image reconstruction from human brain activity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">A</forename><surname>Culotta</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational inference for Bayesian mixtures of factor analysers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; K.-R</forename><forename type="middle">M</forename><surname>Üller</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partial least squares yields shrinkage estimators</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goutis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The elements of statistical learning: Data mining, inference, and predictions</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reduced-rank regression for the multivariate linear model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Izenman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="248" to="264" />
		</imprint>
	</monogr>
	<note>Journal of Multivariate Analysis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tutorial on variational approximation methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced mean field methods: Theory and practice</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Saad</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The kernel algorithm for PLS</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On Bayesian PCA: Automatic dimensionality selection and analytic solution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Babacan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Conference on Machine Learning</title>
		<imprint>
			<publisher>International Machine Learning Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Support union recovery in high-dimensional multivariate regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krämer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3940</biblScope>
			<biblScope unit="page" from="34" to="51" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel partial least squares regression in reproducing kernel Hilbert space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Trejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse multivariate regression with covariance estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="947" to="962" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the decoding of intracranial data using sparse orthonormalized partial least squares</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A J</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">26017</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian CCA via group sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Conference on Machine Learning</title>
		<imprint>
			<publisher>International Machine Learning Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational Bayesian approach to canonical correlation analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="905" to="910" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A multiblock partial least squares algorithm for investigating complex chemical systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Wangen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kowalsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path models with latent variables: The NIPALS approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quantitative sociology: International perspectives on mathematical and statistical model building</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blalock</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="307" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PLS-regression: A basic tool of chemometrics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sj Östr Öm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eriksson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="109" to="130" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dimension reduction and coefficient estimation in multivariate linear regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monteiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="329" to="346" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
