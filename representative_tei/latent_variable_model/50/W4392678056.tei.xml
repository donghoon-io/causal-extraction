<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-02">2 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lezhong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeppe</forename><surname>Revall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frisvad</forename><surname>Mark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Jensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siavash</forename><forename type="middle">Arjomand</forename><surname>Bigdeli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-02">2 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.04965v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real image Model output User input Text input Model output User input A man in the street, monochrome An old man wearing VR set Image input as depth Figure 1. Our method takes one of three types of user input and generates a stereo image. Accepted user inputs: (a) a photo, (b) a text prompt, or (c) a user's image as a depth map and a prompt. We use a latent diffusion model pretrained on either images (a, b) or depth maps (c).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale language-image (LLI) models have become prominent in recent years, acclaimed for their advanced generative semantic and compositional abilities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref>. Their distinctiveness lies in their training on extensive language-image datasets, enabling them to interpret and generate content from diverse linguistic and visual contexts. Utilizing innovative image generative techniques such as auto-regressive and diffusion models <ref type="bibr" target="#b9">[10]</ref> have significantly advanced the synergy between linguistic understanding and image generation. This has led to a new era of creative and semantically rich image synthesis, marking notable advancements in artificial intelligence and computer vision.</p><p>A significant recent development in the VR/AR field is Apple's introduction of Vision Pro, which has the potential to drive rapid advancements in this field. Despite the growing production of 3D content by various manufacturers, and related research <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref> in recent years, the availability of stereo multimedia content, which offers a depth-enhanced visual experience, remains relatively scarce. As the VR/AR era looms, the limitations of existing image generation models that are confined to producing 2D images become increasingly apparent. However, there is currently no relevant research that attempts to use image generation models to directly generate stereo image pairs. In response to this challenge, we introduce a novel methodology. Through modification of the Stable Diffusion model's latent variable, we have devised an efficient end-to-end approach, eliminating the need for additional models like inpainting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> for post-processing to generate stereo images. Figure <ref type="figure">1</ref> presents some examples. Code is available at <ref type="url" target="https://github.com/lezs/StereoDiffusion">https://github.com/lezs/StereoDiffusion</ref>.</p><p>We address the constraints of traditional image generation models that employ an inpainting pipeline. Our approach is to generate stereo image pairs by adjusting the latent variable of the Stable Diffusion model, see Figure <ref type="figure" target="#fig_1">2</ref>. We use Symmetric Pixel Shift Masking Denoise and Self-Attention layer modifications to align the generated right-side image with the left-side image. This method allows for a lightweight solution that can be seamlessly integrated into the original Stable Diffusion model without the need for fine-tuning. To the best of our knowledge, our approach represents the first instance of generating stereo images by modifying the latent variable of Stable Diffusion. Compared with other methods, our approach enables the training-free end-to-end rapid generation of high-quality stereo images using only the original Stable Diffusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Latent space of a Latent Diffusion Model. Diffusion models, notably the Denoising Diffusion Implicit Models (DDIM) <ref type="bibr" target="#b31">[32]</ref>, have made significant strides in image generation. The DDIM sampling algorithm revealed that using the same initial noise results in consistent high-level features across different generative paths, highlighting initial noise as a potent latent image encoding <ref type="bibr" target="#b31">[32]</ref>. This discovery aids in modifying images by adjusting the Stable Diffusion latent variable. A key challenge in stereo image generation is maintaining content consistency between paired images. Researchers are focusing on image editing techniques using Stable Diffusion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, such as the "prompt-toprompt" method <ref type="bibr" target="#b9">[10]</ref>, which involves altering the model's cross-attention during sampling for text-prompt-based image editing. ControlNet <ref type="bibr" target="#b45">[46]</ref> is also a notable work in the field, but instead of utilizing the latent space, the authors trained ControlNet on a large dataset to better control the generation of desired images by Stable Diffusion. Additionally, ControlNet primarily focuses on pose control and lacks the capability for pixel-level modifications of images. Although effective, these methods are less suited for tasks needing precise pixel-level manipulation, like stereo image generation, due to their reliance on text prompts for image modification.</p><p>Video generation by Latent Diffusion Model. Ensuring the consistency of images within the same batch in Stable Diffusion has long been a challenge in video generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>. VideoComposer addresses this by incorporating an STC-encoder into the Latent Diffusion Model's U-Net architecture, ensuring consistency in the generated image content <ref type="bibr" target="#b36">[37]</ref>. Similarly, VideoLDM achieves impressive video generation results by introducing 3D convolution layers and temporal attention layers into the spatial and temporal layers of U-Net <ref type="bibr" target="#b1">[2]</ref>. However, these methods require fine-tuning of the original models and substantial amounts of data. Generally, this is not an issue for video generation, but for achieving stereo image generation, the available stereo image data is quite limited, mostly comprising road traffic images initially intended for autonomous driving depth prediction services. In the video generation field, attempts such as Tune-A-Video <ref type="bibr" target="#b41">[42]</ref> have explored zero-shot video generation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>. This work utilizes a technique called ST-Attn to maintain the continuity of videos. We employ a comparable approach to ensure consistency between the left and right images.</p><p>3D photography and inpainting. Traditional imagebased reconstruction and rendering methods require complex capture setups, involving numerous images with significant baselines <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref>. Currently, limited research endeavors directly focus on generating stereo images. Many studies have concentrated on generating 3D photos, a technique allowing subtle changes in the camera angle for observing photos from different perspectives <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref>. Among 3D image generation techniques, 3D Photography Inpainting is a notable approach <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>. This method employs inpainting to generate 3D images. After passing the input image through a depth estimation model, they map the image onto a mesh and apply changes in perspective based on the depth map of the original image. Inpainting is then utilized to fill the gaps left by transformed pixels in the original image. This approach significantly differs from our modification of the Stable Diffusion latent space. Although this method could be adopted as post-processing of an image generated through Stable Diffusion, it requires additional steps and consumes more time.</p><p>3D scene generation by pretrained Stable Diffusion. Recent studies have used model distillation with pretrained Stable Diffusion models for text-based 3D reconstruction. DreamFusion <ref type="bibr" target="#b22">[23]</ref> employes 'Score Distillation Sampling' (SDS) to initialize and render a NeRF model, improving with Imagen-surrounding score distillation loss. Variational Score Distillation (VSD) <ref type="bibr" target="#b37">[38]</ref> further enhances 3D scene quality. These methods can create stereo images via rendering but are time-intensive. Without full 3D scenes, our method provides a faster solution for generating stereo image pairs. </p><formula xml:id="formula_0">! " #$%" ! " ! " &amp;'()" Attention modified U-Net # # $ # % # # &amp; $ &amp; % &amp; Self-Attn Layers Attention modified U-Net Unidirectional Self-Attn ! "*+ #$%" ! "*+ &amp;'()"</formula><p>Run &amp; -" times The Disparity Map for generating stereo image pairs can be obtained from depth models such as DPT <ref type="bibr" target="#b25">[26]</ref> or MiDas <ref type="bibr" target="#b26">[27]</ref>. The pipeline only shows the Unidirectional Self-Attention operation, designed to align the right-side image with the left-side image, a method that satisfies general needs. Bidirectional Self-Attention, being a mutual operation, would be represented by bidirectional arrows in the image. The orange box in the image depicts the concept of Symmetric Pixel Shift Masking Denoise, with details explained in Sec. 3.2. The cross attention part of the sampling process is omitted for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Diverging from conventional inpainting methods, our approach is distinctively simple and training-free. With seamless integration into the original Stable Diffusion framework, we provide end-to-end generation of stereo image pairs, eliminating the need for post-processing. Our method leverages a disparity map in the early denoising stage to apply a Stereo Pixel Shift (Sec. 3.1) to the latent vector of the left image. This process generates the latent vector for the right image through disparity. To address the inconsistency issues between the left and right images during the denoising process, we employ a Symmetric Pixel Shift Masking Denoise (Sec. 3.2) technique and a Self-Attention module (Sec. 3.3) to align the right image with the left one. Since our method exclusively manipulates the latent variable, it can be applied across various image generation tasks in different Stable Diffusion models. This versatility stems from the technique's focus on latent space operations, making it adaptable to a wide range of scenarios within the Stable Diffusion framework (Sec. 3.4). Our method only requires a disparity map, which can be obtained by various depth estimation models like DPT <ref type="bibr" target="#b25">[26]</ref>, MiDas <ref type="bibr" target="#b26">[27]</ref> etc. and does not require camera calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stereo Pixels Shift</head><p>For the task of generating stereo images, fine-tuning models on large stereo datasets like KITTI <ref type="bibr" target="#b16">[17]</ref> seems intuitive. However, after fine-tuning the model using various methods such as ControlNet <ref type="bibr" target="#b3">[4]</ref> and LoRA <ref type="bibr" target="#b10">[11]</ref>, the results of the generated images remains unsatisfactory. A major flaw of this approach is that even if we could generate a high-quality stereo image pairs, the types of images generated will be limited to driving scenes similar to KITTI, losing the most important feature of Stable Diffusion: its diversity. Inspired by the Denoising Diffusion Implicit Models (DDIM) sampling technique for Stable Diffusion <ref type="bibr" target="#b31">[32]</ref>, we present a new method, Stereo Pixels Shift, without the aforementioned drawbacks. Utilizing DDIM for sampling from generalized generative processes, a latent vector sample x t-1 is generated from a sample x t via a noise predictor ϵ θ :</p><formula xml:id="formula_1">x t-1 = √ α t-1 x t - √ 1 -α t ϵ (t) θ (x t ) √ α t predicted x0 + 1 -α t-1 -σ 2 t ϵ (t) θ (x t ) direction pointing to xt + σ t ϵ t random noise ,<label>(1)</label></formula><p>where ϵ t is noise following a standard Gaussian distribution N (0, I), independent of x t , and α t controls the noise scale at step t with α 0 := 1. If we set σ t = 0 for all t and the same model ϵ θ is used, the generative results are consistent and identical, making the forward process deterministic, given x t-1 and x 0 . Thus, the result of x t-1 depends solely on x t . During the denoise process at a certain step t ′ , if we modify x t ′ to x ′ t ′ , subsequently, x ′ t ′ -1 is denoised based on x ′ t ′ , eventually generating x ′ 0 which is different from the original x 0 . This pivotal insight enables the practical application of Stable Diffusion for stereo image generation. To align with this approach, we scale down the disparity map to match the dimensions of the latent space. Subsequently, we manipulate the latent vector on a pixel-by-pixel basis, guided by the disparity map. Given the relatively small size of the latent vector, this process does not entail a substantial computational overhead.</p><p>Assuming that the two images have parallel optical axes, we derive a disparity map from a depth map using</p><formula xml:id="formula_2">D(x, y) = f B Z(x, y) ,<label>(2)</label></formula><p>where (x, y) is a point in image space, Z is the depth map, f represents the focal length, and B is the baseline distance (i.e., the distance between the two cameras). Typically, we normalize the range of the disparity map D(x, y) to be in [0, 1]. When the disparity map is generated by a model rather than being measured by actual devices, the conversion process is unnecessary, since many depth estimation models are capable of directly generating disparity maps. The Stereo Pixel Shift operation S is expressed by</p><formula xml:id="formula_3">x right (x, y) = x left (x -s D(x, y), y) ,<label>(3)</label></formula><p>where x (left or right) denotes the latent variable x t , x left (xs D(x, y), y) represents the position in the latent space that is shifted left by D(x, y) pixels relative to the position (x, y) in the latent space, and s is a scaling factor that controls the range of disparity, i.e., the pixel shift distance of the point closest to the observer in the right image relative to the left. Within reasonable limits, a larger value of s enhances the stereo effect of the generated images, usually restricted to within 10% of the image width. Excessively large s values can cause discomfort or blurriness rather than a sense of depth. However, using this method on images directly can lead to problems like flying pixels, as it causes individual pixels to warp into the empty spaces between two depth surfaces <ref type="bibr" target="#b38">[39]</ref>. But since we operate on pixels in the latent space, individual pixel issues are typically resolved in the subsequent denoising and decoding processes. Thus, our method is straightforward, requiring no additional processing such as sharpening of the moved pixels.</p><p>The reason why we can apply Stereo Pixel Shift to the latent variable is that, after a certain step, there is a spatial position correspondence between the latent variable and the generated image. According to diffusion process theory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>, sampling can be represented as</p><formula xml:id="formula_4">x t = ᾱt x 0 + (1 -ᾱt )ϵ ,<label>(4)</label></formula><p>where ϵ ∼ N (0, I) and ᾱt = t i=1 α i . Applying the Fourier transform on both sides, we have</p><formula xml:id="formula_5">F(x t ) = ᾱt F(x 0 ) + (1 -ᾱt ) F(ϵ) .<label>(5)</label></formula><p>If the step t is small, then ᾱt ≈ 1, which indicates that early-stage sampling involves low-frequency signals primarily defining the contours of the generated image. When the step t is large, ᾱt ≈ 0, high-frequency signals in the laterstage sampling refine the image details. This results in a significant disparity between the generated image and the original image if pixel offsets are applied too early during the sampling steps. Applying pixels shifts too late maintains high consistency in image content, but results in noticeable artifacts in the generated images. We found through experiments that setting t to 20% of the total denoise step usually works well. Additionally, the appropriate sampling steps for pixel offsets vary depending on the size of the objects in the images, see Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Symmetric Pixel Shift Masking Denoise</head><p>After applying the Stereo Pixel Shift, the right latent vector becomes inconsistent with the left one, potentially leading to discrepancies in the moved subject content following the denoising process. According to Eq. 1, a pixel shift applied to x t ′ to obtain x ′ t ′ results in a slight difference between x ′ t ′ -1 and x t ′ -1 . This difference accumulates during the subsequent denoising process, leading to variations in the final generated image. As a result, when the denoising algorithm is applied, it may interpret the shifted areas differently, potentially causing variations in how the subject matter appears after processing. This challenge is crucial in stereo image generation, as maintaining symmetry and coherence between the two sides is essential for creating a convincing and realistic stereo effect.</p><p>To circumvent the issue, inspired by the concept of inpainting, we propose a Symmetric Pixel Shift Masking Denoise method. We create a mask for the area where the stereo pixel shift is applied. At regular intervals, defined by specific steps t ′ , the values from the masked region of the left latent space are copied to the corresponding area of the mask in the right latent space. Consequently, the denoising process for the right image can be reformulated from Eq. 1 as</p><formula xml:id="formula_6">x ′ t ′ -1 = √ α t ′ -1 X ′ t ′ - √ 1 -α t ′ ϵ (t ′ ) θ (x ′ t ′ ) √ α t ′ + 1 -α t ′ -1 ϵ (t ′ ) θ (x ′ t ′ ) ,<label>(6)</label></formula><p>where x ′ t ′ represents the right latent vector after undergoing a pixel shift, and the ith element of X ′ t ′ is expressed by</p><formula xml:id="formula_7">X ′ t ′ ,i = S(x t ′ -1,i , D) if M i = True, x ′ t ′ ,i otherwise,<label>(7)</label></formula><p>where S represents the operation of Stereo Pixel Shift in Eq. 3, D is the corresponding disparity map of the image, and M is a Boolean matrix of the same shape as x that signifies the mask, with values set to True for the pixels that have been shifted. The variable x t ′ -1 denotes the latent vector of the left image at time step t ′ -1.</p><p>We note that if the area shifted is left blank (i.e., filled with zeros), the denoised region might become blurry. We address this blurriness by filling the shifted blank area with random noise using</p><formula xml:id="formula_8">x (deblur) t ′ ,i = x t ′ ,i if M i = False, ϵ t ′ ,i otherwise,<label>(8)</label></formula><p>where ϵ t ′ denotes random noise, M is the same mask as the one in Eq. 7. However, the effectiveness varies with different images. Sometimes, it may even lead to a decrease in the quality of the generated images. A detailed effects analysis of the Deblur technique is presented in the ablation studies described in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-Attention layers modification</head><p>As numerous studies have attempted to modify the attention mechanisms within Stable Diffusion to achieve the goal of modifying the original images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, we tackle this challenge by utilizing both Unidirectional and Bidirectional Self-Attention mechanisms. This method eliminates the need for fine-tuning the model to adjust its weights. Refer to supplementary materials for detailed explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Bi/Uni-directional Attention Modification</head><p>Require: A text condition C, a left latent variable z t-1 and a right latent variable z ′ t-1 . Ensure: An edited right latent variable z ′ * t-1 and an edited latent latent variable</p><formula xml:id="formula_9">z * t-1 if bidirection. 1: (z t-1 , z ′ t-1 ), (M t , M ′ t ) ← ϵ θ ((z t , z ′ t ), t, C); 2: M t , M ′ t ← Edit (M t , M ′ t , t) ; 3: if Unidirection then 4: (z t-1 , z ′ * t-1 ) ← ϵ θ ((z t , z ′ t ), t, C){M ′ ← M ′ t } 5: return (z t-1 , z ′ * t-1 ) 6: else if Bidirection then 7: (z * t-1 , z ′ * t-1 ) ← ϵ θ ((z t , z ′ t ), t, C){M ← M t , M ′ ← M ′ t } 8: return (z * t-1 , z ′ * t-1 ) 9: end if</formula><p>Our modified approach is listed in Algorithm 1. The term ϵ θ ((z t , z ′ t ), t, C) represents the computation of a single step t of the diffusion process, which yields the noisy image z t-1 and the attention map M t . Here, (z t , z ′ t ) denote the left and right latent variables, respectively. In our implementation, these latent variables are stacked together along the batch size dimension. However, they are presented separately here for ease of explanation. The expression ϵ θ ((z t , z ′ t ), t, C){M ′ ← M ′ t } denotes the diffusion step where the attention map M is superseded by an additional map M . We define the function Edit (M t , M ′ t , t) as a general edit function, designed to process the t-th attention maps of the left and right latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Application scenarios</head><p>As shown in Figure <ref type="figure">1</ref>, our method is compatible with various types of Stable Diffusion models, enabling it to: (a) produce the corresponding right-side image from an existing left-side image; (b) generate stereo images from text prompts; (c) produce the corresponding right-side image from an existing left-side image, where the pair shares the same composition but differs in content. For text-to-image and depth-to-image tasks, the initial noise is randomly generated. Thus, it is sufficient to apply a pixel shift to the denoised noise after a specific denoising step, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. However, for generating stereo image pairs of an existing image, it is necessary to use null-text inversion <ref type="bibr" target="#b18">[19]</ref> to obtain the latent space of the original image. A straightforward inversion technique was proposed for DDIM sampling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. This technique is grounded in the hypothesis that the ordinary differential equation (ODE) process is reversible, especially in scenarios involving small step sizes. The diffusion process is executed in reverse, meaning the transition is from z 0 to z T , contrary to the typical z T to z 0 progression:</p><formula xml:id="formula_10">z t+1 = α t+1 α t z t + 1 α t+1 -1 - 1 α t -1 ε θ (z t , t, C) . (9)</formula><p>Here, ε θ is a noise predictor including an embedding of a text condition C, while z 0 is the encoding of the provided real image. A guidance scale parameter w is used to blend between a noise predictor with no text condition (w = 0) and ε θ with C.</p><p>To address the inefficiency of mapping each noise vector to a single image, we start with a default DDIM inversion at w = 1 as the pivot trajectory. Subsequently, we optimize around this trajectory using a standard guidance ratio of w &gt; 1. In practical applications, individual optimizations are conducted for each step t during the diffusion process, aiming to closely approximate the initial trajectory z * :</p><formula xml:id="formula_11">min z * t-1 -z t-1 2 2 ,<label>(10)</label></formula><p>where z t-1 represents the intermediate result of the optimization. We thus substitute the default blank text embedding with an optimized embedding. This is because the generated results are significantly influenced by the unconditional prediction <ref type="bibr" target="#b18">[19]</ref>. The other methods are represented solely by their results on these specific images, which do not necessarily reflect the best, average, or worst SSIM scores achievable by those methods. We do this to facilitate a direct comparison of the effects of each method on the same image. We also provide LPIPS scores for reference and close-ups of the images generated by the primary benchmark methods for inspection of details.</p><p>Table <ref type="table">1</ref>. Quantitative evaluation results for Middlebury and KITTI: the results of generating right-side images from left-side images and disparity maps using different methods. We assess the similarity between the generated and the original images using PSNR, SSIM, and LPIPS. 'GT' indicates the use of ground truth disparity maps, while 'pseudo' denotes the use of disparity maps generated by a depth estimation model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have compared our results with traditional methods such as 'leave blank' and 'stretch'. Additionally, we have selected the 3D Photography techniques of Shih et al. <ref type="bibr" target="#b30">[31]</ref> for comparison, as well as the RePaint method of Lugmayr et al. <ref type="bibr" target="#b15">[16]</ref>, which involves using Stable Diffusion for inpainting images processed by the traditional 'leave blank' method.</p><p>It is important to emphasize that RePaint is not inherently designed for generating stereo image pairs. However, we believe that employing inpainting techniques to fill in the blank areas after creating stereo images is a very straightforward and common approach. Thus, we have chosen to compare with the latest model that achieves good results in various metrics within the same Stable Diffusion framework. This comparison is intended to demonstrate the innovation and advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative evaluation</head><p>Since currently no metrics exists specifically for the comparison of stereo image pair generation, we quantitatively evaluate our results using the Middlebury <ref type="bibr" target="#b29">[30]</ref> and KITTI <ref type="bibr" target="#b16">[17]</ref> datasets. We evaluate the performance by generating the right-side image from the left-side image and its disparity map. We then compare the model-generated right-side image with the ground truth image. We calculated the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) between the generated image and the ground truth. The results are in Table <ref type="table">1</ref> with some visuals in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>We provide the settings used for each method, more comparison figures, and a detailed explanation of the quantitative evaluation results in the supplemental document. In Table <ref type="table" target="#tab_2">3</ref>, we compare the time consumption of different methods for generating a single stereo image pair using an NVIDIA RTX3090 graphics card. Our method offers the capability to quickly generate high-quality stereo image pairs with direct integration into Stable Diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">User evaluations</head><p>In our user tests, we adopted a more practical and usercentric approach. The users' input were text prompts used to generate stereo image pairs using Stable Diffusion. For benchmarking, we compared this with other methods by generating the left-side images using Stable Diffusion, obtaining the corresponding disparity maps via a depth estimation model, and then using the respective methods to generate stereo image pairs. We utilized Google Cardboard and presented the stereo images on mobile phones, inviting participants to assess the image quality and correctness of the 3D perception. Ratings ranged from 0 to 5, with 5 being the highest and 0 being the lowest. Some test pictures are shown in Figure <ref type="figure" target="#fig_1">12</ref> of the supplemental document.</p><p>The results of the user tests showed that our method has the highest average but it did not significantly outperform the others. This was anticipated, as when viewing stereo images, people tend to focus more on the overall image rather than the details. In terms of ease of use, our proposed method has a clear advantage. It is simpler, does not require an additional inpainting model, and can be seamlessly integrated with Stable Diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>We conducted ablation studies on the proposed method to evaluate the impact of images guided by either Groundtruth disparity maps or Pseudo disparity maps (generated by a depth estimation model), as well as the effects of using Symmetric Pixel Shift Masking Denoise, Attention Layer Modification, and Deblur techniques on PSNR, SSIM, and LPIPS scores. The results are shown in Table <ref type="table">2</ref>. Figure <ref type="figure">5</ref> presents a visual representation of an example from the Middlebury dataset and KITTI to intuitively demonstrate the impact of each factor on the image generation outcomes, explaining the reason that scores using Groundtruth disparity maps in the Middlebury dataset are unexpectedly lower than those using Pseudo disparity maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Discussion</head><p>Our method relies on the disparity map. If the results generated by other depth estimation models are inaccurate, our method will also be unable to produce high-quality stereo images. Furthermore, when using high-precision disparity maps obtained from device measurements, the results may not be entirely satisfactory, as shown in Table <ref type="table">2</ref> and Figure <ref type="figure">5</ref>.</p><p>When using our depth to stereo image model, one may observe overlapping areas in the generated images. This issue might stem from the LatentDepth2ImageDiffusion model we used, which tends to fill blank areas with pixels from adjacent main subjects rather than background elements. In such cases, a better-quality image can be generated by first generating a single image using the Depth2Image model, and then applying our Image to Stereo Image Pairs method, as illustrated in Figure <ref type="figure" target="#fig_5">7</ref>.</p><p>We found that our method can be used for inpainting tasks with the original text prompt to a Stable Diffusion image Table <ref type="table">2</ref>. Ablation study on Middlebury and KITTI. In the 'Disparity Map' column, 'GT' and 'Pseudo' respectively indicate the use of groundtruth disparity maps or disparity maps generated by a Depth Estimation Model. In the 'Technique Applied' column, 'Attn Layer,' 'SPSMD,' and 'Deblur' represent the use of Self-Attention Layers Modification, Symmetric Pixel Shift Masking Denoise, and Deblur techniques, respectively. The symbol '✓' denotes the adoption of these respective techniques. Bold numbers represent the best scores for that column. When employing Attn Layer and SPSMD together, LPIPS has a better score, but the effect of Deblur varies from image to image. When the LPIPS scores are comparable, the higher SSIM score indicates the better similarity. An example is shown in Figure <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disparity map</head><p>Technique   model. We conducted a simple test where, after obtaining x t using null-text inversion, we applied various masking ratios to the right side and tested whether Stable Diffusion could 10% masking area 30% masking area 50% masking area fill in the blank areas within the mask during denoising. The results, as shown in Figure <ref type="figure" target="#fig_6">8</ref>, indicate that our method is somewhat effective for inpainting when a smaller area of the image is masked. However, when a larger portion of the image is masked, the inpainting results exhibit a strong patchwork appearance. Applying our method to inpainting tasks might require further modifications to both the model and the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel method for generating stereo image pairs by modifying the latent vector of Latent Stable Diffusion. We implement Stereo Pixel Shift on the left latent vector and its corresponding disparity map, and during the denoising process, we ensure consistency between the left and right images through Symmetric Pixel Shift Masking Denoise and Self-Attention Layer Modification. Our approach differs fundamentally from traditional inpainting pipelines and can be seamlessly integrated into existing Stable Diffusion models, offering end-to-end capabilities for text prompt to stereo image, depth to stereo image, and image to stereo image generation, all without the need for fine-tuning any parameters and using only the original Stable Diffusion model. Our method achieved better scores on both the KITTI and Middlebury datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Quantitative evaluation experiments setting</head><p>In this section, we will provide a detailed description of the settings for each method. 3D Photography does not provide a direct method for generating stereo image pairs, its output is a mesh, which requires rendering to obtain images. Therefore, we manually set the left and right camera matrices as follows:</p><formula xml:id="formula_12">M left =     1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1     , M right =     1 0 0 -0.04 0 1 0 0 0 0 1 0 0 0 0 1     .</formula><p>After rendering with these settings, we obtain the left and right images of the stereo image.</p><p>Given that Stable Diffusion can only generate images of 512 × 512 resolution, and the Middlebury dataset images are about 5 million pixels, we scaled both the dataset images and the corresponding depth maps to 512 × 512. For the Middlebury dataset, whose groundtruth disparity maps are noisy, we applied a Gaussian blur with a radius of 3 to smooth the disparity maps. Regarding KITTI dataset, where the image size is 375×1242 with an aspect ratio of approximately 3.3, directly scaling images to 512 × 512 could lead to excessive stretching, negatively impacting many models' performance. Therefore, we proportionally scaled the images to 512 × 1696 and then applied a center crop to 512 × 512. Because a null-text inversion technique is required, we used the Stable Diffusion version 1.5 for this test, setting the denoising steps to 50.</p><p>For the 3D photography method <ref type="bibr" target="#b30">[31]</ref>, we used the disparity map generated by the integrated MiDaS model <ref type="bibr" target="#b26">[27]</ref> within its framework instead of the groundtruth disparity map. This was due to the extensive time required-up to two hours-for mesh reconstruction of a single image using the groundtruth disparity map with 3D photography. We hypothesize that this inefficiency arises when 3D photography attempts to reconstruct stereo image pairs from the disparity map, necessitating operations like breaking up discontinuous vertices in the mesh. Such processes become computationally intensive when the groundtruth disparity map is excessively noisy, leading to a proliferation of isolated vertices that consume substantial CPU resources. For the purpose of benchmarking and considering the rarity of obtaining groundtruth disparity maps in practical scenarios, we evaluated the results using both groundtruth disparity maps (denoted as GT disparity) and pseudo disparity maps generated by depth estimation models (denoted as Pseudo disparity). The depth estimation model we employed was DPT <ref type="bibr" target="#b25">[26]</ref>. Since the use of Deblur results in lower scores, neither method employed deblur; details can be found in Sec. ??. When creating stereo images using RePaint <ref type="bibr" target="#b15">[16]</ref>, we generate a mask for the blank areas left after moving the left-side image and then perform inpainting on the masked areas. The inet256 model was utilized for this purpose, and was trained on ImageNet. Since RePaint's maximum supported output image size is 256 × 256, we downsized the images to 256 × 256 before conducting inpainting. However, considering that all other methods are evaluated at a 512 × 512 resolution, for fairness, we only upscaled the inpainted area within the mask from 256 × 256 to 512 × 512, while maintaining the original resolution for the area outside the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Analysis of Quantitative evaluation results</head><p>The use of the null-text inversion <ref type="bibr" target="#b18">[19]</ref> technique inherently causes distortion in images. On the Middlebury dataset, reference scores (for images generated by Stable Diffusion to be the same as the input) are: PSNR = 27.967, SSIM = 0.847, LPIPS = 0.046. The reference scores for the KITTI dataset are: PSNR = 25.615, SSIM = 0.762, LPIPS = 0.072. These scores represent the best possible outcomes achievable with the method we proposed. The quantitative analysis results, as seen in Table <ref type="table">1</ref>, indicate that our proposed method achieves state-of-the-art scores on both the datasets. Furthermore, as illustrated in Fig. <ref type="figure" target="#fig_7">9</ref>, we selected images representing the best LPIPS, those closest to the average LPIPS, and the worst LPIPS from each method. This selection was made to visually demonstrate the differences in images generated by each method. Fig. <ref type="figure" target="#fig_3">4</ref> showcases images with the lowest SSIM, closest to the average SSIM, and the highest SSIM scores when using our method. We also include a comparison to the outcomes when other methods are applied to the same images. We have also magnified some details to facilitate an intuitive comparison of the primary methods.</p><p>We also noted that the scores for the KITTI dataset are lower compared to those of the Middlebury dataset. However, if we convert the best scores into percentages relative to the Stable Diffusion reference scores, the results are as follows. For the Middlebury dataset, with SSIM = 0.551, it is 65.1% of the best score of 0.847, and for LPIPS = 0.173, the reference score of 0.046 constitutes 26.6% of the best score of 0.173 (the higher the percentage, the better). Similarly, for the KITTI dataset, SSIM is 63.1% of the reference score of 0.762, and the reference score for LPIPS of 0.072 is 35.1% This larger scale factor means that, when generating stereo image pairs, the corresponding pixels in the KITTI dataset images have to move a greater distance, resulting in more extensive blank areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Analysis of Ablation</head><p>Deblur has a certain negative impact on LPIPS and SSIM scores on Middlebury dataset, with a more pronounced effect on SSIM. This is because blurred images contain fewer highfrequency details, implying less noise and finer details. Since SSIM focuses more on large-scale structural features at lower frequencies, these features might appear more pronounced and consistent in blurred images, leading to higher SSIM scores. Unlike traditional metrics like SSIM or PSNR, LPIPS emphasizes perceptual differences rather than just pixel-level discrepancies, hence the lesser impact of Deblur on LPIPS scores. A lower LPIPS score with highter SSIM scores indicates closer approximation to the original image. On the KITTI dataset, the scores for Groundtruth and Pseudo disparity maps are more aligned with general expectations. Compared to the high-precision and complex Groundtruth disparity maps in the Middlebury dataset, the Groundtruth disparity maps in the KITTI dataset are relatively straightforward, mostly depicting driving scenes. Therefore, stereo images guided by Groundtruth disparity maps scored higher than those guided by Pseudo disparity maps. We believe that the positive effect of Deblur in the KITTI data set is due to the large scale factor s, which makes the larger blank area left after Pixel shift unable to be filled during denoise. It's also important to note that the LPIPS score is a better indicator of the overall similarity of images. Therefore, a higher SSIM score accompanied by a higher LPIPS score does not necessarily imply a greater similarity to the original image, as demonstrated in Fig. <ref type="figure">5</ref>. However, when the LPIPS scores are comparable, the SSIM score becomes a more effective measure for assessing the similarity of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Attention module modification details</head><p>Within the Stable Diffusion model, the denoising U-Net is structured as a series of basic blocks. Each basic block incorporates a residual block, a self-attention module, and a cross-attention module which can be represented as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>.</p><formula xml:id="formula_13">Attention(Q, K, V ) = Softmax QK ⊤ √ d V ,<label>(11)</label></formula><p>where Q represents the query, while K and V represent the key and value, respectively, and d is the output dimension of the key and query features. The values are obtained through linear projection. When there is an input context, it functions as cross-attention. In the absence of context, it operates as self-attention. Cross-attention is commonly employed in tasks involving text-guided image editing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In the case of self-attention, non-rigid editing cannot be performed as the semantic layout and structures are maintained. Similar to sharing semantic information between different samples in the same batch using 3D convolution to align content across batches in video generation tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>, applying self-attention between samples within the same batch has a comparable effect <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Querying the left-side image using the key and value of the right-side image in a unidirectional manner, enhancing the alignment from right image to left, is termed unidirectional self-attention. In contrast, employing queries from both the left and right sides to mutually query each other is referred to as bidirectional self-attention. However, bidirectional self-attention has a significant drawback: it aligns the left and right images with each other, thereby altering the input left-side image. Although this can enhance alignment, it is not a suitable option when users wish to keep the input image unchanged. Thus, despite its potential to improve alignment, the bidirectional approach may not be preferable if it is crucial to maintain the integrity of the input image. The algorithm is explained in the appendix.</p><p>We apply this attention control to all layers of the U-Net to achieve the best alignment results. Although another study observed that applying attention control to all layers results in exactly the same images <ref type="bibr" target="#b2">[3]</ref>, in our method, stereo shifts have already been applied, which leads to content consistency while the main subject is shifted to different positions, precisely the outcome we desire. In this section, we briefly present our initial attempts at fine-tuning Stable Diffusion for generating stereo image pairs. This approach was unsuccessful in producing highquality stereo image pairs. ControlNet <ref type="bibr" target="#b3">[4]</ref>, known for its capability to manipulate the posture of images generated by Stable Diffusion, produces images that are structurally similar to the input image but with different content. We hypothesized that this might be beneficial for generating stereo images. Consequently, we adopted an architecture similar to ControlNet. A neural network block F (•; Θ) with a set of parameters Θ transforms a feature map x into another feature map y. y = F (x; Θ) .</p><p>We have frozen all the parameters Θ of the original Stable Diffusion model and created a trainable copy Θ c . The neural network blocks are interconnected through a distinctive convolution layer, which is initialized with zero weights and biases. The operation can be represented by the following equation y c = F(x; Θ) + Z (F (x + Z (c; Θ z1 ) ; Θ c ) ; Θ z2 ) , <ref type="bibr" target="#b12">(13)</ref> where y c represents the output of this neural network block. The operation Z(•; •) denotes a zero convolution operation, and {Θ z1 , Θ z2 } represents two instances of parameters, each corresponding to a distinct instance of the zero convolution operation.</p><p>Using ControlNet only maintains the general content of the images, which is insufficient for generating stereo image pairs. We aim for Stable Diffusion to generate stereo image pairs concurrently. To achieve this, we align even-numbered images in the batch with their adjacent odd-numbered counterparts, such as 0 with 1, and 1 with 2, to create a stereo effect between each adjacent pair. Inspired by VideoLDM <ref type="bibr" target="#b1">[2]</ref>, we introduce a 3D convolution layer and a temporal attention layer into the Stable Diffusion architecture. These layers are added after Stable Diffusion's existing spatial layers in the U-Net. The function of 3D convolution layers is to break the information isolation between different samples in the same batch. Before feeding the intermediate features to the 3D convolution layer, we reshape the features from [b c h w] to [b/2 2 c h w], where b, c, h, w represent batch size, color channel, height, and width, respectively. The 2 in the reshaped second item represents the left and right images, allowing the newly added 3D convolution block to learn the distribution of the left and right stereo image pairs. The structure of the temporal attention layer is same as that in Stable Diffusion, assisting the 3D convolution layer in distinguishing different timesteps during the denoise process.</p><p>However, the use of ControlNet combined with 3D convolution layers is still insufficient to generate stereo image pairs. Despite a certain degree of consistency between the left and right images, the main objects within these images do not maintain a strict correspondence. For example, a car appearing in the center of the left image may appear in a considerably random position in the right image. Although the KITTI dataset is captured with the same devices and, in theory, 3D convolution blocks should be able to learn the devices' parameters and estimate the displacement of objects in the right image relative to the left, this proves to be quite challenging in practice. Hence, we introduced a disparity map as an additional condition. Our purpose was to use the disparity map of the left image as guidance to assist the 3D convolution blocks in estimating the pixel displacement in the right image. Using the disparity map as an additional condition for Stable Diffusion significantly improved the quality of the generated images, but the detail quality still did not meet our standards. Even when limiting the generation type to driving scenes, the probability of producing flawed images remained high. Therefore, we abandoned this approach. Fig. <ref type="figure">10</ref> shows the example of images generated using fine-tuned Stable Diffusion.  However, this approach may induce certain changes in the original images, which are currently uncontrollable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Ablation of Bidirectional attention and Stereo Pixel Shift</head><p>Incorporating Bidirectional Attention and applying Stereo Pixel Shift to both the left and right latent variable can alter the original image, making it unsuitable for quantitative analysis. Therefore, we only partially showcase the results of the text prompt to stereo image generation, as depicted in Fig. <ref type="figure" target="#fig_10">11</ref>. The simultaneous application of Bidirectional Attention and Stereo Pixel Shift to both left and right latent variable may induce changes in the original image. These modifications are currently uncontrollable. However, this may suggest a new potential of our approach: a method of controlling the generated images, akin to ControlNet, but without the need for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">User test images</head><p>In Fig. <ref type="figure" target="#fig_1">12</ref>, we show the example images used for our user evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The pipeline of our Stereo Diffusion. The process starts with random noise and denoising of it to generate a stereo image pair.The operation of Stereo Pixel Shift is represented by Eq. 3. The Disparity Map for generating stereo image pairs can be obtained from depth models such as DPT<ref type="bibr" target="#b25">[26]</ref> or MiDas<ref type="bibr" target="#b26">[27]</ref>. The pipeline only shows the Unidirectional Self-Attention operation, designed to align the right-side image with the left-side image, a method that satisfies general needs. Bidirectional Self-Attention, being a mutual operation, would be represented by bidirectional arrows in the image. The orange box in the image depicts the concept of Symmetric Pixel Shift Masking Denoise, with details explained in Sec. 3.2. The cross attention part of the sampling process is omitted for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Comparing the outcomes of applying stereo shifts at different steps of denoising, reveals varying optimal configurations for different images. Implementing shifts too early could result in significant content alterations, while shifts applied too late might lead to noticeable artifacts in the images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The same image generated using different methods. The rows present, respectively, the image with the lowest (worst) SSIM score, the image closest to the average SSIM score, and the image with the highest (best) SSIM score generated using our method. The other methods are represented solely by their results on these specific images, which do not necessarily reflect the best, average, or worst SSIM scores achievable by those methods. We do this to facilitate a direct comparison of the effects of each method on the same image. We also provide LPIPS scores for reference and close-ups of the images generated by the primary benchmark methods for inspection of details.</figDesc><graphic coords="6,323.24,197.35,154.00,76.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Ablation example of Middlebury (up) and KITTI (down).In the images, 'P' and 'G' respectively denote whether the image has been guided by a Pseudo disparity map or a Groundtruth disparity map. 'A', 'S', and 'D' indicate the use of Attention layers modification, Symmetric Pixel Shift Masking Denoise, and Deblur technique, respectively. The lower scores associated with the use of Groundtruth disparity maps in Middlebury may be attributed to their generally higher precision and complexity. This heightened detail can render pixel shift operations during image generation more intricate and sensitive. Our Stereo Pixel Shift operation is executed within a smaller latent space (64×64), where minor pixels, such as those around tree trunks and leaves, might be overlooked. In contrast, disparity maps generated by depth estimation models, with their lower precision, are more conducive to Pixel Shift in the latent space without sacrificing image detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Limitation: Generating compositionally similar stereo images directly from a disparity map may sometimes fail. However, this issue can be mitigated by first generating a compositionally similar left image using the disparity map, and then employing the Image to Stereo Image method to generate the right image. This two-step process helps avoid such failures.</figDesc><graphic coords="8,178.40,524.90,104.03,52.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Tests for inpainting tasks using our proposed method, the red-colored areas represent the masked regions.</figDesc><graphic coords="8,333.82,279.20,124.62,62.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparing different methods by Perceptual Image Patch Similarity (LPIPS) scores.We evaluate the right-side images generated from left-side images and disparity maps using various methods: 'Worst LPIPS', 'Average LPIPS', and 'Best LPIPS'. These represent, respectively, the images with the highest (worst) LPIPS score, the image closest to the average LPIPS score, and the image with the lowest (best) LPIPS score for each method. We also annotate each image with its Structural Similarity Index Measure (SSIM) for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>11 .Figure 10 .</head><label>1110</label><figDesc>Figure 10. Example of images generated by stereo fine-tuned Stable Diffusion: The images reveals that while the generated left and right images exhibit certain similarities, the extent of this resemblance falls significantly short of the requirements for stereo imaging. Even during training, maintaining pixel-level consistency between the left and right images proves challenging, and the quality of images generated during tests exhibits notable deficiencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Unidirectional attention + only shift right latent variable Unidirectional attention + shift both latent variable Bidirectional attention + only shift right latent variable Bidirectional attention + shift both latent variable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Ablation of Bidirectional attention and Stereo Pixel Shift:The implementation of Bidirectional attention and the simultaneous application of Stereo Pixel Shift to the left and right latent variables theoretically enhances the consistency between the two images. However, this approach may induce certain changes in the original images, which are currently uncontrollable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>T2SI</cell><cell>D2SI</cell><cell>I2SI</cell></row><row><cell cols="3">3D Photography [31] 245 (231) 247 (231)</cell><cell>231</cell></row><row><cell>Repaint[16]</cell><cell cols="2">338 (324) 340 (324)</cell><cell>324</cell></row><row><cell>Ours</cell><cell>32 (18)</cell><cell>18</cell><cell>40 (17)</cell></row><row><cell>Depth to Stereo</cell><cell></cell><cell></cell><cell></cell></row><row><cell>image model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>"An astronaut</cell><cell></cell><cell></cell><cell></cell></row><row><cell>is riding horse"</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Left</cell><cell>Right</cell><cell></cell></row><row><cell>Depth to</cell><cell></cell><cell>Image to Stereo</cell><cell></cell></row><row><cell>image model</cell><cell></cell><cell>image model</cell><cell></cell></row><row><cell>"An astronaut</cell><cell></cell><cell></cell><cell></cell></row><row><cell>is riding horse"</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Left</cell><cell></cell><cell>Right</cell></row></table><note><p><p><p>Time cost for different methods in seconds. We measure the total time consumed for each usage scenario, including the time taken to generate the images using Stable Diffusion. The scenarios are text to stereo image (T2SI), depth to stereo image (D2SI), and image to stereo image (I2SI). The time in parenthesis is the cost excluding the time spent on generating images with Stable Diffusion. For D2SI, our method, being directly integrated into Stable Diffusion, requires only a single pass of sampling to generate stereo image pairs. In I2SI, our method requires the use of null-text inversion</p><ref type="bibr" target="#b18">[19]</ref> </p>for xt, resulting in an extra 23 seconds.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Align your latents: high-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MasaCtrl: tuning-free mutual self-attention control for consistent image aynthesis and editing</title>
		<author>
			<persName><forename type="first">Mingdeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Control-a-video: controllable text-to-video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13840</idno>
		<imprint/>
	</monogr>
	<note>cs.CV], 2023. 3, 2</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CogView2: Faster and better text-to-image generation via hierarchical transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16890" to="16902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AnimateDiff: animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Casual 3D photography</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhib</forename><surname>Alsisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="234" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep blending for free-viewpoint image-based rendering</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">True</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="257" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prompt-to-prompt image editing with cross-attention control</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005">2023. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>2022. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Worldsheet: Wrapping the world in a 3D sheet for view synthesis from a single image</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12528" to="12537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-based rendering in the gradient domain</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="199" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized deep 3D shape prior via part-discretized diffusion process</title>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishun</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="16784" to="16794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-2-3-45: any single image to 3D mesh in 45 seconds without per-shape optimization</title>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haian</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22226" to="22246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RePaint: inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local light field fusion: practical view synthesis with prescriptive sampling guidelines</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Null-text inversion for editing real images using guided diffusion models</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research (PMLR)</title>
		<meeting>Machine Learning Research (PMLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research (PMLR)</title>
		<meeting>Machine Learning Research (PMLR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16784" to="16804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero-shot image-to-image translation</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2023 Conference Proceedings</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DreamFusion: text-to-3D using 2D diffusion</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion autoencoders: toward a meaningful and decodable representation</title>
		<author>
			<persName><forename type="first">Konpat</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nattanat</forename><surname>Chatthee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Suttisak Wizadwongsa, and Supasorn Suwajanakorn</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with CLIP latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125[cs.CV],2022.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Confer-ence on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pix-elSynth: generating a 3D-consistent experience from a single image</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14104" to="14113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="36479" to="36494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Highresolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Daniel Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">York</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nera</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Porter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">photography using context-aware layered depth inpainting</title>
		<author>
			<persName><forename type="first">Meng-Li</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Yang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2020. 2, 6</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005">2021. 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Plug-and-play diffusion features for text-driven image-toimage translation</title>
		<author>
			<persName><forename type="first">Narek</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1921" to="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zero-shot video editing using off-the-shelf image diffusion models</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17599</idno>
		<imprint/>
	</monogr>
	<note>cs.CV], 2023. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VideoComposer: compositional video synthesis with motion controllability</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7594" to="7611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ProlificDreamer: high-fidelity and diverse text-to-3D generation with variational score distillation</title>
		<author>
			<persName><forename type="first">Zhengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8406" to="8441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning stereo from single images</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="722" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reconstructing scenes with mirror and glass surfaces</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Butterfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="102" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiview compressive coding for 3D reconstruction</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9065" to="9075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tune-a-video: one-shot tuning of image diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">Jay Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">AniPortraitGAN: animatable 3D portrait generation from 2D image collections</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2023 Conference Proceedings</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2023" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Diffusion models: a comprehensive survey of methods and applications</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenda</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scaling autoregressive models for content-rich text-to-image generation</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stereo magnification: learning view synthesis using multiplane images</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="65" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
