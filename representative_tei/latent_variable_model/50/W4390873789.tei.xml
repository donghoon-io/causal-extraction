<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EGC: Image Generation and Classification via a Diffusion Energy-Based Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-04-13">13 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiushan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuofan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<country>Kong</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EGC: Image Generation and Classification via a Diffusion Energy-Based Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-13">13 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.02012v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution p(y|x)), the forward pass in EGC is a classifier that outputs a joint distribution p(x, y), enabling an image generator in its backward pass by marginalizing out the label y. This is done by estimating the classification probability given a noisy image from the diffusion process in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achieving superior classification accuracy and robustness against adversarial attacks on CIFAR-10. This work represents the first successful attempt to simultaneously excel in both tasks using a single set of network parameters. We believe that EGC bridges the gap between discriminative and generative learning. Code will be released at <ref type="url" target="https://github.com/GuoQiushan/EGC">https://github.com/GuoQiushan/EGC</ref>. The scatters plots on the vertical N/A line represent the image generation models, which are not available for classification. The scatter plot on the horizontal N/A line represents the classification model, which is not available for image generation. Remarkably, EGC achieves superior performance in both tasks with a single neural network, demonstrating its effectiveness in bridging the gap between discriminative and generative learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image classification and generation are two fundamental tasks in computer vision that have seen significant advancements with the development of deep learning models. However, many state-of-the-art approaches that perform well in one task often exhibit poor performance in the other or are not suitable for the other task. Both tasks can be formulated from a probabilistic perspective, where image classification task is interpreted as a conditional probability distribution p(y|x), and image generation task is the transformation of a known and easy-to-sample probability distribution p(z) to a target distribution p(x).</p><p>As an appealing class of probabilistic models, Energy-Based Models (EBM) <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b58">60]</ref> can explicitly model complex probability distribution and be trained in an unsupervised manner. Furthermore, standard image classification models and some image generation models can be reinterpreted as an energybased model <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34]</ref>. From the perspective of EBM, a standard image classification model can be repurposed as an image generation model by leveraging the gradient of its inputs to guide the generation of new images. Despite the desirable properties, EBMs face challenges in training due to the intractability of computing the exact likelihood and synthesizing exact samples from these models. Arbitrary energy models often exhibit sharp changes in gradients, leading to unstable sampling with Langevin dynamics. To ameliorate this issue, spectral normalization <ref type="bibr" target="#b35">[37]</ref> is typically adopted for constraining the Lipschitz constant of the energy model <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b14">16]</ref>. Even with this regularization technique, the samples generated by the energy-based model are still not competitive enough because the probability distribution of real data is usually sharp in the highdimensional space, providing inaccurate guidance for image sampling in the low data density regions.</p><p>Diffusion models <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b45">47]</ref> have demonstrated competitive and even superior image generation performance compared to GAN <ref type="bibr" target="#b11">[13]</ref> models. In diffusion models, images are perturbed with Gaussian noise through a diffusion process for training, and the reverse process is learned to transform the Gaussian distribution back to the data distribution. As pointed out in <ref type="bibr" target="#b44">[46]</ref>, perturbing data points with noise populates low data density regions to improve the accuracy of estimated scores, resulting in stable training and image sampling.</p><p>Motivated by the flexibility of EBM and the stability of diffusion model, we propose a novel energy-based classifier and generator, namely EGC, which achieves superior performance in both image classification and generation tasks using a single neural network. EGC is a classifier in the forward pass and an image generator in the backward pass. Unlike a conventional classifier that predicts the condition distribution p(y|x) of the label given an im-age, the forward pass in EGC models the joint distribution p(x, y) of the noisy image and label, given the source image. By marginalizing out the label y, the gradient of logprobability of the noisy image (i.e., unconditional score) is used to restore image from noise. The classification probability p(y|x) provides classifier guidance together with unconditional score within one step backward pass.</p><p>We demonstrate the efficacy of EGC model on Im-ageNet, CIFAR-10, CIFAR-100, CelebA-HQ and LSUN datasets. The generated samples are of high fidelity and comparable to GAN-based methods, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Additionally, our model shows superior classification accuracy and robustness against adversarial attacks. On CIFAR-10, EGC surpasses existing methods of learning explicit EBMs with an FID of 3.30 and an inception score of 9.43 while achieving a remarkable classification accuracy of 95.9%. This result even exceeds the classification performance of the discriminative model Wide ResNet-28-12, which shares a comparable architecture and number of parameters with our model. On ImageNet-1k, EGC achieves an FID of 6.05 and an accuracy of 78.9%, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. We also demonstrate that naively optimizing the gradients of explicit energy functions as the score functions outperforms optimizing the probability density function via Langevin sampling. Besides, EGC model does not require constraining the Lipschitz constant as in the previous methods <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b14">16]</ref> by removing the normalization layers and inserting spectral normalization layer. More interestingly, we demonstrate that the neural network effectively models the target data distribution even though we adopt optimization of the Fisher divergence instead of the probability p θ (x t ).</p><p>Our contributions are listed as follows:</p><p>(1) We propose a novel energy-based model, EGC, bridging the gap between discriminative and generative learning. In EGC, the forward pass is a classification model that predicts the joint distribution p(x, y) and the backward pass is a generation model that denoises data using the score function and conditional guidance. (2) Our EGC model achieves competitive generation results to state-ofthe-art approaches, while obtaining superior classification results using a single neural network. EGC surpasses existing methods of explicit EBMs by a significant margin. <ref type="bibr" target="#b1">(3)</ref> We demonstrate that EGC model can be applied in inpainting, semantic interpolation, high-resolution image generation (∼ 1024 2 ) and robustness improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Energy-based Models. Unlike most other probabilistic models, Energy-Based Models do not place a restriction on the tractability of the normalizing constant, which confers upon them greater flexibility in modeling complex probability distributions. However, the intractability of the normalization constant renders their training challenging. BiDVL [22] proposes a bi-level optimization framework to facilitate learning of energy-based latent variable models. Yin et al. <ref type="bibr" target="#b55">[57]</ref> explore adversarial training for learning EBMs. CLEL <ref type="bibr" target="#b30">[32]</ref> improves the training of EBMs using contrastive representation learning, which guides EBMs to better understand the data structure for faster and more memoryefficient training. EGSDE <ref type="bibr" target="#b59">[61]</ref> proposes energy-guided stochastic differential equations to guide the inference process of a pretrained SDE for realistic and faithful unpaired image-to-image translation. In contrast to the above methods, we reinterpret the standard classification network as an EBM to approximate the joint distribution of the noisy samples in diffusion process. Our method enables the forward pass to work as a classification model and the backward pass to provide both unconditional and conditional scores within one step.</p><p>Denoising Diffusion Models (DDMs), originating from <ref type="bibr" target="#b42">[44]</ref>, are to learn from noisy data and generate samples by reversing the diffusion process. Score based generative model <ref type="bibr" target="#b44">[46]</ref> is introduced to train denoising models with multiple noise levels and draw samples via Langevin dynamics during inference. Several works have improved the design and demonstrated the capability of synthesizing high-quality images <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b18">20]</ref>. DDPM <ref type="bibr" target="#b17">[19]</ref> generates high-quality images synthesis results using diffusion probabilistic models. Guided-Diffusion <ref type="bibr" target="#b2">[4]</ref> firstly achieved better performance than GAN by utilizing the classifier guidance. Latent Diffusion <ref type="bibr" target="#b40">[42]</ref> further proposes latent diffusion mod-els which operate on a compressed latent space of reduced dimensionality to save computational cost. DDMs circumvent the issue of intractable normalizing constants in EBMs by modeling the score function instead of the density function. Different from the score-based diffusion models, our EGC explicitly models the probability distribution, and both forward and backward passes are meaningful.</p><p>Unified Classification and Generation Models. Xie et al. <ref type="bibr" target="#b52">[54]</ref> first draw the connection between the discriminative and generative power of a ConveNet with EBMs. Based on the idea, Grathwohl et al. <ref type="bibr" target="#b14">[16]</ref> propose to re-interpret a discriminative classifier as an EBM modeling joint distribution of x and y, which enables image synthesis and classification within one framework. Introspective Neural Networks <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33]</ref> share a similar insight to imbue the classifier with generative power, leading to increased robustness in adversarial attacks. In contrast to the above methods, our method adopts the diffusion process to improve the accuracy of estimated scores for stable training and image sampling. Consequently, the regularization and training tricks required by other models are not necessary for our EGC model. EGC significantly outperforms the hybrid model, JEM <ref type="bibr" target="#b14">[16]</ref>, by a substantial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, the EGC model consists of a classifier that models the joint distribution p(x, y) by estimating the energy during the forward pass. The conditional probability p(y|x) is produced by Softmax function. The backward pass of the network produces both the unconditional score (∇ x log p(x)) and the class guidance (∇ x log p(y|x)) in a single step by marginalizing out y. We adopt the diffusion process to populate low data density regions and optimize the unconditional score using Fisher divergence, which circumvents the direct optimization of normalized probability density.</p><p>Background. An energy-based model <ref type="bibr" target="#b29">[31]</ref> is defined as</p><formula xml:id="formula_0">p θ (x) = exp(-E θ (x)) Z(θ) ,<label>(1)</label></formula><p>to approximate any probability density function p data (x) for x ∈ R D , where E θ (x) : R D → R, known as the energy function, maps each data point to a scalar, and</p><formula xml:id="formula_1">Z(θ) = exp(-E θ (x)</formula><p>)dx, analytically intractable for high-dimensional x, is the partition function. Typically, one can parameterize the energy function with a neural network f θ (x) = -E θ (x). The de facto standard for learning probabilistic models from i.i.d data is maximum likelihood estimation. The log-likelihood function is</p><formula xml:id="formula_2">L(θ) = E x∼p data (x) [log p θ (x)] 1 N N i=1 log p θ (x i ),<label>(2)</label></formula><p>where we observe N samples x i ∼ p data (x). The gradient of the log-probability of an EBM is composed of two terms:</p><formula xml:id="formula_3">∂log p θ (x) ∂θ = ∂f θ (x) ∂θ -E x ∼p θ (x ) [ ∂f θ (x ) ∂θ ],<label>(3)</label></formula><p>The second term can be approximated by drawing the synthesized samples from the model distribution p θ (x ) with Markov Chain Monte Carlo (MCMC). Langevin MCMC first draws an initial sample from a simple prior distribution and iteratively updates the sample until a mode is reached, which can be formalized as follows:</p><formula xml:id="formula_4">x i+1 ← x i + c∇ x log p(x i ) + √ 2c i ,<label>(4)</label></formula><p>where x 0 is randomly sampled from a prior distribution (such as Gaussian distribution), and i ∼ N (0, I). However, for high-dimensional distributions, it takes a long time to run MCMC to generate a converged sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Energy-Based Model with Diffusion Process</head><p>Diffusion models gradually inject noise into the source samples during the nosing process, which can be formulated as a Markov chain:</p><formula xml:id="formula_5">q(x 1:T |x 0 ) = T t=1 q(x t |x t-1 ), q(x t |x t-1 ) = N (x t ; √ α t x t-1 , β t I),<label>(5)</label></formula><p>where x 0 represents the source samples and α t = 1 -β t .</p><p>For an arbitrary timestep t, one can directly sample from the following Gaussian distribution without iterative sampling,</p><formula xml:id="formula_6">q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1 -ᾱt )I)<label>(6)</label></formula><p>Based on Bayes theorem, one can reverse the noising process by the posterior q(x t-1 |x t , x 0 ):</p><formula xml:id="formula_7">βt = 1 -ᾱt-1 1 -ᾱt β t μ(x t , x 0 ) = √ ᾱt-1 β t 1 -ᾱt x 0 + √ α t (1 -ᾱt-1 ) 1 -ᾱt x t q(x t-1 |x t , x 0 ) = N (x t-1 ; μ(x t , x 0 ), βt I)<label>(7)</label></formula><p>x 0 is unknown during the denoising process, so we approximate the posterior with p θ (x t-1 |x t ) = q(x t-1 |x t , x 0 = µ θ (x t )) to denoise the observed sample x t .</p><p>According to Tweedie's Formula <ref type="bibr" target="#b5">[7]</ref>, one can estimate the mean of a Gaussian distribution, given a random variable z ∼ N (z; µ z , Σ z ):</p><formula xml:id="formula_8">E[µ z |z] = z + Σ z ∇ z log p(z)<label>(8)</label></formula><p>By applying Tweedie's Formula to Equation 6, the estimate for the mean of the noised sample x t can be represented as:</p><formula xml:id="formula_9">√ ᾱt x 0 = x t + (1 -ᾱt )∇ xt log q(x t |x 0 )<label>(9)</label></formula><p>Since the noised sample x t decomposes as a sum of two terms: x t = √ ᾱt x 0 + √ 1 -ᾱt t , the score function ∇ xt log q(x t |x 0 ) can be expressed as:</p><formula xml:id="formula_10">∇ xt log q(x t |x 0 ) = - t √ 1 -ᾱt<label>(10)</label></formula><p>We approximate the probability density function q(x t |x 0 ) by an Energy-based model p θ (x t ), and optimize the parameters by minimizing the Fisher divergence between q(x t |x 0 ) and p θ (x t ):</p><formula xml:id="formula_11">D F = E q [ 1 2 ∇ xt log q(x t |x 0 ) -∇ xt log p θ (x t ) 2 ] (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>For Energy-based models, the score can be easily obtained,</p><formula xml:id="formula_13">∇ x log p θ (x) = ∇ x f θ (x).</formula><p>Compared with directly optimizing the log-probability of EBM (Equation <ref type="formula" target="#formula_3">3</ref>), Fisher divergence circumvents optimizing the normalized densities parameterized by Z(θ) and the target score can be directly sampled from a Gaussian distribution via Equation <ref type="formula" target="#formula_10">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">EGC</head><p>The energy-based model has an inherent connection with discriminative models. For the classification problem with C classes, a discriminative neural classifier maps a data sample x ∈ R D to a vector of length C known as logits.</p><p>The probability of y-th label is represented using the Softmax function:</p><formula xml:id="formula_14">p(y|x) = exp(f (x)[y]) y exp(f (x)[y ]) ,<label>(12)</label></formula><p>where f (x)[y] is the y-th logit. Using Bayes theorem, the discriminative conditional probability can be expressed as p(y|x) = p(x,y) y p(x,y ) . By connecting Equation 1 and 12, the joint probability of data sample x and label y can be modeled as:</p><formula xml:id="formula_15">p θ (x, y) = exp(f θ (x)[y]) Z(θ)<label>(13)</label></formula><p>By marginalizing out y, the score of data sample x is obtained as:</p><formula xml:id="formula_16">∇ x log p θ (x) = ∇ x log y exp(f θ (x)[y])<label>(14)</label></formula><p>Different from the typical classifiers, the free energy function E θ (x) = -log y exp(f θ (x)[y]) is also optimized for generating samples. We propose to integrate the energy-based classifier with the diffusion process to achieve both strong discriminative performance and generative performance. Specifically, we approximate the conditional probability density function q(x t , y|x 0 ) with an energy-based classifier p θ (x t , y). Due to the optimization of Fisher divergence for our EBM, we factorize the log-likehood as:</p><formula xml:id="formula_17">log p θ (x t , y) = log p θ (x t ) + log p θ (y|x t )<label>(15)</label></formula><p>The score ∇ xt log p θ (x t ) is optimized by minimizing the Fisher divergence as shown in the Equation 11 and 14. As for the conditional probability p θ (y|x t ), we simply adopt the standard cross-entropy loss to optimize. One of the advantage of integrating the energy-based classifier with diffusion process is that the classifier provides guidance to explicitly control the data we generate through conditioning information y. By Bayes theorem, the conditional score can be derived as:</p><formula xml:id="formula_18">∇log p θ (x t |y) = ∇log p θ (x t ) + ∇log p θ (y|x t ) (16)</formula><p>The joint probability p θ (x t , y) is parameterized with a neural network. And the forward propagation of our EGC model is a discrimination model to predict the conditional probability p θ (y|x), while the backward propagation of the neural network is a generation model to predict the score and classifier guidance to gradually denoise data.</p><p>Overall, the training loss of an EGC model is formulated as:</p><formula xml:id="formula_19">L = E q [ 1 2 ∇ xt log q(x t |x 0 ) -∇ xt log p θ (x t ) 2 - C i=1 q(y i |x t , x 0 ) log p θ (y i |x t )],<label>(17)</label></formula><p>where the first term is reconstruction loss for a noised sample, the second term is a classification loss that encourages the denoising process to generate samples that are consistent with the given labels. The training procedure is summarized in Algorithm 1, where we adopt the noise as the target score to ensure the stable optimization of the neural network. Additionally, a hyperparameter γ is introduced to balance the two loss terms.</p><p>Algorithm 1 Training repeat Sample t ∼ Unif({1, ..., T }) Sample data pair (x 0 , y), Sample noise ∼ N (0, I)</p><formula xml:id="formula_20">x t = √ ᾱt x 0 + √ 1 -ᾱt Take gradient descent step on ∇ θ ( ∇ xt log p θ (x t ) + 2 -γ C i=1 q(y i |x t ) log p θ (y i |x t )) until converged.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct a series of experiments to evaluate the performance of EGC model on image classification and generation benchmarks. The results show that our model achieves performance rivaling the state of the art in both discriminative and generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>For conditional learning, we consider CIFAR-10 [28], CIFAR-100 and ImageNet <ref type="bibr" target="#b1">[3]</ref> dataset to evaluate the proposed EGC model. Both CIFAR-10 and CIFAR-100 contain 50K training images. The ImageNet training set is composed of about 1.28 million images from 1000 different categories. For unconditional learning, we train unsupervised EGC models on CelebA-HQ <ref type="bibr" target="#b21">[23]</ref>, which contains 30K training images of human faces, and LSUN Church <ref type="bibr" target="#b56">[58]</ref>, which contains about 125K images of outdoor churches. For ImageNet-1k, CelebA-HQ and LSUN Church, we follow latent diffusion models (LDM) <ref type="bibr" target="#b40">[42]</ref> to convert images at 256×256 resolution to latent representations at 32×32 or 64×64 resolutions, using the pre-trained image autoencoder provided by LDM <ref type="bibr" target="#b40">[42]</ref>. We adopt the same UNet architecture as <ref type="bibr" target="#b2">[4]</ref> and attach an attention pooling module to it, like CLIP <ref type="bibr" target="#b39">[41]</ref>, to predict logits. More training details can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hybrid Modeling</head><p>EGC model. We first train an EGC model on CIFAR-10 dataset. The accuracy on CIFAR-10 validation dataset, Inception Score (IS) <ref type="bibr" target="#b41">[43]</ref> and Frechet Inception Distance (FID) <ref type="bibr" target="#b15">[17]</ref> are reported to quantify the performance of our model. Our model demonstrates a remarkable accuracy of 95.9% on the CIFAR-10 validation dataset, surpassing the performance of the discriminative model Wide ResNet-28-12, which has a comparable architecture and number of parameters. Furthermore, the sampling quality of our model outperforms a majority of existing Explicit EBM, GAN models and Score-Based models, with IS of 9.43 and FID of 3.30. These results showcase the potential of our proposed model to enhance image classification performance and generative capability of EBM. On CIFAR-100, our proposed EGC model achieves comparable generative results with state-of-the-art GAN-based methods, while outperforming the hybrid model JEM <ref type="bibr" target="#b14">[16]</ref> in terms of classification accuracy, as shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We conducted additional experiments on the more challenging dataset, ImageNet. The results, presented in Table 3, provide evidence that our proposed EGC model per-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc(%) IS(↑) FID(↓) Ours 77.9 11.50 4.88 FQ-GAN <ref type="bibr" target="#b60">[62]</ref> N/A 7.15 9.74 LeCAM (BigGAN) <ref type="bibr" target="#b46">[48]</ref> N/A -11.2 StyleGAN2 + DA <ref type="bibr" target="#b23">[25]</ref> N/A -15.22 JEM <ref type="bibr" target="#b14">[16]</ref> 72.2 --Wide ResNet-28-12 <ref type="bibr" target="#b57">[59]</ref> 79.5 N/A N/A forms well on this dataset, achieving IS of 189.5 FID of 6.77 and Top-1 Accuracy of 72.5%. The model was trained using only random flip as data augmentation. We recognize that incorporating stronger augmentation techniques could lead to even better results. By merely incorporating the RandResizeCrop data augmentation, we achieve a significant increase of 78.9% in accuracy on the ImageNet dataset.</p><p>Unsupervised EGC model. EGC model can be trained in an unsupervised manner without knowing the label of image. By marginalizing out the variable y in Equation <ref type="formula" target="#formula_16">14</ref>, the score function can be optimized by minimizing the Fisher divergence. As shown in Table <ref type="table" target="#tab_3">4</ref>, Unsupervised EGC model achieves FID of 7.75 on CelebA-HQ 256×256 and FID of 8.97 on LSUN Church 256×256. These results demonstrate that Unsupervised EGC model outperforms the other state-of-the-art Energy-Based models. Although the scorebased diffusion model, DDPM, exhibits slightly better performance, it is noteworthy that optimizing the gradient of the neural network for Unsupervised EGC model is a more challenging task. We believe that a network architecture specifically designed for optimizing the gradient will lead CelebA-HQ 256×256 FID(↓) Ours 7.75 ATEBM <ref type="bibr" target="#b55">[57]</ref> 17.31 VAEBM <ref type="bibr" target="#b51">[53]</ref> 20.38 CF-EBM <ref type="bibr" target="#b61">[63]</ref> (128×128) 23.50 NVAE <ref type="bibr" target="#b47">[49]</ref> 45.11 Glow <ref type="bibr" target="#b25">[27]</ref> 68.93 ProgressiveGAN <ref type="bibr" target="#b21">[23]</ref> 8.03</p><p>LSUN Church 256×256 FID(↓) Ours 8.97 VAEBM <ref type="bibr" target="#b51">[53]</ref> (64×64) 13.51 ATEBM <ref type="bibr" target="#b55">[57]</ref> 14.87 DDPM <ref type="bibr" target="#b17">[19]</ref> 7.89  to the same or even better results than DDPM.</p><p>Ablation study. The results in Table <ref type="table" target="#tab_4">5</ref> demonstrate the effectiveness of the proposed EGC framework for image synthesis and classification. Unsupervised EGC model serves as a good baseline, achieving FID of 5.36. The EGC model achieves 95.9% accuracy on the test set and an additional 1.87 FID improvement, demonstrating the success of learning the joint probability. Moreover, we make use of class labels for conditional image synthesis. The classifier guidance ∇log p θ (y|x t ) guides the denoise process towards the class label y, resulting in a 0.19 FID improvement. To investigate the effect of neural network architecture, we train a model based on the standard feedforward ResNet commonly used for image classification. The comparison of the last and penultimate lines in Table <ref type="table" target="#tab_4">5</ref> reveals that the U-Net architecture benefits from the short-cut connections specifically designed to propagate fine details from the inputs x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Application and Analysis</head><p>Interpolation. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, our model is capable of producing smooth interpolation between two generated samples. Following DDIM <ref type="bibr" target="#b43">[45]</ref>, we perform an interpolation between the initial white noise samples x T . We compare the generated samples from the interpolated noise with the samples obtained through interpolation in the generated samples x 0 . The results demonstrate that our method  Image inpainting. A promising application of energybased models is to use the learned prior model for filling masked regions of an image with new content. Following <ref type="bibr" target="#b33">[35]</ref>, we obtain a sequence of masked noisey image at different timesteps and fill the masked pixels with the denoised sample given the previous iteration. The qualitative results on CelebA-HQ 256×256 are presented in Figure <ref type="figure" target="#fig_4">5</ref>, demonstrating that our model is capable of realistic and semantically meaningful image inpainting.</p><p>Robustness. Adversarial attacks are a common threat to neural networks, especially when the model weights are accessible. A common white-box attack is FGSM, perturbing the inputs with the gradients enlarging the loss. We investigate the robustness of models trained on the CIFAR-10 dataset using FGSM <ref type="bibr" target="#b12">[14]</ref> and PGD <ref type="bibr" target="#b34">[36]</ref> attacks under the L ∞ constraint. The accuracy curve in Figure <ref type="figure" target="#fig_6">7</ref> demonstrates that the EGC model outperforms the standard Wide ResNet-28-12 classifier and JEM <ref type="bibr" target="#b14">[16]</ref> in terms of adversarial robustness. The results suggest that leveraging the joint probability distribution learned by energy-based models can enhance the model's robustness against adversarial attacks.</p><p>Visualize Energy. As detailed in Section 3.2, we adopt a direct optimization of the Fisher divergence instead of the probability p θ (x t ). Therefore, we are interested to see  whether the neural network would effectively model the target Gaussian distribution. Given the difficulty in illustrating a high-dimensional Gaussian distribution, we present that the unnormalized probability density of noised sample x t in Figure <ref type="figure" target="#fig_5">6a</ref>. Notably, the density exhibits a similar shape to the folded normal distribution, suggesting that the probability distribution learned by the neural network closely approximates the Gaussian distribution. In Figure <ref type="figure" target="#fig_5">6b</ref>, we select two orthogonal noises to plot the two-dimensional probability density function, which exhibits a similar shape to the Gaussian distribution.</p><p>Conditional sampling. As illustrated in Section 3.2, the classifier provides guidance to explicitly control the data we generate through conditioning information y. We feed the network with a fixed class label and random noise to check the qualitative results. As shown in Fig. <ref type="figure" target="#fig_8">8</ref>, the diversity is promised by the random noise and the semantic consistency is guaranteed by the classification guidance. We further in-  vestigate the relationship between guidance scale with sample diversity. By increasing the guidance scale from 2 to 200, the diversity decreases significantly. An interesting observation is that the mode finally collapse to the class prototype. Meanwhile, the generated image with a high guidance scale are of high fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce a novel energy-based model, EGC, to bridge the gap between discrimative and generative learning. We formulate the image classification and generation tasks from a probabilistic perspective, and find that the energy-based model is suitable for both tasks. To alleviate the difficult of optimizing the normalized probability density of energy-based model, we introduce diffusion process to populate the low data density regions for better estimated score, and score matching to circumvent optimizing the object loss with the normalizing constant Z(θ). In EGC, the forward pass models the joint distribution of the noisy image and label, while the backward pass of EGC calculates both conditional and unconditional scores to encourage the denoising process to generate samples consistent with the given labels. We achieve high-quality image synthesis and competitive image classification accuracy using a single neural network. We believe that EGC can serve as a new baseline for unifying discriminative and generative models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generated samples from our EGC models on CelebA-HQ 1024×1024, LSUN Church 256×256, CelebA-HQ 256×256 and ImageNet 256×256 datasets, shown from left to right.</figDesc><graphic coords="1,50.11,218.76,495.00,123.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: FID and classification accuracy on ImageNet 256×256 dataset. The scatters plots on the vertical N/A line represent the image generation models, which are not available for classification. The scatter plot on the horizontal N/A line represents the classification model, which is not available for image generation. Remarkably, EGC achieves superior performance in both tasks with a single neural network, demonstrating its effectiveness in bridging the gap between discriminative and generative learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (A) Diffusion Model estimates the score (noise) from the noisy scaled image. (B) Standard Classification Model outputs the logits for minimizing the cross-entropy loss. (C) GAN Model is composed of a generator model that synthesizes new samples and a discriminator that classifies samples as either real or fake. (D) EGC Model estimates the joint distribution p(x, y) for classification via the forward propagation of a neural network and leverages the score estimated from the backward propagation to generate samples from Gaussian noise. Z represents the normalizing constant, which is only relevant to the model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interpolation results between the leftmost and rightmost generated samples. x T denotes the interpolation the noise samples. x 0 means the interpolation on the generated samples. The results demonstrate that our proposed method exhibits superior semantic interpolation effects compared to direct interpolation of generated samples in the latent space.</figDesc><graphic coords="7,308.86,255.06,236.25,78.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Inpainting results on CelebA-HQ dataset with resolution of 256×256. The top row shows the mask images, while the bottom row displays the corresponding inpainted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) The unnormalized probability density. The x-axis represents the noise level of x t , which is the mean of the abstract value of the noise at each timestep. The probability density exhibits a similar shape to the folded normal distribution. (b) The density function of the noised samples at t = 500. The noise is produced by linearly combining two orthogonal noises. The probability density exhibits a similar shape to the Gaussian distribution. (c) The density function of the noised samples at t = 200.</figDesc><graphic coords="8,211.36,72.00,176.98,117.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Robustness evaluation of EGC model on CIFAR-10 with PGD and FGSM adversarial attacks. Our proposed classifier exhibits considerable improvement in adversarial robustness compared to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The conditional sampling results demonstrate that the random noise promises the diversity and the classification guidance guarantees the semantic consistency. As the guidance scale increase, the diversity significantly decreases and the mode collapse to the class prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Acc(%) IS(↑) FID(↓)</cell></row><row><cell>Hybrid Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>95.9</cell><cell>9.43</cell><cell>3.30</cell></row><row><cell>Glow [27]</cell><cell>67.6</cell><cell>3.92</cell><cell>48.9</cell></row><row><cell>R-Flow [2]</cell><cell>70.3</cell><cell>3.60</cell><cell>46.4</cell></row><row><cell>IGEBM [6]</cell><cell>49.1</cell><cell>8.30</cell><cell>37.9</cell></row><row><cell>JEM [16]</cell><cell>92.9</cell><cell>8.76</cell><cell>38.4</cell></row><row><cell>Explicit EBM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diff Recovery [12]</cell><cell>N/A</cell><cell>8.30</cell><cell>9.58</cell></row><row><cell>VAEBM [53]</cell><cell>N/A</cell><cell>8.43</cell><cell>12.2</cell></row><row><cell>ImprovedCD [5]</cell><cell>N/A</cell><cell>7.85</cell><cell>25.1</cell></row><row><cell>CF-EBM [63]</cell><cell>N/A</cell><cell>-</cell><cell>16.7</cell></row><row><cell>EBMs-VAE [55]</cell><cell>N/A</cell><cell>6.65</cell><cell>36.2</cell></row><row><cell>CoopFlow [56]</cell><cell>N/A</cell><cell>-</cell><cell>15.8</cell></row><row><cell>CEM [50]</cell><cell>N/A</cell><cell>8.68</cell><cell>36.4</cell></row><row><cell>ATEBM [57]</cell><cell>N/A</cell><cell>9.10</cell><cell>13.2</cell></row><row><cell>HATEBM [18]</cell><cell>N/A</cell><cell>-</cell><cell>19.30</cell></row><row><cell>Adaptive CE [52]</cell><cell>N/A</cell><cell>-</cell><cell>65.01</cell></row><row><cell>CLEL [32]</cell><cell>N/A</cell><cell>-</cell><cell>8.61</cell></row><row><cell>GANs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN [1]</cell><cell>N/A</cell><cell>9.22</cell><cell>14.7</cell></row><row><cell>SNGAN [37]</cell><cell>N/A</cell><cell>8.22</cell><cell>21.7</cell></row><row><cell>StyleGAN* [24]</cell><cell>N/A</cell><cell>8.99</cell><cell>9.90</cell></row><row><cell>Score-Based Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DDPM [19]</cell><cell>N/A</cell><cell>9.46</cell><cell>3.17</cell></row><row><cell>NCSN [46]</cell><cell>N/A</cell><cell>8.87</cell><cell>25.32</cell></row><row><cell>NCSN-v2 [47]</cell><cell>N/A</cell><cell>8.40</cell><cell>10.87</cell></row><row><cell>Discriminative Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wide ResNet-28-12 [59]</cell><cell>95.6</cell><cell>N/A</cell><cell>N/A</cell></row></table><note><p>CIFAR-10 hybrid modeling results. 'N/A' means the corresponding result is not available. We report the result of Wide ResNet-28-12, which has a similar architecture, number of parameters and computational cost to our proposed model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of EGC model on CIFAR-100 dataset. Our model outperforms other state-of-the-art generative models in terms of FID and IS, while achieving superior classification accuracy.</figDesc><table><row><cell>Method</cell><cell>Acc(%)</cell><cell>IS(↑)</cell><cell>FID(↓)</cell></row><row><cell>EGC  ‡</cell><cell>78.9</cell><cell>231.3</cell><cell>6.05</cell></row><row><cell>EGC  †</cell><cell>72.5</cell><cell>189.5</cell><cell>6.77</cell></row><row><cell>EGC</cell><cell>70.4</cell><cell>79.9</cell><cell>17.5</cell></row><row><cell>ADM-Classifier [4]</cell><cell>64.3</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>HATEBM [18] (128×128)</cell><cell>N/A</cell><cell>-</cell><cell>29.37</cell></row><row><cell>IGEBM [6] (128×128)</cell><cell>N/A</cell><cell>28.6</cell><cell>43.7</cell></row><row><cell>IDDPM [39]</cell><cell>N/A</cell><cell>-</cell><cell>12.3</cell></row><row><cell>ADM [4]</cell><cell>N/A</cell><cell>100.98</cell><cell>10.94</cell></row><row><cell>LDM-VQ-8 [42]</cell><cell>N/A</cell><cell>201.56</cell><cell>7.77</cell></row><row><cell>VQGAN [8]</cell><cell>N/A</cell><cell>78.3</cell><cell>15.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of EGC model on ImageNet-1k 256×256 dataset using only random flip as data augmentation. † represents jointly training a conditional and an unconditional model.</figDesc><table /><note><p>‡ represents incorporating the RandResizeCrop data augmentation. We believe that a stronger augmentation strategy would likely yield improved results.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of Unsupervised EGC models on the CelebA-HQ and LSUN Church datasets. Our model outperforms the other state-of-the-art Energy-Based Models in terms of FID.</figDesc><table><row><cell cols="3">EBM Classifier Guidance Network Acc(%) FID(↓)</cell></row><row><cell>U-Net</cell><cell>N/A</cell><cell>5.36</cell></row><row><cell>U-Net</cell><cell>95.9</cell><cell>3.49</cell></row><row><cell>U-Net</cell><cell>95.9</cell><cell>3.30</cell></row><row><cell>ResNet</cell><cell>95.9</cell><cell>7.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The ablative results on CIFAR-10 dataset. EBM refers to Unsupervised EGC model model. Classifier represents EGC, and Guidance indicates the use of the gradient of the label for generating samples. Additionally, we evaluate the effectiveness of our method using an energybased model based on a standard feedforward ResNet as often used for image classification.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Implementation Details Algorithm 2 Sampling Sample x T ∼ N (0, I) for t = T, ..., 1 do Sample noise ∼ N (0, I) if t &gt; 1, else = 0</p><p>We adopt the UNet architecture used in LDM <ref type="bibr" target="#b40">[42]</ref> and IDDPM <ref type="bibr" target="#b37">[39]</ref>, with the group normalization layers retained. To improve convergence speed, we do not include spectral normalization and weight normalization regularization. We adjust the channel width, multiplier, attention resolution, and depth compared to IDDPM and LDM, as shown in Tab. 6. We use the conv resample instead of the Res-Down/Up Block to upsample and downsample features. To optimize our model, we set the learning rate to 0.0001, batch size to 128 and weight decay to 0 across all datasets except ImageNet, for which we use a batch size of 512. The Adam optimizer is used to update the model parameters. To save computational resources, the ImageNet and LSUN Church images are compressed to 32×32×4 latent features by the KL-autoencoder <ref type="bibr" target="#b40">[42]</ref>, while CelebA-HQ images are compressed to 64×64×3 latent features. To augment the data, we randomly flip the images for all the datasets except CIFAR, for which all the images for training are padded with 4 pixels on each side and a 32 × 32 crop is randomly sampled from the padded image or its horizontal flip, and cutout is used to avoid overfitting. The classification results at t = 0 are reported in Table <ref type="table">1</ref>-3. To balance the reconstruction and classification losses, we set γ = 0.001 for CIFAR and γ = 0.005 for ImageNet in Algorithm. 1.</p><p>We follow the sampling strategy used in DDPM and describe it in detail in Algo. 2. To conduct conditional sampling, we replace the unconditional score ∇ xt log p θ (x t ) with the conditional score ∇ xt log p θ (x t |y). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><surname>Jacobsen</surname></persName>
		</author>
		<title level="m">Residual flows for invertible generative modeling. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2021. 3, 5, 6</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01316</idno>
		<title level="m">Improved contrastive divergence training of energy based models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tweedie&apos;s formula and selection bias</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>American Statistical Association</publisher>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1602" to="1614" />
		</imprint>
	</monogr>
	<note>Journal of the</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03852</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generative convnets via multi-grid modeling and sampling</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junpei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9155" to="9164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flow contrastive estimation of energy-based models</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7518" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08125</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4392" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2007">2019. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning probabilistic models from generator latent spaces with hat ebm</title>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2020. 2, 3, 6, 7</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Introspective classification with convolutional nets</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilevel doubly variational learning for energy-based latent variable models</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhu</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aichun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hichem</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18460" to="18469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energybased models</title>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introspective neural networks for generative modeling</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fujie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Guiding energy-based models via contrastive latent variables</title>
		<author>
			<persName><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sejun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wasserstein introspective neural networks</title>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21464" to="21475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Repaint: Inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09770</idno>
		<title level="m">On learning non-convergent short-run mcmc toward energy-based model</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2022. 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2019. 2, 3, 6</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Regularizing generative adversarial networks under limited data</title>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7921" to="7931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Nvae: A deep hierarchical variational autoencoder. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">19667-19679, 2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A unified contrastive energy-based model for understanding the generative ability of adversarial training</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13455</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Energyinspired self-supervised pretraining for vision models</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01384</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive multi-stage density ratio estimation for learning latent space energy-based model</title>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><surname>Vaebm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00654</idno>
		<title level="m">A symbiosis between variational autoencoders and energy-based models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingnian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning energybased model with variational auto-encoder as amortized sampler</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10441" to="10451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06924</idno>
		<title level="m">A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning energy-based models with adversarial training</title>
		<author>
			<persName><forename type="first">Xuwang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2022. 2, 3, 6, 7</date>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06635</idno>
		<title level="m">Egsde: Unpaired image-to-image translation via energyguided stochastic differential equations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02088</idno>
		<title level="m">Feature quantization improves gan training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning energybased generative models via coarse-to-fine expanding and sampling</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
