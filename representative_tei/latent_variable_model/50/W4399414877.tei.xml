<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Effect Identification in LiNGAM Models with Latent Confounders</title>
				<funder ref="#_euzsbS8">
					<orgName type="full">SNF</orgName>
				</funder>
				<funder>
					<orgName type="full">IGSSE/TUM-GS</orgName>
				</funder>
				<funder ref="#_QSw9gYn">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-04">4 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Tramontano</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaroslav</forename><surname>Kivva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saber</forename><surname>Salehkaleybar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
						</author>
						<title level="a" type="main">Causal Effect Identification in LiNGAM Models with Latent Confounders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-04">4 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.02049v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting the impact of an unseen intervention in a system is a crucial challenge in many fields, such as medicine <ref type="bibr" target="#b40">(Sanchez et al., 2022;</ref><ref type="bibr">Michoel &amp; Zhang, 2023)</ref>, policy evaluation <ref type="bibr" target="#b2">(Athey &amp; Imbens, 2017)</ref>, fair decision-making <ref type="bibr" target="#b22">(Kilbertus et al., 2017)</ref>, and finance <ref type="bibr" target="#b15">(de Prado, 2023)</ref>. Randomized experiments form the gold standard for addressing this challenge, but are often infeasible due to ethical concerns or prohibitively high costs. To tackle situations in which only observational data are available, one needs to make additional assumptions on the underlying causal system. The field of causal inference strives to formalize such assumptions. One notable approach in causal inference is modeling causal relationships through structural equation models (SEM) <ref type="bibr" target="#b36">(Pearl, 2009)</ref>. In this framework, a random vector is associated with a directed acyclic graph (DAG). Each vector component is associated with a node in the graph and is a function of the random variables corresponding to its parents in the graph and its corresponding exogenous noise.</p><p>In this paper, we mainly focus on the identifiability of causal effects in the important subclass of linear SEM and characterize graphically which causal effects can be uniquely determined from the observational data. When the exogenous noises in a linear SEM are Gaussian, the entire information about the model is contained in the covariance matrix among the variables, with all the higher-order moments of the distribution being uninformative. This entails that the causal structure as well as other causal quantities are often not identifiable from mere observational data. The most prominent instance in the context of causal structure learning is that the causal graph is identifiable only up to an equivalence class (e.g., <ref type="bibr">Drton, 2018, §10)</ref>. This motivated the widespread use of the linear non-Gaussian additive noise model (LiNGAM), where the exogenous noises are non-Gaussian.</p><p>The seminal work of <ref type="bibr" target="#b42">Shimizu et al. (2006)</ref> showed that in the setting of LiNGAM, the true underlying causal graph is uniquely identifiable when all the variables are observed. Since then, a rich literature on this topic has emerged, focusing mainly on the identification and the estimation of the causal graph; see, e.g., <ref type="bibr" target="#b0">Adams et al. (2021)</ref>; <ref type="bibr" target="#b41">Shimizu (2022)</ref>; <ref type="bibr">Yang et al. (2022)</ref>; <ref type="bibr">Wang et al. (2023)</ref>; <ref type="bibr">Wang &amp; Drton (2023)</ref> for recent results that allow for the presence of hidden variables. Indeed, linear models remain the backbone of problem abstraction in many scientific disciplines, because they offer simple qualitative interpretations and can be learned with moderate sample sizes (Pe'er &amp; Hacohen, 2011, Principle 1). In particular, the LiNGAM model finds application in diverse scientific fields, such as Neuroscience <ref type="bibr">(Chiyohara et al., 2023)</ref>, Economics <ref type="bibr" target="#b10">(Ciarli et al., 2023)</ref>, or Epidemiology <ref type="bibr">(Barrera &amp; Miljkovic, 2022)</ref>.</p><p>Within the LiNGAM literature, causal effect identification has received less attention; only few recent work <ref type="bibr" target="#b23">(Kivva et al., 2023;</ref><ref type="bibr" target="#b45">Shuai et al., 2023)</ref> have exploited the non-Gaussianity to provide identification formulas that work for specific causal graphs. The only graphical criteria for identification are given in <ref type="bibr" target="#b39">Salehkaleybar et al. (2020)</ref>; <ref type="bibr">Yang et al. (2022)</ref>. The main drawback of the aforementioned papers is that they target simultaneous recovery of all the causal effects. However, in many applications, we are interested in causal effects of only some subset of variables on others. Indeed, it may be the case that some causal effects are identifiable while others are not; see Figure <ref type="figure">2</ref> for an example in the context of proxy variable graphs. In this paper, we provide necessary and sufficient graphical conditions for the generic identifiability (see Section 2.3 for the exact definition) of direct and total causal effects between a given pair of observed variables in a LiNGAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contribution</head><p>Our main contributions are as follows:</p><p>• We provide necessary and sufficient graphical criteria for the generic identifiability of the causal effect, both when the causal graph is known a priori (Section 3.1) and when it is unknown (Section 3.2).</p><p>• We propose sound and complete algorithms that check our criteria in polynomial time in the size of the graph, in both considered settings (Section 3.4).</p><p>• For practical estimation of the effect of interest, we propose an adaptation of the RICA algorithm for Independent Component Analysis <ref type="bibr" target="#b27">(Le et al., 2011)</ref>. Experimental results show that the proposed method can provide better estimates of causal effects when compared with previous work (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notation</head><p>A directed graph is a pair G = (V, E) where V := {1, . . . , p} is the set of nodes and E ⊆ {(i, j) | i, j ∈ V, i ̸ = j} is the set of edges. We denote a pair (i, j) ∈ E as i → j.</p><formula xml:id="formula_0">A (directed) path from node i to node j in G is a sequence of nodes π = (i 1 = i, . . . , i k+1 = j) such that i s → i s+1 ∈ E for s ∈ {1, . . . , k}. A cycle in G is a path from a node i to itself. A Directed Acyclic Graph (DAG) is a directed graph without cycles. If i → j ∈ E,</formula><p>we say that i is a parent of j, and j is a child of i. If there is a path from i to j in G, we say that i is an ancestor of j and j is a descendant of i. The sets of parents, children, ancestors, and descendants of a given node i are denoted by pa(i), ch(i), an(i), and de(i), respectively. In our work, we distinguish between observed and latent variables by partitioning the nodes in two sets V = O∪L, of respective sizes p o and p l . Moreover, we define the set of observed descendants of a node i as de o (i) := (de(i) ∪ {i}) ∩ O.</p><p>We write vectors and matrices in boldface. The entry (i, j) of a matrix A is denoted by [A] i,j . Let I, J be subsets of the row and column sets of A, respectively. We denote the submatrix containing only the rows in I and the columns in J as [A] I,J . For a permutation σ, P σ denotes the associated permutation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model</head><p>Let G = (V, E) be a fixed DAG on p nodes. In a fixed probability space, let V = (V 0 , . . . , V p ) be a random vector taking values in R p and satisfying the following structural equation model:</p><formula xml:id="formula_1">V = AV + N = BN,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">[A] j,i = 0 if i → j / ∈ E, B := (I -A) -1</formula><p>, and the enteries of the exogenous noise vector N are assumed to be jointly independent and non-Gaussian. V is partitioned to</p><formula xml:id="formula_3">[V o , V l ]</formula><p>, where V o is observed of dimension p o , while V l is latent and of dimension p l . We can rewrite (1) as</p><formula xml:id="formula_4">V o V l = A o,o A o,l A l,o A l,l V o V l + N o N l ,</formula><p>which implies that the observed random vector satisfies</p><formula xml:id="formula_5">V o = B ′ N = B o B l N o N l (2) with B ′ := [(I -A) -1 ] O,V .</formula><p>We refer to this model as the latent variable LiNGAM (lvLiNGAM).<ref type="foot" target="#foot_4">foot_4</ref>  <ref type="bibr">Salehkaleybar et al. (2020, §3)</ref> show that the matrix B ′ can be expressed as follows:</p><formula xml:id="formula_6">B o = (I-D) -1 , B l = (I-D) -1 A o,l (I-A l,l ) -1 , (3) with D = A o,o + A o,l (I -A l,l ) -1 A l,o .</formula><p>The matrices B ′ and D contain information on the interventional distributions of V o . In particular,<ref type="foot" target="#foot_5">foot_5</ref> </p><formula xml:id="formula_7">[B ′ ] i,j = ∂E(V i | do(V j )) ∂V j , [D] i,j = ∂E(V i | do(V pa(i) )) ∂V j .<label>(4)</label></formula><p>In other words, [B ′ ] j,i is the average total causal effect of j on i, while [D] j,i is the average causal effect of j on i that is not mediated by other observed nodes. With slight abuse of terminology, we refer to the entries of B ′ as total causal effects and to those of D as direct causal effects. <ref type="bibr" target="#b19">Hoyer et al. (2008)</ref> show that for any lvLiNGAM model, an associated canonical model exists, in which, in the corresponding graph, all the latent nodes have at least two children and have no parents. We refer to the graph corresponding to a canonical model as a canonical graph. The original and the associated canonical model are observationally and causally equivalent <ref type="bibr">(Hoyer et al., 2008, §3)</ref>. Subsequently, without loss of generality, we will assume our model is canonical in this sense. <ref type="bibr">Salehkaleybar et al. (2020, Cor. 11)</ref> proved that the number of latent variables is identifiable in canonical models.</p><p>Remark 2.1. Throughout the paper, we assume that the number of latent variables p l is known.</p><p>In canonical models, A l,o = A l,l = 0, and in particular</p><formula xml:id="formula_8">B o = (I -A o,o ) -1 , B l = (I -A o,o ) -1 A o,l . (5) For every canonical G, let R G A be the set of all p × p real matrices A such that [A] i,j = 0 if j → i / ∈ G. Let R G be the set of all p o × p matrices, B ′ = [B o , B l ] that can be obtained from a matrix A ∈ R G</formula><p>A from (5). Let NG p be the set of p dimensional, non-degenerate, jointly independent non-Gaussian random vectors, and let M(G) be the set of all p o dimensional random vectors that can be expressed according to (2), where the matrix B ′ ∈ R G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Identifiability</head><p>According to (2), the mechanism generating the observational distribution, i.e., the probability distribution of V o , only depends on the matrix B ′ and the exogenous noise vector N. Therefore, the parameters of interest such as the causal effects in (4), are functions ϕ(B ′ ). As generally there are multiple pairs (B ′ , N), with different B ′ s, that generate the same observational distribution, it is important to clarify whether a parameter ϕ is identifiable. This holds if ϕ(B ′ ) takes the same value in all considered pairs (B ′ , N) generating the same observational distribution. Adding further assumptions about the causal graph may limit the datagenerating mechanisms compatible with the observational data, leading to more parameters becoming identifiable.</p><p>In this paper, we study the identification of causal effects in two scenarios: when the causal graph G is known and when it is not. The remainder of the section provides a formal description of the resulting parameter identification problems in the lvLiNGAM setting.</p><p>Fix a DAG G. The observational random vector is obtained from the noise and the matrix B ′ according to the mapping</p><formula xml:id="formula_9">Φ G : R G × NG p - → M(G) (B ′ , N) → B ′ N = B o B l N o N l .<label>(6)</label></formula><p>Let ϕ be a real-valued function on R G . We say that the parameter ϕ is globally identifiable without knowledge of the graph if for every pair (B ′ , N) ∈ R G × NG p and the associated observed random vector</p><formula xml:id="formula_10">V o := Φ G (B ′ , N) ∈ M(G), there does not exist a DAG G and a pair ( B′ , Ñ) ∈ R G × NG p such that Φ G ( B′ , Ñ) = d V o but ϕ(B ′ ) ̸ = ϕ( B′ ).<label>(7)</label></formula><p>Here, = d denotes equality in distribution of two random vectors. Note that the above definition allows for consideration of any G, possibly, different from G.</p><p>In contrast, we say that ϕ is globally identifiable with knowledge of the graph if for every pair (B ′ , N) ∈ R G × NG p and the associated observed random vector</p><formula xml:id="formula_11">V o := Φ G (B ′ , N) ∈ M(G), there does not exist another pair ( B′ , Ñ) ∈ R G × NG p with Φ G ( B′ , Ñ) = d V o but ϕ(B ′ ) ̸ = ϕ( B′ ).<label>(8)</label></formula><p>In latent variable models such as lvLiNGAM, global identifiability, as defined above, is often too stringent of a condition and fails to apply even in many commonly used models such as the instrumental variable model. Thus, a weaker notion of so-called generic identifiability is often considered for linear models <ref type="bibr">(Maathuis et al., 2019, §16.4</ref>). We say that the parameter ϕ is generically identifiable with (or without) knowledge of the graph, if the condition in (8) (or ( <ref type="formula" target="#formula_10">7</ref>)) holds for every</p><formula xml:id="formula_12">(B ′ , N) ∈ (R G \ B except ) × NG p where B except is a Lebesgue measure zero subset of R G .</formula><p>Our results give necessary and sufficient conditions for generic identifiability of the parameters [A o,o ] i,j and [B o ] i,j , both with and without knowledge of the underlying graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Results on Causal Effect Identification</head><p>This section presents our main identifiability results for lvLiNGAM. In Sections 3.2 and 3.1, we characterize the identifiable causal effects, respectively, with or without knowledge of the graph. In Section 3.3, we provide several examples in which our identifiability criteria hold; in Section 3.4, we propose an algorithm to certify our criteria efficiently in time. Finally, in Section 3.5, we present an estimation algorithm to estimate the causal effect from the observational data. All proofs appear in the Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Identification with an Unknown Graph</head><p>In <ref type="bibr" target="#b39">Salehkaleybar et al. (2020)</ref>, it is shown that if the total causal effect between any ancestor-descendent pair is nonzero, the mixing matrix B ′ can be identified by means of overcomplete independent component analysis (ICA), up to a permutation of the columns corresponding to a pair (j, l) ∈ O × L such that de o (j) = de o (l). This leads to a graphical criterion for the identification of the entire mixing matrix, which we rephrase herein using our notation.</p><p>Theorem 3.1. <ref type="bibr">(Salehkaleybar et al., 2020, Theo. 16</ref>) For B ′ ∈ R G , matrix B o is generically identifiable without knowing the causal graph if and only if there are no pairs j ̸ = i ∈ V such that de o (j) = de o (i).</p><p>Remark 3.2 (The scaling matrix). Equation ( <ref type="formula">5</ref>) implies that as long as we are focused on identifying the causal effect between observed variables alone, the scaling of the latent columns does not make a difference. Hence, without loss of generality, in the sequel, we assume that all the mixing matrices are scaled in such a way that the first non-zero entry in each column is equal to 1. In other words, A il = 1 if i and l are, respectively, observed and latent variables and i is the first child of l in a given causal order. Note that this is always the case if de o (l) = de o (i).</p><p>In this section, we extend the result in Theorem 3.1 by providing necessary and sufficient graphical conditions for the generic identifiability for the entries of B o and A o,o .</p><p>Our first result refines the graphical condition of Theorem 3.1 by adding ( <ref type="formula" target="#formula_14">10</ref>). This provides a complete graphical characterization of the identifiable total causal effects.</p><p>Theorem 3.3 (Total causal effect). Consider any two observed variables i and j. The total causal effect of j on i is generically identifiable without knowledge of the graph if and only if there is no l ∈ L, such that</p><formula xml:id="formula_13">de o (j) = de o (l), and<label>(9)</label></formula><p>i ∈ de</p><formula xml:id="formula_14">G \j o (l),<label>(10)</label></formula><p>where de</p><formula xml:id="formula_15">G \j o (l)</formula><p>is the set of observed descendents of l in the graph obtained from G by removing all the edges pointing to j.</p><p>The next result provides a complete graphical characterization of the identifiable direct causal effects.</p><p>Theorem 3.4 (Direct causal effect). Consider any two observed variables i and j. The direct causal effect of j on i is generically identifiable without knowledge of the graph if and only if there are no pairs (k, l) ∈ O × L such that</p><formula xml:id="formula_16">de o (k) = de o (l), (<label>11</label></formula><formula xml:id="formula_17">) i ∈ ch(l), (12) k ∈ ch(j) ∪ {j}. (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Example 3.5 (Identification with instrumental variables).</p><p>The mixing matrix for the instrumental variable (IV) graph <ref type="bibr">(Cunningham, 2021, §7.1)</ref> in Figure <ref type="figure" target="#fig_0">1</ref> has the following form</p><formula xml:id="formula_19">B ′ =   1 0 0 0 b T I 1 1 0 b T I b Y T b Y T b Y T + b LY 1   . The parameter of interest in the IV graph of Figure 1 is b Y T , i.e., the causal effect of the treatment T on the outcome Y . Since de o (T ) = de o (L) = {T, Y }, we can consider B′ = B ′ • P σ , Ñ = P -1 σ • N,</formula><p>where σ is the transposition that permutes the columns corresponding to T and L. From (6), one can see that</p><formula xml:id="formula_20">Φ GIV ( B′ , Ñ) = d Φ G IV (B ′ , N), ∀ N ∈ NG p ,</formula><p>where GIV is the graph obtained from the IV graph after adding an edge from I to Y (see Figure <ref type="figure" target="#fig_0">12</ref> in Appendix B).</p><formula xml:id="formula_21">Since [ B′ ] Y T ̸ = [B ′ ] Y T</formula><p>the total causal effect of T on Y is not identifiable without knowing the true causal graph.</p><formula xml:id="formula_22">G IV : I T Y L bT I bY T 1 bY L Figure 1. Instrumental variable graph.</formula><p>The parameters in blue are identifiable without knowledge of the graph, while the parameters in red are not identifiable.</p><p>Notice that we could derive this result by applying Theorem 3.4 to the pair (k, l) = (T, Y ) directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Identification with a Known Graph</head><p>A permutation of two columns of the matrix B ′ may result in different graphs. This implies that if the graph is known, we can narrow down the set of possible permutations. In this section, we study how this additional assumption allows us to identify a parameter of interest (which is not generically identifiable). We will discuss an instance of this situation in Example 3.7.</p><p>The first result of this section provides a characterization of the column permutations that leave the graph unchanged. Theorem 3.6. For every B ′ outside a Lebesgue zero subset of R G , let B′ = B ′ • P σ , where σ is any permutation. We have B′ ∈ R G , if and only if</p><formula xml:id="formula_23">de o (i) = de o (σ(i)), ∀i ∈ V,</formula><p>and there are no i, j ∈ O and l ∈ L such that</p><formula xml:id="formula_24">i ∈ pa(j) ∪ {j}, σ(j) = l, ch(l) \ ch(i) ̸ = ∅.</formula><p>Example 3.7 (Example 3.5 continued). We now show that the causal effect of T on Y is identifiable if we assume the graph in Figure <ref type="figure" target="#fig_0">1</ref> is the true underlying graph.</p><p>The adjacency matrix corresponding to B′ has the following form</p><formula xml:id="formula_25">Ão,o = I -B-1 o =   1 0 0 b T I 1 0 -b T I b Y L b Y T + b Y L 1   .</formula><p>This form is not compatible with the graph in Figure <ref type="figure" target="#fig_0">1</ref> since the entry corresponding to the edge from I to Y is nonzero. Hence, if we were told that the graph in Figure <ref type="figure" target="#fig_0">1</ref> is the true underlying causal structure, the only valid permutation of B ′ would be the identity. Therefore, we can identify b Y T , the parameter of our interest.</p><p>Notice that we could derive this result by applying Theorem 3.6 to the triple (i, j, l) = (I, T, L) directly. See Example B.3 for more discussion of the IV graph.</p><p>Using Theorem 3.6, we can refine the criteria of Section 3.1 for the setting of known causal graph. In particular, the next two theorems characterize all the total and direct causal effects that are identifiable when the graph is known. Theorem 3.8 (Total Causal Effect). Consider any two observed variables i and j. The total causal effect of j on i is generically identifiable with knowledge of the graph if and only if there is no l ∈ L, such that</p><formula xml:id="formula_26">de o (j) = de o (l),<label>(14)</label></formula><p>i ∈ de</p><formula xml:id="formula_27">G \j o (l), (15) ch(l) \ ch(k) = ∅, ∀k ∈ pa(j) ∪ {j}.<label>(16)</label></formula><p>Theorem 3.9 (Direct Causal Effect). Consider any two observed variables i and j. The direct causal effect of j on i is generically identifiable if and only if there are no pairs</p><formula xml:id="formula_28">(k, l) ∈ O × L such that de o (k) = de o (l), (<label>17</label></formula><formula xml:id="formula_29">) i ∈ ch(l), (18) k ∈ ch(j) ∪ {j}, (19) ch(l) \ ch(k 1 ) = ∅, ∀k 1 ∈ pa(k) ∪ {k}. (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>We conclude the section by giving the graphical condition for identification of the whole mixing matrix with knowledge of the graph. Corollary 3.10 (Mixing matrix identification). For B ′ ∈ R G , the entire matrix B o is generically identifiable with knowledge of the graph if and only if there are no i, j ∈ O, and l ∈ L such that (14), (15), and (16) are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Examples</head><p>We now highlight different scenarios in which the results in this section allow us to relax standard assumptions in identifying the causal effect. For the sake of simplicity in presentation, in the first two scenarios, we only consider two latent confounders in the system. Although these scenarios can be easily extended to any arbitrary number of latent confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Proxy Variables</head><p>The presence of proxy variables allow the identification of the causal effect in linear models. In particular, it has been shown that the causal effect of treatment T on outcome Y is identifiable if the following two conditions hold (see, e.g., <ref type="bibr">Kuroki &amp; Pearl (2014, §4)</ref>, <ref type="bibr">Liu et al. (2023, §2)</ref>):</p><p>(a) There are as many proxies as there are confounders, (b) W ⊥ ⊥ (T, Y ) | L for every proxy variable W and every latent confounder L.</p><formula xml:id="formula_31">L 1 L 2 T Y W Figure 2.</formula><p>Causal graph with proxy variable. The parameter corresponding to the blue edge is generically identifiable if the graph is known, while those corresponding to the red edges are not. The dashed edge is dropped in the corresponding canonical model.</p><p>Consider the causal graph in Figure <ref type="figure">2</ref>. Both aforementioned conditions are violated as there are two latent confounders but a single proxy and there is an edge from this proxy to the treatment. Yet, in lvLiNGAM, we show that the causal effect from T to Y is identifiable. For both the latent variables we have W ∈ de o (L) \ de o (T ), hence condition ( <ref type="formula" target="#formula_26">14</ref>) cannot be satisfied. Using Theorem 3.8, this implies that the causal effect of our interest is identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Longitudinal Data</head><p>The causal graph associated with a longitudinal data model <ref type="bibr" target="#b21">(Imai &amp; Kim, 2019)</ref> is given in the left plot in Figure <ref type="figure">3</ref>, where L is the latent confounder, T is the time-varying treatment, and Y is the time-varying outcome. The common identifiability assumptions in a linear setting are that the causal effect is constant through time, i.e., [A] Y1,T1 = [A] Y2,T2 and there is no time-varying confounding, see, e.g., <ref type="bibr">Cunningham (2021, §8)</ref>. In lvLiNGAM, these assumptions can be relaxed by having access to some covariates. In particular, suppose that there is a covariate C i such that de o (L i ) = de o (C i ) for every time period i (see the right plot in Figure <ref type="figure">3</ref>). We have</p><formula xml:id="formula_32">C i ∈ de o (L i ) \ de o (T i</formula><p>) and the condition ( <ref type="formula" target="#formula_26">14</ref>) is not satisfied. This implies that the causal effect of T i on Y i is identifiable for every time period i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Underspecified Instruments</head><p>A standard assumption for identifying the causal effect using instrumental variables is that there are at least as many instruments as treatments <ref type="bibr">(Brito &amp; Pearl, 2002, Thm. 1)</ref>.</p><p>Recently, <ref type="bibr" target="#b1">Ailer et al. (2023)</ref> showed that in the linear underspecified setting, i.e., when the number of instruments is less than the number of treatments, one can identify the projection of the treatment on the instrument space, but this can be different from the causal effect. We now show that in the lvLiNGAM model, the treatment effects are identifiable in the underspecified case.</p><formula xml:id="formula_33">L C T 1 T 2 Y 1 Y 2 L 1 L 2 C 1 C 2 T 1 T 2 Y 1 Y 2 Figure 3.</formula><p>The causal graphs for longitudinal data. The graph on the right allows for a time-varying confounder. The parameters corresponding to the blue edges are generically identifiable with the knowledge of the graph, while the ones corresponding to the red edges are not. The dashed line is dropped in the corresponding canonical model.</p><p>We say that I is a valid instrument for the treatments T 1 , . . . , T n on Y if the following conditions hold</p><formula xml:id="formula_34">I ∈ pa(T i ) ∀ i ∈ {1, . . . , n}, I ⊥ G \T Y,</formula><p>where ⊥denotes d-separation <ref type="bibr">(Pearl, 2009, §1.</ref>2), and G \T is the graph obtained from G by removing all the edges from T i to Y . See Figure <ref type="figure">4</ref> for an instance with two treatments and one instrument.</p><p>For every latent variable</p><formula xml:id="formula_35">L i such that de o (T i ) = de o (L i ) and Y ∈ ch(L i ), we have Y ∈ ch(L i ) \ ch(I).</formula><p>Hence condition ( <ref type="formula" target="#formula_27">16</ref>) does not hold. By Theorem 3.8, in lvLiNGAM, the causal effect of T i on Y is identifiable for every i, even when only one instrumental variable is available.</p><formula xml:id="formula_36">I T 1 T 2 Y L 1 L 2 Figure 4</formula><p>. An example of causal graph for underspecified instrumental variable. The parameters corresponding to the blue edges are generically identifiable with knowledge of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Certifying Identifiability</head><p>In this section, we prove that the graphical condition for identification given in Theorem 3.8 can be certified in polynomial time in the size of the graph, and we provide a sound and complete algorithm (Algorithm 1) for this task.</p><p>The algorithms for the other identification criteria can be found in Appendix B.3. Theorem 3.11. Algorithm 1 is sound and complete for certifying the generic identifiability of the total causal effect of j on i with knowledge of the graph G. The computational complexity of the algorithm is Sort ch(l) according to the topological order defined in step 2 7:</p><formula xml:id="formula_37">O(p l (p 2 o + |E|)) = O(p 3 ).</formula><formula xml:id="formula_38">if ch(l)[1] = j then 8:</formula><p>Compute de</p><formula xml:id="formula_39">G \j o (l) 9: if i ∈ de G \j o (l) then 10: Compute de o (l) 11: if de o (l) = de o (j) then {(15)} 12: ID ← FALSE {(14)} 13:</formula><p>Compute pa(j)</p><p>14:</p><p>for all k ∈ pa(j) ∪ {j} do 15:</p><formula xml:id="formula_40">if ch(l) \ ch(k) ̸ = ∅ then 16: ID ← TRUE {(16)} 17:</formula><p>L ← L \ {l} 18: RETURN: ID Remark 3.12. Algorithm 1 is simple in the sense that it directly checks the graphical conditions in Theorem 3.8. This is not the case for checking most identifiability results in linear models, which often requires building an auxiliary graph and solving a maximum-flow problem on it (which becomes prohibitive for large graphs), <ref type="bibr" target="#b7">(Brito &amp; Pearl, 2006;</ref><ref type="bibr" target="#b24">Kumor et al., 2020;</ref><ref type="bibr" target="#b4">Barber et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Estimation Algorithms</head><p>When the graph structure is unknown, <ref type="bibr">Salehkaleybar et al. (2020, Alg. 1)</ref> proposed an algorithm that first solves an overcomplete ICA problem and then post-process the estimated mixing matrix to enumerate all the possible causal effects. This usually entails solving a high-dimensional non-convex optimization problem. If the DAG structure is known, one can enforce this knowledge to reduce the problem's dimensionality from p 2 to |E| and improve the performance. We follow this approach and propose an adaptation of the RICA algorithm for recovering the mixing matrix <ref type="bibr" target="#b27">(Le et al., 2011)</ref>.</p><p>The objective function optimized by RICA is a weighted sum of two terms; the first term is a contrast function that measures the non-Gaussianity of the exogenous noise, e.g., the l 1 -loss, and the second term is a reconstruction loss that enforces the orthonormality of the rows of the mixing matrix. The only instance in which the rows of a matrix in R G might be orthonormal is when all the causal effects are zero. Hence, we drop the reconstruction loss and only optimize the contrast function.</p><p>For a given DAG G, given observed data X 1 , . . . , X N ∈ R po , and a contrast function g, our algorithm solves the following optimization problem Graphical RICA: arg min</p><formula xml:id="formula_41">B ′ ∈R G 1 N N i=1 g((B ′ ) T • X i ).</formula><p>We evaluate the performance of our algorithm in comparison with existing methods in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>There is a rich literature on graphical criteria for the identifiability of causal effects. In the nonparametric setting, the ID algorithm is a sound and complete algorithm that solves the global identification problem given the causal graph <ref type="bibr" target="#b44">(Shpitser &amp; Pearl, 2006;</ref><ref type="bibr" target="#b43">Shpitser, 2023)</ref>.</p><p>In the parametric case, most results are for the semi-Markovian and linear Gaussian models; for these models, a necessary and sufficient criterion for global identifiability is known, <ref type="bibr" target="#b17">(Drton et al., 2011)</ref>, while a complete characterization for generic identifiability remains unknown, <ref type="bibr" target="#b24">(Kumor et al., 2020)</ref>. For Gaussian models with explicit linear confounders, a sufficient graphical criterion for generic identifiability was proposed in <ref type="bibr" target="#b4">Barber et al. (2022)</ref>.  <ref type="formula">2023</ref>) provide sufficient graphical conditions for the identification of the mixing matrix using explicit moment equations. <ref type="bibr" target="#b23">Kivva et al. (2023)</ref> proposed an identification formula that works for the causal graph with one proxy variable; in contrast, <ref type="bibr" target="#b45">Shuai et al. (2023)</ref> proved that if one assumes that only the treatment is non-Gaussian and pre-treatment covariates are available, then the causal effect can be identified in the presence of latent confounders. <ref type="bibr" target="#b47">Tramontano et al. (2024)</ref> proposed a necessary and sufficient graphical criterion for the identifiability of the direct causal effect in linear models for acyclic-directed mixed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Identification</head><p>We used Algorithms 1 and 2 to check the identifiability of a causal effect for randomly selected edges in random graphs. The graphs are generated according to an Erdős-Rényi model in which we ensure that the sampled graphs are canonical. The probability of the causal effect of a randomly selected edge being identifiable versus the probability of 3 The code to replicate the experiments can be found at : <ref type="url" target="https://github.com/danieletramontano/Causal-Effect-Identification-in-LiNGAM-Models-with-Latent-Confounders">https://github.com/danieletramontano/Causal-Effect-Identification-in-LiNGAM-Models-with-Latent-Confounders</ref>. accepting an edge in the graph generation model is plotted in Figure <ref type="figure" target="#fig_2">5</ref>. For each setup, we randomly sample 500 graphs. Interestingly, we found that for all the graphs and all the edges we sampled, the corresponding causal effects are identifiable with the knowledge of the graph. As expected, when we do not assume the graph is known, the probability of identifying the causal effect of randomly selected edge drops and it depends both on the density of the graph and the proportion of observed nodes (see Figure <ref type="figure" target="#fig_2">5</ref>). The average run time of Algorithm 1 for different graph sizes is shown in Figure <ref type="figure">6</ref>. It is noteworthy that our algorithm can handle graphs with a thousand nodes in about a second; this is due to the simplicity of our identification criteria, as explained in Remark 3.12.</p><p>Figure <ref type="figure">6</ref>. On the x-axis, the size of the graph. On the y-axis, the average running time in seconds. po/p is fixed to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Causal Effect Estimation</head><p>In this section, we provide experimental results for Graphical RICA (GRICA), which we introduced in Section 3.5. We present experiments for data generated according to the causal structures in Figure <ref type="figure" target="#fig_3">7</ref>, and compare the performance of GRICA with the state-of-the-art. As a measure of performance, we used the relative error metric given by: error = |Estimated Value -True Value|/|True Value|. Further experimental results are provided in Appendix C.  We evaluated performance of the GRICA with respect to the observational sample size. We considered the settings that are compatible with the graphs G 1 and G 2 of Figure <ref type="figure" target="#fig_3">7</ref> to recover the direct causal effect of variable T on Y . We compared the performance of GRICA with the adaptation of the RICA algorithm implemented in Salehkaleybar et al.</p><formula xml:id="formula_42">L 1 T Y W G 1 L 1 T Y W G 2 L 1 Z T Y W G 3</formula><p>(2020) and the recently developed Cross-Moment method in <ref type="bibr" target="#b23">Kivva et al. (2023)</ref>. We assumed all exogenous noises have the same distribution. Moreover, all direct causal coefficients in matrix A are generated uniformly at random from [-1, -0.5] ∪ [0.5, 1]. 4 In Figure <ref type="figure" target="#fig_4">8</ref>, we observe that GRICA consistently recovers the correct causal effect for both graphs, even with a few number of samples. Note that 4 We consider this interval to ensure a fair comparison with RICA's implementation for causal effect estimation in <ref type="bibr" target="#b39">Salehkaleybar et al. (2020)</ref> as it requires the absolute values of all causal coefficients to be smaller than one.  the Cross-Moment is a consistent estimator for the causal graph G 1 . It performs better than the rest when there are enough samples to compute high-order moments accurately. Furthermore, the RICA algorithm often gets stuck in bad local minima, and as a result is unstable.</p><p>2. Relative error vs observations noise. In Figure <ref type="figure" target="#fig_6">9</ref>, we illustrate how the variances of certain exogenous noise impact the accuracy of the estimation. All causal coefficients in both settings are set to one.</p><p>For the experiment over G 1 , we scaled the standard deviation of exogenous noise N W corresponding to variable W by a factor displayed on the x-axis of Figure <ref type="figure" target="#fig_6">9a</ref>. We observed that GRICA performs similarly to the Cross-Moment method, which is specifically designed for the graph G 1 .</p><p>The experiment for G 3 is similar to the one performed in <ref type="bibr" target="#b23">Kivva et al. (2023)</ref>. Here, we scaled N W and N Z by Ratio and 1/Ratio, respectively, where Ratio is plotted on the x-axis of Figure <ref type="figure" target="#fig_6">9b</ref>. We compared the performance of GRICA with RICA and methods developed specifically for this causal graph in <ref type="bibr" target="#b23">(Kivva et al., 2023;</ref><ref type="bibr" target="#b46">Tchetgen et al., 2020)</ref>. As depicted in Figure <ref type="figure" target="#fig_6">9b</ref>, GRICA can benefit from the noiseless observation of L 1 through proxy Z, while all other algorithms are affected by the presence of noise in the observations from W .</p><p>3. Causal effect estimation on random graphs. In this section, we will present experiments for larger causal graphs. In particular, we compare the GRICA algorithm with the RICA algorithm on graphs randomly sampled from an Erdős-Rényi model. The probability of accepting an edge is set to 1/2. Similarly to the previous experiments, we generated all causal coefficients in the matrix A uniformly at random from [-1, -0.5] ∪ [0.5, 1]. As a measure of performance, we use the normalized Frobenius loss between the estimated mixing matrix and the true one, i.e., || B -B|| F /||B|| F . Note that according to the experiments in Section 5.1, we expect all the causal effects of interest to be identifiable. Figures <ref type="figure" target="#fig_8">10a</ref> and<ref type="figure" target="#fig_8">10b</ref> show the experimental results for the Erdős-Rényi model in graphs with one latent and five observed variables and graphs with two latent and ten observed variables, respectively. The results are averaged over ten trials. As can be seen, GRICA significantly outperforms the RICA algorithm in larger causal graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Consistency Guarantees</head><p>The OICA problem is known to be identifiable but not separable; see <ref type="bibr" target="#b18">(Eriksson &amp; Koivunen, 2004)</ref>. This implies that, as opposed to the complete ICA, minimizing a measure of non-Gaussianity does not necessarily lead to the identification of the mixing matrix.</p><p>In the past, various Expectation-Maximization (EM) algorithms have been proposed to work under specific parametric models for the exogenous noise model, e.g., Mixture of Gaussians <ref type="bibr" target="#b35">(Olshausen &amp; Millman, 1999)</ref> or Laplace distri-bution <ref type="bibr" target="#b28">(Lewicki &amp; Sejnowski, 2000)</ref>. These algorithms have been proven to be consistent if the assumptions are satisfied. At the same time, they are very computationally demanding and have been shown to perform poorly in practice. For this reason, the core of the research on the topic has shifted to heuristic methods, in which the mixing matrix is found as a solution to a suitable smooth optimization problem. The RICA algorithm <ref type="bibr" target="#b27">(Le et al., 2011)</ref> is arguably the most prominent in this class of algorithms and has already been used in causality, see, e.g., <ref type="bibr">Yang et al. (2022)</ref>. However, due to the complications stated above, the RICA algorithm has not yet been equipped with consistency guarantees.</p><p>Being an adaptation of the RICA algorithm, also the large sample size performance of our algorithm is not well understood in rigorous mathematical terms. However, we point out that the same parametrization of the mixing matrix can be utilized in any OICA algorithm to reduce the dimensionality of the problem. What our experiments suggest is that this simple step can improve remarkably the performances of OICA algorithms, when applied to the estimation of causal effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We considered the problem of generic identifiability of causal effects in LiNGAM models when only observational data are available. We solved the problem by providing efficiently implementable, graphical criteria for identification of the causal effect with and without knowledge of the graph. To estimate the effect, we proposed a flexible adaptation of the RICA algorithm, <ref type="bibr" target="#b27">(Le et al., 2011)</ref>, that incorporates the knowledge of the graph to reduce the dimension in the optimization problem.</p><p>To conclude, we highlight possible future directions.</p><p>Cyclic Models. When the acyclicity assumption is dropped, <ref type="bibr" target="#b26">Lacerda et al. (2008)</ref> showed that, in the fully observed case, the structure of the graph can be recovered up to an equivalence class. There are no explicit graphical criteria for the identification of the causal effects in cyclic LiNGAM models. It is worth exploring whether our proof techniques could be extended to this setting. Relaxing non-Gaussianity. <ref type="bibr" target="#b33">Ng et al. (2023)</ref> proved that in the complete ICA case, non-Gaussianity of the noises can be relaxed if the mixing matrix has a specific sparse structure. Extensions of these results to the overcomplete case might offer results on identification of the causal effects when some of the exogenous noises are allowed to be Gaussian. Non-linear ICA. There is an active line of work that aims to exploit non-linear ICA, <ref type="bibr" target="#b20">(Hyvärinen et al., 2024)</ref>, to construct causal discovery algorithms, <ref type="bibr" target="#b38">(Reizinger et al., 2023)</ref>. There is no literature that leverages these results to understand the identifiability of the causal effects.</p><p>Wang, Y. S., Kolar, M., and Drton, M. Confidence sets for causal orderings. arXiv:2305.14506, 2023.</p><p>Yang, Y., Ghassami, A., Nafea, M., Kiyavash, N., Zhang, K., and Shpitser, I. Causal discovery in linear latent variable models subject to measurement error. Advances in Neural Information Processing Systems, 35, 2022.</p><p>Definition A.9. Let I = {i 1 , . . . , i n }, J = {j 1 , . . . , j n } ⊂ V. P = P 1 , . . . , P n is system of paths between I, J, if there exists a permutation σ P ∈ S n , such that P k ∈ P(i k , j σ P (k) ). We denote the set of all such systems by P(I, J). Moreover, a system of paths is non-intersecting if P k ∩ P l = ∅ for k ̸ = l. The set of all such systems is denoted by P(I, J). The path monomial associated to P , is defined as:</p><formula xml:id="formula_43">a P = a P1 • • • • • a Pn . i 1 i 2 j 1 j 2 c i 1 i 2 j 1 j 2</formula><p>Figure 11. The system on the left has no sided intersection while the system on the right has.</p><p>Theorem A.10 (Gessel-Viennot-Lindström Lemma). Let I, J ⊂ V having the same size; then it holds that det(I -A) -1 I,J = P ∈P(J,I)</p><p>(-1) σ P a P = P ∈ P(J,I)</p><p>(-1) σ P a P .</p><p>In particular, det(I -A) -1</p><formula xml:id="formula_44">I,J = 0 ∈ R[G A ] if and only if P(J, I) = ∅.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs and Lemmas</head><p>B.1. Proofs of Section 3.1</p><p>Proof of Theorem 3.3. From <ref type="bibr">Salehkaleybar et al. (2020, Theo. 15)</ref>, we know that the entire j-th column B ′ is identifiable if and only if there is no latent variable l such that de o (j) = de o (l). Thus, we know that if this condition is satisfied, the entry [B ′ ] i,j is identifiable. Hence, we can assume that such an l exists to conclude the proof.</p><p>We will prove that</p><formula xml:id="formula_45">[P σ B ′ ] i,j = [B ′ ] i,l ̸ = [B ′ ] i,j , in general, if and only if i ∈ de G \j o (l)</formula><p>, where σ is the transposition that swaps j and l. In particular, we will show that</p><formula xml:id="formula_46">[B ′ ] i,j -[B ′ ] i,l ∈ R[G]</formula><p>where R[G] (the coordinate ring associate to G as defined in Definition A.1), is a non-zero polynomial if and only if the graphical condition is satisfied. Notice that from Corollary A.5, we know this is enough to show that the parameters are generically identifiable.</p><p>Using Lemma A.8, we can write the entries of B ′ as</p><formula xml:id="formula_47">[B ′ ] i,l = (I -A) -1 i,l = P ∈P(l,i)</formula><p>a P , where P(l, i) denotes the set of all the paths from l to i. Now let P j (l, i) be the set of directed path from l to i that passes through j, and P \j (l, i) = P(l, i) \ P j (l, i). We can rewrite the formula above as follows</p><formula xml:id="formula_48">[B ′ ] i,l = P ∈P(l,i) a P = P ∈Pj (l,i) a P + Q∈P \j (l,i) a Q .</formula><p>Note that P ∈ P j (l, i) if and only if there are P j ∈ P(l, j) and P i ∈ P(j, i) such that a P = a Pj a Pi . Finally, we can write [B ′ ] i,l as:</p><formula xml:id="formula_49">P ∈Pj (l,i) a P + Q∈P \j (l,i) a Q = Pi∈P(j,i) Pj ∈P(l,j) a Pj a Pi + Q∈P \j (l,i) a Q = Pi∈P(j,i) a Pi [B ′ ]i,j Pj ∈P(l,j) a Pj [B ′ ] i,l + Q∈P \j (l,i) a Q = [B ′ ] i,j [B ′ ] j,l + Q∈P \j (l,i) a Q =[B ′ ] i,j + Q∈P \j (l,i) a Q ,<label>(22)</label></formula><p>where we used [B ′ ] j,l = 1 from Remark 3.2 that we can assume without loss of generality. Finally, we have</p><formula xml:id="formula_50">[B ′ ] i0,j - [B ′ ] i0,l = Q∈P \j (l,i0) a Q ∈ R[G A ].</formula><p>From Lemma A.8, we know it is the zero polynomial if and only if</p><formula xml:id="formula_51">P \j (l, i 0 ) = ∅, i.e., i 0 / ∈ de G \j o (l). Notice that we proved that [B ′ ] i,j -[B ′ ] i0,l is non zero in R[G A ],</formula><p>that is enough to show thanks to Lemma A.4. Lemma B.1. Let j and l be observed and latent variables, respectively, such that de o (j) = de o (l). Let B′ = B ′ P σ , where σ is the transposition swapping the columns corresponding to l and j. For every i, k ∈ O, and h ∈ L we have:</p><formula xml:id="formula_52">[ Ão,o ] i,k = [A o,o ] i,k + 1 ch(l) (i)c j l,i [I -A o,o ] j,k ,<label>(23)</label></formula><formula xml:id="formula_53">[ Ão,l ] i,h = [A o,l ] i,h -1 ch(l) (i)c j l,i [A o,l ] j,h ,<label>(24)</label></formula><p>where c j l,i = (-1) i+j Q∈P \j (l,i) a Q , and 1 X represent the indicator function of a set X.</p><p>In particular,</p><formula xml:id="formula_54">[ Ão,o ] i,k = [A o,o ] i,k , if i / ∈ ch(l) or k / ∈ pa(j) ∪ {j}.</formula><p>Proof of Lemma B.1. We first show the equality in ( <ref type="formula" target="#formula_52">23</ref>). According to ( <ref type="formula" target="#formula_49">22</ref>), Bo can be written as B o + C:</p><formula xml:id="formula_55">C i,k =      Q∈P \j (l,i) a Q if k = j and i ∈ de G \j o (l), 0 otherwise. (<label>25</label></formula><formula xml:id="formula_56">)</formula><p>From the formula for the inverse of a matrix, we know that</p><formula xml:id="formula_57">[B o + C] -1 i,k = (-1) i+k det([B o + C] \k,\i ).</formula><p>Note that det(B o + C) = 1 since it is a lower triangular matrix with one on the diagonal. Thus, we can imply that</p><formula xml:id="formula_58">[ Ão,o ] j,k = [(B o + C) -1 ] j,k = [B o ] -1 j,k = [A o,o ] j,k</formula><p>. Hence, we only need to consider the entries of <ref type="bibr">[ Ão,o ]</ref> i,k where i ̸ = j. To compute det([B o + C] \k,\i ) with respect to the j-th column, we have:</p><formula xml:id="formula_59">det([B o + C] \k,\i ) = (-1) s+j [B o ] s,j det([B o ] \(j,s),\(i,j) ) + (-1) s+j C s,j det([B o ] \(j,s),\(i,j) ) = det([B o ] \k,\i ) + (-1) s+j C s,j det([B o ] \(j,s),\(i,j) ) = (-1) i+j [A o,o ] i,k + s∈de G \k o (l) (-1) s+j Q∈P \j (l,s) a Q det([B o ] \(k,s),\(i,j) ),</formula><p>where in the last equality, we plug in C s,j according to (25).</p><p>Based on Theorem A.10,</p><formula xml:id="formula_60">det([B o ] \(k,s),\(i,j) ) = det([(I -A o,o ) -1 ] \(k,s),\(i,j) ) = P ∈ P(O\{i,j},O\{k,s})</formula><p>(-1) σ P a P .</p><p>In the following, we show that that P(O \ {i, j}, O \ {k, s}) can be non-empty only if:</p><formula xml:id="formula_61">s = i or s → i ∈ G, k = j or k → j ∈ G. (<label>26</label></formula><formula xml:id="formula_62">)</formula><p>To show this, let P = {π 1 , . . . , π j-1 , π j+1 , . . . , π i-1 , π i+1 , . . . , π p } be a system of paths in P(O \ {i, j}, O \ {k, s}), where with π a we denote the path starting at the node a. Consider the path π s = sa 0 . . . a t . If there is a x ∈ {0, . . . , t} such that a x / ∈ {i, j}, then π s would intersect π ax and thus P would have an intersection. The same argument applies for π k . Therefore,</p><formula xml:id="formula_63">π s = sa 0 , π k = kb 0 , {a 0 , b 0 } = {i, j}.</formula><p>To conclude the argument, please note that, since from (25) we see that C s,j = 0 if s / ∈ de G \j o (l), we can restrict ourselves to the case in which s ∈ de G \j o (l) ⊆ de(j). Since the graph is acyclic, this implies a 0 = i and b 0 = j.</p><p>We now prove that if for all the paths from l to i, there is an observed variable s 0 ̸ = i on the path, the quantity</p><formula xml:id="formula_64">s∈de G \k o (l) (-1) s+j Q∈P \j (l,s) a Q det([B o ] \(k,s),\(i,j) ),</formula><p>is equal to 0. We can rewrite the sum above as follows:</p><formula xml:id="formula_65">s∈de G \k o (l)</formula><p>Q∈P \j (l,s) P ∈ P(O\{i,j},O\{k,s}) (-1) s+j+σ P a Q a P .</p><p>(</p><formula xml:id="formula_66">)<label>27</label></formula><p>Let π 0 be a path from l to i that can be decomposed in the following way:</p><formula xml:id="formula_67">π0 l → • • • → s 0 Q0 → i,</formula><p>for some s 0 ∈ V. If s 0 is observed, the monomial associated with this path will appear in the sum twice in ( <ref type="formula" target="#formula_66">27</ref>); the first time when considering Q = Q 0 and s = s 0 while the second time when considering Q = π 0 and s = i. The last thing to prove is that the sign will differ in the two cases. This comes from the Leibniz expansion for the determinant, (21). Indeed, the sign associated with each monomial is the sign associated with the permutation in the Leibniz formula. In the first case, the associated permutation is the following:</p><formula xml:id="formula_68">σ 1 (a) =          a if a / ∈ {s 0 , i, j}, j if a = s 0 , s 0 if a = i, k if a = j,</formula><p>while for the second case, the associated permutation is:</p><formula xml:id="formula_69">σ 2 (a) =      a if a / ∈ {j, i}, j if a = i, k if a = j.</formula><p>As the two permutations can be obtained one from the other via the transposition corresponding to swapping the columns corresponding to j and s 0 , they have different signs.</p><p>The same argument also implies that all the elements in (27), involving s ∈ pa(i) cancel out allowing us to rewrite the sum as:</p><formula xml:id="formula_70">Q∈P \j (l,i) P ∈ P(O\{i,j},O\{i,k}) (-1) i+j+σ P a Q a P .</formula><p>With the same argument used to prove (26), one can see that the only element in P(O \ {i, j}, O \ {i, k}), is the system of paths that sends k to j directly and all the other elements remain fixed. This system of paths has a negative sign if k ̸ = j and a positive sign otherwise, implying that the sum is equal to:</p><formula xml:id="formula_71">(-1) i+j [I -A o,o ] j,k Q∈P \j (l,i) a Q ,</formula><p>which concludes the first part of the proof. We now prove (24). Using (5) we can write <ref type="bibr">[ Ão,l ]</ref> </p><formula xml:id="formula_72">i,h = [( Bo,o ) -1 ( Bo,l )] i,h = s∈O [( Bo,o ) -1 ] i,s [( Bo,l )] s,l .</formula><p>In particular, we are going to prove the following</p><formula xml:id="formula_73">[A σ o,o ] i,k = [A o,o ] i,k + s 1 ch(ls) (i)c js ls,i [I -A o,o ] js,k + r σ ,<label>(29)</label></formula><p>where σ s is the transposition that swaps j s with l s and r σ is either 0 or a polynomial of degree at least two in R[G A ]. We proceed by induction on n. If n is equal to 1, then we apply Lemma B.1 with r σ = 0.</p><p>In order to proceed with the induction step, let us define σ [s] = σ s • • • σ 1 so that we can write σ as σ k+1 • σ <ref type="bibr">[k]</ref> . From the induction, we know that</p><formula xml:id="formula_74">[A σ [n] o,o ] i,k = [A o,o ] i,k + n s=1 1 ch(ls) (i)c js ls,i [I -A o,o ] js,k + r σ [n] ,</formula><p>and by construction we can write</p><formula xml:id="formula_75">B σ = B σ [n] + C (n+1) where C (n+1) i,k =      Q∈P \j n+1 (ln+1,i) a Q if k = j n+1 and i ∈ de G \j o (l n+1 ), 0 otherwise.<label>(30)</label></formula><p>Following the same steps of the proof of Lemma B.1, with the only difference of using B σ in place of B′ , and B σ [n] in place of B ′ we obtain</p><formula xml:id="formula_76">[A σ ] i,k = [A σ [n] ] i,k + s (-1) s+jn+1 C (n+1) s,jn+1 det([B σ [n] o ] /(k,s),/(i,jn+1) ).<label>(31)</label></formula><p>Thus, the only thing that is left to prove is that the last term on the right-hand side of the equation is equal to</p><formula xml:id="formula_77">1 ch(ln+1) (i)c jn+1 ln+1,i [I -A o,o ] jn+1,j + r σn+1 .</formula><p>In order to do so note that using the same formula as above we can write B</p><formula xml:id="formula_78">σ [n] o = B σ [n-1] o + C n , and thus det([B σ [n] o ] /(k,s),/(i,jn+1) ) = det([B σ [n-1] o ] /(k,s),/(i,jn+1) ) + r ′ σ [n-1] .</formula><p>Plugging the above equation in (31), concludes the proof, following the same steps as in the proof of Lemma B.1.</p><p>Proof of Theorem 3.8. The only difference with respect to Theorem 3.3 is the condition ch(l) \ ch(k 1 ) = ∅, ∀k 1 ∈ pa(k) ∪ {k}, that comes from Theorem 3.6. Indeed, if this condition is not satisfied, then the permutation that swaps the columns corresponding to k and l, cannot result in a model in R G .</p><p>Proof of Theorem 3.9. As in the proof of Theorem 3.8, the difference from the conditions of Theorem 3.4 and Theorem 3.9 is condition (20). This is, again, a direct consequence of Theorem 3.6.</p><p>Proof of Corollary 3.10. The mixing matrix B o is identifiable if and only if all of its parameters are. Hence, the result is a direct consequence of Theorem 3.8.</p><p>Example B.3 (Examples 3.5 and 3.7 continued). Here, we report the mixing matrices corresponding to the two models that are compatible with the observed distribution</p><formula xml:id="formula_79">B ′ =   1 0 0 0 b T I 1 1 0 b T I b Y T b Y T b Y T + b LY 1   , B′ =   1 0 0 0 b T I 1 1 0 b T I b Y T b Y T + b LY b Y T 1   ,</formula><p>together with the corresponding adjacency matrices</p><formula xml:id="formula_80">A o,o =   1 0 0 b T I 1 0 0 b Y T 1   , Ão,o =   1 0 0 b T I 1 0 -b T I b Y L b Y T + b Y L 1   .</formula><p>The two models are depicted in Figure <ref type="figure" target="#fig_0">12</ref>. A parameter is generically identifiable without knowledge of the graph if it is the same for both models. Since there is only one model that is compatible with G IV , all the parameters are generically identifiable with knowledge of the graph. These results are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><formula xml:id="formula_81">G IV : I T Y L b T I b Y T 1 b Y L GIV : I T Y L b T I -b T I b Y L b Y T + b Y L 1 -b Y L Figure 12</formula><p>. The graphs corresponding to the two models. </p><formula xml:id="formula_82">I → T ✓ ✓ ✓ ✓ I → Y ✓ ✓ ✓ ✗ T → Y ✓ ✓ ✗ ✗ B.</formula><p>3. Proofs of Section 3.4</p><p>Theorem B.4. Algorithms 1, 2, 3, 4, 5, and 6 are sound and complete for solving the corresponding identifiability queries, as summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Moreover, the computational complexity the algorithms is Proof of Theorem 3.11 and Theorem B.4. In order to prove that correctness of Algorithm 1, it is enough to show that if de o (l) = de o (j) then j &lt; k for every k ∈ ch(l) \ {j}. Assume by contradiction that there is k &lt; j in ch(l). Then k cannot be a descendent of j and so de o (l) ̸ = de o (j). This implies that the total causal effect is identifiable if the condition at line 7 : in Algorithm 1 is not satisfied for any latent variable. The rest of the algorithm is just a translation of the conditions in Theorem 3.8.</p><formula xml:id="formula_83">O(p l (p 2 o + |E|)) = O(p 3 ).</formula><p>The nodes can be arranged in a topological order in O(p + |E|) time with one run of depth-first search, see e.g., <ref type="bibr">Cormen et al. (2009, §22.4)</ref>. Computing the descendants of a node can be done again with depth-first search in the graph in which only the observed nodes are considered since we assume no edges from observed to latent variables. ). The result for Theorem B.4 follows in the same way. Note that a consequence of Lemma A.4 is that when one is interested in identifying the whole matrix, it is not necessary to distinguish between direct and total causal effects. We used Algorithm 5 and Algorithm 6 on randomly generated graphs. In Figure <ref type="figure" target="#fig_2">5</ref>, we report the percentage of graphs in which all the causal effects are identifiable for graphs of size 10 and 100. The graphs are generated according to an Erdős-Rényi model in which we ensure that the graph we sample is canonical. The probability of acceptance of an edge is plotted on the x-axis. For each setup, we randomly sample 500 graphs.</p><p>For the case in which the graph is known, we find the same qualitative behavior observed in Section 5.1. In contrast, when we do not assume the graph to be known, the probability that all the parameters in the graph are identifiable drops drastically. We found the same qualitative behavior with larger graphs.</p><p>The average running time for different graph sizes is shown in Figure <ref type="figure" target="#fig_0">14</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Estimation</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Total Causal Effect Identification with Known GraphINPUT: V = O ∪ L, G, {ch(i) | i ∈ V}, (j, i) 1: ID ← TRUE 2: Sort Vaccording to an ascending topological order 3: Compute de o (j) 4: while ID == TRUE and |L| &gt; 0 do 5: l ← L[1]{The first element in the list} 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>linear non-Gaussian models, Salehkaleybar et al. (2020) proposed necessary and sufficient graphical criterion for the identifiability of the whole mixing matrix, Yang et al. (2022) defines a notion of equivalence class for lvLiNGAM models, and Cai et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. On the x-axis, the probability of acceptance of an edge. On the y-axis, the percentage of identifiable parameters.</figDesc><graphic coords="7,309.28,192.81,230.31,114.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The causal graphs considered in the experiments.</figDesc><graphic coords="8,92.13,147.32,169.63,137.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Relative error vs sample size</figDesc><graphic coords="8,90.11,301.63,173.66,137.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Relative error vs noise scaling ratio.</figDesc><graphic coords="8,346.94,201.59,164.00,117.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) One latent and five observed variables. (b) Two latent and ten observed variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Results for the Erdős-Rényi model.</figDesc><graphic coords="9,90.21,409.12,173.46,123.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Therefore, it has a cost of O(p o + |E|), sorting ch(l k ) in line 5 of the algorithm is O(p o log(p o )), while computing the parents of a node can be done in O(p 2 o ). Therefore, the internal loop that starts at line 4 has the complexity of O(p 2 o + |E|) and this is repeated at most p l times. Hence, the loop costs O(p l (p 2 o + |E|)). The final cost is O(p l (p 2 o + |E|) + p + |E|) = O(p l (p 2 o + |E|)) = O(p 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 2 Figure 13 .Figure 14 .</head><label>21314</label><figDesc>Figure 13. On the x-axis, the probability of acceptance of an edge. The y-axis shows the percentage of graphs in which all the parameters are identifiable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 15. G4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 20 .</head><label>20</label><figDesc>Figure 17. Exponential distribution</figDesc><graphic coords="24,335.86,385.47,192.46,153.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of the identifiable parameters in the IV graph.</figDesc><table><row><cell cols="2">Known DAG Unknown DAG</cell></row><row><cell>TCE DCE TCE</cell><cell>DCE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Summary of the identification algorithms, with the corresponding identifiability queries.</figDesc><table><row><cell></cell><cell cols="2">Known DAG</cell><cell cols="2">Unknown DAG</cell></row><row><cell></cell><cell>TCE</cell><cell>DCE</cell><cell>TCE</cell><cell>DCE</cell></row><row><cell>Given Pair</cell><cell cols="4">Algorithm 1 Algorithm 3 Algorithm 2 Algorithm 4</cell></row><row><cell cols="5">Complete Matrix Algorithm 5 Algorithm 5 Algorithm 6 Algorithm 6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Technical University of Munich, Munich, Germany</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>EcolePolytechnique Fédérale de Lausanne, Lausanne, Switzerland</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Leiden Institute of Advanced Computer Science, Leiden University, Netherlands</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Munich Center for Machine Learning, Munich, Germany. Correspondence to: Daniele Tramontano &lt;daniele.tramontano@tum.de&gt;. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4"><p>Although our results are presented for the lvLiNGAM model, our analysis only relies on the identifiability of the mixing matrix B ′ up to permutation and scaling. This can be also achieved in other settings as explained inYang et al. (2022,  §2.3)  orAdams  et al. (2021,  §3). Hence, our results also hold in these settings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>Please seePearl (2009,  §3)  for the definition of do intervention.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This project has received funding from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement No <rs type="grantNumber">883818</rs>) and supported in part by the <rs type="funder">SNF</rs> project <rs type="grantNumber">200021 204355/1</rs>, <rs type="projectName">Causal Reasoning Beyond Markov Equivalencies. DT</rs>'s PhD scholarship is funded by the <rs type="funder">IGSSE/TUM-GS</rs> via a <rs type="institution">Technical University of Munich-Imperial College London Joint Academy of Doctoral Studies</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QSw9gYn">
					<idno type="grant-number">883818</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funded-project" xml:id="_euzsbS8">
					<idno type="grant-number">200021 204355/1</idno>
					<orgName type="project" subtype="full">Causal Reasoning Beyond Markov Equivalencies. DT</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notions of Non-Linear Algebra</head><p>In this section, we give the basic definitions of non-linear algebra we will need for the proofs; we refer the interested reader to <ref type="bibr">Cox et al. (2015)</ref>; <ref type="bibr" target="#b31">Michałek &amp; Sturmfels (2021)</ref> for more details.</p><p>Definition A.1. For every natural number n, we denote the ring of polynomials in n variables x 1 , . . . , x n by R[x 1 , . . . , x n ]. Let S be a, possibly infinite, subset of R[x 1 , . . . , x n ]. The affine variety associated to it is defined as V(S) = {x ∈ R n | f (x) = 0, ∀f ∈ S}. The vanishing ideal associated to a variety V is I(V) = {f ∈ R[x 1 , . . . , x n ] | f (x) = 0 ∀x ∈ V}. The coordinate ring of V is defined as R[V] = R[x 1 , . . . , x n ]/I(V).</p><p>Lemma A.2 (Lemma <ref type="bibr" target="#b34">(Okamoto, 1973)</ref>). Let f (x 1 , . . . , x n ) be a polynomial in real variables x 1 , . . . , x n , which is not identically zero. The set of zeros of the polynomial is a Lebesgue measure zero subset of R n .</p><p>Remark A.3 (Notation). For a given matrix A, we denote the submatrix in which the i-th row and the j-th column are excluded by</p><p>, where with the symbol ∼, we denote an isomorphism of affine varieties, see, e.g., <ref type="bibr">Cox et al. (2015, Def. 6, §5)</ref> </p><p>and R[a i,j | j → i ∈ G] are isomorphic as rings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The isomorphism R G</head><p>A ∼ R |e| comes directly from its definition. Indeed it is easy to see that R G A is an |e|-dimensional linear subspace of R p×p = (a i,j ) i,j∈p×p , defined by the linear equations a i,i = 1, and a i,j = 0, ∀i, j ∈ V such that j → i / ∈ G.</p><p>To prove the isomorphism R G ∼ R G A , we need to prove that there is a polynomial bijective map between the two spaces. From (5), and using</p><p>Let us call this polynomial map ψ and assume ψ(A) = ψ( Ã). Then from the definition of ψ we have</p><p>Ão,l that implies A o,l = Ão,l and so A = Ã.</p><p>The isomorphisms between the rings come from <ref type="bibr">Cox et al. (2015, §5, Thm. 9</ref>).</p><p>Corollary A.5. Let f ∈ R[G] be a non-zero polynomial. Then the subset of R G on which f vanishes is a Lebesgue measure 0 subset of R G .</p><p>Proof. Thanks to the isomorphism in Lemma A.4, we can apply Lemma A.2 to R G .</p><p>Definition A.6 (Leibniz Expansion of the Determinant). For any M ∈ R n×n , the determinant of M can be computed using the following formula:</p><p>where S(n) is the set of all the permutations of n elements, and (-1) σ P is the sign of the permutation. See, e.g., <ref type="bibr">Axler (2015, Def. 10.33</ref>) for more details.</p><p>Definition A.7. Let π ∈ P(j, i). The path monomial associated to it is defined as</p><p>Lemma A.8. Let A defined as in (1). We have</p><p>We first prove the case h = l. In this case, by definition of B′ , we have</p><p>where, again, we used [A] j,l = 1, that comes from Remark 3.2. For the case h ̸ = l we use again ( <ref type="formula">23</ref>), and write</p><p>where for the last equality we only used A o,l = (B o ) -1 B l that comes from (5).</p><p>Proof of Theorem 3.4. From Lemma B.1, we can conclude that if there are no k and l satisfying the conditions in Equations ( <ref type="formula">11</ref>)-( <ref type="formula">13</ref>), then the entry [A o,o ] i,j remains unchanged when swapping the columns of B ′ , proving that the condition is sufficient. To prove the necessity, it is enough to show that</p><p>which is equivalent to proving that P \j (l, i) ̸ = ∅. This is true since i ∈ ch(l) from Equation ( <ref type="formula">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proofs of Section 3.2</head><p>Lemma B.2. For every B ′ outside of a Lebesgue zero subset of R G , let B′ = B ′ • P σ , where σ is the transposition that swaps j and l, and P σ is the associated permutation matrix. We have B′ ∈ R G if and only if</p><p>and there is no</p><p>Proof of Lemma B.2. From <ref type="bibr">Salehkaleybar et al. (2020, Thm. 15)</ref>, we know that the only permutations that result in matrices in R G for some DAG G, are the ones for which</p><p>Hence this condition is necessary.</p><p>The edges in G are given by the support of the matrix Ã. In particular G = G if and only if</p><p>We provide the proof only for A o,o since the one for A o,l follows the same argument only using (24) instead of (23).</p><p>Thus, the only the entries to consider are i ∈ ch(l) and k ∈ pa(j) ∪ {j}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We know that</head><p>Therefore, the condition in (28) fails if and only if there is an i ∈ ch(l) \ ch(j).</p><p>Proof of Theorem 3.6. In Lemma B.2, we have shown that the statement is true if σ is a transposition. We now assume that σ = σ n • • • σ 1 where σ s is a transposition for every s ∈ {1, . . . , n}, such that if σ s (j) ̸ = j for some j then also σ(j) ̸ = j. This can be done without loss of generality since every permutation can be written this way.</p><p>We can see that the condition is sufficient from Lemma B.2; if it is satisfied, the support of A σ cannot change. In order to prove that the condition is also necessary, we need to verify that if there is a σ s such that [A σs ] i,k ̸ = 0 then [A σ ] i,k ̸ = 0 as well. Again, we will prove the result for a pair of observed variables i and k since the result follows the same way when considering latent variables. Experimental setup. All the experiments in this subsection are done on the synthetic data generated according to the specific causal structure established for it. To generate synthetic data we specify all exogenous noises to be i.i.d., and select all non-zero entries within the matrix A through uniform sampling from [-1, -0.5] ∪ [0.5, 1]. In the following, we display the results for the setups described in Table <ref type="table">3</ref>. </p><p>On the figures, we plot the average relative error over 10 independent experiments, with its standard deviation visualized with a transparent area filled with the respective color. From the experiments, we see that GRICA outperforms other methods in all setups, except the one for the graph G 1 . Note that for this specific graph, the Cross-Moment method gives us an exact statistical solution that requires computing high-order moments. This results in a less statistically robust estimation when the sample size is small, but it is more accurate if the sample size is large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2. MODEL MISSPECIFICATION</head><p>Here we consider the robustness of GRICA algorithm when data generation process is not linear SEM, but close to it. For these experiments the data is generated with the following machanism:</p><p>where T → Y is a direct causal effect that we want to estimate. In the following, we display the results for the setups described in Table <ref type="table">4</ref>. </p><p>On the figures, we plot the average relative error over 10 independent experiments, with its standard deviation visualized with transparent area filled with respective color. We observe that GRICA outperforms all other methods for these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3. RELATIVE ERROR VS OBSERVATIONS NOISE</head><p>These experiments illustrate how the variances of certain exogenous noise impact the performance of GRICA algorithm in comparison to the state-of-the-art algorithms for the considered settings. Herein we considered three main setups. In each of the setup we initialize all the non-zero entries of matrix A to be equal to 1 and all exogenous noises are modeled as i.i.d. distributions.</p><p>1. Causal structure: G 1 . The experiment for this causal structure is illustrated in Figure <ref type="figure">20a</ref>. In this experiment, we initialize all exogenous noises as Exponential distributions. Then we scaled the standard deviation of exogenous noise N W corresponding to the variable W by a factor displayed on x-axis of the Figure <ref type="figure">20a</ref> and plotted the performance of GRICA algorithm against the performance of RICA algorithm.</p><p>2. Causal structure: G 3 . The experiment for this causal structure is illustrated in Figure <ref type="figure">20b</ref>. In this experiment, we initialize all exogenous noises as Exponential distributions. Then we scaled N W and N Z by Ratio and 1/Ratio, respectively, where Ratio is plotted on the x-axis of Figure <ref type="figure">20a</ref> and plotted the performance of GRICA algorithm against the performance of RICA, Cross-Moment method and method proposed by <ref type="bibr" target="#b46">Tchetgen et al. (2020)</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential underspecified instrument selection for cause-effect estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
		</imprint>
		<respStmt>
			<orgName>Proceedings of Machine Learning Research. PMLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The state of applied econometrics: Causality and policy evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Linear algebra done right</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>rd ed. edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Half-trek criterion for identifiability of latent variable models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sturma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The link between the two epidemics provides an opportunity to remedy obesity while dealing with covid-19</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miljkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Policy Modeling</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized instrumental variables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;02</title>
		<meeting>the Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;02</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphical condition for identification in recursive sem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06</title>
		<meeting>the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal discovery with latent confounders based on higher-order cumulants</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning, ICML&apos;23</title>
		<meeting>the 40th International Conference on Machine Learning, ICML&apos;23</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Proprioceptive short-term memory in passive motor learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chiyohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-I</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imamizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Does exporting cause productivity growth? evidence from chilean firms. Structural Change and Economic Dynamics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ciarli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moneta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ideals, varieties, and algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Undergraduate Texts in Mathematics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to computational algebraic geometry and commutative algebra</title>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>fourth edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Causal inference: The mixtape</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Yale university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M L</forename><surname>De Prado</surname></persName>
		</author>
		<title level="m">Causal Factor Investing: Can Factor Investing Become Scientific</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algebraic problems in structural equation modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th anniversary of Gröbner bases</title>
		<meeting><address><addrLine>Japan, Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">77</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global identifiability of linear structural equation models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sullivant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifiability, separability, and uniqueness of linear ica models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koivunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Special Section on Probabilistic Rough Sets and Special Section on PGM&apos;06</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identifiability of latent-variable and structural-equation models: From linear to nonlinear</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">When Should We Use Unit Fixed Effects Regression Models for Causal Inference with Longitudinal Data?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A crossmoment approach for causal effect estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyseventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient identification in linear structural causal models with auxiliary cutsets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kumor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measurement bias and effect restoration in causal inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models by independent components analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI&apos;08</title>
		<meeting>the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI&apos;08</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ica with reconstruction cost for efficient overcomplete feature learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000-02">02 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Proximal causal inference for synthetic control with surrogates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Varjão</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09527</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Handbook of Graphical Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<editor>Wainwright, M.</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Chapman &amp; Hall/CRC Handbooks of Modern Statistical Methods. CRC Press</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Invitation to nonlinear algebra</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">211</biblScope>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
	<note>Graduate Studies in Mathematics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal inference in drug discovery and development</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the identifiability of sparse ica without assuming non-gaussianity</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distinctness of the eigenvalues of a quadratic form in a multivariate sample</title>
		<author>
			<persName><forename type="first">M</forename><surname>Okamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning sparse codes with a mixture-of-gaussians prior</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 12, [NIPS Conference</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999-12-04">November 29 -December 4, 1999. 1999</date>
			<biblScope unit="page" from="841" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<title level="m">Models, reasoning, and inference</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Principles and strategies for developing network models in cancer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hacohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jacobian-based causal discovery with nonlinear ICA</title>
		<author>
			<persName><forename type="first">P</forename><surname>Reizinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning linear non-gaussian causal models in the presence of latent variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">39</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Causal machine learning for healthcare and precision medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Voisey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Statistical Causal Discovery: LiNGAM Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">When does the id algorithm fail?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03750</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>AAAI&apos;06</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Identification and estimation of causal effects using nongaussianity and auxiliary covariates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14895</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An introduction to proximal causal learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Parameter identification in linear non-gaussian causal models under general confounding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tramontano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Etesami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.20856</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal discovery with unobserved confounding and non-gaussian data</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">271</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
