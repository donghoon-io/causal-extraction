<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Multi-Step Critiquing for VAE-based Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-07">7 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Diego</forename><surname>Antognini</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DIEGO ANTOGNINI and BOI FALTINGS</orgName>
								<orgName type="institution">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DIEGO ANTOGNINI and BOI FALTINGS</orgName>
								<orgName type="institution">Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Multi-Step Critiquing for VAE-based Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-07">7 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3460231.3474249</idno>
					<idno type="arXiv">arXiv:2105.00774v2[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational Recommendation</term>
					<term>Critiquing</term>
					<term>Variational Autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown that providing personalized explanations alongside recommendations increases trust and perceived quality. Furthermore, it gives users an opportunity to refine the recommendations by critiquing parts of the explanations. On one hand, current recommender systems model the recommendation, explanation, and critiquing objectives jointly, but this creates an inherent trade-off between their respective performance. On the other hand, although recent latent linear critiquing approaches are built upon an existing recommender system, they suffer from computational inefficiency at inference due to the objective optimized at each conversation's turn. We address these deficiencies with M&amp;Ms-VAE, a novel variational autoencoder for recommendation and explanation that is based on multimodal modeling assumptions. We train the model under a weak supervision scheme to simulate both fully and partially observed variables. Then, we leverage the generalization ability of a trained M&amp;Ms-VAE model to embed the user preference and the critique separately. Our work's most important innovation is our critiquing module, which is built upon and trained in a self-supervised manner with a simple ranking objective. Experiments on four real-world datasets demonstrate that among state-of-the-art models, our system is the first to dominate or match the performance in terms of recommendation, explanation, and multi-step critiquing.</p><p>Moreover, M&amp;Ms-VAE processes the critiques up to 25.6x faster than the best baselines. Finally, we show that our model infers coherent joint and cross generation, even under weak supervision, thanks to our multimodal-based modeling and training scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems accurately capture user preferences and achieve high performance. However, they offer little transparency regarding their inner workings. It has been shown that providing explanations along with item recommendations enables users to understand why a particular item has been suggested and hence to make better decision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Additionally, explanations increase the system's overall transparency and trustworthiness <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>An important advantage of explanations is that they provide a basis for feedback. If users understand what has generated the suggestions, they can refine the recommendations by interacting directly with the explanations. Critiquing is a conversational recommendation method that incrementally adapts recommendations in response to user preferences <ref type="bibr" target="#b7">[8]</ref>. Example critiquing was introduced in information retrieval <ref type="bibr" target="#b43">[44]</ref> and first applied to recommender systems in <ref type="bibr" target="#b4">[5]</ref>. Recognizing that critiquing is most useful when applied in multiple steps, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b28">[29]</ref> introduced mechanisms based on constraint programming <ref type="bibr" target="#b40">[41]</ref> with an application to travel planning. Multi-step critiquing with constraint programming was recognized as a form of preference elicitation, which enabled the analysis and optimization of its performance <ref type="bibr" target="#b12">[13]</ref> and the addition of suggestions for active preference elicitation <ref type="bibr" target="#b42">[43]</ref>, which yielded dramatic improvements in decision accuracy in user studies. Multi-step critiquing was also shown to be superior to compound Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</p><p>â€¢ ğ‘ˆ , ğ¼ , and ğ¾: The user, the item, and the keyphrase sets, respectively.</p><p>â€¢ ğ‘¹ âˆˆ R |ğ‘ˆ |Ã— |ğ¼ | : The user-by-item interaction matrix obtained with implicit feedback. Entries ğ’“ ğ‘¢,ğ‘– of 1 (respectively 0) denote a positive (respectively negative or unobserved) interaction between the user ğ‘¢ and item ğ‘–.</p><p>â€¢ ğ‘² âˆˆ R |ğ‘ˆ |Ã— |ğ¾ | : The binary user-keyphrase matrix that reflects the user ğ‘¢'s keyphrase-usage preference. Given user reviews from a corpus, we extract keyphrases that describe item attributes from all reviews (see Section 4.1).</p><p>â€¢ ğ‘² ğ‘° âˆˆ R |ğ¼ |Ã— |ğ¾ | : The binary item-keyphrase matrix. The process is similar to ğ‘² with the aggregation per item.</p><p>â€¢ rğ‘¢ âˆˆ R |ğ¼ | and kğ‘¢ âˆˆ R |ğ¾ | : The predicted feedback and keyphrase explanation, respectively.</p><p>â€¢ ğ’› ğ‘¢ âˆˆ R |ğ» | : The user ğ‘¢'s latent embedding of dimension ğ» from the observed interaction ğ’“ ğ‘¢ and keyphrase-usage preference ğ’Œ ğ‘¢ .</p><p>â€¢ ğ’„ ğ‘¡ ğ‘¢ âˆˆ R |ğ¾ | : A one-hot vector of length |ğ¾ |. The only positive value indicates the index of the keyphrase to be critiqued by the user ğ‘¢ at a given step ğ‘¡ of the user interaction with the recommender system.  ğ¼  ğ‘–,ğ’„ = 0, âˆ€ğ‘– âˆˆ ğ¼ }: The set of items that do not contain the critiqued keyphrase ğ’„.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Autoencoder for Recommendation (VAE)</head><p>A variational autoencoder (VAE) <ref type="bibr" target="#b19">[20]</ref> is a generative model of the form ğ‘ ğœƒ (ğ’™, ğ’›) = ğ‘ (ğ’›)ğ‘ ğœƒ (ğ’™ |ğ’›), where ğ‘ (ğ’›) is a prior and the likelihood ğ‘ ğœƒ (ğ’™ |ğ’›) is parametrized by a neural network with parameters ğœƒ . The model learns to maximize the marginal likelihood of the data ğ‘ ğœƒ (ğ’™) (i.e., the evidence that is intractable) by approximating the true unknown posterior ğ‘ ğœƒ (ğ’›|ğ’™) with a variational posterior ğ‘ ğœ™ (ğ’›|ğ’™). Applied to recommendation systems, the collaborative-filtering VAE <ref type="bibr" target="#b22">[23]</ref> considers as input data the sparse user preferences ğ’“ ğ‘¢ over |ğ¼ | items. More formally, the model optimizes a variational lower bound on the log likelihood of all observed user feedback ğ‘¢ âˆˆğ‘ˆ log ğ‘ (ğ’“ ğ‘¢ ) through stochastic gradient descent:</p><formula xml:id="formula_0">log ğ‘ (ğ’“ ğ‘¢ ) â‰¥ âˆ« ğ’› ğ‘¢ ğ‘ ğœ™ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) log ğ‘ ğœƒ (ğ’“ ğ‘¢ , ğ’› ğ‘¢ ) ğ‘ ğœ™ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) ğ‘‘ğ’› ğ‘¢ â‰¥ E ğ‘ ğœ™ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) log ğ‘ ğœƒ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ ğœ™ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ ) ,<label>(1)</label></formula><p>where ğ’› ğ‘¢ is sampled<ref type="foot" target="#foot_0">foot_0</ref> from the distribution ğ‘ ğœ™ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) with parameters ğ ğ‘¢ and ğšº ğ‘¢ , and D KL [ğ‘, ğ‘] denotes the Kullback-Leibler divergence (KL) between the distributions ğ‘ and ğ‘. In practice, the prior ğ‘ (ğ’›) is usually a spherical Gaussian with parameters ğ and ğšº. Finally, ğ›½ is a hyperparameter that controls the strength of the regularization relative to the reconstruction error, as motivated by the ğ›½-VAE of <ref type="bibr" target="#b17">[18]</ref>, and is slowly annealed to 1, similarly to <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Co-embedding of Language-based Feedback with the Variational Autoencoder (CE-VAE)</head><p>Thus far, the variational autoencoder can only recommend items without generating any form of explanation. A recent study <ref type="bibr" target="#b25">[26]</ref> proposed the CE-VAE model, which integrates an explanation and critiquing module based on keyphrases.</p><p>The authors support critiquing by first modeling the joint probability of a user's item preferences and keyphrase usage:</p><formula xml:id="formula_1">log ğ‘ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = log ğ‘ (ğ’Œ ğ‘¢ |ğ’“ ğ‘¢ ) + log ğ‘ (ğ’“ ğ‘¢ ) = E ğ‘ Î¦ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) log ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -D KL ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ )<label>(2)</label></formula><formula xml:id="formula_2">+ E ğ‘ Î¨ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) log ğ‘ Î˜ ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) + H ğ‘ Î¨ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) + E ğ‘ Î¨ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) ğ‘ (ğ’› ğ‘¢ ) ,</formula><p>where H is the entropy. Then, they incorporate an additional objective to learn a projection from the critiquing feedback into the latent space via another encoder (an inverse feedback loop). In other words, they reintroduce the user's keyphrase usage ğ’Œ ğ‘¢ to approximate the variational lower bound of ğ‘ (ğ’› ğ‘¢ ) by marginalizing over ğ’Œ ğ‘¢ . More formally:</p><formula xml:id="formula_3">log ğ‘ (ğ’› ğ‘¢ ) â‰¥ E ğ‘ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) log ğ‘ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) -D KL ğ‘(ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) || ğ‘ (ğ’Œ ğ‘¢ )<label>(3)</label></formula><formula xml:id="formula_4">â‰ˆ E ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) log ğ‘ Î˜ â€² ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) -D KL ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) || ğ‘ (ğ’Œ ğ‘¢ ) ,</formula><p>where ğ‘ (ğ’Œ ğ‘¢ ) is a prior following a standard normal distribution and the weights of ğ‘(ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) are shared with ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ).</p><p>Finally, once the model is trained on the full objective function, the critiquing process for the critique ğ’„ ğ‘¢ is performed as follows: ( Overall, the CE-VAE framework is effective in practice for recommendation, keyphrase explanation, and single-step critiquing. However, it suffers from two key deficiencies that limit its performance (as we later show empirically):</p><p>(1) The model learns a function to project the critiqued keyphrase into the user's latent space, from which the feedback and the explanation are predicted. This mapping is learned via an autoencoder, which perturbs the training. Thus, there is an inherent trade-off between the performance of the recommendation and that of the explanation.</p><p>(2) Although the joint objective also maximizes a latent representation likelihood with the Kullback-Leibler terms, it is unclear whether the inverse function embeds the critique effectively and whether the mean reflects a critiquing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">M&amp;MS-VAE: A MIXTURE-OF-EXPERTS MULTIMODAL VARIATIONAL AUTOENCODER</head><p>Our goal is to build a more generalizable representation of users' preferences that is based on their observed interactions and keyphrase usage. Figure <ref type="figure">1</ref> depicts the graphical model of our proposed M&amp;Ms-VAE, and Figure <ref type="figure">2</ref> shows the training scheme. Then, we leverage this representation to efficiently embed the user critiques and learn, in a self-supervised fashion, a blending module to re-rank recommended items for multi-step critiquing. Figure <ref type="figure">3</ref> illustrates the workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Like previously developed variational autoencoders for recommendation, we assume that the observed user ğ‘¢'s interactions ğ’“ ğ‘¢ and the keyphrase-usage preference ğ’Œ ğ‘¢ are generated from a latent representation of the user preferences.</p><p>Differently from prior work, we seek to learn the joint distribution ğ‘ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) under weak supervision. Our main goal is to learn a more generalizable representation of the user preferences. Therefore, we aim to design a generative model that can recommend and generate keyphrase explanation jointly but also independently from each of the observed variables (i.e., cross-modal generation). It also allows us to apply the same technique to users who have not written reviews or to cases in which keyphrases are unavailable. If this goal is achieved, we can then embed effectively the user's observed interactions, the user's keyphrase preference, and the critique with the same inference network ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ).</p><p>Inspired by multimodal generative models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>, we treat ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ as different modalities, and we assume they are conditionally independent given the common latent variable ğ’› ğ‘¢ . In other words, we assume a generative model</p><formula xml:id="formula_5">of the form ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ ) = ğ‘ (ğ’› ğ‘¢ )ğ‘ Î˜ ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ )ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ).</formula><p>An advantage of such a factorization is that if ğ’“ ğ‘¢ or ğ’Œ ğ‘¢ is unobserved, we can safely ignore it when evaluating the marginal likelihood <ref type="bibr" target="#b46">[47]</ref>.</p><p>We start with the derivation of the joint log likelihood ğ‘¢ âˆˆğ‘ˆ log ğ‘ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) over the observed interactions ğ’“ ğ‘¢ and keyphrase-usage preference ğ’Œ ğ‘¢ and all users ğ‘¢ as shown in Figure <ref type="figure">1</ref>:  </p><formula xml:id="formula_6">log ğ‘ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = log âˆ« ğ’› ğ‘¢ ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ )ğ‘‘ğ’› ğ‘¢ â‰¥ E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ ) , (4) ğ’› ğ’“ ğ’Œ Î¦ ğ‘˜ Î¦ ğ‘Ÿ Î˜ ğ‘˜ Î˜ ğ‘Ÿ ğ‘¢ âˆˆ {1 . . . ğ‘ˆ } Fig.</formula><formula xml:id="formula_7">C ! D ! E " ! (F ! |H ! ) E " " (F ! |I ! ) H ! I ! C ! # , D ! # C ! $ , D ! $ 1) Full 5's Observation E " ! (F ! |H ! ) Missing K % H ! C ! # , D ! # 2) Only 5's Interactions Missing H ! E " " (F ! |I ! ) I ! C ! $ , D ! $ 3) Only 5's Keyphrase Usage F ! L &amp; " (I ! |F ! ) L &amp; ! (H ! |F ! ) M H ! N I ! Fig. 2.</formula><formula xml:id="formula_8">ğœ ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ), ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) = ğ›¼ â€¢ ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) + (1 -ğ›¼) â€¢ ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) with ğ›¼ = ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£³ 1 2 , if ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ are observed, 1, if only ğ’“ ğ‘¢ is observed, 0, if only ğ’Œ ğ‘¢ is observed.<label>(6)</label></formula><p>We set the weights uniformly to explicitly enforce an equal contribution from each ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ when both are observed during training. In the case of an unobserved modality, we shift the importance distribution toward the presented one, which generalizes to weakly supervised learning (see Section 3.2). This is an important factor, because the inference network ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) will later induce the critique representation. Finally, one might be tempted to learn ğ›¼ jointly with the variational lower bound or dynamically. However, doing so might miscalibrate the precisions of the ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) or</p><formula xml:id="formula_9">ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ )</formula><p>and thus be detrimental to the whole model in terms of both prediction performance and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Strategy</head><p>Combining Equations 5 and 6 gives the full objective function, and M&amp;Ms-VAE can be trained on a complete dataset where all ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ are provided. However, in doing so, we never train the individual inference networks ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) and ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ); only the relationship between the observed user interactions and keyphrase-usage preferences is captured. As a consequence, at inference, it is unclear how the model performs with a missing observation.</p><p>To reach our goal of recommending given at least ğ’“ ğ‘¢ and embedding the critique effectively with the inference network</p><formula xml:id="formula_10">MoE MoE âˆ€" Keyphrase Explainer ! ! " ! ! " "#$% # " &amp;' * ! " "#$% * Latent Space pool -center -beach ! " &amp;' ! " ! * Critiquing "<label>9</label></formula><formula xml:id="formula_11">1 st Forward Critiquing New Forward</formula><p>Rating Classifier for each target item ğ‘–, where ğ’“ val ğ‘¢,ğ‘– = 1 do 5:</p><formula xml:id="formula_12">Encoder ! * E " ! (F ! |H ! ) Missing K % H ! Past 5's Interactions Missing H ! E " " (F ! |I ! ) O ! ' 1 st critique Missing H ! E " " (F ! |I ! ) O ! ' 1 st critique Missing H ! E " " (F ! |I ! ) O ! ' 5's 1 st Critique F ! ( MoE F(â‹…) MoE ?(â‹…) F ! ' â‹® F ! ) Q Blending Module V(â‹…) S F ! ) L &amp; ! (H ! |F ! ) M H * + F ! ( L &amp; ! (H ! |F ! ) M H * , S F ! ' L &amp; ! (H ! |F ! ) M H * - â‹¯ L &amp; " (I ! |F ! ) N I ! Fig. 3. Workflow</formula><p>Randomly sample a critique ğ’„ âˆˆ ğ¾\ğ’Œ ğ¼ ğ‘– 6:</p><p>Compute the item sets ğ¼ +ğ’„ and ğ¼ -ğ’„</p><formula xml:id="formula_13">7: Update ğ· â† ğ· âˆª {(ğ‘¢, ğ‘–, ğ’„, ğ¼ +ğ’„ , ğ¼ -ğ’„ )} 8:</formula><p>return Synthetic dataset ğ· ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ), we propose a training strategy that mimics weakly supervised learning, similarly to <ref type="bibr" target="#b46">[47]</ref>. Moreover, this allows us to handle incomplete datasets, where some samples are partially observed: data that contain only ğ’“ ğ‘¢ or ğ’Œ ğ‘¢ . <ref type="foot" target="#foot_1">2</ref>The training strategy is shown in Figure <ref type="figure">2</ref>. For each minibatch, we compute the gradient on the evidence lower bound of the joint observation and each single observation ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ . Our final training objective for all users ğ‘¢ is</p><formula xml:id="formula_14">L (ğ‘¹, ğ‘² ) = âˆ‘ï¸ ğ‘¢âˆˆğ‘ˆ ğœ† â€¢ E ğ‘ Î¦ (ğ’›ğ‘¢ |ğ’“ğ‘¢ ,ğ’Œğ‘¢ ) log ğ‘ Î˜ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) + log ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) | | ğ‘ (ğ’› ğ‘¢ ) ğ¸ğ¿ğµğ‘‚ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) + âˆ‘ï¸ ğ‘¢âˆˆğ‘ˆ ğœ† â€¢ E ğ‘ Î¦ğ‘Ÿ (ğ’›ğ‘¢ |ğ’“ğ‘¢ ) log ğ‘ Î˜ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ Î¦ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) | | ğ‘ (ğ’› ğ‘¢ ) ğ¸ğ¿ğµğ‘‚ (ğ’“ ğ‘¢ ) + âˆ‘ï¸ ğ‘¢âˆˆğ‘ˆ ğœ† â€¢ E ğ‘ Î¦ ğ‘˜ (ğ’›ğ‘¢ |ğ’Œğ‘¢ ) log ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) | | ğ‘ (ğ’› ğ‘¢ ) , ğ¸ğ¿ğµğ‘‚ (ğ’Œ ğ‘¢ )<label>(7)</label></formula><p>where ğœ† and ğ›½ control the strength of the reconstruction error and regularization, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Supervised Critiquing with M&amp;Ms-VAE</head><p>The purpose of critiquing is to refine the recommendation rğ‘¢ based on the user ğ‘¢'s interaction with the explanation kğ‘¢ , represented with a binary vector. The user can accept the recommended items, at which point the session terminates. In the other case, the user can provide a critique ğ’„ ğ‘¡ ğ‘¢ and obtain a new recommendation rğ‘¡ ğ‘¢ . The process is repeated over ğ‘‡ iterations until the user ğ‘¢ is satisfied with the recommendation. Each critique ğ’„ ğ‘¡ ğ‘¢ is encoded as a one-hot vector where the positive value indicates a keyphrase the user ğ‘¢ dislikes. The overall process is depicted in Figure <ref type="figure">3</ref>.</p><p>We leverage the generalization ability of the trained M&amp;Ms-VAE, especially the inference models ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) and ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ). We use the former to represent the initial user preferences ğ’“ ğ‘¢ and the latter to embed the critique ğ’„ ğ‘¡ ğ‘¢ . However, a crucial question remains: how should we blend the user representation ğ’› ğ‘¢ with the ğ‘¡ th critique representation ğ’› ğ‘¡ ğ‘¢ ? Prior work has implemented a blending function as a simple average <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref> or as a linear programming task that looks for a convex combination of embeddings provided with a specific linear optimization objective <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. As we demonstrate empirically later, the former yields poor performance when iterated for multi-step critiquing, whereas the latter is computationally slow because the optimization is performed for each critique and it cannot leverage GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Blending Function Design.</head><p>We propose to learn a blending function ğœ‰ (â€¢) built upon a trained M&amp;Ms-VAE model whose weights are frozen. This two-step approach has several advantages:</p><p>(1) The original training of M&amp;Ms-VAE is not perturbed by the critiquing objective.</p><p>(2) ğœ‰ (â€¢) is decoupled from the model. It allows more flexibility in its architecture, objective function, and training.</p><p>We assume that each critique is independent, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>. At time step ğ‘¡, we express the new user preferences zğ‘¡ ğ‘¢ with a linear interpolation between the original latent representation ğ’› 0 ğ‘¢ and the critique ğ’„ ğ‘¡ ğ‘¢ 's representation ğ’› ğ‘¡ ğ‘¢ . More precisely, we use the gating mechanism of the gated recurrent unit <ref type="bibr" target="#b9">[10]</ref> (we omit the biases to reduce notational clutter):</p><formula xml:id="formula_15">zğ‘¡ ğ‘¢ = ğœ‰ (ğ’› 0 ğ‘¢ , ğ’› ğ‘¡ ğ‘¢ ) = â„ 2 â„ 2 = (1 -ğ‘¢ 1 ) â€¢ ğ‘› 1 + ğ‘¢ 1 â€¢ â„ 1 ğ‘› 1 = tanh(ğ‘Š ğ‘–ğ‘› ğ’› ğ‘¡ ğ‘¢ + ğ‘Š â„ğ‘› (ğ‘Ÿ 1 âŠ™ â„ 1 )</formula><p>)</p><formula xml:id="formula_16">ğ‘¢ 1 = ğœ (ğ‘Š ğ‘–ğ‘¢ ğ’› ğ‘¡ ğ‘¢ + ğ‘Š â„ğ‘§ â„ 1 ) ğ‘Ÿ 1 = ğœ (ğ‘Š ğ‘–ğ‘Ÿ ğ’› ğ‘¡ ğ‘¢ + ğ‘Š â„ğ‘Ÿ â„ 1 ) â„ 1 = (1 -ğ‘¢ 0 ) â€¢ ğ‘› 0 + ğ‘¢ 0 â€¢ â„ 0 ğ‘› 0 = tanh(ğ‘Š ğ‘–ğ‘› ğ’› 0 ğ‘¢ + ğ‘Š â„ğ‘› (ğ‘Ÿ 0 âŠ™ â„ 0 )) ğ‘¢ 0 = ğœ (ğ‘Š ğ‘–ğ‘¢ ğ’› 0 ğ‘¢ + ğ‘Š â„ğ‘§ â„ 0 ) ğ‘Ÿ 0 = ğœ (ğ‘Š ğ‘–ğ‘Ÿ ğ’› 0 ğ‘¢ + ğ‘Š â„ğ‘Ÿ â„ 0 ) ,<label>(8)</label></formula><p>where ğ‘Š ğ‘–ğ‘Ÿ ,ğ‘Š ğ‘–ğ‘¢ ,ğ‘Š ğ‘–ğ‘› ,ğ‘Š â„ğ‘Ÿ ,ğ‘Š â„ğ‘§ ,ğ‘Š â„ğ‘› , and the bias vectors are the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Training. Thanks to our assumption and the generalization ability of M&amp;Ms-VAE, we can learn the weights of the blending module ğœ‰ (â€¢) by creating a synthetic dataset based only on the validation set (see Algorithm 1). For each user and observed interaction, we randomly sample a keyphrase ğ’„ that is inconsistent with the target item. Then, we calculate the item sets ğ¼ +ğ’„ that contain the critique and, symmetrically, the item sets ğ¼ -ğ’„ for those that do not contain it. 3   Our final objective is to re-rank items based on the user preferences and the provided critique ğ’„. Recall that M&amp;Ms-VAE's weights are frozen. Let r0 ğ‘¢ be the user ğ‘¢'s initial predictions rğ‘¢ and r1 ğ‘¢ those inferred from zğ‘¡ ğ‘¢ after the critique. We express this overall ranking-based objective via two differentiable max-margin objective functions:</p><formula xml:id="formula_17">L ( R0 , R1 , ğ‘¢, ğ’„, ğ¼ +ğ’„ , ğ¼ -ğ’„ ) = âˆ‘ï¸ ğ‘– + âˆˆğ¼ +ğ’„ max 0, â„ -( r0 ğ‘¢,ğ‘– + -r1 ğ‘¢,ğ‘– + ) + âˆ‘ï¸ ğ‘– -âˆˆğ¼ -ğ’„ max 0, â„ -( r1 ğ‘¢,ğ‘– --r0 ğ‘¢,ğ‘– -) ,<label>(9)</label></formula><p>where â„ is the margin. Intuitively, ğœ‰ (â€¢) is encouraged to create a representation zğ‘¡ ğ‘¢ from which ğ‘ Î˜ ğ‘Ÿ (â€¢) gives a lower ranking to the items affected by the critique in the next iteration (i.e., r1 ğ‘¢,ğ‘– + &lt; r0 ğ‘¢,ğ‘– + ) and a higher ranking to the unaffected items (i.e., r1 ğ‘¢,ğ‘– -&gt; r0 ğ‘¢,ğ‘– -). Finally, Equation 9 is efficiently parallelizable on both CPUs and GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we proceed to evaluate the proposed M&amp;Ms-VAE model in order to answer the following questions:</p><p>â€¢ RQ 1: How does M&amp;Ms-VAE perform in terms of recommendation and explanation performance?</p><p>â€¢ RQ 2: Can M&amp;Ms-VAE with the self-supervised critiquing objective enable multi-step critiquing?</p><p>â€¢ RQ 3: What is our proposed critiquing algorithm's computational time complexity compared to prior work?</p><p>â€¢ RQ 4: How does M&amp;Ms-VAE perform under weak supervision; how coherent is the joint and cross generation?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the quantitative performance of M&amp;Ms-VAE using four real-world, publicly available datasets: BeerAdvocate <ref type="bibr" target="#b26">[27]</ref>, Amazon CDs&amp;Vinyl <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, Yelp <ref type="bibr" target="#b11">[12]</ref>, and HotelRec <ref type="bibr" target="#b0">[1]</ref>. Each contains more than 100k reviews with five-star ratings. For the purpose of Top-N recommendation, we binarize the ratings with a threshold ğ‘¡ &gt; 3.5. Because people tend to rate beers and restaurants positively, we set the threshold ğ‘¡ &gt; 4 and ğ‘¡ &gt; 4.5, respectively. We split each dataset 3 In early experiments, we generalized ğœ‰ (ğ’› 0 ğ‘¢ , ğ’› ğ‘¡ ğ‘¢ ) to ğœ‰ (ğ’› 0 ğ‘¢ , ğ’› 1 ğ‘¢ , . . . , ğ’› ğ‘¡ ğ‘¢ ) and updated Algorithm 1 accordingly. However, our synthetic dataset cannot cover such a space due to the exponential number of combinations; session-based recommenders require millions of real sessions as training data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.  <ref type="table" target="#tab_5">1</ref> shows the statistics of the datasets. All contain complete observations. The datasets do not contain preselected keyphrases. Hence, we extract the keyphrases for the explanations and critiquing with the frequency-based processing of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>. Some examples are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Across experiments, we treat the prior and the likelihood as standard normal and multinomial distributions, respectively.</p><p>The inference and generative networks consist of a two-layer neural network with a tanh nonlinearity as the activation function between the layers. We normalize the input and use dropout <ref type="bibr" target="#b37">[38]</ref>. For learning, we employ the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with AMSGrad <ref type="bibr" target="#b29">[30]</ref> and a learning rate of 5 â€¢ 10 -5 . We anneal linearly the regularization parameter ğ›½ of the Kullback-Leibler terms. For the baselines, we reused the authors' code and tuning procedure. We select hyperparameters and architectures for each model by evaluating NDCG on the validation set. We limit the search to a maximum of 100 trials. For critiquing, we tune our blending module on the synthetic dataset with the Falling MAP metric on the validation set, which measures the effect of a critique <ref type="bibr" target="#b44">[45]</ref>. For reproducibility purposes, we include additional details and the best hyperparameters in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RQ 1:</head><p>How does M&amp;Ms-VAE perform in terms of recommendation and explanation performance?</p><p>4.3.1 Baselines. We compare our proposed M&amp;Ms-VAE model to the following baseline models. POP returns the most popular items without any kind of personalization. AutoRec <ref type="bibr" target="#b35">[36]</ref> is a neural autoencoder-based recommendation system. BPR <ref type="bibr" target="#b31">[32]</ref> is a Bayesian personalized ranking model that explicitly optimizes pairwise rankings. CDAE <ref type="bibr" target="#b47">[48]</ref> denotes a collaborative denoising autoencoder that is specifically optimized for implicit feedback recommendation tasks. NCE-PLRec <ref type="bibr" target="#b45">[46]</ref> represents the linear recommendation projected by noise-contrastive estimation; it augments PLRec with noise-contrasted item embeddings. PLRec <ref type="bibr" target="#b34">[35]</ref> is the ablation variant of NCE-PLRec without the noise-contrastive estimation. PureSVD <ref type="bibr" target="#b10">[11]</ref> denotes a similarity-based recommendation method that constructs a similarity matrix through SVD decomposition of the implicit rating matrix. VAE-CF is the variational autoencoder for collaborative filtering described in Section 2.2. CE-VNCF <ref type="bibr" target="#b44">[45]</ref> is the extension of the neural collaborative filtering model <ref type="bibr" target="#b14">[15]</ref> that is augmented with an explanation and a critiquing neural component. Finally, CE-VAE <ref type="bibr" target="#b25">[26]</ref> is a significant improvement over CE-VNCF, and it produces state-of-the-art performance (more details in Section 2.3). For a fair comparison, we encode the user observations in M&amp;Ms-VAE using solely the inference network ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Top-N Recommendation</head><p>Performance. We report the following five metrics: R-Precision and NDCG; MAP, Precision, and Recall at different Top-N. The main results are presented in Table <ref type="table">2</ref>. We make the following key observations. Overall, our proposed M&amp;Ms-VAE model shows the best recommendation performance for all metrics on three datasets and nearly all metrics on the CDs&amp;Vinyl dataset. Compared to the original VAE recommender (VAE-CF), M&amp;Ms-VAE achieves an improvement of 13% on average. We conjecture that the additional loss terms (i.e., ğ¸ğ¿ğµğ‘‚ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) and Table <ref type="table">2</ref>. Top-N recommendation results of all datasets. Bold and underline denote the best and second-best results, respectively. We omit the error bars because the 95% confidence interval is in 4 th digit.  ğ¸ğ¿ğµğ‘‚ (ğ’Œ ğ‘¢ )) help to generate better user representations by leveraging both the user preferences and keyphrase usage with the mixture of experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP@N Precision</head><p>Table <ref type="table">3</ref>. Top-K keyphrase explanation quality of all datasets. Bold and underline denote the best and second-best results, respectively. We omit the error bars because the 95% confidence interval is in 4 th digit.</p><p>NDCG@K MAP@K Precision@K Recall@K  M&amp;Ms-VAE also significantly outperforms CE-VAE on the Yelp and Hotel datasets (by a factor of 1.9 and 1.7, respectively) and achieves an average improvement of 9% on the Beer and CDs&amp;Vinyl datasets. We remark the same trend with CE-VNCF. These results emphasize the noise introduced in CE-VAE and CE-VNCF during training when learning the mapping between the keyphrases and the latent space. This is even more pronounced with a large number of keyphrases (i.e., over 100). In contrast, M&amp;Ms-VAE is more robust thanks to our factorization and training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Interestingly, PureSVD exhibits the second-best performance on the CDs&amp;Vinyl and Yelp datasets. This shows that classic algorithms often remain competitive with state-of-the-art VAE-based recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Top-K Explanation</head><p>Performance. We also compare M&amp;Ms-VAE with the user and item-popularity baselines <ref type="bibr" target="#b44">[45]</ref> that predict the explanation through counting and ranking the frequency of keyphrases for the users (symmetrically, the items) in the training set. Among the recommender baselines, only CE-VAE and CE-VNCF produce an explanation alongside the recommendation. We report the following metrics: NDCG, MAP, Precision, and Recall at different Top-K. Table <ref type="table">3</ref> contains the main results. Both popularity baselines clearly underperform, showing that the task is not trivial (see Table <ref type="table" target="#tab_5">1</ref> for the number of keyphrases per dataset). Remarkably, the proposed M&amp;Ms-VAE model significantly outperforms the CE-VNCF baseline by a factor of 2.5 on average and by approximately by 3.5 on MAP and Precision. Moreover, M&amp;Ms-VAE retrieves 89% percent of relevant keyphrases within the Top-20 explanations on the CDs&amp;Vinyl dataset.</p><p>We observe that CE-VAE performs similarly to M&amp;Ms-VAE but still slightly underperforms by approximately 2% on average. Finally, we note that CE-VNCF achieves the best results in terms of Recall for the Beer dataset and Recall@5 for the CDs&amp;Vinyl and Hotel datasets. Nevertheless, as seen in the recommendation performance, CE-VNCF significantly underperforms, highlighting the trade-off between recommendation and explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RQ 2:</head><p>Can M&amp;Ms-VAE with the self-supervised critiquing objective enable multi-step critiquing? 4.4.1 Baselines. UAC <ref type="bibr" target="#b24">[25]</ref> denotes uniform average critiquing, in which the user embedding and all critique embeddings are averaged uniformly. BAC <ref type="bibr" target="#b24">[25]</ref> is balanced average critiquing. It first averages the critique embeddings and then averages them again with the initial user embedding. CE-VAE and CE-VNCF were introduced in Section 4.3.1. During training, both learn an inverse feedback loop between a critique and the latent space. At inference, they average the original user embedding with the critique embedding. LLC-Score <ref type="bibr" target="#b24">[25]</ref> and LLC-Rank <ref type="bibr" target="#b21">[22]</ref> first extend the PLRec recommender system <ref type="bibr" target="#b34">[35]</ref> to co-embed keyphrases into the user embedding space with a linear regression. Afterwards, the models apply a weighted average between the initial user embedding and each critique embedding; the weights are optimized in a linear programming formulation; LLC-Score uses a max-margin scoring-based objective and LLC-Rank a ranking-based objective. To limit computational complexity, the authors limit the number of constraints to the top-100 rated items. For a fair comparison, we also consider in M&amp;Ms-VAE the top-100 rated items meeting the criteria for ğ¼ +ğ’„ and ğ¼ -ğ’„ for each critique ğ’„, although the computational time remains identical using the full sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">User Simulation.</head><p>Similarly to prior work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, we conduct a user simulation to asses each model's performance in a multi-step conversational recommendation scenario. Concretely, the simulation considers all users and follows Algorithm 1 with the following differences: (1) we track the conversational interaction session of simulated users by selecting all target items from their test set, (2) the maximum allowed critiquing iterations is set to 10, and, (3) the conversation stops if the target item appears within the top-N recommendations on that iteration.</p><p>As in <ref type="bibr" target="#b21">[22]</ref>, we simulate a variety of user-critiquing styles. For each, the candidate keyphrases to critique are inconsistent (i.e., disjoint) with the target item's known keyphrase list. We experiment with the following three methodologies:</p><p>(1) Random: we assume the user randomly chooses a keyphrase to critique.</p><p>(2) Pop: we assume the user selects a keyphrase to critique according to the general keyphrase popularity.</p><p>(3) Diff: we assume the user critiques a keyphrase that deviates the most from the known target item description.</p><p>To do so, we compare the top recommended items' keyphrase frequency to the target item's keyphrases and select the keyphrase with the largest frequency differential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Multi-</head><p>Step Critiquing Performance. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, we asses the models over all users and all items on the test set using two metrics: the average success rate and session length. The former is the percentage of target items that successfully reach a rank within the Top-N, and the latter is the average length of these sessions (with a limit of 10 iterations). In our experiment framework, for each user and target item, we sample alongside 299 unseen items.</p><p>The results for each dataset and each keyphrase critiquing selection method are depicted in Figure <ref type="figure" target="#fig_2">4</ref>. Overall, all models' performance is generally better on the Beer and CDs&amp;Vinyl datasets due to the higher density in terms of the number of interactions. Generally, all models tend to find the target item within the Top-20 in more than half the time and under six turns. This highlights that in practice, users are likely to find the desired item in a limited amount of time.</p><p>Impressively, M&amp;Ms-VAE clearly outperforms all the baselines on both metrics on the Beer, CDs&amp;Vinyl, and Yelp datasets. On the Hotel dataset, the success rate is significantly higher for the Random and Pop cases and similar for the Diff case, whereas the session length is higher for the Pop and Diff selection methods. This is unsurprising, because the blending module is trained only once on the random keyphrase critiquing selection (i.e., no assumption on the user's behavior).</p><p>Although the simple self-supervision objective in M&amp;Ms-VAE mimics only one-step random critiquing, we remark that the training strategy generalizes for multi-step critiquing and other keyphrase critiquing selection as well. This</p><formula xml:id="formula_18">N =1 N =5 N =10 N =20 0.1 0.3 0.5 0.7 0.9 Success Rate â†‘ Random Keyphrase Critiquing Selection N =1 N =5 N =10 N =20 (a) Beer dataset. Pop Keyphrase Critiquing Selection N =1 N =5 N =10 N =20 Diff Keyphrase Critiquing Selection N =1 N =5 N =10 N =20 0.1 0.3 0.5 0.7 0.9 Success Rate â†‘ N =1 N =5 N =10 N =20 (b) CDsVinyl dataset. N =1 N =5 N =10 N =20 N =1 N =5 N =10 N =20 0.1 0.3 0.5 0.7 0.9 Success Rate â†‘ N =1 N =5 N =10 N =20 (c) Yelp dataset. N =1 N =5 N =10 N =20 N =1 N =5 N =10 N =20 0.1 0.3 0.5 0.7 0.9 Success Rate â†‘ N =1 N =5 N =10 N =20 (d) Hotel dataset. N =1 N =5 N =10 N =20</formula><p>shows that M&amp;Ms-VAE efficiently embeds the critique, thanks to the multimodal modeling.</p><p>We observe that the simple UAC and BAC methods perform similarly or better than CE-VNCF and CE-VAE. However, they are outperformed by LLC-Score, LLC-Rank, and M&amp;Ms-VAE. These results confirm our observation in Section 2.3 that the critiquing objective introduces noise during training and does not accurately reflect the critiquing mechanism.    significantly underperform on both metrics in 10 out of 12 cases. This highlights the effectiveness of our proposed critiquing algorithm compared to linear aggregation methods.</p><p>4.5 RQ 3: What is the critiquing computational time complexity for M&amp;Ms-VAE compared to prior work?</p><p>Now, we aim to empirically determine how the critiquing in M&amp;Ms-VAE compares to the best baselines in Section 4.4, LLC-Score and LLC-Rank, in terms of computational time. Because the baselines can not leverage the GPU due to their optimization framework, we also run M&amp;Ms-VAE on the CPU. We follow the same experiment settings as in Section 4.4</p><p>and limit ourselves to 1,000 users and the Random keyphrase critiquing selection method. All models process exactly 10 critiques for each user-item pair. We employ a machine with a 2.5GHz 24-core CPU, Titan X GPU, and 256GB memory.</p><p>Figure <ref type="figure" target="#fig_1">5</ref> shows the average computational time over 10 runs. Particularly, the Figure <ref type="figure" target="#fig_5">5a</ref> denotes the critiquing computational time in milliseconds, and Figure <ref type="figure" target="#fig_5">5b</ref> the overall simulation time in minutes. Impressively, we observe that the critiquing in M&amp;Ms-VAE's is approximately 7.5x faster on CPU and up to 25.6x on the GPU than LLC-Score and LLC-Rank. This shows that once the critiquing module of M&amp;Ms-VAE is trained, which takes less than five minutes on the machine, the model achieves a lower latency (batch size of one). In Figure <ref type="figure" target="#fig_5">5b</ref>, the overall simulation in M&amp;Ms-VAE is at least 3.1x faster on CPU and approximately 5.3x faster on GPU. Finally, in real-life applications, we could leverage multiple users' critiques simultaneously and increase the throughput by considering a larger batch size. is consistent across the other metrics). On the full datasets, we note that the explanation performance is comparable across the three variants and higher than those of the baselines in Table <ref type="table">3</ref>. Regarding the recommendation performance, M&amp;Ms-VAE r ğ‘¢ obtains the best results, followed closely by M&amp;Ms-VAE r ğ‘¢ , k ğ‘¢ , and M&amp;Ms-VAE k ğ‘¢ clearly underperforms.</p><p>This aligns with the observations in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>: recommender systems are limited if they use only review text as input, and not all reviews can be useful. Nevertheless, compared to Table <ref type="table">2</ref>, M&amp;Ms-VAE k ğ‘¢ always achieves better recommendation performance than the popularity baseline, and it performs better than AutoRec and CDAE on two datasets.</p><p>The bottom row of Figure <ref type="figure">6</ref> show the relative drop in performance on both metrics. The explanation performance seems unaffected by the sparsity, showing that the explanation task remains simple in comparison with the recommendation task. Remarkably, with only 50% fully observed inputs and the rest partially observed, the recommendation performance of M&amp;Ms-VAE r ğ‘¢ and M&amp;Ms-VAE r ğ‘¢ , k ğ‘¢ is decreased by only 9% on average. More so, with 90% partial observations, the model can still achieve more than 70% of its performance quality on the full datasets. Finally, these results emphasize that M&amp;Ms-VAE can effectively learn the joint distribution even in a weakly supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Recommendations can have much more impact if they are supported by explanations that can be critiqued. Previous research has developed methods that either perform poorly in multi-step critiquing or suffer from computational inefficiency at inference. In this paper, we presented M&amp;Ms-VAE, a novel variational autoencoder for recommendation and explanation that treats the user preference and keyphrase usage as different observed variables. Additionally, we proposed a strategy that mimics weakly supervised learning and trains the inference networks jointly and independently.</p><p>Our second contribution is a new critiquing module that leverages the generalizability of M&amp;Ms-VAE to embed the user preference and the critique. With a self-supervised objective and a synthetic dataset, we enable multi-step critiquing in M&amp;Ms-VAE. Experiments on four datasets show that M&amp;Ms-VAE ( <ref type="formula" target="#formula_0">1</ref>) is the first model to obtain substantially better recommendation, explanation, and multi-critiquing performance, (2) processes critiques up to 25.6x faster than previous state-of-the-art methods, and (3) produces coherent joint and cross generation, even under weak supervision.</p><p>A  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL TRAINING DETAILS</head><p>The official baselines' codes from the respective authors, including the tuning procedure, are available in 4567 . The final hyperparameters for all models and datasets are shown in Table <ref type="table">5</ref>. For all experiments, we used the following hardware:</p><p>â€¢ CPU: 2x Intel Xeon E5-2680 v3 (Haswell), 2x 12 cores, 24 threads, 2.5 GHz, 30 MB cache; â€¢ RAM: 16x16GB DDR4-2133;</p><p>â€¢ GPU: 1x Nvidia Titan X Maxwell; â€¢ OS: Ubuntu 18.04; â€¢ Software: Python 3.6, PyTorch 1.6.1, CUDA 10.2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>NDCG ğ‘ = 5 ğ‘ = 10 ğ‘ = 20 ğ‘ = 5 ğ‘ = 10 ğ‘ = 20 ğ‘ = 5 ğ‘ = 10 ğ‘ = 20 Beer POP 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ğ¾ = 5</head><label>5</label><figDesc>ğ¾ = 10 ğ¾ = 20 ğ¾ = 5 ğ¾ = 10 ğ¾ = 20 ğ¾ = 5 ğ¾ = 10 ğ¾ = 20 ğ¾ = 5 ğ¾ = 10 ğ¾ = 20 Beer UserPop 0.0550 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Multi-step critiquing performance after 10 turns of conversation. For each dataset and keyphrase critiquing selection method, we report the average success rate (left y-axis) and session length (right y-axis) at different Top-N with 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Finally, we note</head><label></label><figDesc>that LLC-Score performs similarly to LLC-Rank in most cases. When compared to M&amp;Ms-VAE, both</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Average total computational time of the simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Average time consumed for completing 10 runs for 1,000-user simulation after ten turns of conversation with 95% confidence intervals. LLC-Score and LLC-Rank cannot leverage GPUs; we thus report the performance of M&amp;Ms-VAE on CPU and GPU. Additionally, we report the inference speedup compared to the slowest model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>â€¢ ğ’› ğ‘¡ ğ‘¢ âˆˆ R |ğ» | : The latent representation of the critique ğ’„ ğ‘¡ ğ‘¢ . â€¢ zğ‘¡ ğ‘¢ âˆˆ R |ğ» | : The updated latent representation of the user after the critique ğ’„ ğ‘¡ ğ‘¢ . â€¢ ğ¼ +ğ’„ âˆˆ {ğ‘– |ğ’Œ ğ¼ ğ‘–,ğ’„ = 1, âˆ€ğ‘– âˆˆ ğ¼ }: The set of items that contain the critiqued keyphrase ğ’„. â€¢ ğ¼ -ğ’„ âˆˆ {ğ‘– |ğ’Œ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1) compute the critique representation ğ’› ğ‘ ğ‘¢ with ğ‘ Î˜ â€² ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ), (2) average both the user latent representation ğ’› ğ‘¢ and the critique representation ğ’› ğ‘ ğ‘¢ , and (3) predict the new feedback rğ‘¢ with the generative network ğ‘ Î˜ ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1. Probabilistic-graphical-model view of our proposed M&amp;Ms-VAE model. Both the implicit feedback ğ’“ ğ‘¢ and the keyphrase ğ’Œ ğ‘¢ are generated from user ğ‘¢'s latent representation ğ’› ğ‘¢ . Solid lines denote the generative model, whereas dashed lines denote the variational approximation.</figDesc><table><row><cell>Mixture of Experts ?(â‹…)</cell><cell></cell><cell></cell></row><row><cell>âˆ€"</cell><cell>" 9</cell><cell>Rating Classifier</cell></row><row><cell cols="2">Encoder</cell><cell>Keyphrase Explainer</cell></row><row><cell></cell><cell>!</cell><cell></cell></row><row><cell cols="2">Latent Space !  *</cell><cell>! " "#$</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The proposed M&amp;Ms-VAE architecture and training scheme. Each pass infers the parameters ğ ğ‘¢ and ğˆ ğ‘¢ with the mixture of experts using either the joint inference network ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) or one of the individual networks (ğ‘ Î¦ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) or ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ), respectively). The final gradient is computed on the sum of each ğ¸ğ¿ğµğ‘‚ ( â€¢) term.where we assume that the prior distribution ğ‘ (ğ’›) is a standard normal distribution and ğ›½ is a hyperparameter that controls the strength of the regularization relative to the reconstruction error. Thanks to our assumption that ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ are conditionally independent given the common latent variable ğ’› ğ‘¢ , we can rewrite Equation 4 as follows:ğ¸ğ¿ğµğ‘‚ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) + log ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -ğ›½ D KL ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ ) .Learning the variational joint posterior ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) of Equation5under its current form requires ğ’“ ğ‘¢ and ğ’Œ ğ‘¢ to be presented at all times, thus making cross-modal recommendation difficult. Following our assumption, we can factorize the joint variational posterior as a function ğœ (â€¢) of unimodal posteriors (or experts) ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) and ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ):ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = ğœ ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ), ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) , similarly to<ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref>. In our case, the function ğœ (â€¢) should be (1) robust to overconfident experts if the marginal posterior ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ) or ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ) has low density, and (2) robust to missing</figDesc><table><row><cell>(5)</cell></row></table><note><p>unobserved variable ğ’“ ğ‘¢ or ğ’Œ ğ‘¢ . Therefore, we propose to rely on a mixture of experts (MoE) with uniform weights:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>of considering the recommendation of items to a user ğ‘¢ over ğ‘¡ +1 time steps. First, M&amp;Ms-VAE produces the initial set of recommended items r0 ğ‘¢ = rğ‘¢ using only the historical observed interactions ğ’“ ğ‘¢ . Then, the user can provide a critique ğ’„ ğ‘¡ ğ‘¢ that is encoded into ğ’› ğ‘¡ ğ‘¢ via the inference model ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ). The blending module combines the previous representations ğ’› 0 ğ‘¢ , ğ’› 1 ğ‘¢ , . . . , ğ’› ğ‘¡ ğ‘¢ into zğ‘¡ ğ‘¢ , from which the subsequent recommendation rğ‘¡ ğ‘¢ is computed. This process continues until the user ğ‘¢ accepts the recommendation and ceases to provide additional critiques. Algorithm 1 Synthetic Critiquing Dataset Creation 1: function Generate(ğ‘¹ val , ğ‘² ğ‘° )</figDesc><table><row><cell>2:</cell><cell>Synthetic dataset ğ· â† {}</cell></row><row><cell>3:</cell><cell>for each user ğ‘¢ do</cell></row><row><cell>4:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Descriptive statistics of the datasets. Coverage shows the ratio of reviews having at least one of the selected keyphrases (KPs) Dataset #Users #Items #Interactions Sparsity #KPs KP Coverage Avg. KPs/Review AVG. KPs/User</figDesc><table><row><cell>Beer</cell><cell>6,370</cell><cell>3,669</cell><cell>263,244</cell><cell>1.13%</cell><cell>75</cell><cell>99.27%</cell><cell>7.16</cell><cell>1,216</cell></row><row><cell cols="2">CDs&amp;Vinyl 6,060</cell><cell>4,395</cell><cell>152,783</cell><cell>0.57%</cell><cell>40</cell><cell>74.59%</cell><cell>2.13</cell><cell>73</cell></row><row><cell>Yelp</cell><cell>9,801</cell><cell>4,706</cell><cell>140,496</cell><cell>0.30%</cell><cell>234</cell><cell>96.65%</cell><cell>7.45</cell><cell>300</cell></row><row><cell>Hotel</cell><cell>7,044</cell><cell>4,874</cell><cell>143,612</cell><cell>0.42%</cell><cell>141</cell><cell>99.99%</cell><cell>17.42</cell><cell>419</cell></row><row><cell cols="6">into 60%/20%/20% for the training, validation, and test sets. Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>4.6 RQ4: How does M&amp;Ms-VAE perform under weak supervision; is the joint &amp; cross inference coherent?We first quantify the coherence of joint and cross generations of our M&amp;Ms-VAE model. We denote three cases at test time: (1) only the user's interactions are used, and the encoder is ğ‘ Î¦ ğ‘Ÿ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ); (2) only the user's keyphrase preference is used, and the encoder is ğ‘ Î¦ ğ‘˜ (ğ’› ğ‘¢ |ğ’Œ ğ‘¢ ); and (3) both used with the encoder ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ). Second, we simulate incomplete supervision by randomly selecting a subset of the training with fully observed samples. The other one is split into two even parts: the first includes only observed ğ’“ ğ‘¢ and the second ğ’Œ ğ‘¢ . We retain the models and settings of Section 4.3. M&amp;Ms-VAE ru -Inference from r u 2) M&amp;Ms-VAE ku -Inference from k u 3) M&amp;Ms-VAE ru,ku -Inf. from r u &amp; k u Fig.6. Recommendation (left y-axis) and explanation (right y-axis) results averaged on 5 runs with different combinations of modalities observed at inference. The x-axis denotes the percentage of fully observed inputs during training; the rest is partially observed.</figDesc><table><row><cell></cell><cell>Beer</cell><cell>CDsVinyl</cell><cell>Yelp</cell><cell>Hotel</cell></row><row><cell></cell><cell></cell><cell>Rec. NDCG</cell><cell cols="2">Exp. Recall@20</cell></row><row><cell>0.00 0.05 0.10 0.15 0.20 0.25 â†‘ Rec. NDCG -30% -20% -10% 0% Drop in % â†‘</cell><cell>1) Full 80% 60% 40% 20%</cell><cell cols="2">Full 80% 60% 40% 20%</cell><cell>Full 80% 60% 40% 20%</cell><cell>0.15 0.30 0.45 0.60 0.75 0.90 Exp. Recall@20 â†‘ -30% -20% -10% 0% Drop in % â†‘</cell></row><row><cell></cell><cell cols="4">Percentage of fully observed inputs in the training set.</cell></row><row><cell cols="5">Figure 6 shows the results averaged on five runs for the three cases at different levels of supervision. The top row</cell></row><row><cell cols="5">presents the explanation and recommendation performance in terms of NDCG and Recall@20 (the model behavior</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>M&amp;MS-VAE DERIVATION log ğ‘ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = log âˆ« ğ’› ğ‘¢ ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ )ğ‘‘ğ’› ğ‘¢ (10) ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ ) ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) = log E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ ) ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) â‰¥ E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ , ğ’› ğ‘¢ ) -E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ )(13)= E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) + E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ (ğ’› ğ‘¢ ) -E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) (14) = E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ (ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -D KL ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ )(15)= E ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ ,ğ’Œ ğ‘¢ ) log ğ‘ Î˜ ğ‘Ÿ (ğ’“ ğ‘¢ |ğ’› ğ‘¢ ) + log ğ‘ Î˜ ğ‘˜ (ğ’Œ ğ‘¢ |ğ’› ğ‘¢ ) -D KL ğ‘ Î¦ (ğ’› ğ‘¢ |ğ’“ ğ‘¢ , ğ’Œ ğ‘¢ ) || ğ‘ (ğ’› ğ‘¢ )</figDesc><table><row><cell>âˆ«</cell><cell></cell><cell></cell></row><row><cell>= log</cell><cell>ğ‘‘ğ’› ğ‘¢</cell><cell>(11)</cell></row><row><cell>ğ’› ğ‘¢</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(12)</cell></row><row><cell></cell><cell></cell><cell>(16)</cell></row><row><cell>B KEYPHRASE EXAMPLES</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc>Some keyphrases mined from the reviews. We manually grouped them by types for a better understanding.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Keyphrases</cell></row><row><cell></cell><cell>Head</cell><cell>white, tan, offwhite, brown</cell></row><row><cell></cell><cell>Malt</cell><cell>roasted, caramel, pale, wheat, rye</cell></row><row><cell>Beer</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Color</cell><cell>golden, copper, orange, black, yellow</cell></row><row><cell></cell><cell>Taste</cell><cell>citrus, fruit, chocolate, cherry, plum</cell></row><row><cell></cell><cell>Genre</cell><cell>rock, pop, jazz, rap, hip hop, R&amp;B</cell></row><row><cell></cell><cell>Instrument</cell><cell>orchestra, drum</cell></row><row><cell>CDs&amp;Vinyl</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Style</cell><cell>concert, opera</cell></row><row><cell></cell><cell>Religious</cell><cell>chorus, christian, gospel</cell></row><row><cell></cell><cell>Cuisine</cell><cell>chinese, thai, italian, mexican, french</cell></row><row><cell></cell><cell>Drink</cell><cell>tea, coffee, bubble tea, wine, soft drinks</cell></row><row><cell>Yelp</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Food</cell><cell>chicken, beef, fish, pork, seafood, cheese</cell></row><row><cell></cell><cell cols="2">Price &amp; Service cheap, pricy, expensive, busy, friendly</cell></row><row><cell></cell><cell>Service</cell><cell>bar, lobby, housekeeping, guest, shuttle</cell></row><row><cell></cell><cell>Cleanliness</cell><cell>toilet, sink, tub, smoking, toiletry, bathroom</cell></row><row><cell>Hotel</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Location</cell><cell>airport, downtown, city, shop, restaurant</cell></row><row><cell></cell><cell>Room</cell><cell>bed, tv, balcony, terrace, kitchen, business</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Using the reparametrization trick<ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>: ğ’› ğ‘¢ = ğ ğ‘¢ + ğœ–ğˆ ğ‘¢ , where ğœ– âˆ¼ N (0, I ğ» ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It also enables another way to solve the cold-start problem: new users can select a set of items and/or relevant keyphrases that reflect their preferences.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/wuga214/NCE_Projected_LRec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/k9luo/DeepCritiquingForVAEBasedRecSys</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/wuga214/DeepCritiquingForRecSys</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/litosly/RankingOptimizationApproachtoLLC</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MULTI-STEP CRITIQUING ON THE WHOLE SET OF ITEMS</head><p>Here we replicate the experiment in Section 4.4, but we use instead all the available items (see Table <ref type="table">1</ref> for the sizes). When the evaluation is conducted on 300 items (see Figure <ref type="figure">4</ref>), we see that users indeed find a specific item using our technique with a high success rate (i.e., around 90%). However, in Figure <ref type="figure">7</ref> where thousands of items are available, the results</p><p>show that current methods are not yet good enough to achieve similar results for such a large number. Nevertheless, M&amp;Ms-VAE clearly outperforms on average other methods and still achieves an average success rate of 30%. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4917" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interacting with Explanations through Critiquing</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-aware autoencoders for explainable recommender systems</title>
		<author>
			<persName><forename type="first">Vito</forename><surname>Bellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Schiavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Di Noia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azzurra</forename><surname>Ragone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><forename type="middle">Di</forename><surname>Sciascio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 3rd Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-Based Navigation of Complex Information Spaces</title>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">D</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crowd-based personalized natural language explanations for recommendations</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Gilbert Terveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Attentional Rating Regression with Review-Level Explanations</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1583" to="1592" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 World Wide Web Conference</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Critiquing-based recommenders: survey and emerging trends</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards Explainable Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehul</forename><surname>Parsana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Nineth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Twenty-Nineth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.3555" />
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance of Recommender Algorithms on Top-n Recommendation Tasks</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Turrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/1864708.1864721</idno>
		<ptr target="https://doi.org/10.1145/1864708.1864721" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM Conference on Recommender Systems</title>
		<meeting>the Fourth ACM Conference on Recommender Systems<address><addrLine>Barcelona, Spain; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
	<note>RecSys &apos;10</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://www.yelp.com/dataset" />
		<title level="m">Yelp Open Dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Designing Example-critiquing Interaction</title>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Torrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Viappiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Intelligent User Interfaces</title>
		<meeting>the 9th International Conference on Intelligent User Interfaces<address><addrLine>Island of Madeira, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004-01-01">2004. 2004-01-01</date>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
	<note>IUI &apos;04</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883037" />
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee, Republic and Canton of</title>
		<meeting><address><addrLine>MontrÃ©al, QuÃ©bec, Canada; Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>Proceedings of the 25th International Conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks with Top-k Gains for Session-Based Recommendations</title>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3269206.3271761</idno>
		<ptr target="https://doi.org/10.1145/3269206.3271761" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management<address><addrLine>Torino, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
	<note>CIKM &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Trust-Related Effects of Expertise and Similarity Cues in Human-Generated Recommendations</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Donkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalin-Mihai</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Ziegler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Ranking Optimization Approach to Latent Linear Critiquing for Conversational Recommender Systems</title>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383313.3412240</idno>
		<ptr target="https://doi.org/10.1145/3383313.3412240" />
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys &apos;20)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
		<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive Assessment of User Preference Models: The Automated Travel Assistant</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Lesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on User Modeling</title>
		<meeting>the Sixth International Conference on User Modeling</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent Linear Critiquing for Conversational Recommender Systems</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hojin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference<address><addrLine>Taipei, Taiwan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>WWW &apos;20). Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2535" to="2541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Critiquing for VAE-Based Recommender Systems</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401091</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Attitudes and Attributes from Multi-aspect Reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2012.110</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2012.110" />
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767755</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767755" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;15)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enriching Buyers&apos; Experiences: The SmartClient Approach</title>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>The Hague, The Netherlands; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
	<note>CHI &apos;00)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the convergence of Adam and Beyond</title>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating Compound Critiquing Recommenders: A Real-User Study</title>
		<author>
			<persName><forename type="first">James</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Mcginty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250910.1250929</idno>
		<ptr target="https://doi.org/10.1145/1250910.1250929" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Electronic Commerce</title>
		<meeting>the 8th ACM Conference on Electronic Commerce<address><addrLine>San Diego, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
	<note>EC &apos;07)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno>UAI &apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How Useful Are Reviews for Recommendation? A Critical Review and Potential Improvements</title>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401281</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401281" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1845" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical linear models for large-scale one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders Meet Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742726</idno>
		<ptr target="https://doi.org/10.1145/2740908.2742726" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web<address><addrLine>Florence, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;15 Companion)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models</title>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/0ae" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>AlchÃ©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>cb3b499ad1fca944e6f5c836-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imant</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Vogt</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/43bb" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 1b62a5e374c63cb22fa457b4-Paper.pdf</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6100" to="6110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Explaining recommendations: Design and evaluation</title>
		<author>
			<persName><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="353" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Smart Clients: Constraint Satisfaction as a Paradigm for Scaleable Intelligent Information Systems</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Torrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1017940426216</idno>
		<ptr target="https://doi.org/10.1023/A:1017940426216" />
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="69" />
			<pubPlace>Hingham, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Factorized Multimodal Representations</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Preference-based search using example-critiquing with suggestions</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Viappiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="465" to="503" />
			<date type="published" when="2006">2006. 2006</date>
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">RABBIT: an interface for database access</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederich</forename><forename type="middle">N</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Tou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference</title>
		<meeting>the ACM Conference</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="83" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep Language-Based Critiquing for Recommender Systems</title>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems<address><addrLine>Copenhagen, Denmark; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
	<note>RecSys &apos;19</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Noise Contrastive Estimation for One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee</forename><surname>Loong Soon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Rai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331201</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal Generative Models for Scalable Weakly-Supervised Learning</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<idno type="DOI">10.1145/2835776.2835837</idno>
		<ptr target="https://doi.org/10.1145/2835776.2835837" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
	<note>WSDM &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring explanation effects on consumers&apos; trust in online recommender agents</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><forename type="middle">P</forename><surname>Curley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="421" to="432" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
