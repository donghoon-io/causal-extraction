<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vladislav Bína; Radim Jiroušek On computations with causal compositional models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vladislav</forename><surname>Bína</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Theory and Automation AS CR</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Radim</forename><surname>Jiroušek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Theory and Automation AS CR</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Ína</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Theory and Automation AS CR</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Jirou Šek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Theory and Automation AS CR</orgName>
								<address>
									<postCode>2015</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vladislav Bína; Radim Jiroušek On computations with causal compositional models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.14736/kyb-2015-3-0525</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal model</term>
					<term>conditioning</term>
					<term>intervention</term>
					<term>extension Classification: 65C50</term>
					<term>97K50</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Institute of Mathematics of the Czech Academy of Sciences provides access to digitized documents strictly for personal use. Each copy of any part of this document must contain these Terms of use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In many situations one would like to know what would happen if the current state of world is changed by an external intervention. For example, the governor of the national bank plans to intervene in favor (or, against the strength) of the national currency, and it would be nice to reveal all the consequences before such action is realized. Similar situations occur in everyday work of physicians: It would be nice to know in advance the effects of a specific treatment applied to a patient. To simplify our presentation, in this paper we will illustrate our considerations on a trivial example. Assume we trade in cement and have reliable suppliers, which enables us to control the price just on the basis of demand. Whenever the demand for cement increases we can slightly increase the price, and vice versa, which, as we believe, maximizes our profit. The situation is depicted in Figure <ref type="figure" target="#fig_10">1</ref>. Imagine that a current demand for cement is on its minimum, we keep the price very low and therefore our profit is almost negligible. The question is whether we can afford to increase the price of cement regardless of the low current demand. What will be the impact of such an action to our profit. It is well known that the questions asked in the previous paragraph cannot be answered solely with the help of statistical data <ref type="bibr" target="#b0">[2]</ref>. Such data may serve to answer the question: What was our profit in case that the price of cement was high? But now, we are asking the question: What will be our profit if we set the price of cement high? Naturally, in the past when the price was high, the demand was high, too, and therefore also the profit was high. Now, we cannot expect the increase of the price will increase the demand, rather the opposite. Therefore, such a direct utilization of the statistical data collected in the past is useless.</p><p>Nevertheless, when studying the book <ref type="bibr" target="#b5">[7]</ref> by Judea Pearl we can see that such data can help us for causal models construction. Naturally, such data will not help us to find out causal relations. They can help us to reveal a dependence of variables but not the direction of causality. This must be done on the basis of some other knowledge, and the data can be used just to estimate the necessary parameters, the necessary probabilities. For a more detailed introduction to the causal inference and interesting examples of application in (strategic) management see, e. g., book by Michael Ryall and Aaron Bramson <ref type="bibr" target="#b6">[8]</ref>.</p><p>Most of the tools for causal model description use directed graphs to express the (asymmetric) relations of cause and effect (for an example see <ref type="bibr">[1]</ref>). It also holds for Pearl's causal networks <ref type="bibr" target="#b5">[7]</ref>. In contrast to this common approach, in this paper we take advantage of the algebraic apparatus of compositional models <ref type="bibr" target="#b1">[3]</ref>. In fact, this paper is a continuation of the contribution to IPMU 2014 conference <ref type="bibr" target="#b2">[4]</ref>, where the first ideas on causal compositional models were published. The present paper is organized as follows. Section 2 recalls the main operator, which is used to assemble compositional models, so called operator of composition, and its properties. In Section 3, we show difference between conditioning and intervention, define causal compositional models and present general formulae for the computation of intervention in causal compositional models. The examples illustrating both the application of these formulae and the situations, in which they are of no help can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASIC NOTIONS AND NOTATION</head><p>In this paper we consider a set of n finite valued variables {X 1 , X 2 , . . . , X n }, subsets of which are denoted by lower-case Roman alphabets (e. g., x, y, and z). X i denotes the set of values (states) of variable X i , and analogously, for sets of variables x, y the respective Cartesian products of all combinations of their values are denoted by x , y , respectively. Elements of these sets, i. e., the (combinations of) values of variables will be denoted by lower-case boldface Roman characters (e. g., a ∈ x ).</p><p>Conditional distributions will be denoted using a standard notation, e. g., π(y|X), or, in case that we consider conditioning by a specific value of variable X by π(y|X = a). This must be strictly differentiated from the result of intervention, for which we use Pearl's do operator; i. e., π(y|do(X = a)) denotes the effect on y caused by the intervention do(X = a), which sets variable X to a value a ∈ X .</p><p>For a probability distribution π(x) its marginal distribution for y ⊂ x is denoted either by π(y), or by π ↓y . The latter notation will especially be used when marginalizing a distribution expressed in the form of a formula, or when referring to the value of probability distribution π for a specific state a (combination of values of variables): π ↓y (a). In this paper we are interested also in an opposite operation called extension. By this term we understand any distribution κ defined for a superset of variables, i. e., κ(z) for z ⊃ x, such that κ(x) = π(x). The set of all extensions of distribution π(x) for variables z ⊃ x will be denoted by Ψ[π; z]. Now, consider two distributions π(x) and κ(y). Obviously, there exists their joint extension if and only if they are consistent, i. e., if π(x ∩ y) = κ(x ∩ y). In case that they are not consistent then one can be interested in getting an extension of π containing from κ as much information as possible. Speaking more precisely, one can look for a distribution µ(x ∪ y) that is a projection of κ into the set of all extensions Ψ[π; x ∪ y]: then Theorem 6.2 in <ref type="bibr" target="#b1">[3]</ref> states that this type of projection can be got as a composition of π and κ that is defined by the formula</p><formula xml:id="formula_0">µ(x ∪ y) = π(x) κ(y) =    π(x)•κ(y) κ(x∩y) if π(x ∩ y) κ(x ∩ y),</formula><p>undefined otherwise.</p><p>(1)</p><p>Since the operator of composition is one of the key notions of this paper let us recall its most important properties proven in <ref type="bibr" target="#b1">[3]</ref>.</p><p>Theorem 2.1. Suppose π(x), κ(y) and λ(z) are probability distributions such that κ ↓x∩y π ↓x∩y , λ ↓(x∪y)∩z (π κ) ↓(x∪y)∩z and λ ↓x∩z π ↓x∩z . Then the following statements hold:</p><p>1. (Domain): π κ is a probability distribution for x ∪ y.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">(Simple marginalization):</head><formula xml:id="formula_1">If (x ∩ y) ⊆ w ⊆ (x ∪ y) then, (π κ) ↓w = π ↓x∩w κ ↓y∩w .</formula><p>The reader certainly noticed that the assumptions on the dominance relations in the previous assertion guarantee that the compositions appearing in the individual properties are well defined. To make the following exposition more lucid, and to avoid repeating again and again the necessary assumptions on the dominance relation, let us make the following convention: Whenever the operator of composition appears in this paper, the respective relation of dominance is supposed to hold, so that all the compositions are always well defined. Notice that this can be trivially secured by supposing that all the distributions are strictly positive, but in this case we would lost a possibility to model logical (deterministic) dependence.</p><p>Recall that Property 6 from Theorem 2.1 states that the composition operator is generally not associative. This disadvantage is, in a way, compensated by the existence of an alternative operator, called anticipating composition operator, which is defined as follows. Consider probability distributions π(x) and κ(y) and another subset of variables z. Then,</p><formula xml:id="formula_2">π(x) z κ(y) = π(x) • κ((z \ x) ∩ y) κ(y).<label>(2)</label></formula><p>Notice that this composition operator is parameterized by subset z.</p><formula xml:id="formula_3">If (z \ x) ∩ y = ∅ then π z κ = π κ.</formula><p>An important property of this operator was proven in <ref type="bibr" target="#b1">[3]</ref>.</p><p>Theorem 2.2. For probability distributions λ(z), π(x) and κ(y) it holds that</p><formula xml:id="formula_4">(λ π) κ = λ (π z κ).<label>(3)</label></formula><p>Adopting notation from <ref type="bibr" target="#b9">[11]</ref> we will define a degenerate one-dimensional distribution δ a (X), which is a distribution of variable X achieving probability 1 for value X = a ∈ X , i. e.,</p><formula xml:id="formula_5">δ a (X) = 1 if X = a, 0 otherwise.</formula><p>So, distribution δ a (X) carries the sure information that X = a. This provides a possibility to formulate the following assertion concerning conditional distributions.</p><p>Theorem 2.3. For probability distribution π(x) and x ⊇ y ∪ {X}, X ∈ y it holds that</p><formula xml:id="formula_6">π(y|X = a) = (δ a (X) π(x)) ↓y .<label>(4)</label></formula><p>P r o o f . Denote z = y∪{X}. Then, using Property 11 of Theorem 2.1, one can compute</p><formula xml:id="formula_7">(δ a (X) π(x)) ↓y = (δ a (X) π(x)) ↓z ↓y = (δ a (X) π(z)) ↓y .</formula><p>Property 1 of Theorem 2.1 says that δ a (X) π(z) is a probability distribution of variables z, and from Formula (1) defining the operator of composition it is clear that δ a (X) π(y, X = b) = 0 if b = a. In opposite case, i. e., if b = a, then</p><formula xml:id="formula_8">δ a (X) π(z) = δ a (X) • π(y, X = a) π(X = a) = π(y|X = a).</formula><p>Therefore, marginalizing</p><formula xml:id="formula_9">(δ a (X) π(z)) ↓y = b∈ X δ a (X) π(y, X = b) = π(y|X = a),</formula><p>we get a distribution of variables y, which finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CAUSAL MODELS</head><p>Pearl's causal models <ref type="bibr" target="#b5">[7]</ref> are expressed in a form of Bayesian networks whose directed graphs define the corresponding causal relations: all arrows head from causes to consequences. In other words, relation of parents and children in the graph correspond to the relation of causes and consequences.</p><p>Conditioning in a causal model is just a conditioning in the corresponding Bayesian network, and the intervention in a causal model is defined as a conditioning in a modified Bayesian network; the modification consists in deleting all the edges heading to the node, in which the intervention is realized. This is because the intervention is performed in spite of any influence from other nodes (variables) of the considered causal model.</p><p>Let us illustrate the necessity of distinguishing between conditioning and intervention by a trivial example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Example</head><p>Let us go back to our cement trading example. To dispatch unusually high number of orders we have to prolong loading shifts. To describe this relation formally consider two (for simplicity binary) variables describing whether we have got unusually many orders (variable Q), and whether we have a prolonged loading shift (variable L). Naturally, since we assume that great quantity of orders Q = y results in a prolonged shift L = y, there is an evident causal relation between these variables: Q is a cause for L. Clearly, these variables are mutually dependent, and therefore, denoting the respective probability distribution π(Q, L), we quite naturally expect that π(Q</p><formula xml:id="formula_10">= y|L = y) &gt; π(Q = y) and π(L = y|Q = y) &gt; π(L = y).</formula><p>The situation changes when, instead of conditioning, we consider intervention. Using Pearl's notation we denote by do(Q = y) the situation when somebody arranges that we get a great number of orders to dispatch. Analogously, do(L = y) denotes the situation when we decide to organize a long shift. In this case, it is natural to expect that receiving an enormous number of orders results in the necessity to hold a prolonged loading shift, but organizing a prolonged shift does not increase the number of received orders. Therefore while</p><formula xml:id="formula_11">π(L = y|do(Q = y)) &gt; π(L = y), π(Q = y|do(L = y)) = π(Q = y).</formula><p>Let us express this consideration in a formal model. Consider variable X and a set of variables C(X) that are causes of X. It means that the behavior of variable X is fully described by some π(x), for x = {X}∪C(X). Using Properties 3 and 5 from Theorem 2.1 one can immediately see that π(x) = π(C(X)) π(x), which seems to be unnecessarily complex, but which may be, as we are now going to show, for causal models very useful.</p><p>Theorem 2.3 says that</p><formula xml:id="formula_12">π(C(X)|X = a) = (δ a (X) π(x)) ↓C(X) ,<label>(5)</label></formula><p>and taking into consideration that π(x) = π(C(X)) π(x) we get</p><formula xml:id="formula_13">π(C(X)|X = a) = (δ a (X) (π(C(X)) π(x))) ↓C(X) .</formula><p>Knowing that the operator of composition is not associative, we know that generally</p><formula xml:id="formula_14">(δ a (X) (π(C(X)) π(x))) ↓C(X) = ((δ a (X) π(C(X))) π(x)) ↓C(X) .</formula><p>The difference between these two expressions follows immediately from the application of Properties 3 and 11 (see Theorem 2.1) to the right hand part of the above inequality</p><formula xml:id="formula_15">((δ a (X) π(C(X))) π(x)) ↓C(X) = (δ a (X) π(C(X))) ↓C(X) = (δ a (X) ↓∅ π(C(X)) = π(C(X)).</formula><p>So, computing ((δ a (X) π(C(X))) π(x)) ↓C(X) we get the marginal π(C(X)). In the next section we will see that it must be this way because, as we will show,</p><formula xml:id="formula_16">((δ a (X) π(C(X))) π(x)) ↓C(X) = π(C(X)|do(X = a)),</formula><p>and, as illustrated by the example above, the intervention that changes (fixes) the value of an effect variable does not influence behavior of its causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal compositional models</head><p>In this paper we express the considered causal models in a form of compositional models, which are known to be equivalent to Bayesian networks (for details see <ref type="bibr" target="#b1">[3]</ref>). Let us consider a set of variables w = {X 1 , X 2 , . . . , X n }, and for each variable X i ∈ w let C(X i ) ⊂ w be its causes. Here we consider only Markovian models <ref type="bibr" target="#b5">[7]</ref>, i. e., the models in which variables can be ordered (without loss of generality we assume it is the ordering X 1 , X 2 , . . . , X n ) such that causes are always before their effects. So, we assume that</p><formula xml:id="formula_17">X k ∈ C(X i ) =⇒ k &lt; i,</formula><p>which, as the reader certainly noticed, means that C(X 1 ) = ∅, and excludes feedback models from our consideration.</p><p>In keeping with the above notation, denote x i = C(X i ) ∪ {X i }, and let π i (x i ) denote the distribution describing local behavior of X i . This means that we consider a causal model</p><formula xml:id="formula_18">2 κ(X 1 , X 2 , . . . , X n ) = π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ).<label>(6)</label></formula><p>In the next section we will show how to compute the result of an intervention to a variable X ∈ w. To prove the respective formula we will have to show that it realizes conditioning in a modified causal model, in a model in which C(X) = ∅. So, we want now to find a causal compositional model, which differs from the considered model given by Formula 6 only in the fact that for the new model C(X) = ∅. Since we assume that the original model is Markovian, it is evident that the new model is Markovian, too. Let us show that the new model is</p><formula xml:id="formula_19">3 κ (X 1 , X 2 , . . . , X n ) = π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ),<label>(7)</label></formula><p>where i = min{k : X ∈ x k } (which means that X = X i ). If i = 1 then validity of expression ( <ref type="formula" target="#formula_19">7</ref>) is obvious (it follows directly from Property 3 from Theorem 2.1). If i = 2, then first applying Property 5 (keep in mind that C(X 1 ) = ∅, and therefore x 1 = {X 1 }, and x 1 ∩ {X} = ∅) and then Property 3 one gets</p><formula xml:id="formula_20">π 2 (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) π 2 (X) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) π 2 (X) π 3 (x 3 ) . . . π n (x n ).</formula><p>For i &gt; 2 we have first to apply Property 5, then (n -2) times Property 10, and finally again Property 3:</p><formula xml:id="formula_21">π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) π i (X) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) π 2 (x 2 ) π i (X) π 3 (x 3 ) . . . π n (x n ) = . . . = π 1 (x 1 ) π 2 (x 2 ) . . . π i-1 (x i-1 ) π i (X) π i (x i ) . . . π n (x n ) = π 1 (x 1 ) π 2 (x 2 ) . . . π i (X) . . . π n (x n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Intervention in causal compositional models</head><p>Theorem 3.1. For the causal compositional model κ given by Formula (6), and for arbitrary</p><formula xml:id="formula_22">X ∈ x 1 ∪ . . . ∪ x n , a ∈ X , y ⊆ x 1 ∪ . . . ∪ x n \ {X}, κ(y|do(X = a)) = δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) ↓y .<label>(8)</label></formula><p>P r o o f . Since we follow Pearl's idea, we define the intervention as a conditioning in the altered model, i. e., κ(y|do(X = a)) = κ (y|X = a), where κ is given by Formula (7). Using Theorem 2.3</p><formula xml:id="formula_23">κ(y|do(X = a)) = κ (y|X = a) = δ a (X) π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) ↓y .</formula><p>The expression δ a (X) π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) can further be simplified using n times Property 8 and at last once Property 3</p><formula xml:id="formula_24">δ a (X) π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) = δ a (X) π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n-1 (x n-1 ) π n (x n ) = . . . = δ a (X) π i (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n-1 (x n-1 ) π n (x n ) = δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n-1 (x n-1 ) π n (x n ),<label>(9)</label></formula><p>and therefore</p><formula xml:id="formula_25">κ(y|do(X = a)) = (δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n-1 (x n-1 ) π n (x n )) ↓y ,</formula><p>which was to be proven.</p><p>Quite often we are interested in a effect of intervention do(X = a) to a single variable Y ∈ y. Though the focus of this paper is not oriented to finding optimum computational algorithms (for algorithmic solution of computational problems the author is referred to Malvestuto's papers <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6]</ref>), we want to show that principally the general expression (8) can be further transformed into simpler formulae. The level of simplification is dependent on the first appearance of Y in Formula <ref type="bibr" target="#b4">(6)</ref>. Therefore, we keep the notation i = min{k : X ∈ x k }, and denote j = min{k : Y ∈ x k }.</p><p>When modifying Formula (8) we will distinguish two situations according to the relation of values i and j: whether j &lt; i, or j &gt; i (obviously, i = j, because each <ref type="foot" target="#foot_3">4</ref>First, assume j &lt; i.</p><formula xml:id="formula_26">x i = {X i } ∪ C(X i )).</formula><p>In this case applying Property 5 and afterwards several times (precisely (j -1)-times) Property 10 we get</p><formula xml:id="formula_27">δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) δ a (X) π 2 (x 2 ) . . . π n (x n ) = π 1 (x 1 ) π 2 (x 2 ) . . . π j (x j ) δ a (X) π j+1 (x j+1 ) . . . π i (x i ) . . . π n (x n ),</formula><p>and therefore (using Property 2)</p><formula xml:id="formula_28">κ(Y |do(X = a)) = δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) ↓{Y } = π 1 (x 1 ) π 2 (x 2 ) . . . π j (x j ) ↓{Y } = κ(Y ). (<label>10</label></formula><formula xml:id="formula_29">)</formula><p>Now, consider the other possibility j &gt; i.</p><p>Using analogous reasoning as in the previous step we start computing (δ a (X) π 1 (x 1 ) . . . π n (x n ))</p><p>↓xi∪...∪xj = (π 1 (x 1 ) . . . π i-1 (x i-1 ) δ a (X) π i (x i ) . . . π j (x j )) ↓xi∪...∪xj .</p><p>First notice that the term π i (x i ) can be deleted because of Property 3. Further, denoting x i ∪ . . . ∪ x j = z, we can apply several times Property 11</p><formula xml:id="formula_30">(δ a (X) π 1 (x 1 ) . . . π n (x n )) ↓z = (π 1 (x 1 ) . . . π i-1 (x i-1 ) δ a (X) π i+1 (x i+1 ) . . . π j (x j )) ↓z = (π 1 (x 1 ) . . . π i-1 (x i-1 ) δ a (X) π i+1 (x i+1 ) . . . π j-1 (x j-1 )) ↓z\{Xj } π j (x j ) = . . . = (π 1 (x 1 ) . . . π i-1 (x i-1 )) ↓z\{Xi,...,Xj } δ a (X) π i+1 (x i+1 ) . . . π j (x j ),</formula><p>from which we can deduce a general formula The last question to be answered in this section is how to compute the effect of a multiple intervention. But it is an easy task. Consider a causal compositional model κ(w) = π 1 (x 1 ) . . . π n (x n ), and (for simplicity) two variables V, X ∈ w. Let a ∈ X , b ∈ V , and denote y = w \ {V, X}. Then</p><formula xml:id="formula_31">κ(Y |do(X = a)) = π 1 (x 1 ) . . . π i-1 (x i-1 ) ↓z\{Xi,...,Xj } δ a (X) π i+1 (x i+1 ) . . . π j (x j ) ↓{Y } .<label>(11)</label></formula><formula xml:id="formula_32">κ(y|do((V, X) = (b, a)) = κ(y|do(V = b), do(X = a)) = (δ b (V ) δ a (X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n )) ↓y = δ (b,a) (V, X) π 1 (x 1 ) π 2 (x 2 ) . . . π n (x n ) ↓y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXAMPLES</head><p>When evaluating the impact on variable Y of an intervention to variable X we usually need to take into account other related factors. And quite often these related factors cannot be observed (some authors call them confounders or covariates). In this context a natural question arises: Can we evaluate the effect of intervention even in the presence of hidden (unobservable) confounders? The answer is, in some cases, positive (see, e. g., <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b8">10]</ref>) and in the rest of this paper we present simple examples illustrating how to use the formalism of causal compositional models for this purpose. To be able to describe all four examples just with one causal model, let us enrich the cement trade model from Figure <ref type="figure" target="#fig_10">1</ref> by four more variables as depicted in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Example 1</head><p>First, let us consider a simple situation when we intend to evaluate the effect of restriction of the production output O on the demand D, i. e., our aim is to compute κ(D|do(O = o)). Following the recipe from Section 3 we realize that i = 4 and j = 2 and we can directly use the Formula (10) as a result for j &lt; i and obtain simple result</p><formula xml:id="formula_33">κ(D|do(O = o)) = (π 1 (S) π 2 (D, S)) ↓{D} .</formula><p>We can see that the result contains a marginal (of the original model) in variable D which can be estimated from data, and that the considered intervention has, in the considered model, no impact on variable D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Example 2</head><p>If the cement traders want to analyze the effect of restriction of the production output O on the price P , then, according to the Section 3, we get i = 4 and j = 6, which leads to the application of Formula <ref type="bibr" target="#b9">(11)</ref>. So, in this case we arrive at</p><formula xml:id="formula_34">κ(P |do(O = o)) = (π 1 (S) π 2 (D, S) π 3 (M, S)) ↓{D,M } δ o (O) π 5 (C, M, O = o) π 6 (C, D, P ) ↓{P }</formula><p>.</p><p>Again we succeeded in elimination of the hidden variable S, the marginal of original model (π 1 (S) π 2 (D, S) π 3 (M, S)) ↓{D,M } is distribution in variables D and M and can be estimated from data. This is an example of situation fulfilling the back-door criterion (see Pearl <ref type="bibr" target="#b5">[7]</ref>) and the result is the same as the one obtained using Pearl's back-door adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Example 3</head><p>If evaluating π(G|do(P = p)), it is apparent that we should proceed according to Formula <ref type="bibr" target="#b9">(11)</ref>, because in this case we get i = 6 and j = 7. However, using this formula we cannot eliminate the hidden variable S because S is a direct cause of G. In this case we obtain an expression κ(G|do(P = p)) So, even in this case we obtain a result containing the marginals of the model that can be estimated from the data.</p><formula xml:id="formula_35">= π 1 (S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Example 4</head><p>The last example describes an (admittedly rather artificial) situation, when the government is considering to run huge road infrastructure project and minister of industry wants to know the impact of increased demand for cement D on the producer's cost C, i. e., we want to evaluate Also in this case we need to employ particular properties from Theorem 2.1 because, again, hidden variable S is the parent of D. First of all we shall simplify the model because variables P and G cannot influence the demand. Let us eliminate G and then P (and hence also both π 6 and π 7 ) using the pair of Properties 11 and 3 twice. After this, we get the following expression, which is further modified:</p><formula xml:id="formula_36">κ(C|do(D = d)) = δ d (D) π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) ↓{C} (3) = δ d (D) π 1 (S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) ↓{C} (8) = δ d (D) π 1 (S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) ↓{C} (10) = δ d (D) π 4 (D, O) π 1 (S) π 3 (M, S) π 5 (C, M, O) ↓{C} = δ d (D) π 4 (D, O) π 1 (S) π 3 (M, S) {D,O} π 5 (C, M, O) ↓{C} ,</formula><p>where the property number is again indicated upon the equality sign. The last modification was performed according to Theorem 2.2. In order to evaluate the causal effect we need to eliminate the hidden variable S, thus, let us focus our attention to the corresponding subexpression</p><formula xml:id="formula_37">π 1 (S) π 3 (M, S) {D,O} π 5 (C, M, O) ↓{C,M,O} = π 1 (S) π 3 (M, S) • π 5 (O) π 5 (C, M, O) ↓{C,M,O}<label>(3)</label></formula><p>= π 1 (S) π 2 (S) π 3 (M, S) contains variable D which is, however, marginalized out in this expression. Here we employ Pearl's idea of extension used in his front-door adjustment <ref type="bibr" target="#b5">[7]</ref>. The reader can realize that it is the way how to take into account the mutual dependence of variables C, D, O (notice that it plays the same role that is realized by inheritance of parents in Shachters edge reversal rule <ref type="bibr" target="#b7">[9]</ref>).</p><formula xml:id="formula_38">• π 5 (O) π 5 (C, M, O) ↓{C,M,O}<label>(11)</label></formula><formula xml:id="formula_39">= π 1 (S) π 2 (D, S) π 3 (M, S) ↓{M,S} • π 5 (O) π 5 (C, M, O) ↓{C,M,O}<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented an alternative algebraical approach to model the effect of an external intervention. To achieve this goal the proposed methodology employs causal compositional models and its properties. In Section 3 we derived general formulae for the evaluation of causal effect in models where latent variable does not appear among the parents of effect variable. Studied possibilities are illustrated by four examples showing both the cases when general method is applicable and when it fails. The presented examples show that the causal effect of an intervention can be evaluated even in some cases of models containing latent variables. Notice that the structure of expression (12) goes beyond the usual compositional sequences; it corresponds to more general non-sequential models whose structures are studied by Malvestuto in <ref type="bibr" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors want to express their great acknowledgement to an anonymous referee whose constructive comments helped to substantially increase the legibility of the paper. The research was supported by GA ČR under grant no. 15-00215S. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>DOI: 10 Fig. 1 .</head><label>101</label><figDesc>Fig. 1. An example of causal relations (cement trading).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>µ(x ∪ y) = arg min λ∈Ψ[π;x∪y] Div(λ(y); κ(y)). If the considered divergence is the Kullback-Leibler divergence 1 Div(λ(y); κ(y)) =    a∈ y :λ ↓y (a)&gt;0λ ↓y (a) log λ ↓y(a)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 .</head><label>2</label><figDesc>(Composition preserves first marginal): (π κ) ↓x = π. 3. (Reduction:) If y ⊆ x then, π κ = π. 4. (Non-commutativity): In general, π κ = κ π. 5. (Commutativity under consistency): π and κ are consistent if and only if π κ = κ π. 6. (Non-associativity): In general, (π κ) λ = π (κ λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 .</head><label>7</label><figDesc>(Associativity under special condition I):If x ⊃ (y ∩ z) then, (π κ) λ = π (κ λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 .</head><label>8</label><figDesc>(Associativity under special condition II):If y ⊃ (x ∩ z) then, (π κ) λ = π (κ λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 .</head><label>9</label><figDesc>(Stepwise composition): If (x ∩ y) ⊆ w ⊆ y then, (π κ ↓w ) κ = π κ. 10. (Exchangeability): If x ⊃ (y ∩ z) then, (π κ) λ = (π λ) κ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Causal model: factors influencing the gain in cement trading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 .</head><label>2</label><figDesc>So, in all the following examples we consider the following causal compositional model κ(C, D, G, M, O, P, S) = π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>↓{G} ( 9 )↓{G} ( 8 )↓{G} ( 11 )</head><label>9811</label><figDesc>π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) ↓{C,D,S} δ p (P ) π 7 (C, D, G, P, S) ↓{G} , which cannot be evaluated from observable data.Nevertheless, several properties from Theorem 2.1 provide a possibility to eliminate the hidden variable and compute the effect of intervention even in this case. The elicitation of the causal effect is as follows (the labels above the signs of equation refer to the properties of Theorem 2.1 used):κ(G|do(P = p)) = δ p (P ) π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O)π 6 (C, D, P ) π 7 (C, D, G, P, S) ↓{G} 4×(8) = δ p (P ) π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S) = δ p (P ) π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) ↓{C,D} π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S) ↓{G} (7) = δ p (P ) π 1 (S) . . . π 5 (C, M, O) ↓{C,D} π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S) = δ p (P ) π 1 (S) . . . π 5 (C, M, O) ↓{C,D} π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S) = δ p (P ) κ(C, D, G, M, O, P, S) ↓{C,D} κ(C, D, G, M, O, P, S) ↓{C,D,G} ↓{G} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>κ(C|do(D = d)) = δ d (D) π 1 (S) π 2 (D, S) π 3 (M, S) π 4 (D, O) π 5 (C, M, O) π 6 (C, D, P ) π 7 (C, D, G, P, S) ↓{C} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>= π 1</head><label>1</label><figDesc>(S) π 2 (D, S) π 3 (M, S) • π 5 (O) π 5 (C, M, O) ↓{C,M,O} .Notice that in the last formula, the expression π 1 (S) π 2 (D, S) π 3 (M, S) is a threedimensional marginal of κ, i. e. κ(D, M, S). It contains hidden variable S but having the data, we can easily estimate its two-dimensional marginalκ(D, M ) = π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } ,which enables us to eliminate variable S. So we getπ 1 (S) π 2 (D, S) π 3 (M, S) • π 5 (O) π 5 (C, M, O) ↓{C,M,O}(11)= π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } • π 5 (O) π 5 (C, M, O) ↓{C,M,O} = π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } {O} π 5 (C, M, O) ↓{C,M,O} . All these computations show that we can evaluate the effect of the considered intervention according to the formula κ(C|do(D = d)) = δ d (D) π 4 (D, O) π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } {O} π 5 (C, M, O) ↓{C,M,O} ↓{C} , (12) where the marginal κ(D, H) = π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } can be estimated from data. Let us remark that the expression π 1 (S) π 2 (D, S) π 3 (M, S) ↓{D,M } {O} π 5 (C, M, O) ↓{C,M,O}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(</head><label></label><figDesc>Received March 1, 2015) R E F E R E N C E S [1] A. Detwarasiti and R. D. Shachter: Influence diagrams for team decision analysis. Decision Analysis 2 (2005), 4, 207-228. DOI:10.1287/deca.1050.0047</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>λ(y) κ(y) denote that κ(y) dominates λ(y), which holds (in the considered finite setting) when κ(b) = 0 =⇒ λ(b) = 0 for all b ∈ y .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>To avoid too many brackets in formulae, we do not use them if the operator of composition is performed from left to right, i. e., π 1 (x 1 ) π 2 (x 2 ) . . . πn(xn) = (. . . (π 1 (x 1 ) π 2 (x 2 )) . . . π n-1 (x n-1 )) πn(xn).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>It is obvious that, due to Property 3 of Theorem 2.1, the term π i (x i ) can be deleted from right hand side of Formula (7). We prefer not doing it since it leads to simpler and more elegant formulae.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The reader familiar with Bayesian network theory certainly noticed that in case that X and Y are independent, then their mutual placement in the sequence can be changed by selecting another, fully equivalent ordering of variables. This fact, naturally, plays an important role in algorithmic solutions of the computational problems, but this, as said at another place, is not the goal of this paper.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal reasoning through intervention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hagmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lagnado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Waldmann</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780195176803.003.0007</idno>
	</analytic>
	<monogr>
		<title level="j">Causal Learning: Psychology, Philosophy, and Computation</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gopnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Schulz</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="86" to="101" />
			<date type="published" when="2007">2007</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Foundations of compositional model theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiroušek</surname></persName>
		</author>
		<idno type="DOI">10.1080/03081079.2011.562627</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gen. Syst</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="623" to="678" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On causal compositional models: Simple examples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiroušek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-08795-5_53</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. on Inf. Processing and Management of Uncertainty -Part I</title>
		<meeting>15th Int. Conf. on Inf. essing and Management of Uncertainty -Part I</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Equivalence of compositional expressions and independence relations in compositional models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Malvestuto</surname></persName>
		</author>
		<idno type="DOI">10.14736/kyb-2014-3-0322</idno>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="322" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marginalization in models generated by compositional expressions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Malvestuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1017/cbo9780511803161</idno>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ryall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bramson</surname></persName>
		</author>
		<idno type="DOI">10.4324/9780203076835</idno>
		<title level="m">Inference and Intervention: Causal Models for Business Analysis. Routledge</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating influence diagrams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shachter</surname></persName>
		</author>
		<idno type="DOI">10.1287/opre.34.6.871</idno>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="871" to="882" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causation, Prediction and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-2748-9</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Lecture Notes in Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Tucci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.5506v1[cs.AI]</idno>
		<title level="m">Introduction to Judea Pearl&apos;s Do-Calculus</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Jarošovská 1117/II, 37701 Jindřichův Hradec</title>
		<imprint/>
		<respStmt>
			<orgName>Vladislav Bína, Faculty of Management in Jindřichův Hradec, University of Economics in Prague</orgName>
		</respStmt>
	</monogr>
	<note>Czech Republic. e-mail: bina@fm.vse.cz</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Radim</forename><surname>Jiroušek</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Faculty of Management in Jindřichův Hradec, University of Economics in Prague and Institute of Information Theory and Automation -the Czech Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note>Pod Vodárenskou věží 4, 182 08 Praha 8. Czech Republic. e-mail: radim@utia.cas.cz</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
