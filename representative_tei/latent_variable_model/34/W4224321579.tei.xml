<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multichannel DenseNet Architecture for Classification of Mammographic Breast Density for Breast Cancer Detection</title>
				<funder ref="#_Pr6sYte">
					<orgName type="full">Taif University, Taif</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-25">25 April 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kamal</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Electronics and Electrical Engineering</orgName>
								<orgName type="institution">Lovely Professional University</orgName>
								<address>
									<region>Jalandhar</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shivaji</forename><forename type="middle">D</forename><surname>Pawar</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Lovely Professional University</orgName>
								<address>
									<settlement>Jalandhar</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">SIES Graduate School of Technology</orgName>
								<address>
									<settlement>Navi</settlement>
									<region>Mumbai</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suhas</forename><forename type="middle">G</forename><surname>Sapate</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Annasaheb Dange College of Engineering and Technology</orgName>
								<address>
									<region>Sangli</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geetanjali</forename><forename type="middle">Y</forename><surname>Yadav</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">NMC Royal Medical Centre Karama</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roobaea</forename><surname>Alroobaea</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Department Computer Science</orgName>
								<orgName type="department" key="dep2">College of Computers and Information Technology</orgName>
								<orgName type="institution">Taif University</orgName>
								<address>
									<settlement>Taif</settlement>
									<region>Saudi Arabia</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sabah</forename><forename type="middle">M</forename><surname>Alzahrani</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Department Computer Science</orgName>
								<orgName type="department" key="dep2">College of Computers and Information Technology</orgName>
								<orgName type="institution">Taif University</orgName>
								<address>
									<settlement>Taif</settlement>
									<region>Saudi Arabia</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mustapha</forename><surname>Hedabou</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">VI Polytechnic University</orgName>
								<address>
									<settlement>Ben Guerir</settlement>
									<region>Mohammed</region>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">VIT University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of the Western Cape</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Xiao-Zhi Gao</orgName>
								<orgName type="institution" key="instit2">University of Eastern</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multichannel DenseNet Architecture for Classification of Mammographic Breast Density for Breast Cancer Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-25">25 April 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3389/fpubh.2022.885212</idno>
					<note type="submission">This article was submitted to Digital Public Health, a section of the journal Frontiers in Public Health Received: 27 February 2022 Accepted: 14 March 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>breast cancer</term>
					<term>BIRADS Density Classification</term>
					<term>DenseNet</term>
					<term>deep learning</term>
					<term>multichannel architecture</term>
					<term>mammographic breast density</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Percentage mammographic breast density (MBD) is one of the most notable biomarkers. It is assessed visually with the support of radiologists with the four qualitative Breast Imaging Reporting and Data System (BIRADS) categories. It is demanding for radiologists to differentiate between the two variably allocated BIRADS classes, namely, "BIRADS C and BIRADS D." Recently, convolution neural networks have been found superior in classification tasks due to their ability to extract local features with shared weight architecture and space invariance characteristics. The proposed study intends to examine an artificial intelligence (AI)-based MBD classifier toward developing a latent computer-assisted tool for radiologists to distinguish the BIRADS class in modern clinical progress. This article proposes a multichannel DenseNet architecture for MBD classification. The proposed architecture consists of four-channel DenseNet transfer learning architecture to extract significant features from a single patient's two a mediolateral oblique (MLO) and two craniocaudal (CC) views of digital mammograms. The performance of the proposed classifier is evaluated using 200 cases consisting of 800 digital mammograms of the different BIRADS density classes with validated density ground truth. The classifier's performance is assessed with quantitative metrics such as precision, responsiveness, specificity, and the area under the curve (AUC). The concluding preliminary outcomes reveal that this intended multichannel model has delivered good performance with an accuracy of 96.67% during training and 90.06% during testing and an average AUC of 0.9625. Obtained results are also validated qualitatively with the help of a radiologist expert in the field of MBD. Proposed architecture achieved state-of-the-art results with a fewer number of images and with less computation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Breast cancer has its reputation as a deadly disease and it is the second most frequent event of departure for the women community in society. It is frequently diagnosed in women with 12.3% compared to the average population <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref>. The incidence of breast cancer is associated with many biomarkers such as calcifications, masses, mammographic breast density (MBD), and architectural distortion. Advanced apprehension and proactive strategy are the unique alternatives to protect the lives of breast cancer cases and regress their subconscious shock <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref>. MBD is an essential biomarker in interpreting digital mammograms and preparing a systematic mammography screening program. According to epidemiological investigations, females with highly dense breast tissue risk developing breast malignancy <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b9">10)</ref>. MBD is a radiographically visible density on the mammogram, consisting of lobular elements, ducts, and fibrous connective tissue compared to the lucent fatty tissue in the breast <ref type="bibr" target="#b10">(11)</ref>. The reports are prepared during the digital mammography screening program as per the American College of Radiology Breast Imaging Reporting and Data System (ACR BIRADS) catalog. This catalog was last modified in November 2015 <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b12">13)</ref>. As per the BIRADS, MBD gets classified into the four significant groups of mammographic breast density as class A, class B, class C, and class D, which reduces the sensitivity of digital mammography <ref type="bibr" target="#b13">(14)</ref>. Figure <ref type="figure" target="#fig_0">1</ref> depicts all the MBD BIRADS classes.</p><p>Different scientific studies have revealed that digital mammograms' sensitivity strongly depends on the density class of the breast tissue. In dense breasts, the sensitivity of mammograms is as low as 63%, while in the lowdensity breast, there is an exponential rise of 87%. Hence, patients with high-density breasts have to go for additional imaging such as tomosynthesis, ultrasound, or breast MR to enhance cancer detection chances <ref type="bibr" target="#b14">(15)</ref>. Researchers have proposed many semiautomatic and automatic approaches for measuring breast density from the last two decades. However, the assessment of mammographic breast density (MBD) is subjective, which expert radiologists do <ref type="bibr" target="#b15">(16)</ref>. Despite this labeling, due to poor interreader and intrareader reproducibility, MBD classification has many limitations <ref type="bibr" target="#b16">(17)</ref>.</p><p>For an automatic objective assessment of MBD classification, many study efforts have been in progress from the last few decades. The initial study focuses on image processing techniques such as area-based thresholding, region growing, and clustering algorithms. A significant step formulated was the emergence of the machine learning (ML) algorithms based on different extracted image topographies from the histogram, texture intensities, patterns, and image acquisition parameters. Nowadays, deep learning algorithms provide yet another leap forward in MBD classification. Deep learning algorithms can go substantially deeper and discover all the significant features from the image. Due to system architecture and hardware improvements, it is possible to train deep learning architecture intensely. This improvement makes deep learning architecture an excellent tool for medical image analysis. Many deep learning architectures such as LeNet, (visual geometry group) VGG19, highway networks, residual networks, and DenseNet are recorded in literature.</p><p>Nevertheless, before every deep learning network lets more intelligence, a new study intricacy happens, the "vanishing gradient problem." DenseNet (dense convolutional network) provides unique insight to secure the best data flow between layers to solve the connectivity problem. This interface immediately combines all the layers, agreeing on characteristic map dimensions in feed-forward type; hence, the individual layer receives input from all the previous layers also transfers its distinct map to all the farther layers. Thus, the DenseNet concatenate feature map passes through all the subsequent layers instead of summarizing features such as ResNet. This concept includes L (L+1)/2 connections instead of L connections, identifying a dense connectivity pattern <ref type="bibr" target="#b17">(18)</ref>. The consequence of this connectivity guide, i.e., DenseNet architecture, provides the following advantages: The key feature of this architecture is classifier performance that is evaluated with 800 digital mammograms among the diverse BIRADS density categories. Results are confirmed exclusively by expert radiologists and objectively with classification accuracy and the area under the curve (AUC). Various sections described in this article are as below. Related study section covers unique existing deep learning algorithms for MBD classification, Study dataset section presents the aspects of the source dataset, Proposed methodology section represents the proposed architecture, and Analysis of experiment and outcomes section introduces the empirical outcomes. Finally Discussion section discusses the future scope and Segment 7 concludes this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED STUDY</head><p>Mammographic breast density classification is a long-lasting study area due to more thought-provoking and challenging image preprocessing, segmentation, and classification tasks. Successful deep learning classifiers such as ResNet, VGGNet, and GoogLeNet give brand-new pathological imaging and investigation prospects. This section describes the remarkable of such existing methods used for MBD classification. In summation, these classifiers offer better results on different imaging modalities for image classification. Intrinsically, high interreader fluctuations are the prime problem in MBD   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BI_RADS Density Class Number of images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-A 200</head><p>Class-B 200</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-C 200</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-D 200</head><p>Total 800</p><p>(RCC), left MLO (LMLO), and right MLO (RMLO)] of 2,00,000 screening images were given individually to train the prototype. The proposed architecture is deep and consists of 100 hidden units between input and output layers. All the hidden layers use Rectified Linear Unit (ReLU) as an activation unit. This model is tested for two-and four-class classification and achieved 81. In a comparative study for evaluating the performance of deep learning and transfer learning on a similar dataset, Lee et al. <ref type="bibr" target="#b16">(17)</ref> proposed both the approaches on 22,000 mammographic images. In this approach, expert radiologists marked density ground truth for the input images. Initially, the model's training starts with 500 images with the AUC of 0.942 and the final value of the AUC is 0.9882 reported on the whole dataset. Then, the proposed model is cross-validated with the transfer learning (ImageNet) application and obtained the overall AUC of 0.9857. Thus, this study shows that both the deep learning and transfer learning applications provide almost identical results on the equivalent dataset. The requirement of a larger dataset for training is an essential need of deep learning architecture. Shi et al. <ref type="bibr" target="#b21">(22)</ref> proposed optimized lightweight deep learning architecture to optimize deep learning performance on smaller datasets. This architecture combines three convolutional neural networks (CNNs), one dense layer, and an output layer with the SoftMax function. Data augmentation is done with additional image processing to increase the numbers in the dataset. This architecture was trained and tested on the 322 mini-Mammographic Image Analysis Society (MIAS) dataset. This architecture provides overall accuracy of 83.6% on four-class classification. The main limitation of lightweight architecture on smaller datasets is the low stability of the network, which may occasionally cause large and significant variations in the accuracy. Data augmentation can enlarge the dataset in this method, but it is still challenging to get well-trained convolutional layers due to little diversity between the original and generated datasets. In the literature, there are two ways that are recorded to enhance the model performance with a smaller dataset, which are generative adversarial network (GAN) and another is transfer learning <ref type="bibr" target="#b22">(23)</ref>.</p><p>Recently, different researchers proposed the concept of transfer learning on a smaller dataset. For example, Kaiser et al. <ref type="bibr" target="#b23">(24)</ref> proposed new architecture that can take all the four views of single patients to classify MBD into two-class classification (dense and nondense). For this purpose, the author proposed four-channel VGGNet architecture to extract all the features with average global pooling from input mammograms. Before the classification layer, to concatenate all the input layer features, two dense layers are used. Then, the proposed model is trained with 5-fold cross-validation and recorded 88% classification accuracy with the AUC of 0.954. Finally, subjective assessment is done with a panel of 32 radiologists to compare interobserver variability. In this approach, interobserver variability for breast density assessment is observed even high in two-class classification. Thus, the automated processes for MBD can help to minimize interobserver variability. Despite different automated approaches, MBD assessment is subjective and consists of intra-and interobserver variations. Objective evaluation of other commercially viable methods consists of mixed evaluation results. Another fundamental limitation is that most of existing deep learning methods need a higher dataset and validated density ground truth; hence, data acquisition becomes difficult for researchers. In addition, mammographic images are vendor specific, making deep learning more robust; training the deep learning model through different vendor-specific samples is required, another bottleneck in MBD classification. All the limitations mentioned above result in moderate objective MBD classification accuracy.</p><p>The primary motivation behind this study is to investigate the transfer learning application of DenseNet architecture toward enhancing the classification accuracy of MBD. A significant contribution is the design and development of multichannel architecture to utilize four mammographic views of a single patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STUDY DATASET</head><p>The intended study utilizes the openly available dataset from digital database for screening mammography (DDSM) <ref type="bibr" target="#b24">(25)</ref>, consisting of 2,620 samples of various classes labeled as benign, normal, and malignant with confirmed pathogeny data. The aimed algorithm uses 200 Right-MLO, 200 Left_MLO, 200 R_CC, and 200 L_CC views. A total of 800 mammograms are used for training and testing purposes. The ground truth of each class is labeled with the help of specialist radiologists team into four classes as 0, 1, 2, and 3 as a four MBD classes. All the density groups consist of 200 cases of different images (MLO, LMO, R_CC, and L_CC). Table <ref type="table" target="#tab_1">1</ref> presents the details of the ground truth input dataset used in this proposed study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROPOSED METHODOLOGY</head><p>This segment explains the proposed MBD classification technique and divided into three subsections as presented subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation of Pectoral Muscle</head><p>Instead of using raw images for models' training, the proposed method preprocesses both the LMO and MLO views. Input mammographic images consist of high-intensity marks of artifacts and tags and pectoral muscle. Those fields in the breast region can reduce the MBD classification accuracy <ref type="bibr" target="#b25">(26,</ref><ref type="bibr" target="#b26">27)</ref>. The proposed method uses the depth-first search algorithm, our previous study <ref type="bibr" target="#b27">(28)</ref>, to remove pectoral muscle, artifacts, and tags from all the MLO and LMO views. This preprocessing study helps the model to classify all the BIRADS classes correctly with less input images. Depth-first search (DFS) algorithm identify all the unwanted high-intensity areas (artifacts and pectoral muscle) of MLO and CC views and removes them, which further help the model to make the correct decision. Figures <ref type="figure" target="#fig_2">3,</ref><ref type="figure" target="#fig_3">4</ref> depict the input and output images after preprocessing.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrast Enhancement</head><p>Mammographic breast density classification is a function of the density of fibroglandular tissues inside the breast. Contrast enhancement helps to improve the visibility of fibroglandular tissues. Subsequently, it helps to improve the classification accuracy of deep learning models. In literature, many contrast enhancement methods are recorded to enhance the quality of medical images. These methods are "histogram equalization (HE), " "adaptive histogram equalization (AHE), " and "wavelet transform (WT) coefficients" <ref type="bibr" target="#b28">(29)</ref><ref type="bibr" target="#b29">(30)</ref><ref type="bibr" target="#b30">(31)</ref>. But, these documented practices take ample processing time and are less effective in noise reduction. Another recorded method is the "unsharp masking (USM) method, " which enhances the local contrast inside the image by limiting the global contrast. Still, this technique creates artifacts in the image. Due to this, the image looks artificial; therefore, it is not fitting for the enrichment of the medical images <ref type="bibr" target="#b31">(32)</ref><ref type="bibr" target="#b32">(33)</ref><ref type="bibr" target="#b33">(34)</ref>.</p><p>In contrast to gain agreement for the high-frequency element of the image, which is the basic principle behind "adaptive contrast enhancement (ACE), " it consists of limitation in terms of high processing time <ref type="bibr" target="#b31">(32)</ref><ref type="bibr" target="#b32">(33)</ref><ref type="bibr" target="#b33">(34)</ref><ref type="bibr" target="#b34">(35)</ref><ref type="bibr" target="#b35">(36)</ref><ref type="bibr" target="#b36">(37)</ref><ref type="bibr" target="#b37">(38)</ref>. In MBD classification, local details are more important than global features and reduce processing time; the proposed architecture uses the contrast limited adaptive histogram equalization (CLAHE) algorithm for limited contrast enrichment of fibroglandular tissues <ref type="bibr" target="#b38">(39)</ref><ref type="bibr" target="#b39">(40)</ref><ref type="bibr" target="#b40">(41)</ref>. The first merit of this method helps to minimize the edge shadowing effect and noise produced in homogeneous input digital mammograms. Second, small images known as tiles are used instead of the entire image to perform the CLAHE operation. Hence, contrast enhancement of each tile histogram matches with exponential distribution or Rayleigh distribution. Moreover, to overcome artificial-induced borders, neighboring tiles are connected by bilinear insertion. This advanced CLAHE technique is outlined under:</p><p>1. Initially, all the input mammograms are divided into 8 × 8 non-overlapping contextual fields of equal sizes and later a histogram of various contextual regions is calculated. 2. The clip limit (β) is the threshold parameter used to alter the contrast of the image, which is calculated by Equation <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">β = M × N L (1 + ∝ 100 (S max -1)) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Where β is the clip termination, calculated as eight by several experiments, M × N is the number of pixels in each field, L is the number of gray scales, and α is a clip factor (0-100). It is the highest allowable slope, which is set to be 4 for this analysis.</p><p>3. Each histogram is aligned, so that its maximum does not surpass further than the clip boundary. 4. The transformation function, which is described below, is used to modify the histogram.</p><formula xml:id="formula_2">t (r k ) = k j=0 p r r j<label>(2)</label></formula><p>Where p r r j = n j n</p><p>Equations ( <ref type="formula" target="#formula_2">2</ref>) and (3) describe the probability function of input image gray scale value j, n is a total number of pixels in input mammogram image, and n j is input pixel number of gray value j.</p><p>5. The adjacent tiles were joined by bilinear interpolation and the image gray scale values were altered according to the revised histograms.</p><p>In this method, the contrast factor is limited to 0.01 to prevent oversaturation of the image, specifically inhomogeneous areas for optimized output. Furthermore, the number of bins for the histogram structure is restricted to 64 over the uniform distribution for contract enhancing transformations. Figure <ref type="figure" target="#fig_4">5</ref> depicts the result of contrast enhancement of the CLAHE algorithm on input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multichannel Model Development</head><p>This article proposes the feature learning ability of multichannel DenseNet architecture presented by Huang et al. ( <ref type="formula">18</ref>) toward MBD classification. The proposed method uses four independent DenseNet architecture as four-channel architecture known as multichannel architecture. This architecture is competent in taking all the four views of an individual patient for the classification of MBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversion of Gray Scale Image Into RGB</head><p>The DenseNet model is pretrained on red, green and blue channel (RGB) images, but the proposed study uses the gray scale image as the input image. To appear gray scale image as an RGB, perform repetition of image array three times due to which the same image appears on the channels. Then, after duplication of the input image, all the input images are resized into 320 × 320 × 3. Figure <ref type="figure" target="#fig_5">6</ref> depicts the conversion of the gray scale image into a three-channel RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Convolutional Layer</head><p>The four input channels of the proposed architecture are marked as L_CC, L_MLO, R_CC, and R_MLO. The fundamental merit of this combination is that all the four views of digital mammography are processed concurrently. Each input layer of DenseNet architecture consists of a convolution layer of the kernel of 7 × 7 with a stride of 2. This convolution operation reduces the input size of the images to 112 × 112 × 3. Input image further passes through a pooling layer of 3 × 3 maximum pooling with stride 2 × 2. Thus, the input layer's convolution and pooling operation reduce the input image size to 56 × 56 × 3 and before passing to the dense blocks. Figure <ref type="figure" target="#fig_6">7</ref> depicts the proposed multichannel architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of DenseNet Neural Structure</head><p>The DenseNet architecture consists of different design variants such as DenseNet121, DenseNet169, DenseNet 201, and DenseNet 264. The DenseNet architecture's fundamental merit is the structure of dense layers precisely designed to take care of downsampling and feature concatenation. Therefore, out of four variants, the proposed architecture uses the DenseNet121 network structure, consisting of a combination of dense block layers and transition layers. Thus, the proposed model uses 58 convolutional layers and a growth rate (k = 12), including four dense and two transition layers. In addition, the proposed model consists of comparatively fewer parameters hence, saving computational memory and reducing the overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Block Layer</head><p>In four dense blocks, the individual layer is responsible for forming a k-characteristic map after convolution, which also maintains feature maps of each layer are in the same size. K convolution kernels extract all the features from the layers. Parameter k is known as a hyperparameter in DenseNet, which is the growth rate of the network. Each dense layer receives the different inputs from previous layers to reduce computation and enhance the efficiency of the dense block.</p><p>The dense block internally uses the bottleneck layer (1 × 1 convolution layer between batch normalization, ReLU, and 3 × 3 convolution layer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transition Layer</head><p>This section consists of a batch normalization layer and a one × one convolution layer followed by a two × two average pooling layer. This layer combines two nearby dense block layers to reduce the feature map size. A combination of 4 dense blocks and transition layers converts the image size into 7 × 7 × 3, further provided to the output layer. Each layer connects to the previous stage as an input described by Equation ( <ref type="formula" target="#formula_4">4</ref>):</p><formula xml:id="formula_4">X l = H l ( x 0 , x 1 , . . . . . . , x l-1 )<label>(4)</label></formula><p>A non-linear transformation function H l (.) is responsible for combining series output of batch normalization, ReLU, pooling, and convolution operation. Figure <ref type="figure" target="#fig_7">8</ref> depicts the design architecture of dense layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Classification Layer</head><p>The output layer of the proposed architecture consists of a specific average pooling layer for each channel to extract meaningful features. Extracted features are flattened by the flatten layer and are given to the individual dense layer. MBD features received from four-channel are concatenated together with two concatenation blocks. Subsequently, the third concatenation block joins all the proposed method features together. The three dense layers accept all the features collected together and, finally, the classification layer receives the output of three dense layers for classification. The proposed method uses the SoftMax classifier to classify output into four classes as per the BIRADS Density Classification. Table <ref type="table">2</ref> presents the specifications of the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANALYSIS OF EXPERIMENT AND OUTCOMES</head><p>The experimentally proposed design is trained and tested upon the PyTorch framework on Google Colaboratory, a free online cloud-based Jupiter notebook environment. The proposed method input dataset is not sufficient to split into the train, validate, and test data sets; hence, training and testing of the model are done in two phases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase I</head><p>The entire model is trained with stochastic gradient descent (SGD) algorithm using batch sizes 4 and 30 epoch on the whole dataset. SGD is an optimization algorithm that estimates the error gradient for the model's current state with an example of a training set; after this, it updates the weights of the model using backpropagation <ref type="bibr" target="#b34">(35)</ref><ref type="bibr" target="#b35">(36)</ref><ref type="bibr" target="#b36">(37)</ref>. Equation ( <ref type="formula" target="#formula_5">5</ref>) describes weight updating mechanism in SGD algorithm.</p><formula xml:id="formula_5">w new = W old -n∇Qi(w new )<label>(5)</label></formula><p>Where w new is new weight, is weight at previous iteration is learning rate and weight gradient. The primary merit of this algorithm is that it updates parameters for each training example and performs one update at one time. Thus, SGD is faster and can also learn online. The weight updating step size is the learning rate of the model. The learning rate is a configurable hyperparameter that controls the speed by which the model learns. The initial learning rate for this model is 0.1 (default value) and further divided by ten at 50 and 75% of the total training epochs. The categorical cross-entropy acts as a loss function in this model, quantifying the difference between four probability distributions. This loss function works well with the SoftMax activation function in multiclass classification. Equation ( <ref type="formula" target="#formula_6">6</ref>) describes the categorical cross-entropy mathematically, which is:</p><formula xml:id="formula_6">C.E. = - c i t i log(s i )<label>(6)</label></formula><p>Where C.E. is cross-entropy t i and s i ground truth and the convolutional neural network (CNN) score for each class i in c. Table <ref type="table">3</ref> presents the setting of different hyperparameters used to obtain the optimized results of the proposed architecture.</p><p>During the training on the entire dataset, the best classification accuracy score was 96.35% at 18 epochs with a loss factor of 0.1344. Figure <ref type="figure" target="#fig_0">10</ref> depicts the training phase results on the dataset as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase II</head><p>After training the model on the entire dataset, the proposed model performance is validated by spitting the image dataset in a ratio of 80% as training and 20% as testing. The proposed model performed significantly well on all the BIRADS density classes during the testing phase and recorded the best classification accuracy, 90.00%, with a validation loss of 0.3814. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Evaluation</head><p>The proposed multichannel DenseNet architecture performance is analyzed from the confusion matrix of the model on the test dataset. Figure <ref type="figure" target="#fig_11">12A</ref> shows the proposed architecture's heat map (confusion matrix) on the test dataset. The heat map helps to analyze which category is correctly classified by the proposed architecture. The main diagonal darker Where N All is the total number of images and Na, Nb, Nc, and Nd signify the number of images in MBD classes A, B, C, and D.</p><formula xml:id="formula_7">P = N t N i × 100 (9) R = Nt Nf × 100<label>(10)</label></formula><p>Where, Nt is the correct number of predictions of a specific category and Ni is all the number of forecasts of a class and indicates the actual number of the category, among them are precision and recall, respectively. The model accuracy is the ratio of the sum of the diagonal elements to all the elements. Thus, it acts as an indicator of the overall prediction of the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>Inconsistency in MBD assessment is the fundamental reason behind unnecessary extra screening procedures and cause for patient anxiety <ref type="bibr" target="#b40">(41)</ref><ref type="bibr" target="#b41">(42)</ref><ref type="bibr" target="#b42">(43)</ref>. Due to improved system architecture and hardware capability, the deep learning model can be an alternative for medical image classification. Still, the need for a larger dataset and vanishing gradient are the primary bottleneck issues to obtain state-of-the art results from deep learning models <ref type="bibr" target="#b43">(44)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison With Existing Methods</head><p>While there are variations in related datasets and evaluation methods, the straightforward comparison is challenging to researchers. This section compiles the proposed algorithm's comparison state with existing classifications. To study the interobserver variation in MBD assessment, N Kaiser et al. <ref type="bibr" target="#b23">(24)</ref> proposed the novel multichannel VGG architecture. This approach uses a total of 8,150 digital mammograms, divided into 600 cases. This method recorded 88% two-class classification accuracy (dense and nondense) with the AUC of 0.954. Besides, these results are also compared with the 32 individual radiologist's panel's density ground truth. This study reveals that the deep learning approach performs better than average radiologists. Thus, we only need to refine the deep learning model for MBD classification. However, the fundamental limitation of this method is that the gradient flows from the final layer to the initial layer; hence, vanishing gradient problem takes place, which increases training time and reduces the classification accuracy. In the proposed method, due to DenseNet architecture, all the layers are directly connected in feed-forward nature, acting as an effective solution for vanishing gradient and reducing training time. Thus, the results of the proposed algorithm outperform this method on a smaller dataset. Another method directly comparable to the proposed method is the optimized lightweight deep learning architecture proposed by Shi et al. <ref type="bibr" target="#b21">(22)</ref>. The elemental focus of this method is to overcome the requirement of the larger dataset and vanishing gradient problem of the deep learning algorithm. This method combines three CNN layers, one dense layer, and an output layer to classify MBD. This architecture is tested on the 322 mini-MIAS dataset with different data augmentation techniques and recorded 83.6% classification accuracy. However, due to the smaller dataset, this architecture has limitations regarding moderate classification accuracy and low stability of the network. Therefore, the proposed method used the concept of transfer learning and multichannel architecture to overcome these limitations. As a result, the proposed model outperforms this method in classification accuracy on a smaller dataset. Table <ref type="table" target="#tab_6">5</ref> provides the comparative state of the proposed method with other different existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Future Study</head><p>Even though the proposed method has improved the classification performance of the BIRADS density classes, some issues still need to be addressed. First, this study uses a smaller amount of image data and no image enhancement strategies are used to expand the dataset. Hence, model performance, especially stability during validation, is affected due to limited image data and model found it a little confusing to classify classes A and B. Therefore, in future study, data enhancement techniques will improve the model's performance. Second, the proposed study addresses only one type of dataset; hence, this approach does not address the robustness of the model. Future study will address the robustness of the model by training the model with different vendor-specific image datasets and testing results of all the mentioned state-of-the-art methods with the proposed method with the same work environment. The proposed study will be undoubtedly helpful in addressing the issues mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>In summary, the primary objective behind this study is to classify MBD as per the BIRADS classification. This study proposes the novel approach of multichannel architecture with DenseNet121 for the objective assessment of MBD. The proposed framework uses the four views of a single patient to enhance feature learning ability through a multiview approach. In the method, image contrast enhancement and preprocessing of the input image are implemented to enhance the condition of the training image data. The input images are processed through multichannel architecture to extract and fuse all the features. Analysis of the results suggests that the proposed model successfully distinguishes between all the BIRADS density classes, but is predominantly found superior in the two most distinctive and challenging BIRADS categories: "BIRADS_C" and "BIRADS_D." Classification accuracy of the proposed model is recorded at 96.67% during training and 90.06% during testing and the average AUC of 0.9625. The introduced design consists of some weaknesses discussed and will be addressed in future study; with certain modifications, the proposed method is suitable for application in clinical workflow in breast cancer screening to avoid false recalls.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc>FIGURE 1 | The proposed multichannel architecture for mammographic breast density classification.</figDesc><graphic coords="3,113.63,70.03,368.40,170.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 |</head><label>2</label><figDesc>FIGURE 2 | BIRADS classification-(A) fatty-class A (B) fat with some fibro glandular tissue -class B (C) heterogeneous dense-class C (D) extremely dense-class D (Image courtesy: Densebreast-info.org).</figDesc><graphic coords="3,127.64,288.43,340.08,120.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 |</head><label>3</label><figDesc>FIGURE 3 | Input raw images-(A) Left_MLO (B) Left_CC (C) Right_MLO (D) Right_CC.</figDesc><graphic coords="5,127.64,69.68,340.08,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 4 |</head><label>4</label><figDesc>FIGURE 4 | Output Images after segmentation and cropping -(A) Left_MLO (B) Left_CC (C) Right_MLO (D) Right_CC.</figDesc><graphic coords="5,127.64,255.03,340.08,129.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 5 |</head><label>5</label><figDesc>FIGURE 5 | Contrast enhancement of input images.</figDesc><graphic coords="5,56.64,431.99,481.92,117.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 6 |</head><label>6</label><figDesc>FIGURE 6 | Conversion of grayscale image appears as an RGB.</figDesc><graphic coords="6,48.50,168.32,238.08,144.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 7 |</head><label>7</label><figDesc>FIGURE 7 | The proposed multichannel Dense-Net Framework for BIRADS classification.</figDesc><graphic coords="6,113.63,372.96,368.40,312.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 8 |</head><label>8</label><figDesc>FIGURE 8 | The architecture of dense layer.</figDesc><graphic coords="8,85.14,69.71,425.28,216.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9</head><label>9</label><figDesc>depicts the image data distribution during training (phase I) and training and testing (phase II).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11</head><label>11</label><figDesc>depict the outcomes of validation over the training model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FIGURE 11 |</head><label>11</label><figDesc>FIGURE 11 | Validation results of the proposed model in phase-II (A) model accuracy (B) model loss.</figDesc><graphic coords="10,68.50,70.03,198.48,284.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIGURE 12 |</head><label>12</label><figDesc>FIGURE 12 | (A) The Heat map (B) and the ROC curve of the proposed model.</figDesc><graphic coords="10,328.28,70.16,199.44,227.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,328.78,252.68,198.48,269.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 |</head><label>1</label><figDesc>Input dataset used for testing and validation of proposed algorithm.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The first block of this architecture consists of a convolutional layer, a batch normalization layer, a leaky ReLU as an activation function, and a two-dimensional (2D) max pooling. A series of four sections consisting of three residual modules uses the output features of the input block. Leaky ReLU with α = 0.2 activation functions was used to train the architecture. Categorical cross-entropy as a loss function validates the performance of the model. Maximum accuracy with four-class classification is 78% and the two-class accuracy is 89.4%.</figDesc><table /><note><p>1 and 82.5% classification accuracy. Thus, this model provides moderate classification accuracy despite an extensive dataset. Lizzi et al. (21) suggested a residual convolutional network for MBD classification. The recommended design consists of 41 convolutional layers formed in residual blocks with 2 million parameters.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>presents the model's overall performance in detail</cell></row><row><cell>and variation in precision rate, the recall rate, and the F1-score</cell></row><row><cell>rate under different categories. The number (or percentage) of</cell></row><row><cell>accurate positive samples for all the four BIRADS density classes</cell></row><row><cell>are 92, 75.5, 92.2, and 94.7% of their respective totals. From the</cell></row><row><cell>results shown in Table 4, there is no confusion between classes A</cell></row><row><cell>and C, B and C, and C and D. The proposed algorithm results</cell></row><row><cell>are consistent with the results evaluated by the radiologists,</cell></row><row><cell>which are a positive sign that indicates that deep learning models</cell></row><row><cell>are helpful for the classification of MBD. Another graphical</cell></row><row><cell>technique utilized to investigate the realization of computer-</cell></row><row><cell>aided diagnostic methods is the receiver operating characteristic</cell></row><row><cell>(ROC), as shown in Figure 12B. This curve analysis performance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 |</head><label>4</label><figDesc>Performance parameter of the proposed method.</figDesc><table><row><cell>BIRADS density classes</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>Overall classification accuracy</cell><cell>Overall AUC</cell></row><row><cell>Predominantly fatty-class A</cell><cell>1</cell><cell>0.866</cell><cell>0.92</cell><cell>0.9006</cell><cell>0.9625</cell></row><row><cell>Fat with some fibro glandular tissue class B</cell><cell>0.77</cell><cell>0.77</cell><cell>0.755</cell><cell></cell><cell></cell></row><row><cell>Heterogeneous dense-class C</cell><cell>0.857</cell><cell>1</cell><cell>0.922</cell><cell></cell><cell></cell></row><row><cell>Extremely dense-class D</cell><cell>0.90</cell><cell>1</cell><cell>0.947</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 |</head><label>5</label><figDesc>Comparative status of the proposed method with current state-of-the-art methods.</figDesc><table><row><cell>References</cell><cell>Dataset</cell><cell>Proposed method</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Frontiers in Public Health | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>April 2022 | Volume 10 | Article 885212</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors are grateful to the <rs type="institution">Taif University</rs> Researchers Supporting Project number (<rs type="grantNumber">TURSP-2020/36</rs>), <rs type="funder">Taif University, Taif</rs>, <rs type="person">Saudi Arabia</rs> and gratefully acknowledge <rs type="person">Dr. Yogesh G. Yadav</rs> and <rs type="person">Dr. Atul E. Sawant</rs>, <rs type="person">Specialist Radiologist</rs>, <rs type="person">Lifecare Hospital, Musaffah</rs>, <rs type="person">Abu Dhabi</rs> for their valuable and timely help in marking ground truth on some difficult cases of mammogram from DDSM dataset and providing subjective evaluation of same.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pr6sYte">
					<idno type="grant-number">TURSP-2020/36</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY STATEMENT</head><p>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Evaluation of the classification results of the intended architecture is performed in terms of precision, recall, the F1score, and classification accuracy. Among those parameters, precision is the proportion of samples with optimistic predictions concerning the total number of correct positive samples. The recall ratio of correctly predicted samples to the whole samples and the F1-score are the precision and recall weight. Finally, classification accuracy is the total correct predictions to the total number of samples. Equations ( <ref type="formula">7</ref>) to (10) define precision, recall, the F1-score, and classification accuracy, respectively:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>All authors listed have made a substantial, direct, and intellectual contribution to the work and approved it for publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><p>Publisher's Note: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p><p>Copyright © 2022 Pawar, Sharma, Sapate, Yadav, Alroobaea, Alzahrani and Hedabou. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Current strategies for the prevention of breast cancer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno-Aspitia</surname></persName>
		</author>
		<idno type="DOI">10.2147/BCTT.S39114</idno>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Targets Therapy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patterns of cancer incidence, mortality, and prevalence across five continents: defining priorities to reduce cancer disparities in different geographic regions of the world</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamangar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Dores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1200/JCO.2005.05.2308</idno>
	</analytic>
	<monogr>
		<title level="j">J Clin Oncol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2137" to="2150" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breast cancer diagnosis using abnormalities on ipsilateral views of digital mammograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sapate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Talbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakur</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbe.2019.04.008</idno>
	</analytic>
	<monogr>
		<title level="j">Biocybern Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="290" to="305" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">COVID-19 patient health prediction using boosted random forest algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Iwendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peshkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pasupulet</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpubh.2020.00357</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Front Public Health</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="218" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identification of malnutrition and prediction of BMI from facial images using real-time image processing and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dhanamjayulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Nizhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maddikunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gadekallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Iwendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1049/ipr2.12222</idno>
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="647" to="658" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification of COVID-19 individuals using adaptive neuro-fuzzy inference system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Iwendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahboob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rizwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00530-021-00774-w</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia System</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tackling pandemics in smart cities using machine learning architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ngabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ibeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Iwendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Masabo</surname></persName>
		</author>
		<idno type="DOI">10.3934/mbe.2021418</idno>
	</analytic>
	<monogr>
		<title level="j">Math Biosci Eng</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mammographic density and the risk and detection of breast cancer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dungan</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1090-798X(08)79014-3</idno>
	</analytic>
	<monogr>
		<title level="j">Obstetr Gynecol Women Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="214" to="215" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The association of increased weight, body mass index, and tissue density with the risk of breast carcinoma in Vermont</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Vacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Muss</surname></persName>
		</author>
		<idno type="DOI">10.1002/1097-0142(20000715)89:2&lt;369::aid-cncr23&gt;3.0.co;2-j</idno>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="2000">2000. 20000715</date>
		</imprint>
	</monogr>
	<note>&lt;369::aid-cncr23&gt;3.0.co;2-j</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Radiomics based detection and characterization of suspicious lesions on full field digital mammograms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Sapate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Talbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakur</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2018.05.017</idno>
	</analytic>
	<monogr>
		<title level="j">Comput Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reproductive and lifestyle risk factors and mammographic density in Mexican women</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lajous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Tamimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>López-Ridaura</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.annepidem.2015.08.006</idno>
	</analytic>
	<monogr>
		<title level="j">Ann Epidemiol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="868" to="873" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning approach towards mammographic breast density measurement for breast cancer risk prediction: an overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sapate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3599187</idno>
	</analytic>
	<monogr>
		<title level="j">Proc ICAST</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monitoring social distancing using artificial intelligence for fighting COVID-19 virus spread</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alyami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alosaimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alroobaea</surname></persName>
		</author>
		<idno type="DOI">10.4018/IJOSSP.2021070104</idno>
	</analytic>
	<monogr>
		<title level="j">Int J Open Source Softw Proc</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="48" to="63" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">IoT-enabled healthcare systems using block chain-dependent adaptable services</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alroobaea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Almulihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Alharithi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shoaib</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-021-01584-7</idno>
	</analytic>
	<monogr>
		<title level="j">Pers Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classification of breast tissue density in digital mammograms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vidivelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIIECS.2017.8276139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIIECS. Coimbatore</title>
		<meeting>ICIIECS. Coimbatore</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training Very Deep Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated mammographic breast density estimation using a fully convolutional network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.12763</idno>
	</analytic>
	<monogr>
		<title level="j">Med Physics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1178" to="1190" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Determination of mammographic breast density using a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ciritsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eberhard</forename><forename type="middle">M</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1259/bjr.20180691</idno>
	</analytic>
	<monogr>
		<title level="j">Br J Radiol</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">20180691</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Breast density classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8462671</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. Calgary, AB</title>
		<meeting>ICASSP. Calgary, AB</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6682" to="6686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual convolutional neural networks to automatically extract significant breast density features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laruina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Retico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Fantacci</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-29930-9_3</idno>
	</analytic>
	<monogr>
		<title level="j">Commun Inform Sci</title>
		<imprint>
			<biblScope unit="volume">1089</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning from small dataset for birads density classification of mammography images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ITME.2019.00034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ITME</title>
		<meeting>ITME</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000006</idno>
	</analytic>
	<monogr>
		<title level="j">Found Trends Mach Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mammographic breast density classification using a deep neural network: assessment on the basis of inter-observer variability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fieselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ritschl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ritschl</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2513420</idno>
	</analytic>
	<monogr>
		<title level="j">SPIE-Intl Soc Opt Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">International workshop on digital mammography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kopans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Med Physics in Proceedings of IWDM</title>
		<meeting><address><addrLine>Toronto, ON</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An overview of pectoral muscle extraction algorithms applied to digital mammograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sapate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Talbar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-33793-7_2</idno>
	</analytic>
	<monogr>
		<title level="j">Stud Comput Intell</title>
		<imprint>
			<biblScope unit="volume">651</biblScope>
			<biblScope unit="page" from="19" to="54" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Design of Intelligent Applications Using Machine Learning and Deep Learning Techniques 1st ed. London: Francis and Taylor</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Sapate</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781003133681-8</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmentation of pectoral muscle from digital mammograms with depth-first search algorithm towards breast density classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Sapate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Yadav</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbe.2021.08.005</idno>
	</analytic>
	<monogr>
		<title level="j">Biocy Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1224" to="1241" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving histogrambased image contrast enhancement using gray-level information histogram with application to X-ray images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijleo.2011.05.017</idno>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="511" to="520" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An approach to adaptive enhancement of diagnostic X-Ray images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Öktem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niittylahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lemmetti</surname></persName>
		</author>
		<idno type="DOI">10.1155/S1110865703211069</idno>
	</analytic>
	<monogr>
		<title level="j">Eur J Appl Sig Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="430" to="436" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generalized unsharp masking algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2010.2092441</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transac Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1249" to="1261" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image contrast enhancement based on a histogram transformation of local standard deviation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/42.730397</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transac Med Imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="518" to="531" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Color object segmentation and tracking using flexible statistical model and level-set</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bourouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Channoufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alroobaea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubaiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andejany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-020-09809-2</idno>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tool Appl</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5809" to="5831" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Computer and Cyber Security: Principles, Algorithm, Applications and Perspective 1st ed</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hedabou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CRC Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asymptotic and finite-sample properties of estimators based on stochastic gradients</title>
		<author>
			<persName><forename type="first">P</forename><surname>Toulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<idno type="DOI">10.1214/16-AOS1506</idno>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1694" to="1727" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classification of breast density categories based on SE-Attention neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D-A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2020.105489</idno>
	</analytic>
	<monogr>
		<title level="j">Comput Method Program Biomed</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105489</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">FADETPM: Novel approach of file assured deletion based on trusted platform module in lecture notes in networks and systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Iggaramen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedabou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-97719-5_4</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer Verlag</publisher>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An electronic voting system based on homomorphic encryption and prime numbers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azougaghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkasmi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIAS.2015.7492759</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIAS. Kuala Lumpur</title>
		<meeting>ICIAS. Kuala Lumpur</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">IBE-based design for assured deletion in cloud storage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bentajer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abouelmehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Igarramen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">El</forename><surname>Fezazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<idno type="DOI">10.1080/01611194.2018.1549123</idno>
	</analytic>
	<monogr>
		<title level="j">Cryptologia</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="254" to="265" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An Iot framework for screening of covid-19 using real-time data from wearable sensors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mukhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubaiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alroobaea</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijerph18084022</idno>
	</analytic>
	<monogr>
		<title level="j">Int J Environ Res Public Health</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="56" to="62" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Storage and proximity management for centralized personal health records using an ipfs-based optimization algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mubashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rizwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gadekallu</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0218126622500104</idno>
	</analytic>
	<monogr>
		<title level="j">J Circuits Syst Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2250010</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FL-PMI: federated learning-based person movement identification through wearable devices in smart healthcare systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Arikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Prathiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alazab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gadekallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22041377</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1377</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Providing diagnosis on diabetes using cloud computing environment to the people living in rural areas of India</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gadekallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakshmanna</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12652-021-03154-4</idno>
	</analytic>
	<monogr>
		<title level="j">J Ambient Intell Humaniz Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BCD-WERT: a novel approach for breast cancer detection using whale optimization based efficient features and extremely randomized tree algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Batool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noorwali</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.390</idno>
	</analytic>
	<monogr>
		<title level="j">PeerJ Comput Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">390</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
