<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Variational Posterior of Dirichlet Process Deep Latent Gaussian Mixture Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amine</forename><surname>Echraibi</surname></persName>
							<email>&lt;amine.echraibi@orange.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<settlement>Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institute Mines-Telecom Atlantique</orgName>
								<address>
									<settlement>Brest Sandrine Vaton Stéphane Gosselin</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Flocon-Cholet</surname></persName>
							<email>&lt;joachim.floconcholet@orange.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<settlement>Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stéphane</forename><surname>Gosselin</surname></persName>
							<email>&lt;stephane.gosselin@orange.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<settlement>Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sandrine</forename><surname>Vaton</surname></persName>
							<email>&lt;sandrine.vaton@imt-atlantique.fr&gt;</email>
							<affiliation key="aff1">
								<orgName type="institution">Institute Mines-Telecom Atlantique</orgName>
								<address>
									<settlement>Brest Sandrine Vaton Stéphane Gosselin</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Variational Posterior of Dirichlet Process Deep Latent Gaussian Mixture Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thanks to the reparameterization trick, deep latent Gaussian models have shown tremendous success recently in learning latent representations. The ability to couple them however with nonparametric priors such as the Dirichlet Process (DP) hasn't seen similar success due to its non parameterizable nature. In this paper, we present an alternative treatment of the variational posterior of the Dirichlet Process Deep Latent Gaussian Mixture Model (DP-DLGMM), where we show that the prior cluster parameters and the variational posteriors of the beta distributions and cluster hidden variables can be updated in closed-form. This leads to a standard reparameterization trick on the Gaussian latent variables knowing the cluster assignments. We demonstrate our approach on standard benchmark datasets, we show that our model is capable of generating realistic samples for each cluster obtained, and manifests competitive performance in a semi-supervised setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nonparametric Bayesian priors, such as the Dirichlet Process (DP), have been widely adopted in the probabilistic graphical community. Their ability to generate an infinite amount of probability distributions using a discrete latent variable makes them ideally suited for automatic model selection. The most famous applications of the DP have been however limited to classical probabilistic graphical models such as Dirichlet Process Mixture Models and Hierarchical Dirichlet Process Hidden Markov Models <ref type="bibr" target="#b0">(Blei et al., 2006;</ref><ref type="bibr" target="#b3">Fox et al., 2008;</ref><ref type="bibr" target="#b19">Zhang et al., 2016)</ref>.</p><p>Second workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models (ICML 2020), Virtual Conference Recently, deep generative models such as Deep Latent Gaussian Models (DLGMs) and Variational AutoEncoders (VAEs) <ref type="bibr" target="#b8">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b16">Rezende et al., 2014)</ref> have shown huge success in modeling and generating complex data structures such as images. Various proposals to generalize these models to the mixture and nonparametric mixture cases have been made <ref type="bibr">(Nalisnick et al., 2016;</ref><ref type="bibr">Nalisnick &amp; Smyth, 2016;</ref><ref type="bibr" target="#b2">Dilokthanakul et al., 2016;</ref><ref type="bibr" target="#b6">Jiang et al., 2016)</ref>. Introducing such priors on top of the deep generative model can improve its generative capabilities, preserve class structure in the latent representation space, and offer a nonparametric way of performing model selection with respect to the size of the generative model.</p><p>The main challenge posed by such models lies in the inference process. Deep generative models with continuous latent variables owe their success mainly to the reparameterization trick <ref type="bibr" target="#b8">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b16">Rezende et al., 2014)</ref>. This approach provides an efficient and scalable method for obtaining low variance estimates of the gradient of the variational lower bound with respect to variational posterior parameters. Applying this approach directly to the variational posterior of the DP is not straightforward, due to the fact that a reparameterization trick for the beta distributions is hard to obtain <ref type="bibr" target="#b17">(Ruiz et al., 2016)</ref>. One approach to bypass this issue have been proposed by <ref type="bibr">(Nalisnick &amp; Smyth, 2016)</ref>, where the authors used the Kumaraswamy distribution <ref type="bibr" target="#b10">(Kumaraswamy, 1980)</ref> as a higher entropy alternative for the beta distribution in the variational posterior. However, by deriving the nature of the variational posterior directly from the variational lower bound, we can show that the appropriate distribution is in fact the beta distribution.</p><p>In this paper we provide an alternative treatment of the variational posterior of the DP-DLGMM, where we combine classical variational inference to derive the variational posteriors of the beta distributions and cluster hidden variables, and neural variational inference for the hidden variables of the latent Gaussian model. This leads to gradient ascent updates over the parameters present in nonlinear transformations where the reparameterization trick can be applied knowing the cluster assignment. As for the remaining parameters, closed-form solutions can be obtained by maximization of the evidence lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dirichlet Process Deep Latent Gaussian Mixture Models</head><p>Generalizing deep latent Gaussian models to the Dirichlet process mixture case can be obtained by adding a Dirichlet process prior on the hidden cluster assignments. We denote these cluster assignments by z. Following the assignment of a cluster hidden variable, a deep latent Gaussian model is defined for the assigned cluster similar to <ref type="bibr" target="#b16">(Rezende et al., 2014)</ref>. We adopt the stick-breaking construction of the Dirichlet Process <ref type="bibr" target="#b18">(Sethuraman, 1994)</ref>. The generative process of the model (figure <ref type="figure">1</ref>) is given by:</p><formula xml:id="formula_0">β k ∼ Beta(•; 1, η) π k = β k k-1 l=1 (1 -β l ) z n |π ∼ Cat(•|π) (l) n ∼ N (•; 0, I) ∀l h (L) n = m (L) zn + s (L) zn (L) n h (l) n = f W (l) zn h (l+1) n + s (l) zn (l) n x n |h (1) n , z n ∼ p X • |f W (0) zn h<label>(1) n where h (l)</label></formula><p>n ∈ R p l is the l th layer hidden representation constructed using a nonlinear transformation f W (l) zn represented by a neural network for the cluster assignment z n . For simplicity, we consider diagonal covariance matrices for each layer where the diagonal elements are (s (l) zn,j ) 2 1≤j≤p l , hence represents the element-wise product. The generalization to full covariance matrices is straightforward using the Cholesky decomposition.</p><p>We denote by η the concentration parameter of the Dirichlet process which is a hyperparameter to be tuned manually. The term p X represents the emission distribution of the observable x n , usually chosen to be a normal distribution for continuous variables or the Bernoulli distribution for binary variables. We denote the parameters of the generative model by:</p><formula xml:id="formula_1">Θ = {m (L) 1:∞ , s (L) 1:∞ , W (0:L-1) 1:∞ , s (1:L-1) 1:∞ }</formula><p>The model thus has an infinite number of parameters due to the Dirichlet process prior. Furthermore, the posterior distribution of the hidden variables cannot be computed in closed-form. In order to perform inference on the model we need to use approximate methods such as Markov Chain Monte Carlo (MCMC) or Variational Inference. MCMC methods are not suitable for high dimensional models such as the DP-DLGMM, where convergence of the Markov chain to the true posterior can prove to be slow and hard to diagnose <ref type="bibr" target="#b1">(Blei et al., 2017)</ref>.</p><formula xml:id="formula_2">β z n h (l+1) n1 h (l+1) np l+1 h (l) n1 h (l) np l l ∈ {0, ..., L -1} n ∈ {1, ..., N } Figure 1.</formula><p>The graphical representation of the generative process of the model, with the convention x = h (0) .</p><p>In the next section, we develop a structured variational inference algorithm for DP-DLGMM. We show that by choosing a suitable structure for the variational posterior, closed-form solutions can be obtained for the updates of the truncated variational posteriors of the beta distributions, the variational posteriors of the cluster hidden variables, and the optimal prior parameters {m (L) , s (L) } maximizing the evidence lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structured Variational Inference</head><p>For a brief review of variational methods, we denote by x 1:N the N samples present in the dataset supposed to be independent and identically distributed. The log-likelihood of the model is intractable due to the required marginalization of all the hidden variables. In order to bypass this marginalization, we introduce an approximate distribution q Φ and use Jensen's inequality to obtain a lower bound <ref type="bibr" target="#b7">(Jordan et al., 1999)</ref>:</p><formula xml:id="formula_3">l(Θ) = ln p Θ (x 1:N ) = ln z 1:N p Θ (x 1:N , z 1:N , h (1:L) 1:N , β)dh (1:L) 1:N dβ ≥ E z 1:N ,h (1:L) 1:N ,β∼qΦ ln p Θ (x 1:N , z 1:N , h (1:L) 1:N , β) q Φ (z 1:N , h (1:L) 1:N , β|x 1:N ) L(Θ, Φ).</formula><p>(1)</p><p>We can show that if the distribution q Φ is a good approximation of the true posterior, maximizing the evidence lower bound (ELBO) with respect to the model parameters Θ is equivalent to maximizing the log-likelihood. For deep generative models, most state-of-the-art methods use inference networks to construct the posterior distribution <ref type="bibr" target="#b16">(Rezende et al., 2014;</ref><ref type="bibr">Nalisnick &amp; Smyth, 2016)</ref>. For deep mixture models with discrete latent variables, this approach leads to a mixture density variational posterior where the reparameterization trick requires additional investigation <ref type="bibr" target="#b4">(Graves, 2016)</ref>. Our approach combines standard variational Bayes and neural variational inference. We approximate the true posterior using the following structured variational posterior:</p><formula xml:id="formula_4">q Φ (z 1:N , h (1:L) 1:N , β|x 1:N ) = N n=1 L l=1 q ψ (l) zn (h (l) n |x n , z n ) × q φn (z n |x n ) T t=1 q γt (β t |x 1:N ), (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where T is a truncation level for the variational posterior of the beta distributions obtained by supposing that q(β T = 1) = 1 <ref type="bibr" target="#b0">(Blei et al., 2006)</ref>. We assume a factorized posterior over the hidden layers h (1:L) n</p><p>, where the intra-layer dependencies are conserved.</p><p>3.1. Deriving the variational posteriors q φn and q γt Deriving the nature of the posterior distributions of the hidden layers h (1:L) n using the variational approach is intractable due to the nonlinearities present in the model. Thus, we take a similar approach to <ref type="bibr" target="#b16">(Rezende et al., 2014)</ref>, and we assume that the variational posterior is specified by an inference network, where the parameters of the distribution are the outputs of deep neural networks µ ψ (l)</p><formula xml:id="formula_6">t and Σ ψ (l) t of parameters ψ (l)</formula><p>t for the l th layer and the t th cluster:</p><formula xml:id="formula_7">q ψ (l) t (h (l) n |x n , z n = t) = N h (l) n ; µ ψ (l) t (x n ), Σ ψ (l) t (x n ) .</formula><p>In contrast to the hidden layers, we can use the proposed variational posterior of equation ( <ref type="formula" target="#formula_4">2</ref>) to derive closed-form solutions for q φn and q γt . Let us consider the Kullback-Leibler definition of the ELBO L:</p><formula xml:id="formula_8">L(Θ, Φ) = -D KL [q Φ (•|x 1:N )||p Θ (•, x 1:N )] .</formula><p>By plugging the variational posterior and isolating β t terms and z n terms, we can analytically derive the optimal distributions q γt and q φn maximizing L:</p><formula xml:id="formula_9">q γt (β t |x 1:N ) = Beta(β t ; γ 1,t , γ 2,t ) q φn (z n |x n ) = Cat(z n ; φ n ),</formula><p>where the fixed point equations for the variational parameters φ n and γ t are:</p><formula xml:id="formula_10">γ 1,t = 1 + N n=1 φ n,t<label>(3)</label></formula><formula xml:id="formula_11">γ 2,t = η + N n=1 T r=t+1 φ n,r<label>(4)</label></formula><formula xml:id="formula_12">ln φ n,t = const + E β∼q [ln π t ] + E h (1:L) n ∼q ψ (1:L) t ln p X (x n , h (1:L) n |z n = t) + l H q ψ (l) t (•|z n = t, x n ) s.t. T t=1 φ n,t = 1,<label>(5)</label></formula><p>The fixed point equation of φ n,t , requires the evaluation of the expectation over the hidden layers, this can be performed by sampling from the variational posterior of each hidden layer and then forwarding the sample using the generative model:</p><formula xml:id="formula_13">E h (1:L) n ∼q ψ (1:L) t ln p X (x n , h (1:L) n |z n = t) ≈ 1 S S s=1 ln p X x n , h (1:L)(s) n,t |z n = t where: h (l)(s) n,t ∼ q ψ (l) t (h (l) n |x n , z n = t).<label>(6)</label></formula><p>A key insight here is the following: if a cluster t is incapable of reconstructing a sample x n from the variational posterior, this will reinforce the belief that x n should not be assigned to that cluster. Furthermore, the estimation of the expectation can be performed using the same reparameterization trick that we will develop in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Closed-Form updates for m (L)</head><p>1:T and s (L) 1:T</p><p>In addition to the variational posteriors of the beta distributions and the cluster assignments, closed-form solutions can be obtained for the updates of m (L)</p><p>1:T and s (L) 1:T . Let us reconsider the evidence lower bound of equation ( <ref type="formula" target="#formula_0">1</ref>), where we isolate only terms dependent on the prior parameters. We have:</p><formula xml:id="formula_14">L(m (L) 1:T , s (L) 1:T ) = const - n,t φ n,t × D KL N (µ ψ (L) t (x n ), Σ ψ (L) t (x n ))||N (m (L) t , V (L) t ) .</formula><p>where</p><formula xml:id="formula_15">V (L) t = diag (s (L) t,j ) 2 1≤j≤p L</formula><p>represents the covariance matrix of the L th layer. By setting the derivative of L with respect to the parameters to zero, we obtain:</p><formula xml:id="formula_16">m (L) t = 1 N t N n=1 φ n,t µ ψ (L) t (x n ) N t = N n=1 φ n,t<label>(7)</label></formula><formula xml:id="formula_17">V (L) t = 1 N t N n=1 φ n,t I Σ ψ (L) t (x n ) + (µ ψ (L) t (x n ) -m (L) t )(µ ψ (L) t (x n ) -m (L) t ) T , (8)</formula><p>where to extract the diagonal elements we perform an elementwise multiplication by the identity matrix I. The update rules obtained are similar to the M-Step of a classical Gaussian Mixture Model, except in this case the updates are performed on the last hidden layer of the generative model, and the E-step of equation ( <ref type="formula" target="#formula_12">5</ref>) takes into account all the hidden layers. Detailed derivation of the previous equations are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stochastic Backpropagation</head><p>We next show how to perform stochastic backpropagation in order to maximize L with respect to the parameters ψ and Λ = {W (0:L-1) 1:T , s</p><p>(1:L) 1:T }. Similarly to the previous section, we isolate the terms in the evidence lower bound dependent on ψ and Λ. We have:</p><formula xml:id="formula_18">L(ψ, Λ) = const+ n,t φ n,t l H q ψ (l) t (•|z n = t, x n ) + E h (1:L) n ∼q ψ (1:L) t ln p X (x n , h (1:L) n |z n = t) .<label>(9)</label></formula><p>By taking the expectation over the hidden cluster variables z n , we obtain conditional expectations over the hidden layers h (1:L) n knowing the cluster assignment. In order to backpropagate gradients of Λ and ψ, it suffices to perform a reparameterization trick for each cluster assignment at each hidden layer (proof in Appendix A). We can achieve this by sampling:</p><p>(l) n,t ∼ N (0, I), a sample from the posterior of the l th hidden layer can then be obtained by the following transformation:</p><formula xml:id="formula_19">h (l) n,t = µ ψ (l) t (x n ) + (l) n,t Σ ψ (l) t (x n ),</formula><p>where Σ ψ (l) t (x n ) is supposed to be a diagonal matrix for simplicity. Following the previous analysis, we can derive an algorithm to perform inference on the proposed model, where between iterations of the fixed point update steps, E epochs of gradient ascent are performed to obtain a local maximum of the ELBO with respect to Λ and ψ. Algorithm 1 summarizes the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Semi-Supervised Learning (SSL)</head><p>4.1. SSL using the DP-DGLMM In this section, similarly to <ref type="bibr" target="#b9">(Kingma et al., 2014)</ref> we consider a partially labeled dataset x 1:N = D l ∪ D u , where Algorithm 1 Variational Inference for the DP-DLGMM Input: x 1:N , T, η, α Initialize φ, Λ, ψ while not converged do update: γ t ∀t {(3), (4)} update:</p><formula xml:id="formula_20">m (L) t ∀t {(7)} update: V (L) t ∀t {(8)} for each epoch do Λ ← Λ + α∂ Λ L ψ ← ψ + α∂ ψ L end for update: φ n,t ∀n, ∀t {(5)} end while D l = {x n , y n } n</formula><p>is the labeled part, y n represents the label of the sample x n , and D u represents the unlabeled part. The log likelihood can be divided for the labeled and unlabeled parts as:</p><formula xml:id="formula_21">l(Θ) = ln p Θ (x 1:N ) = xn∈D l ln p Θ (x n ) + xn∈Du ln p Θ (x n ) = xn∈D l ln p Θ (x n , z n = y n ) + xn∈Du ln p Θ (x n ).</formula><p>The last equation follows from the fact that p Θ (x n |z n = y n ) = 0. By dividing the labeled and unlabeled parts of the dataset, we can follow the same approach presented in section 3 in order to derive a variational inference algorithm. In this case, the fixed point updates and the gradient ascent steps remain unchanged if we set φ n,y n = 1 for a labeled x n sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The predictive distribution</head><p>In order to make predictions using the model, we need to evaluate the predictive distribution. Given a new sample x N +1 , the objective is to evaluate the following quantity p(z N +1 = k|x 1:N +1 ). This task requires an intractable marginalization over all the other hidden variables. However, similarly to <ref type="bibr" target="#b0">(Blei et al., 2006)</ref>, we can use the variational posterior to approximate the true posterior, which in turn leads to simpler expectation terms:</p><formula xml:id="formula_22">p(z N +1 = k|x 1:N +1 ) ∝ p(z N +1 = k, x N +1 |x 1:N ) ∝ ∼ E β∼q [π k (β)] ×E h (1:L) N +1 ∼q ψ (1:L) k p X x N +1 |f Λ (h (1:L) N +1 ), z n = k<label>(10)</label></formula><p>where f Λ (•) represents the forward pass over the generative model. The expectation with respect to the beta terms can be computed in closed-form as a product of expectations over the beta posteriors. The second expectation can be evaluated using the Monte-Carlo estimator of equation ( <ref type="formula" target="#formula_13">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation of the semi-supervised classification</head><p>We evaluate the semi-supervised classification capabilities of the model. We train our DP-DLGMM model on the MNIST dataset (LeCun &amp; Cortes, 2010) with train-valid-test splits equal to {45000, 5000, 10000} similarly to <ref type="bibr">(Nalisnick &amp; Smyth, 2016)</ref>, with 10 % labelisation randomly drawn. We run the process for 5 iterations, and we evaluate our model on the test set. We report the mean and standard deviation of the classification error in percentages in Table <ref type="table">1</ref>. Our method produces a competitive score with existing state-of-the art methods: Deep Generative Models (DGM) <ref type="bibr" target="#b9">(Kingma et al., 2014)</ref> and Stick-Breaking Deep Generative Models (SB-DGM) <ref type="bibr">(Nalisnick &amp; Smyth, 2016)</ref>. Unlike the previous approaches, the loss was not up-weighted for the labeled samples. Figure <ref type="figure" target="#fig_0">2</ref> shows the t-SNE projections <ref type="bibr" target="#b12">(Maaten &amp; Hinton, 2008)</ref> obtained with 10 % of the labels provided. We notice that by introducing a small fraction of labels the class structure was highly preserved in the latent space. 6.13 ± .13 4.86 ± .14 3.95 ± .15 2.90 ± .17</p><p>Table <ref type="table">1</ref>. Semi-supervised classification error (%) on the MNIST test set with 10 % labelisation. Comparison with <ref type="bibr">(Nalisnick &amp; Smyth, 2016)</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data generation and visualization</head><p>To further test our model, we generate samples for each cluster from the models trained on both the MNIST and SVHN <ref type="bibr" target="#b15">(Netzer et al., 2011)</ref> datasets. The MNIST model is trained in an unsupervised manner, and the SVHN model is trained with semi-supervision where we provide 1000 randomly generated labels. The samples obtained are represented in figure <ref type="figure" target="#fig_2">4</ref>. For the unsupervised model, we notice that the clusters are representative of the shape of each digit. We plot the t-SNE projections of the MNIST test set of the unsupervised model in Figure <ref type="figure" target="#fig_1">3</ref>. We notice that the digits belonging to the same true class tend to group with each other. However, two groups of the same class can be very separated in the embedding space. The interpretation we can draw from this effect is that the DP-DLGMM tends to separate the latent space in order to distinguish between the variations of hidden representations of the same class. The clusters obtained are not always representative of the true classes which is a common effect with infinite mixture models. In a full unsupervised setting, data can be explained by multiple correct clusterings. This effect can simply be countered by adding a small supervision (figure <ref type="figure" target="#fig_0">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a variational inference method for Dirichlet Process Deep Latent Gaussian Mixture Models. Our approach combines classical variational inference and neural variational inference. The algorithm derived is thus a standard variational inference algorithm, with fixed point updates over a subset of the parameters presenting linear dependencies. The parameters present in nonlinear transformations are updated using standard gradient ascent where the reparameterization trick can be applied for the variational posterior of the stochastic hidden layers knowing the cluster assignments. Our approach shows promising results both for the unsupervised and semi-supervised cases.</p><p>In future work, stochastic variational inference can be explored to speed-up the training procedure. Our approach can also be generalized to other types of deep probabilistic graphical models.</p><p>A. Proof of the reparameterization trick knowing the cluster assignment</p><p>The evidence lower bound of our model can be written in its general form as: where ˆ is a sample drawn from N (0, I), thus we can backpropagate stochastic gradients for each class assignment.</p><formula xml:id="formula_23">L(θ, ψ) = T t=1 φ t × E h∼N (µ ψ t (x),σ ψ t (x) 2 I) [f θ (h)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stochastic Variational Inference</head><p>Updating the Λ and ψ parameters using E epochs of gradient ascent significantly adds to the complexity of Algorithm 1. One possible approach is to perform stochastic variational inference <ref type="bibr" target="#b5">(Hoffman et al., 2013)</ref> for fixed point update equations. This allows for the use of the same batch of data for the gradient ascent steps of Λ and ψ and the stochastic updates of the fixed point equations. Let us consider a batch x 1:B , the updates in this case are:</p><formula xml:id="formula_24">γ (t+1) 1,k = (1 -ρ t )γ (t) 1,k + ρ t N B γ1,k γ (t+1) 2,k = (1 -ρ t )γ (t) 2,k + ρ t N B γ2,k m (t+1) k = (1 -ρ t )m (t) k + ρ t N B mk V (t+1) k = (1 -ρ t )V (t) k + ρ t N B Vk ,</formula><p>where γ1,k , γ2,k , mk , Vk , are computed for the minibatch x 1:B using equations (3),( <ref type="formula" target="#formula_11">4</ref>),(7), and (8) respectively. In order to guarantee convergence ρ t must satisfy:</p><formula xml:id="formula_25">t ρ t = ∞ and t ρ 2 t &lt; ∞.</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2. t-SNE plot of the second stochastic hidden layer on the MNIST test set for the semi-supervised (10% labels) version of the DP-DLGMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3. t-SNE plot of the second stochastic hidden layer on the MNIST test set for the unsupervised version of the DP-DLGMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Generated samples from the DP-DLGMM model for the unsupervised version on the MNIST dataset (left) and the semisupervised version on the SVHN dataset (right).</figDesc><graphic coords="6,90.64,71.32,291.62,88.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>NN</head><label></label><figDesc>(h, µ ψt (x), σ ψt (x) 2 I)f θ (h(h, µ ψt (x), σ ψt (x) 2 I) × f θ (h)|∂ h t |d .By introducing the following transformation:h t ( ) = µ ψt (x) + σ ψt (x)∼ N (0, I), and using the density transformation lemma:N (h, µ ψt (x), σ ψt (x) 2 I)|∂ h t | = N ( ; 0, I),we have:L(θ, ψ) = T t=1 φ t E [f θ (µ ψt (x) + σ ψt (xθ (µ ψt (x) + σ ψt (x) ˆ ).</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>A. Detailed derivation of the variational inference algorithm:</p><p>The evidence lower bound of equation ( <ref type="formula">1</ref>) can be written as :</p><p>in the following we develop this equation in order to derive the nature of the variational posteriors and their fixed point updates.</p><p>A.1. Computing q γt (β t |x 1:N ):</p><p>The distribution q * (β t ) maximizing L, minimizes the kullback-leibler term. Hence:</p><p>By isolating the terms dependent on z n in L, we obtain:</p><p>Supplementary material</p><p>Hence, the optimal distribution q * φn (z n |x n ) maximizing L satisfies:</p><p>where, 1:T present in the evidence lower bound, we have:</p><p>where, V</p><p>. By setting the derivative of L to zero:</p><p>Hence, the closed-form update equations:</p><p>is a diagonal matrix we extract the diagonal updates by multiplying elementwise by the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The predictive distribution:</head><p>In order to perform inference for a new sample x N +1 , we need to compute the predictive distribution p(z N + 1|x 1:N +1 ) : By approximating the true posteriors with the variational posteriors:</p><p>p(β|x 1:N ) ≈ q(β) and p(h|x 1:N , z N +1 = k) ≈ q(h|z N +1 = k)</p><p>We conclude:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational inference for dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<title level="m">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An hdp-hmm for systems with state persistence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05690</idno>
		<title level="m">Stochastic backpropagation through mixture density distributions</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generalized probability density function for double-bounded random processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumaraswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06197</idno>
		<title level="m">Stick-breaking variational autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximate inference for deep latent gaussian mixtures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The generalized reparameterization gradient</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T R</forename><surname>Aueb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A constructive definition of dirichlet priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica sinica</title>
		<imprint>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic variational inference for the hdp-hmm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gultekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="800" to="808" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
