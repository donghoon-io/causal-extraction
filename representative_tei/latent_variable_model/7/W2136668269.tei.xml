<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Predict Gaze in Egocentric Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Predict Gaze in Egocentric Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer's behaviors. Specifically, we compute the camera wearer's head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocentric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advent of wearable cameras, such as GoPro and Google Glass, there has been an increasing interest in egocentric vision-the automatic analysis of video captured from a first-person perspective. A key component in egocentric vision is the egocentric gaze <ref type="bibr" target="#b12">[13]</ref>. Because a person senses the visual world through a series of fixations, egocentric gaze measurements contain important cues regarding the most salient objects in the scene, and the intentions and goals of the camera-wearer. Previous works have demonstrated the utility of gaze measurements in object discovery <ref type="bibr" target="#b16">[17]</ref> and action recognition <ref type="bibr" target="#b6">[7]</ref>.</p><p>This paper addresses the problem of egocentric gaze prediction, which is the task of predicting the user's point-ofgaze given an egocentric video. Previous work on gaze prediction in computer vision has primarily focused on saliency detection <ref type="bibr" target="#b1">[2]</ref>. Previous saliency models can be roughly categorized into either (1) bottom-up approaches <ref type="bibr" target="#b10">[11]</ref> where the gaze is attracted by the discontinuities of low level features, such as color, contrast and edge; or (2) top-down approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref> where the gaze is directed by high level semantics, such as tasks, objects or scene. However, none of these approaches seem to be sufficient to predict egocentric gaze in the context of hand-eye coordination tasks. Salien-cy detection can be effective for visual search, but does not identify the key regions in a manipulation task. Task-driven methods can be effective, but require the identification of current activity, which is an open problem in itself. In this work we explore a third alternative: We address the question of whether measurements of head and hand movements can be used to predict gaze, without reference to saliency or activity models.</p><p>Egocentric gaze in a natural environment is the combination of gaze direction (the line of sight in a head-centered coordinate system), head orientation, and body pose. Especially during object manipulation tasks, eye, head and hand are in continual motion, and the coordination of their movements is requisite <ref type="bibr" target="#b18">[19]</ref>. For example, large head movement is almost always accompanied by a large gaze shift <ref type="bibr" target="#b13">[14]</ref>. Also, the gaze point tends to fall on the object that is currently being manipulated by the first person <ref type="bibr" target="#b13">[14]</ref>. These evidences suggest that we can model the gaze of the first person by exploring the coordination of eye, hands and head, using egocentric cues alone.</p><p>The first part of our paper focuses on gaze prediction. Our major contribution is leveraging the implicit cues that are provided by first person, such as hand location and pose, head/hand motion, for predicting gaze in egocentric vision. We begin with an analysis of gaze tracking data from a wearable eye tracker and demonstrate that: (1) egocentric gaze is statistically different from on-screen eye-tracking;</p><p>(2) there exists a strong coordination of eye, head and hand movements in the object manipulation tasks; (3) these coordinations can be used for predicting gaze in the egocentric setting. Moreover, we build a graphical model for gaze prediction that accounts for eye-hand and eye-head coordinations, and combines the temporal dynamics of gazes. The model requires no information of task or action, predicts gaze position at each frame and identifies moments of fixation. Our gaze prediction results outperform all stateof-the-art bottom-up and top-down saliency detection algorithms by a large margin on two publicly available datasets.</p><p>The second part of our paper explores applications of gaze prediction in egocentric vision. We provide extensive experimental results on two important applications in Figure <ref type="figure">1</ref>. Overview of our approach. We leverage implicit cues that are provided by the first person in an egocentric setting for gaze prediction. Our egocentric features includes head/hand motion and hand location/pose. We design a graphical model for gaze prediction that take account for eye-hand and eye-head coordinations, and combines the temporal dynamics of gazes. Our model predicts gaze position at each frame and identifies moments of fixation with only egocentric videos. We demonstrate two important applications of gaze prediction: object segmentation and gaze prediction. Our gaze prediction, object segmentation and action recognition results outperform several state-of-the-art methods. egocentric vision: (1) foreground object segmentation and (2) egocentric action recognition. Simply by plugging in our gaze prediction, we observe a significant performance boost in comparison to several state-of-the-art methods. In object segmentation, the performance of our model is even comparable to alternative approaches that use ground-truth human gazes. We conclude that our gaze prediction model promises a great prospect for egocentric vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Saliency Detection</head><p>Most of the bottom-up saliency models are based on the Feature Integration Theory <ref type="bibr" target="#b10">[11]</ref>. Low level features such as color, intensity and orientation across scales are aggregated to measure the distinctiveness of a local region. Several methods also model saliency in a probabilistic framework, where the rarity of the feature defines its saliency <ref type="bibr" target="#b1">[2]</ref>. Other approaches also include spectral methods <ref type="bibr" target="#b8">[9]</ref>, sparse and efficient coding <ref type="bibr" target="#b9">[10]</ref>, graphical model <ref type="bibr" target="#b7">[8]</ref> and learning based model <ref type="bibr" target="#b11">[12]</ref>. A recent review of saliency detection can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Very few computational methods have addressed the topdown saliency model. Torralba et al. <ref type="bibr" target="#b23">[24]</ref> considered the contextual guidance of eye movement by combining low level features with high level scene semantics. Our method is closely related to Borji et al. <ref type="bibr" target="#b2">[3]</ref>. They considered a driving simulation scenario where motor action is available as the top-down feature, and learned a direct mapping from both bottom-up and top-down features to fixations. Instead, we address real object manipulation tasks using egocentric vision, assume no additional information other than the video and utilize only egocentric cues for gaze prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Eye, Hand and Head Coordination</head><p>Eye-hand and eye-head coordination has been studied for decades in psychology literature. Most of them are qual-itative rather than quantitative. Land and Hayhoe <ref type="bibr" target="#b14">[15]</ref> studied gaze behavior in natural tasks such as tea making. They found eye fixation usually precedes hand movement by a fraction of second. Pelz et al. <ref type="bibr" target="#b18">[19]</ref> explored the temporal coordination of eye, head, and hand movements while subjects performed a simple block-copying task, where they discovered regular, rhythmic pattern of eye, head, and hand movements. These studies suggest a strong coordination of eye, hand and head movements in natural tasks. However, the coordination is variable under different situations <ref type="bibr" target="#b13">[14]</ref>.</p><p>In the computer vision community, Ba and Odobez [1] presented a model for the recognition of people's visual focus of attention in meetings by approximating gaze direction with head orientation. Yu and Ballard <ref type="bibr" target="#b25">[26]</ref> proposed a HMM model for action recognition based on eye-head-hand coordination in an egocentric setting by tracking gaze, head and hands using additional sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Egocentric Vision</head><p>Egocentric vision is an emerging field in computer vision. The first person perspective provides a consistent view point, a high quality image measurement and minimum amount of occlusion for objects. Spriggs et al. <ref type="bibr" target="#b22">[23]</ref> addressed the segmentation and classification of activities using the first-person sensing. Fathi et al. <ref type="bibr" target="#b5">[6]</ref> proposed to model object, action and activity jointly in egocentric vision. Other egocentric applications include object and activity detection <ref type="bibr" target="#b19">[20]</ref> and video summarization <ref type="bibr" target="#b15">[16]</ref>. Yamada et al. <ref type="bibr" target="#b24">[25]</ref> combined bottom-up visual saliency with ego-motion information for egocentric gaze prediction in a walking or sitting setting. The most relevant work is Fathi et al. <ref type="bibr" target="#b6">[7]</ref>. They presented a joint method for egocentric gaze prediction and action recognition. However, their model requires object masks and action annotations for gaze prediction and the performance drops significantly if gazes are not available or inaccurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Egocentric Cues for Gaze Prediction</head><p>We focus on object manipulation tasks in a meal preparation setting, and explore the possibility of gaze prediction using egocentric cues, including hand/head movement and hand location/pose. The coordination of eye, head and hand, as we show in this section, bridges the gap between these egocentric cues and gaze prediction.</p><p>Throughout the paper, we use public dataset GTEA Gaze and a subset of GTEA Gaze+ (first 15 videos of 5 subjects and 3 recipes) from <ref type="bibr" target="#b6">[7]</ref>. Both datasets contain egocentric videos of meal preparation with gaze tracking results and action annotations. We also consider MIT eye tracking dataset <ref type="bibr" target="#b11">[12]</ref> for comparing gaze statistics. The MIT dataset includes gaze points from 15 subjects watching 1003 images on a screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Eye-Head Coordination</head><p>Several psychophysical experiments have indicated that eye gaze and head pose are coupled in various tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. For example, large head movement is almost always accompanied by a large gaze shift. We explore the eyehead coordination in the object manipulation task by a data driven approach. The gaze statistics suggest a sharp center bias and a strong correlation between head motion and gaze shifts. These findings thus provide powerful cues for gaze prediction.</p><p>Egocentric Head Cues: The direction of first person's head is implicitly represented by the egocentric video itself. In the egocentric setting, the camera is mounted on the first-person's head, continuously capturing the scene in front of the first-person. Thus, the center of the image in the video already gives a rough direction towards which the first-person's head is oriented. We can also estimate the head movement by a global motion vector. Due to the substantial motion in egocentric videos, we apply Large Displacement Optical Flow (LDOF) <ref type="bibr" target="#b3">[4]</ref> between two consecutive frames to get the motion field. Flows on each non-hand pixel are averaged into a 2D global motion vector. Head movements along horizontal and vertical directions are then approximated by the inverse tangent of the global motion vector divided by camera focal length. The approximation, albeit simple, provides a reasonable estimate.</p><p>Center Bias: Our first observation is a sharp center bias of egocentric gaze points. We fit a 2D Gaussian as the center prior to all gaze points in GTEA Gaze and GTEA Gaze+ dataset, respectively, as shown in Fig 2 . In comparison, we also visualize the center prior as a 2D Gaussian from MIT eye tracking dataset <ref type="bibr" target="#b11">[12]</ref>. Egocentric gaze has a much smaller variance in space. This is due to the fact that egocentric vision captures a first-person's perspective in 3D world, where the gaze often aligns with the head orientation. In this case, the needs of large gaze shifts are usually compensated by head movements plus small gaze shifts. Thus, head orientation is a good approximation of gaze. Note that the preference of gaze towards the bottom part of the image is influenced by table-top object manipulation tasks.</p><p>Correlation between Gaze Shifts and Head Motion: We also observe a tight correlation between head motion and gaze shift in the horizontal direction. A scatter plot of gaze shifts (from the center) against head motion for GTEA Gaze+ dataset is shown in Fig 2b . The plot suggests a linear correlation in the horizontal direction, especially for large gaze shifts. Intuitively, one tends to look at his right side if he turns his head towards right. This is again in consistent with the empirical finding. The correlation, therefore, allows us to predict gaze location from head motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Eye-Hand Coordination</head><p>Eye-hand coordination is the key to good performance in object manipulation tasks. Eye gaze generally guides the movement of the hands to target <ref type="bibr" target="#b14">[15]</ref>. Moreover, it has also been shown <ref type="bibr" target="#b20">[21]</ref> that the proprioception of limbs may influence gaze shift, where the hands are used to guide eye movements. We introduce the concept of manipulation point, align gaze points with respect to the first person's hands and discover clusters in the aligned gaze density map, suggesting a strong eye-hand coordination. This suggest that we can predict egocentric gaze by looking into the first-person's hand information.</p><p>Egocentric Hand Cues: Information of hands, including their locations, poses and movements are important egocentric cues for object manipulation. However, accurate tracking of hands in egocentric video is a nontrivial task. We seek to segment the hands from the video and discriminate between left/right/intersecting hands, which provides Figure <ref type="figure">3</ref>. Top row: Hand segmentation and manipulation points (red dots). We present four different hand configurations and the correspondent manipulation points. The hands are colored by their configurations. Bottom row: Aligned gaze density map. We align the gaze points into the hand's coordinates by selecting the manipulation points as the origin, and projecting the gaze point into the new coordinate system every frame. We then plot the density map by averaging the aligned gaze points across all frames within the dataset. High density clusters can be found around the manipulation points, indicating spatial structures for eye-hand coordination. a rough hand pose estimation. We apply textonBoost <ref type="bibr" target="#b21">[22]</ref> with CRF to segment hands in each frame. For each region, we extract spatial and shape features (centroid, orientation of major axis, eccentricity and the area of the hand masks) and train a SVM to assign it to one of the three choices mentioned above. In addition, we assume there are at most two hands from the first person in a single frame. We greedily select at most two confident hand regions (with its area larger than a threshold). We also force mutual exclusiveness between region labels. For example, we can not assign a same label (single left/right hand/intersection hands) to more than one of the hand regions. And intersecting hands and single left/right hand can not show up simultaneously. Hand detection provides hand masks and configurations for each frame. We also extract hand motion by averaging optical flow vectors within the hand mask.</p><p>Manipulation Point: A major challenge for modeling eye-hand coordination is how to represent hands with various poses. Instead of tracking the hand pose, we introduce manipulation point by analyzing hand shapes at each frame. A manipulation point is defined as a control point where the first person is mostly likely to manipulate an object using his hands. For example, for a single left hand, manipulation usually happens on right tip of the hand. For two intersecting hands, the manipulation point is generally around the intersecting part. To find the manipulation point, we match the hand's boundary to configuration dependent templates. Examples can be found in These spatial distributions are consistent with the observation that people tend to look at the object they are manipulating. Thus, they offer a simple cue for gaze prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Gaze Prediction in Egocentric Video</head><p>We have witness strong cues for gaze by the coordination of eye, hand and head movement. However, the flexility of the coordination makes it hard to design a hand-crafted model. Therefore, we present a learning based framework to incorporate all these egocentric cues for gaze prediction. The core of our method lies in a graphical model that combines egocentric cues at a single frame with a temporal model of gaze shifts.</p><p>Our gaze prediction consist of two parts: predicting the gaze position at each frame and identifying the fixations among all gazes. Fixation is defined as the pause of gaze within a spatially limited region (0.5 -1 degree) for a minimum period of time (80 -120ms) <ref type="bibr" target="#b17">[18]</ref>. The modeling of fixations captures the temporal dynamics of gazes. We discuss the egocentric features, design our model and provide our inference algorithm in this section.  <ref type="figure">4</ref>. The graphical model of our gaze prediction method. Our model combines single frame egocentric cues with temporal dynamics of gazes. We extract features zt at each frame t, predict its gaze position gt and identify its moments of fixation mt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Features</head><p>We extract egocentric features regarding the first person's hand and head cues. The feature vector z t for frame t contains the manipulation point (2D), the global motion vector (2D), the hand motion vector (2D), the hand configuration (1D categorical). Therefore, for every frame, we get a 7 dimensional feature if hands are detected or a 2 dimensional feature if no hands are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Model</head><p>Denote the gaze point at frame t as g t = [g x t , g y t ] T ∈ R 2 and its binary label as m t = {0, 1}, where m t = 1 denotes g t is a fixation. Given egocentric cues {z t } for all frames t = 1...K, our goal is to infer the gaze points {g t } and its label {m t = {0, 1}}. We model the conditional probability P ({g t , m t } K t=1 |{z t } K t=1 ) as</p><formula xml:id="formula_0">P ({gt, mt} K t=1 |{zt} K t=1 ) = K t=1 P (gt|zt) K t=1 P (mt|g N (t) ),<label>(1)</label></formula><p>where g N (t) are the temporal neighbors of g t . In our model, we set neighborhood to be two consecutive frames (133ms for GTEA Gaze and 80ms for GTEA Gaze+). The choice corresponds to the minimum duration of an eye fixation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. The model consists of 1) P (g t |z t ) a single frame gaze prediction model given z t ; 2) P (m t |g N (t) ) a temporal model that couples fixation m t and gaze prediction g N (t) . The graphical model is shown in Fig <ref type="figure">4</ref>.</p><p>Single Frame Gaze Prediction: We use random regression forest for gaze prediction in a single frame. A random regression forest is an ensemble of decision trees. For each branch node, a feature is selected from a random subset of all features and a decision boundary is set by minimizing the Minimum Square Error (MSE). The leaf nodes keep the mean value of all training samples that end up in the node. And the final result is the weighted average of all leaf nodes that a testing sample reaches. We choose random forest since our feature vector z t contains categorical data, which is easy to handle in a decision tree. We train two separate models for gaze prediction, one with both hand and head cues and one with only head cues. Our model will step back to head motion cues if no hands are detected.</p><p>For simplicity, we train two regression forests for horizontal and vertical direction separately. The regression builds a map f between feature vector z t to a 2D image coordinates gt = f (z t ), i.e. the prediction of gaze point at frame t. The probability P (g t |z t ) is then modeled as a Gaussian centered at gt with covariance</p><formula xml:id="formula_1">Σ s ∈ R 2×2 P (gt|zt) ∝ exp -gt -gt 2 Σs ,<label>(2)</label></formula><p>where g t -gt 2 Σs = (g t -gt ) T Σ -1 s (g t -gt ) is the Mahalanobis distance.</p><p>Fixations and Gazes: Gaze prediction and fixation detection are tightly coupled. On one hand, fixation m t can be detected given all gaze points. On the other hand, there is a strong constraint over gaze locations if we know current gaze point is a fixation. For example, g t should be close to g t-1 if m t = 1. Therefore, we model the conditional probability P (m t |g N (t) ) as</p><formula xml:id="formula_2">P (mt|g N (t) ) ∝ exp   -mt i∈N (t) gi -gt 2 2  <label>(3)</label></formula><p>where m i can be obtained by a fixation detection algorithm given gaze points g N (t) . Here we use a velocity-threshold based fixation detection <ref type="bibr" target="#b17">[18]</ref>: a fixation is detected if velocity of gaze points are below a threshold c over a minimum amount of time (two frames in our case).</p><formula xml:id="formula_3">mt = i∈N (t) -sign( gi -gt 2 2 -c) + 1 2 , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where sign(x) = -1 if x &lt; 0 and sign(x) = 1 if x &gt;= 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference and Learning</head><p>Inference: To get the gaze points {g t } K t=1 and fixations {m t } K t=1 , we apply Maximum Likelihood (ML) estimation of Eq <ref type="bibr" target="#b0">(1)</ref>. The minimization of negative log likelihood function is given by</p><formula xml:id="formula_5">min {g t ,m t } K t=1 -log(P ({gt} K t=1 , {mt} K t=1 |{zt} K t=1 )) = -log K t=1 P (gt|zt) K t=1 P (mt|g N (t) ) = K t=1 gt -gt 2 Σs + λ K-1 t=1 mt gt+1 -gt 2 2 s.t. mt = -sign( gt+1 -gt 2 2 -c) + 1 2 ∀t (5)</formula><p>Projected gradient descent is used to obtain a local minimum of Eq <ref type="bibr" target="#b4">(5)</ref>. We first perform gradient descent over the object function assuming m t is known and ignore the constraints. m t is then updated to make all constraints feasible. These two steps run iteratively until convergence. Intuitively, the optimization follows a EM like updating by (1) identifying fixations m t by velocity-thresholds given all gaze predictions g t and (2) smoothing the gaze points g t given fixation labels m t . Updating m t given g t is straightforward, we estimate m t using Eq (4). Updating g t given m t is more challenging, since g t and g t+1 are coupled together with m t . Given m t , we can rewrite Eq (5) using its matrix form. Let G = [g 1 ... g K ] T , G = [g 1 ... gK ] T and m = [m 1 ... m K ] T . Denote matrix A as the Toeplitz matrix correspondent to the convolution kernels</p><formula xml:id="formula_6">[-1 1] T . The updating of G is equal to min G G -G 2 Σs + λ m T AG 2 2 (6)</formula><p>The solution of Eq ( <ref type="formula">6</ref>) is given by setting the first order derivative to zero</p><formula xml:id="formula_7">G * = Σs + λA T mm T A -1</formula><p>Σs G.</p><p>Learning: Learning the model is relatively easy. We first train the single frame random regression tree, using 40 trees. The parameters needed to be determined now are the velocity threshold c, the covariance matrix Σ s and the constant λ. We select c to be roughly the distance of 1 degree of angular error (50/80 pixels for GTEA Gaze and GTEA Gaze+ respectively). Σ s defines the Mahalanobis distance between gaze points, and is learned by re-sending training samples into random forest and re-estimating the error covariance. We empirically select λ = 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Gaze Prediction</head><p>We use two standard, measures to assess the performance of our gaze prediction method: Area Under (ROC) Curve (AUC) and Average Angular Error (AAE). AUC measures the consistency between a predicted saliency map and the ground truth gaze points in an image, and is widely used in the saliency detection literature. AAE measures the angular distance between the predicted gaze point (e.g. the most salient point) and the groundtruth gaze, and is widely used in the gaze tracking literature. Since our method outputs a single predicted gaze point, we generate a saliency map that can be used for AUC scoring by convolving an isotropic Gaussian over the predicted gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Results</head><p>Both GTEA Gaze and GTEA Gaze+ dataset contain gaze data from eye tracking glasses, which are used as ground truth for gaze prediction. We compare our results with five competing methods: a baseline center prior prediction using 2D Gaussian, three bottom-up saliency detection algorithms (Itti and Koch <ref type="bibr" target="#b10">[11]</ref>, GBVS <ref type="bibr" target="#b7">[8]</ref>, Hou et al. <ref type="bibr" target="#b9">[10]</ref>) and one topdown saliency algorithm <ref type="bibr" target="#b6">[7]</ref>. For all the previous methods, we use the authors' own implementations for benchmarking purposes. The motion cues in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref> are enabled for fair comparison. One issue is that Fathi et al. <ref type="bibr" target="#b6">[7]</ref> requires action labels for gaze prediction. We supply their method Our method consistently generates more accurate predictions, with less AAE than <ref type="bibr" target="#b6">[7]</ref> for 75% of all frames (67% for 2D Gaussian). Right: ROC curves for different methods. Our method requires no information about action or task, and largely outperforms the bottom-up and top-down gaze prediction method.</p><p>with ground truth action labels in all of our experiments. We want to emphasis that our method does not use either bottom-up features or top-down action labels.</p><p>For GTEA Gaze dataset, we use the same training (13 videos) and testing (4 videos) split as <ref type="bibr" target="#b6">[7]</ref> for fair comparison. For GTEA Gaze+ dataset, we perform a five-fold cross validation by using 4 subjects for training and 1 subject for testing. For all our results, we average over 10 runs of random forest. We cannot compare to the results of <ref type="bibr" target="#b6">[7]</ref> on the GTEA Gaze+ dataset, since their method requires object annotations for training, and so far no annotations have been released for this dataset.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows the quantitative comparison of AUC, AAE and the ROC curve in GTEA Gaze. Our method with head cues achieved AUC score of 82.3% and AAE of 10.68 degree. Adding hand cues significantly improved the AUC score (86.7%) and reduced the AAE (8.85 degree). Our temporal model added another 1% of AUC and 0.5 degree of AAE. Overall, our combined model achieved AUC score of 87.8%, where the state-of-the-art <ref type="bibr" target="#b6">[7]</ref> gives 83.6% by using the ground truth action labels in testing. Our method also ranks highest for AAE with 8.35 degree, where the second best is 2D Gaussian (10.16 degree).</p><p>Our method works surprisingly well and outperform the sophisticated top-down method <ref type="bibr" target="#b6">[7]</ref> by 4.2%. Our method benefits from using the strong egocentric cues (head, hand and eye coordination) for gaze prediction and bypasses the challenging object segmentation step required by <ref type="bibr" target="#b6">[7]</ref>. Another interesting finding is that the center prior gives better accuracy than all of the bottom-up results in AUC with a reasonable AAE. These results suggest that egocentric cues can provide a reliable gaze estimate without low-level im- age features or high-level task constraints.</p><p>We also tested our method in GTEA Gaze+ dataset. The results, including AUC, AAE and the ROC curve are shown in Fig 6 . Our final method has the best AUC of 86.7% and the best AAE of 7.93 degree, outperforming the second best (2D Gaussian) by 4.8% and 0.7 degree respectively. Using head motion already outperformed the center prior and adding hand cues further improved the results. Again, the center prior performs better than bottom-up methods. One possible explanation is that bottom-up saliency may be an effective predictor for visual search tasks, where image features may naturally draw the viewer's attention during a scan. However, for hand-eye coordination tasks the gaze is naturally-coordinated with the head, making the head orientation a more effective approximation. In addition, GTEA Gaze+ dataset provides ground truth labels for fixations from the eye tracking glasses. Our temporal model for fixation detection achieved 84.7% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Object Segmentation</head><p>We further demonstrate that gaze prediction can be used to segment task-relevant foreground objects. Each video (of the 6 testing videos) is split into non-overlapping 1.5second clips. We selected the video clips that has an action label which involves object manipulation. Two annotators were asked to select a frame within the clip and manually segment the foreground object that is involved in the action. We obtained 234 object masks from 300 video clips selected from 6 of the videos in GTEA Gaze+. More details can be found in supplementary materials. CPMC achieves the same score by the first 4 segments with human gaze, and by the first 6 segments using our gaze prediction. We also improve CPMC results by 2.6% over top 100 segments using gaze, with only a small performance gap between human gaze and predicted gaze. (b) Confusion matrix of action recognition using predicted gaze on GTEA Gaze dataset (25 classes). The average accuracy is 32.8% in comparison to the baseline 29% <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Results</head><p>We used both our gaze prediction results and the ground truth gaze point to seed two different methods for extracting foreground object regions: ActSeg <ref type="bibr" target="#b16">[17]</ref> and CPMC <ref type="bibr" target="#b4">[5]</ref>. Given the ground truth segmentation, we score the effectiveness of object segmentation under both predicted and measured gaze, thereby obtaining an alternate characterization of the effectiveness of our gaze prediction method. ActSeg <ref type="bibr" target="#b16">[17]</ref> takes gaze points as the input and outputs one object segment per gaze point. It assumes that the gaze point always lies within the object boundary and segments the object by finding the most salient boundary. CPMC <ref type="bibr" target="#b4">[5]</ref> uniformly samples seed points across the entire image, then generates object hypothesis and ranks them. We modified the implementation of CPMC to put the same number of dense seeds only in the vicinity of the gaze point.</p><p>We studied the relationship between foreground object masks and fixation points in our dataset. We found that 82.9% (194/224) of our object annotations contain a fixation in the same frame. And 75.2% of the fixations lies within the foreground object boundary. Moreover, 94.3% of the fixations lies within the 80 pixels (1 degree) from the nearest foreground object boundary. The statistics suggest that human gaze tends to focus on task-relevant objects <ref type="bibr" target="#b14">[15]</ref>. However, it is not always true that the fixation always lies in the object boundary <ref type="bibr" target="#b16">[17]</ref>. Possible explantation includes micro saccade <ref type="bibr" target="#b14">[15]</ref> or gaze tracking error.</p><p>We score the segmentation results by the mean best overlapping scores, defined as the average of best overlap (interaction over union) between a segment and the ground truth. We measure the performance of CPMC by selecting the top K candidates and varying the number of K. The results are reported in <ref type="bibr">Fig 7(a)</ref>. For ActSeg, human gaze gives 31.5% and our gaze prediction gives 21.7% in mean best overlapping score. For CPMC, we get equivalent performance to ActSeg from the first 6 segments, and then improve the results by 2.6% by using predicted gaze with the first 100 segments. The performance using our gaze prediction method is comparable to that using ground truth gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Action Recognition</head><p>Egocentric gaze is not only useful for foreground object segmentation, but also helps to recognize first-person's action. We report action recognition results on GTEA Gaze dataset by plugging in our predicted gaze into the implementation of <ref type="bibr" target="#b6">[7]</ref>, as shown in <ref type="bibr">Fig 7(b)</ref>. Their method extracts motion and appearance features from a small region around a gaze point and trains a SVM classifier combined with HMM for action recognition.</p><p>We compare our results against <ref type="bibr" target="#b6">[7]</ref>. Using our gaze prediction, we improve the action recognition result to 32.8% from the state-of-the-art <ref type="bibr" target="#b6">[7]</ref> at 29%. The upper bound of the method is given by human gaze at an accuracy of 47%. For 7 out of 25 classes, we perform better than <ref type="bibr" target="#b6">[7]</ref>. Again, we can not report results on GTEA Gaze+ due to the lack of object annotations. We notice the large gap between our gaze prediction and real human gaze. However, we conclude the gap is only partly due to gaze prediction since the performance of method <ref type="bibr" target="#b6">[7]</ref> is sensitive to input gaze points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We described a novel approach to gaze prediction in egocentric video. Our method is motivated by the fact that in an egocentric setting, the behaviors of the first-person provide strong cues for predicting the gaze direction. We presented a model that both describes the dynamic behavior of the gaze and also reliably predicts the locations of the gazed points in video. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on GTEA Gaze and GTEA Gaze+ datasets. Finally, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>Portions of this research were supported in part by NSF Awards 0916687 and 1029679, ARO MURI award 58144-NS-MUR, and by the Intel Science and Technology Center in Pervasive Computing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) Center bias (from left to right) for MIT eye tracking dataset, GTEA Gaze dataset and GTEA Gaze+ dataset. Egocentric gaze has a much smaller variance in space. Thus, head orientation provides a good approximation for gaze direction in egocentric videos. (b) A scatter plot of head movement against gaze shift along vertical and horizontal direction in GTEA Gaze+ dataset. The plot suggests a linear correlation in the horizontal direction.</figDesc><graphic coords="3,73.74,72.00,188.99,152.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 3. A manipulation point provides an anchor with respect to current hand pose, and allows us to align gaze points into the hand's coordinates. Gaze around Hands: We align the gaze points to the first-person's hands by setting the manipulation points as the origin (See Fig 3). The density maps of the aligned gaze points for four different hand configurations are plotted in Fig 3. For both GTEA Gaze and GTEA Gaze+ datasets, we observe high density around the manipulation point. The data suggest interesting spatial relationship between manipulation points and gaze points. For single left/right hand, the gaze tends to fall on top right/top left region, where taking/putting actions might happen. For two separate hands, subjects are more likely to look in the middle, where the object usually stays. For two intersecting hands, gaze shifts towards the bottom, partly due to openning/closing actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 4. The graphical model of our gaze prediction method. Our model combines single frame egocentric cues with temporal dynamics of gazes. We extract features zt at each frame t, predict its gaze position gt and identify its moments of fixation mt.</figDesc><graphic coords="5,97.36,72.00,141.76,81.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Left: AUC scores and AAE for 8 different methods in GTEA Gaze dataset. Our combined model achieves the highest AUC score (87.8%) and lowest AAE (8.35 Degree) among all methods.Our method consistently generates more accurate predictions, with less AAE than<ref type="bibr" target="#b6">[7]</ref> for 75% of all frames (67% for 2D Gaussian). Right: ROC curves for different methods. Our method requires no information about action or task, and largely outperforms the bottom-up and top-down gaze prediction method.</figDesc><graphic coords="6,314.77,72.00,224.42,146.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Left: AUC scores and AAE for 7 different methods in GTEA Gaze+ dataset. Again, our combined model outperforms all other methods in both AUC score and AAE. Our method has less AAE than second best (2D Gaussian) for 69% of all frames. Right: ROC curves for different methods. It is interesting to find that the 2D Gaussian consistently outperforms bottom-up methods.</figDesc><graphic coords="7,56.02,72.00,224.44,140.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Foreground object segmentation results. We plug in our gaze prediction into two different algorithms. For ActSeg, human gaze achieves 31.5% and our gaze prediction reaches 21.7%.CPMC achieves the same score by the first 4 segments with human gaze, and by the first 6 segments using our gaze prediction. We also improve CPMC results by 2.6% over top 100 segments using gaze, with only a small performance gap between human gaze and predicted gaze. (b) Confusion matrix of action recognition using predicted gaze on GTEA Gaze dataset (25 classes). The average accuracy is 32.8% in comparison to the baseline 29%<ref type="bibr" target="#b6">[7]</ref>.</figDesc><graphic coords="7,314.77,72.00,224.42,106.05" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiperson visual focus of attention from head pose and meeting contextual cues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic learning of taskspecific visual attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2012. 1, 2, 3, 6, 7, 8</date>
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998. 1, 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">First-person vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2442" to="2453" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The coordination of rotations of the eyes, head and trunk in saccadic turns produced in natural situations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2004">2004. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In what ways do eye movements contribute to everyday activities?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3559" to="3565" />
			<date type="published" when="2001">2001. 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active visual segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The coordination of eye, head, and hand movements in a natural task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Loeber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coordinate transformations for hand-guided saccades</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="465" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal segmentation and activity classification from first-person sensing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Spriggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention prediction in egocentric video using motion and visual saliency</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Image and Video Technology</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7087</biblScope>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding human behaviors based on eyehead-hand coordination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biologically Motivated Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2525</biblScope>
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
