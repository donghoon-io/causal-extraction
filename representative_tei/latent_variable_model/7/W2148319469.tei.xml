<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors</title>
				<funder ref="#_mjV8p7U">
					<orgName type="full">Engineering &amp; Physical Sciences Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2005-11-09">November 9, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Girolami</surname></persName>
							<email>girolami@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Rogers</surname></persName>
							<email>srogers@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-11-09">November 9, 2005</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling from the parameter posterior. By adopting such a data augmentation strategy, dispensing with priors over regression coefficients in favour of Gaussian Process (GP) priors over functions, and employing variational approximations to the full posterior we obtain efficient computational methods for Gaussian Process classification in the multi-class setting 1 . The model augmentation with additional latent variables ensures full a posteriori class coupling whilst retaining the simple a priori independent GP covariance structure from which sparse approximations, such as multi-class Informative Vector Machines (IVM), emerge in a very natural and straightforward manner. This is the first time that a fully Variational Bayesian treatment for multi-class GP classification has been developed without having to resort to additional explicit approximations to the non-Gaussian likelihood term. Empirical comparisons with exact analysis via MCMC and Laplace approximations illustrate the utility of the variational approximation as a computationally economic alternative to full MCMC and it is shown to be more accurate than the Laplace approximation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In <ref type="bibr" target="#b0">(Albert and Chib, 1993)</ref> it was first shown that by augmenting binary and multinomial probit regression models with a set of continuous latent variables y k , corresponding to the k'th response value where y k = m k + , ∼ N (0, 1) and m k = j β kj x j , an exact Bayesian analysis can be performed by Gibbs sampling from the parameter posterior. As an example consider binary probit regression on target variables t n ∈ {0, 1}, the probit likelihood for the nth data sample taking unit value (t n = 1) is P (t n = 1|x n , β) = Φ(β T x n ), where Φ is the standardised Normal Cumulative Distribution Function (CDF). Now, this can be obtained by the following marginalisation P (t n = 1, y n |x n , β)dy n = P (t n = 1|y n )p(y n |x n , β)dy n and as by definition P (t n = 1|y n ) = δ(y n &gt; 0) then we see that the required marginal is simply the normalizing constant of a left truncated univariate Gaussian so that P (t n = 1|x n , β) = δ(y n &gt; 0)N yn (β T x n , 1)dy n = Φ(β T x n ).</p><p>The key observation here is that working with the joint distribution P (t n = 1, y n |x n , β) = δ(y n &gt; 0)N yn (β T x n , 1) provides a straightforward means of Gibbs sampling from the parameter posterior which would not be the case if the marginal term, Φ(β T x n ), was employed in defining the joint distribution over data and parameters.</p><p>This data augmentation strategy can be adopted in developing efficient methods to obtain binary and multi-class Gaussian Process (GP) <ref type="bibr" target="#b27">(Williams and Rasmussen, 1996</ref>) classifiers as will be presented in this paper. With the exception of <ref type="bibr" target="#b17">(Neal, 1998)</ref>, where a full Markov Chain Monte Carlo (MCMC) treatment to GP based classification is provided, all other approaches have focussed on methods to approximate the problematic form of the posterior<ref type="foot" target="#foot_2">foot_2</ref> which allow analytic marginalisation to proceed. Laplace approximations to the posterior were developed in <ref type="bibr" target="#b26">(Williams and Barber, 1998)</ref> whilst lower &amp; upper bound quadratic likelihood approximations were considered in <ref type="bibr" target="#b6">(Gibbs, 2000)</ref>. Variational approximations for binary classification were developed in <ref type="bibr" target="#b23">(Seeger, 2000)</ref> where a logit likelihood was considered and mean field approximations were applied to probit likelihood terms in <ref type="bibr" target="#b18">(Opper and Winther, 2000)</ref>, <ref type="bibr" target="#b3">(Csato et al, 2000)</ref> respectively. Additionally, incremental <ref type="bibr" target="#b20">(Quinonero-Candela and Winther, 2003)</ref> or sparse approximations based on Assumed Density Filtering (ADF) <ref type="bibr" target="#b4">(Csato and Opper, 2002)</ref>, Informative Vector Machines (IVM) <ref type="bibr" target="#b13">(Lawrence, et al 2003)</ref> and Expectation Propagation (EP) <ref type="bibr" target="#b16">(Minka, 2001;</ref><ref type="bibr" target="#b10">Kim, 2005)</ref> have been proposed. With the exceptions of <ref type="bibr" target="#b26">(Williams and Barber, 1998;</ref><ref type="bibr" target="#b6">Gibbs, 2000;</ref><ref type="bibr" target="#b22">Seeger and Jordan, 2004;</ref><ref type="bibr" target="#b10">Kim, 2005)</ref> the focus of most recent work has largely been on the binary GP classification problem. In <ref type="bibr" target="#b22">(Seeger and Jordan, 2004</ref>) a multi-class generalisation of the IVM is developed where the authors employ a multinomial-logit softmax likelihood. However, considerable representational effort is required to ensure that the scaling of computation and storage required of the proposed method matches that of the original IVM with linear scaling in the number of classes. In contrast, by adopting the probabilistic representation of <ref type="bibr" target="#b0">(Albert and Chib, 1993)</ref> we will see that GP based K-class classification and efficient sparse approximations (IVM generalisations with scaling linear in the number of classes) can be realised by optimising a strict lower-bound of the marginal likelihood of a multinomial probit regression model which requires the solution of K computationally independent GP regression problems whilst still operating jointly (statistically) on the data. We will also show that the accuracy of this approximation is comparable to that obtained via MCMC.</p><p>The following section now introduces the multinomial-probit regression model with Gaussian Process priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multinomial Probit Regression</head><p>Define the data matrix as X = [x 1 , • • • , x N ] T which has dimension N × D and the N × 1 dimensional vector of associated target values as t where each element t n ∈ {1, • • • , K}. The N × K matrix of GP random variables m nk is denoted by M. We represent the N × 1 dimensional columns of M by m k and the corresponding K × 1 dimensional rows by m n . The N × K matrix of auxiliary variables y nk is represented as Y, where the N × 1 dimensional columns are denoted by y k and the corresponding K × 1 dimensional rows as y n . The M × 1 vector of covariance kernel hyper-parameters for each class<ref type="foot" target="#foot_3">foot_3</ref> is denoted by ϕ k and associated hyper-parameters</p><formula xml:id="formula_0">ψ k &amp; α k complete the model.</formula><p>The graphical representation of the conditional dependency structure in the auxiliary variable multinomial probit regression model with GP priors in the most general case is shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prior Probabilities</head><p>From the graphical model in Figure (1) a priori we can assume class specific GP independence and define model priors such that</p><formula xml:id="formula_1">m k |X, ϕ k ∼ GP (ϕ k ) = N m k (0, C ϕ k )</formula><p>, where the matrix C ϕ k , of dimension N × N defines the class specific GP covariance<ref type="foot" target="#foot_4">foot_4</ref> . Typical examples of such GP covariance functions are radial basis style functions such that the i, j'th element of each C ϕ k is defined as exp{-M d=1 ϕ kd (x idx jd ) 2 } where in this case M = D, however there are many other forms of covariance functions which may be employed within the GP function prior, see for example <ref type="bibr">(McKay, 2003)</ref>.</p><p>As in <ref type="bibr" target="#b0">(Albert and Chib, 1993)</ref> we employ a standardised normal noise model such that the prior on the auxilliary variables is y nk |m nk ∼ N y nk (m nk , 1) to ensure appropriate matching with the probit function. Of course rather than having this variance fixed it could also be made an additional free parameter of the model and therefore would yield a scaled probit function. For the presentation here we restrict ourselves to the standardised model and consider extensions to a scaled probit model as possible further work. The relationship between the additional latent variables y n (denoting the n'th row of Y) and the targets t n as defined in multinomial probit regression <ref type="bibr" target="#b0">(Albert and Chib, 1993)</ref> is adopted here, i.e.</p><formula xml:id="formula_2">t n = j if y nj = max 1≤k≤K {y nk }<label>(1)</label></formula><p>This has the effect of dividing R K (y space) into K non-overlapping Kdimensional cones C k = {y : y k &gt; y i , k = i} where R K = ∪ k C k and so each P (t n = i|y n ) can be represented as δ(y ni &gt; y nk ∀ k = i). We then see that similar to the binary case where the probit function emerges from explicitly marginalising the auxiliary variable the multinomial probit takes the form given below, where details are given in Appendix I.</p><formula xml:id="formula_3">P (t n = i|m n ) = δ(y ni &gt; y nk ∀ k = i) K j=1 p(y nj |m nj )dy = C i K j=1 p(y nj |m nj )dy = E p(u) j =i Φ(u + m ni -m nj )</formula><p>where the random variable u is standardised normal i.e. p(u) = N (0, 1). An hierarchic prior on the covariance function hyper-parameters is employed such that each hyper-parameter has, for example, an independent exponential distribution ϕ kd ∼ Exp(ψ kd ) and a gamma distribution is placed on the mean values of the exponential ψ kd ∼ Γ(σ k , τ k ) thus forming a conjugate pair. Of course, as detailed in <ref type="bibr" target="#b7">(Girolami and Rogers, 2005)</ref>, a more general form of covariance function can be employed that will allow the integration of heterogeneous types of data which takes the form of a weighted combination of base covariance functions. The associated hyper-hyper-parameters</p><formula xml:id="formula_4">α = {σ k=1,••• ,K , τ k=1,••• ,K</formula><p>} can be estimated via type-II maximum likelihood or set to reflect some prior knowledge of the data. Alternatively, vague priors can be employed such that, for example, each σ k = τ k = 10 -6 . Defining the parameter set as Θ = {Y, M} and the hyper-parameters as</p><formula xml:id="formula_5">Φ = {ϕ k=1,••• ,K , ψ k=1,••• ,K } the joint likelihood takes the form below. p(t, Θ, Φ|X, α) = N n=1 K i=1 δ(y ni &gt; y nk ∀ k = i)δ(t n = i) × K k=1 p(y nk |m nk )p(m k |X, ϕ k )p(ϕ k |ψ k )p(ψ k |α k ) (2)</formula><p>4 Gaussian Process Multi-Class Classification</p><p>We now consider both exact and approximate Bayesian inference for GP classification with multiple classes employing the multinomial-probit regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exact Bayesian Inference: The Gibbs Sampler</head><p>The representation of the joint likelihood (Equation <ref type="formula">2</ref>) is particularly convenient in that samples can be drawn from the full posterior over the model parameters (given the hyper-parameter values) p(Θ|t, X, Φ, α) using a Gibbs sampler in a very straightforward manner with scaling per sample of O(KN 3 ). Full details of the Gibbs sampler are provided in Appendix IV and this sampler will be employed in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Bayesian Inference: The Laplace Approximation</head><p>The Laplace approximation of the posterior over GP variables, p(M|t, X, Φ, α) (where Y is marginalised), requires finding the mode of the unnormalised posterior. Approximate Bayesian inference for GP classification with multipleclasses employing a multinomial-logit (softmax) likelihood has been developed previously in <ref type="bibr" target="#b26">(Williams and Barber, 1998)</ref>. Due to the form of the multinomial-logit likelihood a Newton iteration to obtain the posterior mode will scale at best as O(KN 3 ). Employing the multinomial-probit likelihood we find that each Newton step will scale as O(K 3 N 3 ) and details are provided in Appendix V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate Bayesian Inference: A Variational and Sparse Approximation</head><p>Employing a variational Bayes approximation <ref type="bibr" target="#b1">(Beal, 2003;</ref><ref type="bibr" target="#b9">Jordan, et al 1999;</ref><ref type="bibr">McKay, 2003)</ref> by using an approximating ensemble of factored posteriors such that p(Θ|t, X, Φ, α)</p><formula xml:id="formula_6">≈ i=1 Q(Θ i ) = Q(Y)Q(M)</formula><p>for multinomialprobit regression is more appealing from a computational perspective as a sparse representation, with scaling O(KN S 2 ) (where S is the subset of samples entering the model and S N ), can be obtained in a straightforward manner as will be shown in the following sections. The lower bound<ref type="foot" target="#foot_5">foot_5</ref> , see for example <ref type="bibr" target="#b1">(Beal, 2003;</ref><ref type="bibr" target="#b9">Jordan, et al 1999;</ref><ref type="bibr">McKay, 2003)</ref>, on the marginal likelihood log p(t|X, Φ, α) ≥ E Q(Θ) {log p(t, Θ|X, Φ, α)}-E Q(Θ) {log Q(Θ)} is minimised by distributions which take an unnormalised form of Q(Θ i ) ∝ exp E Q(Θ\Θ i ) {log P (t, Θ|X, Φ, α)} where Q(Θ\Θ i ) denotes the ensemble distribution with the i th component of Θ removed. Details of the required posterior components are given in the Appendix.</p><p>The approximate posterior over the GP random variables takes a factored form such that</p><formula xml:id="formula_7">Q(M) = K k=1 Q(m k ) = K k=1 N m k ( m k , Σ k ) (3)</formula><p>where the shorthand tilde notation denotes posterior expectation i.e. f (a) = E Q(a) {f (a)} and so the required posterior mean for each k is given as</p><formula xml:id="formula_8">m k = Σ k y k where Σ k = C ϕ k (I + C ϕ k ) -1</formula><p>(see Appendix I for full details). We will see that each row, y n , of Y will have posterior correlation structure induced ensuring that the appropriate class-conditional posterior dependencies will be induced in M. It should be stressed here that whilst there are K a posteriori independent GP processes the associated K-dimensional posterior means for each of N data samples induces posterior dependencies between each of the K columns of M due to the posterior coupling over each of the auxiliary variables y n . We will see that this structure is particularly convenient in obtaining sparse approximations <ref type="bibr" target="#b13">(Lawrence, et al 2003)</ref> for the multi-class GP in particular.</p><p>Due to the multinomial probit definition of the dependency between each element of y n and t n (Equation.1) the posterior for the auxiliary variables follows as</p><formula xml:id="formula_9">Q(Y) = N n=1 Q(y n ) = N n=1 N tn yn ( m n , I)<label>(4)</label></formula><p>where N tn yn ( m n , I) denotes a conic truncation of a multivariate Gaussian such that if t n = i where i ∈ {1, • • • , K} then the i'th dimension has the largest value. The required posterior expectations y nk for all k = i and y ni follow as</p><formula xml:id="formula_10">y nk = m nk - E p(u) N u ( m nk -m ni , 1)Φ n,i,k u E p(u) Φ(u + m ni -m nk )Φ n,i,k u<label>(5)</label></formula><formula xml:id="formula_11">y ni = m ni - j =i y nj -m nj (6)</formula><p>where Φ n,i,k u = j =i,k Φ(u + m nim nj ), and p(u) = N u (0, 1). The expectations with respect to p(u) which appear in Equation ( <ref type="formula" target="#formula_10">5</ref>) can be obtained by quadrature or straightforward sampling methods.</p><p>If we also consider the set of hyper-parameters, Φ, in this variational treatment then the approximate posterior for the covariance kernel hyperparameters takes the form of</p><formula xml:id="formula_12">Q(ϕ k ) ∝ N m k (0, C ϕ k ) M d=1 Exp(ϕ kd | ψ kd )</formula><p>and the required posterior expectations can be estimated employing importance sampling. Expectations can be approximated by drawing S samples such that each ϕ s kd ∼ Exp( ψ kd ) and so</p><formula xml:id="formula_13">f (ϕ k ) ≈ S s=1 f (ϕ s k )w(ϕ s k ) where w(ϕ s k ) = N m k 0, C ϕ s k S s =1 N m k 0, C ϕ s k (7)</formula><p>This form of importance sampling within a variational Bayes procedure has been employed previously in <ref type="bibr" target="#b12">(Lawrence, et al 2004)</ref>. Clearly the scaling of the above estimator per sample is similar to that required in the gradient based methods which search for optima of the marginal likelihood as employed in GP regression and classification e.g. <ref type="bibr">(McKay, 2003)</ref>.</p><p>Finally we have that each</p><formula xml:id="formula_14">Q(ψ kd ) = Γ ψ kd (σ k + 1, τ k + ϕ kd )</formula><p>and the associated posterior mean is simply ψ kd = (σ k + 1)/(τ k + ϕ kd ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summarising Variational Multi-Class GP Classification</head><p>We can summarise what has been presented by the following iterations which, in the general case, for all k and d, will optimise the bound on the marginal likelihood (explicit expressions for the bound are provided in Appendix III).</p><formula xml:id="formula_15">m k ← C ϕ k (I + C ϕ k ) -1 ( m k + p k ) (8) ϕ k ← s ϕ s k w(ϕ s k )<label>(9)</label></formula><formula xml:id="formula_16">ψ kd ← σ k + 1 τ k + ϕ kd (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where each ϕ s kd ∼ Exp( ψ kd ), w(ϕ s k ) is defined as previously and p k is the k th column of the N × K matrix P whose elements p nk are defined by the rightmost terms in Equations (5 &amp; 6) i.e. for t n = i then for all k = i p nk = -</p><formula xml:id="formula_18">E p(u){ Nu( m nk -m ni ,1)Φ n,i,k u } E p(u) {Φ(u+ m ni -m nk )Φ n,i,k u } and p ni = -j =i p nj .</formula><p>These iterations can be viewed as obtaining K One against All binary classifiers, however, most importantly they are not statistically independent of each other but are a posteriori coupled via the posterior mean estimates of each of the auxiliary variables y n . The computational scaling will be linear in the number of classes and cubic in the number of data points O(KN 3 ). It is worth noting that if the covariance function hyper-parameters are fixed then the costly matrix inversion only requires to be computed once. The Laplace approximation will require a matrix inversion for each Newton step when finding the mode of the posterior <ref type="bibr" target="#b26">(Williams and Barber, 1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Binary Classification</head><p>Previous variational treatments of GP based binary classification include <ref type="bibr" target="#b23">(Seeger, 2000;</ref><ref type="bibr" target="#b18">Opper and Winther, 2000;</ref><ref type="bibr" target="#b6">Gibbs, 2000;</ref><ref type="bibr" target="#b4">Csato and Opper, 2002;</ref><ref type="bibr" target="#b3">Csato et al, 2000)</ref>. It is however interesting to note in passing that for binary classification, the outer plate in Figure ( <ref type="formula" target="#formula_2">1</ref>) is removed and further simplification follows as only K -1 i.e. one set of posterior mean values requires to be estimated and as such the posterior expectations m = C ϕ (I + C ϕ ) -1 y now operate on N × 1 dimensional vectors m and y. The posterior Q(y) is now a product of truncated univariate Gaussians and as such the expectation for the latent variables y n has an exact analytic form. For a unit-variance Gaussian truncated below zero if t n = 1 and above zero if t n = -1 the required posterior mean y has elements which can be obtained by the following analytic expression derived from straightforward results for corrections to the mean of a Gaussian due to truncation 6 y n = m n + t n N mn (0, 1)/ Φ(t n m n ). So the following iteration will guarantee an increase in the bound of the marginal likelihood</p><formula xml:id="formula_19">m ← C ϕ (I + C ϕ ) -1 ( m + p)<label>(11)</label></formula><p>where each element of the N ×1 vector p is defined as</p><formula xml:id="formula_20">p n = t n N mn (0, 1)/Φ(t n m n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Variational Predictive Distributions</head><p>The predictive distribution, P (t new = k|x new , X, t) 7 , for a new sample x new follows from results for standard GP regression. The N × 1 vector C new ϕ k contains the covariance function values between the new point and those contained in X, and c new ϕ k denotes the covariance function value for the new point and itself. So the GP posterior p(m new |x new , X, t) is a product of K Gaussians each with mean and variance</p><formula xml:id="formula_21">m new k = y T k (I + C ϕ k ) -1 C new ϕ k σ 2 k,new = c new ϕ k -(C new ϕ k ) T (I + C ϕ k ) -1 C new ϕ k 6 For t = +1 then y = +∞ 0 yN y ( m, 1)/{1 -Φ(-m)}dy = m + N ¡ m (0, 1)/Φ( m) and for t = -1 then y = 0 -∞ yN y ( m, 1)/Φ(-m)dy = m -N ¡ m (0, 1)/Φ(-m). 7</formula><p>Conditioning on Y, ϕ, ψ, and α is implicit.</p><p>using the following shorthand</p><formula xml:id="formula_22">ν new k = 1 + σ 2</formula><p>k ,new then it is straightforward (details in Appendix II) to obtain the predictive distribution over possible target values as</p><formula xml:id="formula_23">P (t new = k|x new , X, t) = E p(u) j =k Φ 1 ν new j u ν new k + m new k -m new j</formula><p>where, as before, u ∼ N u (0, 1). The expectation can be obtained numerically employing sample estimates from a standardised Gaussian. For the binary case then the standard result follows</p><formula xml:id="formula_24">P (t new = 1|x new , X, t) = δ(y new &gt; 0)N y new ( m new , ν new ) dy new = 1 -Φ - m new ν new = Φ m new ν new 5 Sparse Variational Multi-Class GP Classi- fication</formula><p>The dominant O(N 3 ) scaling of the matrix inversion required in the posterior mean updates in GP regression has been the motivation behind a large body of literature focusing on reducing this cost via reduced rank approximations <ref type="bibr" target="#b28">(Williams and Seeger, 2001)</ref> and sparse online learning <ref type="bibr" target="#b4">(Csato and Opper, 2002;</ref><ref type="bibr" target="#b20">Quinonero-Candela and Winther, 2003)</ref> where Assumed Density Filtering (ADF) forms the basis of online learning and sparse approximations for GP's. Likewise in <ref type="bibr" target="#b13">(Lawrence, et al 2003)</ref> the Informative Vector Machine (IVM) (refer to <ref type="bibr" target="#b14">(Lawrence, et al 2005)</ref> for comprehensive details) is proposed which employs informative point selection criteria <ref type="bibr" target="#b21">(Seeger, et al 2003)</ref> and ADF updating of the approximations of the GP posterior parameters. Only binary classification has been considered in <ref type="bibr" target="#b13">(Lawrence, et al 2003;</ref><ref type="bibr" target="#b4">Csato and Opper, 2002;</ref><ref type="bibr" target="#b20">Quinonero-Candela and Winther, 2003)</ref> and it is clear from <ref type="bibr" target="#b22">(Seeger and Jordan, 2004</ref>) that extension of ADF based approximations such as IVM to the multi-class problem is not at all straightforward when a multinomial-logit softmax likelihood is adopted. However, we now see that sparse GP based classification for multiple classes (multi-class IVM) emerges as a simple by-product of online ADF approximations to the parameters of each Q(m k ) (multivariate Gaussian). The ADF approximations when adding the n th data sample, selected at the l th of S iterations, for each of the K GP posteriors, Q(m k ), follow simply from details in <ref type="bibr" target="#b14">(Lawrence, et al 2005)</ref> as given below.</p><formula xml:id="formula_25">Σ k,n ← C n ϕ k -M T k M k,n<label>(12)</label></formula><formula xml:id="formula_26">s k ← s k - 1 1 + s kn diag Σ k,n Σ T k,n<label>(13)</label></formula><formula xml:id="formula_27">M l k ← 1 √ 1 + s kn Σ T k,n<label>(14)</label></formula><formula xml:id="formula_28">m k ← m k + y nk -m nk 1 + s kn Σ k,n<label>(15)</label></formula><p>Each y nkm nk = p nk as defined in Section (4.4) and can be obtained from the current stored approximate values of each m n1 , • • • , m nK via equations (5 &amp; 6), Σ k,n , an N × 1 vector, is the n th column of the current estimate of each Σ k , likewise C n ϕ k is the n th column of each GP covariance matrix. All elements of each M k and m k are initialised to zero whilst each s k has initial unit values. Of course there is no requirement to explicitly store each N × N dimensional matrix Σ k , only the S × N matrices M k and N × 1 vectors s k require storage and maintenance. We denote indexing into the l th row of each M k by M l k , and the n th element of each s k by s kn which is the estimated posterior variance.</p><p>The efficient Cholesky factor updating as detailed in <ref type="bibr" target="#b14">(Lawrence, et al 2005)</ref> will ensure that for N data samples, K distinct GP priors, and a maximum of S samples included in the model where S &lt;&lt; N then at most O(KSN ) storage and O(KN S 2 ) compute scaling will be realised.</p><p>As an alternative to the entropic scoring heuristic of <ref type="bibr" target="#b21">(Seeger, et al 2003;</ref><ref type="bibr" target="#b13">Lawrence, et al 2003)</ref> we suggest that an appropriate criterion for point inclusion assessment will be the posterior predictive probability of a target value given the current model parameters for points which are currently not included in the model i.e. P (t m |x m , {m k }, {Σ k }), where the subscript m indexes such points. From the results of the previous section this is equal to P r(y m ∈ C tm=k ) which is expressed as</p><formula xml:id="formula_29">E p(u) j =k Φ 1 ν jm [uν km + m mk -m mj ] (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>where k is the value of t m , ν jm = 1 + s jm , and so the data point with the smallest posterior target probability should be selected for inclusion. This scoring criterion requires no additional storage overhead as all m k and s k are already available and it can be computed for all m not currently in the model in, at most, O(KN ) time 8 . Intuitively points in regions of low target posterior certainty, i.e. class boundaries, will be the most influential in updating the approximation of the target posteriors. And so the inclusion of points with the most uncertain target posteriors will yield the largest possible translation of each updated m k into the interior of their respective cones C k . Experiments in the following section will demonstrate the effectiveness of this multi-class IVM.</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Illustrative Multi-Class Toy Example</head><p>Ten dimensional data vectors, x, were generated such that if t = 1 then 0.5 &gt; x 2 1 + x 2 2 &gt; 0.1, for t = 2 then 1.0 &gt; x 2 1 + x 2 2 &gt; 0.6 and for t = 3 then [x 1 , x 2 ] T ∼ N (0, 0.01I) where I denotes an identity matrix of appropriate dimension. Finally x 3 , • • • , x 10 are all distributed as N (0, 1). Both the first two dimensions are required to define the three class labels with the remaining eight dimensions being irrelevant to the classification task. Each of the three target values were sampled uniformly thus creating a balance of samples drawn from the three target classes.</p><p>Two hundred and forty draws were made from the above distribution and the sample was used in the proposed variational inference routine with a further 4620 points being used to compute a 0-1 loss class prediction error. A common radial basis covariance function of the form exp{d ϕ d |x id -x jd | 2 } was employed and vague hyper-parameters, σ = τ = 10 -3 were placed on the length-scale hyper-parameters ψ 1 , • • • , ψ 10 . The posterior expectations of the auxiliary variables y were obtained from Equations 5 &amp; 6 where the Gaussian integrals were computed using 1000 samples drawn from p(u) = N (0, 1). The variational importance sampler employed 500 samples drawn from each Exp( ψ d ) in estimating the corresponding posterior means ϕ d for the covariance function parameters. Each M and Y were initialised randomly and ϕ had unit initial values. In this example the variational iterations ran for fifty steps where each step corresponds to the sequential posterior mean updates of Equation (8,9,10). The value of the variational lower-bound was  <ref type="bibr" target="#b17">(Neal, 1998)</ref> where the eight irrelevant features are effectively removed from the model.</p><p>From Figure (2.c) we can see that the development of the predictive performance (out of sample) follows that of the lower-bound (Figure <ref type="figure" target="#fig_1">2</ref>.a) achieving a predictive performance of 99.37% at convergence. As a comparison to our multi-class GP classifier we use a Directed Acyclic Graph (DAG) SVM <ref type="bibr" target="#b8">(Platt, et al 2000)</ref> (assuming equal class distributions the scaling 9 is O (N 3 K -1 )) on this example. Employing the values of the posterior mean values of the covariance function length scale parameters (one for each of the ten dimensions) estimated by the proposed variational procedure in the RBF kernel of the DAG SVM a predictive performance of 99.33% is obtained. So, on this dataset, the proposed GP classifier has comparable performance, under 0-1 loss, to the DAG SVM. However the estimation of the covariance function parameters is a natural part of the approximate Bayesian inference routines employed in GP classification. There is no natural method of obtaining estimates of the ten kernel parameters for the SVM without resorting to cross-validation (CV), which in the case of a single parameter, is feasible but rapidly becomes infeasible as the number of parameters increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparing Laplace &amp; Variational Approximations to Exact Inference via Gibbs Sampling</head><p>This section provides a brief empirical comparison of the Variational approximation, developed in previous sections, to a full MCMC treatment employing the Gibbs sampler detailed in Appendix IV. In addition, a Laplace approximation is also considered in this short comparative study.</p><p>Variational approximations provide a strict lower-bound on the marginal likelihood and it is this bound which is one of the approximations attractive characteristics. However it is less well understood how much parameters obtained from such approximations differ from those obtained via exact methods. Preliminary analysis of the asymptotic properties of variational estimators is provided in <ref type="bibr" target="#b25">(Wang and Titterington, 2004)</ref>. A recent experimental study of EP and Laplace approximations to binary GP classifiers has been undertaken by <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref> and it is motivating to consider a similar comparison for the variational approximation in the multiple-class setting. In <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref> it was observed that the marginal and predictive likelihoods, computed over a wide range of covariance kernel hyper-parameter values, were less well preserved by the Laplace approximation than the EP approximation when compared to that obtained via MCMC. We then consider the predictive likelihood obtained via the Gibbs sampler and compare this to the variational and Laplace approximations of the GP-based classifiers.</p><p>The toy dataset from the previous section is employed and, as in <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref>, a covariance kernel of the form s exp{-ϕ d x idx jd 2 } is adopted. Both s &amp; ϕ are varied in the range (log scale) -1 to +5 and at each pair of hyper-parameter values a multinomial-probit GP classifier is induced using (a) MCMC via the Gibbs sampler, (b) the proposed variational approximation, (c) a Laplace approximation of the probit model. For the Gibbs sampler, after a burn-in of 2000 samples, the following 1000 samples were used for inference purposes and the predictive likelihood (probability of target values in the test set) and test error (0-1 error loss) was estimated from the 1000 post-burn-in samples as detailed in Appendix IV.</p><p>We firstly consider a binary classification problem by merging classes 2 &amp; 3 of the toy data set into one class. The first thing to note from Figure (3) is that the predictive likelihood response under the variational approximation preserves, to a rather good degree, the predictive likelihood response obtained when using Gibbs sampling across the range of hyper-parameter values. However the Laplace approximation does not do as good a job in replicating the levels of the response profile obtained via MCMC over the range of hyper-parameter values considered and this finding is consistent with the results of <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref>. The Laplace approximation to the multinomial-probit model has O(K 3 N 3 ) scaling (Appendix V) which limits its application to situations where the number of classes is small. For this reason, in the following experiments we instead consider the multinomial-logit Laplace approximation <ref type="bibr" target="#b26">(Williams and Barber, 1998)</ref>. In Figure (4) the isocontours of predictive likelihood for the toy dataset in the multi-class setting under various hyper-parameter settings are provided.</p><p>As with the binary case the variational multinomial-probit approximation provides predictive likelihood response levels which are good representations of those obtained from the Gibbs sampler. The Laplace approximation for the multinomial-logit suffers from the same distortion of the contours as does the Laplace approximation for the binary probit, in addition the information in the predictions is lower. We note, as in <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref>, that for s = 1 (log s = 0) the Laplace approximation compares reasonably with results from both MCMC and variational approximations.</p><p>In the following experiment four standard multi-class datasets (Iris, Thyroid, Wine and Forensic Glass) from the UCI Machine Learning Data Repository<ref type="foot" target="#foot_7">foot_7</ref> along with the toy data previously described are used. For each dataset a random 60% training / 40% testing split was used to assess the performance of each of the classification methods being considered and 50 random splits of each data set were used. For the toy dataset 50 random train and test sets were generated. The hyper-parameters, for an RBF covariance function taking the form of exp{d ϕ d x idx jd 2 }, were estimated employing the Variational importance sampler and these were then fixed and employed in all the classification methods considered. The marginal likelihood for the Gibbs sampler was estimated simply by using 1000 samples from the GP prior. For each dataset and each method (multinomial-logit Laplace approximation, Variational approximation &amp; Gibbs sampler) the marginal likelihood (lower-bound in the case of the variational approximation), predictive error (0-1 loss) and predictive likelihood were measured. The results, given as the mean and standard deviation over the 50 data splits, are listed in Table (6.2).</p><p>The predictive likelihood obtained from the multinomial logit Laplace approximation is consistently, across all datasets, lower than that of the Variational approximation and the Gibbs sampler. This indicates that the proximation are in close agreement with those of MCMC whilst the Laplace approximation suffers from some inaccuracy and this has also been reported for the binary classification setting in <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Multi-Class Sparse Approximation</head><p>A further 1000 samples were drawn from the toy data generating process already described and these were used to illustrate the sparse GP multi-class classifier in operation. The posterior mean values of the shared covariance kernel parameters estimated in the previous example were employed here and so the covariance kernel parameters were not estimated. The predictive posterior scoring criterion proposed in Section ( <ref type="formula" target="#formula_10">5</ref>) was employed in selecting points for inclusion in the overall model. To assess how effective this criterion is random sampling was also employed to compare the rates of convergence of both inclusion strategies in terms of predictive 0-1 loss on a held out test set of 2385 samples. A maximum of S = 50 samples were to be included in the model defining a 95% sparsity level.</p><p>In <ref type="bibr">Figure (5.a)</ref> the first two dimensions of the 1000 samples are plotted with the three different target classes denoted by ×, +, • symbols. The isocontours of constant target posterior probability at a level of 1/3 (the decision boundaries) for each of the three classes are shown by the solid and dashed lines. What is interesting is that the 50 included points (circled) all sit close to, or on, the corresponding decision boundaries as would be expected given the selection criteria proposed. These can be considered as a probabilistic analogue to the support vectors of an SVM. The rates of 0-1 error convergence using both random and informative point sampling are shown in <ref type="bibr">Figure (5.b)</ref>. The procedure was repeated twenty times, using the same data samples, and the error bars show one standard deviation over these repeats. It is clear that, on this example at least, random sampling has the slowest convergence, and the informative point inclusion strategy achieves less than 1% predictive error after the inclusion of only 30 data points. Of course we should bridle our enthusiasm by recalling that the estimated covariance kernel parameters are already supplied. Nevertheless, multi-class IVM makes Bayesian GP inference on large scale problems with multiple classes feasible as will be demonstrated in the following example. The averaged predictive performance (percentage predictions correct) over twenty random starts (dashed line denotes random sampling and solid line denotes informative sampling) are shown with the slowest converging plot characterizing what is achieved under a random sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Large Scale Example of Sparse GP Multi-Class Classification</head><p>The Isolet 11 dataset comprises of 6238 examples of letters from the alphabet (26) spoken in isolation by 30 individual speakers, and each letter is represented by 617 features. An independent collection of 1559 spoken letters is available for classification test purposes. The best reported test performance over all 26 classes of letter was 3.27% error achieved using 30-bit error-correcting codes with an artificial neural network. Here we employ a single RBF covariance kernel with a common inverse length-scale of 0.001 (further fine tuning is of course possible) and a maximum of 2000 points from the available 6238 are to be employed in the sparse multi-class GP classifier. As in the previous example data is standardized, both random and informative sampling strategies were employed, with the results given in Figure ( <ref type="formula">6</ref>) illustrating the superior convergence of an informative sampling strategy.</p><p>After including 2000 of the available 6238 samples in the model, under the informative sampling strategy, a test error rate of 3.52% is achieved. We are unaware of any multi-class GP classification method which has been applied to such a large scale problem both in terms of data samples available and the number of classes. The predictive likelihood computed on held-out data for both random sampling (solid line with '+' markers) and informative sampling (solid line with ' ' markers). The predictive likelihood is computed once every 50 inclusion steps. (b) The predictive performance (percentage predictions correct) achieved for both random sampling (solid line with '+' markers) and informative sampling (solid line with ' ' markers)</p><p>A recent paper <ref type="bibr" target="#b19">(Qi, et al 2004)</ref> has presented an empirical study of ARD when employed to select basis functions in Relevance Vector Machine (RVM) <ref type="bibr" target="#b24">(Tipping, 2000)</ref> classifiers. It was observed that a reliance on the marginal likelihood alone as a criterion for model identification ran the risk of overfitting the available data sample by producing an overly sparse representation. The authors then employ an approximation to the leave-one-out error, which emerges from the EP iterations, to counteract this problem. For Bayesian methods which rely on optimising in-sample marginal likelihood (or an appropriate bound) then great care has to be taken when setting the convergence tolerance which determines when the optimisation routine should halt. However, in the experiments we have conducted this phenomenon did not appear to be such a problem with the exception of one dataset as will be discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with Multi-class SVM</head><p>To briefly compare the performance of the proposed approach to multi-class classification with a number of multi-class SVM methods we consider the recent study of <ref type="bibr" target="#b5">(Duan and Keerthi, 2005)</ref>. In that work four forms of multiclass classifier were considered; WTAS -one-versus-all SVM method with winner takes all class selection; MWVS -one-versus-one SVM with a maximum votes class selection strategy; PWCK -one-versus-one SVM with probabilistic outputs employing pairwise coupling (see <ref type="bibr" target="#b5">(Duan and Keerthi, 2005)</ref> for details); PWCK -Kernel logistic regression with pairwise coupling of binary outputs. In <ref type="bibr" target="#b5">(Duan and Keerthi, 2005)</ref> thorough and extensive cross-validation was employed to select the length-scale parameters (single) of the Gaussian kernel and the associated regularisation parameters which were used in each of the SVM's. The proposed importance sampler is employed to obtain the posterior mean estimates for both single and multiple length scales (VBGPS -Variational Bayes Gaussian Process Classification -Single length scale) (VBGPM -Variational Bayes Gaussian Process Classification -Multiple length scales) for a common GP covariance shared across all classes. We monitor the bound on the marginal and consider convergence has been achieved when less than a 1% increase in the bound is observed for all datasets except for ABE where a 10% convergence criterion was employed due to a degree of overfitting being observed after this point. In all experiments, data was standardised to have zero mean and unit variance.</p><p>The percentage test errors averaged over each of the 20 data splits (mean </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion &amp; Discussion</head><p>The main novelty of this work has been to adopt the data augmentation strategy employed in obtaining an exact Bayesian analysis of binary &amp; multinomial probit regression models for GP based multi-class (of which binary is a specific case) classification. Whilst a full Gibbs sampler can be straightforwardly obtained from the joint likelihood of the model, approximate inference employing a factored form for the posterior is appealing from the point of view of computational effort &amp; efficiency. The variational Bayes procedures developed provide simple iterations due to the inherent decoupling effect of the auxiliary variable between the GP components related to each class. The scaling is still of course dominated by an O(N 3 ) term due to the matrix inversion required in obtaining the posterior mean for the GP variables and the repeated computing of multivariate Gaussians required for the weights in the importance sampler. However with the simple decoupled form of the posterior updates we have shown that ADF based online and sparse estimation yields a full multi-class IVM which has linear scaling in the number of classes and the number of available data points and this is achieved in a most straightfoward manner. An empirical comparison with full MCMC suggests that the variational approximation proposed is superior to a Laplace approximation. Further ongoing work includes an investigation into the possible equivalences between EP and variational based approximate inference for the multi-class GP classification problem as well as developing a variational treatment to GP based ordinal regression <ref type="bibr" target="#b2">(Chu and Ghahramani, 2005)</ref>.</p><p>8 Appendix I</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Q(M)</head><p>We employ the shorthand Q(ϕ) = k Q(ϕ k ) in the following. Consider the Q(M) component of the approximate posterior. We have</p><formula xml:id="formula_31">Q(M) ∝ exp E Q(Y)Q(ϕ) n k log p(y nk |m nk ) + log p(m k |ϕ k ) ∝ exp E Q(Y)Q(ϕ) k log N y k (m k , I) + log N m k (0|C ϕ k ) ∝ k N y k (m k , I)N m k 0, C -1 ϕ k -1</formula><p>and so we have</p><formula xml:id="formula_32">Q(M) = K k=1 Q(m k ) = K k=1 N m k ( m k , Σ k )</formula><p>where</p><formula xml:id="formula_33">Σ k = I + C -1 ϕ k -1</formula><p>and m k = Σ k y k . Now each element of C -1 ϕ k is a nonlinear function of ϕ k and so, if considered appropriate, a first-order approximation can be made to the expectation of the matrix inverse such that C</p><formula xml:id="formula_34">-1 ϕ k ≈ C -1 ϕ k in which case Σ k = C ϕ k I + C ϕ k -1 . 8.2 Q(Y) Q(Y) ∝ exp E Q(M) n log p(t n |y n ) + log p(y n |m n ) ∝ exp n log p(t n |y n ) + log N yn ( m n |I) ∝ n N yn ( m n , I) δ(y ni &gt; y nk ∀ k = i)δ(t n = i)</formula><p>Each y n is then distributed as a truncated multivariate Gaussian such that for t n = i the i th dimension of y n is always the largest and so we have,</p><formula xml:id="formula_35">Q(Y) = N n=1 Q(y n ) = N n=1 N tn yn ( m n , I)</formula><p>where N tn yn (., .) denotes a K-dimensional Gaussian truncated such that the dimension indicated by the value of t n is always the largest.</p><p>The posterior expectation of each y n is now required. Note that</p><formula xml:id="formula_36">Q(y n ) = Z -1 n k N y nk ( m nk , 1)</formula><p>where Z n = P r(y n ∈ C) and C = {y n : y nj &lt; y ni , j = i}. Now</p><formula xml:id="formula_37">Z n = P r(y n ∈ C) = +∞ -∞ N y ni ( m ni , 1) j =i y ni -∞ N y nj ( m nj , 1)dy ni dy nj = E p(u) j =i Φ(u + m ni -m nj )</formula><p>Where u is a standardised Gaussian random variable such that p(u) = N u (0, 1). For all k = i the posterior expectation follows as</p><formula xml:id="formula_38">y nk = Z -1 n +∞ -∞ y nk K j=1 N y nj ( m nj , 1)dy nj = Z -1 n +∞ -∞ y ni -∞ y nk N y nk ( m nk , 1) j =i,k N y ni ( m ni , 1)Φ(y ni -m nj )dy ni dy nk = m nk -Z -1 n E p(u) N u ( m nk -m ni , 1) j =i,k Φ(u + m ni -m nj )</formula><p>The required expectation for the i th component follows as</p><formula xml:id="formula_39">y ni = Z -1 n +∞ -∞ y ni N y ni ( m ni , 1) j =i Φ(y ni -m nj )dy ni = m ni + Z -1 n E p(u) u j =i Φ(u + m ni -m nj ) = m ni + k =i ( m nk -y nk )</formula><p>The final expression in the above follows from noting that for a random variable u ∼ N (0, 1) and any differentiable function g(u) then E{ug(u)} = E{g (u)} in which case</p><formula xml:id="formula_40">E p(u) u j =i Φ(u + m ni -m nj ) = k =i E p(u) N u ( m nk -m ni , 1) j =i Φ(u + m ni -m nj ) 8.3 Q(ϕ k )</formula><p>For each k we obtain the posterior component</p><formula xml:id="formula_41">Q(ϕ k ) ∝ exp E Q(m k )Q(ψ k ) (log p(m k |ϕ k ) + log p(ϕ k |ψ k )) = Z k N m k (0|C ϕ k ) d Exp ϕ kd ( ψ kd )</formula><p>where Z k is the corresponding normalising constant for each posterior which is unobtainable in closed form. As such the required expectations can be obtained by importance sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Q(ψ k )</head><p>The final posterior component required is</p><formula xml:id="formula_42">Q(ψ k ) ∝ exp E Q(ϕ k ) (log p(ϕ k |ψ k ) + log p(ψ k |α k )) ∝ d Exp ϕ kd (ψ kd )Γ ψ kd (σ k , τ k ) = d Γ ψ kd (σ k + 1, τ k + ϕ kd )</formula><p>and the required posterior mean values follow as</p><formula xml:id="formula_43">ψ kd = σ k +1 τ k + ϕ kd 9 Appendix II</formula><p>The predictive distribution for a new point x new can be obtained by firstly marginalising the associated GP random variables such that</p><formula xml:id="formula_44">p(y new |x new , X, t) = p(y new |m new )p(m new |x new , X, t)dm new = K k=1 N m new k (y new k , 1)N m new k ( m new k , σ new k )dm new k = K k=1 N y new k ( m new k , ν new k )</formula><p>where the shorthand ν new k = 1 + σ 2 k ,new is employed. Now that we have the predictive posterior for the auxilliary variable y new the appropriate conic truncation of this spherical Gaussian yields the required distribution P (t new = k|x new , X, t) as follows. Using the following shorthand</p><formula xml:id="formula_45">P (t new = k|y new ) = δ(y new k &gt; y new i ∀ i = k)δ(t new = k) ≡ δ k,new then P (t new = k|x new , X, t) = P (t new = k|y new )p(y new |x new , X, t)dy new = C k p(y new |x new , X, t)dy new = δ k,new K k=1 N y new k ( m new k , ν new k ) dy new k = E p(u) j =k Φ 1 ν new j u ν new k + m new k -m new j</formula><p>This is the probability that the auxiliary variable y new is in the cone C k so </p><formula xml:id="formula_46">K k=1 P (t new = k|x new , X, t) = K k=1 C k p(y new |x new , X, t)dy new = R K p(y new</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Appendix III</head><p>The variational bound conditioned on the current values of ϕ k , ψ k , α k (i.e. assuming these are fixed values) can be obtained in the following manner using the expansion of the relevant components of the lower-bound. </p><formula xml:id="formula_47">k E Q(m k ) {log Q(m k )} - (19) n E Q(yn) {log Q(y n )} (<label>(18)</label></formula><formula xml:id="formula_48">)<label>20</label></formula><p>expanding each component in turn obtains</p><formula xml:id="formula_49">- 1 2 k n y 2 nk + m 2 nk -2 y nk m nk - N K 2 log 2π (21) - 1 2 k log |C ϕ k | - 1 2 k m T k C -1 ϕ k m k - 1 2 k trace C -1 ϕ k Σ k - N K 2 log 2π (22) - N K 2 - N K 2 log 2π - 1 2 k log |Σ k | (23) - 1 2 k n y 2 nk + m 2 nk -2 y nk m nk - n log Z n - N 2 log 2π<label>(24)</label></formula><p>Combining and manipulating <ref type="bibr">(21,22,23, and 24)</ref> gives the following expression for the lower-bound.</p><p>- An additional Metropolis-Hastings sub-sampler can be employed within the above Gibbs sampler to draw samples from the posterior p(Θ, Φ|t, X, α) if the covariance function hyper-parameters are to be integrated out.</p><formula xml:id="formula_50">N K 2 log 2π + N 2 log 2π + N K 2 - 1 2 k trace{Σ k } - 1 2 k m T k C -1 ϕ k m k - 1 2 k trace C -1 ϕ k Σ k -<label>1</label></formula><p>12 Appendix V</p><p>The Laplace approximation requires the Hessian matrix of second-order derivatives of the joint log-likelihood with respect to each m n . The derivatives of the noise component, log P (t n = k|m n ) = log E p(u) j =k Φ(u + m nkm nj ) , follow as below, where we denote expectation with respect to a Gaussian truncated in the cone C k as E N This then defines an N K × N K dimensional Hessian matrix which, unlike the Hessian of the multinomial-logit counterpart, cannot be decomposed into a diagonal plus multiplicative form (refer to <ref type="bibr" target="#b26">(Williams and Barber, 1998)</ref> for details), due to the cross-diagonal elements E N k y {y ni y nj }, and so the required matrix inversions of the Newton step and those required to obtain the predictive covariance will operate on a full N K × N K matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigureFigure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of the conditional dependencies within the general multinomial probit regression model with Gaussian Process priors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Convergence of the Lower Bound on the Marginal-Likelihood for the toy data set considered. (b) Evolution of estimated posterior means for the inverse squared length scale parameters (precision parameters) in the RBF covariance function, (c) Evolution of out-of-sample predictive performance on the toy data set. Likewise the development of the estimated posterior mean values for the covariance function parameters ϕ d , Figure (2.b), shows Automatic Relevance Detection (ARD) in progress<ref type="bibr" target="#b17">(Neal, 1998)</ref> where the eight irrelevant features are effectively removed from the model.From Figure (2.c) we can see that the development of the predictive performance (out of sample) follows that of the lower-bound (Figure2.a) achieving a predictive performance of 99.37% at convergence. As a comparison to our multi-class GP classifier we use a Directed Acyclic Graph (DAG) SVM<ref type="bibr" target="#b8">(Platt, et al 2000)</ref> (assuming equal class distributions the scaling 9 is O (N 3 K -1 )) on this example. Employing the values of the posterior mean values of the covariance function length scale parameters (one for each of the ten dimensions) estimated by the proposed variational procedure in the RBF kernel of the DAG SVM a predictive performance of 99.33% is obtained. So, on this dataset, the proposed GP classifier has comparable performance, under 0-1 loss, to the DAG SVM. However the estimation of the covariance function parameters is a natural part of the approximate Bayesian inference routines employed in GP classification. There is no natural method of obtaining estimates of the ten kernel parameters for the SVM without resorting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Isocontours of predictive likelihood for binary classification problem (a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Isocontours of predictive likelihood for multi-class classification problem (a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) Scatter plot of the first two dimensions of the 1000 available data sample. Each class is denoted by ×, +, • and the decision boundaries denoted by the contours of target posterior probability equal to 1/3 are plotted in solid and dashed line. The fifty points selected based on the proposed criterion are circled and it is clear that these sit close to the decision boundaries. (b) The averaged predictive performance (percentage predictions correct) over twenty random starts (dashed line denotes random sampling and solid line denotes informative sampling) are shown with the slowest converging plot characterizing what is achieved under a random sampling strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure6: (a) The predictive likelihood computed on held-out data for both random sampling (solid line with '+' markers) and informative sampling (solid line with ' ' markers). The predictive likelihood is computed once every 50 inclusion steps. (b) The predictive performance (percentage predictions correct) achieved for both random sampling (solid line with '+' markers) and informative sampling (solid line with ' ' markers)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Five multi-class datasets from the UCI Machine Learning Data Repository were employed: ABE (16 dimensions &amp; 3 classes) -a subset of the Letters dataset using the letters 'A', 'B' &amp; 'E'; DNA (180 dimensions &amp; 3 classes); SAT (36 dimensions &amp; 6 classes) -Satellite Image; SEG (18 dimensions &amp; 7 classes) -Image Segmentation; WAV (21 dimensions &amp; 3 classes) -Waveform. For each of these, (Duan and Keerthi, 2005) created twenty random partitions into training and test sets for three different sizes of training set, ranging from small to large. Here we consider only the smallest training set sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>|x new , X, t)dy new = 1 thus yielding a properly normalised posterior distribution over classes 1, • • • K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E</head><label></label><figDesc>Q(M)Q(Y) {log p(y nk |m nk )} + (17) k E Q(M) {log p(m k |X, ϕ k )} -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>where each Z n = E p(u) j =i Φ(u + m nim nj ) .11 Appendix IV Details of the Gibbs sampler required to obtain samples from the posterior p(Θ|t, X, Φ, α) now follow. From the definition of the joint likelihood (Equation 2) it is straightforward to see that the conditional distribution for each y n | m n will be a truncated Gaussian defined in the cone C tn , centered at m n with identity covariance and denoted byN tn y (m n , I). The distribution for each m k | y k is multivariate Gaussian with covariance Σ k = C ϕ k (I + C ϕ k ) -1and mean Σ k y k . Thus the Gibbs sampler, for each n and k, k )where the superscript (i) denotes the i th sample drawn. The dominant scaling will be O(KN 3 ) per sample draw. With the multinomial probit likelihood for a new data point defined asP (t new = k|m new ) = E p(u12 is then obtained from P (t new = k|x new , X, t) = P (t new = k|m new )p(m new |x new , X, t) dm newA Monte-Carlo estimate of the above required marginal posterior expectation can be obtained by drawing samples from the full posterior distribution, p(Θ|t, X, Φ, α), using the above sampler. Then for each Θ (i) sampled an additional set of samples m new,s k are drawn, such that for each k,m newk ) T (I + C ϕ k ) -1 C new ϕ k and the associated variance is σ 2 k,new = c new ϕ k -(C new ϕ k ) T (I + C ϕ k ) -1 C new ϕ k .The approximate predictive distribution can then be obtained by the following Monte</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>SVM &amp; Variational Bayes GP Multi-class Classification Comparison ± standard deviation) are reported in Table.2. For each dataset the classifiers which obtained the lowest prediction error and whose performances were indistinguishable from each other at the 1% significance level using a paired t-test are highlighted in bold. An asterisk, , highlights the cases where the proposed GP-based multi-class classifiers were part of the best performing set. We see that in three of the five datasets performance equal to the best performing SVM's is achieved by one of the GP-based classifiers without recourse to any cross-validation or in-sample tuning with comparable performance being achieved for SAT &amp; DNA. The performance of VBGPM is particularly poor on DNA and this is possibly due to the large number (180) of binary features.</figDesc><table><row><cell></cell><cell>WTAS</cell><cell>MWVS</cell><cell>PWCP</cell><cell cols="3">PWCK VBGPM VBGPS</cell></row><row><cell>SEG</cell><cell>9.4±0.5</cell><cell>7.9±1.2</cell><cell>7.9±1.2</cell><cell>7.5±1.2</cell><cell>7.8±1.5</cell><cell>11.5±1.2</cell></row><row><cell cols="2">DNA 10.2±1.3</cell><cell>9.9±0.9</cell><cell>8.9±0.8</cell><cell>9.7±0.7</cell><cell>74.0±0.3</cell><cell>13.3±1.3</cell></row><row><cell cols="2">ABE 1.9±0.8</cell><cell>1.9±0.6</cell><cell>1.8±0.6</cell><cell>1.8±0.6</cell><cell>1.8±0.8</cell><cell>2.4±0.8</cell></row><row><cell cols="2">WAV 17.2±1.4</cell><cell cols="4">17.8±1.4 16.4±1.4 15.6±1.1 25.2±1.2</cell><cell>15.6±0.7</cell></row><row><cell cols="5">SAT 11.1±0.6 11.0±0.7 10.9±0.4 11.2±0.6</cell><cell>12.0±0.4</cell><cell>12.1±0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>k y {•} ∂ ∂m ni log P (t n = k|m n ) = 1 P (t n = k|m n ) C k (y nim ni )N yn (m, I)dy = E N k y {y ni }m ni and ∂ 2 ∂m nj ∂m ni log P (t n = k|m n ) = E N k y {y ni y nj } -E N k y {y ni }E N k y {y nj }δ ij</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Matlab code to allow replication of the reported results is available at http://www. dcs.gla.ac.uk/people/personal/girolami/pubs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2005/VBGP/index.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The likelihood is nonlinear in the parameters due to either the logistic or probit link functions required in the classification setting</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>This is the most general setting, however it is more common to employ a single and shared GP covariance function across classes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>The model can be defined by employing K -1 GP functions and an alternative truncation of the Gaussian over the variables y nk however for the multi-class case we define a GP for each class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>The bound follows from the application of Jensens inequality e.g. log p(t|X) = log p(t,Θ|X) Q(Θ) Q(Θ)dΘ ≥ Q(Θ) log p(t,Θ|X) Q(Θ) dΘ</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>This assumes the use of standard quadratic optimisation routines.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>http://www.ics.uci.edu/ ∼ mlearn/MPRepository.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>The dataset is available from http://www.ics.uci.edu/ ∼ mlearn/databases/ isolet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>Conditioning on Φ and α is implicit.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by <rs type="funder">Engineering &amp; Physical Sciences Research Council</rs> grants <rs type="grantNumber">GR/R55184/02 &amp; EP/C010620/1</rs>. The authors are grateful to <rs type="person">Chris Williams</rs>, <rs type="person">Jim Kay</rs> and <rs type="person">Joaquin Quiñonero Candela</rs> for motivating discussions regarding this work. In addition the comments and suggestions made by the anonymous reviewers helped to significantly improve the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mjV8p7U">
					<idno type="grant-number">GR/R55184/02 &amp; EP/C010620/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Toy-Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laplace</head><p>Variational Gibbs Sampler Marginal  Predictive Error 3.97 ± 2.00 3.65 ± 1.95 3.49 ± 1.69 Predictive   predictions from the Laplace approximation are less informative about the target values than both other methods considered. In addition the Variational approximation yields predictive distributions which are as informative as those provided by the Gibbs sampler, however the 0-1 prediction errors obtained across all methods do not differ as significantly. In <ref type="bibr" target="#b11">(Kuss and Rasmussen, 2005)</ref> a similar observation was made for the binary GP classification problem when Laplace and EP approximations were compared to MCMC. It will then be interesting to further compare EP and Variational approximations in this setting.</p><p>We have observed that the predictions obtained from the variational ap-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistcial Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">422</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational Algorithms for Approximate Bayesian Inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian Processes for Ordinal Regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1019" to="1041" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient Approaches to Gaussian Process Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Csato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fokue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schottky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="252" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse online gaussian processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Csato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="641" to="668" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Which is the Best Multi-class SVM Method? An Empirical Study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Multiple Classifier Systems</title>
		<meeting>the Sixth International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational gaussian process classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1458" to="1464" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchic Bayesian models for kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large margin dags for multi-class classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="547" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://home.postech.ac.kr/∼grass/publication/" />
		<title level="m">Bayesian and Ensemble Kernel Classifiers</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Pohang University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Assessing Approximate Inference for Binary Gaussian Process Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1679" to="1704" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the variability in cDNA microarray image processing by Bayesian inference</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rashbass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soullier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="518" to="526" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast sparse gaussian process methods: The informative vector machine</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extensions of the informative vector machine</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sheffield Machine Learning Workshop</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Winkler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</editor>
		<meeting>the Sheffield Machine Learning Workshop<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Information Theory, Inference, and Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regression and classification using gaussian process priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics 6</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bernardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="475" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaussian processes for classification: Mean field algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2655" to="2684" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predictive automatic relevance determination by expectation propagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<meeting>the twenty-first International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental gaussian processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems 15</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast forward selection to speed up sparse gaussian process regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</editor>
		<meeting>the Ninth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>661</idno>
		<title level="m">Sparse Gaussian Process Classification With Multiple Classes. Department of Statistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian Model Selection for Support Vector Machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian Processes and Other Kernel Classifiers. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="603" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the relevance vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Convergence and asymptotic normality of variational Bayesian approximations for exponential family models with missing values</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<idno>No.04-02</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian classification with gaussian processes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1342" to="1351" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gaussian processes for regression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="598" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using the Nystrom method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
