<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_BMvKzGw">
					<orgName type="full">Research Grants Council of the Hong Kong Special Administrative Region</orgName>
				</funder>
				<funder ref="#_3s4bCwg #_HQ7Ac4Q">
					<orgName type="full">Small Project Fund of YNUFE</orgName>
				</funder>
				<funder ref="#_d2xGJJA">
					<orgName type="full">Hong Kong Research Grants Councils GRF</orgName>
				</funder>
				<funder ref="#_KdtjPPM">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_HfTKGTF">
					<orgName type="full">Science Fund of Yunnan Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Statistics and Mathematics</orgName>
								<orgName type="institution">Yunnan University of Finance and Economics</orgName>
								<address>
									<postCode>650221</postCode>
									<settlement>Kunming</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">L H</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics and Actuarial Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon</settlement>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNNLS.2012.2183006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bilinear Probabilistic Principal Component Analysis Jianhua Zhao, Philip L. H. Yu, and James T. Kwok Abstract-Probabilistic principal component analysis (PPCA) is a popular linear latent variable model for multi-layer performing dimension reduction on 1-D data in a probabilistic manner. However, when used on 2-D data such as images, PPCA suffers from the curse of dimensionality due to the subsequently large number of model parameters. To overcome this problem, we propose in this paper a novel probabilistic model on 2-D data called bilinear PPCA (BPPCA). This allows the establishment of a closer tie between BPPCA and its nonprobabilistic counterpart. Moreover, two efficient parameter estimation algorithms for fitting BPPCA are also developed. Experiments on a number of 2-D synthetic and real-world data sets show that BPPCA is more accurate than existing probabilistic and nonprobabilistic dimension reduction methods. Index Terms-2-D data, dimension reduction, expectation maximization, principal component analysis, probabilistic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ANY REAL-WORLD applications involve high- dimensional data. However, the interesting structure inside the data often lies in a low-dimensional space. Dimension reduction, which aims to find a compact and meaningful data representation, is thus a useful tool for data visualization, interpretation, and analysis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Probabilistic modeling of dimension reduction is an important research topic in data mining, pattern recognition, machine learning, and statistics <ref type="bibr" target="#b2">[3]</ref>. Compared with its nonprobabilistic counterparts, probabilistic models enable different sources of data uncertainty to be well studied by means of probability theory. Consequently, statistical inference and Bayesian (or variational Bayesian) methods can be performed, and missing data can be handled in a principled way. Moreover, probabilistic models can be easily extended in various ways. For example, they can be extended to probabilistic mixture models to accommodate for heterogeneous data <ref type="bibr" target="#b3">[4]</ref>, can be modified to accommodate for discrete data <ref type="bibr" target="#b4">[5]</ref>, and can also be robustified to handle outliers with the incorporation of a heavy-tailed noise distribution (such as the student t-distribution) <ref type="bibr" target="#b5">[6]</ref>.</p><p>Principal component analysis (PCA) <ref type="bibr" target="#b6">[7]</ref> is one of the most popular techniques for dimension reduction. While the standard PCA is nonprobabilistic, Moghaddam and Pentland <ref type="bibr" target="#b7">[8]</ref> extended it to a probabilistic framework, and Tipping and Bishop <ref type="bibr" target="#b3">[4]</ref> derived the probabilistic PCA (PPCA) from the classical linear latent variable model. In particular, PPCA is an important development since it inherits all the advantages of a probabilistic model while including PCA as a special case.</p><p>However, PPCA, like its nonprobabilistic counterpart, is formulated for the 1-D data where observations are vectors. To apply PPCA to 2-D data where the observations are matrices (such as images), one possible solution is to first vectorize the data and then apply PPCA to the resultant 1-D data. However, vectorization destroys the natural matrix structure and may lose potentially useful local structure information among columns/rows <ref type="bibr" target="#b8">[9]</ref>. Moreover, for 2-D data such as images, the resultant vectorized data is very high-dimensional (typically over tens of thousands of pixels) and thus suffers from the curse of dimensionality <ref type="bibr" target="#b9">[10]</ref>.</p><p>Instead of using vectorization, several nonprobabilistic models have been proposed in recent years that extend PCA directly for 2-D data. Examples include the generalized lowrank approximation of matrices (GLRAM) <ref type="bibr" target="#b10">[11]</ref> and 2-DPCA <ref type="bibr" target="#b11">[12]</ref>. This overcomes the curse of dimensionality and significantly reduces the computation cost. Moreover, GLRAM achieves a high compression ratio (i.e., much fewer space is needed for storing the data), which is particularly important for large-scale high-dimensional data. Empirically, these methods can achieve competitive or even better recognition performance than PCA, especially when the sample size is small relative to feature dimensionality. Inspired by these encouraging results, attempts have been made to formulate a probabilistic model for GLRAM so that it can enjoy similar advantages as PPCA has over PCA <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Following the classical linear latent variable model as in PPCA, they formulated the same model (which is called probabilistic second-order PCA (PSOPCA) in <ref type="bibr" target="#b12">[13]</ref>), but with different learning algorithms.</p><p>Despite all these successes, the relationship between PSOPCA and GLRAM, unlike that between PPCA and PCA <ref type="bibr" target="#b3">[4]</ref>, has not been well established. For example, it is shown that the factor loading matrix in PPCA spans the principal subspace of the covariance matrix. However, a similar result for PSOPCA has only been obtained in the special case of zero-noise limit <ref type="bibr" target="#b13">[14]</ref>. Moreover, parameter estimation in PPCA can be easily performed by either including the latent variables (i.e., missing data) or not, as closed-form updates are available for the constituent steps in both cases. In contrast, parameter estimation in PSOPCA <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref> requires the inclusion of latent variables. Otherwise, it is unclear if a closed-form update is still possible. This can be of practical significance as estimation algorithms not involving latent variables usually converge faster, as the convergence rate of the expectation maximization (EM) algorithm is determined by the portion of missing information in the complete data <ref type="bibr" target="#b14">[15]</ref>.</p><p>Motivated by PPCA, we proposed in this paper a novel probabilistic model called bilinear PPCA (BPPCA) that addresses these problems. While both BPPCA and PSOPCA are probabilistic models for 2-D data, BPPCA is more advantageous in that it bears closer relationships and similarities with PPCA. In particular: 1) BPPCA performs PPCA in the row and column directions alternately; 2) similar to PPCA, the maximum likelihood estimators (MLE) of BPPCA's model parameters span the principal subspaces of the column and row covariance matrices; and 3) as in PPCA, efficient closed-form expressions are available for the parameter update steps in BPPCA, with or without the use of latent variables.</p><p>The remainder of this paper is organized as follows. Section II reviews some related works. Section III proposes the BPPCA model and Section IV is devoted to the maximum likelihood estimation of BPPCA. Section V gives some empirical studies to compare BPPCA with some related methods. Section VI closes this paper with some concluding remarks.</p><p>In this paper, the transpose of vector/matrix is denoted by the superscript , and the identity matrix by I. Moreover, • F denotes the Frobenius norm, tr(•) is the matrix trace, vec(•) is the vectorization operator, ⊗ is the Kronecker product, and N d (μ, ) is the d-dimensional normal distribution with mean μ and covariance matrix .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Minimum-Error Formulation of PCA</head><p>Let {x n } N n=1 (where each x n ∈ R d ) be a set of observations. We assume that the data has been centered. In the minimumerror formulation <ref type="bibr" target="#b0">[1]</ref>, PCA finds the optimal projection matrix U ∈ R d×q (where the latent dimensionality q &lt; d and the projection vectors are orthonormal U U = I) and lowdimensional representations t n ∈ R q (n = 1, . . . , N) that minimize the mean squared error (MSE) of the reconstructed observations</p><formula xml:id="formula_0">(1/N) N n=1 x n -Ut n 2 .</formula><p>The U solution consists of, up to an arbitrary rotation, the q leading eigenvectors of the sample covariance matrix</p><formula xml:id="formula_1">S = 1 N N n=1 x n x n (1)</formula><p>and the t n solution is U x n . As x 1x 2 Ut 1 -Ut 2 = t 1t 2 , the Euclidean distance x 1x 2 in the d-dimensional space can be approximated by the distance t 1t 2 in the lower q-dimensional space. This reduces the amount of computation from O(d) to O(q). In addition, classification using the reduced representations usually leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Maximum-Variance Formulation of PCA</head><p>In the maximum-variance formulation <ref type="bibr" target="#b6">[7]</ref>, PCA tries to sequentially find the projections u 1 , u 2 , . . . , u q (where each u k = 1) such that the variance of the projected data u k x (k = 1, . . . , q) is maximized max</p><formula xml:id="formula_2">u k cov(u k x) = max u k u k u k .</formula><p>(</p><p>Here,</p><formula xml:id="formula_4">= cov(x) = E[xx ]</formula><p>is the population covariance matrix of the (centered) observations x. Again, the U = [u 1 , u 2 , . . . , u q ] solution consists of the q leading eigenvectors of . If is estimated by its MLE, which is the sample covariance matrix S, then both the minimum-error and maximumvariance formulations lead to the same U solution (up to an arbitrary rotation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Probabilistic Principal Component Analysis (PPCA)</head><p>PPCA <ref type="bibr" target="#b3">[4]</ref> is a restricted factor analysis model</p><formula xml:id="formula_5">x = Cz + μ + , z ∼ N q (0, I), ∼ N d (0, σ 2 I)<label>(3)</label></formula><p>where C ∈ R d×q is the factor loading matrix, z ∈ R q is the latent representation independent of , μ ∈ R d is the mean vector, and σ 2 &gt; 0 is the isotropic noise variance. Both the probability density distribution of x and the conditional probability density distribution of z given x are the multivariate normal distribution</p><formula xml:id="formula_6">x ∼ N d (μ, ), z|x ∼ N q M -1 C (x -μ), σ 2 M -1<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">= CC + σ 2 I, M = C C + σ 2 I.<label>(5)</label></formula><p>Given a set of observations X = {x n } N n=1 , the MLE of μ is simply the sample mean x. As in Section II-A, we assume that x is zero, and the sample covariance matrix is then given by <ref type="bibr" target="#b0">(1)</ref>. The MLE of θ = (C, σ 2 ) can be obtained by maximizing the log likelihood, <ref type="foot" target="#foot_0">1</ref> which is, up to a constant</p><formula xml:id="formula_8">L(θ |X ) = - N 2 ln | | + tr -1 S . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Setting its derivative with respect to θ to zero, and assuming that rank(S) &gt; q, we obtain</p><formula xml:id="formula_10">C = U -σ 2 I 1 2 V (<label>7</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">σ 2 = 1 d -q d i=q+1 λ i (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where V is an arbitrary orthogonal matrix, U = [u 1 , . . . , u q ], and = diag(λ 1 , . . . , λ q ), with</p><formula xml:id="formula_14">{u i } d i=1 , {λ i } d i=1 (λ 1 ≥ λ 2 ≥ • • • ≥ λ d )</formula><p>being the eigenvectors and eigenvalues of S.</p><p>Alternatively, (6) can be maximized by using the wellknown EM algorithm <ref type="bibr" target="#b14">[15]</ref>. This requires the introduction of missing data and consists of an E-step and a M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-</head><p>Step: Let the missing data be Z = {z n } N n=1 . The complete data log likelihood is N n=1 ln { p(x n |z n ) p(z n )}. Its expectation (up to a constant) with respect to the distribution p(Z|X ) leads to the so-called Q-function</p><formula xml:id="formula_15">Q(θ ) = - 1 2 N n=1 d ln σ 2 + σ -2 E x n -Cz n 2 |x n</formula><p>where the involved expectations E[z n |x n ] and E[z n z n |x n ] can be easily obtained from (4) as</p><formula xml:id="formula_16">E[z n |x n ] = M -1 C x n ,<label>(9)</label></formula><formula xml:id="formula_17">E[z n z n |x n ] = σ 2 M -1 + E[z n |x n ]E[z n |x n ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-Step:</head><p>We maximize Q with respect to C and σ<ref type="foot" target="#foot_1">foot_1</ref> , yielding</p><formula xml:id="formula_18">C = N n=1 x n E[z n |x n ] N n=1 E[z n z n ] -1 , σ 2 = 1 Nd N n=1 x n 2 -E[z n |x n ] C x n .</formula><p>Similar to the maximum-variance formulation of PCA, we may classify observations based on the expected latent representation E[z|x]. Note that since the small eigenvalues of the covariance matrix tend to be underestimated <ref type="bibr" target="#b17">[18]</ref>, PPCA regularizes automatically by increasing its small eigenvalues to σ 2 in <ref type="bibr" target="#b4">(5)</ref>. Consequently, a regularized latent representation</p><formula xml:id="formula_19">E[z|x] = (C C + σ 2 I) -1 C x is produced in (9).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Minimum-Error Formulation for Bilinear Dimension Reduction</head><p>Inspired by the minimum-error formulation of 1-D PCA, several techniques have been proposed in recent years that perform dimension reduction on the 2-D data directly. Examples include the GLRAM <ref type="bibr" target="#b10">[11]</ref> and 2-DPCA <ref type="bibr" target="#b11">[12]</ref>.</p><p>Let {X n } N n=1 (where each X n ∈ R d c ×d r ) be a set of 2-D centered observations. GLRAM finds the optimal transformation matrices U c ∈ R d c ×q c , U r ∈ R d r ×q r (where the columns are orthogonal and q c &lt; d c , q r &lt; d r ) and low-dimensional representations</p><formula xml:id="formula_20">T n ∈ R q c ×q r (n = 1, . . . , N) such that the MSE 1 N N n=1 X n -U c T n U r 2 F (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>of the reconstructed observations {U c T n U r } N n=1 is minimized. Given an initial U r , (10) can be minimized by iterating the following two steps until convergence.</p><p>1) U c ← the q c leading eigenvectors of</p><formula xml:id="formula_22">G c = 1 N N n=1 X n U r U r X n . (<label>11</label></formula><formula xml:id="formula_23">)</formula><p>2) U r ← the q r leading eigenvectors of</p><formula xml:id="formula_24">G r = 1 N N n=1 X n U c U c X n . (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>After convergence, T n is obtained as U c X n U r . On the other hand, 2-DPCA only applies a linear transformation on the right side of the data matrix. Hence, it can be viewed as a special case of GLRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Probabilistic Extensions of GLRAM</head><p>Recently, several works attempt to formulate a probabilistic model for GLRAM so that it can enjoy similar advantages as PPCA has over PCA <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Following the classical linear latent variable model as in PPCA, they formulate the following model, which is called PSOPCA in <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_26">X = CZR + W + , Z ∼ N q c ,q r (0, I, I), ∼ N d c ,d r (0, σ 2 I, σ 2 I)<label>(13)</label></formula><p>where N q c ,q r and N d c ,d r are matrix-variate normal distributions, 2 C ∈ R d c ×q c and R ∈ R d r ×q r are the column and row factor loading matrices, respectively, W ∈ R d c ×d r is the mean matrix, and σ 2 &gt; 0 is the noise variance. Different learning algorithms for this model are proposed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and <ref type="bibr" target="#b13">[14]</ref>.</p><p>As mentioned in Section I, the relationship between PSOPCA and GLRAM is not as well-established as that between PPCA and PCA. For example, for PPCA, it can be seen from ( <ref type="formula" target="#formula_10">7</ref>) that the factor loading matrix C spans the principal subspace of the covariance matrix. In the special case of zero-noise limit, a similar result for PSOPCA is obtained in <ref type="bibr" target="#b13">[14]</ref>. Specifically, they showed that the column and row factor loading matrices C and R span the principal subspaces of the respective covariance matrices in GLRAM. However, it is unclear how to extend this for the general noise case. Moreover, as seen in Section II-C, parameter estimation in PPCA can be efficiently performed by either including the latent variables (i.e., missing data) or not, as closed-form updates are available for the constituent steps in both cases. In contrast, parameter estimation in PSOPCA <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref> requires the inclusion of latent variables. Otherwise, it is unclear if a closed-form update will still be available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BPPCA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proposed Model</head><p>In this section, we extend PPCA in (3) to 2-D data. The proposed model, which will be called BPPCA, is defined as</p><formula xml:id="formula_27">⎧ ⎨ ⎩ X = CZR + W + C r + c R + , Z ∼ N q c ,q r (0, I, I), r ∼ N q c ,d r (0, I, σ 2 r I), c ∼ N d c ,q r (0, σ 2 c I, I), ∼ N d c ,d r (0, σ 2 c I, σ 2 r I) (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>where Z is the latent matrix, c ∈ R d c ×q r is the column noise, r ∈ R q c ×d r is the row noise, ∈ R d c ×d r is the common noise (which are assumed to be independent of each other), C ∈ R d c ×q c and R ∈ R d r ×q r are the column and row factor loading matrices, respectively. σ 2 c &gt; 0 and σ 2 r &gt; 0 are the column and row noise variances, respectively. W ∈ R d c ×d r is the mean matrix. Obviously, when d r = 1 or d c = 1, the BPPCA model in <ref type="bibr" target="#b13">(14)</ref> reduces to the PPCA model in <ref type="bibr" target="#b2">(3)</ref>. Similarly, if we remove the c and r terms, <ref type="bibr" target="#b13">(14)</ref> reduces to the PSOPCA model in <ref type="bibr" target="#b12">(13)</ref> when σ c = σ r . As will be seen in Section IV, the introduction of the c and r terms enables the model to have a number of interesting characteristics that are not available under PSOPCA.</p><p>From <ref type="bibr" target="#b13">(14)</ref>, it is easy to obtain that</p><formula xml:id="formula_29">CZR ∼ N d c ,d r (0, CC , RR ), C r ∼ N d c ,d r (0, CC , σ 2 r I), c R ∼ N d c ,d r (0, σ 2 c I, RR ). Consequently, X follows the matrix-variate normal distribution N d c ,d r (W, c , r ), where c = CC + σ 2 c I; r = RR + σ 2 r I.<label>(15)</label></formula><p>Thus, as in PPCA, BPPCA is characterized by a normal distribution on X and a low-rank covariance structure [see ( <ref type="formula" target="#formula_7">5</ref>) and <ref type="bibr" target="#b14">(15)</ref>]. Note that this can be extended to other constrained covariance structures and nonnormal distributions. Another characteristic of BPPCA is the use of a separable covariance structure via the CZR term in ( <ref type="formula" target="#formula_27">14</ref>). This will be studied in more detail in Section III-B. Similar to PPCA <ref type="bibr" target="#b3">[4]</ref>, not all the BPPCA parameters can be uniquely identified. However, as a subspace learning method <ref type="bibr" target="#b18">[19]</ref>, the subspaces of interest (that are spanned by the columns of C and R) can still be uniquely identified up to: 1) orthogonal rotations of the factor loading matrices, latent matrix, column and row noise matrices; and 2) scaling of the column and row factor loading matrices. Interested readers are referred to Appendix VI for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bilinear Transformation and Separable Covariance</head><p>In the BPPCA model ( <ref type="formula" target="#formula_27">14</ref>), the observed 2-D data is related to a lower-dimensional latent matrix Z via the transformation</p><formula xml:id="formula_30">X ≡ CZR . (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>This is called a bilinear transformation as X is linear with respect to C (resp. R) when R (resp. C) is fixed. Note that a similar modeling assumption is also used in <ref type="bibr" target="#b12">(13)</ref> for the PSOPCA model. Proposition 1: The use of the bilinear transformation ( <ref type="formula" target="#formula_30">16</ref>) is equivalent to the assumption of a separable (Kronecker product) covariance matrix on X</p><formula xml:id="formula_32">cov(vec( X)) = r ⊗ c (<label>17</label></formula><formula xml:id="formula_33">)</formula><p>where r ∈ R d r ×d r and c ∈ R d c ×d c are the row and column covariance matrices of X, respectively. Proof: Since the covariance of vec(Z) is I, the covariance of X is given by <ref type="bibr" target="#b16">(17)</ref>, where r = RR and c = CC . Conversely, if the covariance matrix of X is separable as in <ref type="bibr" target="#b16">(17)</ref>, there exist C and R such that c = CC and r = RR . Then <ref type="bibr" target="#b15">(16)</ref> holds with Z = C -1 XR -1 .</p><p>The separable covariance assumption has been successfully used in a variety of applications. Examples include the spatialtemporal modeling of environmental data <ref type="bibr" target="#b19">[20]</ref>, channel modeling in multiple-input multiple-output communications <ref type="bibr" target="#b20">[21]</ref>, and signal modeling of MEG/EEG data <ref type="bibr" target="#b21">[22]</ref>. This covariance structure arises when the variables can be cross-classified by two (or, in general, three or more) vector-valued factors <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. For the 2-D data considered here, this corresponds to vec( X) = a ⊗ b, where a and b are variables in the row and column directions, respectively, and with cov(a</p><formula xml:id="formula_34">) = r , cov(b) = c .</formula><p>Obviously, a separable covariance is more restrictive than a full covariance matrix. For example, in the simplest case where d r = d c = 2, it can be shown that separability imposes the following constraints on the covariance matrix = [σ i j ] and the associated correlation matrix ρ = [ρ i j ] <ref type="bibr">[</ref> </p><formula xml:id="formula_35">+ d r (d r + 1)</formula><p>) for the separable covariance <ref type="foot" target="#foot_2">3</ref> ), leading to reduced algorithm complexity and often more accurate estimators <ref type="bibr" target="#b23">[24]</ref>. This can be attributed to the bias-variance tradeoff <ref type="bibr" target="#b24">[25]</ref>, in which one can trade bias for lower variance, leading to better generalization. As will be seen in Section V-E, empirical results also confirm that our BPPCA model (which is based on separable covariance) outperforms PPCA (which uses a nonrestrictive covariance). Note that the use of a restrictive covariance structure is common in machine learning. For example, for linear discriminant analysis (LDA), the even more restrictive diagonal covariance assumption leads to the diagonal LDA <ref type="bibr" target="#b25">[26]</ref>, which is found to perform well on high-dimensional microarray data. Recently, Dryden et al. <ref type="bibr" target="#b26">[27]</ref> proposed a related dimension reduction technique for 2-D data called factored PCA (FPCA) which also assumes separable covariance. Indeed, it can be seen from ( <ref type="formula" target="#formula_29">15</ref>) that FPCA can be regarded as a special case of BPPCA with σ 2 c → 0, σ 2 r → 0 and q c = d c , q r = d r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Probabilistic Graphical Models for BPPCA</head><p>To further understand model <ref type="bibr" target="#b13">(14)</ref>, it is helpful to rewrite it as</p><formula xml:id="formula_36">⎧ ⎨ ⎩ X = CY r + W + Y r , Y r = ZR + r , Y r = c R +<label>(18)</label></formula><p>where Y r ∈ R q c ×d r and Y r ∈ R d c ×d r are latent matrices. This can be interpreted as a two-stage representation of BPPCA.</p><p>From the projection point of view, X is first projected onto Y r in the column direction. Then Y r and residual Y r are further projected in the row direction onto Z and c , respectively. From the generative model point of view, Y r and Y r are first generated in the row direction and X is then generated in the column direction. Alternatively, by introducing another two latent matrices Y c ∈ R d c ×q r and Y c ∈ R d c ×d r , model ( <ref type="formula" target="#formula_27">14</ref>) can be rewritten as first projecting (resp. generating) in the row (resp. column) direction and then projecting (resp. generating) in the column (resp. row) direction  <ref type="bibr" target="#b13">(14)</ref>. (b) Two-stage generative model <ref type="bibr" target="#b17">(18)</ref>, with row followed by column. (c) Two-stage generative model <ref type="bibr" target="#b18">(19)</ref>, with column followed by row.</p><formula xml:id="formula_37">⎧ ⎨ ⎩ X = Y c R + W + Y c , Y c = CZ + c , Y c = C r + . (<label>19</label></formula><formula xml:id="formula_38">) X n C σ 2 c R σ 2 r Z n N W (a) X n C σ 2 c R σ 2 r Z n N W Y r n (b) X n C σ 2 c R σ 2 r Z n N W Y c n (c)</formula><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the probabilistic graphical models of BPPCA corresponding to the three ways of generating X [( <ref type="formula" target="#formula_27">14</ref>), <ref type="bibr" target="#b17">(18)</ref>, and ( <ref type="formula" target="#formula_37">19</ref>)].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Probability Distributions</head><p>In the following, we list the various probability distributions that can be obtained from the BPPCA model. Derivations can be found in Appendix VI</p><formula xml:id="formula_39">Y r ∼ N q c ,d r (0, I, r ), Y r ∼ N d c ,d r (0, σ 2 c I, r ), Y c ∼ N d c ,q r (0, c , I), Y c ∼ N d c ,d r (0, c , σ 2 r I), X ∼ N d c ,d r (W, c , r ) (20) Z|Y r ∼ N q c ,q r (Y r RM -1 r , I, σ 2 r M -1 r ) (21) Y r |X ∼ N q c ,d r M -1 c C (X -W), σ 2 c M -1 c , r<label>(22)</label></formula><formula xml:id="formula_40">Y c |X ∼ N d c ,q r (X -W)RM -1 r , c , σ 2 r M -1 r (<label>23</label></formula><formula xml:id="formula_41">)</formula><p>where c and r are given by <ref type="bibr" target="#b14">(15)</ref> and</p><formula xml:id="formula_42">M c = C C + σ 2 c I; M r = R R + σ 2 r I.<label>(24)</label></formula><p>IV. MAXIMUM LIKELIHOOD ESTIMATION OF BPPCA</p><p>In this section, we show how the BPPCA parameters are estimated from a given set of observations X = {X n } N n=1 . From <ref type="bibr" target="#b19">(20)</ref>, the MLE of W is obviously the sample mean (1/N) N n=1 X n . As in PPCA, we assume that the data has been centered. The MLE of the remaining parameters, θ = (C, σ 2 c , R, σ 2 r ), can be obtained by maximizing the (incomplete-data) log likelihood of the BPPCA model, which is, up to a constant</p><formula xml:id="formula_43">L(θ |X ) = - 1 2 N n=1 {d r ln | c | + d c ln | r | + tr -1 c X n -1 r X n . (<label>25</label></formula><formula xml:id="formula_44">)</formula><p>Due to the bilinear nature of BPPCA, it is natural to develop iterative procedures for the maximization of L. As in PPCA, it will be seen that the parameter estimation here can be easily performed by either including the latent variables (i.e., missing data) or not. Specifically, we will first present in Section IV-A a procedure based on the conditional maximization (CM) algorithm <ref type="bibr" target="#b27">[28]</ref>, which does not require the inclusion of latent variables. Then, in Section IV-B, an EM-type algorithm, which involves latent variables, will be proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CM Algorithm</head><p>The CM algorithm is a special case of the coordinate ascent algorithm in the optimization literature <ref type="bibr" target="#b28">[29]</ref>, where the objective function [which is the incomplete-data log likelihood L (25) here] is maximized with respect to a subset of the variables at each iteration. In the following, we divide the parameters into two subsets, {C, σ 2 c } and {R, σ 2 r }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) CM-Step 1:</head><p>We maximize L with respect to C and σ 2 c , with {R, σ 2 r } fixed. Equation ( <ref type="formula" target="#formula_43">25</ref>) is then reduced to</p><formula xml:id="formula_45">L c C, σ 2 c |X = - Nd r 2 ln | c | + tr -1 c S c (<label>26</label></formula><formula xml:id="formula_46">)</formula><p>where</p><formula xml:id="formula_47">S c = 1 Nd r N n=1 X n -1 r X n (27) = 1 Nd r N n=1 d r i=1 x ni -1 r x ni</formula><p>is the sample covariance matrix of the columns of X n 's. Note that ( <ref type="formula" target="#formula_45">26</ref>) is similar to <ref type="bibr" target="#b5">(6)</ref>. Hence, using the same derivation as in Section II-C, we obtain</p><formula xml:id="formula_48">C = U c c -σ 2 c I 1 2 V c (<label>28</label></formula><formula xml:id="formula_49">)</formula><formula xml:id="formula_50">σ 2 c = 1 d c -q c d c i=q c +1 λ ci (<label>29</label></formula><formula xml:id="formula_51">)</formula><p>where U c , V c , and c are defined similarly as their counterparts in Section II-C (i.e., V c is an arbitrary orthogonal matrix,</p><formula xml:id="formula_52">U c = [u c1 , . . . , u cq c ] and c = diag(λ c1 , . . . , λ cq c ), with {u ci } d c i=1 , {λ ci } d c i=1 (λ c1 ≥ λ c2 ≥ • • • ≥ λ cd c</formula><p>) being the eigenvectors and eigenvalues of S c ).</p><p>2) CM-step 2: We maximize L with respect to R and σ 2 r , with {C, σ 2 c } fixed. This maximization is analogous to that in CM-step 1. Define the sample covariance matrix of the rows of X n 's as</p><formula xml:id="formula_53">S r = 1 Nd c N n=1 X n -1 c X n . (<label>30</label></formula><formula xml:id="formula_54">)</formula><p>Then <ref type="bibr" target="#b24">(25)</ref> becomes</p><formula xml:id="formula_55">L r (θ |X ) = - Nd c 2 ln | r | + tr -1 r S r</formula><p>Algorithm 1 CM algorithm for BPPCA Input: Data X and (random) initialization of R, σ 2 r . 1: Compute the sample mean X and center the data as X n ← X n -X. 2: repeat 3: CM-step 1: Compute S c via <ref type="bibr" target="#b26">(27)</ref>. Update C and σ 2 c via (28) and ( <ref type="formula" target="#formula_50">29</ref>). 4: CM-step 2: Compute S r via <ref type="bibr" target="#b29">(30)</ref>. Update R and σ 2 r via (31) and <ref type="bibr" target="#b31">(32)</ref>. 5: until change of L is smaller than a threshold.</p><formula xml:id="formula_56">Output: (C, R, σ 2 c , σ 2 r ).</formula><p>and the optimal solution is</p><formula xml:id="formula_57">R = U r r -σ 2 r I 1 2 V r (<label>31</label></formula><formula xml:id="formula_58">)</formula><formula xml:id="formula_59">σ 2 r = 1 d r -q r d i=q r +1 λ ri (<label>32</label></formula><formula xml:id="formula_60">)</formula><p>where U r , V r , and r are defined similarly as in CM-step 1 (but based on ( <ref type="formula" target="#formula_53">30</ref>)).</p><p>The whole CM algorithm is shown in Algorithm 1. Since the CM algorithm is based on coordinate descent, both CM-steps 1 and 2 will increase the log likelihood L. Moreover, it can be easily seen that the so-called "space filling" condition 4 is satisfied here. Hence, the CM algorithm is guaranteed to converge to a stationary point of L under the same convergence conditions as for standard EM <ref type="bibr" target="#b29">[30]</ref>. The covariance of these Nd r transformed observations is S c in <ref type="bibr" target="#b26">(27)</ref>. Thus, CM-step 1 performs PPCA on these transformed observations. Similarly, CM-step 2 performs PPCA on the transformed rows of X n (Nd c i.i.d. transformed observations). On the other hand, PSOPCA fails to provide such an important connection.</p><p>Moreover, similar to PPCA, ( <ref type="formula" target="#formula_48">28</ref>) and <ref type="bibr" target="#b30">(31)</ref> show that the MLE of the factor loading matrices C and R are principal subspaces of the column and row covariance matrices S c and S r , respectively (up to scaling and rotation).</p><p>Comparing <ref type="bibr" target="#b10">(11)</ref>, ( <ref type="formula" target="#formula_24">12</ref>), <ref type="bibr" target="#b26">(27)</ref>, and (30), we can find that G c and G r are different from S c and S r . Therefore, the principal components by BPPCA and GLRAM are in general different.</p><p>2) Computational Complexity: The most expensive computations are on the formations of S c in <ref type="bibr" target="#b26">(27)</ref>, S r in <ref type="bibr" target="#b29">(30)</ref> and their eigen-decompositions. Using</p><formula xml:id="formula_61">-1 c = 1 σ 2 c (I -CM -1 c C ). (<label>33</label></formula><formula xml:id="formula_62">)</formula><p>4 Loosely speaking, this means unconstrained maximization is allowed over the whole parameter space <ref type="bibr" target="#b29">[30]</ref>.</p><p>S c can be computed as</p><formula xml:id="formula_63">S c = 1 Nd r σ 2 c n X n X n -(X n C)M -1 c (X n C) . Computing X n X n and X n C take O(d 2 c d r ) and O(d c d r q c ) time, respectively. Given X n C, computing (X n C)M -1 c (X n C) takes O(d 2</formula><p>c q c ) time. Let t be the number of CM iterations. The total cost of forming all the S c 's is O(Nd</p><formula xml:id="formula_64">2 c d r ) + O(Nt (d c d r q c + d 2 c q c )).</formula><p>Similarly, the cost of computing all the S r 's is O(Nd 2 r d c ) + O(Nt (d r d c q r + d 2 r q r )). Eigendecompositions of S c and S r take O(td 3 c ) and O(td 3 r ), respectively. Hence, the total cost is</p><formula xml:id="formula_65">O(N[d c d r (d c + d r )]) + O(Nt[(d c + d r ) 2 max(q c , q r )]) + O(t[d 3 c + d 3 r ]</formula><p>). This is similar to that of GLRAM <ref type="bibr" target="#b10">[11]</ref> except for the extra first term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alternating Expectation Conditional Maximization (AECM) Algorithm</head><p>In this section, we fit the BPPCA model by an EM-type algorithm called AECM algorithm <ref type="bibr" target="#b30">[31]</ref>. Compared to the CM algorithm developed in Section IV-A, EM-type algorithms often enjoy lower computation complexity <ref type="bibr" target="#b13">[14]</ref>, though their convergence can be slower due to the inclusion of missing information <ref type="bibr" target="#b14">[15]</ref>.</p><p>The AECM algorithm is a flexible and powerful generalization of the standard EM <ref type="bibr" target="#b30">[31]</ref>. It is well-known that EM performs an E-step to obtain the so-called Q function followed by a M-step to maximize Q with respect to all parameters. In some cases, the M-step in EM is difficult to solve while it is possible to sequentially and conditionally maximize Q (CMQ) with respect to subsets of parameters. This yields the ECM algorithm <ref type="bibr" target="#b29">[30]</ref> that replaces the M-step by a sequence of CMQ steps. In some cases, instead of maximizing Q, some CMQ steps can be performed through less data augmentation with the advantage of faster convergence. This leads to the AECM algorithm that replaces the E-step by several E-steps. The salient feature of AECM is that the augmented complete data is allowed to vary between E-steps yet convergence is guaranteed <ref type="bibr" target="#b30">[31]</ref>. A specific application of ECM and AECM to mixtures of factor analyzers can be found in <ref type="bibr" target="#b31">[32]</ref>.</p><p>The AECM algorithm for BPPCA consists of two cycles, each with its own E-step and CM-step. As in Section IV-A, we divide the parameters into the two subsets</p><formula xml:id="formula_66">θ 1 = (C, σ 2 c ) and θ 2 = (R, σ 2 r ). 1) In cycle 1, its E-step treats (X , Y r ) = {X n , Y r n } N</formula><p>n=1 as the complete data, which is then maximized with respect to θ 1 (given θ 2 ) in its CM-step. E-Step: The complete data log likelihood is</p><formula xml:id="formula_67">L com,c (θ 1 |X , Y r ) = N n=1 ln p(X n |Y r n ) p(Y r n ) .</formula><p>Given θ = (θ 1 , θ 2 ), we compute the expected L com,c (up to a constant) with respect to the distribution p(Y r |X , θ )</p><formula xml:id="formula_68">Q c (θ 1 ) = - 1 2 N n=1 d r d c ln σ 2 c +σ -2 c tr{E[(X n -CY r n ) -1 r (X n -CY r n ) |X n ]} .</formula><p>From <ref type="bibr" target="#b21">(22)</ref>, it is easy to obtain the required expectations</p><formula xml:id="formula_69">E[Y r n |X n ] = M -1 c C X n (34)</formula><p>and</p><formula xml:id="formula_70">E[Y r n -1 r Y r n |X n ] = d r σ 2 c M -1 c + E[Y r n |X n ] -1 r E[Y r n |X n ].<label>(35</label></formula><p>) CM-Step: Given θ 2 , we maximize Q c with respect to θ 1 and obtain</p><formula xml:id="formula_71">C = N n=1 X n -1 r E[Y r n |X n ] • N n=1 E[Y r n -1 r Y r n |X n ] -1<label>(36)</label></formula><formula xml:id="formula_72">σ 2 c = 1 Nd r d c tr N n=1 X n -1 r X n -X n -1 r E[Y r n |X n ] C . (<label>37</label></formula><formula xml:id="formula_73">)</formula><p>2) In cycle 2, its E-step treats</p><formula xml:id="formula_74">(X n , Y c n ) = {X n , Y c n } N</formula><p>n=1 as the complete data, which is then maximized with respect to θ 2 (given θ 1 ) in its CM-step. E-Step: The complete data log likelihood is</p><formula xml:id="formula_75">L com,r (θ 2 |X , Y c ) = N n=1 ln { p X n |Y c n p(Y c n )}.</formula><p>Given the updated θ 1 , we compute the expected L com,r with respect to the distribution p(Y c |X , θ 1 , θ 2 ), up to a constant, as <ref type="formula" target="#formula_40">23</ref>), the required expectations can be obtained as</p><formula xml:id="formula_76">Q r (θ 2 ) = - 1 2 N n=1 d r d c ln σ 2 r + σ -2 r tr{E[(X n -Y c R ) -1 c (X n -Y c R )|X n ]} . From (</formula><formula xml:id="formula_77">E[Y c n |X n ] = X n RM -1 r (<label>38</label></formula><formula xml:id="formula_78">)</formula><p>and</p><formula xml:id="formula_79">E[Y c n -1 c Y c n |X n ] = d c σ 2 r M -1 r + E[Y c n |X n ] -1 c E[Y c n |X n ]. (39) CM-Step: Given θ 1 , we maximize Q r with respect to θ 2 and obtain R = N n=1 X n -1 c E[Y c n |X n ] • N n=1 E[Y c n -1 c Y c n |X n ] -1</formula><p>(40)</p><formula xml:id="formula_80">σ 2 r = 1 Nd r d c tr N n=1 X n -1 c X n -X n -1 c E[Y c n |X n ] R . (<label>41</label></formula><formula xml:id="formula_81">)</formula><p>The whole algorithm is summarized in Algorithm 2. It can be observed that cycles 1 and 2 are guaranteed to increase the log likelihood L of BPPCA. Under standard regularity conditions and the space-filling condition, the AECM algorithm is also guaranteed to converge to a stationary point of L <ref type="bibr" target="#b30">[31]</ref>.</p><p>Algorithm 2 AECM algorithm for BPPCA Input: Data X and (random) initialization of (C, R, σ 2 c , σ 2 r ). 1: Compute the sample mean X and center the data as X n ← X n -X. </p><formula xml:id="formula_82">(C, R, σ 2 c , σ 2 r ).</formula><p>1) Computational Complexity: The most expensive computations are on the formations of matrices <ref type="bibr" target="#b27">(28)</ref> and <ref type="figure">O(d c d r (q c</ref> + q r )). Hence, the total cost of AECM is O(Ntd c d r (q c + q r )). Note that its per-iteration complexity is typically lower than that of the CM algorithm, especially when one or both data dimensionalities (d c and d r ) is high.</p><formula xml:id="formula_83">N n=1 X n -1 r E[Y r n |X n ] in</formula><formula xml:id="formula_84">N n=1 X n -1 c E[Y c n |X n ] in (31). The cost of E[Y r n |X n ] is O(d c d r q c ). Using (33), computation of X n -1 r E[Y r n |X n ] can be reduced to</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compression and Reconstruction</head><p>In this section, we compare the compressed representations and reconstructions under PPCA <ref type="bibr" target="#b3">[4]</ref> and the proposed BPPCA. The key difference is that the operators in PPCA are linear while those in BPPCA are bilinear.</p><p>In the following, we let θ be the MLE of θ , x and X be the reconstructed values of x and X, respectively.  <ref type="formula">21</ref>) and ( <ref type="formula" target="#formula_39">22</ref>), this can be computed as</p><formula xml:id="formula_85">E[Z|X] = E[E[Z|Y r ]|X] = M -1 c C (X -W) R M -1 r (42)</formula><p>where the inner expectation is with respect to the distribution p(Z|Y r ) and the outer one is with respect to p(Y r |X). b) Bilinear reconstruction: Given the compressed representation E[Z|X], we can reconstruct X from ( <ref type="formula" target="#formula_27">14</ref>) as Arc length distance between the estimated and true principal subspaces at different sample sizes.</p><formula xml:id="formula_86">X = CE[Z|X] R + W. Using (42), we have X -W = C M -1 c C (X -W) R M -1 r R.</formula><p>In general, this is not a biorthogonal projection since C M -1  c C and R M -1 r R are not projection matrices, except when σ 2 c → 0 and σ 2 r → 0. c) Biorthogonal bilinear reconstruction:</p><p>We can also reconstruct X as</p><formula xml:id="formula_87">X ort h = C( C C) -1 M c E[Z|X] M r ( R R) -1 R + W. Using (42), we have X ort h -W = C( C C) -1 C (X - W) R( R R) -1 R , which is a biorthogonal projection since C( C C) -1 C and R( R R) -1 R are projection matrices.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we perform experiments on a number of synthetic and real-world data sets. Unless otherwise stated, the CM algorithm with fast convergence (Section IV-A) is used for BPPCA. For BPPCA and GLRAM, iteration is stopped when the relative change in the objective (|1 -L (t ) /L (t +1) |) is smaller than a threshold tol (= 10 -5 in the experiments) or the number of iterations exceeds a certain maximum t max (= 20). For PSOPCA, we use the variational EM learning algorithm in <ref type="bibr" target="#b13">[14]</ref> for the general noise case and follow their experimental setting to set t max = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accuracies of the Estimators</head><p>In this experiment, we sample a 2-D synthetic data set from a 10×10-D matrix-variate normal distribution. The column and row covariance matrices have different eigenvalues (5, 4.5, 4, 1, . . . , 1 and 10, 9, 8, 2, . . . , 2, respectively, of which the first three are dominant), and their leading principal components are the same</p><formula xml:id="formula_88">([1/ √ 2, -1/ √ 2, 0, . . . , 0] , [0, 0, 1/ √ 2, -1/ √ 2, 0, . . . , 0] and [0, 0, 0, 0, 1/ √ 2, -1/ √ 2, 0, . . . , 0]</formula><p>). We compare the accuracies of the following methods in estimating the dominant principal subspace of the data.</p><p>1) BPPCA, with q c = q r = 3; 2) PSOPCA <ref type="bibr" target="#b13">[14]</ref>, with q c = q r = 3; and 3) PPCA <ref type="bibr" target="#b3">[4]</ref> on the vectorized 1-D data, with q = q c q r = 9. The subspaces of BPPCA and PSOPCA are spanned by the columns of R ⊗ C in ( <ref type="formula" target="#formula_27">14</ref>) and ( <ref type="formula" target="#formula_26">13</ref>), respectively. The performance criterion is the arc length distance between the estimated subspace and the true one <ref type="bibr" target="#b32">[33]</ref>. Let P, P ∈ R 100×9 be the orthogonal bases of the two subspaces, respectively. The arc length between them is defined as θ 2 , where θ = [θ 1 , . . . , θ 9 ] , with {cos(θ i )} 9 i=1 being the singular values of P P. To reduce statistical variability, results for all the methods are averaged over 50 repetitions.</p><p>Fig. <ref type="figure">2</ref> shows the arc length distances obtained at different sample sizes (N). It can be observed that: 1) as N increases, the principal subspaces obtained by all three methods all converge to the true one; and 2) with limited sample size, BPPCA performs best, which is then followed by PSOPCA, and (as expected) PPCA is the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitivity to Initialization</head><p>Recall that random initialization is used in the CM and AECM algorithms (Algorithms 1 and 2). Our experience suggests that such a simple scheme works well in practice, and almost identical stationary points of the likelihood are obtained with different random initializations. To illustrate this, we report in the following an experiment on sensitive analysis, using the data set in Section V-A (with sample size N = 200). The setup follows that used for the GLRAM in <ref type="bibr" target="#b10">[11]</ref>. In the first trial for CM, R is initialized as [I, 0 ] and σ 2 r as 0.01. For the other nine trials of CM and all ten trials of AECM, the initializations are random. To measure the differences among solutions obtained with different initializations, we measure the arc length distance between the principal subspace for the solution obtained with CM's first trial and those from the other random initializations.</p><p>Results for CM and AECM are shown in Tables I and II, respectively. As can be seen, different initializations converge to the same log-likelihood value and almost identical principal subspace (up to rotation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence of CM and AECM</head><p>In this experiment, we compare the convergence speeds of the CM algorithm (Section IV-A) and AECM algorithm (Section IV-B) for BPPCA. We use the same data set (with  sample size N = 500) from Section V-A. Again, we fit the data with q c = q r = 3. For demonstration purpose, we set tol = 10 -8 . Moreover, C or R are initialized randomly and σ 2 c , σ 2 r are set to 0.01. Fig. <ref type="figure" target="#fig_4">3</ref>(a) plots the evolution of the log likelihood value versus the number of iterations. It can be observed that CM converges in a few iterations while AECM requires around 60 iterations to achieve comparable likelihood value. This is consistent with the theoretical result that the inclusion of missing information may yield slower convergence <ref type="bibr" target="#b14">[15]</ref>.</p><p>However, as the per-iteration complexity of AECM is lower than that of CM, it is interesting to investigate whether AECM could actually be more efficient. Fig. <ref type="figure" target="#fig_4">3(b</ref>) plots the evolution of their log likelihood values versus CPU time. It can be observed that CM is indeed more efficient than AECM on this data set. In general, this is to be expected when the data dimensionality is not high.</p><p>When one/both of the data dimensionalities (d c and d r ) is high, AECM can become more efficient, as is demonstrated in the following experiment. We sample a data set (with sample size 50) from a 500 × 20-D matrix-variate normal distribution with latent dimensions q c = q r = 3. Again, we fit the data with the true latent dimensions. Fig. <ref type="figure" target="#fig_4">3(c</ref>) plots the evolution of their log likelihood values versus CPU time. It can be observed that AECM is more efficient than CM on this high-dimensional data set.  <ref type="formula" target="#formula_16">9</ref>) GLRAM <ref type="bibr" target="#b10">[11]</ref> U c XU r PCA <ref type="bibr" target="#b10">[11]</ref> U vec(X) FPCA <ref type="bibr" target="#b26">[27]</ref> U c XU r 2-DPCA <ref type="bibr" target="#b11">[12]</ref> XU r</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification Performance on Face Data Sets</head><p>In this section, we perform face recognition experiments on two real-world image data sets. 1) XM2VTS <ref type="foot" target="#foot_4">5</ref> , which contains images for 295 individuals. Each individual has eight images taken over a period of four months. The image size is 51 × 55. 2) AR, which contains 126 individuals. Each individual has 26 images. As in <ref type="bibr" target="#b33">[34]</ref>, we use a subset containing 100 individuals (50 men and 50 women), and each person has 14 nonoccluded images with variations in expression and illumination. The image size is 100 × 100. The data is randomly split into training and test sets, such that each class has two, three, or four training samples. The classification error rate, averaged over 20 such repetitions, will be reported. Table <ref type="table" target="#tab_2">III</ref> lists the dimension reduction methods to be compared and their corresponding representations in the reduced-dimensional space. After the compressed representations by each method are obtained, the one-nearestneighbor classifier is then used to obtain the error rates. For all these methods, all possible dimensionalities of the compressed representation are tried and with the best results reported.</p><p>Table <ref type="table" target="#tab_5">IV</ref> shows the error rates obtained by the various methods. The following can be seen.</p><p>1) BPPCA and PPCA substantially outperforms GLRAM, PCA and FPCA. 2) BPPCA is better than PPCA, and this can be attributed to the use of the underlying 2-D data structure. 3) BPPCA is significantly better than PSOPCA. This indicates that the features obtained by BPPCA are significantly superior than those by PSOPCA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance on Data With Nonseparable Covariance</head><p>Recall that BPPCA relies on the assumption of separable covariance (Section III-B). For low-dimensional data, Lu and Zimmerman <ref type="bibr" target="#b22">[23]</ref> proposed a likelihood ratio test for separability. However, for high-dimensional data sets such as those used in Section V-D, this test is impractical as the data covariance matrix becomes singular <ref type="bibr" target="#b34">[35]</ref>.</p><p>To study how nonseparability affects BPPCA, we will examine its performance on the classical iris data set, which is known to have a nonseparable covariance structure <ref type="bibr" target="#b22">[23]</ref>. The iris data set has four variables: sepal length, sepal width, petal length, and petal width. In <ref type="bibr" target="#b22">[23]</ref>, they considered the two crossing factors: "plant part" (sepal or petal) and "physical dimension" (length or width). Using the likelihood ratio test in <ref type="bibr" target="#b22">[23]</ref>, it is shown that these two factors are not separable.</p><p>Table V compares the error rates for BPPCA and PPCA, using the one-nearest-neighbor classifier as in Section V-D. As can be seen, even though BPPCA relies on the separable covariance assumption, it is still significantly better than PPCA (which uses a nonrestrictive covariance). The difference is especially prominent on small training sets. This thus supports the observation in Section III-B that separability can trade bias for lower variance, leading to better generalization even on data sets with nonseparable covariance structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a bilinear probabilistic model called BPPCA for probabilistic dimension reduction on 2-D data. This signals a breakthrough from the classical 1-D latent variable model to the 2-D case. We developed two maximum likelihood estimation algorithms for BPPCA, one is based on CM while the other is based on AECM. The CM algorithm has faster convergence but higher per-iteration complexity, while the AECM algorithm has slower convergence but scales better on high-dimensional data. Similar to PPCA, we showed that the MLE of the BPPCA parameters (C and R) are principal subspaces of the column and row covariance matrices (up to scaling and rotation). In contrast, PSOPCA fails to provide such an important connection. Moreover, empirical results on synthetic data and real-world data sets demonstrate the usefulness of BPPCA over existing methods.</p><p>Nowadays, many real-world data sets are in the form of 3-D or even higher-order tensor <ref type="bibr" target="#b35">[36]</ref>. For example, color images and grayscale video sequences can be regarded as 3-D data, while color video sequence can be regarded as 4-D. Recently, GLRAM has been extended to MPCA for the handling of tensor data <ref type="bibr" target="#b36">[37]</ref>. In the future, we will also consider extending BPPCA, and the accompanying CM and AECM algorithms, along this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MATRIX-VARIATE NORMAL DISTRIBUTION</head><p>The matrix-variate normal distribution is a normal distribution with separable covariance matrix <ref type="bibr" target="#b16">(17)</ref>  <ref type="bibr" target="#b37">[38]</ref>. It is a generalization for the multivariate normal distribution in 1-D. Formally, it is defined as follows.</p><p>Definition 1: A random matrix X ∈ R d c ×d r is said to follow matrix-variate normal, denoted N d c ,d r (W, c , r ), with mean matrix W, column covariance matrix c ∈ R d c ×d c and row covariance matrix r ∈ R d r ×d r , if vec(X) ∼ N d c ×d r (vec(W), r ⊗ c ). The pdf of X is given by</p><formula xml:id="formula_89">p(X) = (2π) -1 2 d r d c | c | -1 2 d r | r | -1 2 d c etr - 1 2 -1 c (X -W) -1 r (X -W)<label>(43)</label></formula><p>where etr(•) = exp(tr(•)). The pdf (43) of the matrix-variate normal is obtained by rewriting the pdf of vec(X) in vector form into the equivalent matrix form. If d r = 1 or d c = 1, the matrix-variate normal degenerates to multivariate normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ROTATION AND SCALING INDETERMINACIES OF BPPCA</head><p>The BPPCA model is unique up to the following transformations.</p><p>1) Orthogonal rotations of the factor loading matrices, latent matrix, column and row noise matrices: For any orthogonal matrices V c ∈ R q c ×q c and V r ∈ R q r ×q r , it is easy to see that</p><formula xml:id="formula_90">CZR + W + C r + c R + = (CV c )(V c ZV r )(V r R ) + W + (CV c )(V c r ) +( c V r )(V r R ) + .</formula><p>As a subspace learning method, we are interested in the subspaces spanned by the columns of C and R and hence this rotation indeterminacy is not a matter of concern. 2) Scaling of the column and row factor loading matrices.</p><p>By multiplying (C, σ c ) by a positive constant a and (R, σ r ) by a -1 simultaneously, it is easy to see that</p><formula xml:id="formula_91">CZR + W + C r + c R + = aCZR a -1 +W + aC r a -1 +a c R a -1 +a a -1 .</formula><p>This is not a problem as we are usually interested in: 1) the Kronecker product of the column and row parameters (instead of either one of them); and 2) the column and row principal subspaces, and the variance ratios contained in these subspaces. For 1), clearly, the effect of scaling can be eliminated. For 2), the scaling effect is also eliminated as follows. It can be seen from ( <ref type="formula" target="#formula_53">30</ref>) that the change C → aC and σ 2 c → aσ 2 c leads to S r → a -1 S r and hence its eigenvalues λ ri → a -1 λ ri , i = 1, . . . , d r . Consequently, U r remains unchanged, r → a -1 r , R → a -1 R in (31) and σ 2 r → a -1 σ 2 r in <ref type="bibr" target="#b31">(32)</ref>. Thus, the row principal subspace spanned by U r is unchanged and the variance ratio q r i=1 a -1 λ ri / d r i=1 a -1 λ ri is unchanged as well. A similar conclusion can be drawn for the column principal subspace and its variance ratio. Thus, the scaling effect is eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C DERIVATIONS FOR THE PROBABILITY DISTRIBUTIONS</head><p>IN SECTION III-D From ( <ref type="formula" target="#formula_36">18</ref>) and (43), the probability density of Y r given Z can be obtained as</p><formula xml:id="formula_92">p(Y r |Z) = (2πσ 2 r ) -1 2 d r q c etr - 1 2 (Y r -ZR )σ -2 r (Y r -ZR ) (44)</formula><p>and the prior density of the latent matrix Z is p(Z) = (2π) -1 2 q r q c etr - </p><formula xml:id="formula_93">= (2π) -1 2 d r q c etr - 1 2 Y r -1 r Y r (<label>46</label></formula><formula xml:id="formula_94">)</formula><p>where r is given by <ref type="bibr" target="#b14">(15)</ref>. Using the Bayes' rule, the conditional density of Z given Y r is p(Z|Y r ) = (2π) -1 2 q r q c etr -</p><formula xml:id="formula_95">1 2 (Z -Y r RM -1 r )σ -2 r M r (Z -Y r RM -1 r )</formula><p>where M r is given by <ref type="bibr" target="#b23">(24)</ref>. Similarly, from ( <ref type="formula" target="#formula_36">18</ref>) and ( <ref type="formula" target="#formula_89">43</ref> Substituting Y r = X -CY r -W into (49) and using (46), we have</p><formula xml:id="formula_96">p(X) = p(X|Y r ) p(Y r )dY r = (2π) -1 2 d r d c | c | -1 2 d r | r | -1 2 d c etr - 1 2 -1 c (X -W) -1 r (X -W)</formula><p>where r is given by <ref type="bibr" target="#b14">(15)</ref>, and the conditional density of Y r given X is</p><formula xml:id="formula_97">p(Y r |X) = (2π) -1 2 d r q c etr - 1 2 σ -2 c M c (Y r -M -1 c C X) -1 r (Y r -M -1 c C X)</formula><p>where M c is given by <ref type="bibr" target="#b23">(24)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Probabilistic graphical models for BPPCA. (a) Original generative model<ref type="bibr" target="#b13">(14)</ref>. (b) Two-stage generative model<ref type="bibr" target="#b17">(18)</ref>, with row followed by column. (c) Two-stage generative model<ref type="bibr" target="#b18">(19)</ref>, with column followed by row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Remarks: Note that the two CM-steps are equivalent to performing PPCA. Recall from (20) that X n ∼ N d c ,d r (0, c , r ). In CM-step 1, with R and σ 2 r fixed, X n -1/2 r ∼ N d c ,d r (0, c , I) and its columns {x ni -1/2 r } i=1,...,d r ,n=1,...,N are i.i.d. and follow N d c (0, c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1) PPCA: a) Compression: Given an observation x, we take E[z|x] in (9) in the low-dimensional latent space as the compressed representation. b) Linear reconstruction: Given the compressed representation E[z|x], we can reconstruct x = CE[z|x]+ μ from (3). Using (9), x -μ = C M -1 C (x -μ). In general, C M -1 C is not a projection matrix [25], except when σ 2 → 0. c) Orthogonal linear reconstruction: We can also reconstruct as xorth = C( C C) -1 ME[z|x] + μ [4]. Using (9), we have xorth -μ = C( C C) -1 C (x -μ), in which C( C C) -1 C is a projection matrix [4]. 2) BPPCA: a) Compression: Similar to PPCA, we take E[Z|X] as the compressed representation. Using (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 2.Arc length distance between the estimated and true principal subspaces at different sample sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Changes in log likelihood L for the CM (solid) and AECM (dotted) algorithms (a) with the number of iterations on the first synthetic data set, (b) with CPU time on the first synthetic data set, and (c) with CPU time on the second high-dimensional synthetic data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>and (45), we have the marginal density of Y r p(Y r ) = p(Y r |Z) p(Z)dZ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 r ) - 1 2 d r d c etr - 1 2 σ - 2 c</head><label>212</label><figDesc>), the conditional density of Y r given c isp(Y r | c ) = (2πσ 2 c σ (Y rc R )σ -2 r (Y rc R ) (47)and the prior distribution of the noise matrix c is p( c ) = (2πσ 2 c ) -1 2 q r d c etrand (48), we obtainp(Y r ) = p(Y r | c ) p( c )d c = (2πσ 2 c ) -1 2 d r d c | r | -1 2 d c etr -1 2 σ -2 c Y r -1 r Y r . (49)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>23]: 12 = ρ 34 , ρ 13 = ρ 24 , ρ 23 = ρ 14 , and ρ 14 = ρ 12 ρ 13 .</figDesc><table><row><cell>σ 11 σ 33</cell><cell>=</cell><cell>σ 22 σ 44</cell><cell>,</cell></row><row><cell cols="4">ρ Despite such a restrictive covariance structure, separabil-</cell></row><row><cell cols="4">ity can significantly reduce the number of parameters in</cell></row><row><cell cols="4">the model (from 1/2d c d r (d c d r + 1) for the full covariance</cell></row><row><cell cols="2">to (1/2) (d</cell><cell></cell><cell></cell></row></table><note><p>c (d c + 1)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I NEGATIVE</head><label>I</label><figDesc>LOG-LIKELIHOOD VALUES AND ARC LENGTH DISTANCES OBTAINED FOR TEN DIFFERENT INITIALIZATIONS OF CM</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Iteration</cell><cell></cell><cell></cell></row><row><cell>Trial</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Distance</cell></row><row><cell>1</cell><cell>76714.2</cell><cell>60168.4</cell><cell>60168.0</cell><cell>60168.0</cell><cell>0</cell></row><row><cell>2</cell><cell>71073.9</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>7.74e-08</cell></row><row><cell>3</cell><cell>73446.2</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>7.15e-08</cell></row><row><cell>4</cell><cell>72262.2</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.00e-07</cell></row><row><cell>5</cell><cell>72716.3</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.17e-07</cell></row><row><cell>6</cell><cell>72890.9</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.50e-07</cell></row><row><cell>7</cell><cell>73390.6</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>8.94e-08</cell></row><row><cell>8</cell><cell>70523.1</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.10e-07</cell></row><row><cell>9</cell><cell>72355.2</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.29e-07</cell></row><row><cell>10</cell><cell>72554.5</cell><cell>60168.2</cell><cell>60168.0</cell><cell>60168.0</cell><cell>1.30e-07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV AVERAGED</head><label>IV</label><figDesc>ERROR RATES (MEAN±STD %) OBTAINED BY THE VARIOUS METHODS ON THE FACE DATA SETS. THE METHOD THAT IS STATISTICALLY SIGNIFICANTLY BETTER (WITH A P-VALUE OF 0.05 USING THE TWO-SAMPLE ONE-TAILED t-TEST) THAN THE OTHER</figDesc><table><row><cell></cell><cell cols="3">METHODS IS MARKED</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Number of training images per individual</cell></row><row><cell>Data set</cell><cell>Method</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>BPPCA</cell><cell>19.8±2.7</cell><cell>15.3±2.2</cell><cell>11.3±1.8</cell></row><row><cell></cell><cell>PSOPCA</cell><cell>31.8±2.8</cell><cell>25.1±2.2</cell><cell>19.9±2.1</cell></row><row><cell>XM2VTS</cell><cell>PPCA FPCA</cell><cell>25.5±3.2 26.2±3.0</cell><cell>19.0±2.1 20.9±2.2</cell><cell>14.5±2.1 16.3±2.1</cell></row><row><cell></cell><cell>GLRAM</cell><cell>26.5±3.1</cell><cell>21.1±2.1</cell><cell>16.5±2.1</cell></row><row><cell></cell><cell>PCA</cell><cell>26.6±3.1</cell><cell>21.2±2.0</cell><cell>16.6±2.0</cell></row><row><cell></cell><cell>2DPCA</cell><cell>26.7±3.1</cell><cell>21.1±2.0</cell><cell>16.6±2.0</cell></row><row><cell></cell><cell>BPPCA</cell><cell>36.1±5.8</cell><cell>25.3±6.0</cell><cell>24.1±6.2</cell></row><row><cell></cell><cell>PSOPCA</cell><cell>44.0±7.5</cell><cell>29.2±7.3</cell><cell>27.4±6.8</cell></row><row><cell>AR</cell><cell>PPCA FPCA</cell><cell>44.6±10.1 58.8±11.6</cell><cell>27.3±5.4 42.0±4.5</cell><cell>24.5±5.9 41.8±10.3</cell></row><row><cell></cell><cell>GLRAM</cell><cell>58.8±11.6</cell><cell>42.0±4.5</cell><cell>41.8±10.3</cell></row><row><cell></cell><cell>PCA</cell><cell>58.9±11.6</cell><cell>42.2±4.4</cell><cell>41.9±10.2</cell></row><row><cell></cell><cell>2DPCA</cell><cell>58.9±11.6</cell><cell>42.1±4.5</cell><cell>41.8±10.3</cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell cols="5">AVERAGED ERROR RATES (MEAN±STD %) OBTAINED BY BPPCA AND</cell></row><row><cell cols="5">PPCA ON THE IRIS DATA SET. THE METHOD THAT IS STATISTICALLY</cell></row><row><cell cols="5">SIGNIFICANTLY BETTER (WITH A p-VALUE OF 0.05 USING THE</cell></row><row><cell cols="5">TWO-SAMPLE ONE-TAILED t-TEST) THAN THE OTHER METHOD IS</cell></row><row><cell></cell><cell></cell><cell>MARKED</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Number of training samples per class</cell></row><row><cell>Method</cell><cell>5</cell><cell>15</cell><cell>25</cell><cell>35</cell></row><row><cell>BPPCA</cell><cell>5.2±2.2</cell><cell>3.5±1.1</cell><cell>3.2±1.2</cell><cell>3.2±1.8</cell></row><row><cell>PPCA</cell><cell>9.4±3.0</cell><cell>7.1±2.1</cell><cell>5.6±1.8</cell><cell>4.3±2.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For an alternative Bayesian framework for PPCA, interested readers are referred to<ref type="bibr" target="#b15">[16]</ref> and<ref type="bibr" target="#b16">[17]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A review on matrix-variate normal distributions is in Appendix VI.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For example, when d c = d r = 20, the full covariance has 80, 200 parameters while the separable covariance has only</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="420" xml:id="foot_3"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Available from http://www.face-rec.org/databases/.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Zhao was supported in part by the <rs type="funder">Science Fund of Yunnan Province</rs> under Grant <rs type="grantNumber">2010CD070</rs> and Grant <rs type="grantNumber">2011Z010</rs>, and the <rs type="funder">Small Project Fund of YNUFE</rs> under Grant <rs type="grantNumber">YC10D028</rs> and Grant <rs type="grantNumber">YCT1013</rs>. The work of <rs type="person">P. L. H. Yu</rs> was supported by the <rs type="funder">Hong Kong Research Grants Councils GRF</rs> under Grant <rs type="grantNumber">HKU 706710P</rs>. The work of <rs type="person">J. T. Kwok</rs> was supported by the <rs type="funder">Research Grants Council of the Hong Kong Special Administrative Region</rs> under Grant <rs type="grantNumber">615209</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HfTKGTF">
					<idno type="grant-number">2010CD070</idno>
				</org>
				<org type="funding" xml:id="_3s4bCwg">
					<idno type="grant-number">2011Z010</idno>
				</org>
				<org type="funding" xml:id="_HQ7Ac4Q">
					<idno type="grant-number">YC10D028</idno>
				</org>
				<org type="funding" xml:id="_d2xGJJA">
					<idno type="grant-number">YCT1013</idno>
				</org>
				<org type="funding" xml:id="_BMvKzGw">
					<idno type="grant-number">HKU 706710P</idno>
				</org>
				<org type="funding" xml:id="_KdtjPPM">
					<idno type="grant-number">615209</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustered Nyström method for large scale manifold learning and dimension reduction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1576" to="1587" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advanced probabilistic models for clustering and projection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Faculty Math. Comput. Sci. Statist., Univ. Munich</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analysers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Latent Variable Models and Factor Analysis, 1st ed</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bartholomew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic PCA for t distributions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997-07">Jul. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GPCA: An efficient dimension reduction scheme for image compression and retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janardan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM Int. Conf. Knowl. Discov. Data Min</title>
		<meeting>10th ACM Int. Conf. Knowl. Discov. Data Min<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix-variate factor analysis and its applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1821" to="1826" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized low rank approximations of matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="167" to="191" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2-D PCA: A new approach to appearance-based face representation and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic interpretations and extensions for a family of 2D PCA-style algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD Workshop Data Min. Using Matri. Tensors</title>
		<meeting>KDD Workshop Data Min. Using Matri. Tensors<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix-variate and higher-order probabilistic projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="392" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc. Series B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Advances in Neural Information Proceeding System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Bayesian PCA</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Bayesian principal component analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smídl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4101" to="4123" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized discriminant analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">405</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a spatially smooth subspace for face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vision Pattern Recognit<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-temporal analysis of multivariate environmental monitoring data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Multivariate Environ. Stat</title>
		<meeting>Multivariate Environ. Stat</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="347" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating MIMO channel covariances from training data under the Kronecker model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating stationary dipoles from MEG/EEG data contaminated with spatially and temporally correlated background noise</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>De Munck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Huizenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Heethaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1565" to="1572" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The likelihood ratio test for a separable covariance matrix</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Probabil. Lett</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="457" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On estimation of covariance matrices with Kronecker product structure</title>
		<author>
			<persName><forename type="first">K</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="478" to="491" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparison of discrimination methods for the classification of tumors using gene expression data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridlyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">457</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factored principal components analysis, with applications to face recognition</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brignell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The EM Algorithm and Extensions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Nonlinear Programming: A Unified Approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zangwill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation via the ECM algorithm: A general framework</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="278" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The EM algorithm: An old folk-song sung to a fast new tune</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Dyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="567" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast ML estimation for the mixture of factor analyzers via an ECM algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1956" to="1961" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The geometry of algorithms with orthogonality constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. App</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="353" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tests for high-dimensional covariance matrices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">490</biblScope>
			<biblScope unit="page" from="810" to="819" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Handbook of Blind Source Separation, Independent Component Analysis and Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MPCA: Multilinear principal component analysis of tensor objects</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="39" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
		<title level="m">Matrix Variate Distributions</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
