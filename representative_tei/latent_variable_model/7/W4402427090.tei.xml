<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-13">13 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shian</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junmei</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Delu</forename><surname>Zeng</surname></persName>
						</author>
						<title level="a" type="main">Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-13">13 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.06710v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian Process Latent Variable Models</term>
					<term>Variational Inference</term>
					<term>Annealed Importance Sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gaussian Process Latent Variable Models (GPLVMs) have become increasingly popular for unsupervised tasks such as dimensionality reduction and missing data recovery due to their flexibility and non-linear nature. An importance-weighted version of the Bayesian GPLVMs has been proposed to obtain a tighter variational bound. However, this version of the approach is primarily limited to analyzing simple data structures, as the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets. In this work, we propose an Annealed Importance Sampling (AIS) approach to address these issues. By transforming the posterior into a sequence of intermediate distributions using annealing, we combine the strengths of Sequential Monte Carlo samplers and VI to explore a wider range of posterior distributions and gradually approach the target distribution. We further propose an efficient algorithm by reparameterizing all variables in the evidence lower bound (ELBO). Experimental results on both toy and image datasets demonstrate that our method outperforms state-of-the-art methods in terms of tighter variational bounds, higher log-likelihoods, and more robust convergence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Gaussian processes (GPs) <ref type="bibr" target="#b0">[1]</ref> have become a popular method for function estimation due to their non-parametric nature, flexibility, and ability to incorporate prior knowledge of the function. Gaussian Process Latent Variable Models (GPLVMs), introduced by <ref type="bibr" target="#b1">[2]</ref>, have paved the way for GPs to be utilized for unsupervised learning tasks such as dimensionality reduction and structure discovery for high-dimensional data. It provides a probabilistic mapping from an unobserved latent space H to data-space X.</p><p>The work by <ref type="bibr" target="#b2">[3]</ref> proposed a Bayesian version of GPLVMs and introduced a variational inference (VI) framework for training GPLVMs using sparse representations to reduce model complexity. This method utilizes an approximate surrogate estimator g(X, H) to replace the true probability term p(X), i.e. E q(H) [g(X, H)] = p(X). VI typically defines an evidence Jian Xu is with School of Mathematics, South China University of Technology in Guangdong Province, China(e-mail:2713091379@qq.com). Shian Du is with the Shenzhen International Graduate School, Tsinghua University, Shenzhen, China(e-mail:dsa1458470007@gmail.com).</p><p>Junmei Yang, Delu Zeng are affiliated with School of Electronic and Information Engineering at South China University of Technology in Guangdong Province, China. Qianli Ma is afffliated with School of Computer Science and Engineering at South China University of Technology in Guangdong Province, China.</p><p>Delu Zeng is the corresponding author (dlzeng@scut.edu.cn ).</p><p>lower bound (ELBO) as the loss function for the model in place of log p(X). To describe the accuracy of this lower bound, we discuss a Taylor expansion of log p(X), E q(H) [log g(X, H)] ≈ log p(X) -</p><formula xml:id="formula_0">1 2 var q(H) g(X, H) p(X)<label>(1)</label></formula><p>The formula has been discussed in numerous works, including <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Therefore, as the variance of the estimator decreases, the ELBO becomes tighter. Based on this formula and the basic principles of the central limit theorem, importanceweighted (IW) VI <ref type="bibr" target="#b5">[6]</ref> seeks to reduce the variance of the estimator by repeatedly sampling from the proposal distribution q(H), i.e., g (X, H)</p><formula xml:id="formula_1">= 1 K K k=1 p(X,H k ) q(H k )</formula><p>, where H k ∼ q (H k ). An importance-weighted version <ref type="bibr" target="#b6">[7]</ref> of the Bayesian GPLVMs based on this has been proposed to obtain a tighter variational bound. While this method can obtain a tighter lower bound than the classical VI, it is a common problem that the relative variance of this importance-sampling based estimator tends to increase with the dimension of the latent variable. Moreover, the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets.</p><p>The problem of standard importance sampling techniques is that it can be challenging to construct a proposal distribution q(H) that performs well in high-dimensional spaces. To address these limitations, we propose a novel approach for variational learning of GPLVMs by leveraging Stochastic Gradient Annealed Importance Sampling (SG-AIS). AIS is derived from early work by <ref type="bibr" target="#b7">[8]</ref> and has been further developed by <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This approach remains one of the 'gold standard' techniques to estimate the evidence unbiasedly because it explores a wider range of posterior distributions and gradually approach the target distribution <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Specifically, our proposed approach leverages an annealing procedure to transform the posterior distribution into a sequence of intermediate distributions, which can be approximated by using a Langevin stochastic flow. This dynamic is a time-inhomogeneous unadjusted Langevin dynamic that is easy to sample and optimize. We also propose an efficient algorithm designed by reparameterizing all variables in the ELBO. Furthermore, we propose a stochastic variant of our algorithm that utilizes gradients estimated from a subset of the dataset, which improves the speed and scalability of the algorithm . Our experiments on both toy and image datasets show that our approach outperforms state-of-the-art methods in GPLVMs, demonstrating lower variational bounds, higher log-likelihoods, and more robust convergence.</p><p>Overall, our contributions are as follows:</p><p>• We propose a novel approach for variational learning of GPLVMs by leveraging Stochastic Gradient Annealed Importance Sampling (SG-AIS), which addresses the limitations of standard importance sampling techniques and allows for the estimation of the evidence unbiasedly, resulting in a tighter lower bound and a better variational approximation in complex data and high-dimensional space.</p><p>• We propose an efficient algorithm designed by reparameterizing all variables to further improve the estimation of the variational lower bounds. We also leverage stochastic optimization to maximize optimization efficiency.</p><p>• Our experiments on both toy and image datasets demonstrate that our approach outperforms state-of-the-art methods in GPLVMs, showing lower variational bounds, higher log-likelihoods, and more robust convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GPLVM Variational Inference</head><p>In GPLVMs, we have a training set comprising of N Ddimensional real valued observations X = {x n } N n=1 ∈ R N ×D . These data are associated with N Q-dimensional latent variables, H = {h n } N n=1 ∈ R N ×Q where Q &lt; D provides dimensionality reduction <ref type="bibr" target="#b2">[3]</ref>. The forward mapping H → X is described by multi-output GPs independently defined across dimensions D. The work by <ref type="bibr" target="#b2">[3]</ref> proposed a Bayesian version of GPLVMs using sparse representations to reduce model complexity. The formula is described as,</p><formula xml:id="formula_2">p(H) = N n=1 N (h n ; 0, I Q ) p(F | U, H) = D d=1 N (f d ; µ d , Q nn ) p(X | F, H) = N n=1 D d=1 N x n,d ; f d (h n ) , σ 2<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">Q nn = K nn -K nm K -1 mm K mn , µ d = K nm K -1 mm u d , F = {f d } D d=1 , U = {u d } D d=1</formula><p>is the inducing variable <ref type="bibr" target="#b14">[15]</ref>, x d is the d-th column of X, and m is the number of inducing points. K nn is the covariance matrix corresponding to a userchosen positive-definite kernel function k θ (h, h ′ ) evaluated on latent points {h n } N n=1 and parameterized by hyperparameters θ. The kernel hyperparameters are shared across all dimensions D. It is assumed that the prior over U and H factorizes into p(u d ) and p(h n ), where p(u d ) = N (0, K mm ) and p(h n ) = N (0, I Q ). Since h n ∈ R Q is unobservable, we need to do joint inference over f (•) and h. Under the typical mean-field assumption of a factorized approximate posterior q(f d )q(h n ). We denote ψ as all variational parameters and γ as all GP hyperparameters. Thus, we arrive at the classical Mean-Field (MF) ELBO:</p><formula xml:id="formula_4">MF-ELBO(γ, ψ) = N n=1 D d=1 q(f d )q (h n ) log p (x n,d | f d , h n ) dh n df d - N n=1 KL (q (h n ) ∥p (h n )) - D d=1 KL (q(u d )∥p(u d )) ,<label>(3)</label></formula><p>where we use the typical approximation to integrate out the inducing variable,</p><formula xml:id="formula_5">q (f d ) = p (f d |u d )q (u d ) du d .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Importance-weighted Variational Inference</head><p>A main contribution of <ref type="bibr" target="#b6">[7]</ref> is to propose a variational scheme for LV-GP models based on importance-weighted VI <ref type="bibr" target="#b5">[6]</ref> via amortizing the optimization of the local variational parameters. IWVI provides a way of lower-bounding the log marginal likelihood more tightly and with less estimation variance by Jensen's inequality at the expense of increased computational complexity. The IW-ELBO is obtained by replacing the expectation likelihood term in Vanilla VI with a sample average of K terms:</p><formula xml:id="formula_6">IW-ELBO(γ, ψ) = N n=1 D d=1 B n,d - D d=1 KL (q(u d )∥p(u d )) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">B n,d = E f d ,hn log 1 K k p (x n,d | f d , h n,k ) p(h n,k )</formula><p>q(h n,k ) . Although the IW objective outperforms classical VI in terms of accuracy, its effectiveness is contingent on the variability of the importance weights: ) . When these weights vary widely, the estimate will effectively rely on only the few points with the largest weights. To ensure the effectiveness of importance sampling, the proposal distribution defined by q (h n,k ) must therefore be a fairly good approximation to p (x n,d | f d , h n,k ) p (h n,k ), so that the importance weights do not vary wildly. Related theoretical proofs can be seen in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_8">p (x n,d | f d , h n,k ) p(h n,k ) q(h n,k</formula><p>When h n,k is high-dimensional, or the likelihood</p><formula xml:id="formula_9">p (x n,d | f d , h n,k</formula><p>) is multi-modal, finding a good importance sampling distribution can be very difficult, limiting the applicability of the method. Unfortunately, original research by <ref type="bibr" target="#b6">[7]</ref> only discusses the case when h n is a one-dimensional latent variable, and they acknowledge that reliable inference for more complex cases is not yet fully understood or documented. To circumvent this issue, we provide an alternative for GPLVMs using Annealed Importance Sampling (AIS) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, which defines state-of-the-art estimators of the evidence and designs efficient proposal importance distributions. Specially, we propose a novel ELBO, relying on unadjusted Langevin dynamics, which is a simple implementation that combines the strengths of Sequential Monte Carlo samplers and variational inference as detailed in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VARIATIONAL AIS SCHEME IN GPLVMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variational Inference via AIS</head><p>Annealed Importance Sampling (AIS) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> is a technique for obtaining an unbiased estimate of the evidence p(X). To achieve this, AIS uses a sequence of K bridging densities {q k (H)} K k=1 that connect a simple base distribution q 0 (H) to the posterior distribution p(H|X). By gradually interpolating between these distributions, AIS allows for an efficient computation of the evidence. This method is particularly useful when the posterior is difficult to sample from directly, as it allows us to estimate the evidence without evaluating the full posterior distribution directly. We can express this as follows:</p><formula xml:id="formula_10">p(X) = p(X, H)dH = E q fwd (H 0:K ) q bwd (H 0:K ) q fwd (H 0:K )<label>(6)</label></formula><p>where the variational distribution q fwd and the target distribution q bwd can be written as:</p><formula xml:id="formula_11">q fwd (H0:K ) = q0 (H0) T1 (H1 | H0) • • • TK (HK | HK-1) q bwd (H0:K ) = p (X, HK ) TK (HK-1 | HK ) • • • T1 (H0 | H1) .<label>(7)</label></formula><p>Here, we assume T k is a forward MCMC kernel that leaves q k (H) invariant, which ensures that {T k } K k=1 are valid transition probabilities, i.e., q k (H k-1 )T k (H k | H k-1 ) dH k-1 = q k (H k ). And Tk is the "backward" Markov kernel moving each sample H k into a sample H k-1 starting from a virtual sample H K . q fwd represents the chain of states generated by AIS, and q bwd is a fictitious reverse chain which begins with a sample from p(X, H) and applies the transitions in reverse order. In practice, the bridging densities have to be chosen carefully for a low variance estimate of the evidence. A typically method is to use geometric averages of the initial and target distributions to construct the sequence, i.e., q k (H) ∝ q 0 (H)</p><formula xml:id="formula_12">1-β k p(X, H) β k for 0 = β 0 &lt; β 1 &lt; • • • &lt; β K = 1.</formula><p>AIS has been proven theoretically to be consistent as K → ∞ <ref type="bibr" target="#b9">[10]</ref> and achieves accurate estimate of log p(X) empirically with the asymptotic bias decreasing at a 1/K rate <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>With this, we can derive the AIS bound,</p><formula xml:id="formula_13">log p(X) ≥ E q fwd (H 0:K ) log q bwd (H 0:K ) q fwd (H 0:K ) = E q fwd(H 0:K ) [log p (X, H K ) -log q 0 (H 0 ) - K k=1 log T k (H k | H k-1 ) Tk (H k-1 | H k ) ].<label>(8)</label></formula><p>This objective can be obtained by applying Jensen's inequality.</p><p>For the LVGP model, we can naturally derive its AIS lower bound:</p><formula xml:id="formula_14">L AIS (ψ, γ) = N n=1 D d=1 E q fwd (h 0:K )q(f d ) [log p (x n,d | f d , h n,K )] + N n=1 E q fwd (h 0:K ) [log p (h n,K ) -log q 0 (h n,0 )] - K k=1 E q fwd (H 0:K ) log T k (H k | H k-1 ) Tk (H k-1 | H k ) - D d=1 KL (q(u d ) ∥ p(u d ))<label>(9)</label></formula><p>where ψ and γ indicate the sets of all variational parameters and all GP hyperparameters, respectively. Our purpose is to evaluate this bound. First we note that the last KL term is tractable if we assume the variational posteriors of u d are mean-field Gaussian distributions. So we concentrate on the terms in the expectation that we can evaluate relying on a Monte Carlo estimate. It is obvious that</p><formula xml:id="formula_15">log p (x n,d | f d , h n,K )</formula><p>is available in closed form as the conditional likelihood is Gaussian <ref type="bibr" target="#b14">[15]</ref>. Therefore, the first three term can be computed by the popular "reparameterization trick" <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> to obtain an unbiased estimate of the expectation over q fwd (H 0:K ) and q (f d ) (detailed in Section III-C). Afterwards, to evaluate expectation over q fwd , we construct an MCMC transition operator T k which leaves q k invariant via a time-inhomogeneous unadjusted (overdamped) Langevin algorithm (ULA) as used in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> and jointly optimize ψ and γ by stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time-inhomogeneous Unadjusted Langevin Diffusion</head><p>T k can be constructed using a Markov kernel with an invariant density such as MH or HMC, which enables q fwd to converge to the posterior distribution of H. For the sake of simplicity, we consider the transition density T k associated to this discretization,</p><formula xml:id="formula_16">T k (H k | H k-1 ) = N (H k ; H k-1 + η∇ log q k (H k-1 ) , 2ηI)<label>(10)</label></formula><p>where η &gt; 0 is the step size and q k is bridging densities defined in Section III-A. Since we have q k (H) ∝ q 0 (H) 1-β k p(X, H) β k in Section III-A, the annealed potential energy is derived as:</p><formula xml:id="formula_17">∇ log q k (•) = β k ∇ log p(X, •) + (1 -β k ) ∇ log q 0 (•). (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>According to conditional probability formula log p(X, •) = log p (X|•) + log p (•), the model log likelihood simplifies to:</p><formula xml:id="formula_19">∇ log p(X|•) = - 1 2 D d=1 ∇(log det Q nn + σ 2 I + (x d -µ d ) T Q nn + σ 2 I -1 (x d -µ d )).<label>(12)</label></formula><p>Since Eq. ( <ref type="formula" target="#formula_19">12</ref>) is analytical, the gradient can be computed through automatic differentiation <ref type="bibr" target="#b22">[23]</ref>. The dynamical system propagates from a base variational distribution q 0 to a final distribution q K which approximates the posterior density.</p><p>Let η := T /K, then the proposal q fwd converges to the path measure of the following Langevin diffusion (h t ) t∈[0,T ] defined by the stochastic differential equation (SDE),</p><formula xml:id="formula_20">dH t = ∇ log q t (H)dt + √ 2 dB t , H 0 ∼ q 0 (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where (B t ) t∈[0,T ] is standard multivariate Brownian motion and q t corresponds to q k in discrete-time for t = t k = kη. For long times, the solution of the Fokker-Planck equations <ref type="bibr" target="#b23">[24]</ref> tends to the stationary distribution q ∞ (H) ∝ exp(p(X, H)).</p><p>Additional quantitative results measuring the law of h T for such annealed diffusions have been showed in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>For ease of sampling, we define the corresponding Euler-Maruyama discretization as,</p><formula xml:id="formula_22">H k = H k-1 + η∇ log q k (H k-1 ) + 2ηϵ k-1 ,<label>(14)</label></formula><p>where ϵ k ∼ N (0, I), as done in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Since such process is reversible w.r.t. q k , based on <ref type="bibr" target="#b27">[28]</ref>, the reversal Tk is typically realized by,</p><formula xml:id="formula_23">H k-1 = H k + η∇ log q k (H k ) + 2ηε k-1 ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_24">εk-1 = -η 2 [∇ log q k (H k-1 ) + ∇ log q k (H k )] - ϵ k-1 .</formula><p>Based on Eq. ( <ref type="formula" target="#formula_16">10</ref>), the term related to T k in Eq. ( <ref type="formula" target="#formula_14">9</ref>) can be written explicitly as:</p><formula xml:id="formula_25">K k=1 R k-1 = K k=1 log T k (H k | H k-1 ) Tk (H k-1 | H k ) = K k=1 1 2 ∥ε k-1 ∥ 2 -∥ϵ k-1 ∥ 2 . (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>We abbreviate this probability ratio as R k-1 . Additional proofs can be seen in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reparameterization Trick and Stochastic Gradient Descent</head><p>For ease of sampling, we consider a reparameterization version of Eq. ( <ref type="formula" target="#formula_14">9</ref>) based on the Langevin mappings associated with q k given by</p><formula xml:id="formula_27">T k (H k-1 ) = H k-1 + η∇ log q k (H k-1 ) + 2ηϵ k-1 . (17)</formula><p>Based on the identity H k = T k (H k-1 ), we have a representation of H k by a stochastic flow,</p><formula xml:id="formula_28">H k = T k (H k-1 ) = T k • T k-1 • • • • T 1 (H 0 )<label>(18)</label></formula><p>Moreover, for LVGP models, we also have a reparameterization version <ref type="bibr" target="#b28">[29]</ref> of the posteriors of H 0 and f d in Eq. ( <ref type="formula" target="#formula_14">9</ref>), that is,</p><formula xml:id="formula_29">h n,0 = a n + L n ϵ f d = K nm K -1 mm m d + K nn -K nm K -1 mm K mm -S d T S d K -1 mm K mn ϵ f d<label>(19)</label></formula><p>where vectors </p><formula xml:id="formula_30">a n ∈ R Q , m d ∈ R N and</formula><formula xml:id="formula_31">= a n + L n ϵ Set L = -log q 0 (H 0 ) for k = 1 to K do Draw ϵ k from standard Gaussian distribution. Set ∇ log q k (•) = β k ( N B log p (X J |•) + log p (•)) + (1 -β k ) ∇ log q 0 (•) Set H k = H k-1 + η∇ log q k (H k-1 ) + √ 2ηϵ k-1 Set εk-1 = η 2 [∇ log q k (H k-1 ) + ∇ log q k (H k )] - ϵ k-1 Set R k-1 = 1 2 ∥ε k-1 ∥ 2 -∥ϵ k-1 ∥ 2 Set L = L -R k-1 end for Sample mini-batch indices I ⊂ {1, . . . , N } with |I| = B Draw ϵ f d from standard Gaussian distribution for d = 1, 2, ..., D . Set L = L+log p (H K )+ N B log p (X I | ϵ f d , ϵ 0:K-1 , ϵ)- D d=1 KL (q(u d ) ∥ p(u d</formula><p>)) Do gradient desent on L(ψ, γ) until ψ, γ converge can be rewritten as:</p><formula xml:id="formula_32">L AIS (ψ, γ) = N n=1 D d=1 E p(ϵ f d )p(ϵ0:K-1)p(ϵ) [log p (x n,d | ϵ f d , ϵ 0:K-1 , ϵ)] + N n=1 E p(ϵ 0:K-1 )p(ϵ) [log p (h n,K ) -log q 0 (h n,0 )] - K k=1 E p(ϵ 0:K-1 )p(ϵ) R k-1 - D d=1 KL (q(u d ) ∥ p(u d )),<label>(20)</label></formula><p>where R k-1 is defined in Eq. ( <ref type="formula" target="#formula_25">16</ref>) and h n,k is reparameterized as</p><formula xml:id="formula_33">h n,k = T k • T k-1 • • • • T 1 (h n,0 ) = ⃝ k i=1 T i (a n + L n ϵ).</formula><p>In order to accelerate training and sampling in our inference scheme, we propose a scalable variational bounds that are tractable in the large data regime based on stochastic variational inference <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and stochastic gradient descent <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> as described in Algorithm 1.</p><p>Instead of computing the gradient of the full log likelihood, we suggest to use a stochastic variant to subsampling datasets into a mini-batch D J with |X J | = B, where J ⊂ {1, 2, .., N } is the indice of any mini-batch. In the meantime, we replace the p (X, H K ) term in Eq. ( <ref type="formula" target="#formula_11">7</ref>) with another estimator computed using an independent mini-batch of indices I ⊂ {1, 2, .., N } with |X I | = B. We finally derive a stochastic variant of the Stochastic Unadjusted Langevin Diffusion AIS algorithm for the LVGP models as describe in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods and Practical Guidelines</head><p>In the following section, we present two sets of experiments. In the first set of experiments, our aim is to demonstrate the quality of our model in unsupervised learning tasks such as data dimensionality reduction and clustering. This will allow us to evaluate the ability of our model to preserve the original information in the data. In the second set of experiments, we evaluate the expressiveness and efficiency of our model on the task of image data recovery.</p><p>We compare three different approaches: (a) Classical Sparse VI based on mean-field (MF) approximation <ref type="bibr" target="#b2">[3]</ref>; (b) Importance-weighted (IW) VI <ref type="bibr" target="#b6">[7]</ref>; (c) ULA-AIS as given by the algorithm presented in this paper. We also provide guidelines on how to tune the step sizes and annealing schedules in Algorithm 1 to optimize performance. We conducted all our experiments on a Tesla A100 GPU. More details can be seen in Appendix including practical guidelines and runtime analysis due to the space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dimensionality Reduction</head><p>The multi-phase Oilflow data <ref type="bibr" target="#b37">[38]</ref> consists of 1000, 12d data points belonging to three classes which correspond to the different phases of oil flow in a pipeline. We reduced the data dimensions to 10 while attempting to preserve as much information as possible. We report the reconstruction error and MSE with ± 2 standard errors over ten optimization runs. Since the training is unsupervised, the inherent ground-truth labels were not a part of training. The 2d projections of the latent space for oilflow data clearly shows that our model is able to discover the class structure.</p><p>To highlight the strength of our model, we set the same experimental hyperparameters and compare the learning curves of two state-of-the-art models. The results are shown in Fig. <ref type="figure">5</ref>. We also tested our model performance on another toy dataset, Wine Quality <ref type="bibr" target="#b38">[39]</ref>, where we used the white variant of the Portuguese "Vinho Verde" wine. From table I, we observe that after sufficient training, our proposed method yields lower reconstruction loss and MSE than IWVI and MF methods. It is noted that our proposed method does not show an increase in time complexity compared to the baseline method IW (and sometimes even lower as shown in Appendix). Therefore, even though we used a fixed number of iterations, we can ensure the fairness of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Make Predictions in Unseen Data</head><p>We conducted a reconstruction experiment on the MNIST and Frey Faces Data, focusing on how models capture uncertainty when training with missing data in structured inputs. For MNIST, we selected digits 1 and 7 with a latent variable Fig. <ref type="figure">1</ref>. We lowered the data dimensionality using our proposed method in the multi-phase oilflow dataset and visualized a two-dimensional slice of the latent space that corresponds to the most dominant latent dimensions. The inverse lengthscales learnt with SE-ARD kernel for each dimension are depicted in the middle plot, and the negative ELBO learning curves are shown in the right plot. We set the same learning rate and compared the learning curves of two state-of-the-art models, MF and Importance Weighted VI within 3000 iterations. dimensionality of 5. Each image has 784 pixels yielding a 784d data space <ref type="foot" target="#foot_0">1</ref> . For Frey Faces Data, we used the entire dataset with a latent variable dimensionality of 20. The image data set <ref type="bibr" target="#b39">[40]</ref> contains 1965 images of a face taken from sequential frames of a short video. Each image is of size 20×28 yielding a 560d data space. In both cases, we chose 5% of the training set as missing data samples and removed 75% of their pixels, seeking to recover their original appearance. Fig. <ref type="figure" target="#fig_0">2</ref> and Fig. <ref type="figure" target="#fig_2">4</ref> summarize the samples generated from the learned latent distribution. This reconstruction experiment is similar to the  gence of the traditional VI methods. We also present in Fig. <ref type="figure" target="#fig_1">3</ref> a comparison of the negative ELBO convergence curves for Frey Faces datasets between our method and two other state-of-the-art methods. To better illustrate our lower convergence values, we gradually increase the y-axis scale from left to right. An interesting observation is that, compared to the IW and MF methods, our proposed method sometimes exhibits sudden drops in the loss curve, as shown in the leftmost plot of Fig. <ref type="figure" target="#fig_1">3</ref>. This can be attributed to the fact that, by adding Langevin transitions, the algorithm's variational distribution gradually moves from the current distribution towards the true posterior distribution, resulting in sudden drops in the loss function when reaching the target distribution. Thus, such phenomena can be regarded as a common feature of annealed importance sampling and it becomes even more obvious in high-dimensional datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we introduce a novel method for GPLVM through Stochastic Gradient Annealed Importance Sampling. Our approach leverages annealing to transform the posterior distribution into a sequence of tractable intermediate distributions, and utilizes unadjusted Langevin dynamics to estimate the Evidence Lower Bound (ELBO). We observe convincing evidence of the superiority of our method, particularly in highdimensional or complex structured datasets, including lower variational bounds and more robust convergence. Furthermore, we also observe certain features in the loss curve of our method, such as steep drops, which further support our claims.</p><p>Overall, our results show that the proposed method achieves superior performance in both accuracy and robustness, indicating its potential as an effective tool for the variational learning of latent-variable GP Models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>For ease of sampling, we define the corresponding Euler-Maruyama discretization as,</p><formula xml:id="formula_34">H k = H k-1 + η∇ log q k (H k-1 ) + 2ηϵ k-1 ,<label>(21)</label></formula><p>where ϵ k ∼ N (0, I). Based on results by <ref type="bibr" target="#b27">[28]</ref>, the backward step is realized by</p><formula xml:id="formula_35">H k-1 = H k + η∇ log q k (H k ) + 2ηε k-1 ,<label>(22)</label></formula><p>Thus we have,</p><formula xml:id="formula_36">η∇ log q k (H k-1 ) + 2ηϵ k-1 = -η∇ log q k (H k ) -2ηε k-1<label>(23)</label></formula><p>Then, Finally,</p><formula xml:id="formula_37">εk-1 = - η 2 (∇ log q k (H k-1 ) + ∇ log q k (H k )) -ϵ k-1<label>(24)</label></formula><formula xml:id="formula_38">log T k (H k | H k-1 ) Tk (H k-1 | H k ) = log p (ϵ k-1 ) det ∂H k ∂ϵ k-1 p (ε k-1 ) det ∂H k-1 ∂ε k-1 = log p (ϵ k-1 ) p (ε k-1 ) = 1 2 ∥ε k-1 ∥ 2 -∥ϵ k-1 ∥ 2 (<label>25</label></formula><formula xml:id="formula_39">)</formula><p>Instead of computing the gradient of the full log likelihood, we suggest to use a stochastic variant to subsampling datasets into a mini-batch D J with |X J | = B, where J ⊂ {1, 2, .., N } is the indice of any mini-batch. We can thus define an estimator of ∇ log p(X | •) in Eq. ( <ref type="formula" target="#formula_19">12</ref>) as,</p><formula xml:id="formula_40">∇ log p(X | •) ≈ N B ∇ log p(X J | •)<label>(26)</label></formula><p>In the meantime, we replace the p (X, H K ) term in Eq. ( <ref type="formula" target="#formula_11">7</ref>) with another estimator computed using an independent minibatch of indices I ⊂ {1, 2, .., N } with</p><formula xml:id="formula_41">|X I | = B, i.e. p (X, H K ) ≈ p (H K ) p (X I | H K ) N B<label>(27)</label></formula><p>With jointly using the reparameterization trick and stochastic gradient descent, we finally derive a stochastic variant of the Stochastic Unadjusted Langevin Diffusion AIS algorithm for the LVGP models as describe in Algorithm 1.Thanks to GPU acceleration, we can extend the proposed algorithm to larger datasets, such as image-based visual tasks. In this section, we would like to clarify the differences between our approach and the recently proposed Differentiable AIS (DAIS) method <ref type="bibr" target="#b41">[42]</ref>. DAIS was introduced to avoid the non-differentiability issue in AIS due to the Metropolis-Hastings correction step.</p><p>To achieve differentiability and enable the use of gradientbased optimization for maximizing the marginal likelihood as the objective function, DAIS was proposed as a variant of AIS by bypassing the Metropolis-Hastings correction. Furthermore, in <ref type="bibr" target="#b42">[43]</ref>, DAIS was combined with variational inference for black-box inference in probabilistic programming frameworks. However, a significant difference from the algorithm discussed in this paper is that DAIS is not an inhomogeneous Unadjusted Langevin Algorithm (ULA), but a perturbed Hamiltonian system. It is known that Hamiltonian mechanics and Langevin dynamics belong to different dynamics formalisms, i.e., classical mechanics and Langevin statistical mechanics, respectively. For example, leapfrog integration is commonly used in Hamiltonian systems, while the ULA system is typically obtained by solving a reverse stochastic differential equation (SDE).</p><p>Our algorithm takes inspiration from nonequilibrium statistical mechanics <ref type="bibr" target="#b27">[28]</ref> and applies it to the inference of the Bayesian Gaussian process latent variable model (GPLVM). It is important to note that the previous two methods do not discuss the model presented in this paper but provide preliminary evidence of the effectiveness of the method in Bayesian linear regression.</p><p>Therefore, while there are some similarities in terms of using differentiable optimization and combining variational inference with AIS, our approach utilizes nonequilibrium dynamics and is tailored for the Bayesian GPLVM model.</p><p>In the context of this paper, the posterior distribution refers to the distribution of the latent variables given the observed data. This distribution is often intractable and challenging to sample from directly. SG-AIS aims to approximate this posterior distribution by transforming it into a sequence of intermediate distributions, which can be more tractable and easier to sample from.</p><p>The annealing process gradually transforms the posterior distribution by introducing a temperature parameter β . By annealing from β = 0 to β = 1 , we move from an initial distribution, where the posterior is approximated by a simpler distribution to the target posterior distribution itself. The key idea behind annealing is it allows for a smoother exploration of the posterior space. At each intermediate distribution, we can use importance sampling to estimate the evidence by sampling from the proposal distribution and reweighting the samples using the ratios of the target and proposal distributions.</p><p>As the annealing process progresses, the samples from the proposal distribution gradually become more representative of the target distribution. This means that the exploration of the posterior space is not limited to a specific region but covers a wider range of possible configurations of the latent variables. The benefit of this exploration is that it allows for a more accurate estimation of the evidence, which corresponds to a tighter lower bound in the variational learning framework. By gradually annealing the temperature and exploring different distributions, SG-AIS can capture more complex structures in the posterior distribution, leading to better variational approximations in complex data and high-dimensional spaces.</p><p>When using the Unadjusted Langevin Diffusion method for sampling, one key challenge is to determine an appropriate step size η k A fixed step size may work well for some samples but may be suboptimal for others. To address this issue, we can use the Adagrad <ref type="bibr" target="#b43">[44]</ref> optimizer to adaptively adjust the step size based on the historical gradient information. Specifically, for each dimension of the sampled variables, we divide the initial step size by the square root of the sum of squared gradient values for that dimension up to a noise. This technique can help achieve better performance and faster convergence, especially when dealing with complex and highdimensional distributions where finding an appropriate step size is challenging. The adaptive step size adjustment can be implemented in combination with other techniques, such as early stopping, to further improve the sampling efficiency. η k = 0.9 * η k-1 + 0.1 * η 0 √ G k + ϵ where G k is the sum of squared gradient values up to step k in Eq. ( <ref type="formula">17</ref>), ϵ is a small smoothing term to avoid division by zero,and η 0 is the initial step size.</p><p>In the context of Annealed Importance Sampling (AIS), choosing an optimal temperature schedule β k is a challenging task. When choosing an appropriate annealing schedule for Stochastic Gradient Annealed Importance Sampling (SG-AIS), there are several trade-offs and considerations to keep in mind:</p><p>• Computational Efficiency: The annealing schedule should be carefully designed to balance the computational resources required for estimating the evidence. Too many bridging densities can lead to excessive computational burden, while too few densities may result in less accurate estimates.</p><p>• Exploration vs Exploitation: The annealing schedule should strike a balance between exploration and exploitation of the posterior distribution. An aggressive schedule that moves quickly from the base distribution to the posterior may lead to exploration limitations, while a slow schedule may lead to insufficient exploration and inefficiency.</p><p>• Smoothness of Transition: The annealing schedule should ensure a smooth transition between bridging densities.</p><p>Abrupt changes in the densities can result in highvariance importance weights, which may lead to inaccurate estimates. Smooth transitions can be achieved by gradually adjusting the temperature or using appropriate interpolation functions. We often use a linear schedule, where the temperature values are fixed and regularly spaced between 0 and 1. However, this approach may not always work well in practice, as the search space is complex and high-dimensional.</p><p>Alternatively, we can try to learn the temperature values β k directly as additional inference parameters ϕ. This can be done using various techniques, such as gradient-based optimization . By doing so, we can obtain a temperature schedule that is tailored to the specific problem at hand and achieve better sampling performance. Additional experimental information can be seen in Table <ref type="table" target="#tab_2">III</ref>  <ref type="foot" target="#foot_2">2</ref> .</p><p>In our experiments, we observed that the time complexity of Importance-weighted (IW) VI and SG-AIS almost linearly increases with K as K increases.</p><p>In the IW algorithm, the time complexity mainly stems from the K repeated samplings of latent variables to data, which is determined by the time complexity of the GPLVM model itself, O(nm 2 ). As a result, as we increase the number of samples K, the frequency of repeated samplings increases, leading to a linear increase in time complexity.</p><p>In the AIS algorithm, only one sampling of latent variables to data is required, while the intermediate variable sampling is allocated to the annealing procedure, specifically the computation of Langevin stochastic flow. This sampling process is relatively less complex compared to the time complexity of the GPLVM model itself. As depicted in Fig <ref type="figure">5</ref>.</p><p>Therefore, on Frey Faces dataset, as depicted in Table <ref type="table">4</ref>, compared to IW, the time complexity of AIS becomes lower as K reaches a certain threshold. Based on the results presented in the main text, we have robustly demonstrated that our method significantly improves efficiency compared to IW, under the same time consumption. Specially, our training procedure leverages the marginalisation principle of Gaussian distributions and the fact that the data dependent terms of the ELBO factorise across data points and dimensions. This means we can trivially marginalise out the missing dimensions x a , because each individual data point x is modelled as a joint Gaussian. Consider a high-dimensional point x which we split into observed, x o and unobserved x a dimensions, In this formula, the indices of missing and observed dimensions are denoted by a and o respectively, where D = a∪o represents all dimensions in the data. The marginal distributions f d ∈ R N are defined in Eq. ( <ref type="formula" target="#formula_5">4</ref>).The latent variables h n for each data point are informed only by the observed dimensions. Furthermore, we can easily reconstruct the missing dimensions during training by constructing a variational latent distribution q(H), as described in Section 4. This approach enables us to efficiently handle missing dimensions in high-dimensional datasets without requiring major modifications to the overall training process. We have also conducted additional experiments comparing our proposed approach to the Standard GPLVM <ref type="bibr" target="#b44">[45]</ref> in Table <ref type="table">V</ref>. We performed experiments 10 times and averaged the results, analyzing the performance (in terms of MSE and NLL) on four different datasets. Due to limited computational resources, we were only able to run the Standard GPLVM on a subset of the image datasets. For the image reconstruction task, we randomly selected 300 images as the training set and used consistent hyperparameters for the other experiments, as stated in our main paper.</p><formula xml:id="formula_42">d∈a d∈o p (x a , x o | f d , H) dx a = d∈o p (x o | f d , H) (28)</formula><p>In addition, we would like to emphasize that one of our baselines for comparison is the MF method <ref type="bibr" target="#b2">[3]</ref>. As discussed in its experimental section, even the authors of MF approach also suggests that the performance of the Standard GPLVM may not match the Bayesian GPLVM on the Oilflow and Frey Faces datasets. This highlights the advantages of Bayesian methods, which offer greater flexibility, uncertainty modeling, and generalization capabilities compared to the maximum likelihood estimation of the Standard GPLVM. Furthermore, the focus of our paper is to explore a way to combine variational inference, AIS and Bayesian methodology to better estimate the posterior distribution. In this section, we will demonstrate the visual effects of the MF and IW methods on three datasets: Oilflow, MINIST, and Frey Faces. These visualizations will be used for comparison with the main text. There results can be seen in Fig. <ref type="figure" target="#fig_3">6</ref>, Fig. <ref type="figure">7</ref>, Fig. <ref type="figure">8</ref>, Fig. <ref type="figure">9</ref>, Fig. <ref type="figure">10</ref>, Fig. <ref type="figure">11</ref>.</p><p>From the visual appearance, it may seem that all three methods produce similar reconstructions. However, upon closer inspection, we can observe differences in certain details such as brightness and contrast. While these differences may be difficult to discern with the naked eye, we have quantified them using the mean squared error (MSE) between the reconstructed images and the ground truth. The MSE results for all three methods on the test set are reported in Tables 2 in the main text.</p><p>One potential limitation could be the scalability of the method. As the size of the dataset increases, the computational resources required for estimating the evidence using SG-AIS may become more demanding. This is particularly true for large-scale datasets such as ImageNet, which contain millions of images. Running experiments on such massive datasets might pose challenges in terms of computational efficiency and memory requirements.</p><p>We would like to emphasize that GPLVM indeed did not utilize ImageNet as a precedent dataset. Given that ImageNet involves higher-dimensional data, it may be more appropriate to combine GPLVM with other deep learning tools, such as convolutional neural networks (CNNs) and transformers. We are currently exploring broader application scenarios to incorporate these tools effectively and leave room for future work. Thank you for bringing up this point, as it is important to consider the specific requirements and complexities of high-dimensional datasets like ImageNet when exploring the applicability of SG-AIS.</p><p>Additionally, the annealing schedule plays a crucial role in the exploration of the posterior distribution. Designing an appropriate annealing schedule may require domain knowledge or trial and error experimentation. It might be necessary to tune the schedule to ensure a balance between exploration and exploitation, as well as a smooth transition between bridging densities.</p><p>Regarding the applicability of SG-AIS in real-world applications, its performance may depend on the specific characteristics and requirements of the domain. Different datasets and applications may exhibit unique challenges, such as data sparsity, high dimensionality, or non-linear relationships, which could affect the effectiveness of SG-AIS. Evaluating the performance of SG-AIS in different domains and addressing these challenges would require further experimentation and investigation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. In the Brendan faces reconstruction task with 75% missing pixels, the top row represents the ground truth data and the bottom row showcases the reconstructions from the 20-dimensional latent distribution.</figDesc><graphic coords="6,48.96,310.56,514.07,230.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The negative ELBO convergence curves of the three methods on the Frey Faces dataset. It is noted that as the number of iterations increase, the y-axis scale gradually increases from left to right.</figDesc><graphic coords="7,48.96,56.07,514.05,100.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. For the MNIST reconstruction task with 75% missing pixels, we chose digits 1 and 7. The bottom row represents the ground truth data and the top row showcases the reconstructions from the 5-dimensional latent distribution.The left side shows the reconstruction task, while the right side displays the 2-dimensional latent space corresponding to the smallest lengthscales.</figDesc><graphic coords="8,48.96,56.07,514.07,143.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Dimensional Reduction Results for MF method</figDesc><graphic coords="10,48.96,143.41,251.04,149.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Missing Data Recovery Results for MF method. The bottom row represents the ground truth data and the top row showcases the reconstructions from the 20-dimensional latent distribution</figDesc><graphic coords="12,48.96,56.07,514.07,385.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,48.96,188.72,514.07,385.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,48.96,188.72,514.07,385.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,48.96,188.72,514.07,385.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF MF, IW, AND AIS UNDER DIFFERENT NUMBER OF ITERATIONS FOR TWO TOY DATASETS</figDesc><table><row><cell>Dataset</cell><cell>Data Dim</cell><cell>Method</cell><cell cols="2">Iterations Negative ELBO</cell><cell>MSE</cell><cell>Negative Expected Log Likelihood</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>3.44 (0.25)</cell><cell>6.83 (0.27)</cell><cell>-1.42 (0.27)</cell></row><row><cell></cell><cell></cell><cell>MF</cell><cell>2000</cell><cell>-1.67 (0.17)</cell><cell>3.59 (0.13)</cell><cell>-8.38 (0.12)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>-3.07 (0.12)</cell><cell>2.79 (0.11)</cell><cell>-11.24 (0.10)</cell></row><row><cell>Oilflow</cell><cell>(1000,12)</cell><cell>IW</cell><cell>1000 2000</cell><cell>0.01 (0.25) -3.19 (0.15)</cell><cell>4.52 (0.28) 2.77 (0.16)</cell><cell>-6.26 (0.26) -9.46 (0.15)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>-4.13 (0.14)</cell><cell>2.60 (0.15)</cell><cell>-12.20 (0.12)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>0.78 (0.24)</cell><cell>4.99 (0.23)</cell><cell>-4.01 (0.26)</cell></row><row><cell></cell><cell></cell><cell>AIS (ours)</cell><cell>2000</cell><cell>-5.04 (0.15)</cell><cell>2.65 (0.15)</cell><cell>-10.33 (0.16)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>-6.82 (0.12)</cell><cell>2.16 (0.12)</cell><cell>-13.06 (0.11)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>32.69(0.13)</cell><cell>63.98(0.12)</cell><cell>31.71(0.15)</cell></row><row><cell></cell><cell></cell><cell>MF</cell><cell>2000</cell><cell>13.46(0.03)</cell><cell>48.95(0.05)</cell><cell>6.51(0.06)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>11.59(0.03)</cell><cell>45.81(0.04)</cell><cell>4.07(0.05)</cell></row><row><cell>Wine Quality</cell><cell>(1599,11)</cell><cell>IW</cell><cell>1000 2000</cell><cell>22.65(0.07) 11.47(0.02)</cell><cell>50.77(0.06) 40.86(0.03)</cell><cell>19.94(0.09) 3.72(0.04)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>10.73(0.03)</cell><cell>35.23(0.04)</cell><cell>2.71(0.03)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>29.63(0.07)</cell><cell>57.49(0.05)</cell><cell>27.67(0.06)</cell></row><row><cell></cell><cell></cell><cell>AIS (ours)</cell><cell>2000</cell><cell>10.43(0.03)</cell><cell>34.60(0.03)</cell><cell>3.58(0.04)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>8.86(0.04)</cell><cell>32.23(0.04)</cell><cell>2.47(0.03)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Stochastic Unadjusted Langevin Diffusion(ULA) AIS algorithm for GPLVMs Input: training data X, mini-batch size B, sample number K, annealing schedule {β k }, stepsizes η Initialize all DGP hyperparameters γ, all variational parameters ψ repeat Sample mini-batch indices J ⊂ {1, . . . , N } with |J| = B Draw ϵ from standard Gaussian distribution. Set H 0</figDesc><table /><note><p><p><p><p>upper triangular matrixs L n , S d are the variational parameters, ϵ ∈ R Q , ϵ f d ∈ R N are standard Gaussian distribution. After this reparameterization, a change of variable shows that AIS bound in Eq. (</p>9</p>)</p>Algorithm 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF MF, IW, AND AIS UNDER DIFFERENT NUMBER OF ITERATIONS FOR TWO IMAGE DATASETS</figDesc><table><row><cell>Dataset</cell><cell>Data Dim</cell><cell>Method</cell><cell cols="2">Iterations Negative ELBO</cell><cell>MSE</cell><cell>Negative Expected Log Likelihood</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>48274 (443)</cell><cell>468 (9)</cell><cell>46027 (356)</cell></row><row><cell></cell><cell></cell><cell>MF</cell><cell>2000</cell><cell>6346 (20)</cell><cell>95 (1)</cell><cell>4771 (17)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>3782 (15)</cell><cell>69 (0.2)</cell><cell>2822 (3)</cell></row><row><cell cols="2">Frey Faces (1965,560)</cell><cell>IW</cell><cell>1000 2000</cell><cell>42396 (426) 5643 (15)</cell><cell>394 (8) 76 (1)</cell><cell>39936 (312) 4292 (13)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>3596 (14)</cell><cell>63 (0.5)</cell><cell>2535 (4)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>12444 (451)</cell><cell>121 (9)</cell><cell>10543 (322)</cell></row><row><cell></cell><cell></cell><cell>AIS (ours)</cell><cell>2000</cell><cell>5031 (16)</cell><cell>66 (1)</cell><cell>3130 (15)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>3249 (12)</cell><cell>57(0.3)</cell><cell>2226 (3)</cell></row><row><cell></cell><cell></cell><cell>MF</cell><cell>2000</cell><cell>-432.32(0.33)</cell><cell>0.27(0.004)</cell><cell>-552.87(0.28)</cell></row><row><cell>MNIST</cell><cell>(2163,784)</cell><cell>IW</cell><cell>2000</cell><cell>-443.64(0.37)</cell><cell>0.25(0.003)</cell><cell>-567.13(0.31)</cell></row><row><cell></cell><cell></cell><cell>AIS (ours)</cell><cell>2000</cell><cell>-453.18(0.27)</cell><cell>0.25(0.002)</cell><cell>-569.93(0.26)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TRAINING</head><label></label><figDesc>EXPERIMENTAL CONFIGURATION WHERE N AND D DENOTE THE NUMBER OF DATA POINTS AND DATA SPACE DIMENSIONS, Z DENOTES THE NUMBER OF INDUCING INPUTS SHARED ACROSS DIMENSIONS, Q DENOTES THE DIMESIONALITY OF THE LATENT SPACE, LR DENOTES THE LEARNING RATE, K DENOTES THE LENGTH OF THE TRANSITION CHAIN IN ULA-AIS AND IN IW K DENOTES THE NUMBER OF REPETITIONS OF SAMPLING .</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>N</cell><cell>D</cell><cell>Z</cell><cell>Q</cell><cell>LR</cell><cell>K</cell></row><row><cell>Oilflow</cell><cell>Dimensionality Reduction</cell><cell>1000</cell><cell>12</cell><cell>50</cell><cell>10</cell><cell>0.02</cell><cell>5</cell></row><row><cell>Wine Quailty</cell><cell>Dimensionality Reduction</cell><cell>1599</cell><cell>11</cell><cell>50</cell><cell>9</cell><cell>0.02</cell><cell>5</cell></row><row><cell>Frey Face</cell><cell>Missing Data Recovery</cell><cell>1965</cell><cell>560</cell><cell>50</cell><cell>20</cell><cell>0.02</cell><cell>25</cell></row><row><cell>MNIST</cell><cell>Missing Data Recovery</cell><cell>2163</cell><cell>784</cell><cell>50</cell><cell>5</cell><cell>0.02</cell><cell>25</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF RUNNING TIME AMONG MF, IW, AND AIS ALGORITHMS IN ONE EPOCH</figDesc><table><row><cell>Datasets</cell><cell>Method</cell><cell>Time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frey Faces</cell><cell>IW</cell><cell>1.46s (K=5)</cell><cell>2.85s(K=10)</cell><cell>4.06s(K=15)</cell><cell>5.45s(K=20)</cell><cell>7.03s(K=25)</cell></row><row><cell></cell><cell>AIS (Ous)</cell><cell>1.53s (K=5)</cell><cell>2.65s (K=10)</cell><cell>3.79s(K=15)</cell><cell>4.80s(K=20)</cell><cell>5.93s (K=25)</cell></row></table><note><p><p><p>Fig.</p>5</p>. The graphical models of IW and our method. Unlike IW mehtod, only one sampling of latent variables to data is required for our AIS method.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>MSE</head><label></label><figDesc>AND NLL FOR OUR AIS-SG COMPARED TO STANDARD GPLVM IN ALL EXPERIMENTAL DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>method</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>Oilflow (1000,12)</cell><cell>Standard GPLVM</cell><cell>2.45(0.05)</cell><cell>-12.42(0.07)</cell></row><row><cell></cell><cell>AIS (Ours)</cell><cell>1.71(0.04)</cell><cell>-15.81(0.04)</cell></row><row><cell>Wine Quality (1599,11)</cell><cell>Standard GPLVM</cell><cell>30.53(0.03)</cell><cell>2.82 (0.02)</cell></row><row><cell></cell><cell>AIS (Ours)</cell><cell>30.79 (0.04)</cell><cell>2.42(0.03)</cell></row><row><cell>Frey Faces (300,560)</cell><cell>Standard GPLVM</cell><cell>130 (7)</cell><cell>2632 (6)</cell></row><row><cell></cell><cell>AIS (Ours)</cell><cell>115 (6)</cell><cell>2417(5)</cell></row><row><cell>MNIST (300,784)</cell><cell>Standard GPLVM</cell><cell>0.36(0.01)</cell><cell>-484(3)</cell></row><row><cell></cell><cell>AIS (Ours)</cell><cell>0.31(0.01)</cell><cell>-496(2)</cell></row><row><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Since the MNIST dataset converges within</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2000" xml:id="foot_1"><p>iterations, we report the performance of several methods at convergence related work by<ref type="bibr" target="#b2">[3]</ref> and<ref type="bibr" target="#b40">[41]</ref>. More details can be seen in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We test all of our experiments on NVIDIA A100</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix D.</p><p>To demonstrate the effectiveness of our method in producing more accurate likelihoods and tighter variational bounds on image datasets, we present in Table II the negative ELBO, negative log-likelihood, and mean squared error (MSE) for reconstructed images on the Frey Faces and MNIST datasets, comparing with state-of-the-art methods. Our results show that our method achieves lower variational bounds and converges to higher likelihoods, indicating superior performance in highdimensional and multi-modal image data. This suggests that adding Langevin transitions appears to improve the conver-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
	<note>Summer school on machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic non-linear principal component analysis with gaussian process latent variable models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian gaussian process latent variable model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Metflow: A new efficient method for bridging the gap between markov chain monte carlo and variational inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kotelevskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Denain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grinsztajn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12253</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Filtering variational objectives</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Importance weighting and variational inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep gaussian processes with importance-weighted variational inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonequilibrium equality for free energy differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jarzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">2690</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonequilibrium measurements of free energy differences for microscopically reversible markovian systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Crooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1481" to="1487" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential monte carlo samplers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jasra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="436" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Annealing between distributions by averaging moments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sandwiching the marginal likelihood using bidirectional monte carlo</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02543</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the quantitative analysis of decoder-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04273</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Controlled sequential monte carlo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2904" to="2929" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic normalizing flows</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5933" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural langevin dynamics for neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marceau-Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Geometric Science of Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic differentiation in machine learning: a survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Radul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fokker-planck equation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Risken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fokker-Planck Equation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="63" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sampling normalizing constants in high dimensions using inhomogeneous diffusions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Whiteley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07583</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Simulated annealing from continuum to discretization: a convergence analysis via the eyring-kramers law</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02339</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the simulated annealing in rd</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">109086</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonequilibrium candidate monte carlo is an efficient tool for equilibrium simulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Nilmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Crooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Chodera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="E1009" to="E1018" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational inference for deep gaussian processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="361" to="369" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Markovian score climbing: Variational inference with kl (p--q)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Naesseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">510</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic gradient hamiltonian monte carlo</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic gradient hamiltonian monte carlo methods with recursive variance reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Consistency and fluctuations for stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Thiery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Vollmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured stochastic gradient mcmc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alexos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="414" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analysis of multiphase flows using dualenergy gamma densitometry and neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="580" to="593" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling wine preferences by data mining from physicochemical properties</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cerdeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision support systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="553" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributed variational inference in sparse gaussian process regression and latent variable models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Differentiable annealed importance sampling and the perils of gradient noise</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">410</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Surrogate likelihoods for variational annealed importance sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jankowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9881" to="9901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
