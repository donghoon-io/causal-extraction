<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder>
					<orgName type="full">NIA-AA National Institute on Aging and Alzheimer&apos;s Association. NLL Negative Log-Likelihood. OASIS Open Access Series of Imaging Studies</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Alzheimer&apos;s Disease (AD)</term>
					<term>Neuro-imaging (NI)</term>
					<term>Magnetic Resonance Imaging (MRI)</term>
					<term>Positron Emission Tomography (PET)</term>
					<term>Variational Autoencoder (VAE)</term>
					<term>Multi-Task Learning (MTL)</term>
					<term>High Dimensionality</term>
					<term>Heterogeneous Data ADAS-Cog Alzheimer&apos;s Disease Assessment Scale -Cognitive Subscale. ADNI Alzheimer&apos;s Disease Neuroimaging Initiative CCA Canonical Correlation Analysis. CDR Clinical Dementia Rating Scale FDG-PET Fluorodeoxyglucose Positron Emission Tomography. ICA Independent Component Analysis KNN k-Nearest Neighbors. LDA Linear Discriminant Analysis MCI Mild Cognitive Impairment. MCVAE Multi-Channel Variational Autoencoder. MICE Multivariate Imputation by Chained Equations. MIRIAD Minimal Interval Resonance Imaging in Alzheimer&apos;s Disease. MMSE Mini-Mental State Examination. MNAR missing not at random. MRI Magnetic Resonance Imaging PCA Principal Component Analysis. PET Positron Emission Tomography PLS Partial Least Squares. RRR Reduced Rank Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This thesis presents new computational tools for the joint modeling of multi-modal biomedical data, robust to missing data, with application to neuroimaging studies in dementia. The theoretical base for this work is the Variational Autoencoder (VAE), a latent variable generative model well suited for working with complex data as it forces them into a simpler low-dimensional space, able to model data non-linearities.</p><p>The core of this Thesis consists in the Multi-Channel Variational Autoencoder (MCVAE), an extension of the VAE to jointly model latent relationships across multi-modal observations. This is achieved by: 1) constraining the latent distribution of each data modality to a common target prior, 2) forcing these latent distribution to generate all the data modalities through their associated generative functions.</p><p>Moreover, we adapt the MCVAE to a Multi-Task setting, where the problem of dealing with missing data is addressed with a specific optimization scheme following these steps: 1) defining tasks across datasets based on the identification of data subsets presenting compatible modalities, 2) stacking multiple instances of the MCVAE, where each instance models a specific task, 3) sharing the models parameters of common modalities between modeling tasks. Thanks to these actions, the Multi-Task MCVAE allows to learn a joint model for all the data points leveraging on all the available information.</p><p>Overall, this thesis provides a novel investigation of flexible approaches to account for data heterogeneity in the analysis of biomedical information. This work enables new research directions in which medical information can be consistently modeled within a joint probabilistic framework accounting for multiple data modalities, missing information, and biases across different datasets.</p><p>Lastly, thanks to their general formulation, the methodologies here proposed can find applications beyond the neuroimaging research field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>First of all I would like to thank Pr. Nicholas Ayache for being actively present in all the most important moments of the PhD. I want to thank him for his scientific insights and writing advices, but mostly for the constant reassurance and encouragement. Pr. Philippe Robert and Dr. Valeria Manera, for the insightful discussions about the importance of translating the research findings by taking into account the human clinical environment and, on the other way round, to enrich fundamental research with observations from clinical research. Dr. Marco Lorenzi, to whom I just can't say thank you enough for his tireless scientific support over these years, for his contagious enthusiasm, and for having taught me that hard work and attention to details eventually pay you back.</p><p>I would like to thank the jury members, in particular Pr. Gloria Menegaz and Pr. Pietro Michiardi for their voluntary contribution in reviewing this manuscript, their comments and advises. I equally thank Pr. Valentina Garibotto who accepted the role of jury member with manifested joy and interest. I am very happy for having shared this important journey with wonderful colleagues, ending up becoming dear friends: Clément, Jaume, Sara, Santiago, few lines here cannot express adequately my gratitude, so I will just say: thank you! A very special thank you goes to Silvia, who made me aware of what does it really mean to care for something special.</p><p>I reserve these final lines to thank my parents Antonio and Maria Concetta and my little brother Pierpaolo for their interest, support, love, and affection. I never get used to be far from them, but knowing that they are always there for me makes me feel strong and safe in any circumstance.</p><p>Thank you all, sincerely. Luigi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Financial Support</head><p>This work has been supported by:</p><p>• the French government, through the UCA JEDI Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-15-IDEX-01;</p><p>• the French government, through the 3IA Côte d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002;</p><p>• the OPAL infrastructure from Université Côte d'Azur, providing computational resources and support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Context</head><p>Dementia is an umbrella term for several diseases affecting memory, other cognitive abilities and behavior that interfere significantly with a person's ability to maintain their activities of daily living. Although age is the strongest known risk factor for dementia, it is not a normal part of aging.<ref type="foot" target="#foot_0">foot_0</ref> Alzheimer's Disease (AD) is the most common cause of dementia and accounts for 60% to 80% of the cases <ref type="bibr">[Alzheimer Association Report, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Alzheimer's Disease</head><p>Alzheimer's Disease was firstly reported in 1906 by a clinical psychiatrist and neuroanatomist named Alois Alzheimer, who described a 50 year-old woman whom he had followed from her admission for paranoia, progressive sleep and memory disturbance, aggression, and confusion, until her death 5 years later. His report noted distinctive protein plaques and neurofibrillary tangles in the brain post-mortem histology <ref type="bibr" target="#b53">[Hippius, 2003]</ref>,</p><p>Nowadays, a definitive diagnosis of AD can only be established through postmortem brain tissue biopsies, aiming at finding plaques of amyloid proteins in the extracellular space and tangles of tau proteins in the intracellular space. However, clinical interest currently focuses on identifying persons with cognitive impairment who will probably progress to dementia, rather than on identifying the exact underlying pathology, and on the research for pharmacological and non-pharmacological interventions to slow down the degenerative process. Indeed, as researchers are developing an increasing awareness of the complexity of AD and related disorders, the clinical evaluation of progression to dementia through the integration of clinical, imaging, and biological biomarkers is considered as a key step towards the accurate definition of the pathology <ref type="bibr" target="#b18">[Boccardi, 2021]</ref>.</p><p>One of the major challenges for understanding AD is that the pathology evolves unnoticed for a long period (up to 20 years) before the manifestation of clinically recognizable cognitive <ref type="bibr" target="#b39">[Frisoni, 2003;</ref><ref type="bibr" target="#b104">Solomon, 2011]</ref> and behavioral symptoms <ref type="bibr" target="#b99">[Scarmeas, 2007;</ref><ref type="bibr" target="#b36">Fostinelli, 2020]</ref>. Clinicians refer to this stage as to the pre-clinical phase of AD. Therefore, efforts have focused on finding a set of biomarkers that would allow an early detection and follow-up monitoring of the AD hallmarks along the disease progression. In 2011, the National Institute on Aging and Alzheimer's Association (NIA-AA) created separate diagnostic recommendations for the preclinical, mild cognitive impairment, and dementia stages of AD <ref type="bibr" target="#b83">[McKhann, 2011]</ref>. Scientific progress in the interim led to an initiative by the NIA-AA to update and unify the 2011 guidelines. These efforts resulted in the definition of the A/T/N Research Framework, in which the acronym comes for the three main biomarkers categories involved in AD, namely: amyloid, tau and neurodegeneration <ref type="bibr" target="#b58">[Jack, 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Biomarkers</head><p>From what we reported in the previous section, it follows that Alzheimer's Disease can be tracked via biomarkers, accordingly with the A/T/N categories in the Research Framework, which indicate the abnormality of specific physiological processes. For example, measurements of concentration of Aβ 42 and tau proteins in the cerebrospinal fluid (CSF) allow to detect respectively levels of amyloid-beta and tau. The A/T/N criteria define a clear role for tau and amyloid biomarkers in the diagnostic procedure of patients complaining about cognition. In particular, tau-positiveness is necessary but not sufficient to define clinical AD, and tau-positiveness associated to amyloidnegativity denotes the presence of a neurodegenerative disorder belonging to a non-AD continuum. Imaging techniques, such as Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET), are also suited to measure many pathophysiological changes involved in AD. For instance, abnormal deposition of amyloid proteins can be also measured via PET with the 18 F-Florbetapir (AV45) radioactive tracer <ref type="bibr" target="#b28">[Clark, 2011]</ref>. Accumulation of neurofibrillary tangles is quantified via PET with the 18 F-Flortaucipir (AV1451) tracer <ref type="bibr" target="#b13">[Barthel, 2020]</ref>. Finally, neurodegeneration is indicated by cerebral atrophy from MRI scans <ref type="bibr" target="#b38">[Fox, 2004]</ref> and glucose hypo-metabolism from 18 F-Fluorodeoxyglucose-PET <ref type="bibr" target="#b51">[Herholz, 2012;</ref><ref type="bibr" target="#b42">Garibotto, 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Integrating biomarkers</head><p>Although most of the existing researches in dementia focus on only a single modality of biomarkers, there is a general agreement that combining biomarkers improves diagnostic accuracy <ref type="bibr" target="#b26">[Chételat, 2021]</ref>. Fostered by many research initiatives aiming at collecting and sharing data for a better understanding of Alzheimer's Disease and other forms of dementia, the literature on this subject is growing. For example, in <ref type="bibr" target="#b101">[Shaffer, 2013]</ref> the authors, after studying subjects with Mild Cognitive Impairment, found that combining MRI, FDG-PET, and CSF data with routine clinical tests significantly increased the accuracy of predicting conversion to AD compared with clinical testing alone. Specifically, MRI-derived gray matter probability maps and FDG-PET images were analyzed by using Independent Component Analysis (ICA), a data-driven method to extract independent sources of information from whole-brain data. The ICA loading parameters for all MRI and FDG components, along with CSF proteins, were entered into logistic regression models. A variety of models were considered, including all combinations of MRI, PET, and CSF biomarkers with the age, education years, Apolipoprotein-E (ApoE), Alzheimer's Disease Assessment Scale -Cognitive Subscale (ADAS-Cog) as covariates. Similar results were confirmed in <ref type="bibr" target="#b47">[Gupta, 2019]</ref>, where authors proposed a machine learning-based framework to discriminate subjects with AD or MCI utilizing a combination of four different biomarkers: FDG-PET, MRI, CSF protein levels, and ApoE genotype. Here, a kernel-based multi-class support vector machine (SVM) classifier with a grid-search method was applied to optimally select features from the input biomarkers.</p><p>Over the last 20 years, governments, universities, charities and pharmaceutical companies have devoted increasingly significant resources, in terms of funding, time, and effort, to foster knowledge advancements. For example, neuroimaging studies in dementia, such as the Alzheimer's Disease Neuroimaging Initiative (ADNI) <ref type="bibr" target="#b122">[Weiner, 2013]</ref>, the Open Access Series of Imaging Studies (OASIS) <ref type="bibr" target="#b73">[LaMontagne, 2019]</ref>, the Minimal Interval Resonance Imaging in Alzheimer's Disease (MIRIAD) <ref type="bibr" target="#b82">[Malone, 2013]</ref>, have produced huge amounts of heterogeneous, multi-modal, and high-dimensional data, including those coming from MRI and PET Imaging. All these data were collected from subjects with different cognitive conditions, with the aim to find a set of biomarkers that would allow to detect and monitor patho-physiological stages along the disease path.</p><p>Different biomarkers may be combined to provide better insights <ref type="bibr" target="#b10">[Apostolova, 2010]</ref>. Hence, the joint analysis of biomedical data in Dementia studies is important for better clinical diagnosis and to understand the relationship between biomarkers. However, jointly accounting for heterogeneous measures poses important challenges related to the modeling of heterogeneity and to the interpretability of the results. Moreover, when pooling together observations from different studies in order to take advantage of the increased variability and sample size, the joint analysis requires to consistently analyze high-dimensional and heterogeneous information in presence of often non-overlapping acquisition and data processing protocols, with missing data across data samples. The next section addresses these analysis challenges by describing current approaches to multi-modal data modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Current approaches and open challenges</head><p>As described in the previous section, as tackling a complex problem like AD requires to establish links between high-dimensional heterogeneous data sources, a variety of approaches have been proposed in the literature. As this subject will be widely discussed in the introductory sections of Chapters 2 and 3, here we describe the general aspects of current approaches to link data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multivariate methods</head><p>Probably the simplest approach is the massive univariate correlation analysis <ref type="bibr" target="#b89">[Nathoo, 2019]</ref>. Unfortunately this approach is too limited in modeling power, and prone to false positives when the data dimension is high. To overcome the limitations of massunivariate analysis, more advanced methods, such as Canonical Correlation Analysis (CCA), Independent Component Analysis (ICA), Partial Least Squares (PLS), and Reduced Rank Regression (RRR), have successfully been applied in biomedical research (see <ref type="bibr" target="#b77">[Liu, 2014]</ref> for a comprehensive review), along with multi-sources <ref type="bibr" target="#b62">[Kettenring, 1971;</ref><ref type="bibr" target="#b81">Luo, 2015]</ref> and non-linear variants <ref type="bibr" target="#b55">[Huang, 2009;</ref><ref type="bibr">Andrew, 2013a]</ref>. The common formulation of these approaches consists in projecting the observations in a latent low dimensional space where desired statistical properties are enforced, such as maximum correlation (CCA), maximum covariance (PLS), or minimum regression error (RRR). However, since they are not generative, these methods are limited in providing information on how this latent representation is expressed in the observations <ref type="bibr" target="#b49">[Haufe, 2014]</ref>. Generative modeling attempts have been made, such as with the Bayesian-CCA <ref type="bibr" target="#b71">[Klami, 2013]</ref>, where a transformation of a latent variable captures the shared variation between two data sources. Unfortunately, due to scalability issues in the computation of posterior distributions, all the practical applications are limited to model data with very few dimensions. Variational Inference (VI) is a popular approach to compute posterior distributions when the usual integrations are intractable. Variational Inference (VI) has been successfully applied in the recent seminal work on the Variational Autoencoder (VAE) <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr" target="#b96">Rezende, 2014]</ref>, a powerful generative model for high-dimensional single-modality observations. The work developed in this Thesis is largely inspired by the VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Variational Autoencoder</head><p>The VAE is composed by two main elements: the encoder and the decoder. The encoder can be seen as a Bayesian version of the Principal Component Analysis (PCA) <ref type="bibr" target="#b97">[Rolinek, 2019]</ref>, that is it transforms input data, usually high dimensional, to produce a compressed and informative version of the data distribution: for each high-dimensional observation we can associate an expected value and a probability interval into a lower dimensional latent space that captures the main variability in the original data. The decoder works in the opposite sense of the encoder: it is a generator function that, given a compressed low dimensional representation of a data point, produces a distribution maximizing the original data likelihood. The training of a VAE aims at finding the optimal encoder and decoder pair associated with the maximum of information of the original highdimensional data when encoding, with the minimum reconstruction error when decoding.</p><p>To prevent overfitting, the encoding distributions are regularized during training, to ensure that the encoded latent space preserves smoothness properties allowing us to generate new data. The regularization is achieved by minimizing an information theoretic measure, the Kullback-Leibler divergence function, between the encoding distributions and the associate prior, usually chosen to be the factorized isotropic multivariate Gaussian. This is the variational inference of statistics. The term variational comes from the calculus of variation that originally was used to estimate distributions instead of variables.</p><p>Since its first introduction in 2014, research involving the VAE increased steadily, and many research groups are currently involved in improving the performances and capabilities of the VAE. For example, in <ref type="bibr" target="#b98">[Rossi, 2019]</ref>, the authors show that, given the usually high number of parameters required to fit a VAE based model, initialization plays a huge role in their convergence to a good solution, and propose a method to prevent the problem of posterior collapse <ref type="bibr" target="#b81">[Lucas, 2019]</ref>, that is to avoid the trivial solutions of encoding distributions being equal to the prior. Another area of research aims at obtaining more informative encoding distributions. Indeed, the factorized Gaussian usually adopted as regularizer is not enough to guarantee the factorization of the associated encoding distributions. This constitutes a problem as a desirable property of the latent space is to have each dimension disentangled from the others. This is still a very active area of research, with solutions proposed in the context of supervised learning <ref type="bibr" target="#b79">[Lopez-Martin, 2017]</ref>, unsupervised learning <ref type="bibr" target="#b52">[Higgins, 2018]</ref>, and semi-supervised learning <ref type="bibr" target="#b87">[Mita, 2020]</ref>.</p><p>With the VAE is possible to model multi-modal data by stacking all the modalities into a single one. This represents a limit, as modeling stacked data through a VAE may pose interpretability issues. Indeed, it would be generally difficult to disentangle the contribution of a single modality in the description of the latent representation, especially with non-linear encoder and decoder architectures.</p><p>These limitations are crucial when applying VAE, and more general machine learning models, to clinical data. To address this issue, in this Thesis we focused on the extension of VAE approaches to model multi-modal data in a more interpretable manner, by introducing independent encoders and decoders which are jointly linked in the latent space in an information theoretical sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data missingness</head><p>Data scarcity is a critical issue when modeling observations in dementia studies, as some data modalities are costly to obtain and not always available. This is the case, for example, of PET images with radioactive tracers for Aβ 42 and Tau proteins, which are known to play an important role in AD <ref type="bibr" target="#b58">[Jack, 2018]</ref>. As fitting multi-modal models requires to establish correspondences between modalities, subjects with at least one missing modality are generally discarded, yielding to potentially severe loss of available information. An appropriate approach to increase the sample size and take advantage of all the available data, is to gather observations from different studies, although this approach does not solve the problem of missing data. Indeed, according to the cohort study design, there may be views which are specifically absent for a given dataset (i.e.,missing not at random). This potential mismatch across datasets hampers their interoperability, and prevents the gathering of all the available observations into a single, robust and generalizable joint model accounting for the global data variability. This challenge is typically addressed in machine learning in the field of Multi-Task Learning (MTL), where each dataset is associated to a specific modeling task. MTL is usually achieved with specific output layers for every task, and by including a shared latent representation for all of them <ref type="bibr" target="#b31">[DoradoMoreno, 2020]</ref>. It has been successfully applied in classification <ref type="bibr" target="#b27">[Choi, 2019;</ref><ref type="bibr">Zhou, 2019a]</ref> and in feature prediction problems <ref type="bibr" target="#b46">[Gondara, 2018;</ref><ref type="bibr" target="#b120">Wei, 2020]</ref>.</p><p>In this Thesis we develop a generative and probabilistic statistical learning model for the joint analysis of high-dimensional heterogeneous biomedical data, to simultaneously learn from multiple datasets, even in the presence of non fully compatible datasets, and missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Beyond dementia studies</head><p>The problem of coherently modeling heterogeneous data sources is common to many application fields, well beyond the study of Alzheimer's Disease and neurological disorders.</p><p>In oncology studies, for example, the identification of cancer subtypes <ref type="bibr" target="#b63">[Khan, 2020]</ref> plays an important role in revealing useful insights into disease pathogenesis <ref type="bibr">[Li, 2020]</ref> and advancing personalized therapy <ref type="bibr" target="#b111">[Valieris, 2020]</ref>. Although approaches have been proposed to identify cancer subtypes from multiple genomic data sources <ref type="bibr" target="#b127">[Zhang, 2012]</ref>, very few of them are particularly designed to exploit cross-modality correlations. In <ref type="bibr" target="#b84">[Méndez, 2015]</ref>, authors proposed a multi-view consensus clustering methodology for the integration of multimodal MRI images into a unified segmentation framework, aiming at heterogeneity assessment in tumoral lesions. In that work, the modalities adopted for tissue characterization are Dynamic Contrast Enhanced MRI (DCE-MRI), that uses serial acquisition of images during and after the intravenous injection of a contrast agent to assess organ perfusion, and Diffusion Tensor Imaging (DTI), sensitive to the tissue microstructure. The task is particularly challenging as the DCE-MRI is a 4-dimensional acquisition modality of space and time, while the result of a Diffusion Tensor Imaging (DTI) acquisition consists is a second-order tensor for every voxel. Given this important level of data complexity, the advent on novel methods for the joint modeling of highdimensional multi-modal observations are likely to produce further advancements in the oncology research field.</p><p>Another example encompasses the new Information and Communication Technologies (ICT), as they are starting to have a role in the monitoring and behavioral assessment of frail people <ref type="bibr" target="#b72">[König, 2015;</ref><ref type="bibr" target="#b82">Manera, 2020]</ref>. Serious Games (SG), for example, are digital applications specially adapted for purposes other than entertaining; such as rehabilitation, training and education <ref type="bibr">[Robert, 2014]</ref>. As they are likely to produce new forms of data that could be integrated with the classic instruments to better assess the disease severity and progression, multi-modal methods for the joint modeling of heterogeneous data, such as the ones developed in this Thesis, could play an important role in this field, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Objectives and organization of this Thesis</head><p>In this Thesis we develop a general framework to solve the problem of jointly modeling heterogeneous data in the presence of missing observations for both prediction and classification tasks.</p><p>We benchmarked our framework in synthetically generated scenario to assess its general properties, and on real data coming from neuroimaging research studies in Dementia.</p><p>Throughout this work we will adopt the words channels, views, and modalities interchangeably, to refer to an homogeneous set of quantitative measurements.</p><p>The manuscript is organized as follows.</p><p>We present in Chapter 2 the Multi-Channel Variational Autoencoder (MCVAE), an extension of the VAE to jointly model latent relationships across multiple channels, consisting in groups of heterogeneous observations generated from a single source of information.</p><p>In the latent space, this is achieved by constraining the variational distribution of each channel to a common target prior. Moreover, we show how sparse and parsimonious latent representations can be enforced by variational dropout. Experiments on synthetic data show that our model correctly identifies the prescribed latent dimensions and data relationships across multiple testing scenarios. When applied to imaging and clinical data, our method allows to identify the joint effect of age and pathology in describing clinical condition in a large scale clinical cohort.</p><p>We introduce in Chapter 3 the Multi-Task Multi-Channel Variational Autoencoder (MT-MCVAE), an extension of the MCVAE to modeling multi-task (that is multi-dataset) multi-channel observations, where the non-trivial problem of dealing with missing data arises. This problem has been addressed in our current work via a specific optimization scheme requiring an extension of our previous formulation to account for dataset-and channel-specific observations. Simulations on synthetic data show that our method is able to identify a common latent representation of multi-channel datasets, even when the compatibility across datasets is minimal. When jointly analyzing multi-modal neuroimaging and clinical data from real independent dementia studies, the MT-MCVAE is able to mitigate the absence of modalities without having to discard any available information. Moreover, by slightly changing the architecture of the MT-MCVAE, the inferred latent representation can be used to define robust classifiers gathering the combined information across different datasets.</p><p>Chapter 4 and Chapter 5 are of different nature with respect to the previous ones.</p><p>Here, we do not propose new methodologies: we introduce, instead, important work of practical utility for the methods developed in the earlier chapters.</p><p>Specifically, in Chapter 4 we benchmark existing harmonization methodologies for correcting the bias induced by the data domain. Indeed, when integrating data across different studies and datasets to increase the sample size, such as with the MT-MCVAE, the bias induced by the domain shift, that is the existence of different protocols between studies, multiple imaging machine manufacturers, image reconstruction software, and preprocessing algorithms, creates barriers to the integration of multi-centric datasets.</p><p>In Chapter 5 we present the open-source Python package mcvae, where we publicly released the source code of the methods presented in Ch. 2 and Ch. 3 of this thesis along with the necessary documentation, to foster and promote research in joint modeling of heterogeneous data in other domains.</p><p>Finally, we conclude the manuscript with Chapter 6 by summarizing the main contributions of this work. We also present potential extensions built upon the acknowledged limitations to propose future research perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Publications</head><p>The contributions of this manuscript led to the following publications in conferences and peer-reviewed journals.  In this chapter we present the Multi-Channel Variational Autoencoder (MCVAE), a latent variable framework to jointly model complex heterogeneous observations. Here we call channel a group of homogeneous observations. We argue that our framework can be of interest for the neuroimaging community as it can be adopted to model the joint relationship between multi-modal neuroimaging data, such as those coming from Alzheimer's Disease Neuroimaging Initiative (ADNI). Indeed, in this context of high heterogeneity due to the presence of, among many others, Magnetic Resonance Imaging (MRI) data and Positron Emission Tomography (PET) imaging data, that is channels with their own informative content, there is a rational need for methods to establish relationships between observations. To do so, we postulate a single source of information for all the channels, and we use Variational Inference (VI) to infer this single source from them. This is achieved in the latent space by constraining the variational distribution of each channel to a common target prior. Moreover, we show how sparse and parsimonious latent representations can be enforced by variational dropout. This chapter is published in the Proceedings of Machine Learning Research <ref type="bibr" target="#b8">[Antelmi, 2019]</ref> and is based on a previous works presented at the fisrt Workshop on Machine Learning in Clinical Neuroimaging <ref type="bibr">[Antelmi, 2018a]</ref>, and at the 12 th EPICLIN Conference <ref type="bibr">[Antelmi, 2018b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Interpretable modeling of heterogeneous data channels is essential in medical applications, for example when jointly analyzing clinical scores and medical images. Variational Autoencoders VAE are powerful generative models that learn representations of complex data. The flexibility of VAE may come at the expense of lack of interpretability in describing the joint relationship between heterogeneous data. To tackle this problem, in this work we extend the variational framework of VAE to bring parsimony and interpretability when jointly account for latent relationships across multiple channels. In the latent space, this is achieved by constraining the variational distribution of each channel to a common target prior.</p><p>Parsimonious latent representations are enforced by variational dropout. Experiments on synthetic data show that our model correctly identifies the prescribed latent dimensions and data relationships across multiple testing scenarios. When applied to imaging and clinical data, our method allows to identify the joint effect of age and pathology in describing clinical condition in a large scale clinical cohort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction</head><p>Understanding the relationship among heterogeneous data is essential in medical applications, where performing a diagnosis, or understanding the dynamics of a pathology require to jointly analyze multiple data channels, such as demographic data, medical imaging data, and psychological tests.</p><p>Multivariate methods to jointly analyze heterogeneous data, such as Partial Least Squares (PLS), Reduced Rank Regression (RRR), or Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b54">[Hotelling, 1936]</ref> have successfully been applied in biomedical research <ref type="bibr" target="#b77">[Liu, 2014]</ref>, along with multi-channel <ref type="bibr" target="#b62">[Kettenring, 1971;</ref><ref type="bibr" target="#b81">Luo, 2015]</ref> and non-linear variants <ref type="bibr" target="#b55">[Huang, 2009;</ref><ref type="bibr">Andrew, 2013a]</ref>. These approaches are classified as recognition methods, as their common formulation consists in projecting the observations in a latent low dimensional space in which desired characteristics are enforced, such as maximum correlation (CCA), maximum covariance (PLS), or minimum regression error (RRR) <ref type="bibr" target="#b49">[Haufe, 2014]</ref>.</p><p>In their classical formulation these models are not generative as they do not explicitly provide a mean to sample observations when the distribution of latent variables and parameters is known. Bayesian-CCA <ref type="bibr" target="#b71">[Klami, 2013]</ref> actually goes in this direction: it is a generative formulation of CCA, where a transformation of a latent variable captures the shared variation between data channels. A limitation of this method for the application in real data scenarios is scalability, as inference on the posterior distribution results in O(D 3 ) complexity, being D the dimensionality of the data. Consequently, all the practical applications of Bayesian CCA in the earlier works were limited to very few dimensions and channels <ref type="bibr" target="#b70">[Klami, 2007]</ref>.</p><p>Variational Autoencoder (VAE) <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr" target="#b96">Rezende, 2014]</ref> are models that couple a recognition function, or encoder, to infer a lower dimensional representation of the data, with a generative function, or decoder, which transforms the latent representation back to the original observation space. The VAE is a Bayesian model: the latent variables are inferred by estimating the associated posterior distributions. Inference is efficiently performed through amortized inference <ref type="bibr" target="#b66">[Kim, 2018]</ref> by parametrizing the posterior moments with neural networks. The networks are optimized to maximize the associated Evidence Lower Bound (ELBO). VAEs are flexible and can account for any kind of data. Within this setting, the joint analysis of heterogeneous channels can be performed through concatenation of the different data sources. However, modeling concatenated multi-channel data through a VAE may pose interpretability issues, as it is difficult to disentangle the contribution of a single channel in the description of the latent representation. Moreover, at test time, the model can usually be applied only to data presenting all the channels information.</p><p>To tackle this problem, in this work we generalize the VAE by assuming that in a multichannel scenario the latent representation associated to each channel must match a common target distribution This is done by imposing a constraint on the latent representations in an information theoretical sense, where each latent representation is enforced to match a common target prior. We will show that this constraint can be optimized within a variational optimization framework, allowing efficient inference of channel encodings and latent representation.</p><p>Another limitation of the VAE concerns the interpretability of the latent space. In particular, we generally lack of a theoretical justification for the choice of the latent space dimension. This is a key parameter that can profoundly impact the interpretability of the estimated data representation. The optimization of the latent dimension through crossvalidation may also pose generalization problems, especially when the data is scarce.</p><p>To tackle this issue, in this work we investigate a principled theoretical framework for imposing parsimonious representations of the latent space through sparsity constraints.</p><p>We argue that this kind of model may lead not only to improved interpretability, but also to optimal data representation. Indeed, it is known that VAEs suffer from the problem of over-pruning: the variational approximation leads to overly simplified representations, resulting in high model bias due to the impossibility to learn latent distribution different from the prior <ref type="bibr" target="#b20">[Burda, 2015;</ref><ref type="bibr" target="#b2">Alemi, 2017]</ref>. As discussed in <ref type="bibr" target="#b126">[Yeung, 2017]</ref>, over-pruning is a recurrent phenomenon ultimately leading to excessive regularization, even in cases when the model underfits the data. The authors tackle over-pruning with the introduction of a categorical sampler on the latent space dimensions. Another way to tackle overpruning is to enforce sparsity on the latent space. Recently <ref type="bibr" target="#b69">[Kingma, 2015;</ref><ref type="bibr">Molchanov, 2017]</ref> showed that dropout, a technique that regularize neural networks, can be naturally embedded in VAE to lead to a sparse representation of the variational parameters.</p><p>In our work, we leverage on these recent results to enforce sparsity on the proposed multi-channel VAE. In the variational formulation, the dropout parameters are not hyperparameters anymore, and can be directly learned through the optimization of the variational constraint. Code developed in Pytorch <ref type="bibr" target="#b91">[Paszke, 2017]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Method</head><p>We first describe the proposed Multi-Channel Variational Autoencoder ( §2.2.1). In §2.2.2 we present the sparse formulation of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Multi-Channel Variational Autoencoder (MCVAE)</head><p>Let x = {x 1 , . . . , x C } be an observation set of C channels, where each x c is a ddimensional vector. Also, let z denote the l-dimensional latent variable commonly shared by each x c . We assume the following generative process for the observation set:</p><formula xml:id="formula_0">z ∼ p (z) , x c ∼ p (x c |z, θ c ) , for c in 1 . . . C, (2.1)</formula><p>where p (z) is a prior distribution for the latent variable and p (x c |z, θ c ) is a likelihood distribution for the observations conditioned on the latent variable. We assume that the likelihood functions belong to a distribution family P parametrized by the set of parameters θ = {θ 1 , . . . , θ C }.</p><p>In the scenario depicted so far, solving the inference problem allows the discovery of the common latent space from which the observed data in each channel is generated. The solution of the inference problem is given by deriving the posterior p (z|x, θ), that is not always computable analytically. In this case, Variational Inference (VI) can be applied to compute an approximate posterior <ref type="bibr" target="#b14">[Blei, 2016]</ref>.</p><p>Our working hypothesis is that every channel brings by itself some information about the latent variable distribution. As such, it makes sense to approximate the posterior distribution with q (z|x c , φ c ), by conditioning it on the single channel x c and on its variational parameters φ c . Since each channel provides a different approximation, we can impose a constraint enforcing each q (z|x c , φ c ) to be as close as possible to the target posterior distribution. Being the mismatch measured in terms of Kullback-Leibler (D KL ) divergence, we specify this constraint as:</p><formula xml:id="formula_1">arg min q∈Q E c [D KL (q (z|x c , φ c ) ||p (z|x 1 , . . . , x C , θ))] , (2.2)</formula><p>where the approximate posteriors q (z|x c , φ c ) belong to a distribution family Q parametrized by the set of parameters φ = {φ 1 , . . . , φ C }, and represent the view on the latent space that can be inferred from each channel x c . The quantity E c is the average over channels computed empirically. Practically, solving the objective in Eq. ( <ref type="formula" target="#formula_2">2</ref>.2) allows to minimize the discrepancy between the variational approximations and the target posterior. In §2.2.1 we show that the optimization (2.2) is equivalent to the optimization of the following evidence lower bound L (D):</p><formula xml:id="formula_2">L (D) = E c [L c -D KL (q (z|x c , φ c ) ||p (z))] ,<label>(2.3)</label></formula><p>where</p><formula xml:id="formula_3">L c = E q(z|xc,φ c ) C i=1 ln p (x i |z, θ i )</formula><p>is the expected log-likelihood of decoding each channel from the latent representation of the channel x c only. This formulation is valid for any distribution family P and Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivation of the Evidence Lower Bound</head><p>In the following derivation we omit the variational and generative parameters φ and θ to leave the notation uncluttered.</p><p>The formula in <ref type="bibr">(2.2)</ref> states that variational inference is carried out by introducing a set of probability density functions q (z|x c ), belonging to a distribution family Q, that are as close as possible to the target posterior over the latent variable p (z|x = {x 1 , . . . , x C }). Given the intractability of p (z|x) for most complex models, we cannot solve directly this optimization problem. We look then for an equivalent problem, by rearranging the objective:</p><formula xml:id="formula_4">E c [D KL (q (z|x c ) ||p (z|x))] = = E c z q (z|x c ) ln q (z|x c ) -ln p (z|x) dz = E c z q (z|x c ) ln q (z|x c ) -ln p (x|z) -ln p (z) + ln p (x) dz = ln p (x) + E c D KL (q (z|x c ) ||p (z)) -E q(z|xc) [ln p (x|z)] ,</formula><p>where we factorize the true posterior p (z|x) using Bayes' theorem. We can reorganize the terms, such that:</p><formula xml:id="formula_5">ln p (x) -E c [D KL (q (z|x c ) ||p (z|x))] ≥0 = = E c E q(z|xc) [ln p (x|z)] -D KL (q (z|x c ) ||p (z)) lower bound L .</formula><p>(2.4)</p><p>Since the D KL term on the left hand side is always non-negative, the right hand side is a lower bound of the log evidence. Thus, by maximizing the lower bound we also maximize the data log evidence while solving the minimization problem in (2.2).</p><p>We note that the lower bound <ref type="bibr">(2.4</ref>) is composed by a regularization term and a data matching term. The D KL term minimizing the mismatch between the approximate distribution and the target prior acts as a regularizer. The inner expectation term favors the approximate posterior that maximizes the data log-likelihood.</p><p>The hypothesis that every channel is conditionally independent from all the others given z allows to factorize the data likelihood as p (x|z) = C i=1 p (x i |z), so that the lower bound becomes:</p><formula xml:id="formula_6">L = E c [L c -D KL (q (z|x c ) ||p (z))]</formula><p>where</p><formula xml:id="formula_7">L c = E q(z|xc) C i=1 ln p (x i |z) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with VAE</head><p>Our model extends the VAE: the novelty is in the log-likelihood terms L c in Eq. ( <ref type="formula" target="#formula_2">2</ref>.3), representing the reconstruction of the multi-channel data from a single channel only. In case C = 1 the model collapses to a VAE. In the case C &gt; 1, the L c terms considered altogether force each channel to the joint decoding of itself and every other channel at the same time. This characteristic allows to reconstruct missing channels {x i } from the available ones {x j } as:</p><formula xml:id="formula_8">xi = E j E q(z|x j ) [p (x i |z)] . (2.5)</formula><p>An application of Eq. (2.5) is provided in §2.3.4. Our model is different from a VAE where all the channels are concatenated into a single one. In that case there cannot be missing channels if we want to infer the latent space variables, unless recurring to costly data imputation techniques (cf. App. F in <ref type="bibr" target="#b96">[Rezende, 2014]</ref>). Our model is also different from a stack of C independent VAEs, in which the C latent spaces are no more related to each-other. The dependence between encoding and decoding across channels stems from the joint approximation of the posterior distribution (Formula (2.2)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian linear case</head><p>Model (2.1) is completely general and can account for complex non-linear relationships modeled, for example, through deep neural networks. However, for simplicity of interpretation, in what follows we focus our multi-channel variational framework to the Gaussian Linear Model. This is a special case, analogous to Bayesian-CCA <ref type="bibr" target="#b71">[Klami, 2013]</ref>, where the members of the variational family Q and generative family P are Gaussian parametrized by linear transformations. We define the members of the families Q and P as:</p><formula xml:id="formula_9">q (z|x c , φ c ) = N z|V (µ) c x c , diag(V (σ) c x c ) , (2.6) p (x c |z, θ c ) = N x c |G (µ) c z, diag(g (σ) c ) , (2.7)</formula><p>i.e. factorized multivariate Gaussian distributions whose moments are linear transformations depending on the conditioning variables.</p><formula xml:id="formula_10">θ c = {G (µ) c , g<label>(σ)</label></formula><p>c } and</p><formula xml:id="formula_11">φ c = {V (µ) c , V<label>(σ)</label></formula><p>c } are the parameters to be optimized by maximizing the lower bound in <ref type="bibr">(2.3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization of the lower bound</head><p>The optimization starts with a random initialization of the parameters θ = {θ 1 , . . . , θ C } and φ = {φ 1 , . . . , φ C }. The expectations L c in the Eq. ( <ref type="formula" target="#formula_2">2</ref>.3) can be computed by sampling from the variational distributions q (z|x c , φ c ) and, when the prior p (z) = N (0; I) , the D KL term in Eq. ( <ref type="formula" target="#formula_2">2</ref>.3) can be computed analytically (cf. <ref type="bibr">[Kingma, 2014b]</ref>, appendix 2.A). The maximization of L (D) with respect to θ and φ is efficiently carried out through minibatch stochastic gradient descent implemented with the backpropagation algorithm. With Adam <ref type="bibr">[Kingma, 2014a]</ref> we compute adaptive learning rates for the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Inducing Sparse Latent Representations</head><p>In extensive synthetic experiments with the non-sparse version of the multi-channel model, we found that the lower bound <ref type="bibr">(2.</ref>3) generally reaches the maximum value at convergence when the number of fitted latent dimensions coincide with the true one used to generate the data (Sup. Mat.). This procedure provides an heuristic for selecting the latent variable dimensions, and proved to work well in controlled scenarios. However, according to our experience, it fails in most complex cases (Sup. Mat.), and is time consuming. Moreover, our trust in the result depends on the tightness between the model evidence and its lower bound: a factor that is not easy to control. To address this issue, we propose here to automatically infer the latent variable dimensions via a sparsity constraint on z. Having a sparse z as a direct result of one single optimization would be computationally advantageous and it would ease the interpretability of the observation model in (2.1), as the number of relationships to take into account decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization via Dropout</head><p>Dropout <ref type="bibr" target="#b105">[Srivastava, 2014]</ref> and DropConnect <ref type="bibr" target="#b116">[Wan, 2013]</ref> are techniques for regularizing neural networks. The basic block of a neural network is the fully connected layer, composed by a linear transformation of an input vector z into an output vector x, and a non linearity applied to the components of x. Given a generic linear transformation x = Gz, with z and x column vectors, regularization techniques are based on the multiplication of either z (dropout) or G (dropconnect) element-wise by independent Bernoulli random variables. The components of x are hence computed as:</p><formula xml:id="formula_12">x i = k g ik (ξ k z k ),<label>(dropout)</label></formula><p>(2.8)</p><formula xml:id="formula_13">x i = k (ξ ik g ik )z k , (dropconnect) (2.9)</formula><p>where ξ k , ξ ik ∼ B(1 -p) with hyperparameter p known as drop rate. The elements x i are approximately Gaussian for the Lyapunov's central limit theorem <ref type="bibr" target="#b117">[Wang, 2013]</ref>, and their distributions takes the form: <ref type="bibr">(2.10)</ref> where α = p /1-p and θ ik = g ik z k (1 -p). In Gaussian dropout <ref type="bibr" target="#b117">[Wang, 2013]</ref> the regularization is achieved by sampling directly from (2.10).</p><formula xml:id="formula_14">x i ∼ N k θ ik ; α k θ 2 ik ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Dropout and Sparsity</head><p>In the context of the Variational Autoencoder (VAE), posterior distributions on the encoder weights w that take the form w ∼ N µ; αµ 2 are called dropout posteriors <ref type="bibr" target="#b69">[Kingma, 2015]</ref>. The authors of <ref type="bibr" target="#b69">[Kingma, 2015]</ref> show that if the variational posteriors on the encoder weights are dropout posteriors, Gaussian dropout arises from the application of the local reparameterization trick, a method introduced to increase the stability of gradients estimation in training. The only prior on w consistent with the optimization of the lower bound is the improper log-scale uniform: <ref type="bibr">.11)</ref> With this prior, the D KL of the dropout posterior depends only on α and can be numerically approximated. In <ref type="bibr">[Molchanov, 2017]</ref> the authors provide an approximation of D KL , reported in (2.12), to allow this parameter to be learned through the optimization of the lower bound via gradient-based methods:</p><formula xml:id="formula_15">p (ln |w|) = const ⇔ p (|w|) ∝ 1 |w| . (<label>2</label></formula><formula xml:id="formula_16">D KL N w; αw 2 ||p (w) ≈ ≈ -k 1 σ(k 2 + k 3 ln α) + 0.5 ln(1 + α -1 ) + k 1 (2.12) k 1 = 0.63576 k 2 = 1.87320 k 3 = 1.48695 σ(•) Sigmoid function.</formula><p>While the optimization of D KL promotes α → ∞, the implicit drop rate p tends to 1, meaning that the associated weight w can be discarded. Sparsity arises naturally: large values of w correspond to even larger uncertainty αw 2 because of the quadratic relationship and the tendency of the optimization objective to favors α → ∞; therefore, unless that weight is beneficial for the optimization objective, that is to maximize the data log-likelihood, it will be set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Multi-Channel VAE</head><p>Compatibly with standard dropout methods, in our Multi-Channel VAE we define a variational approximation of the latent code z. We note that the local reparameterization trick cannot be straightforwardly applied, since its standard formulation would require to transfer the uncertainty to a lower dimensional variable, such as from G to x in §2.2.2. We notice however that by choosing a dropout posterior for the elements of z,</p><formula xml:id="formula_17">that is if z k ∼ N µ k ; αµ 2</formula><p>k , the output of the first layer with weights g ik of the decoding transformation, before the non-linearity is applied, follows a Gaussian distribution: <ref type="bibr">(2.13)</ref> in which the first two moments are as follows:</p><formula xml:id="formula_18">x i ∼ N k g ik µ k ; α k g 2 ik µ 2 k ,</formula><formula xml:id="formula_19">E [x i ] = E [ k g ik z k ] = k g ik µ k , (2.14) Var [x i ] = Var [ k g ik z k ] = k Var [g ik z k ] + k, j =k Cov [(g ik z k , g ij z j )] = k g 2 ik αµ 2 k = α k g 2 ik µ 2 k , (2.15)</formula><p>with the covariance terms vanishing for the hypothesis of independent elements of z. The analogy with (2.10) holds when θ ik = g ik µ k , and so we can establish a connection with the standard dropout techniques. Specifically, imposing a dropout posterior for the latent code z is analogous to perform dropout on the latent code itself, and dropconnect on the decoder weights. We therefore define the approximate posteriors q (z|x c , φ c ) in Eq. ( <ref type="formula" target="#formula_2">2</ref>.3) and parametrize them to be factorized dropout posteriors, that is, for c in 1 . . . C:</p><formula xml:id="formula_20">q (z|x c , φ c ) = N µ c ; diag( √ α µ c ) 2 , (<label>2.16)</label></formula><p>with µ c = φ c x c , where parameters φ = {α, φ 1 , . . . , φ C } include φ c linear transformations, specific to channel c, while α is shared among all the channels. Following the considerations of <ref type="bibr" target="#b69">[Kingma, 2015]</ref>, the prior distribution p (z) is chosen to be fully factorized by scale-invariant log-uniform priors:</p><formula xml:id="formula_21">p (z) = p (|z i |) , such that p (ln |z i |) ∝ const.</formula><p>(2.17)</p><p>Because of these choices, the D KL term in Eq. ( <ref type="formula" target="#formula_2">2</ref>.3) can be easily computed by leveraging on Eq. (2.12). For the same considerations made in the previous section, we induce a sparse behavior on the components of z and on the associated decoder parameters (cfr. Fig. <ref type="figure" target="#fig_3">2</ref>.1). The variational parameter α can be learned and, as the connection with the dropout techniques is kept, we can leverage on the relationship between α and the dropout rate p to interpret the relative importance of the latent dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experiments</head><p>We first describe our results on extensive synthetic experiments performed with our non sparse model and with its sparse variant. We benchmark these models with respect to the VAE and conclude the Section with the application of our sparse model to real data, related to clinical cases of neurodegeneration.  <ref type="bibr">(2.18)</ref> where for every channel c, R c ∈ R dc×l is a random matrix with l orthonormal columns (i.e., R T c R c = I l ), G c is the linear generative law, and SNR is the signal-to-noise ratio. With this choice, the diagonal elements of the covariance matrix of x c are inversely proportional to SNR, i.e., diag E x c x T c = (1+SNR -1 )I dc . Scenarios where generated by varying one-at-a-time the dataset attributes, as listed in Tab. 3.12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Synthetic Experiments</head><formula xml:id="formula_22">G c = diag R c R T c -1/2 R c , x c = G c z + SNR -1/2 • ,</formula><p>ELBO in non-sparse Multi-Channel VAE. For each generated scenario, we optimized multiple instances of a Gaussian Linear Multi-Channel model, as defined in §2.2.1. At convergence, the loss function (negative lower bound) has a minimum when the number of fitted latent dimensions l fit corresponds to the number of the latent dimensions used to generate the data. When increasing the number of fitted latent dimensions, a sudden decrease of the loss (elbow effect) is indicative that the true number of latent dimensions has been found. These results are summarized in the Supplementary Materials, where we show also that the elbow effect becomes more evident when increasing the number of channels. Ambiguity in identifying the elbow usually arises for high-dimensional data channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Sparse Multi-Channel VAE Benchmark</head><p>This benchmark is based on the data scenarios illustrated in the previous section (Tab. 3.12). For each generated dataset, we optimized our Multi-Channel VAE with dropout posteriors (eq. 2.16) associated to log-uniform priors as in (eq. 2.17).  Results. In Fig. <ref type="figure" target="#fig_3">2</ref>.1 we compare the latent space distributions and the generative parameters derived from the application of the sparse and non-sparse Multi-Channel VAE, after fitting the two models on the same data and by imposing the fitted dimension for the latent space to l fit = 20. As expected, the number of zero elements is considerably higher in the sparse version. We note that the learned dropout rate is very low for the dimensions corresponding to the true latent dimensions used to generate the fitted scenario (Fig. <ref type="figure" target="#fig_3">2</ref>.2). Because of this, model selection can be performed by retaining those latent dimensions satisfying an opportune threshold on the dropout rates. We can see that with the threshold p &lt; 0.2, is possible to safely recover the true number of latent dimensions across all the testing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Comparison with VAE</head><p>We compared the performance of four variational methods applied to the synthetic scenarios. Besides our sparse (sMCVAE) and non-sparse (MCVAE) Multi-Channel models, we considered a VAE, and a stack of independent VAEs (IVAEs). In the VAE cases, channels where concatenated feature-wise to form a single channel. In IVAEs experiments, every channel was independently modeled with a VAE. Each scenario was fitted multiple times, by varying the dimension of the fitted latent space l fit in {1, 2, 4, 10, 20}. The comparison metric is the Mean Absolute Error (MAE) between the generated testing data and the predictions from the inferred latent space. Results. As depicted in Tab. 2.2, in general there is no significant difference between the average MAE for the different models (95% bootstrap confidence interval). However, when comparing the models in terms of number of parameters, our tests show that sMCVAE leads to equivalent reconstruction by pruning a consistent fraction of the parameters (on average 45%).</p><p>In Fig. <ref type="figure" target="#fig_3">2</ref>.3 we restrict the visualization to the cases where snr = 10 and l fit = l (cf. Tab. 3.12). Sparse Multi-Channel models perform consistently better than the non-sparse ones. Although in some cases VAE seems to provide better results (cf. 5-channel case in Fig. <ref type="figure" target="#fig_3">2</ref>.3), in complex cases with many channels the performance of VAE dramatically drops (cf. 10-channel case, ibid.). The IVAEs models leads to the worst performances in the majority of cases. This is expected, as the generated data variability depends on the joint information across channels. By modeling each channel independently, part of this variability is therefore mistaken as noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Medical Imaging data</head><p>Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see www.adni-info.org.</p><p>We analyzed clinical and imaging channels from 504 subjects of the ADNI cohort. We randomly assigned the subjects to a training and testing set through 10-fold cross validation.  <ref type="bibr" target="#b11">[Ashburner, 2000]</ref>. Visual quality check was performed to exclude processing errors. Image intensities were finally averaged over 90 brain regions mapped in the Automated Anatomical Labeling (AAL) atlas <ref type="bibr" target="#b109">[TzourioMazoyer, 2002]</ref> to produce 90 features arrays for each image. Lastly, data was centered and standardized across features. Our sparse multi-channel model ( §2.2.2) was optimized on the resulting multi-channel dataset, along with MCVAE, IVAEs, and VAE models as described in §2.3.3. For each model class, multi-layer architectures were tested, ranging from 1 (linear) up to 4 layers for the encoding and decoding pathways, with a sigmoidal activation applied to all but last layer.  <ref type="bibr">82 .76 .76 .75 .89 .89 .90 .79 .78 .77 .77 .79 .81 .82 .78 .77 MCI .58 .68 .70 .68 .71 .70 .68 .67 .65 .67 .69 .66 .71 .71 .63 .71 Dementia .88 .68 .69 .70 .85 .84 .84 .82 .68 .71 .66 .51 .82 .82 .72 .73</ref> Results. By applying the dropout threshold of 0.2 as identified in the synthetic experiments (Fig. <ref type="figure" target="#fig_3">2</ref>.2), we identify 5 optimal latent dimensions. The encoding of the test set in the latent space given by our sMCVAE model is depicted in Fig. <ref type="figure" target="#fig_3">2</ref>.4, where we limited the visualization to the 2D subspace generated by the two most relevant dimensions. This subspace appears stratified by age and disease status, across roughly orthogonal directions. This disentanglement between aging and disease is confirmed also with other modeling approaches <ref type="bibr" target="#b80">[Lorenzi, 2015;</ref><ref type="bibr" target="#b103">Sivera, 2019]</ref>. We note however that our the model was agnostic to the disease status, and was able to correctly stratify the testing data only thanks to the learned latent representation. This is shown in Tab. 2.3, where the latent representation provided by our sparse Multi-Channel framework leads to competitive predictive performances in predicting the clinical status. Prediction was performed on the testing set via Linear Discriminant Analysis (LDA) fitted on the training latent space. We note that the predictive accuracy is particularly high with the Multi-Channel framework.</p><p>We illustrate the ability of a single layer sMCVAE in reconstructing missing channels by using Eq. <ref type="bibr">(2.5)</ref>, to sample the imaging data from the latent dimensions obtained from the clinical channel. To this end, we sample points from two trajectories in the subspace shown in Fig. <ref type="figure" target="#fig_3">2</ref>.4 to predict the imaging data channels. Trajectory 1 (T r 1 ) follows an aging path centered on the healthy subject group. Trajectory 2 (T r 2 ), starts from the same origin of T r 1 and follows a path were aging is entangled with the pathological variability. We can see these trajectories and the generated imaging channels in Fig. <ref type="figure" target="#fig_3">2</ref>.5. (cfr. Eq. (2.7)) of the four channels associated to the most relevant latent dimension identified by dropout. These generative parameters show a plausible relationship across channels, describing a pattern of early onset AD, associated with abnormal scores (low MMSE, high ADAS-Cog and CDR), gray matter atrophy emerging from the structural MRI, low glucose uptake in the temporal lobes as emerging from the FDG-PET, and high amyloid deposits, coherently with the research literature on Alzheimer's Disease <ref type="bibr">[Dubois, 2014;</ref><ref type="bibr" target="#b58">Jack, 2018]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conclusion</head><p>This paper introduces the Sparse Multi-Channel Variational Autoencoder (MCVAE), an extension of the Variational Autoencoder (VAE), to jointly account for latent relationships across heterogeneous data. Parsimonious and interpretable representations are enforced by variational dropout, leveraging on sparsity to provide an effective mean to model selection in the latent space. In extensive synthetic experiments, we compared the performance of our model against different configurations of the VAE. We found a generally equivalent or superior performance of our model with respect to the benchmark, associated to a compression factor close to 50% on the number of pruned parameters.</p><p>In the real case scenario of Alzheimer's Disease modeling, our model allowed the unsupervised stratification of the latent space by disease status and age, providing evidence for a clinically sound interpretation of the latent space. Nonlinear parameterization of the model seemed not to bring clear advantages in the real case dataset, and needs further investigations. Given the scalability of our variational model, application to high resolution images may be also at reach, although this may require to account for full covariance matrices to take into account spatial relationships. To increase the model classification performance, supervised clustering of the latent space can be introduced, for example, through a categorical sampler in the latent space.Lastly, due to the gen-  eral formulation, the proposed method can find various applications as a general data interpretation technique, not limited to the biomedical research area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28</head><p>Chapter 2 Multi-Channel Variational Autoencoder 2.5 Supplementary Material q q q q q q 2500 2750 3000 3250 1 2 4 10 20</p><p>Fit. lat. dims. Negative Lower Bound C=10, d c = 32, l=1 q q q q q q 3000 3500 4000 4500 5000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 32, l=2 q q q q q q 3000 4000 5000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 32, l=4 q q q q q q 4000 5000 1 2 4 10 20</p><p>Fit. lat. dims.</p><formula xml:id="formula_23">C=10, d c = 32, l=10<label>(a)</label></formula><p>q q q q q q 26 27 28 29 1 2 4 10 20</p><p>Fit. lat. dims. Negative Lower Bound C=2, d c = 4, l=4 q q q q q q 54 57 60 63 66 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=3, d c = 4, l=4 q q q q q q 140 150 160 170 180 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=5, d c = 4, l=4 q q q q q q 550 600 650 700 1 2 4 10 20</p><p>Fit. lat. dims.</p><formula xml:id="formula_24">C=10, d c = 4, l=4<label>(b)</label></formula><p>q q q q q q 30000 40000 50000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 500, l=1 q q q q q q 30000 40000 50000 60000 70000 80000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 500, l=2 q q q q q q 40000 50000 60000 70000 80000 90000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 500, l=4 q q q q q q 40000 50000 60000 70000 80000 90000 1 2 4 10 20</p><p>Fit. lat. dims.</p><formula xml:id="formula_25">C=10, d c = 500, l=10<label>(c)</label></formula><p>q q q q q 0 50000 1 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 500, l=1 (high q. ) q q q q q q -40000 -20000 0 20000 40000 1 2 4 10 20</p><p>Fit. lat. dims.</p><p>C=10, d c = 500, l=2 (high q. ) q q q q q q -40000 Fit. lat. dims.</p><p>C=10, d c = 500, l=4 (high q. ) q q q q q q -25000 0 </p><formula xml:id="formula_26">= E c E q(z|xc,φ c ) [p (x i |z, θ i )] . (b)</formula><p>Mean squared error from the ground truth test data using the Single-Channel reconstruction:</p><formula xml:id="formula_27">xi = E q(z|xi,φ i ) [p (x i |z, θ i )]. (c) Ratio</formula><p>between Multi-vs Single-Channel reconstruction errors: we notice that the error made in ground truth data recovery with multi-channel information is systematically lower than the one obtained with a single-channel decoder. In the previous chapter we presented the Multi-Channel Variational Autoencoder (MC-VAE), a latent variable framework allowing to jointly model heterogeneous data. We used the term channel as a reference to a group of homogeneous observations. In this chapter, however, we decided to change the nomenclature an use the term view, as we found it more appropriate to describe a group of homogeneous features, which indeed gives us a "view", partial and non exhaustive, of the phenomena being measured. In medical imaging data modeling, it is often necessary to increase the sample size by pooling together data from multiple datasets. Training the MCVAE with data coming from multiple datasets is possible with some limitations: 1) after having discarded observations with missing views; 2) when all the datasets have all the views that we want to model. In this chapter we extend the capabilities of the MCVAE framework with a specific optimization scheme that allows the simultaneous learning from multiple datasets, without discarding any observation. This chapter is under review at NeuroImage <ref type="bibr" target="#b9">[Antelmi, 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The joint modeling of neuroimaging data across multiple datasets requires to consistently analyze high-dimensional and heterogeneous information in presence of often non-overlapping sets of views across data samples (e.g. imaging data, clinical scores, biological measurements). This analysis is associated with the problem of missing information across datasets, which can take place in two forms: missing at random (MAR), when the absence of a view is unpredictable and does not depend on the dataset (e.g. due to data corruption); missing not at random (MNAR), when a specific view is absent by design for a specific dataset.</p><p>In order to take advantage of the increased variability and sample size when pooling together observations from many cohorts, and at the same time cope with the ubiquitous problem of missing information, we propose here a multi-task generative latent-variable model where the common variability across datasets stems from the estimation of a shared latent representation across views. Our formulation allows to retrieve a consistent latent representation common to all views and datasets, even in the presence of missing information. Simulations on synthetic data show that our method is able to identify a common latent representation of multi-view datasets, even when the compatibility across datasets is minimal. When jointly analyzing multi-modal neuroimaging and clinical data from real independent dementia studies, our model is able to mitigate the absence of modalities without having to discard any available information. Moreover, the common latent representation inferred with our model can be used to define robust classifiers gathering the combined information across different datasets.</p><p>To conclude, both on synthetic and real data experiments, our model compared favorably to state of the art benchmark methods, providing a more powerful exploitation of multi-modal observations with missing views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Because of the inherent complexity of biomedical data and diseases, researchers are required to integrate data across different studies to increase the sample size and obtain better models [ <ref type="bibr" target="#b74">Le Sueur, 2020]</ref>. When developing integrative models, researchers have to face with multiple concurrent challenges, such as the ones related to datasets interoperability <ref type="bibr" target="#b108">[Tognin, 2020]</ref>, data heterogeneity <ref type="bibr" target="#b19">[Buch, 2020]</ref>, and data missingness [Golriz <ref type="bibr" target="#b45">Khatami, 2020]</ref>. Emblematic is the case of integrative modeling when datasets come from multi-centric studies in cognitive and neurological disorders, such as in Alzheimer's Disease (AD). Here the datasets interoperability is hampered by the existence of different protocols between studies. Because of this, methods whose modeling task are specifically designed on one dataset cannot be directly applied to another one.</p><p>Furthermore, at the level of each single dataset, researchers face the challenge of modeling heterogeneous data, such as multiple imaging modalities, clinical scores and biological measurements. Each of these sources of information represents an important and independent "view" on the disease or phenomena under investigation. Efforts to model multi-view data are increasing in the recent biomedical literature <ref type="bibr" target="#b114">[Vieira, 2020;</ref><ref type="bibr" target="#b113">Venugopalan, 2021]</ref>, where the objective ranges from predicting clinical outcomes <ref type="bibr">[Chen, 2019b;</ref><ref type="bibr" target="#b0">Abi Nader, 2020;</ref><ref type="bibr" target="#b107">Tabarestani, 2020]</ref> to synthesizing new modalities <ref type="bibr" target="#b119">[Wei, 2019;</ref><ref type="bibr" target="#b120">Wei, 2020;</ref><ref type="bibr" target="#b130">Zhou, 2020]</ref>. The key concept of a shared information space between views is widespread in the literature for the joint model of multi-view data. This is the case for well established multivariate linear methods such as Canonical Correlation Analysis (CCA), Partial Least Squares (PLS), Independent Component Analysis (ICA), which are some of the most popular methods for multivariate analyses on imaging data, as documented in a multitude of works from the state of the art (see <ref type="bibr" target="#b77">[Liu, 2014]</ref> for a general review).</p><p>While these studies essentially focus on the general problem of multivariate association modeling, multi-view methods specifically tailored to medical imaging tasks, such as image registration and segmentation have been proposed in parallel. For example, in <ref type="bibr" target="#b95">[Qin, 2019]</ref> the authors propose a registration method for aligning intra-subject multiview images. Although limited to a two images registration setting, in this work views are projected into a common latent space. The proposed registration approach is then built on the latent code and on an image-to-image translation approach. <ref type="bibr" target="#b23">[Chartsias, 2021]</ref> propose a segmentation method based on the learning of information presented jointly in complementary imaging views. From the different inputs views, anatomical factors are encoded into a common latent space and fused to extract more accurate segmentation masks. In <ref type="bibr" target="#b125">[Yang, 2020]</ref> a cross-modality segmentation pipeline is built around a similar concept. In all the works cited so far, the problem of missing data, specifically of missing views during training of multi-view methods, is generally not addressed nor considered. Still, this is a very common problem when joint modeling multiple datasets, especially in neuroimaging research. At the level of the single dataset, views can be missing at random (MAR) for some subjects. Typically, as fitting multi-view models requires to establish correspondences between views, observations with at least one missing view are generally discarded, yielding to potentially severe loss of available information. To mitigate this problem, imputation methods can be applied to infer missing views, by modeling the relationship across views from complete observations. The loss of information is exacerbated when considering multiple datasets altogether. Indeed, according to the cohort study design, there may be views which are specifically absent for a given dataset, hence missing not at random (MNAR). This potential mismatch across datasets hampers their interoperability, and prevents the gathering of all the available observations into a single, robust and generalizable joint model accounting for the global data variability. This challenge is typically addressed in machine learning by the field of Multi-Task Learning (MTL). To address this issue, MTL aims at improving the model interoperative capabilities by exploiting the information extracted from multiple datasets.</p><p>In MTL each task is usually associated to the modeling of a specific dataset and its views only, when the main idea consists is sharing across datasets the parameters learned through each modeling task <ref type="bibr" target="#b22">[Caruana, 1998;</ref><ref type="bibr" target="#b31">DoradoMoreno, 2020]</ref>. As an example of MTL, in model-agnostic meta-learning (MAML) <ref type="bibr" target="#b33">[Finn, 2017]</ref> the training of a model on a variety of learning tasks enforces the generalization on new datasets after few fine tuning iterations. In the context of data assimilation, MTL is usually achieved with specific output layers for every task, and by including a shared latent representation for all of them <ref type="bibr" target="#b31">[DoradoMoreno, 2020]</ref>. This modeling rationale is at the basis of recent MTL based approaches to heterogeneous data assimilation <ref type="bibr" target="#b124">[Wu, 2018;</ref><ref type="bibr">Shi, 2019]</ref>, especially in medical imaging approaches. For example, in <ref type="bibr">[Zhou, 2019b]</ref>, the authors propose a staged deep learning framework for dementia diagnosis classification, able to jointly exploit multi-view data, such as Magnetic Resonance Imaging (MRI), Fluorodeoxyglucose Positron Emission Tomography (FDG-PET), and genetic data. Their approach, where at each stage the model learns feature representations for different combinations of views, solves elegantly the problem of missing data. Although inspiring for their use of the maximum number of available data samples at each stage, the combinatorial nature of their framework makes it in practice applicable only for datasets with very few available views. For example, when considering 3 views, this approach requires to learn 7 networks. With 4 views, the number of networks that need to be trained, considering all the possible couples, triplets and quadruplets of views amounts to 4845; while with 5 views it exceeds 10 32 . Moreover, this framework is currently designed for classification tasks only, excluding the possibility of modality-to-modality prediction.</p><p>With the EmbraceNet (EN) of <ref type="bibr" target="#b27">[Choi, 2019]</ref> the problem of missing data is managed by zero-filling the missing input views and by the application of a specific dropout technique where multinomial samples are used to assign partitions of the latent space to specific views. As there are latent features that are randomly discarded even when the correspondent view is not missing, this represents still a loss of information. Similarly as for the previous work, the proposed framework is currently applicable in classification tasks only. Dropout is at the basis of the Denoising Autoencoder (DAE), as developed by <ref type="bibr" target="#b46">[Gondara, 2018]</ref>. Here an overcomplete deep autoencoder maps input views to a higher dimensional space. The initial dropout layer induces random corruption in the input views, making the model robust to missing data. This framework is currently applicable in feature prediction tasks only.</p><p>The common underlying assumption of these approaches consists in the existence of a proper transformation into a common latent code for the solution of multiple tasks, whether classification or feature prediction. Based on this general assumption, the Multi-Channel Variational Autoencoder (MCVAE) <ref type="bibr" target="#b8">[Antelmi, 2019]</ref> is a recent analysis method allowing the identification of a common latent representation for different views belonging to a single dataset (Fig. <ref type="figure">3</ref>.1). MCVAE extends currently available approaches to account for non-linear transformations from the data to the latent space, while it can be adapted to multiple tasks, including data reconstruction and classification. In spite of the high modeling flexibility, the extension of this method to the analysis of multiple datasets is currently challenging. Training the MCVAE in a multi-dataset context is indeed possible with some limitations: 1) after having discarded observations with missing views; 2) when at train time all the observations are compatible in terms of available views.</p><p>To overcome these limitations, in this work we investigate an extension of MCVAE to simultaneously learn from multiple datasets, even in the presence of non compatible views between datasets, and missing views within datasets. While our formulation naturally extends the original MCVAE approach, to the best of our knowledge no systematic investigation of this approach for the modeling of multi-view and multi-dataset neuroimaging data has been proposed so far. Our extension is built upon the following steps: 1) defining tasks across datasets based on the identification of data subsets presenting compatible views, 2) stacking multiple instances of the MCVAE, where each instance models a specific task, 3) sharing the models parameters of common views between modeling tasks. Thanks to these actions, the framework here proposed allows to learn a joint model for all the subjects without discarding any information (Fig. <ref type="figure">3</ref>.2). The common views between tasks act as a bridge and enable the information to flow through all the other views, while, in the training phase, tasks lacking a particular view will simply not contribute to the learning of those view-specific parameters. All the tasks will nevertheless benefit from the parameters they didn't contribute to learn, for the prediction of their missing views. The proposed variational formulation for computing approximate posterior distributions of the latent variables allows fast and scalable training. Being dataset agnostic, our method allows to integrate all the available data into a joint model, gathering all the available information from multiple datasets at the same time.</p><p>The rest of this paper is structured as follow. In § 3.2 we present the mathematical derivation of the classical MCVAE model that will be used to derive the proposed framework. In § 3.3.1 we show an illustrative application for the joint modeling of MRI and FDG-PET images when some modalities are missing in the training phase. In § 3.3.2, experiments on synthetic data show that the prediction error of missing views is competitive with respect to the one obtained with state of the art imputation methods. In § 3.3.3, experiments on real data from independent multi-modal neuroimaging datasets show that our model generalizes better than dataset-specific models, in both the cases of data reconstruction and diagnosis classification. Lastly we discuss our results and conclude our work with summary remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>In this section we recall the theoretical framework of the Multi-Channel Variational Autoencoder (MCVAE) developed in our previous work <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>, which we now extend to tackle the problem of missing data integration. In § 3.2.1 and § 3.2.2 we introduce our framework, the Multi-Task Multi-Channel Variational Autoencoder (MT-MCVAE), and derive the model in presence of missing data. In § 3.2.3 we propose the new optimization scheme allowing to account for observations with partially missing views. In § 3.2.4 we emphasize the differences between the MCVAE and our current approach. In § 3.2.5 we briefly recall the main parametric functions adopted later in our experiments with missing data. Code developed in Pytorch <ref type="bibr" target="#b92">[Paszke, 2019]</ref> is publicly available at <ref type="url" target="https://gitlab.inria.fr/epione_ML/mcvae">https://gitlab.inria.fr/epione_ML/mcvae</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generative Model</head><p>Let D = {D d } D d=1 be a collection of D independent datasets, where each dataset</p><formula xml:id="formula_28">D d = {x d,n } N d</formula><p>n=1 is composed by N d independent data-points (e.g., subjects in the case of medical imaging datasets). Every dataset D d is associated with a total number of V d available views (e.g., sets of clinical scores and imaging derived phenotypes extracted from multiple imaging modalities), and we assume that each data-point</p><formula xml:id="formula_29">x d,n = {x d,n,v } V d,n v=1 is composed by V d,n views, where V d,n ≤ V d .</formula><p>With the latest inequality we account for data-points with an arbitrary number of missing views.</p><p>For each view x d,n,v we rely on the following generative latent variable model:</p><formula xml:id="formula_30">z d,n ∼ p (z) , x d,n,v ∼ p (x d,n,v |z d,n , θ v ) , for v in 1 . . . V d,n ≤ V d ,<label>(3.1)</label></formula><p>where p (z) is a prior distribution for the latent variable z d,n commonly shared by the V d,n views, and where the likelihood functions p (x d,n,v |z d,n , θ v ) belong to a family of distributions parametrized by θ v , which represents the view-specific generative parameters shared among all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Inference Model</head><p>The exact solution to the inference problem is given by the posterior</p><formula xml:id="formula_31">p z| {x d,n,v , θ v } V d,n</formula><p>v=1 , that is not generally computable analytically. Following <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>, we can nevertheless look for its approximation through Variational Inference (VI) <ref type="bibr" target="#b16">[Blei, 2017]</ref>, applied in our specific context of missing data.</p><p>The variational approximations q (z|x d,n,w , φ w ), where φ w represents the view-specific variational parameters shared among all datasets, are such that:</p><formula xml:id="formula_32">ln p (x d,n,v |θ v ) ≥ L (x d,n ) v = 1 V d,n V d,n w=1 L (x d,n ) w→v , (<label>3.2)</label></formula><p>where:</p><formula xml:id="formula_33">L (x d,n ) w→v = E q d,n,w (z) [ln p (x d,n,v |z, θ v )] -D KL (q d,n,w (z)||p (z)) (3.3)</formula><p>is the lower bound associated to the data-point x d,n when its view v is predicted from its view w. In Fig. <ref type="figure">3</ref>.1 we sketch the model structure induced by Eq. (3.3). The complete derivation of Eq. (3.2) is detailed in the Supplementary Material section of this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Optimization</head><p>Assuming independent observations, the marginal log-likelihood in the left hand side of Eq. (3.2) can be summed up over all the datasets, data-points, and views. As a consequence, inference on the model generative parameters θ = {θ v } and variational parameters φ = {φ w } can be achieved by solving the maximization problem:</p><formula xml:id="formula_34">θ, φ = arg max θ,φ d,n,v L (x d,n ) v = arg max θ,φ d,n,v 1 V d,n V d,n w=1 L (x d,n ) w→v .</formula><p>(3.4)</p><p>We implemented Algorithm 1 to solve Eq. (3.4). The summation in Eq. (3.4) is done for every dataset d along all the available data-points n and their specific views v. If missing, a particular view v will be simply not accounted for that specific observation, without having to discard all the other views that can still contribute to optimize Eq. (3.4). We note that batching data-points with common views can speed up the computation by reducing the number of second level for loop iterations in Algorithm 1. The presence of at least one common view among datasets acts as a link across datasets and allows the information to flow through all the datasets to the other views. In Fig. <ref type="figure">3</ref> </p><formula xml:id="formula_35">L v ← 0 for every view w ∈ V d,n do L v ← L v + L (x d,n )</formula><p>w→v . See Eq. (3.3). end for Accumulate the average L v in the total cost:</p><formula xml:id="formula_36">L ← L + 1 V d,n L v . end for end for end for θ, φ = Optim(φ, θ, ∇ φ L, ∇ θ L).</formula><p>Adam optimizer used to maximize L. end while scheme of our model in a simple case with four views and one common view between batches. i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Comparison with VAE and MCVAE</head><p>In Tab. 3.1 we show how the Multi-Task framework detailed in Algorithm 1 extends the capabilities of the Multi-Channel VAE (MCVAE, <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>), which is itself a multi-view extension of the VAE <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr" target="#b96">Rezende, 2014]</ref>. In our former work we proposed a multi-view generative model trainable only with observation in the training set have all the available views, limited to model one dataset at a time (in the case of datasets with multiple views), after having discarded incomplete observations in that dataset. We address this limitation by allowing missing views in the training set for some observations, thanks to the adapted optimization scheme in Eq. (3.4). This aspect naturally extends the training paradigm of MCVAE to the more challenging scenario of multi-dataset analysis. As in the MCVAE, at test time, the trained MT-MCVAE model can  </p><formula xml:id="formula_37">xd,n,v = 1 V d,n -1 V d,n w=1, w =v E q d,n,w (z) [p (x d,n,v |z, θ v )] , (3.5)</formula><p>where the available views x d,n,w are encoded into the distributions q d,n,w , which are then used to predict the missing view through its decoding distribution p (x d,n,v |z, θ v ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Parameterization</head><p>With the right choice of the functional form of q (z|x d,n,w , φ w ), p (z), and p (x d,n,v |z, θ v ), the right hand side of Eq. (3.2) becomes amenable to computation and optimization, yielding to the maximization of the left hand side, quantity also known as the model evidence. Of course, the choice for the likelihood function p (x d,n,v |z, θ v ) depends on the nature of the view x d,n,v . For example it can be parametrized as a multivariate Gaussian in the case of continuous data (i.e.,imaging derived phenotypes), as a Bernoulli likelihood for dichotomic data, and as a Categorical likelihood for categorical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear parameterization</head><p>In general, the prior distribution p (z) is the multivariate Gaussian distribution N (0; I). The same family of distributions is also commonly used for the variational and likelihood functions, such that respectively:</p><formula xml:id="formula_38">q (z|x d,n,w , φ w ) = N µ = V (µ) w x d,n,w ; Σ = diag V (σ) w x d,n,w , (3.6) p (x d,n,v |z, θ v ) = N µ = G (µ) v z ; Σ = diag g (σ) v , (3.7)</formula><p>where the moments µ and Σ are obtained from linear transformations of the conditioning variables. Here,</p><formula xml:id="formula_39">θ v = {G (µ) v , g (σ) v } and φ w = {V (µ) w , V<label>(σ)</label></formula><p>w } are the parameters to be optimized. A non-linear parameterization can be used as well, for example in the form of deep neural networks.</p><p>In <ref type="bibr" target="#b8">[Antelmi, 2019]</ref> we also introduced the following alternative parameterization for the posterior distribution:</p><formula xml:id="formula_40">q d,n,w (z) = N µ = V (µ) w x d,n,w ; Σ = diag √ α µ 2 , (3.8)</formula><p>which is known as dropout posterior <ref type="bibr" target="#b69">[Kingma, 2015]</ref>. The dropout parameter α has components α i = p i/<ref type="foot" target="#foot_12">foot_12</ref>-p i linked to the probability p i of dropping out the i-th latent variable component <ref type="bibr" target="#b117">[Wang, 2013]</ref>. It has been shown that the association of this dropout posterior with a log-uniform prior distribution p (z) leads to sparse and interpretable models <ref type="bibr">[Molchanov, 2017;</ref><ref type="bibr" target="#b41">Garbarino, 2021]</ref>.</p><p>Thanks to the flexibility of modern neural network frameworks, it is straightforward to implement non linear parametrizations µ = f (µ) (x) and Σ = f (σ) (x) for the mean and covariance functions in the variational and likelihood distributions. Typically it is done by stacking linear or convolution layers, interleaved with non-linear activation functions such as sigmoid and hyperbolic tangent. This modeling is in general highly task-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Illustration on a simplified brain imaging dataset</head><p>In this section we describe a simple experiment where we use MT-MCVAE to model the joint relationship between Magnetic Resonance Imaging (MRI) and Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) images when there are missing data at training time. The trained model will be then applied on test data to the cross-modality reconstruction problem (MRI to FDG-PET and vice versa). Data comes from the ' Adni2' 1 dataset (see details in § 3.3.3, Tab. 3.3), from which we took the MRI and FDG-PET brain imaging modalities. In what follows, each one of this two modalities corresponds to a specific data view. For each subject (n = 424, with both MRI and FDG-PET) we extracted 3 brain slices for each one of the sagittal, coronal, and axial plane. The resulting 3816 slices were randomly allocated to a training and testing set with respectively sizes of 3500 and 316 samples. We downsampled the slices to dimension 28 × 28 (784 pixels). To simulate a datasets with missing views, we controlled for the fraction of observations with complete views (f ) in the training set: this procedure is depicted in Fig. <ref type="figure">3</ref>.3 where we show an example of training dataset created with f = 1/3. For our experiments we took all the 3500 training images and we randomly removed MRI and FDG-PET views to obtain different training sets for which f ∈ {0, 0.25, 0.5, 0.75, 1}. In the case f = 0, from each subjects we kept only its MRI or FDG-PET slices, representing the limit case where no direct relationship between views is observable. In the limit case f = 1, all MRI and FDG-PET are paired, representing the ideal case of no missing views, that is the working case of the MCVAE <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>. We adopted a deep architecture with 4 layers for both encoders and decoders, having ReLU activation functions and layer dimensions of 784 -1024 -1024 -16 in the encoding and 16 -1024 -1024 -784 in the decoding path, an architecture inspired from those used by <ref type="bibr">[Andrew, 2013b]</ref> and <ref type="bibr" target="#b118">[Wang, 2015]</ref> for a similar task on the MNIST dataset <ref type="bibr" target="#b75">[LeCun, 2010]</ref>. We adopted a Gaussian likelihood for the decoders, with independent diagonal covariance parameters, and we trained our model with mini-batches of size 500 for 3000 epochs, after setting up the Adam optimizer with a learning rate of 0.001. Training was repeated 5 times, by changing the initialization random seed of the model parameters. In Tab. 3.2 we show  the Mean Squared Error (MSE) and Negative Log-Likelihood (NLL) when predicting MRI from the FDG-PET slices and vice versa in the testing set. We notice the immediate drop in the error metrics as soon as the parameter f increases, which means that as the model is fed with an increasing proportion of multi-view data points in the training set, its predictions on the testing set become more precise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Synthetic Experiments</head><p>In this section we describe our results on extensive synthetic experiments performed with our model and different benchmark methods in two conditions: 1) missing at random views for each dataset, and 2) datasets with systematically missing views (missing not at random).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preparation</head><p>To simulate multi dataset observations, we sample the latent variable z d,n from a multivariate Gaussian with zero-mean and identity covariance matrix, and subsequently we transform each sample with random linear mapping towards the observation space to obtain x d,n,v . The detailed procedure is described in Sup. Mat.. We then corrupt the observations with increasing levels of noise and we finally remove views in the context of the missing at random (MAR) and missing not at random (MNAR) experiments.</p><p>In the MAR experiments views were randomly removed according to a parameter 0 ≤ f ≤ 1, which controls the fraction of data-points with complete views. In the limit case f = 1, each data-point has all the views, representing the ideal case of no missing views, that is the working case of the Multi-Channel Variational Autoencoder <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>. In the case f = 0, each data-point has one and only one randomly assigned view, representing the extreme case where no direct relationship between views is observable.</p><p>Here our multi-view model collapses into a disjoint series of independent Variational Autoencoders <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr" target="#b96">Rezende, 2014]</ref>. In the general case, each data-point has probability f to have all the views, and probability 1 -f to have a randomly assigned view out of the total available views. The general case represents the case where the relationship between views can be established only through a fraction f of the total available data-points.</p><p>In the MNAR experiments we removed specific views for each simulated dataset, ensuring at the same time the absence of at least one view for a datasets, and the presence of at least one view in common between pairs of datasets. As an example, in the case with three datasets and three views, the association view-dataset can be expressed through the following association matrix A:</p><formula xml:id="formula_41">A =     1 0 1 1 1 0 0 1 1     , (3.9)</formula><p>where A(v, d) = 1 indicates the presence of view v in dataset d. For experimental purposes we limited our MNAR simulations to cases that can be defined with square association matrices having a dimensionality not greater than 5 × 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fitting and Evaluation</head><p>In both MAR and MNAR experiments we fit the synthetic scenarios with our model, where we choose a linear Gaussian parametrization for variational and likelihood distributions, made explicit respectively in Eq. (3.6) and Eq. (3.7), with a latent dimension matched to the one used to generate the data. We trained our model for 10000 epochs which ensured convergence, after setting up the Adam optimizer with a learning rate of 0.001. For each simulated scenario we predicted the missing views according to Eq. (3.5) on testing hold-out datasets.</p><p>Results, cross-validated on 5 folds, are summarized with the Mean Squared Error (MSE) metric on testing hold-out datasets for every simulated scenario. We applied the same evaluation procedure for the benchmark methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark Methods</head><p>Among state of the art multivariate linear and non linear imputation methods, we selected the following benchmark approaches: 1) k-Nearest Neighbors (KNN) with k = {1, 5}; 2) Denoising Autoencoder (DAE) <ref type="bibr" target="#b46">[Gondara, 2018]</ref>; 3) Multivariate Imputation by Chained Equations (MICE) <ref type="bibr" target="#b21">[Buuren, 2000]</ref>.</p><p>For the KNN approach we used the KNNImputer method as implemented in the Scikit-Learn library <ref type="bibr" target="#b93">[Pedregosa, 2011]</ref>. Here each sample's missing values are imputed using the mean value from k nearest neighbors found in the training set, according to their Euclidean distance.</p><p>The Denoising Autoencoder, initially developed by <ref type="bibr" target="#b115">[Vincent, 2008]</ref>, is based on an overcomplete deep autoencoder. It maps input data to a higher dimensional space which, in combination with an initial dropout layer inducing corruption, makes the model robust to missing data. We used the same architecture proposed by <ref type="bibr" target="#b46">[Gondara, 2018]</ref>, that is three hidden layers for encoder and decoder networks, Tanh activation functions, hyperparameter Θ = 7, and dropout p = 0.5, as they proved to provide consistently better results.</p><p>In MICE, as implemented in [ <ref type="bibr" target="#b112">van Buuren, 2011]</ref>, missing values are modeled as a multivariate linear combination of the available features. This methodology is attractive if the multivariate distribution is a reasonable description of the data, which in our case it is by construction. MICE specifies the multivariate imputation model on a variable-byvariable basis by a set of conditional densities, one for each incomplete variable. Starting from an initial imputation, MICE draws imputations by iterating over the conditional densities. In the synthetic tests our model provides the best performances overall, with a mean MSE improvement compared to the best competing method of 17% in MAR cases and 71% in MNAR cases (Fig. <ref type="figure">3</ref>.4). We notice that DAE is not always better than KNN (k = 5), especially in low Signal-to-Noise Ratio (SNR) cases. We were able to fit the MICE model only on MNAR cases with high SNR, where it performed poorly (boxplot not shown), while in all the other cases, including all MAR cases, the model did not converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAE</head><p>In Fig. <ref type="figure">3</ref>.5 we show MAR experiments results stratified by SNR and by the fraction f of data-points with complete views. Here we notice how with already f = 0.25 we can significantly reduce the prediction error on testing data-points compared to the case f = 0, where no relationship between views can be established. Moreover, reaching the ideal case of f = 1, that is when there are no missing views in the dataset, does not improve significantly the prediction performance of our model compared to the case f = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Experiments on Brain Imaging Data</head><p>In this section we describe our results on jointly modeling real medical imaging datasets, independently acquired in the context of dementia studies.</p><p>We executed three kinds of experiments: 1) benchmark evaluation of our model against the best competing methods from the previous section; Subjects enrollment, data collection, and data sharing were approved by the ethic committees associated to each study dataset in accordance with the principles of the Declaration of Helsinki.</p><p>The available imaging modalities comes from the following acquisitions:</p><p>1. structural Magnetic Resonance Imaging (MRI) to measure anatomical volumes in the brain;</p><p>2. Positron Emission Tomography (PET) with 18 F-Fluorodeoxyglucose (FDG) tracer, to measure the glucose uptake, which reflects the functional status of the brain;</p><p>3. PET with the AV45 tracer, to measure the amyloid deposits in the brain;</p><p>4. PET with the AV1451 tracer, to measure the tau protein aggregates in the brain.</p><p>We divided the ADNI dataset into two complementary datasets: ' Adni1', composed by subjects recruited in the initial ADNI1 study <ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref><ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref>, and ' Adni2' composed by those subjects subsequently recruited in ADNI-GO, ADNI2, and ADNI3 (2010-ongoing). Data modalities and acquisition protocols of ' Adni1' present differences from those of ' Adni2'. Specifically, in ' Adni1' and ' Adni2' the MRI imaging was performed respectively on 1.5T and 3T scanners. The two cohorts differs also for the presence of PET imaging data. Therefore we consider these two cohorts as separated datasets.</p><p>To summarize, we grouped our data into five distinct datasets which we named as follows: ' Adni1', ' Adni2', 'Miriad', 'Oasis3', 'Geneva'. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imaging Processing</head><p>The brain scans were processed in order to have measurements on regions defined in the Desikan-Killiany atlas <ref type="bibr" target="#b30">[Desikan, 2006]</ref>. Brain MRI scans were processed with FreeSurfer<ref type="foot" target="#foot_15">foot_15</ref>  <ref type="bibr" target="#b96">[Reuter, 2012]</ref> to measure brain cortical and sub-cortical volumes, and volumes occupied by the cerebrospinal fluid (CSF), for a total of 99 regions of interest.</p><p>Relative Standardized Uptake Value (SUVR) was computed voxel-wise for the PET scans (FDG, AV45, AV1451), processed with SPM <ref type="bibr" target="#b11">[Ashburner, 2000]</ref>. SUVRs were computed using the cerebellum as reference region, and averaged in the same regions used for the MRI, except those containing the CSF, for a total of 94 regions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gathering Observations into Views</head><p>Observations from the five available datasets ( § 3.3.3) were grouped into the following views.</p><p>1. clin: grouping age and Mini-Mental State Examination (MMSE).</p><p>2. MRI: grouping brain volumes computed with FreeSurfer.</p><p>3. FDG: average brain glucose uptake measured through the analysis of FDG-PET scans.</p><p>4. AV45: average brain amyloid deposits measured through the analysis of AV45-PET scans. For each subject belonging to the ' Adni1', ' Adni2', 'Miriad' and 'Geneva' datasets, we choose the first available time-point, or baseline. In 'oasis3', since measurements were mostly acquired in different days, we choose to pair nearby time points across modalities into a single one. Time interval between views within one subject was minimal (AV45 vs MRI: ≤ 90 days, MRI vs clin: ≤ 90 days).</p><p>In Tab. 3.3 we show the number of observations stratified by dataset and view. Size of the intersection (∩) and union (∪) of subjects with available views is also provided. Please note that the only view in common across datasets is the clinical one, composed by MMSE and age features only.</p><p>We adjusted all the views feature-wise with ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref>, a normalization method originally develop in genomics, which was adopted in neuroimaging studies to reduce unwanted sources of variation in the data due to the differences in acquisition protocols among datasets <ref type="bibr" target="#b34">[Fortin, 2017;</ref><ref type="bibr" target="#b35">Fortin, 2018;</ref><ref type="bibr" target="#b89">Orlhac, 2020]</ref>. In ComBat, we set the variable 'age' as main regressor, and ' Adni2' as reference dataset for the training set. The ComBat reference dataset for testing was the whole training split.</p><p>A final feature-wise standardization step was applied by zero centering the data and by rescaling them to have a unity variance. Standardization parameters were computed on the training sets and applied to training and testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Benchmark Validation</head><p>The purpose of this experiment is to validate on real data the benchmarked results obtained with the synthetic experiments ( § 3.3.2).</p><p>As benchmark methods, we choose the best performers on the synthetic experiments, namely KNN5 and DAE. We choose for our MT-MCVAE model a linear Gaussian parameterization for the likelihood and sparse variational distributions of Eq. (3.7) and Eq. (3.8) respectively, the latter with a latent dimension of 32. We trained it for 20000 epochs which ensured convergence, after setting up the Adam optimizer with a learning rate of 0.001. In testing, we set up a dropout threshold for the latent space of 0.5.</p><p>We trained all the models (KNN5, DAE, ours) with data coming from all the datasets except from ' Adni2', left out for testing purposes. We choose the ' Adni2' dataset as testing dataset since it provides all the views, and the highest number of observations per view (Tab. 3.3).</p><p>Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with Eq. (3.5). All results were validated by means of 5-folds cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In Tab. 3.4, we show the MSE metric on predicting missing views in the testing dataset with our model and with the benckmark ones. Best results are in boldface, which show a clear advantage of using our model and confirm our findings in the synthetic experiments.  In STIB and STEB cases (Tab. 3.5), the MT-MCVAE model performs either similarly or statistically better than the MCVAE, especially in cases where the difference between the union and intersection set of observations is higher (cfr. Tab. 3.3).</p><p>In the MTL scenario (Tab. 3.6) there are 12 cases that could be fitted with MT-MCVAE only. We measure an overall better performance of MTL with respect to STIB (7/12 of cases) and with respect to STEB (10/12 of cases).</p><p>In Tab. 3.7, the results on a non linear application of our method in MTL cases show that no improvement is gained when changing the architecture depth (anova test, alpha level 0.05).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head><p>fitted with the MT-MCVAE model only. In all of them we measure a better performance with respect to the best STEB cases.</p><p>In Tab. 3.11, the results on a non linear application of our method in MTL cases show that no improvement is gained when changing the architecture depth (anova test, alpha level 0.05) for both the EmbraceNet and MT-MCVAE models. No significant differences (t-test, alpha level 0.05) are detectable between the EmbraceNet and MT-MCVAE models for any given architecture depth level. This result show that on the classification task the MT-MCVAE is equivalent to advanced MTL approaches from the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>In both the experiments on synthetic and real data, our model compared favorably with respect to state of the art benchmark methods.</p><p>An interesting result is the one presented in Fig. <ref type="figure">3</ref>.5, suggesting that collecting a minimum amount of data-points with complete views is enough for our model to capture the joint relationship among views. The empirical bound on this minimum level of data-points with all available views amounts to 25%. In fact, in our synthetic tests, training on scenarios with completeness level above this bound does not seem to improve significantly the testing results. This condition may be explained by the high collinearity between features due to the linear mappings used to generate the multi-view data. The same bound may be noticed also in our showcase experiment ( § 3.3.1) where we jointly modeled MRI and FDG-PET brain images. This results suggest that our model can reach its highest prediction power also when data collection resources are scarce, such as in studies were the acquisition of complete observations is hampered by economical reasons or subject dropout.</p><p>As a secondary result, we report the positive performance of knn (k = 5) in synthetic scenarios, especially in low snr cases, and on real data experiment, were it is most of the time superior to the DAE. This finding is corroborated by <ref type="bibr" target="#b94">[Platias, 2020]</ref> were knn is found to be superior to methods based on autoencoders.</p><p>The experimental results on real medical imaging datasets <ref type="bibr">(Tab. 3.5,</ref><ref type="bibr">Tab. 3.9)</ref> show on the horizontal axes the clear improvement of our MT-MCVAE method with respect to the MCVAE, that inspired our work, given the very same training and testing conditions for both of models. The features and diagnosis prediction clearly improves when using our method, that allows to not discard observations with missing views in the training phase. On the same tables, when read on the vertical axes, we note that models trained and tested on the same single dataset (STIB cases) tend to be more accurate than those trained on multiple other datasets (STEB cases). This is an expected result pointing to the issue of "domain shift", i.e. when observations coming from different datasets are not identically distributed, leading to generally high "within task" accuracy, and low generalization ability in the "between task" setting. We want to emphasize that we mitigated this problem with a data harmonization step based on ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref>, one of the state-of-the-art normalization method in biomedical applications <ref type="bibr" target="#b34">[Fortin, 2017;</ref><ref type="bibr" target="#b35">Fortin, 2018;</ref><ref type="bibr" target="#b89">Orlhac, 2020]</ref>. For this reason, we believe that the domain shift has a marginal impact for the application proposed in our study, and that those differences on the vertical axes are most likely due to the large variety of number of observations, available views, and differences in stratification by diseases in the datasets (cf. <ref type="bibr">Tab. 3.3,</ref><ref type="bibr">Tab. 3.8)</ref>.</p><p>In feature prediction experiments (Tab. 3.6) we showed that MT-MCVAE models trained jointly on multiple neuroimaging datasets (ADNI, MIRIAD, OASIS-3, Geneva cohort) perform generally better than the ones trained on a single dataset. We suspect that there are two reasons explaining these results. The first is that modeling simultaneously multiple datasets with our method brings more variability and information at play, making the generalization to unseen data less prone to prediction errors. The second reason maybe that every decoder, associated to its specific view, acts, through the shared latent space, as a regularizer for all the other decoders.</p><p>In experiments where we seek to classify subjects to predict their cognitive status (Tab. 3.10), the MT-MCVAE generalizes better to new unseen datasets when trained jointly on multiple datasets (MTL cases) with respect to cases where the training happens on a single dataset. We notice that the best results happen in cases where testing data and training data come from the same dataset (ST cases), that is when the testing dataset is not anymore unseen to our model. This is a different result than the analogous one in the feature prediction experiments, and we argue that the reason may be due to the lack of the regularization mechanism induced by having concurring decoders. Indeed, the MT-MCVAE classifier is composed by a single decoder only, which can become highly specialized in decoding testing data coming from the same dataset of the training data.</p><p>In our non linear experiments we did not capture any improvement by using deep architectures with respect to simple linear mappings, in both feature prediction (Tab. 3.7) and classification tasks (Tab. 3.11) on real neuroimaging datasets. These results are in line with our previous work <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>, were we benchmarked other autoencoding based methods on observations coming from the ADNI dataset. We suspect that this result is due to the general high heterogeneity and relatively small sample size of typical neuroimaging data. Our results on the classification task in multi-view and multi-dataset problem also showed that our approach is equivalent to the EmbraceNet <ref type="bibr" target="#b27">[Choi, 2019]</ref> recently proposed in the literature <ref type="bibr">( § 3.3.3)</ref>. While this finding indicates the ability of MT-MCVAE to provide results compatible with the state of the art in MTL classification problems, we note that the architecture of our framework enables a much larger set of applications than the one tackled by the EmbraceNet, such as cross-modality reconstruction and cross-dataset dimensionality reduction.</p><p>In our work we have thoroughly investigated architectures with a one-to-one correspondence between encoding and decoding views. This makes our model part of the family of the auto-encoders, where the model acts as identity transformation between the input and the output. Other architectures are nevertheless possible, such as the classifier described in § 3.3.3. In general, there may be an m-to-n relationship, with partially overlapping views among m input views and n output views. Investigating the properties of all the possible architectures is beyond the scope of this work.</p><p>As final remark, we want to stress that our model is based on the assumption of independent and identical distributed observations. This assumption may be limiting in healthcare datasets, such as the ones used in this work. In our work we mitigated these biases by harmonizing the datasets before applying our model, and we leave the extension and development of a bias-transparent multi-view models to future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conclusions</head><p>We proposed a new multi-task latent variable generative model able to learn simultaneously from multiple datasets, even in the presence of non-overlapping views among all the datasets. The available overlap between pairs of datasets allows the information to flow through all the views in the dataset pool. Since the learned view-specific parameters are shared among datasets, missing views can be automatically imputed for every dataset. The method proposed in this work is a coherent extension of classical variational generative models, making the training fast and scalable. Being dataset agnostic, our method allows to integrate all the available data into a joint model, gathering all the available information from multiple datasets at the same time. We conducted extensive tests for the joint modeling of synthetically generated data and of multi-modal neuroimaging datasets from independent dementia studies and associated clinical data, showing the competitiveness of our method with respect to the state of the art. Thanks to its general formulation, the proposed method can find applications beyond the neuroimaging research field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Derivation of the Lower Bound</head><p>The exact solution to the inference problem induced by Eq. (3.1) is given by the posterior</p><formula xml:id="formula_42">p z| {x d,n,v , θ v } V d,n</formula><p>v=1 , that is not generally computable analytically. Following <ref type="bibr" target="#b8">[Antelmi, 2019]</ref>, we can nevertheless look for its approximation q (z) through Variational Inference <ref type="bibr" target="#b16">[Blei, 2017]</ref>. By introducing the latent variational approximation q (z), we can derive the lower bound on the marginal log-likelihood for a single data-point as follows:</p><formula xml:id="formula_43">ln p (x d,n,v |θ v ) = ln p (x d,n,v |z, θ v ) p (z) dz = ln q (z) q (z) p (x d,n,v |z, θ v ) p (z) dz = ln E q(z) p (x d,n,v |z, θ v ) p (z) q (z) ≥ E q(z) [ln p (x d,n,v |z, θ v )] -D KL (q (z) ||p (z)) .</formula><p>(3.12)</p><p>To derive the last line of Eq. ( <ref type="formula" target="#formula_30">3</ref>.12) we leverage on the Jensen's inequality and collect the result into a new expectation term and in the Kullback-Leibler divergence term (D KL ).</p><p>We define the distribution function q (z) to depend on a specific dataset d, data-point n, and view w, such that:</p><formula xml:id="formula_44">q (z) = q d,n,w (z) = q (z|x d,n,w , φ w ) , (3.13)</formula><p>where φ w represents the view-specific variational parameters shared among all datasets.</p><p>To force a link among views, we impose the inequality Eq. (3.12) to hold for any w in 1 . . . V d,n . To do so, we average the right hand side of Eq. (3.12) across the V d,n views and rewrite Eq. (3.12) as follows: <ref type="bibr">(3.14)</ref> where</p><formula xml:id="formula_45">ln p (x d,n,v |θ v ) ≥ L (x d,n ) v = 1 V d,n V d,n w=1 L (x d,n ) w→v ,</formula><formula xml:id="formula_46">L (x d,n ) w→v = E q d,n,w (z) [ln p (x d,n,v |z, θ v )] -D KL (q d,n,w (z)||p (z)) (3.15)</formula><p>is the lower bound associated to the data-point x d,n when its view v is predicted from its view w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Data Generation</head><p>Data points with V views x d,n = {x d,n,v } V v=1 with x d,n,v ∈ R fv where created from a common latent code z d,n ∈ R l with l latent dimensions according to the following model: <ref type="bibr">(3.16)</ref> where for every view v, R v ∈ R fv×l is a random matrix with l orthonormal columns (i.e., R T v R v = I l ), G v is the linear generative law, and SNR is the signal-to-noise ratio. With this choice, the diagonal elements of the covariance matrix of x d,n,v are inversely proportional to SNR, i.e. <ref type="figure">,</ref><ref type="figure">diag E x d,</ref><ref type="figure">n,</ref><ref type="figure">v x T   d,</ref><ref type="figure">n,</ref><ref type="figure">v</ref> = (1 + SNR -1 )I fv . This generative Scenarios where generated by varying one-at-a-time the dataset attributes, as listed in Tab. 3.12. In the previous chapter we developed a Multi-Task (MT) optimization scheme allowing to train the Multi-Channel Variational Autoencoder (MCVAE) without completely discarding data-points if one or more views are missing. This allows to increase the data sample size by gathering observations from multiple datasets. Besides the problem of missing views, another complication when increasing the sample size is due to the bias induced by the domain shift. Indeed, when observations come from different non-harmonized datasets, they are not identically distributed, which is the general working hypothesis of many modeling frameworks, such as the MCVAE and MT-MCVAE. For the same reason, models trained on observations coming from one dataset, will perform poorly in an "out-of-sample" observation setting. In the previous chapter we mitigated this problem with a data harmonization step based on ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref>, one of the state-ofthe-art normalization methods in biomedical applications <ref type="bibr" target="#b34">[Fortin, 2017;</ref><ref type="bibr" target="#b35">Fortin, 2018;</ref><ref type="bibr" target="#b89">Orlhac, 2020]</ref>. Since the introduction of ComBat, data harmonization has been proposed through the use of more complex multivariate and non-linear approaches. This chapter proposes a thorough investigation of the modeling capabilities of some of the most popular harmonization methods from the state of the art, to determine their applicability in analysis scenarios as the ones proposed in Chapter 2 and Chapter 3.</p><formula xml:id="formula_47">z d,n ∼ N (0; I l ) , v ∼ N (0; I fv ) , G v = diag R v R T v -1/2 R v , x d,n,v = G v z d,n + SNR -1/2 • v ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The bias induced by the domain shift, that is the existence of different protocols between studies, multiple imaging machine manufacturers, image reconstruction softwares, and preprocessing algorithms, creates barriers to the integration of multi-centric datasets. As early a priori correction methods at acquisition time requires high-level technical expertise and can be used for prospectively acquired data only, to make the most out of existing data late a posteriori correction methods such as ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref> are becoming more and more of interest for the neuroimaging community. Since in the recent literature more advanced multivariate linear (e.g., ComBat <ref type="bibr">[Chen, 2019a]</ref>) and non-linear (e.g., Domain Invariant Variational Autoencoder <ref type="bibr" target="#b56">[Ilse, 2020]</ref>) bias correction methods have been also proposed, in this chapter we develop a quantitative framework to benchmark these approaches on extensive data simulation scenarios. Results show that although more advanced methods can indeed perform better in cases where the number of data features is in the order of thousands, ComBat is nevertheless legit for harmonizing datasets with less features. Assuming that the modeling assumptions we made in this chapter are valid for the data we observe, our results a posteriori justifies the use of ComBat in the previous Chapter 3, where we harmonized observations with features in the order of hundreds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>One of the major challenges for understanding Alzheimer's Disease (AD) is that the pathology evolves unnoticed for a long period (up to 20 years) before the manifestation of clinical, cognitive, and behavioral recognizable symptoms <ref type="bibr" target="#b39">[Frisoni, 2003;</ref><ref type="bibr" target="#b104">Solomon, 2011;</ref><ref type="bibr" target="#b99">Scarmeas, 2007;</ref><ref type="bibr" target="#b36">Fostinelli, 2020]</ref>. Therefore, efforts have focused on finding a set of quantitative biomarkers (e.g., volume, shape, intensity, texture, etc.) that would allow an early detection and follow-up monitoring of the AD hallmarks along the disease progression.</p><p>Radiomics is a procedure that relies on the quantification of vast amounts of biomarkers using high-throughput computing from medical images, such as for example Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET) <ref type="bibr" target="#b29">[DaAno, 2020]</ref>. To demonstrate the potential value of radiomic procedures as a valuable tool for tracking the dementia stages in AD, researchers are required to integrate data across different studies and datasets to increase the sample size, which leads to potentially more robust disease models and statistical inference [ <ref type="bibr" target="#b74">Le Sueur, 2020]</ref>. However, bias induced by the domain shift, that is the existence of different protocols between studies, multiple imaging machine manufacturers, image reconstruction softwares, and preprocessing algorithms, creates barriers to the integration of multi-centric datasets <ref type="bibr" target="#b60">[Jovicich, 2019]</ref>. Indeed, when observations come from different non-harmonized datasets they are not identically distributed, which is the general working hypothesis of many modeling frameworks: aggregating them can lead to poor analysis results, for example due to false discoveries or to miss existing correlations.</p><p>There are generally two non exclusive ways to address this issue <ref type="bibr" target="#b29">[DaAno, 2020]</ref>: 1) early a priori correction at acquisition time, by eliminating or reducing the differences across images that have been acquired on different machines; 2) late a posteriori correction by eliminating or reducing the differences across the features with statistical corrective tools. The early correction requires high-level technical expertise <ref type="bibr" target="#b90">[Palesi, 2019]</ref> to lower the barriers to participate in multi-centric neuroimaging studies <ref type="bibr" target="#b60">[Jovicich, 2019]</ref>, and is feasible for prospectively acquired data only. To make the most out of existing data and past investments, most radiomics studies are generally retrospective, that is conducted by collecting data already acquired. This is why late correction methods are becoming more and more of interest for the neuroimaging community.</p><p>Among late correction methods, ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref> has been shown to be superior to the other existing methods (see <ref type="bibr" target="#b29">[DaAno, 2020]</ref> for a comprehensive review) in controlling the variation related to the domain shift, increasing the correlation among test re-test replicates, and producing the highest of overall performances. The goal of ComBat is to transform the data from each domain, so they have similar mean and variance for each feature, while retaining the biological information of the data. It can also robustly manage high-dimensional data when sample sizes are small, as it uses an empirical Bayes framework to improve the variance of the parameter estimates, which is important for experiments with limited sample size, meta-analyses and clinical diagnosis.</p><p>In the recent work of <ref type="bibr">[Chen, 2019a]</ref>, the authors studied the cortical thickness measurements derived from MRI images in the Alzheimer's Disease Neuroimaging Initiative (ADNI), showing the existence of scanner effects in the covariance of structural imaging measures that are not harmonized by ComBat. To reduce this bias they proposed CovBat <ref type="bibr">[Chen, 2019a]</ref>.</p><p>In the domain of the machine learning, too, methods to deal with the problem of domain shift, such as the Domain Invariant Variational Autoencoder (DIVA), are being proposed to increase the inference power and generalizability of deep learning based methods. The interest in these methods arise from their non-linear modeling capabilities, whereas CovBat and CovBat, in their original formulation, can only model linear relationships.</p><p>As it is important to determine to what extent recently proposed harmonization methods are effective in reducing the domain shift bias, in this work we benchmark, on extensive synthetically generated scenarios, the most promising ones: ComBat, CovBat, DIVA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ComBat</head><p>The acronym ComBat stands for "Combining Batches", and it was proposed by <ref type="bibr" target="#b59">[Johnson, 2007]</ref> to combine multiple batches from gene expression microarray experiments, to the increase statistical power in detecting biological variations. Since non-biological variations, or "batch effects", are frequently observed across batches, it is inappropriate, in general, to combine datasets without adjusting for batch effects.</p><p>ComBat has been recently adopted in the neuroimaging community <ref type="bibr" target="#b34">[Fortin, 2017;</ref><ref type="bibr" target="#b35">Fortin, 2018;</ref><ref type="bibr" target="#b89">Orlhac, 2020]</ref> to combine observations coming from different centers, where the "batch effect" is generally induced by the diversity of existing imaging scanners. Here it is assumed that the observations come from m centers, containing each n i subjects for i = 1, 2, . . . , m. For feature v = 1, 2, . . . , p, let y ijv represent the feature (e.g. volume of a cortical brain region) measure for the subject j at center i. Given these premises, ComBat posits the following generative model:</p><formula xml:id="formula_48">y ijv = α v + X ij β v + γ iv + δ iv ijv (4.1)</formula><p>where α v is the overall measure for feature v, X is a design matrix for the covariates of interest (e.g. gender, age), and β v is the feature-specific vector of regression coefficients corresponding to X. The terms γ iv and δ iv represent the additive and multiplicative center effects. The error terms ijv are assumed to follow a normal distribution with mean zero and variance σ 2 v . ComBat uses an empirical Bayes framework to improve the variance of the parameter estimates of γ iv and δ iv . It estimates an empirical statistical distribution for each of those parameters by assuming that all features v share the same common distribution. After estimating the model parameters, the ComBat residuals and harmonized values are respectively defined as:</p><formula xml:id="formula_49">ComBat ijv = y ijv -αv -X ij βv -γiv δiv y ComBat ijv = ComBat ijv + αv + X ij βv . (4.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">CovBat</head><p>In <ref type="bibr">[Chen, 2019a]</ref>, the authors show that considerable differences in covariance exist across sites, and that the state-of-the-art harmonization techniques do not address this issue. In particular, in ComBat the error term ijv in Eq. ( <ref type="formula" target="#formula_52">4</ref>.1) is assumed to be identical distributed across sites. Its covariance matrix, however, may differ across sites. This is why to further improve the harmonization results, they generalize ComBat for the estimation and correction for the residual covariance differences, renaming the harmonization method in CovBat. The detailed procedure is described in <ref type="bibr">[Chen, 2019a]</ref>, and is based on the Principal Component Analysis (PCA) decomposition of the full data to obtain the full data covariance matrix, assuming that the site-specific covariances can be reconstructed with site-specific eigen-values, and that the residuals are linear combination of the global eigen-vectors. A tuning parameter of the CovBat model is the desired proportion of variance explained, used to threshold the number of principal components for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Domain Invariant Variational Autoencoder</head><p>In <ref type="bibr" target="#b56">[Ilse, 2020]</ref>, the authors consider the problem of domain shift and how to learn unbiased representations given data from a set of domains {d}. To do so, they propose the Domain Invariant Variational Autoencoder (DIVA), a Variational Autoencoder (VAE) <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr" target="#b96">Rezende, 2014]</ref>  </p><formula xml:id="formula_50">L = E q φ x (zx|x)q φ d (z d |x) [ln p θ (x|z x , z d )] VAE + E q φ d (z d |x) [ln p θ d (d|z d )] Domain Classifier -D KL q φ x (z x |x) ||p (z x ) z x regularizer -D KL q φ d (z d |x) ||p ψ d (z d |d) z d regularizer (4.3)</formula><p>where x represents the biased observation, d and z d are respectively the domain label and the portion of the latent space associated to the domain, z x represents the unbiased portion of the latent space, and φ = {φ x , φ d }, θ, ψ d are respectively the encoding, decoding, and prior network parameters. The posterior encoding distributions q φ x (z x |x) and q φ d (z d |x) are regularized by minimizing the Kullback-Leibler divergence (D KL ) from their respective prior distributions.</p><p>At test time, the unbiased estimation of the data is decoded after setting to zero the portion of the latent space z d associated to the classifier:</p><formula xml:id="formula_51">x DIVA = E q φx (zx|x) ln p θ (x|z x , z d = 0) . (4.4)</formula><p>By choosing the appropriate architecture for the encoding and decoding networks, the DIVA can model linear and non linear relationships in the data. In this work we choose a linear architecture (DIVA-1), with one fully connected (FC) layer per network, and a non linear one (DIVA-4), with 4 FC layers interleaved with LeakyReLU activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Qualitative benchmark</head><p>For a better understanding on how the data are harmonized, we now apply the methods described in the previous sections to MRI derived brain volumetric features coming from real research datasets, and measure the change in the feature correlation matrix of each dataset before and after harmonization, and between datasets.</p><p>To do so, we gathered together MRI cortical volumes from the following three datasets: Alzheimer's Disease Neuroimaging Initiative (ADNI) -Study number 1 (adni1) and -Study number 2 (adni2), Open Access Series of Imaging Studies (OASIS) -Study number 3 (oasis3). For an extended description of the datasets and the feature extraction procedure, see Chapter 3 ( § 3.3.3).</p><p>In Tab. 4.1 we show the stratification of all the available data by dataset and diagnosis, along with age statistics. We harmonized the MRI features with ComBat, CovBat, and DIVA, the latter with a linear and non-linear architecture, and measure the change in the feature correlation matrix of data coming from each dataset before and after harmonization, and between datasets.</p><p>Results. In Fig. <ref type="figure">4</ref>.1 we show the correlation matrix of the MRI features before and after harmonization. In Tab. 4.2 and Tab. 4.3 we measure the difference between all the pairs of correlation matrices with the Frobenius norm. We can see that only with</p><p>CovBat the correlation matrices are actually harmonized and very similar between centers (Tab. 4.2).</p><p>With DIVA, in both linear and non-linear cases, the data seems to be highly corrupted after harmonization (Fig. <ref type="figure">4</ref>.1), with the harmonized data structure far from the original data structure in terms of Frobenius norm (Tab. 4.3).</p><p>Table <ref type="table" target="#tab_20">4</ref>.2: Pairwise Frobenius norms between dataset-specific correlation matrices for every harmonization method. We find that ComBat adjustment does not harmonize the correlation matrices whereas CovBat adjustment shows large reductions in the betweendatasets distances. The almost perfect reduction of the Frobenius distances in the DIVA-1 linear cases is spurious, as the original correlation structure is very different from the harmonized ones (see Tab. 4.3, Fig. <ref type="figure">4</ref>.1). In the DIVA-4 non-linear case Frobenius distances generally increases, which is not ideal for an harmonization method.</p><p>Raw ComBat CovBat DIVA-1 (linear) DIVA-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We consider a harmonization method to be successful if it removes the batch effect induced by the domain, and if it preserves biological variability <ref type="bibr" target="#b34">[Fortin, 2017]</ref>. Both conditions must be simultaneously tested on the same set of images. This is why we approach the benchmark by simulating datasets presenting features variability induced by a class label y, which may be thought as a diagnosis label, and at the same time being biased with a domain dependent transformation of the ground truth features value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Synthetic data generation procedure</head><p>To generate synthetic data we rely on the make_classification<ref type="foot" target="#foot_19">foot_19</ref> dataset generator function from the sklearn python library <ref type="bibr" target="#b93">[Pedregosa, 2011]</ref>: this function generates a random n-class classification problem by creating clusters of points normally distributed about vertices of an n-informative-dimensional hypercube and assigns an equal number of clusters to each class. This approach introduces interdependence between these features and adds various types of further noise to the data. Redundant features can be added by random linear combination of the informative ones. The make_classification function is adapted from <ref type="bibr" target="#b48">[Guyon, 2003]</ref> and was designed to generate the "Madelon" dataset, an artificial dataset originally developed for benchmarking two-class classification methods with continuous input variables.</p><p>Furthermore, for every dataset d, we can simulate the domain bias with the following procedure:</p><p>x, y ∼ make_classification(. . .)</p><formula xml:id="formula_52">x d = L d x + α d ,<label>(4.5)</label></formula><p>where x is the bias-free observation and y the associated label; the bias inducing L d is the Cholesky decomposition of a symmetric positive definite matrix Σ d = L d L T d . One way to create Σ d is to randomly generate a matrix of k d-dimensional loadings W ∈ R d×k with k &lt; d, then form covariance matrix WW T and add to it a random diagonal matrix D with positive elements to make WW T + D full rank. The resulting covariance matrix can be normalized to have ones on its diagonal.</p><p>In Fig. <ref type="figure">4</ref>.2 we can see an example of such matrices generated with k number of loadings. The parameter k can be interpreted as the level of data covariance complexity introduced by biasing the observations with the linear transformation L. In Tab. 4.4 we show the parameters, varied one-at-a-time in the prescribed ranges, used to generate synthetic scenarios for the experimental campaign. The ground truth data x generated with make_classification(. . .) is composed by an equal number of informative, redundant, and non-informative features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Quantitative benchmark</head><p>To assess the performance of an harmonization method we trained, for each synthetic scenario and for each harmonization method, two Linear Discriminant Analysis (LDA) classifiers: one to classify the dataset from the harmonized features; the other one to classify the label y from the same features. As baseline we considered the performances of the same classifiers trained on non-harmonized data (x d ).</p><p>For DIVA we choose a linear and non-linear architecture: the linear one (DIVA-1), with encoder and decoders consisting in one linear transformation layers; the non-linear one (DIVA-4) with encoder and decoders consisting in a stack of 4 linear layers, interleaved with LeakyReLU activation functions. The dimension of the latent space is choosen to match the number of informative features used to generate the synthetic scenarios. The DIVA models are trained with Adam <ref type="bibr">[Kingma, 2014a]</ref>, with learning rate = 0.001 for 30k epochs, which ensured convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label classification</head><p>In Fig. <ref type="figure">4</ref>.3 we plot the LDA classifiers accuracy on the label y classification task for different harmonization methods.</p><p>As general pattern, we notice that data harmonized with ComBat and CovBat are always easier to classify than the non-harmonized ones. The accuracy increase with a higher For DIVA, we are not able to identify a clear pattern. In general we notice that the classification performance on DIVA harmonized data are low and in line with those on non-harmonized data. The linear version (DIVA-1) seems to perform generally better than the non-linear one (DIVA-4), although there are very few cases where DIVA-4 is the best overall performer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset classification</head><p>In Fig. <ref type="figure">4</ref>.4 we plot the LDA classifiers accuracy on the dataset classification task for different harmonization methods. As a good harmonization method would ideally remove the dataset induced bias, the best methods are the ones where the classifier performs badly.</p><p>As general pattern, we notice that ComBat and CovBat gives similar results in removing the dataset information, although to reach the random chance level, meaning that the whole information has been removed, we need a high number of data-points.</p><p>With the DIVA methods we are not able to identify a clear pattern, although the linear version (DIVA-1) seems to perform generally better than the non-linear one (DIVA-4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>We expect, for a good harmonization method, to improve the classification task for outof-dataset samples. At the same time we expect the dataset induced bias to be reduced after harmonization, hence reducing a classifier ability to discern between datasets.</p><p>As for the DIVA, we notice already in Fig. <ref type="figure">4</ref>.1 that, although the dataset covariances seem to be harmonized (DIVA-1), they look very different from the original ones (Raw). Indeed, when we measure the distance between datasets (Tab. 4.2, column DIVA-1), this is very low in all cases. In the synthetic experiments results, we also notice that apparently the dataset information is absent from the DIVA-1 harmonized data, as generally the domain classifier has performances around the random-chance level <ref type="bibr">(Fig. 4.4)</ref>. This is likely to be a spurious result, as the distance between original and DIVA-harmonized data (Tab. 4.3) is the highest among all the harmonization methods. Since there are still cases where  the DIVA-4 outperforms all the other methods (Fig. <ref type="figure">4</ref>.3, column feats= 1000, top three rows), we argue that, although generally we would not advise to adopt it as a robust harmonization method, DIVA has the potential to compete with the state of the art. For this reason, further research is needed.</p><p>The performances of ComBat and CovBat seems to be generally comparable. With CovBat the correlation matrices of the datasets are actually harmonized, as we can see it already in Fig. <ref type="figure">4</ref>.1. This is confirmed numerically in Tab. 4.2, where we notice a clear reduction in the dataset differences after CovBat harmonization, while the original covariance structure is preserved (Tab. 4.2). In extensive experiments, the performances of the two methods are generally aligned, both in the label classification (Fig. <ref type="figure">4</ref>.3) and in the datset classification experiments (Fig. <ref type="figure">4</ref>.4), although we notice a significant difference favoring CovBat in label classification tasks with a high number of features (n feats = 1000).</p><p>Lastly, we highlight that our extensive experimental setup is based on the linear generative model of Eq. ( <ref type="formula" target="#formula_52">4</ref>.5), and that supplementary experiments with non-linear generated datasets are necessary to further verify the harmonization performances of the methods here discussed in more complex modeling scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Conclusion</head><p>In this chapter we benchmarked state of the art harmonization methods, needed to compensate the bias induced by the domain shift, that is the existence of different acquisition protocols between centers, that creates barriers to the integration of multicentric datasets in dementia studies.</p><p>We tested the performances of ComBat and CovBat linear harmonization methods, as well as the Domain Invariant Variational Autoencoder (DIVA) based linear and non-linear ones, on extensive synthetically generated scenarios.</p><p>We conclude that the performances of both ComBat and CovBat are robust, and generally aligned between them, and that the latter should be preferred when harmonizing data with a high number of features (n feats ≥ 1000). Keeping in mind the limits of the current work discussed earlier, this result can justify a posteriori the use of ComBat in Chapter 3, where we harmonized neuroimaging derived features in the order of hundreds (cf. Tab. 3.3).</p><p>As for the DIVA based methods, we suggest to adopt them cautiously, as in our experiments we noticed data corruption and generally bad performances on label and dataset classification tasks after harmonization. The existence of experimental cases where the DIVA outperformed all the others suggests that more research is needed to clarify how the DIVA based methods can be improved and made reliable and competitive with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mcvae: an Open Source Python Toolbox</head><p>In this chapter we introduce and document the open-source Python toolbox mcvae, for jointly model low-and high-dimensional heterogeneous data. We provide an objectoriented implementation, extensible with custom modules, such as new encoder-decoder architectures. Statistics and learning algorithms provide methods for feature estimation, imputation, and dimension reduction on the latent space. All associated operations are vectorized for batch computation and provide support for the PyTorch backend <ref type="bibr" target="#b92">[Paszke, 2019]</ref>, enabling GPU acceleration. This chapter presents the package and provides relevant code examples. We show that Mcvae provides reliable building blocks to foster research in joint modeling of heterogeneous data. The source code is freely available online.</p><p>Main repository: <ref type="url" target="https://gitlab.inria.fr/epione_ML/mcvae">https://gitlab.inria.fr/epione_ML/mcvae</ref>. Mirror: <ref type="url" target="https://github.com/ggbioing/mcvae">https://github.com/ggbioing/mcvae</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction</head><p>High-dimensional heterogeneous data naturally arises in many fields of modern scientific research, such as genomics <ref type="bibr" target="#b110">[Uppu, 2018]</ref>, biomedical imaging <ref type="bibr" target="#b86">[Miotto, 2018]</ref>, finance <ref type="bibr" target="#b100">[Sezer, 2020]</ref> and so on. Understanding the relationship among heterogeneous data is essential: for example in medical applications, where performing a diagnosis, or understanding the dynamics of a pathology require to jointly analyze multiple data modalities, such as demographic data, medical imaging data, and psychological tests.</p><p>In Chapter 2 of this Thesis we developed the Multi-Channel Variational Autoencoder (MCVAE), a method for the joint analysis of heterogeneous observations, which is scalable to high-dimensional observations and high sample sizes. To do so, we generalized the Variational Autoencoder (VAE), a state-of-the-art single modality latent variable model (Fig. <ref type="figure">5</ref>.1a), by assuming a single low-dimensional latent variable as the common source of the multi-modal observations (Fig. <ref type="bibr">5.1b)</ref>. This modeling rationale is well suited to model biomedical data, as the patient can be considered as the common source of all the data collected through imaging and medical examinations. For each data modality, independent encoders (from the observation space to the latent space) and decoders (from the latent space to the observation space) can be used to track and interpret the informative path from one modality to another, through the common latent space bottleneck.</p><p>In Chapter 3 we further proposed a Multi-Task (MT) extension for of the MCVAE to make it robust to missing data, especially when gathering observations from different acquisition centers (Fig. <ref type="bibr">5.1c)</ref>. This was done by introducing an optimization scheme specifically designed for maximum data usage, based on the identification of subgroups of observations with common modalities. The common modalities are used to train the encoders and decoders parameters associated to those subgroups, while holding the learning of out-of-group parameters.</p><p>The inheritance relationships between the VAE, MCVAE, and MT-MCVAE, can be naturally and elegantly implemented with the Object-oriented programming (OOP) paradigm. To do so we choose the Python programming language and the PyTorch library <ref type="bibr" target="#b92">[Paszke, 2019]</ref>, which makes the OOP implementation of custom machine learning methods straightforward.</p><p>In this chapter we provide an implementation of the MCVAE and MT-MCVAE by presenting the open-source mcvae package to 1) reduce duplication of efforts in research; and 2) facilitate research in joint modeling of heterogeneous data. The mcvae package comprises the core classes for multi-channel latent variable modeling, with and without missing data, and synthetic datasets generator utilities to simulate modeling scenarios. It was recently presented at the AI4hHealth winter school<ref type="foot" target="#foot_23">foot_23</ref> , during the practical session on "Handling heterogeneity in the analysis of biomedical information"<ref type="foot" target="#foot_24">foot_24</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation overview</head><p>The package mcvae is based on the PyTorch deep learning library <ref type="bibr" target="#b92">[Paszke, 2019]</ref> and is organized in modules. The module mcvae.models contains the latent variable models VAE, MCVAE, and MT-MCVAE. They are implemented as Python classes whose constructors create and initialize their parameters, Since each model extends the previous one, an inheritance scheme is adopted to ease the code readability and the debugging process in development. In Fig. <ref type="figure">5</ref>.2 we show the inheritance design adopted in our package.</p><p>Since the complexity of the models is highly dependent on the modeling task at hand, the user is encouraged to define her/his own VAE module by inheriting from the mcvae.models.Vae class, and redefine solely the init_encoder() and init_decoder() methods to output the desired PyTorch distributions (e.g., <ref type="bibr">Normal, Categorical, Bernoulli, etc.)</ref>. The Mcvae class builds the MCVAE model based on the input data shape and user defined architecture. The MtMcvae class builds the MT-MCVAE model, where the optimization is guided by the observation identifiers (ids) needed to correctly pair observations between channels.</p><p>The sparsity flag is used to model the encoding distributions with Dropout posteriors, that with posterior distributions on the latent variable z = {z i } L i=1 that take the form z i ∼ N µ i ; α i µ 2 i <ref type="bibr" target="#b69">[Kingma, 2015]</ref>. The regularization of a dropout posterior depends only on α i <ref type="bibr">[Molchanov, 2017]</ref>. The dropout parameter α i = p i/1-p i is linked to the probability p i of dropping out the i-th latent variable component <ref type="bibr" target="#b117">[Wang, 2013]</ref>. It has been shown that the association of this dropout posterior with a log-uniform prior distribution p (z) leads to sparse and interpretable models <ref type="bibr">[Molchanov, 2017;</ref><ref type="bibr" target="#b8">Antelmi, 2019;</ref><ref type="bibr" target="#b41">Garbarino, 2021]</ref>. While the regularization promotes α i → ∞, the implicit drop rate p i tends to 1, meaning that the associated latent z i can be discarded. Sparsity arises naturally: large values of z i correspond to even larger uncertainty α i z 2 i because of the quadratic relationship and the tendency of the optimization objective to favors α i → ∞; therefore, unless that latent z i is beneficial for the optimization objective, that is to maximize the data log-likelihood, it will be set to zero.</p><p>We also provide sub-modules and utilities to generate synthetic datasets, with and without missing data, for simulation and benchmark purposes. Scenarios with complete data can be simulated with our mcvae.datasets.synthetic.py submodule. The data missingness is defined through utility routines (mcvae.utilities) that simulate data missing at random (MAR) and data missing not at random <ref type="bibr">(MNAR)</ref>. Working examples for fitting MCVAE and MT-MCVAE models are provided within the released python package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Usage: example of multi-modal learning</head><p>To fit a MCVAE model is just needed to provide multi-modal data formatted as a list of PyTorch tensors [x 1 , x 2 , . . . , x C ], where each element of the list corresponds to each data modality. For each data modality, the first dimension of each x c must be equal to the number of observations (or subjects) in the dataset, that is n c = N , for c = 1, . . . , C. See Chapter 2 for a complete theoretical background and use cases of the MCVAE model.</p><p>If there are missing data in the dataset, that is if n c ≤ N , for c = 1, . . . , C, a list of identifiers should be provided for each data modality, to allow the identification of subgroups of observations with common modalities. The theoretical framework for this use case is developed in Chapter 3.  <ref type="figure">----------------------------------------</ref>   <ref type="figure">----------------------------</ref>  <ref type="figure">---------------------------</ref>   <ref type="bibr">)</ref> def forward ( self , x ) : q = self . encode ( x ) z = q . rsample () p = self . decode ( z ) return x , q , p def loss_function ( self , x , q , p ) : kl = kl_divergence (q , Normal (0 , 1) ) . sum (1) . mean (0) ll = p . log_prob ( x ) . sum (1) . mean (0) total = kl -ll return total</p><formula xml:id="formula_53">: dict() -----------------------------------------<label>encode</label></formula><p>Listing 5.2: PyTorch code that produces a simple linear Variational AutoEncoder. The user can easily extend it with multiple layers, convolution operations, non-linearities, etc., depending on the complexity of the modeling task.</p><p>5.4 Supplementary documentation 5.4.1 Main model classes (mcvae/models)</p><p>• Mcvae: main class used to build a Multi-Channel Variational AutoEncoder as in <ref type="bibr">[Antelmi, 2018a;</ref><ref type="bibr" target="#b8">Antelmi, 2019]</ref> (usage example in Fig. <ref type="figure">??</ref>). Arguments:</p><p>-data: example of a multi-channel dataset from which infer the model architecture (n_channels, n_feats)</p><p>-lat_dim: number of latent dimension -n_channels: number of channels. Can be inferred from "data".</p><p>-n_feats: number of features for each channel. Can be inferred from "data".</p><p>-beta: scaling factor for Kullback-Leibler distance.</p><p>-enc_channels: specify the channels to encode from.</p><p>-dec_channels: specify the channels to decode.</p><p>-sparse: True for a sparse model (default False).</p><p>-vaeclass: basic class for building the Mcvae model.</p><p>-vaeclass_kwargs: dictionary of arguments for "vaeclass".</p><p>• MtMcvae: extension of Mcvae to allow training of multi-channel data with missing observations, as in <ref type="bibr" target="#b9">[Antelmi, 2021]</ref>. This class inherits all the properties of the Mcvae class and extend it with the following argument:</p><p>-ids: a list of observation identifiers for each channel. It is used internally to batch together observations with the same id, necessary to properly update the model parameters during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Utilities</head><p>The MCVAE package comes with utilities that help the user in managing his/her models.</p><p>Fit, load, save (mcvae/models/utils)</p><p>• update_model: to load/copy the model parameters from disk.</p><p>• save_model: to save a trained model to disk.</p><p>• load_or_fit: equivalent to update_model if the model has been saved in a previous training session. If not, this utility trains the model and saves it to disk. It uses a context manager<ref type="foot" target="#foot_27">foot_27</ref> that prevents the model to be trained if there is a job already in place to fit the model. This is useful to avoid conflicts between jobs when heavy experimental campaigns need to be run. Arguments:</p><p>-model: model to optimize.</p><p>-data: training data. It can be also a PyTorch DataLoader for mini-batch training.</p><p>-epochs: number of training epochs.</p><p>-ptfile: path to *.pt file where to save the trained model.</p><p>-minibatch: True if training with mini-batches (default False).</p><p>-force_fit: force the training even if the model is already trained.</p><p>• load_data_from_spreadsheet: utility to load multi-channel observations, with or without missing data, from a spreadsheet. The spreadsheet should contain one sheet per channel. The observation identifier is assumed to be in the first column of every sheet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnostic utilities (mcvae/diagnistics)</head><p>• plot_loss: to check the convergence of the training operation (Fig. <ref type="figure">5</ref>.3).</p><p>• plot_latent_space: to visualize the projection of multi-channel data into the latent space (Fig. <ref type="figure">5</ref>.4).</p><p>• plot_dropout: to easily check which latent dimensions have been dropped out in the sparse MCVAE model (Fig. <ref type="figure">5</ref>.5).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The works illustrated in this Thesis show how the combination of: 1) generative modeling, 2) variational inference, and 3) state of the art machine learning methods, can have a positive impact in solving complex data modeling scenarios, such as when dealing with heterogeneous, high-dimensional observations for multi-modal feature prediction or classification tasks.</p><p>Generative modeling is a powerful designing procedure that we adopted it in Chapter 2 and Ch. 3 to posit a latent variable z as the single source of all the heterogeneous observations collected in neuroimaging datasets for dementia studies. The parallelism with respect to what happen in reality is clear, as usually there is a patient undergoing various medical exams, such as Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) Imaging, who can be considered as the single source of the medical results.</p><p>In the following sections, we summarize the methodological contributions of this Thesis as well as the obtained results. We also propose new applications for our methods and build upon their limitations to propose research perspectives for the field of the joint modeling of heterogeneous data, and we conclude the Thesis with final remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Summary of the main contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Channel Variational Autoencoder (MCVAE)</head><p>In Chapter 2 we introduced a novel latent variable framework to jointly model complex heterogeneous observations. To do so we posit a single latent variable as the source of the variability observed in the data, and we applied Variational Inference and modern learning algorithm to infer that latent source. The joint modeling is promoted in the latent space by constraining the inferred distribution of each data modality to a common target prior. We argue that our framework can be of interest for the neuroimaging community as it can be adopted to model the joint relationship between multi-modal neuroimaging data, such as those ones regularly collected in research dementia studies. Indeed, in this context of high heterogeneity due to the presence of, among many others, Magnetic Resonance Imaging (MRI) data and Positron Emission Tomography (PET) imaging data, that is channels with their own informative content, there is a rational need for methods to establish relationships between observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main contributions</head><p>• We proposed a novel methodology for the modeling of multi-modal heterogeneous data, when these data share a common origin. It is particularly suited for medical application as in this domain the patient can be considered as the common origin of all the data collected to infer his/her unknown diagnostic state.</p><p>• Interpretable latent representations are enforced by variational dropout, leveraging on sparsity to provide an effective mean to model selection in the latent space.</p><p>In the real case scenario of Alzheimer's Disease modeling, our model allowed the unsupervised stratification of the latent space by disease status and age, providing evidence for a clinically sound interpretation of the latent space.</p><p>• Thanks to its general formulation, the proposed method can be applied as a general data interpretation technique, not limited to the biomedical research area.</p><p>• The implementation of the method is open source and freely available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Multi-Channel Variational Autoencoder (MT-MCVAE)</head><p>In medical imaging data modeling, it is often necessary to increase the sample size by pooling together data from multiple datasets, which often introduce the problem of missing data and incompatibilities between datasets. In Chapter 3 we extended the capabilities of the MCVAE framework with a specific optimization scheme that allows the simultaneous learning from multiple datasets, without discarding any observation nor data modality. When a particular data modality is missing from a particular observation, for example if we miss the PET imaging data in the observations set of a patients that includes MRI and other clinical data, it will be simply not contribute to the leaning of the associated data modality parameters, without discarding all the other modalities which can still contribute to the learning of their associated parameters. The presence of at least one common data modality among datasets acts as a link across datasets and allows the information needed for the joint modeling of an MCVAE to flow through all the datasets to the other data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main contributions</head><p>• We introduced a specific optimization scheme allowing the MCVAE to learn simultaneously from multiple datasets, even in the presence of missing data and non-compatible data modalities among all the datasets.</p><p>• The robustness to missing data allowed to reach the same joint modeling performances of the MCVAE with only the 25% of observations with no missing data.</p><p>• Extensive tests for the joint modeling of synthetically generated data and of real multi-modal neuroimaging datasets from independent dementia studies, showed the competitiveness of our method in classification and feature prediction tasks with respect to the state of the art methods.</p><p>• The implementation of the method is open source and freely available online.</p><p>6.2 Future developments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Disjoint representations</head><p>In the work developed in this Thesis, the concept of a shared latent space z between multi-modal observations plays a key role. Although very useful and powerful, it may still be not enough to capture the complexity of real world data.</p><p>An interesting perspective to pursue is to understand what are the peculiarities of each single modality, and how this information, that is not shared with other modalities, is structured. In a more practical sense, the question we may want to answer can be: given a specific medical context, what is the information content in a PET that cannot be predicted from an MRI? In order to answer this question we need to introduce new latent variables and new generative models. In Fig. <ref type="figure" target="#fig_27">6</ref>.1 we compare the current generative model of the MCVAE and MT-MCVAE and possible extensions to take into account modality-specific variables.</p><p>More precisely, in Fig. <ref type="figure" target="#fig_27">6</ref>.1b we propose a simple generative model with one extra latent z c for each modality. A model taking into account the specific contributions of each modality, would probably have a better prediction performance as the latent space would host more information: the shared and the disjoint ones.</p><p>Going one step further the research question may become: what are the latent factors specific to sub-groups of modalities which are disjoint from all the others? In Fig. <ref type="figure" target="#fig_27">6</ref>.1c we propose a generative model for answering this question. In this case we expect even better performances, not only because the latent space would be richer and more structured, but also because the prediction of an i-th modality would benefit simultaneously from the common latent z and the all the other specific and sub-specific latents {z i,j } C j=1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Domain shift compensation</head><p>In Chapter 3 we showed that gathering observations from multiple datasets and modeling them with the MT-MCVAE results in better performances with respect to the MCVAE, which is limited to model one dataset at a time. The MT-MCVAE model is based on the assumption of identical distributed observations across datasets. This assumption may not be necessary true, especially in healthcare datasets where usually dataset releted biases exist. In Ch. 3 we mitigated these biases by harmonizing the datasets with ComBat <ref type="bibr" target="#b59">[Johnson, 2007]</ref> before applying our model. Given these premises, another possible extension of our work would be to embed a domain-shift compensation mechanism to allow the modeling of multi-dataset observations when the datasets are not harmonized.</p><p>In <ref type="bibr" target="#b56">[Ilse, 2020]</ref>, the authors propose the Domain Invariant Variational Autoencoder (DIVA), a VAE based method to to learn unbiased representations given data from a set of datasets, and show that they are able to capture and correct for the biases of the different domains. In   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Temporal modeling</head><p>In Chapter 2 we showed that in the latent space we can disentangle the disease trajectory from the normal aging trajectory. This result emerged from the joint modeling of crosssectional observations, without recurring to the concept of time evolution. For a better understanding of Alzheimer's Disease and other dementia related disorders, it is necessary to further disentangle sub-trajectories associated with different pathological subtypes. Indeed, this is a very active area of research <ref type="bibr" target="#b80">[Lorenzi, 2015;</ref><ref type="bibr" target="#b64">Khanal, 2016;</ref><ref type="bibr" target="#b65">Khanal, 2017]</ref>, By leveraging on the works already developed in the literature, specifically in modeling dynamic phenomena within the Variational Autoencoder framework <ref type="bibr" target="#b44">[Girin, 2020]</ref> applied to longitudinal dementia studies <ref type="bibr" target="#b1">[AbiNader, 2021]</ref>, we propose to generalize our work by embedding it into a longitudinal framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Other applications</head><p>In our Thesis we focused particularly on the topic of data integration for neuroimaging studies in dementia, although the framework developed here for the joint modeling of heterogeneous data is applicable in other contexts, too.</p><p>For example, in the automotive industry cars are becoming more and more equipped with sensors to detect nearby vehicles <ref type="bibr" target="#b78">[Liu, 2021]</ref>, pedestrians <ref type="bibr" target="#b50">[Held, 2021]</ref>, road signs <ref type="bibr" target="#b12">[Barodi, 2020]</ref>, to generally enhance road safety. As these sensors are typically used to gather complementary information about the environment surrounding the car, their integration into a joint model like the one proposed in this Thesis could further increase the overall car safety. Indeed, since a possible sensor failure would result in missing or corrupted data, the compensation coming from all other sensors through the joint modeling would make the inference about the status of the car more robust. Moreover, if temporal dynamics are considered into the modeling, the timely prediction of an imminent danger could be used to adopt preventive safety measurements.</p><p>The same concept can be applied for home safety, for example to monitor the activities of people with compromised autonomy. If we assume to solve all the privacy issues beforehand, here the integration of data coming from the environment itself, such as video cameras or wireless location systems, together with wearable sensors, can be used to robustly detect situation of danger such as falls <ref type="bibr" target="#b132">[Zigel, 2009]</ref>, and as well for assessing the health status through the monitoring of performances during daily activity tasks <ref type="bibr" target="#b106">[Suryadevara, 2012]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Final remarks</head><p>The constant development of mathematical and statistical tools makes it possible to build personalized models for patients, continuously adjustable based on measured health and lifestyle habits assessments. This can ultimately lead to a virtual patient, a digital twin<ref type="foot" target="#foot_29">foot_29</ref> , with detailed description of the state of an individual. With this Thesis we introduced new methodologies allowing to integrate and jointly analyze patients' data in a new and more interpretable manner, contributing to the building of a digital twin, for a better diagnosis, intervention simulation, and treatment, by adopting a data-driven and objective approach to healthcare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glossary</head><p>Alzheimer's Disease is defined by the presence in the brain of extracellular amyloidβ plaques and aggregates of hyperphosphorylated tau in neurofibrillary tangles, independently of the clinical expression of cognitive symptoms <ref type="bibr" target="#b58">[Jack, 2018]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel see View.</head><p>ComBat Harmonization method to remove domain bias when pooling data from different datasets <ref type="bibr" target="#b59">[Johnson, 2007]</ref> .</p><p>CovBat Same as ComBat, with an additional step for covariance harmonization <ref type="bibr">[Chen, 2019a]</ref> .</p><p>Dementia denotes an acquired, insidious, and progressive cognitive and functional impairment. Alzheimer's Disease (AD) is the most common cause of dementia and accounts for 60% to 80% of the cases <ref type="bibr">[Alzheimer Association Report, 2020]</ref> .</p><p>Mild Cognitive Impairment refers to a population without, or with subtle, functional disability, but with an acquired objective cognitive impairment. Representing a clinical syndrome, it encompasses cases progressing to AD (about 50%) or non-AD dementia (about 10 -15%) as well as stable cases (about 35 -40%). MCI cases positive to AD biomarkers can be defined as prodromal AD or MCI due to AD based on research diagnostic criteria <ref type="bibr">[Dubois, 2014]</ref> and consistently also with the 2018 A/T/N framework <ref type="bibr" target="#b58">[Jack, 2018]</ref> .</p><p>Object-oriented programming is a programming paradigm based on the concept of "objects", which can contain data and code: data in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods) .</p><p>View a group of homogeneous features, such as measurements from a specific imaging modality, or clinical scores, or biological measurements, representing an important and independent source of information for the disease or phenomena under investigation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Simple example of a Multi-Task Model learning scheme in the presence of missing not available (NA) views. Arrows represent learnable functions used as network encoders and decoders, transforming respectively input views (e.g., clinical scores, imaging derived phenotypes, . . . ) from the observation space to the representation space (circles) and from the representation space back to the observation space. The separability of the loss function L</p><formula xml:id="formula_54">(x d,n ) v</formula><p>in Eq. (3.2) allows to group together observations into homogeneous learning tasks. For every task, functions associated to missing views (dashed gray arrows) are locally not updated by the learning algorithm. Globally, common latent representations (red circles) across pairs of tasks act as a link allowing the information to flow throughout the views. . . . . . . . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3</head><p>Losses plotted in absolute and relative scale with the plot_loss utility (ll: log-likelihood, kl: Kullback-Leibler divergence; total: kl -ll). . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4</head><p>Multi-channel observations projected in one selected latent dimension z i with the plot_latent_space utility. The utility can optionally take a grouping variable to highlight clusters of points (diagnosis in this figure). . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5</head><p>Mean  <ref type="bibr">)</ref>. In all cases we measure a better performance in the MTL condition with respect to the average STEB one ( †). . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.11</head><p>Diagnosis classification with our model and the EmbraceNet (EN, <ref type="bibr" target="#b27">[Choi, 2019]</ref>). Accuracy in % as mean st.dev. over 5-folds. Results are stratified by the classification task and by the number of layers in the encoder-decoder architecture. We measure no significant difference among architectures depth (anova test, alpha level 0.05) and between models (t-test, alpha level 0.05). . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>Pairwise Frobenius norms between dataset-specific correlation matrices for every harmonization method. We find that ComBat adjustment does not harmonize the correlation matrices whereas CovBat adjustment shows large reductions in the between-datasets distances. The almost perfect reduction of the Frobenius distances in the DIVA-1 linear cases is spurious, as the original correlation structure is very different from the harmonized ones (see <ref type="bibr">Tab. 4.3,</ref><ref type="bibr">Fig. 4.1)</ref>. In the DIVA-4 non-linear case Frobenius distances generally increases, which is not ideal for an harmonization method. . . .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Effect of variational dropout on a synthetic experiment modeled with the Multi-Channel VAE. As expected, the minimum amount of non-zero components of z (left) and generative parameters G (right) is obtained with the sparse model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Figure 2.2: Estimated dropout rates for the latent dimensions when the initial latent dimensions of the Sparse Multi-Channel VAE was set to l fit = 20 on data generated with respectively l = 1, 2, 4, and 10 latent dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Figure 2.3: Testing benchmark of four variational methods applied to the multi-channel scenarios in Tab. 3.12 (cases snr = 10, l fit = l). Sparse Multi-Channel models performs consistently better than non-sparse Multi-Channel ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>6 shows the generative parameters G (µ) c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . 4 :</head><label>24</label><figDesc>Figure 2.4: Stratification of the ADNI subjects (test data) in the sparse latent subspace inferred from the first two least dropped out dimensions. In the same subspace it is possible to stratify subjects in the test-set by: (left) disease status among Alzheimer's Disease (AD), Mild Cognitive Impairment (MCI), Normal Cognition (NC), (right) age, in almost orthogonal directions. Classification accuracy for these subjects is given in the fifth numeric column of Tab. 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 5 :</head><label>25</label><figDesc>Figure 2.5: Generation of imaging data from trajectories in the latent space. (a) Normal aging trajectory (T r 1 ) vs Dementia aging trajectory (T r 2 ) in the latent 2D sub-space. Stars indicate the sampling points along trajectories. The trajectories share the same origin. (b) MRI data evolution. (c) FDG-PET. (d) Amyloid-PET. All the trajectories show a plausible evolution across disease and healthy conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 . 6 :</head><label>26</label><figDesc>Figure 2.6: Generative parameters G (µ) c (cfr. Eq. (2.7)) of the four channels associated to the least dropout latent dimension in the sparse multi-channel model. (Top) Clinical channel parameters. (Bottom) Imaging ch. parameters.</figDesc><graphic coords="43,180.54,142.39,74.63,78.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 . 7 :Figure 2 . 8 :</head><label>2728</label><figDesc>Figure 2.7: Negative lower bound (NLB) on the synthetic training set computed at convergence for all the scenarios. Each bar shows mean ± std.err. of N = 80 total experiments as a function of the number of fitted latent dimensions. Red bars represents experiments where the number of true and fitted latent dimensions coincide. (a) Experimental setup C = 10, d c = 32: NLB stops decreasing when the number of fitted latent dimension coincide with the generated ones; notable gap between the under-fitted and over-fitted experiments (elbow effect). (b) Experimental setup d c = 4 , l = 4: increasing the number of channels C makes the elbow effect more pronounced. (c) Experimental setup C = 10 , d c = 500: with high dimensional data (d c = 500) using the lower bound as a model selection criteria to assess the true number of latent dimensions may end up in overestimation. (d) Restricted (N = 5 total experiments) high quality experimental setup C = 10, d c = 500, S = 10000, SNR = 100: the risk to overestimate the true number of latent dimensions can be mitigated by increasing the SNR and S of the observations in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: General variational framework for our multi-view and multi-dataset model. Compatibly with the MCVAE formulation, for every pair of views w and v there is a prediction path w → v composed by two learnable functions: the encoding distribution q (z|x d,n,w , φ w ) and the decoding likelihood p (x d,n,v |z, θ v ). Parameters φ w and θ v are optimized through Eq. (3.4) to maximize the likelihood of our generative model under the encoding distributions, and at the same time minimize the Kullback-Leibler distance between every encoding distribution and the prior p (z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Figure 3.2: Simple example of a Multi-Task Model learning scheme in the presence of missing not available (NA) views. Arrows represent learnable functions used as network encoders and decoders, transforming respectively input views (e.g., clinical scores, imaging derived phenotypes, . . . ) from the observation space to the representation space (circles) and from the representation space back to the observation space. The separability of the loss function L (x d,n ) vin Eq. (3.2) allows to group together observations into homogeneous learning tasks. For every task, functions associated to missing views (dashed gray arrows) are locally not updated by the learning algorithm. Globally, common latent representations (red circles) across pairs of tasks act as a link allowing the information to flow throughout the views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 . 3 :</head><label>33</label><figDesc>Figure 3.3: Pictorial example of training an imaging dataset with two views: MRI (left side, in gray scale) and FDG-PET (right side, in color scale). In this case we have data from 30 independent observations: 10 with left-views only; 10 with right-views only; 10 with complete views. The fraction of observations with complete views amounts to: f = 1/3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 . 4 :</head><label>34</label><figDesc>Figure 3.4: Mean Squared Error (MSE) of imputation in synthetic held-out datasets(5-folds crossvalidation). Compared to the best competing methods among k-Nearest Neighbor (k = {1, 5}) and Denoise Autoencoder (DAE), our model comes out as the best performer, with a mean MSE improvement of 17% in MAR cases (a) and 71% in MNAR cases(b). Stratification by signal-to-noise ratio (SNR) is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>SNRFigure 3 . 5 :</head><label>35</label><figDesc>Figure 3.5: Mean Squared Error of test sets predictions in synthetic held-out datasets in MAR scenarios. Stratification by SNR and by the fraction f of data-points with complete views is shown. A value of f = 0.25 is enough to reduce the prediction error on testing data-points at the level of the ideal case (f = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>with two output networks: 1) the classic decoding network of the VAE; 2) an auxiliary domain classifier. The latent space z = [z x , z d ] is composed by two independently encoded sub-spaces: z d , regularized by its associated learnable domain-conditioned prior p ψ d (z d |d), and z x , regularized by the Gaussian prior p (z x ) = N (0; I). The domain classifier p θ d (d|z d ) is conditioned only on the sub-space z d , while the decoder p θ (x|z x , z d ) is also conditioned on the sub-space z x . With this setup the latent z x should retain only data information unrelated to the domain. The cost function to be maximized to train a DIVA model is hence built as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Correlation matrices (in range [-1, 1]) before and after MRI gray matter volume harmonization. ComBat-adjusted matrices are visually indistinguishable from Raw, where each center is characterized by its own data covariance, whereas betweencenter differences are still conspicuous, meaning that these covariances are not harmonized. With CovBat covariances are harmonized because between-center differences are less noticeable. With DIVA, both linear an non-linear (4 layers architecture), the original data correlations are lost. All these visual clues are quantified with the Frobenius norm in Tab. 4.2 and Tab. 4.3.</figDesc><graphic coords="85,165.95,524.26,99.25,99.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: The bias inducing matrix L (see Eq. (4.5)) is the Cholesky decomposition of a symmetric positive definite matrix Σ = LL T . One way to create Σ is to randomly generate a matrix of k d-dimensional loadings W ∈ R d×k with k &lt; d, then form covariance matrix WW T and add to it a random diagonal matrix D with positive elements to make WW T + D full rank. The resulting covariance matrix can be normalized to have ones on its diagonal. Here we see examples generated with k ∈ {1, 5, 10} and d = 50 features.</figDesc><graphic coords="87,193.54,429.25,82.75,82.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Label classification accuracy (the higher the better) of a LDA classifier on harmonized synthetic data created by varying the parameters in Tab. 4.4. Random chance level (acc= 0.5) rendered as a dashed line. The accuracy is measured on test observations coming from the dataset not used to train the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: Dataset classification accuracy (the lower the better) of a LDA classifier on harmonized synthetic data created by varying the parameters in Tab. 4.4. Random chance level (acc= 0.5) rendered as a dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Figure 5.1: Generalization of the (a) Variational Autoencoder (VAE) latent variable model to the (b) multi-channel (or multi-view) case, where multiple related views are encoded into and decoded from the same latent space. In (c) further extension to the multi-task case, where a specific optimization scheme allow missing non available (NA) data in the training phase, to jointly model observations from multiple datasets.Arrows represent learnable functions used as network encoders and decoders, transforming respectively input views (e.g., clinical scores, imaging derived phenotypes, . . . ) from the observation space to the representation space (circles) and from the representation space back to the observation space. Globally, common latent representations (red circles) across pairs of tasks act as a link allowing the information to flow throughout the views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>torch.nn.Module mcvae.models.mcvae.Mcvae</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Figure 5.2: Inheritance scheme of the main classes in the mcvae package. Class names/attributes/methods are separated by dashed lines. The user can define her/his own VAE module by redefine new init_encoder() and init_decoder() methods to output the desired distributions (e.g., Normal, Categorical, Bernoulli, etc.). The Mcvae class builds the MCVAE model based on the input data and user defined architecture. The MtMcvae class builds the MT-MCVAE model, where the optimization is guided by the observation identifiers (ids) needed to correctly pair observations between channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>•</head><label></label><figDesc>VAE: basic class for building the Mcvae model (Fig. ??). • ConditionalDistributionNet: basic class for building the VAE model (Fig. ??).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 . 3 :Figure 5 . 4 :</head><label>5354</label><figDesc>Figure 5.3: Losses plotted in absolute and relative scale with the plot_loss utility (ll: loglikelihood, kl: Kullback-Leibler divergence; total: kl -ll).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 5 . 5 : 5 . 4</head><label>5554</label><figDesc>Figure 5.5: Dropout probability of a "sparse" model plotted with the plot_dropout utility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Figure 6.1: (a) Generative model of the Multi-Channel Variational Autoencoder (MCVAE), where a common latent z x is the only source for the observations x c . (b), (c) Possible generative models of a MCVAE with modality specific latent variables, hosting a disentangled, complementary, and richer source of information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 6 .</head><label>6</label><figDesc>2a we show the generative model of the DIVA, where samples z d come from the prior p (z d |d) conditioned on the domain label d. Samples z x comes from a Standard Gaussian as in standard VAE and MCVAE. In Fig. 6.2b we propose a multi-channel extension of the DIVA, where we introduce new latent variables z d,c (one for every channel) coming from the respective prior distributions p (z d,c |d), conditioned on the domain label d as in DIVA.Domain invariance and disjoint modeling can be also combined into a more general model, as the one depicted in Fig.6.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 6 . 2 :</head><label>62</label><figDesc>Figure 6.2: Generative models of: (a) Domain Invariant Variational Autoencoder (DIVA); (b) possible Multi-Channel extension of the DIVA. Samples z d,• come from the prior p (z d,c |d) conditioned on the domain label d. Samples z x comes from a Standard Gaussian as in VAE and MCVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 6 . 3 :</head><label>63</label><figDesc>Figure 6.3: Generative model for a Domain Invariant Multi-Channel Variational Autoencoder with joint (z x ) and disjoint (z c ) latent spaces. It comes from the combination of the models proposed in Fig. 6.1b and Fig. 6.2b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>training an imaging dataset with two views: MRI (left side, in gray scale) and FDG-PET (right side, in color scale). In this case we have data from 30 independent observations: 10 with left-views only; 10 with right-views only; 10 with complete views. The fraction of observations with complete views amounts to: f = 1/3. . . . . . . . . . . . . . . . . . .3.4Mean Squared Error (MSE) of imputation in synthetic held-out datasets(5-folds cross-validation). Compared to the best competing methods among k-Nearest Neighbor (k = {1, 5}) and Denoise Autoencoder (DAE), our model comes out as the best performer, with a mean MSE improvement of 17% in MAR cases (a) and 71% in MNAR cases(b). Stratification by signal-to-noise ratio (SNR) is shown. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Mean Squared Error of test sets predictions in synthetic held-out datasets in MAR scenarios. Stratification by SNR and by the fraction f of data-points with complete views is shown. A value of f = 0.25 is enough to reduce the prediction error on testing data-points at the level of the ideal case (f = 1). 4.1 Correlation matrices (in range [-1, 1]) before and after MRI gray matter volume harmonization. ComBat-adjusted matrices are visually indistinguishable from Raw, where each center is characterized by its own data covariance, whereas between-center differences are still conspicuous, meaning that these covariances are not harmonized. With CovBat covariances are harmonized because between-center differences are less noticeable. With DIVA, both linear an non-linear (4 layers architecture), the original data correlations are lost. All these visual clues are quantified with the Frobenius norm in Tab. 4.2 and Tab. 4.3. . . . . . . . . . . . . . . . . . . . . . . . . 4.2 The bias inducing matrix L (see Eq. (4.5)) is the Cholesky decomposition of a symmetric positive definite matrix Σ = LL T . One way to create Σ is to randomly generate a matrix of k d-dimensional loadings W ∈ R d×k with k &lt; d, then form covariance matrix WW T and add to it a random diagonal matrix D with positive elements to make WW T + D full rank. The resulting covariance matrix can be normalized to have ones on its diagonal. Here we see examples generated with k ∈ {1, 5, 10} and d = 50 features. . . . . . . 4.3 Label classification accuracy (the higher the better) of a LDA classifier on harmonized synthetic data created by varying the parameters in Tab. 4.4. Random chance level (acc= 0.5) rendered as a dashed line. The accuracy is measured on test observations coming from the dataset not used to train the classifier. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Dataset classification accuracy (the lower the better) of a LDA classifier on harmonized synthetic data created by varying the parameters in Tab. 4.4. Random chance level (acc= 0.5) rendered as a dashed line. . . . . . . . . 5.1 Generalization of the (a) Variational Autoencoder (VAE) latent variable model to the (b) multi-channel (or multi-view) case, where multiple related views are encoded into and decoded from the same latent space. In (c) further extension to the multi-task case, where a specific optimization scheme allow missing non available (NA) data in the training phase, to jointly model observations from multiple datasets. Arrows represent learnable functions used as network encoders and decoders, transforming respectively input views (e.g., clinical scores, imaging derived phenotypes, . . . ) from the observation space to the representation space (circles) and from the representation space back to the observation space. Globally, common latent representations (red circles) across pairs of tasks act as a link allowing the information to flow throughout the views. . . . . . . . . . . . . . . . . . . . 5.2 Inheritance scheme of the main classes in the mcvae package. Class names/attributes/methods are separated by dashed lines. The user can define her/his own VAE module by redefine new init_encoder() and init_decoder() methods to output the desired distributions (e.g., Normal, Categorical, Bernoulli, etc.). The Mcvae class builds the MCVAE model based on the input data and user defined architecture. The MtMcvae class builds the MT-MCVAE model, where the optimization is guided by the observation identifiers (ids) needed to correctly pair observations between channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.12 Dataset attributes, varied one-at-a-time in the prescribed ranges, and used to generate scenarios according to Eq. (3.16). . . . . . . . . . . . . . . . . 4.1 MRI Observations stratified by dataset and diagnosis. . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,0.33,-0.00,594.63,303.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data. Luigi Antelmi, Nicholas Ayache, Philippe Robert, and Marco Lorenzi. In ICML 2019 -36th International Conference on Machine Learning, Long Beach, United States, June 2019. Proceedings of Machine Learning Research, (97):302-311, 2019.</figDesc><table /><note><p><p><p><p><p><p><p><ref type="bibr" target="#b8">[Antelmi, 2019]</ref> </p>• Combining</p>Multi-Task Learning and Multi-Channel Variational  </p>Auto-Encoders to Exploit Datasets with Missing Observations -Application to Multi-Modal Neuroimaging Studies in Dementia. Luigi Antelmi, Nicholas Ayache, Philippe Robert, Federica Ribaldi, Valentina Garibotto, Giovanni B Frisoni, and Marco Lorenzi. Under review at NeuroImage 2021. [Antelmi, 2021] • Multi-Chanel Stochastic Variational Inference for the Joint Analysis of Heterogeneous Biomedical Data in Alzheimer's Disease. Luigi Antelmi, Nicholas Ayache, Philippe Robert, and Marco Lorenzi. In Understanding and Interpreting Machine Learning in Medical Image Computing Applications, Granada, Spain, September 2018. Lecture Notes in Computer Science</p>, 11038:15-23, 2018, 11038:15-23,  . [Antelmi, 2018a]   ]    </p>• A method for statistical learning in large databases of heterogeneous imaging, cognitive and behavioral data. Luigi Antelmi, Marco Lorenzi, Valeria Manera, Philippe Robert, and Nicholas Ayache. EPICLIN 2018, Poster session.</p>[Antelmi, 2018b]   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.1 Multi-Channel Variational Autoencoder (MCVAE) . . . . . . . 14</figDesc><table><row><cell>Multi-Channel Variational</cell><cell>2</cell></row><row><cell>Autoencoder</cell><cell></cell></row><row><cell>Contents</cell><cell></cell></row><row><cell>2.1</cell><cell></cell></row></table><note><p>2.2.2 Inducing Sparse Latent Representations . . . . . . . . . . . . . 18 2.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3.1 Synthetic Experiments . . . . . . . . . . . . . . . . . . . . . . 21 2.3.2 Sparse Multi-Channel VAE Benchmark . . . . . . . . . . . . . 21 2.3.3 Comparison with VAE . . . . . . . . . . . . . . . . . . . . . . 23 2.3.4 Medical Imaging data . . . . . . . . . . . . . . . . . . . . . . . 24 2.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.5 Supplementary Material . . . . . . . . . . . . . . . . . . . . . . . . . . 29</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .1: Dataset</head><label>2</label><figDesc>attributes, varied one-at-a-time in the prescribed ranges, and used to generate scenarios according to Eq. (2.18).</figDesc><table><row><cell>Attribute description</cell><cell>Iteration list</cell></row><row><cell>Total channels (C)</cell><cell>2 3 5 10</cell></row><row><cell>Channel dimension (d c )</cell><cell>32</cell></row><row><cell>Latent space dimension (l)</cell><cell>1 2 4 10 20</cell></row><row><cell cols="2">Samples (training and testing) 100 1000</cell></row><row><cell>Signal-to-noise ratio (SNR)</cell><cell>10 1</cell></row><row><cell>Seed (re-initialize R c )</cell><cell>1 2 3 4 5</cell></row><row><cell cols="2">Datasets x = {x c } with c = 1 . . . C channels where created according to the following</cell></row><row><cell>model:</cell><cell></cell></row><row><cell>z ∼ N (0; I l ) ,</cell><cell></cell></row><row><cell>∼ N (0; I dc ) ,</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Compr. Factor</cell><cell>0%</cell><cell>45%</cell><cell>0%</cell></row></table><note><p>2: Benchmark with respect to VAE. (top) Bootstrapped 95% C.I. for the mean absolute error (MAE) difference between each model MAE and the reference MAE of the VAE. (bottom) Average compression factor. MCVAE sMCVAE IVAEs 95% CI [-.13; +.03] [-.12; +.04] [-.10; +.06]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 . 3 :</head><label>23</label><figDesc>Proportion of correctly classified ADNI subjects belonging to the testing hold-out dataset. Classification done by means of Linear Discriminant Analysis using as training data the latent space inferred with the sparse and non sparse models. 10-fold cross validation mean results shown. Within the sparse framework, we selected the subspace generated by the most relevant latent dimensions identified by variational dropout (p &lt; 0.2).</figDesc><table><row><cell>Model:</cell><cell></cell><cell cols="2">MCVAE</cell><cell></cell><cell></cell><cell cols="2">sMCVAE</cell><cell></cell><cell></cell><cell cols="2">IVAEs</cell><cell></cell><cell></cell><cell>VAE</cell><cell></cell><cell></cell></row><row><cell>#layers:</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Normal</cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Multi-Channel Variational Autoencoder Contents 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Generative Model . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Inference Model . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table /><note><p>3.2.3 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 Comparison with VAE and MCVAE . . . . . . . . . . . . . . . 3.2.5 Parameterization . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Illustration on a simplified brain imaging dataset . . . . . . . 3.3.2 Synthetic Experiments . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Experiments on Brain Imaging Data . . . . . . . . . . . . . . . 3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Supplementary Material . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6.1 Derivation of the Lower Bound . . . . . . . . . . . . . . . . . 3.6.2 Data Generation . . . . . . . . . . . . . . . . . . . . . . . . .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.2 the learning Algorithm 1 Multi-view model optimization.</figDesc><table><row><cell>Require:</cell></row><row><cell>Set the dimensionality of z.</cell></row><row><cell>Initialize the model parameters φ, θ.</cell></row><row><cell>Set the optimizer learning rate.</cell></row><row><cell>while φ, θ not converged do</cell></row><row><cell>Initialize the total cost:</cell></row><row><cell>L ← 0</cell></row><row><cell>for every dataset d ∈ D do</cell></row><row><cell>for every datapoint x d,n , n ∈ N d do</cell></row><row><cell>for every view v ∈ V d,n do</cell></row><row><cell>Accumulate the cost of predicting v from w:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 . 1 :</head><label>31</label><figDesc>The Multi-Task Multi-Channel VAE (MT-MCVAE) extends the MCVAE, which is itself an extension of the VAE.</figDesc><table><row><cell>Method</cell><cell cols="3">Train with missing data Test with missing data # views modeled</cell></row><row><cell>VAE</cell><cell>no</cell><cell>no</cell><cell>1</cell></row><row><cell>MCVAE</cell><cell>no</cell><cell>yes</cell><cell>&gt; 1</cell></row><row><cell>MT-MCVAE</cell><cell>yes</cell><cell>yes</cell><cell>&gt; 1</cell></row><row><cell cols="4">estimate missing views xd,n,v from the available ones through the formula:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 . 2 :</head><label>32</label><figDesc>Mean squared error (MSE) and negative log-likelihood (NLL) -the lower the better -measured as mean st.dev. on the reconstructed brain images of the test-set. The MRI were used to infer the FDG-PET slices in the same subject, and vice versa. Results stratified by f , the fraction of observations with no missing views in the training set. Notice the immediate drop in the error metrics as soon as f increases. NLL 96.44 10.33 0.53 0.09 0.16 0.12 -0.07 0.07 -2.63 0.03</figDesc><table><row><cell>f</cell><cell>0.00</cell><cell>0.25</cell><cell>0.50</cell><cell>0.75</cell><cell>1.00</cell></row><row><cell cols="2">MSE 40.72 4.31</cell><cell cols="3">1.77 0.04 1.63 0.06 1.54 0.03</cell><cell>1.51 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 . 3 :</head><label>33</label><figDesc>Number of subjects per view available in each dataset. The last columns provide the size of the intersection (∩) and union (∪) of subjects with available views. Notice how in the jont set no subject has all the modalities. View # features : clin 2 MRI 99 FDG 94 AV45 94 AV1451 94 ∩ ∪</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adni1</cell><cell>740</cell><cell>730</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">730 740</cell></row><row><cell>Adni2</cell><cell>1324</cell><cell>710</cell><cell>424</cell><cell>417</cell><cell>61</cell><cell cols="2">53 1324</cell></row><row><cell>Miriad</cell><cell>67</cell><cell>67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67</cell><cell>67</cell></row><row><cell>Oasis3</cell><cell>529</cell><cell>489</cell><cell>-</cell><cell>148</cell><cell>-</cell><cell cols="2">147 529</cell></row><row><cell>Geneva</cell><cell>999</cell><cell>-</cell><cell>65</cell><cell>120</cell><cell>54</cell><cell>15</cell><cell>999</cell></row><row><cell>Tot. subjects</cell><cell>3659</cell><cell>1996</cell><cell>489</cell><cell>685</cell><cell>115</cell><cell>0</cell><cell>3659</cell></row><row><cell>Tot. datasets</cell><cell>5</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 . 4 :</head><label>34</label><figDesc>Mean Squared Error (MSE) of test data from Adni2. All models were trained on all the available datasets by holding-out data from the Adni2 test dataset. 5-folds cross validation of MSE is shown as mean st.dev. . Best results in boldface are significant with an α level of 0.01 with respect to both competing methods.</figDesc><table><row><cell>View</cell><cell></cell><cell>model</cell></row><row><cell></cell><cell>DAE</cell><cell>KNN5</cell><cell>ours</cell></row><row><cell>clin</cell><cell cols="3">0.73 0.14 0.44 0.05 0.45 0.07</cell></row><row><cell>MRI</cell><cell cols="3">1.23 0.31 0.88 0.15 0.70 0.13</cell></row><row><cell>FDG</cell><cell cols="3">4.20 0.56 4.15 0.59 1.09 0.15</cell></row><row><cell>AV45</cell><cell cols="3">1.45 0.35 1.20 0.25 0.89 0.15</cell></row><row><cell cols="4">AV1451 1.54 0.82 1.44 0.83 1.05 0.45</cell></row><row><cell cols="4">5. AV1451: average brain tau protein aggregates measured through the analysis of</cell></row><row><cell>AV1451-PET scans.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 3 . 5 :</head><label>35</label><figDesc>Mean Squared Reconstruction Error (the lower the better) measured on test dataset views (clinical scores and imaging derived phenotypes) predicted with the Multi-Channel VAE (MCVAE) and the Multi Task MCVAE (MT-MCVAE). 5-folds cross-validation results shown as mean st.dev. . Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset. Experiments were run in two different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case; In all cases the MT-MCVAE performs either similarly or statistically better than the MCVAE, with alpha levels at 0.05 (*), 0.01 (**), and 0.001 (***).</figDesc><table><row><cell></cell><cell></cell><cell>view</cell><cell></cell><cell>clin</cell><cell></cell><cell></cell><cell>MRI</cell><cell></cell><cell></cell><cell>FDG</cell><cell></cell><cell></cell><cell>AV45</cell><cell>AV1451</cell></row><row><cell></cell><cell></cell><cell>model</cell><cell cols="11">MCVAE MT-MCVAE MCVAE MT-MCVAE MCVAE MT-MCVAE MCVAE MT-MCVAE MCVAE MT-MCVAE</cell></row><row><cell cols="3">test on condition train on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adni1</cell><cell>STIB</cell><cell>Adni1</cell><cell cols="2">0.90 0.12 0.89 0.13</cell><cell></cell><cell cols="2">0.85 0.11 0.83 0.12</cell><cell>*</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni2</cell><cell cols="2">0.91 0.17 0.77 0.13</cell><cell>*</cell><cell cols="2">1.02 0.23 0.85 0.11</cell><cell>*  *  *</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell cols="2">0.96 0.17 1.14 0.27</cell><cell></cell><cell cols="2">0.80 0.14 0.82 0.13</cell><cell>*</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Geneva -</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell cols="2">0.83 0.30 0.54 0.10</cell><cell>*</cell><cell cols="2">0.80 0.15 0.76 0.11</cell><cell>*</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Adni2</cell><cell>STIB</cell><cell>Adni2</cell><cell cols="2">0.83 0.11 0.73 0.15</cell><cell></cell><cell cols="2">0.74 0.13 0.70 0.11</cell><cell>*  *</cell><cell cols="2">0.73 0.14 0.59 0.10</cell><cell>*  *  *</cell><cell cols="2">1.03 0.19 0.80 0.10</cell><cell>*  *  *</cell><cell>1.33 0.5</cell><cell>1.18 0.52</cell><cell>*</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell cols="2">0.77 0.18 0.80 0.14</cell><cell></cell><cell cols="2">0.74 0.11 0.75 0.12</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell cols="2">0.73 0.20 0.71 0.18</cell><cell></cell><cell cols="2">0.78 0.13 0.77 0.13</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">Geneva 0.47 0.06 0.48 0.09</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">1.40 0.21 1.09 0.15</cell><cell>*  *  *</cell><cell cols="2">1.10 0.21 0.91 0.15</cell><cell>*  *</cell><cell>1.34 0.52 1.05 0.45</cell><cell>*  *  *</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell cols="2">0.76 0.23 0.61 0.13</cell><cell></cell><cell cols="2">0.68 0.12 0.68 0.11</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">1.32 0.29 1.13 0.26</cell><cell>*  *  *</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Geneva STIB</cell><cell cols="3">Geneva 0.79 0.34 0.98 0.52</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">3.63 1.35 3.18 1.04</cell><cell>*</cell><cell cols="2">1.82 0.57 1.76 0.47</cell><cell>*</cell><cell>1.27 0.82 1.19 0.67</cell><cell>*</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell cols="2">2.57 1.09 2.07 1.05</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">3.01 1.05 2.69 0.77</cell><cell>*</cell><cell cols="2">1.92 0.90 1.41 0.39</cell><cell>1.81 0.81 1.42 0.66</cell><cell>*  *  *</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell cols="2">1.93 0.66 2.28 0.89</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">1.70 0.51 1.63 0.55</cell><cell>*</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Miriad STIB</cell><cell>Miriad</cell><cell cols="2">3.21 1.07 3.23 2.55</cell><cell></cell><cell cols="2">6.39 1.57 6.38 1.52</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell cols="2">6.90 3.33 6.49 3.42</cell><cell></cell><cell cols="2">6.60 1.61 6.73 1.55</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell cols="2">5.60 2.76 3.97 3.14</cell><cell></cell><cell cols="2">5.93 1.90 6.59 1.64</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Geneva -</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell cols="2">6.80 6.52 6.24 4.62</cell><cell></cell><cell cols="2">6.29 1.68 6.23 1.40</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Oasis3 STIB</cell><cell>Oasis3</cell><cell cols="2">0.83 0.33 0.68 0.28</cell><cell></cell><cell cols="2">0.68 0.13 0.66 0.12</cell><cell>*</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">1.58 0.63 1.22 0.26</cell><cell>*  *  *</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell cols="2">1.20 0.25 1.23 0.28</cell><cell></cell><cell cols="2">0.78 0.14 0.79 0.14</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell cols="2">1.11 0.33 1.09 0.24</cell><cell></cell><cell cols="2">0.89 0.18 0.76 0.15</cell><cell>*  *  *</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="2">0.94 0.22 1.02 0.26</cell><cell>*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell cols="2">0.98 0.21 1.02 0.20</cell><cell></cell><cell cols="2">0.83 0.18 0.83 0.18</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">Geneva 0.55 0.28 0.49 0.26</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>1.23</cell></row></table><note><p>0.61 1.11 0.26 * --</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 3 . 6 :</head><label>36</label><figDesc>Mean Squared Reconstruction Error (the lower the better) measured on test dataset views (clinical scores and imaging derived phenotypes) predicted with our model. 5-folds cross-validation results shown as as mean st.dev. . Models were trained on all the available views in the training dataset, independently of their presence in the testingResultsIn Tab. 3.5 and Tab. 3.6 we show the prediction error in terms of MSE for each test dataset and view, on the three experimental conditions described earlier.</figDesc><table><row><cell cols="6">dataset. Experiments were run in three different conditions: 1) when training and</cell></row><row><cell cols="6">testing data are chosen from the same dataset, or Single Task with Internal Benchmark</cell></row><row><cell cols="6">(STIB) learning case; 2) when models trained on one dataset are tested on another</cell></row><row><cell cols="6">dataset, or Single Task with External Benchmark (STEB) case; 3) when models are</cell></row><row><cell cols="6">trained on all the available datasets except the testing one, or Multi Task Learning</cell></row><row><cell cols="6">(MTL). We measure a better performance in the MTL condition with respect to the</cell></row><row><cell cols="6">STIB ( §) in 7/12 of cases, and in 10/12 of cases with respect to the average STEB ( †)</cell></row><row><cell>experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>view</cell><cell>clin</cell><cell>MRI</cell><cell></cell><cell>AV45</cell></row><row><cell cols="2">test dataset condition</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adni1</cell><cell>STIB</cell><cell>0.89 0.13</cell><cell>0.83 0.12</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell cols="2">STEB (avg) 0.82 0.17</cell><cell>0.81 0.12</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell>MTL</cell><cell cols="4">0.45 0.07  § † 0.77 0.10  § † -</cell></row><row><cell>Adni2</cell><cell>STIB</cell><cell>0.73 0.15</cell><cell>0.70 0.11</cell><cell></cell><cell>0.80 0.10</cell></row><row><cell></cell><cell cols="2">STEB (avg) 0.65 0.14</cell><cell>0.73 0.12</cell><cell></cell><cell>1.02 0.21</cell></row><row><cell></cell><cell>MTL</cell><cell cols="3">0.45 0.07  § † 0.70 0.13  †</cell><cell>0.89 0.15  †</cell></row><row><cell>Geneva</cell><cell>STIB</cell><cell>0.98 0.52</cell><cell>-</cell><cell></cell><cell>1.76 0.47</cell></row><row><cell></cell><cell cols="2">STEB (avg) 2.18 0.97</cell><cell>-</cell><cell></cell><cell>1.52 0.47</cell></row><row><cell></cell><cell>MTL</cell><cell>1.80 1.16  †</cell><cell>-</cell><cell></cell><cell>1.35 0.37  § †</cell></row><row><cell>Miriad</cell><cell>STIB</cell><cell>3.23 2.55</cell><cell>6.38 1.52</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell cols="2">STEB (avg) 5.57 3.73</cell><cell>6.52 1.53</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell>MTL</cell><cell cols="4">2.31 1.65  § † 6.17 1.37  § † -</cell></row><row><cell>Oasis3</cell><cell>STIB</cell><cell>0.68 0.28</cell><cell>0.66 0.12</cell><cell></cell><cell>1.22 0.26</cell></row><row><cell></cell><cell cols="2">STEB (avg) 0.96 0.25</cell><cell>0.79 0.16</cell><cell></cell><cell>1.07 0.26</cell></row><row><cell></cell><cell>MTL</cell><cell>0.72 0.09  †</cell><cell>0.81 0.15</cell><cell></cell><cell>1.09 0.30  §</cell></row><row><cell cols="6">Table 3.7: Mean Squared Reconstruction Error (mean (st.dev.), the lower the better) measured</cell></row><row><cell cols="6">on clinical scores and imaging derived phenotypes predicted with our MT-MCVAE</cell></row><row><cell cols="6">model in MTL experiments. Results stratified by the number of layers in the encoder-</cell></row><row><cell cols="6">decoder architecture. We measure no significant differences among architectures</cell></row><row><cell cols="6">(anova statistical test at an alpha level of 0.05). Best overall results in boldface.</cell></row><row><cell cols="2">#layers 1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>clin</cell><cell cols="5">0.97 0.49 1.05 0.65 1.04 0.60 1.02 0.50</cell></row><row><cell>MRI</cell><cell cols="5">2.09 0.92 2.14 0.69 2.13 0.68 2.13 0.68</cell></row><row><cell>AV45</cell><cell cols="5">1.09 0.29 1.16 0.25 1.15 0.26 1.15 0.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.3 Experiments</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 3 . 9 :</head><label>39</label><figDesc>Experiment of diagnosis classification run with the Multi-Channel VAE (MCVAE) and the Multi Task MCVAE (MT-MCVAE). 5-folds classification accuracy in % is shown as mean (standard deviation). Since there are no MCI in miriad and oasis3 datasets, the classification tests ' AD vs MCI' and 'MCI vs NC' are meaningless and not reported. Since there are no NC in the geneva dataset, the classification tests ' AD vs NC' and 'MCI vs NC' are meaningless and not reported. Experiments were run in two different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case. In all cases the MT-MCVAE model performs either similarly or statistically better than the MCVAE, with alpha levels at 0.05 (*), 0.01 (**), and 0.001 (***).</figDesc><table><row><cell></cell><cell></cell><cell>classification task</cell><cell cols="2">AD vs MCI</cell><cell></cell><cell cols="2">AD vs NC</cell><cell></cell><cell></cell><cell></cell><cell>MCI vs NC</cell></row><row><cell></cell><cell></cell><cell>model</cell><cell>MCVAE</cell><cell cols="2">MT-MCVAE</cell><cell>MCVAE</cell><cell cols="5">MT-MCVAE MCVAE MT-MCVAE</cell></row><row><cell cols="3">test dataset condition train dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adni1</cell><cell>STIB</cell><cell>Adni1</cell><cell>72.70 3.72</cell><cell>72.87 4.37</cell><cell></cell><cell>81.69 2.97</cell><cell>81.51 3.14</cell><cell></cell><cell></cell><cell cols="2">62.00 8.91 62.90 8.72</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni2</cell><cell>47.48 3.56</cell><cell>58.96 3.55</cell><cell>*  *  *</cell><cell>68.50 4.86</cell><cell>73.77 2.80</cell><cell cols="2">*</cell><cell cols="2">53.12 6.42 59.65 2.76</cell><cell>*</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell>-</cell><cell>-</cell><cell></cell><cell>82.58 4.75</cell><cell>80.82 3.16</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>48.57 6.48</cell><cell>62.31 6.43</cell><cell cols="2">*  *</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Geneva</cell><cell>36.52 5.29</cell><cell>46.61 8.03</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Adni2</cell><cell>STIB</cell><cell>Adni2</cell><cell>50.58 3.90</cell><cell>80.07 2.53</cell><cell>*  *  *</cell><cell>82.86 3.28</cell><cell>87.92 3.46</cell><cell cols="2">*</cell><cell cols="2">58.63 4.27 65.56 1.11</cell><cell>*  *</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell>57.59 2.61</cell><cell>58.23 2.87</cell><cell></cell><cell>64.21 3.36</cell><cell>64.21 3.52</cell><cell></cell><cell></cell><cell cols="2">63.05 2.00 62.75 1.80</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell>-</cell><cell>-</cell><cell></cell><cell>70.32 7.29</cell><cell>70.20 7.17</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>68.24 2.97</cell><cell>75.72 1.90</cell><cell cols="2">*  *</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Geneva</cell><cell>64.49 2.98</cell><cell>63.98 3.30</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Geneva</cell><cell>STIB</cell><cell>Geneva</cell><cell>65.76 3.62</cell><cell>77.70 8.12</cell><cell>*</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell>29.17 5.87</cell><cell>30.08 5.49</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell cols="2">38.61 15.08 70.11 2.90</cell><cell>*  *</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Miriad</cell><cell>STIB</cell><cell>Miriad</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="3">83.85 13.84 86.70 15.68</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="3">74.18 14.37 74.18 14.37</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="3">74.95 11.58 78.90 11.54</cell><cell>*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Oasis3</cell><cell>-</cell><cell>-</cell><cell></cell><cell cols="3">45.71 18.08 66.04 19.35</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>Oasis3</cell><cell>STIB</cell><cell>Oasis3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>74.47 2.49</cell><cell>80.35 3.59</cell><cell cols="2">*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>STEB</cell><cell>Adni1</cell><cell>-</cell><cell>-</cell><cell></cell><cell>49.16 6.34</cell><cell>48.22 5.78</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Adni2</cell><cell>-</cell><cell>-</cell><cell></cell><cell>67.86 3.80</cell><cell>75.42 4.68</cell><cell cols="2">*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Miriad</cell><cell>-</cell><cell>-</cell><cell></cell><cell>64.48 8.65</cell><cell>62.02 9.74</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 3 .</head><label>3</label><figDesc>10: Experiment of diagnosis classification run with our model. 5-folds classification accuracy in % is shown as mean (standard deviation). Experiments were run in three different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case; 3) when models are trained on all the available datasets except the testing one, or Multi Task Learning (MTL). In all cases we measure a better performance in the MTL condition with respect to the average STEB one ( †).</figDesc><table><row><cell></cell><cell cols="3">classification task AD vs MCI AD vs NC</cell></row><row><cell cols="2">test dataset condition</cell><cell></cell><cell></cell></row><row><cell>Adni1</cell><cell>STIB</cell><cell>72.87 4.37</cell><cell>81.51 3.14</cell></row><row><cell></cell><cell>STEB (avg)</cell><cell>52.79 5.79</cell><cell>72.30 4.13</cell></row><row><cell></cell><cell>MTL</cell><cell cols="2">59.30 2.08  † 81.86 3.26  †</cell></row><row><cell>Adni2</cell><cell>STIB</cell><cell>80.07 2.53</cell><cell>87.92 3.46</cell></row><row><cell></cell><cell>STEB (avg)</cell><cell>61.11 3.09</cell><cell>70.04 4.20</cell></row><row><cell></cell><cell>MTL</cell><cell cols="2">67.82 1.91  † 85.16 2.13  †</cell></row><row><cell>Geneva</cell><cell>STIB</cell><cell>77.70 8.12</cell><cell>-</cell></row><row><cell></cell><cell>STEB (avg)</cell><cell>50.10 4.20</cell><cell>-</cell></row><row><cell></cell><cell>MTL</cell><cell cols="2">52.54 4.82  † -</cell></row><row><cell>Miriad</cell><cell>STIB</cell><cell>-</cell><cell>86.70 15.68</cell></row><row><cell></cell><cell>STEB (avg)</cell><cell>-</cell><cell>73.04 15.09</cell></row><row><cell></cell><cell>MTL</cell><cell>-</cell><cell>98.46 3.44  †</cell></row><row><cell>Oasis3</cell><cell>STIB</cell><cell>-</cell><cell>80.35 3.59</cell></row><row><cell></cell><cell>STEB (avg)</cell><cell>-</cell><cell>61.89 6.73</cell></row><row><cell></cell><cell>MTL</cell><cell>-</cell><cell>77.70 4.22  †</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 3 .11:</head><label>3</label><figDesc>Diagnosis classification with our model and the EmbraceNet (EN,<ref type="bibr" target="#b27">[Choi, 2019]</ref>). Accuracy in % as mean st.dev. over 5-folds. Results are stratified by the classification task and by the number of layers in the encoder-decoder architecture. We measure no significant difference among architectures depth (anova test, alpha level 0.05) and between models (t-test, alpha level 0.05).</figDesc><table><row><cell>Condition: MTL (avg)</cell><cell cols="2">AD vs NC</cell><cell cols="2">AD vs MCI</cell></row><row><cell></cell><cell>ours</cell><cell>EN</cell><cell>ours</cell><cell>EN</cell></row><row><cell># layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="4">85.79 3.26 85.34 2.30 59.89 2.94 61.02 3.47</cell></row><row><cell>2</cell><cell cols="4">79.04 5.56 77.68 4.86 56.93 5.02 61.38 3.21</cell></row><row><cell>3</cell><cell cols="4">79.78 5.92 78.60 5.34 56.55 5.33 62.07 3.59</cell></row><row><cell>4</cell><cell cols="4">82.47 4.11 77.12 7.22 57.49 6.03 61.29 5.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 3 .12:</head><label>3</label><figDesc>Dataset attributes, varied one-at-a-time in the prescribed ranges, and used to generate scenarios according to Eq. (3.16). Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.2.1 ComBat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.2.2 CovBat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.2.3 Domain Invariant Variational Autoencoder . . . . . . . . . . . 67</figDesc><table><row><cell>Attribute description</cell><cell>Iteration list</cell></row><row><cell>Total views (V )</cell><cell>3 4 5</cell></row><row><cell>Features per view (f v )</cell><cell>5 10 100</cell></row><row><cell cols="2">Latent space dimension (l) 2 4 8</cell></row><row><cell>Training Samples</cell><cell>100 500 1000</cell></row><row><cell>Testing Samples</cell><cell>1000</cell></row><row><cell cols="2">Signal-to-noise ratio (SNR) 1 3 10 100</cell></row><row><cell>Seed (re-initialize R v )</cell><cell>1 2 3 4 5</cell></row></table><note><p>4.2.4 Qualitative benchmark . . . . . . . . . . . . . . . . . . . . . . 68 4.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.3.1 Synthetic data generation procedure . . . . . . . . . . . . . . 71 4.3.2 Quantitative benchmark . . . . . . . . . . . . . . . . . . . . . 73 4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 4 .1: MRI</head><label>4</label><figDesc>Observations stratified by dataset and diagnosis.</figDesc><table><row><cell></cell><cell cols="2">diagnosis</cell><cell></cell><cell>Age</cell></row><row><cell></cell><cell cols="4">AD MCI NC Total mean st.dev. [min, max]</cell></row><row><cell>Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>adni1</cell><cell cols="2">399 168 163</cell><cell>730</cell><cell>75.19 6.94 [54, 91]</cell></row><row><cell>adni2</cell><cell cols="2">222 268 220</cell><cell>710</cell><cell>71.97 6.96 [55, 90]</cell></row><row><cell>oasis3</cell><cell>121</cell><cell>1 367</cell><cell>489</cell><cell>70.62 9.32 [42, 97]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 4 . 3 :</head><label>43</label><figDesc>Pairwise Frobenius norms between between correlation matrices of harmonized and raw data for every dataset. With ComBat the original covariance matrices are unchanged (|| • || F = 0). With CovBat the harmonization tends to slightly change the original covariance matrices (|| • || F &lt; 10) to harmonize them, while with both DIVA methods the original covariance structures become very different from the original ones (|| • || F &gt; 40).</figDesc><table><row><cell>4 (non-linear)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 4 . 4 :</head><label>44</label><figDesc>Parameters, varied one-at-a-time in the prescribed ranges, used to generate synthetic scenarios for the experimental campaign.</figDesc><table><row><cell>Parameter description</cell><cell>Iteration list</cell></row><row><cell>total features</cell><cell>50 100 1000</cell></row><row><cell>informative features</cell><cell>1/3 of total</cell></row><row><cell>redundant features</cell><cell>1/3 of total</cell></row><row><cell cols="2">non-informative features 1/3 of total</cell></row><row><cell cols="2">observations per features 1 2 5 10</cell></row><row><cell>complexity of L d (k)</cell><cell>1 3 5 7 9</cell></row><row><cell>additive bias (α d )</cell><cell>+10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Experimentsnumber of training examples given the same number of features in the datasets. We also notice that when the bias complexity "k" induced by the linear transformation L d increases, CovBat seems to better harmonize the data, especially in cases with high number of features.</figDesc><table /><note><p>4.3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Class names/attributes/methods are separated by dashed lines. The user can define her/his own VAE module by redefine new init_encoder() and init_decoder() methods to output the desired distributions (e.g.,Normal, Categorical, Bernoulli, etc.). The Mcvae class builds the MCVAE model based on the input data and user defined architecture. The MtMcvae class builds the MT-MCVAE model, where the optimization is guided by the observation identifiers (ids) needed to correctly pair observations between channels.In Listing 5.1 we can see a bare minimum script to fit a MCVAE model. The basic VAE class is used to build the finally architecture of the MCVAE.The user can also define new VAE based building blocks depending on his/her modeling purposes. In Listing 5.2 we show the prototype of a simple VAE block. Code to fit a MCVAE model. The user defined VAE class is used as a prototype to build the MCVAE.</figDesc><table><row><cell>def __init__ ( self , in_features , out_features ) :</cell></row><row><cell>super () . __init__ ()</cell></row><row><cell>self . mu = torch . nn . Linear ( in_features , out_features )</cell></row><row><cell>self . logvar = torch . nn . Linear ( in_features , out_features )</cell></row><row><cell>Working examples for fitting MCVAE and MT-MCVAE models are provided within the def forward ( self , x ) :</cell></row><row><cell>released python package. It includes examples of modeling synthetic datasets with and loc = self . mu ( x )</cell></row><row><cell>without missing data. scale = self . logvar ( x ) . exp () . pow (0.5)</cell></row><row><cell># !/ usr / bin / env python return Normal ( loc = loc , scale = scale )</cell></row><row><cell>from torch . optim import Adam</cell></row><row><cell>from mcvae . models import Mcvae , VAE</cell></row><row><cell>from mcvae . models . utils import load_or_fit class MyVAE ( torch . nn . Module ) :</cell></row><row><cell>from mcvae . diagnostics import plot_loss</cell></row><row><cell>def __init__ ( self , n_feats , lat_dim , * args , ** kwargs ) :</cell></row><row><cell># X must be a list of C tensors ,</cell></row><row><cell># corresponding to the channels / views you want to model jointly . super () . __init__ ()</cell></row><row><cell>X = torch . load ( ' my_data_file . pt ')</cell></row><row><cell>self . encode = C o n d i t i o n a l D i s t r i b u t i o n N e t (</cell></row><row><cell># check that there are no missing data in_features = n_feats ,</cell></row><row><cell>n = len ( X [0]) out_features = lat_dim ,</cell></row><row><cell>for x in X : )</cell></row><row><cell>assert len ( x ) == n self . decode = C o n d i t i o n a l D i s t r i b u t i o n N e t (</cell></row><row><cell>in_features = lat_dim ,</cell></row><row><cell># Instantiate an empty model out_features = n_feats ,</cell></row><row><cell># by choosing the number of latent dimensions</cell></row><row><cell># and set up the sparsity flag</cell></row><row><cell>model = Mcvae (</cell></row><row><cell>data =X , lat_dim =15 ,</cell></row><row><cell>vaeclass = VAE ,</cell></row><row><cell>sparse = True ,</cell></row><row><cell>)</cell></row><row><cell># Choose an optimizer and a learning rate</cell></row><row><cell>model . optimizer = Adam ( params = model . parameters () , lr =1 e -3)</cell></row><row><cell># Load the model from ' ptfile ' if exists</cell></row><row><cell># otherwise fit it and save it to ' ptfile '.</cell></row><row><cell>load_or_fit ( model , data =X , epochs =10000 , ptfile = ' my_model . pt ')</cell></row><row><cell>Listing 5.1: import torch</cell></row><row><cell>from torch . distributions import Normal , kl_divergence</cell></row></table><note><p>class C o n d i t i o n a l D i s t r i b u t i o n N e t ( torch . nn . Module ) :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>Squared Reconstruction Error (the lower the better) measured on test dataset views (clinical scores and imaging derived phenotypes) predicted with the Multi-Channel VAE (MCVAE) and the Multi Task MCVAE (MT-MCVAE). 5-folds cross-validation results shown as mean st.dev. . Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset. Experiments were run in two different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case; In all cases the MT-MCVAE performs either similarly or statistically better than the MCVAE, with alpha levels at 0.05 (*), 0.01 (**), and 0.001 (***). . . . . . . . . . . . . . . . . .3.6Mean Squared Reconstruction Error (the lower the better) measured on test dataset views (clinical scores and imaging derived phenotypes) predicted with our model. 5-folds cross-validation results shown as as mean st.dev. . Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset. Experiments were run in three different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case; 3) when models are trained on all the available datasets except the testing one, or Multi Task Learning (MTL). We measure a better performance in the MTL condition with respect to the STIB ( §) in 7/12 of cases, and in 10/12 of cases with respect to the average STEB ( †) experiments. . . . . . . . . . . . . . 3.7 Mean Squared Reconstruction Error (mean (st.dev.), the lower the better) measured on clinical scores and imaging derived phenotypes predicted with our MT-MCVAE model in MTL experiments. Results stratified by the number of layers in the encoder-decoder architecture. We measure no significant differences among architectures (anova statistical test at an alpha level of 0.05). Best overall results in boldface. . . . . . . . . . . . . . . . . . . . . Experiment of diagnosis classification run with the Multi-Channel VAE (MC-VAE) and the Multi Task MCVAE (MT-MCVAE). 5-folds classification accuracy in % is shown as mean (standard deviation). Since there are no MCI in miriad and oasis3 datasets, the classification tests ' AD vs MCI' and 'MCI vs NC' are meaningless and not reported. Since there are no NC in the geneva dataset, the classification tests ' AD vs NC' and 'MCI vs NC' are meaningless and not reported. Experiments were run in two different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case. In all cases the MT-MCVAE model performs either similarly or statistically better than the MCVAE, with alpha levels at 0.05 (*), 0.01 (**), and 0.001 (***). . . . . . . . . . . . . . . . . . . . . . . . . . . 3.10 Experiment of diagnosis classification run with our model. 5-folds classification accuracy in % is shown as mean (standard deviation). Experiments were run in three different conditions: 1) when training and testing data are chosen from the same dataset, or Single Task with Internal Benchmark (STIB) learning case; 2) when models trained on one dataset are tested on another dataset, or Single Task with External Benchmark (STEB) case; 3) when models are trained on all the available datasets except the testing one, or Multi Task Learning (MTL</figDesc><table><row><cell>3.9</cell></row></table><note><p>3.8 Number of subjects stratified by dataset and diagnosis: Alzheimer's Disease (AD); Mild Cognitive Impairment (MCI); Normal Cognition (NC). . . . . .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.who.int/health-topics/dementia</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>1.2 Integrating biomarkers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>1.4  Objectives and organization of this Thesis</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>1.5 Publications</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>2.1 Introduction</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Chapter 2 Multi-Channel Variational Autoencoder</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>2.2 Method</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>2.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>2.4 Conclusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>2.5 Supplementary Material</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>Chapter 3 Multi-Task Multi-Channel Variational Autoencoder</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p>3.2 Method</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_12"><p>adni.loni.usc.edu. The ADNI was launched in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2003" xml:id="foot_13"><p>as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. For up-to-date information, see www.adni-info.org.3.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_14"><p>3.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_15"><p>surfer.nmr.mgh.harvard.edu   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_16"><p>3.4 Discussion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_17"><p>3.6 Supplementary Material</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_18"><p>4.2 Methods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_19"><p>https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_ classification.html4.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_20"><p>Chapter 4 Benchmark of Harmonization Methods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_21"><p>4.4 Discussion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_22"><p>4.5 Conclusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_23"><p>https://ai4healthschool.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_24"><p>https://epione.gitlabpages.inria.fr/flhd/heterogeneous_data/heterogeneous_data.html# multi-channel-variational-autoencoder</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_25"><p>5.2 Implementation overview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_26"><p>5.3Usage: example of multi-modal learning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_27"><p>https://book.pythontips.com/en/latest/context_managers.html5.4 Supplementary documentation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_28"><p>6.2 Future developments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_29"><p>https://en.wikipedia.org/wiki/Digital_twin6.3 Final remarks</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acronyms</head></div>
			</div>


			<div type="funding">
<div><p><rs type="institution">NC Normal Cognition</rs>. <rs type="funder">NIA-AA National Institute on Aging and Alzheimer's Association. NLL Negative Log-Likelihood. OASIS Open Access Series of Imaging Studies</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Sources</head><p>Data used in the preparation of this section were obtained from the following sources.</p><p>1. The Alzheimer's Disease Neuroimaging Initiative (ADNI), a database of brain imaging and related clinical data of cognitively normal subjects, and on patients presenting various degrees of cognitive decline.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Feature Prediction</head><p>The purpose of this experiment is to compare, in features prediction experiments, the generalization performance the MCVAE model with respect to our new Multi Task extension (MT-MCVE). This experiment was run in three different conditions:</p><p>1. Single Task with Internal Benchmark (STIB): when training and testing data are chosen from the same dataset;</p><p>2. Single Task with External Benchmark (STEB): when models trained on one dataset are tested on another one;</p><p>3. Multi-Task Learning (MTL): when models are trained on all the available datasets except the testing one.</p><p>In STIB and STEB experiments, both MCVAE and MT-MCVAE models are trained on the same views, but while in MCVAE we need to discard observations with missing views from the training set, with MT-MCVAE we can include them by grouping together observations with common views into homogeneous tasks. In MTL experiments, MCVAE models cannot be trained because no observation has simultaneously all the views.</p><p>We choose for both MCVAE and MT-MCVAE a linear Gaussian parameterization for the likelihood and variational distributions as in Eq. (3.7) and Eq. (3.8) respectively. Models were trained on all the available views in the training dataset. We trained them for 20000 epochs which ensured convergence, after setting up the Adam optimizer with a learning rate of 0.001. Prediction performances were evaluated with the Mean Squared Error (MSE) metric, measured on the available views in the testing dataset, reconstructed with Eq. (3.5).</p><p>Non-linear experiments were also made on the MTL scenarios with our MT-MCVAE model, where the encoding and decoding distributions were parametrized with neural networks with up to 4 layers and LeakyReLU activation functions. In this case we choose hidden dimension as the mean value between the input features and latent dimension (32 features), rounded towards the nearest integer (e.g., for the MRI views and a depth of 3 layers we used a symmetric encoding-decoding architecture with dimensions: 99 -66 -66 -32 -66 -66 -99). Training for 20000 epochs with Adam and a learning rate of 0.001 ensured convergence.</p><p>All results were validated by means of 5-folds cross-validation. For both MCVAE and MT-MCVAE we choose a linear Gaussian parameterization for the variational distributions as in Eq. (3.8). To adapt the models to this new classification experiment, we adopt as decoding function for the latent variable z, the following Categorical likelihood:</p><p>where y d,n is the diagnosis associated to the data-point n in the dataset d. The probability vector π is a two dimensional vector representing the class probability for each of the three binary comparisons across the three diagnostic classes, namely AD vs MCI, AD vs NC, MCI vs NC ,and is parametrized with a linear transformation of the latent z by the matrix θ.</p><p>Non-linear experiments were also made on the MTL scenarios with our MT-MCVAE model, benchmarked against the EmbraceNet (EN) method <ref type="bibr" target="#b27">[Choi, 2019]</ref>, where the encoding distributions were parametrized with neural networks with up to 4 layers and LeakyReLU activation functions. Training for 20000 epochs with the Adam optimizer and a learning rate of 0.001 ensured convergence.</p><p>Models were trained on all the available views in the training dataset, independently of their presence in the testing dataset. Classes probabilities were inferred from the all the available views in the testing dataset with the following equation:</p><p>(3.11)</p><p>We attributed to each subject the diagnostic class with the highest inferred probability.</p><p>The performance on test datasets was evaluated by measuring the classification accuracy (%). All results were validated by means of 5-folds cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In Tab. </p><p>reconstruction errors: we notice that the error made in ground truth data recovery with multi-channel information is systematically lower than the one obtained with a single-channel decoder. . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>General variational framework for our multi-view and multi-dataset model.</p><p>Compatibly with the MCVAE formulation, for every pair of views w and v there is a prediction path w → v composed by two learnable functions: the encoding distribution q (z|x d,n,w , φ w ) and the decoding likelihood p (x d,n,v |z, θ v ). Parameters φ w and θ v are optimized through Eq. (3.4) to maximize the likelihood of our generative model under the encoding distributions, and at the same time minimize the Kullback-Leibler distance between every encoding distribution and the prior p (z). . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Dataset attributes, varied one-at-a-time in the prescribed ranges, and used to generate scenarios according to Eq. ( <ref type="formula">2</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monotonic Gaussian Process for spatio-temporal disease progression modeling in brain imaging data</title>
		<author>
			<persName><forename type="first">Abi</forename><surname>Nader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abi</forename><surname>Nader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020-01">2020. Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simulating the outcome of amyloid treatments in Alzheimer&apos;s disease from imaging and clinical data</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Abinader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Abinader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Commun</title>
		<imprint>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fixing a Broken ELBO</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Alzheimer&apos;s disease facts and figures</title>
		<imprint>
			<date type="published" when="2020-03">2020. 2020. Mar. 2020</date>
			<publisher>Alzheimer Association Report</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="103" />
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Dement</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A method for statistical learning in large databases of heterogeneous imaging, cognitive and behavioral data. EPICLIN 2018 -12ème Conférence Francophone</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco Lorenzi ; Danail</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeike</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ipek</forename><surname>Mostafa Kia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">F</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Marquand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommy</forename><surname>Duchesnay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><surname>Löfstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Underst. Interpret. Mach. Learn. Med. Image Comput. Appl</title>
		<editor>
			<persName><forename type="first">Luigi</forename><surname>Antelmi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valeria</forename><surname>Manera</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename><surname>Robert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicholas</forename><surname>Ayache</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018-05">2018. 2018. May 2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Multi-channel Stochastic Variational Inference for the Joint Analysis of Heterogeneous Biomedical Data in Alzheimer&apos;s Disease. Epidémiologie Clinique / CLCC 2018 -25èmes Journées des statisticiens des Centre de Lutte Contre le Cancer. cit. on pp. 9, 11</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th Int. Conf. Mach. Learn. Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov</title>
		<meeting>36th Int. Conf. Mach. Learn. Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining Multi-Task Learning and Multi-Channel Variational Auto-Encoders to Exploit Datasets with Missing Observations -Application to Multi-Modal Neuroimaging Studies in Dementia</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Ribaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Garibotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">under review at NeuroImage</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2021-01">2021. Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D PIB and CSF biomarker associations with hippocampal atrophy in ADNI subjects</title>
		<author>
			<persName><forename type="first">Liana</forename><forename type="middle">G</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><forename type="middle">S</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amity</forename><forename type="middle">E</forename><surname>Andrawis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sona</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Babakchanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Morra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">W</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Q</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">M</forename><surname>Trojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">R</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">C</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jagust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><forename type="middle">A</forename><surname>Koeppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">M</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010-08">2010. Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voxel-based morphometrythe methods</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage 11.6 Pt</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2000-06">2000. June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Enhanced Approach in Detecting Object Applied to Automotive Traffic Roads Signs</title>
		<author>
			<persName><forename type="first">Anass</forename><surname>Barodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdrrahim</forename><surname>Barodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Benbrahim</surname></persName>
		</author>
		<author>
			<persName><surname>Tamtaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 6th International Conference on Optimization and Applications (ICOA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">First Tau PET Tracer Approved: Toward Accurate In Vivo Diagnosis of Alzheimer Disease</title>
		<author>
			<persName><forename type="first">Barthel</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Barthel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2020-10">2020. Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno>eprint: 1601.00670</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The strategic biomarker roadmap for the validation of Alzheimer&apos;s diagnostic biomarkers: methodological update</title>
		<author>
			<persName><forename type="first">Marina</forename><surname>Boccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Boccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>Dodich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angèle</forename><surname>Albanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gayet-Ageron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Festari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><forename type="middle">N</forename><surname>Ashton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">E</forename><surname>Leuzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><forename type="middle">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Rabinovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oskar</forename><surname>Drzezga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agneta</forename><surname>Hansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName><surname>Ossenkoppele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Villemagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Winblad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><surname>Garibotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021-03">2021. Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dissecting diagnostic heterogeneity in depression by integrating neuroimaging and genetics</title>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">M</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><surname>Liston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychopharmacology</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2020-08">2020. Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Importance Weighted Autoencoders</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015-09">2015. Sept. 2015</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Buuren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Van Buuren</surname></persName>
		</author>
		<author>
			<persName><surname>Groothuis-Oudshoorn</surname></persName>
		</author>
		<idno>PG/VGZ/00.038</idno>
		<title level="m">Multivariate Imputation by Chained Equations: MICE V1.0 User manual</title>
		<meeting><address><addrLine>Leiden</addrLine></address></meeting>
		<imprint>
			<publisher>TNO Prevention and Health</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multitask Learning</title>
		<author>
			<persName><forename type="first">Caruana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learn. to Learn</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note>Rich Caruana</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangle, Align and Fuse for Multimodal and Semi-Supervised Image Segmentation</title>
		<author>
			<persName><forename type="first">; A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papanastasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D E</forename><surname>Semple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S A</forename><surname>Dharmakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Removal of Scanner Effects in Covariance Improves Multivariate Pattern Analysis in Neuroimaging Data</title>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanne</forename><forename type="middle">C</forename><surname>Beer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochang</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-view Learning with Feature Level Fusion for Cervical Dysplasia Diagnosis</title>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunnv</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding our way through the labyrinth of dementia biomarkers</title>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Chételat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Chételat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Arbizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Barthel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriaan</forename><forename type="middle">A</forename><surname>Garibotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lammertsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><surname>Morbelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Elsmarieke Van De Giessen</surname></persName>
		</author>
		<author>
			<persName><surname>Drzezga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021-04">2021. Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EmbraceNet: A robust deep learning architecture for multimodal classification</title>
		<author>
			<persName><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Seok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2019-11">2019. Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Bedell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">B</forename><surname>Beach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Bilker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Pontecorvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">P</forename><surname>Hefti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Flitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hank</forename><forename type="middle">F</forename><surname>Krautkramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Edward Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Murali Doraiswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwan</forename><forename type="middle">N</forename><surname>Fleisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">H</forename><surname>Sabbagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Sadowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><forename type="middle">P</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><surname>Zehntner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Skovronsky</surname></persName>
		</author>
		<idno>AV45-A07 Study Group</idno>
	</analytic>
	<monogr>
		<title level="m">Use of Florbetapir-PET for Imaging β-Amyloid Pathology</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harmonization strategies for multicenter radiomics investigations</title>
		<author>
			<persName><forename type="first">; R</forename><surname>Daano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Da-Ano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Visvikis</surname></persName>
		</author>
		<author>
			<persName><surname>Hatt</surname></persName>
		</author>
		<idno>24TR02</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2020-12">2020. Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</title>
		<author>
			<persName><forename type="first">Rahul</forename><forename type="middle">S</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Ségonne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><forename type="middle">C</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">L</forename><surname>Blacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><forename type="middle">M</forename><surname>Buckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Paul</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">T</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">S</forename><surname>Hyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><surname>Killiany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Doradomoreno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-task learning for the prediction of wind power ramp events with deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorado-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salcedo-Sanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hervás-Martínez ; Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">H</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Jacova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaj</forename><surname>Luis Molinuevo</surname></persName>
		</author>
		<author>
			<persName><surname>Blennow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Steven T Dekosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Selkoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Cappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiaan</forename><surname>Crutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Engelborghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Odile</forename><surname>Galasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Habert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agneta</forename><surname>Jicha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florence</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rabinovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Salloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Sarazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo C De</forename><surname>Epelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">J</forename><surname>Vellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lon</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaakov</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Scheltens</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2014-06">Mar. 2020. June 2014</date>
		</imprint>
	</monogr>
	<note>Lancet. Neurol.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Finn ; Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
		<idno>PMLR 70</idno>
		<imprint>
			<date type="published" when="2017-03">2017. Mar. 2017</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Harmonization of multi-site diffusion tensor imaging data</title>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birkan</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Tunç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosha</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Ruparel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">D</forename><surname>Roalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><forename type="middle">C</forename><surname>Satterthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><forename type="middle">E</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragini</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><surname>Shinohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2017-11">2017. Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Harmonization of cortical thickness measurements across scanners and sites</title>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><forename type="middle">I</forename><surname>Cullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">D</forename><surname>Sheline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irem</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Aselcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Fava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">L</forename><surname>Mcinnis</surname></persName>
		</author>
		<author>
			<persName><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myrna</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><surname>Shinohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2018-02">2018. Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fostinelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Eating Behavior in Aging and Dementia: The Need for a Comprehensive Assessment</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Fostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramona</forename><forename type="middle">De</forename><surname>Amicis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Giustizieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuliano</forename><surname>Binetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simona</forename><surname>Bertoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Battezzati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><forename type="middle">F</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Nutr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 64</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imaging cerebral atrophy: normal ageing to Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><surname>Schott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2004-01">2004. Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Frisoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Diagnosis of Alzheimer Disease Before It Is Alzheimer Dementia</title>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Padovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars-Olof</forename><surname>Wahlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Neurol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1023</biblScope>
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 64</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Investigating hypotheses of neurodegeneration by learning dynamical systems of protein propagation in the brain</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Garbarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Garbarino</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Garibotto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Clinical validity of brain fluorodeoxyglucose positron emission tomography as a biomarker for Alzheimer&apos;s disease in the context of a structured 5-phase development framework</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Garibotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Herholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Boccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnese</forename><surname>Picco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Varrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agneta</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Nobili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Ratib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dynamical Variational Autoencoders: A Comprehensive Review</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Diard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><surname>Alameda-Pineda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12595</idno>
		<imprint>
			<date type="published" when="2020-08">2020. Aug. 2020</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Challenges of Integrative Disease Modeling in Alzheimer&apos;s Disease</title>
		<author>
			<persName><forename type="first">Golriz</forename><surname>Khatami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Sepehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golriz</forename><surname>Khatami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Birkenbihl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Domingo-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Mol. Biosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2020-01">2020. Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MIDA: Multiple Imputation Using Denoising Autoencoders</title>
		<author>
			<persName><forename type="first">Gondara</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lovedeep</forename><surname>Gondara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohadeseh</forename><surname>Ganji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prediction and Classification of Alzheimer&apos;s Disease Based on Combined Features From Apolipoprotein-E Genotype</title>
		<author>
			<persName><forename type="first">Yubraj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goo-Rak</forename><surname>Kumar Lama</surname></persName>
		</author>
		<author>
			<persName><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cerebrospinal Fluid, MR, and FDG-PET Imaging Biomarkers</title>
		<imprint>
			<date type="published" when="2019-10">2019. Oct. 2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Design of experiments for the NIPS 2003 variable selection benchmark</title>
		<author>
			<persName><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2003 workshop on feature extraction and feature selection</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
	<note>Isabelle Guyon</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the interpretation of weight vectors of linear models in multivariate neuroimaging</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Meinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Görgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Dylan</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><surname>Bießmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Novel Approach for Model-Based Pedestrian Tracking Using Automotive Radar</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dagmar</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steinhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><forename type="middle">T</forename><surname>Brandmeier</surname></persName>
		</author>
		<author>
			<persName><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Use of FDG PET as an imaging biomarker in clinical trials of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Herholz</surname></persName>
		</author>
		<author>
			<persName><surname>Herholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomark. Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012-08">2012. Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Towards a Definition of Disentangled Representations</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<imprint>
			<date type="published" when="2018-12">2018. Dec. 2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The discovery of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Hanns</forename><surname>Hippius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hippius</surname></persName>
		</author>
		<author>
			<persName><surname>Neundörfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogues Clin. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2003-03">2003. Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relations Between Two Sets of Variates</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
		<author>
			<persName><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1936-12">1936. Dec. 1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Nonlinear measures of association with kernel canonical correlation analysis and applications</title>
		<author>
			<persName><forename type="first">Su-Yun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei-Hsien</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhsing Kate</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Plan. Inference</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilse</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DIVA: Domain Invariant Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Arbel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marleen</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maxime</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Herve</forename><surname>Lombaert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</editor>
		<meeting>Machine Learning Research<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="322" to="348" />
		</imprint>
	</monogr>
	<note>cit. on pp. 64, 67, 94</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">NIA-AA Research Framework: Toward a biological definition of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaj</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">C</forename><surname>Blennow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billy</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Budd Haeberlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Jagust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Jessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enchi</forename><surname>Karlawish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Luis Molinuevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Creighton</forename><surname>Montine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">P</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">C</forename><surname>Rankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Scheltens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><forename type="middle">M</forename><surname>Siemers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reisa</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cerise</forename><surname>Sperling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliezer</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurie</forename><surname>Masliah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><surname>Silverberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s Dement</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="535" to="562" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 6, 25, 103</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adjusting batch effects in microarray expression data using empirical Bayes methods</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">W Evan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="66" />
			<date type="published" when="2007-01">2007. Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jovicich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Harmonization of neuroimaging biomarkers for neurodegenerative diseases: A survey in the imaging community of perceived barriers and suggested actions</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Jovicich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Barkhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Herholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Mulert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Berckel</surname></persName>
		</author>
		<author>
			<persName><surname>Frisoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assess. Dis. Monit</title>
		<editor>
			<persName><forename type="first">Jorge</forename><surname>Jovicich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Frisoni</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
	<note>Diagnosis</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Canonical analysis of several sets of variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kettenring</surname></persName>
		</author>
		<author>
			<persName><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1971">1971. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multimodal Brain Tumor Classification Using Deep Learning and Robust Feature Selection: A Machine Learning Application for Radiologists</title>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Attique</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majed</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robertas</forename><surname>Alhaisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Damaševičius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syed</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><surname>Bukhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020-08">2020. Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A biophysical model of brain deformation to simulate and analyze longitudinal MRIs of patients with Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Bishesh</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2016-07">2016. July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simulating Longitudinal Brain MRIs with Known Volume Changes and Realistic Variations in Image Intensity</title>
		<author>
			<persName><forename type="first">Bishesh</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Semi-Amortized Variational Autoencoders</title>
		<author>
			<persName><surname>Kim ; Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02550</idno>
		<imprint>
			<date type="published" when="2018-02">2018. Feb. 2018</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kingma ; Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>Dec. 2014. eprint: 1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Learn. Represent</title>
		<meeting>2nd Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Variational Dropout and the Local Reparameterization Trick</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma ; Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; C</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N D</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2015">2015. 2015</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
	<note>cit. on pp. 13, 19, 20</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Local Dependent Components</title>
		<author>
			<persName><forename type="first">Arto</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2007, the 24th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><surname>Omnipress</surname></persName>
		</editor>
		<meeting>ICML 2007, the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bayesian Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">Arto</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virtanen</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Seppo</surname></persName>
		</author>
		<author>
			<persName><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="965" to="1003" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 4, 12, 17</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Validation of an automatic video monitoring system for the detection of instrumental activities of daily living in dementia patients</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crispim</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Derreumaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-David</forename><surname>Bensadoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frans</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Verhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Aalten</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Alzheimers. Dis</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease</title>
		<author>
			<persName><forename type="first">Pamela</forename><forename type="middle">J</forename><surname>Lamontagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tammie</forename><forename type="middle">L S</forename><surname>Lamontagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Benzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Hornbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Hassenstab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Moulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">E</forename><surname>Vlassenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Raichle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cruchaga</surname></persName>
		</author>
		<author>
			<persName><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 47</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The challenges in data integration -heterogeneity and complexity in clinical trials and patient registries of Systemic Lupus Erythematosus</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Sueur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Helen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sueur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nophar</forename><surname>Geifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Res. Methodol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2020-12">2020. Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Lecun ; Yann Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist2" />
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
		<respStmt>
			<orgName>ATT Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Identification of pan-cancer Ras pathway activation with deep learning</title>
		<author>
			<persName><forename type="first">Li ; Xiangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Chun</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020-10">2020. Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A review of multivariate analyses in imaging genetics</title>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Calhoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2014-03">2014. Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Multimodal Motion Prediction with Stacked Transformers</title>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11624</idno>
		<imprint>
			<date type="published" when="2021-03">2021. Mar. 2021</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Conditional Variational Autoencoder for Prediction and Feature Recovery Applied to Intrusion Detection in IoT</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Lopezmartin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belen</forename><surname>Lopez-Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Carro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Sanchez-Esguevillas</surname></persName>
		</author>
		<author>
			<persName><surname>Lloret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Basel)</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Disentangling normal aging from Alzheimer&apos;s disease in structural magnetic resonance images</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2015-01">2015. Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Tensor Canonical Correlation Analysis for Multi-View Dimension Reduction</title>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Yong Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015">2019. 2019. 2015</date>
		</imprint>
	</monogr>
	<note>Understanding posterior collapse in generative latent variable models</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Recommendations for the Nonpharmacological Treatment of Apathy in Brain Disorders</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><forename type="middle">R</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Macmanus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Manera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Abrahams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Agüera-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaci</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auriane</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cécile</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masud</forename><surname>Hanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">L</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribel</forename><surname>Lockwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratko</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Radakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florindo</forename><surname>Slachevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anaïs</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><forename type="middle">Davide</forename><surname>Tribouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frans</forename><surname>Trimarchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Verhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radia</forename><surname>Yesavage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Zeghari</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Geriatr. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013-04">2013. Apr. 2013. Apr. 2020</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The diagnosis of dementia due to Alzheimer&apos;s disease: Recommendations from the National Institute on Aging-Alzheimer&apos;s Association workgroups on diagnostic guidelines for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">M</forename><surname>Mckhann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Mckhann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Knopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">T</forename><surname>Chertkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">R</forename><surname>Hyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><forename type="middle">H</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Kawas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Klunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">J</forename><surname>Koroshetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Manly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Mayeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Mohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">N</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Rossor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">C</forename><surname>Scheltens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Creighton</forename><forename type="middle">H</forename><surname>Weintraub</surname></persName>
		</author>
		<author>
			<persName><surname>Phelps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s Dement</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2011-05">2011. May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName><surname>Méndez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Multiview cluster ensembles for multimodal MRI segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Andrés</forename><surname>Méndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gloria</forename><surname>Menegaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep learning for healthcare: review, opportunities and challenges</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2018-11">2018. Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Molchanov, 2017] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov</title>
		<author>
			<persName><forename type="first">Graziano</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Michiardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09360</idno>
		<idno>arXiv: 1701.05369</idno>
		<imprint>
			<date type="published" when="2017">Oct. 2020. 2017</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
	<note>Variational Dropout Sparsifies Deep Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">How can we combat multicenter variability in MR radiomics? Validation of a correction procedure</title>
		<author>
			<persName><forename type="first">Farouk</forename><forename type="middle">S</forename><surname>Nathoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linglong</forename><surname>Nathoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Orlhac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Lecler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Savatovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Goya-Outi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédérique</forename><surname>Nioche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Charbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédérique</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Frouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irène</forename><surname>Duron</surname></persName>
		</author>
		<author>
			<persName><surname>Buvat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Stat</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019-03">2019. Mar. 2019. Sept. 2020</date>
		</imprint>
	</monogr>
	<note>Eur. Radiol.. cit. on pp. 49, 59</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">MRI quality data assessment in the Italian IRCCS advanced neuroimaging network using ACR phantoms</title>
		<author>
			<persName><forename type="first">Fulvia</forename><surname>Palesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Palesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Nigri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Gianeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Pirastru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Laganà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Biagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia A M Gandini</forename><surname>Grazia Bruzzone</surname></persName>
		</author>
		<author>
			<persName><surname>Wheelerkingshott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMRM 27th Annu. Meet. Exhib</title>
		<meeting>ISMRM 27th Annu. Meet. Exhib</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>cit. on pp. 36, 79, 80</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A Comparison of Machine Learning Methods for Data Imputation</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Platias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Platias</surname></persName>
		</author>
		<author>
			<persName><surname>Petasis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">11th Hell. Conf. Artif. Intell. ACM</title>
		<imprint>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2020-09">2020. Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Unsupervised Deformable Registration for Multi-modal Images via Disentangled Representations</title>
		<author>
			<persName><forename type="first">Bibo</forename><surname>Qin ; Chen Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kamen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C S</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Yushkevich</surname></persName>
		</author>
		<author>
			<persName><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Med. Imaging</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Recommendations for the use of Serious Games in people with Alzheimer&apos;s Disease, related disorders and frailty</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herminia</forename><forename type="middle">Diana</forename><surname>Schmansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Danilo</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hã©lene</forename><surname>Kã ¶nig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandrine</forename><surname>Amieva</surname></persName>
		</author>
		<author>
			<persName><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Franã §ois Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ceccaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Ariel</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stã©phane</forename><surname>Kenigsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">M</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Orgogozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Piano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Touchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Vellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>Yesavage</surname></persName>
		</author>
		<author>
			<persName><surname>Manera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Aging Neurosci</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">67</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012-01">2012. 2012. Jan. 2014. Mar. 2014</date>
		</imprint>
	</monogr>
	<note>NeuroImage</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Variational Autoencoders Pursue PCA Directions (by Accident)</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Rolinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Rolinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019-06">2019. June 2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Good Initializations of Variational {B}ayes for Deep Models</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><surname>Filippone</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<title level="s">Kamalika Chaudhuri and Ruslan Salakhutdinov</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5487" to="5497" />
		</imprint>
	</monogr>
	<note>Proc. 36th Int. Conf. Mach. Learn. cit. on p. 5</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Disruptive Behavior as a Predictor in Alzheimer Disease</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Scarmeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Scarmeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Blacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Hadjigeorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davangere</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Devanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaakov</forename><surname>Honig</surname></persName>
		</author>
		<author>
			<persName><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Neurol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">1755</biblScope>
			<date type="published" when="2007-12">2007. Dec. 2007</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 64</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Financial time series forecasting with deep learning : A systematic literature review: 2005-2019</title>
		<author>
			<persName><forename type="first">Omer</forename><forename type="middle">Berat</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><forename type="middle">Murat</forename><surname>Ugur Gudelek</surname></persName>
		</author>
		<author>
			<persName><surname>Ozbayoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2020-05">2020. May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Predicting Cognitive Decline in Subjects at Risk for Alzheimer Disease by Using Combined Cerebrospinal Fluid, MR Imaging, and PET Biomarkers</title>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">R</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">C</forename><surname>Petrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kingshuk</forename><forename type="middle">Roy</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><forename type="middle">D</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Murali</forename><surname>Edward Coleman</surname></persName>
		</author>
		<author>
			<persName><surname>Doraiswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013-02">2013. Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models</title>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>D\textquotesingle Alché-Buc</surname></persName>
		</author>
		<author>
			<persName><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A model of brain morphological changes related to aging and Alzheimer&apos;s disease from cross-sectional assessments</title>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Sivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019-09">2019. Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Comorbidity and the rate of cognitive decline in patients with Alzheimer dementia</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Letitia</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingemar</forename><surname>Dobranici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Kåreholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Tudose</surname></persName>
		</author>
		<author>
			<persName><surname>Lȃzȃrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Geriatr. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1244" to="1251" />
			<date type="published" when="2011-12">2011. Dec. 2011</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 64</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinton</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Wireless sensors network based safe home to care elderly people: Behaviour detection</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Suryadevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suryadevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Gaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Rayudu</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors Actuators A Phys</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2012-10">2012. Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A distributed multitask multimodal approach for the prediction of Alzheimer&apos;s disease in a longitudinal study</title>
		<author>
			<persName><forename type="first">Solale</forename><surname>Tabarestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryamossadat</forename><surname>Tabarestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Aghili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Cabrerizo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naphtali</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosie</forename><forename type="middle">E</forename><surname>Rishe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Curiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjan</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Duara</surname></persName>
		</author>
		<author>
			<persName><surname>Adjouadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020-02">2020. Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Towards Precision Medicine in Psychosis: Benefits and Challenges of Multimodal Multicenter Studies-PSYSCAN: Translating Neuroimaging Findings From Research into Clinical Practice</title>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Tognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrika</forename><forename type="middle">H</forename><surname>Tognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Van Hell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inge</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName><surname>Winter-Van Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Bossong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Kempton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Modinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Fusar-Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Mechelli</surname></persName>
		</author>
		<author>
			<persName><surname>Dazzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schizophr. Bull</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2020-02">2020. Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Automated Anatomical Labeling of Activations in SPM Using a Macroscopic Anatomical Parcellation of the MNI MRI Single-Subject Brain</title>
		<author>
			<persName><forename type="first">;</forename><surname>Tzouriomazoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tzourio-Mazoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Landeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papathanassiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Crivello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2002-01">2002. Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A Review on Methods for Detecting SNP Interactions in High-Dimensional Genomic Data</title>
		<author>
			<persName><forename type="first">Suneetha</forename><surname>Uppu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Uppu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">P</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinforma</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2018-03">2018. Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Deep Learning Predicts Underlying Features on Pathology Images with Therapeutic Relevance for Breast and Gastric Cancer</title>
		<author>
			<persName><surname>Valieris ; Renan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Valieris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><forename type="middle">Aparecida</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><forename type="middle">Passos</forename><surname>Bueno De Toledo Osório</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><surname>Andres Rosales Mitrowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Dirce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><forename type="middle">Noronha</forename><surname>Carraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israel Tojal Da</forename><surname>Dias-Neto</surname></persName>
		</author>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">mice: Multivariate Imputation by Chained Equations in R</title>
		<author>
			<persName><forename type="first">;</forename><surname>Van Buuren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Stef Van Buuren</surname></persName>
		</author>
		<author>
			<persName><surname>Groothuis-Oudshoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Multimodal deep learning models for early detection of Alzheimer&apos;s disease stage</title>
		<author>
			<persName><forename type="first">Janani</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">May</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2021-12">2021. Dec. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multimodal integration</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><surname>Hugo Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Pinaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Garcia-Dias</surname></persName>
		</author>
		<author>
			<persName><surname>Mechelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. -Methods Appl. to Brain Disord. Elsevier</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Mach. Learn. -ICML &apos;08</title>
		<meeting>25th Int. Conf. Mach. Learn. -ICML &apos;08<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Regularization of Neural Networks using DropConnect</title>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learn</title>
		<meeting>30th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learn</title>
		<editor>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</editor>
		<meeting>30th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Stochastic optimization for deep CCA via nonlinear orthogonal iterations</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Allert. Conf. Commun. Control. Comput. IEEE</title>
		<imprint>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2015-09">2015. 2015. Sept. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Predicting PET-derived demyelination from multimodal MRI using sketcher-refiner adversarial training for multiple sclerosis</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedetta</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Bodini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Durrleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Stankoff</surname></persName>
		</author>
		<author>
			<persName><surname>Colliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Predicting PET-derived myelin content from multisequence MRI for individual longitudinal analysis in multiple sclerosis</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedetta</forename><surname>Bodini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Tonietto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Durrleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Colliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Stankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title/>
		<author>
			<persName><surname>Weiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">The Alzheimer&apos;s Disease Neuroimaging Initiative: A review of papers published since its inception</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><forename type="middle">P</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><forename type="middle">A</forename><surname>Beckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><forename type="middle">J</forename><surname>Cairns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Jagust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Siuciak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holly</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Q</forename><surname>Trojanowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-09">Sept. 2013</date>
			<publisher>Alzheimer&apos;s Dement</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Multimodal Generative Models for Scalable Weakly-Supervised Learning</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Cross-Modality Segmentation by Self-supervised Semantic Alignment in Disentangled Content Space</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James S Duncan ; Shadi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daguang</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Domain Adapt. Represent. Transf. Distrib. Collab. Learn</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Tackling Over-pruning in Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Anitha</forename><surname>Yeung ; Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03643</idno>
		<imprint>
			<date type="published" when="2017-06">2017. June 2017</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Discovery of multi-dimensional modules by integrative analysis of cancer genomic data</title>
		<author>
			<persName><forename type="first">Shihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianghong Jasmine</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Effective feature learning and fusion of multimodality data using stage-wise deep neural network for dementia diagnosis</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Effective feature learning and fusion of multimodality data using stage-wise deep neural network for dementia diagnosis</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2019-02">2019. Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title/>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Hi-Net: Hybrid-Fusion Network for Multi-Modal MR Image Synthesis</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">experiment modeled with the Multi-Channel VAE. As expected, the minimum amount of non-zero components of z (left) and generative parameters G (right) is obtained with the sparse model</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Zigel</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Zigel</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Israel</forename><surname>Litvak</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">*</forename><surname>Gannot</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2009-12">2009. Dec. 2009</date>
		</imprint>
	</monogr>
	<note>A Method for Automatic Fall Detection of Elderly People Using Floor Vibrations and Sound-Proof of Concept on Human Mimicking Doll Falls</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Estimated dropout rates for the latent dimensions when the initial latent dimensions of the Sparse Multi-Channel VAE was set to l fit = 20 on data generated with respectively l = 1, 2, 4, and 10 latent dimensions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Testing benchmark of four variational methods applied to the multi-channel scenarios in Tab. 3.12 (cases snr = 10, l fit = l)</title>
	</analytic>
	<monogr>
		<title level="m">Sparse Multi-Channel models performs consistently better than non-sparse Multi-Channel ones</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">In the same subspace it is possible to stratify subjects in the test-set by: (left) disease status among Alzheimer&apos;s Disease (AD), Mild Cognitive Impairment (MCI), Normal Cognition (NC), (right) age, in almost orthogonal directions</title>
		<imprint/>
	</monogr>
	<note>Stratification of the ADNI subjects (test data) in the sparse latent subspace inferred from the first two least dropped out dimensions. Classification accuracy for these subjects is given in the fifth numeric column of Tab. 2.3</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Normal aging trajectory (T r 1 ) vs Dementia aging trajectory (T r 2 ) in the latent 2D sub-space. Stars indicate the sampling points along trajectories. The trajectories share the same origin. (b) MRI data evolution. (c) FDG-PET. (d) Amyloid-PET. All the trajectories show a plausible evolution across disease and healthy conditions</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>Generation of imaging data from trajectories in the latent space</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Generative parameters G (µ) c (cfr. Eq. (2.7)) of the four channels associated to the least dropout latent dimension in the sparse multi-channel model. (Top) Clinical channel parameters</title>
		<imprint/>
	</monogr>
	<note>Imaging ch. parameters. .</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Dropout probability of a &quot;sparse&quot; model plotted with the plot_dropout utility</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Generative model of the Multi-Channel Variational Autoencoder (MC-VAE), where a common latent z x is the only source for the observations x c . (b), (c) Possible generative models of a MCVAE with modality specific latent variables, hosting a disentangled, complementary, and richer source of information</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Generative models of: (a) Domain Invariant Variational Autoencoder</title>
	</analytic>
	<monogr>
		<title level="j">DIVA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">possible Multi-Channel extension of the DIVA. Samples z d,• come from the prior p (z d,c |d) conditioned on the domain label d. Samples z x comes from a Standard Gaussian as in</title>
		<author>
			<orgName type="collaboration">VAE and MCVAE. . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Generative model for a Domain Invariant Multi-Channel Variational Autoencoder with joint (z x ) and disjoint (z c ) latent spaces. It comes from the combination of the models proposed in Fig. 6.1b and Fig</title>
		<imprint/>
	</monogr>
	<note>6.2b. . . . . .</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">With CovBat the harmonization tends to slightly change the original covariance matrices (|| • || F &lt; 10) to harmonize them, while with both DIVA methods the original covariance structures become very different from the original ones (|| • || F &gt; 40)</title>
		<idno>. . . . . . . . . 69</idno>
		<imprint/>
	</monogr>
	<note>Pairwise Frobenius norms between between correlation matrices of harmonized and raw data for every dataset. With ComBat the original covariance matrices are unchanged (|| • || F = 0)</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">varied one-at-a-time in the prescribed ranges, used to generate synthetic scenarios for the experimental campaign</title>
		<author>
			<orgName type="collaboration">. . . . . . . . . . . .</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parameters</title>
		<imprint>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
