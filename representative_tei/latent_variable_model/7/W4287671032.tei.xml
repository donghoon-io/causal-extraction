<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ordinal-Content VAE: Isolating Ordinal-Valued Content Factors in Deep Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
							<email>vladimir@cs.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Samsung AI Center Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ordinal-Content VAE: Isolating Ordinal-Valued Content Factors in Deep Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised representation learning</term>
					<term>Latent factor learning</term>
					<term>Ordinal data</term>
					<term>Bayesian deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In deep representational learning, it is often desired to isolate a particular factor (termed content) from other factors (referred to as style). What constitutes the content is typically specified by users through explicit labels in the data, while all unlabeled/unknown factors are regarded as style. Recently, it has been shown that such content-labeled data can be effectively exploited by modifying the deep latent factor models (e.g., VAE) such that the style and content are well separated in the latent representations. However, the approach assumes that the content factor is categoricalvalued (e.g., subject ID in face image data, or digit class in the MNIST dataset). In certain situations, the content is ordinal-valued, that is, the values the content factor takes are ordered rather than categorical, making content-labeled VAEs, including the latent space they infer, suboptimal. In this paper, we propose a novel extension of VAE that imposes a partially ordered set (poset) structure in the content latent space, while simultaneously making it aligned with the ordinal content values. To this end, instead of the iid Gaussian latent prior adopted in prior approaches, we introduce a conditional Gaussian spacing prior model. This model admits a tractable joint Gaussian prior, but also effectively places negligible density values on the content latent configurations that violate the poset constraint. To evaluate this model, we consider two specific ordinal structured problems: estimating a subject's age in a face image and elucidating the calorie amount in a food meal image. We demonstrate significant improvements in content-style separation over previous non-ordinal approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Identifying underlying sources of variability that explain the generative process of high dimensional data is one of the key problems in recent deep representation learning. One of the main objectives is to learn proper latent vector representation for these factors, leading to latent representation that is succinct, faithfully reconstructs the original data, and disentangles different factors from each other <ref type="bibr" target="#b0">[1]</ref>.</p><p>The unsupervised representation learning ambitiously aims to learn the factors solely from the observed data without any supervision <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b8">[9]</ref>. However, most of these approaches face the inherent impossibility <ref type="bibr" target="#b9">[10]</ref> of identifying the unsupervised factors. To alleviate this challenge, several weakly or semi-supervised learning frameworks have been proposed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Among those, <ref type="bibr" target="#b16">[17]</ref> proposes a contentstyle separation framework, where the goal is to isolate a particular factor (termed content) from remaining factors (referred to as style). What constitutes the content is typically specified by users through explicit labels in the data, while all unlabeled/unknown factors are regarded as style.</p><p>Under the setting of content labels for the data, more specifically in the form of paired data (x, c) where x is a data instance (e.g., image) and c ∈ {1, . . . , K} is its content label taking K different levels, it would be natural to define and learn K latent content vectors {v i } K i=1 , one for each content value, namely v 1 for c = 1, v 2 for c = 2, and so on. The group-level VAE model proposed in <ref type="bibr" target="#b16">[17]</ref> (denoted here as ML-VAE) is an extension of VAE <ref type="bibr" target="#b17">[18]</ref> with the modeling assumption that each instance x with content value c = i is generated from both the content latent vector v i and the instance-specific style latent vector s. This implies that all the instances x with the same content value c = i, share the content latent vector v i whereas the latent vector s captures the remaining factor variations specific to each instance. The proposed modified variational learning encourages style and content to be well separated in the latent representations, which is empirically demonstrated on the applications of isolating subject ID (content) in face images from the other factors (e.g., facial pose and expression), and the digit class (content) in handwritten digit images from e.g., writing style.</p><p>Unlike ML-VAE, we consider a different setup where the content factor is ordinal-valued, i.e., the values the content factor takes are ordered rather than categorical. For instance, a subject's age in a face image represents such an ordinal quantity; the calorie amount of a food meal, represented by its image, is naturally ordinal-valued. If the content factor c has such an ordinal structure c ∈ {1 &lt; 2 &lt; • • • &lt; K}, it is important to embed this structure as we form the content space. Specifically, for two instances x and x whose content values c and c are close to each other, their respective content latent vectors v and v should also be proximal, and vice versa. More formally, for any triplet i &lt; j &lt; k,</p><formula xml:id="formula_0">||v i -v k || &gt; max{||v i -v j ||, ||v j -v k ||}<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2009.03034v1 [cs.LG] 7 Sep 2020</head><p>This condition will establish a good alignment between the content latent vectors and the ordinal content values, thus yielding a model closer to the true data generation process. However, ML-VAE assumes the content factor is categorical, making it suboptimal in the ordinal-content setting. Specifically, they assume any content class (e.g., c = 1) is equally different from all other classes (e.g., c = 2 or c = 8). In ordinal setting, c = 1 is closer to c = 2 than c = 8. Enforcing this constraint in an effective and principled manner can be challenging: Directly incorporating the triplet inequalities as regularization loss into the objective function of ML-VAE is heuristic, and it never guarantees that the ordinal constrains <ref type="bibr" target="#b0">(1)</ref> are satisfied in the embedded space.</p><p>In this paper we propose a new principled approach to ordinal content VAE learning by constructing an appropriate prior for the labeled content space. This is in contrast to the iid standard normal prior in the non-ordinal models like ML-VAE. The prior is constructed by imposing a restrictive partially ordered set (poset) constraint, encoded in a novel conditional Gaussian spacing model. This prior assigns negligible density to the configurations of content vectors that violate the poset constraint. A key benefit of this model is that the joint prior becomes Gaussian, albeit with a full covariance, maintaining the closed form of the KL divergence term in the variational objective. Still the prior is fully factorized over the latent dimensions, leading to a computationally tractable model. Moreover, the number of parameters in the proposed prior is only O(d • K), where d is the dimensionality of the content latent vector, reinforcing the models tractability. We test our approach on both synthetic and real datasets, including the age of the subject as content in face images and the calorie amount as content in pizza images. Our model achieves significant improvement over the previous non-ordinal approaches in content-style separation, both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ORDINAL CONTENT LEVEL VAE</head><p>Setup and Goal. Our proposed Ordinal Content Level VAE (OL-VAE) model, is an extension of VAE <ref type="bibr" target="#b17">[18]</ref> and the ML-VAE for dealing with ordinal-valued content factors. We assume a semi-supervised setup with the content-annotated paired data (x, c). The content c is discrete and ordinal valued:</p><formula xml:id="formula_1">1 &lt; • • • &lt; K.</formula><p>All other factors of variation are regarded as style, and we assume the style factors are not labeled. Our goal is to learn the latent representation, a pair of vectors (v, s) where v is responsible for content and s for style encoding, such that they are well separated and disentangled. That is, the change of v exclusively affects the content aspect and, conversely, the style latent s is independent of the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unifying Graphical Model</head><p>Although the OL-VAE model in Fig. <ref type="figure" target="#fig_0">1</ref> is general enough to unify both OL-VAE and ML-VAE within the same  framework, the main difference is the prior distribution of the content latents. We introduce a unified model to facilitate the expositions of how we can pose the prior distribution of the (ordinal) content latents (Sec. II-B) and the variational inference (Sec. II-C).</p><p>An observed data instance x is considered to be generated from a pair of latent vectors (v, s), the content and the style. Since the content label c ∈ {1, . . . , K} is available in data, it is reasonable to let v take one of K predefined content reference vectors {v i } K i=1 , where v i is the content representation of c = i:</p><formula xml:id="formula_2">P (v | c = i, v 1 , . . . , v K ) = δ(v -v i )<label>(2)</label></formula><p>where δ(•) is the Dirac's delta function. The reference vectors are random variables, and once sampled and fixed, v is associated deterministically to the content label c, its index. This differentiates v from s in that an individual random variable s exists for each instance x, but there is one v i that governs all instances x with the same content c = i. Using the plate notation, the graphical model can be defined as in Fig. <ref type="figure" target="#fig_0">1</ref>. Here n indicates the data instance, among N . The full joint distribution can then be written as:</p><formula xml:id="formula_3">P {v i } K i=1 , {(c n , v n , x n , s n )} N n=1 = P (v 1 , . . . , v K ) × N n=1 P (c n )P (v n | c n , {v i } K i=1 )P (s n )P (x n |v n , s n ) (3) P (x n |v n , s n</formula><p>) is the decoder model that generates an image x from the pair of latents, and the choice of P (c n ) is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Priors</head><p>For the style prior, a natural choice is the iid standard normal, P (s) = N (0, I) and P ({s n } N n=1 ) = N n=1 N (s n ; 0, I), as in the VAE. For the content reference vectors, in the ML-VAE they used fully factorized standard normal distributions, P (v 1 , . . . , v K ) = K i=1 N (v i ; 0, I). Using this prior in (3), exactly recovers ML-VAE's full joint model. However, for the ordinal-valued content, where proximity in the content label values needs to be reflected in the latent space, the iid prior is suboptimal, unable to preserve the ordinal property.</p><p>To have a more sensible ordinal content prior model, one would want to impose the triplet ordering constraint in <ref type="bibr" target="#b0">(1)</ref>. One way to meet this constraint is to have a partially ordered set (poset) for the content latents. For each dimension l = 1, . . . , d, we make sure that the l-th elements of the latent vectors are ordered, namely</p><formula xml:id="formula_4">[v 1 ] l &lt; [v 2 ] l &lt; • • • &lt; [v K ] l , where [v i ] l is the l-th entry of the vector v i (i.e., v i = [[v i ] 1 , [v i ] 2 , . . . , [v i ] d ]</formula><p>). Any set of K vectors aligned in a poset formation satisfies the content-distance constraints (1). Fig. <ref type="figure" target="#fig_1">2</ref> illustrates this for d = 2-dim latent space. To impose the poset constraint in a prior, we propose a novel conditional Gaussian spacing model. It effectively places negligible probability density on configurations that violate the poset constraint. The model results in a joint Gaussian distribution over the K vectors, fully correlated over i = 1, . . . , K, yet fully factorized over dimensions l = 1, . . . , d, making variational inference computationally tractable.</p><p>Conditional Gaussian Spacing Model. We consider a dimension-wise independent distribution,</p><formula xml:id="formula_5">P (v 1 , . . . , v K ) = d l=1 P l ([v 1 ] l , . . . , [v K ] l ),<label>(4)</label></formula><p>where P l is the density over K variables from l-th dimension of the latent vectors. We model P l by a product of predecessor-conditioned Gaussians. For simplicity we drop the subscript l in notation, and abuse v i to denote</p><formula xml:id="formula_6">[v i ] l , P (v 1 , . . . , v K ) to refer to P l ([v 1 ] l , . . . , [v K ] l )</formula><p>. Now the model is:</p><formula xml:id="formula_7">P (v 1 , . . . , v K ) = P (v 1 )P (v 2 |v 1 ) • • • P (v K |v K-1 ),<label>(5)</label></formula><p>where the conditionals are defined as:</p><formula xml:id="formula_8">P (v 1 ) = N (v 1 ; µ 1 , σ 2 1 ) (6) P (v 2 |v 1 ) = N (v 2 ; v 1 + ∆ 2 :=µ2 , σ 2 2 )<label>(7)</label></formula><formula xml:id="formula_9">P (v 3 |v 2 ) = N (v 3 ; v 2 + ∆ 3 :=µ3 , σ 2 3 )<label>(8)</label></formula><p>. . .</p><formula xml:id="formula_10">P (v K |v K-1 ) = N (v K ; v K-1 + ∆ K :=µ K , σ 2 K )<label>(9)</label></formula><p>We define P (v i |v i-1 ) as a Gaussian centered at</p><formula xml:id="formula_11">µ i := v i-1 + ∆ i with variance σ 2 i . That is, ∆ i (&gt; 0)</formula><p>is the spread between the predecessor sample v i-1 and the mean µ i (See Fig. <ref type="figure" target="#fig_1">2</ref> for the intuition). We consider {σ i , ∆ i } K i=1 and µ 1 to be the free parameters of the model that can be learned from data. To guarantee that we meet the poset constraint, we make each conditional distribution (pillar) separated from its adjacent neighbors through the following constraints:</p><formula xml:id="formula_12">∆ i ≥ 3σ i .<label>(10)</label></formula><p>With ( <ref type="formula" target="#formula_12">10</ref>) <ref type="foot" target="#foot_0">1</ref> , the likelihood that</p><formula xml:id="formula_13">v i ≤ v i-1 is negligible enforcing the desired ordering v 1 &lt; v 2 &lt; • • • &lt; v K .</formula><p>The joint density for (6-9) now admits a closed-form. Since everything is Gaussian and linear here, so the joint density P (v 1 , v 2 , . . . , v K ) must be Gaussian. One can show that the means and covariances of the full joint Gaussian model can be written as follows (See Appendix A for the proof):</p><formula xml:id="formula_14">E[v i ] = µ 1 + ∆ 2 + • • • + ∆ i (for i ≥ 2) (11) Cov(v i , v j ) = σ 2 1 + • • • + σ 2 min(i,j)<label>(12)</label></formula><p>E.g., for K = 3, the joint distribution</p><formula xml:id="formula_15">P (v 1 , v 2 , v 3 ) is: N   µ 1 µ 1 + ∆ 2 µ 1 + ∆ 2 + ∆ 3   :=a ,   σ 2 1 σ 2 1 σ 2 1 σ 2 1 σ 2 1 + σ 2 2 σ 2 1 + σ 2 2 σ 2 1 σ 2 1 + σ 2 2 σ 2 1 + σ 2 2 + σ 2 3   :=C<label>(13</label></formula><p>) where we denote the mean vector and covariance matrix of the joint Gaussian by a and C, respectively. Plugging this back in our original prior model (4), we have:</p><formula xml:id="formula_16">P (v 1 , . . . , v K ) = d l=1 N ([V] l ; a l , C l ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_17">[V] l := [v 1 ] l , . . . , [v K ] l</formula><p>is the K-dim vector collecting l-th dim elements from v i 's. Also, a l and C l , for each l = 1, . . . , d, are defined by <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref> with their own free parameters, denoted as:</p><formula xml:id="formula_18">µ l 1 , {∆ l i , σ l i } K i=1 .</formula><p>The covariance C l are not diagonal. However, the model is factorized over l = 1, . . . , d, the fact exploited in the next section to make the variational inference tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variational Inference</head><p>Given the content-labeled data {(x n , c n )} N n=1 , we approximate the posterior by the following variational density, decomposed into the content and style latents. The content posterior is further factorized over the content levels c = 1, . . . , K,</p><formula xml:id="formula_19">Q {v i } K i=1 , {s n } N n=1 := K i=1 Q c v i |{x n } n∈Gi N n=1 Q s s n |x n ,<label>(15)</label></formula><p>where G i = {n : c n = i} is the set of the training instances with content label c = i. For the encoders Q c and Q s , we adopt deep networks that take an input x and output the means and variances of the Gaussian-distributed latents. However, since Q c requires a group of samples {x n } n∈Gi as its input, instead of adopting a complex group encoder such as the neural statisticians <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we use a simple product-of-expert rule, also adopted in ML-VAE:</p><formula xml:id="formula_20">Q c v|{x n } n∈G ∝ n∈G Q c (v|x n ). (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>Since each Q c (v|x n ) is Gaussian, the product ( <ref type="formula" target="#formula_20">16</ref>) admits a Gaussian (Appendix B), and the upper bound of the data log-likelihood can be written as (Appendix C):</p><formula xml:id="formula_22">K i=1 E Qc(vi|Gi) n∈Gi E Qs(s n |x n ) log P (x n |v i , s n ) -KL K i=1 Q c (v i |G i ) P (v 1 , . . . , v K ) - N n=1 KL Q s s n |x n P (s n ) ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_23">Q c (v i |G i ) is shortcut notation for Q c v i |{x n } n∈Gi .</formula><p>Here P (s n ) = N (0, I) as usual, and P (v 1 , . . . , v K ) is our ordinal-constrained prior <ref type="bibr" target="#b13">(14)</ref> derived in the previous section. The first reconstruction loss term and the third style KL divergence are the same as those of ML-VAE, with the key difference in the second term. Below, we make a full derivation for its closed-form formula. Content latent KL term. Although both distributions in the KL term are Gaussians, the full dependency of v i 's over i = 1, . . . , K in P (v 1 , . . . , v K ) can be problematic if d and K are large. Specifically, as the input dimensionality of the distribution is d • K, the Cholesky decomposition of (d • K × d • K) covariance matrix, required for computing the KL divergence, might be prohibitive if d and/or K are large. However, along the latent dimensions l = 1, . . . , d, the prior distribution P (v 1 , . . . , v K ) is factorized as in <ref type="bibr" target="#b13">(14)</ref>. Thus for a dimension-wise factorized encoder model,</p><formula xml:id="formula_24">Q c (v i |G i ) = d l=1 Q c ([v i ] l |G i ), a standard choice in the VAE-based auto-encoding literature, we can reduce the complexity from O((d • K) 3 ) down to O(d • K 3 ).</formula><p>More formally, the second term in (36) can be written as:</p><formula xml:id="formula_25">d l=1 KL K i=1 Q c ([v i ] l |G i ) P l ([v 1 ] l , . . . , [v K ] l ) . (18)</formula><p>Each summand in <ref type="bibr" target="#b17">(18)</ref> is a KL divergence between Gaussians and can be written (up to constant) as:</p><formula xml:id="formula_26">1 2 Tr(C -1 l S l ) + (a l -m l ) C -1 l (a l -m l ) + log |C l | |S l | ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_27">K i=1 Q c ([v i ] l |G i ) is denoted by N (m l , S l )</formula><p>, with S l diagonal by definition. The inverse and determinant of C l can be computationally tractable as K is typically not large<ref type="foot" target="#foot_1">foot_1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>Learning succinct but meaningful and interpretable representations of data is the main goal of (deep) representation learning. The learned representations can be not just useful for downstream tasks serving as features (e.g., higher-level classification), but also crucial for data analysis that often requires high quality of interpretability. Recent approaches to representation learning broadly fall into three types of learning setups. The first is the supervised setup <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref> that can make full use of factor-labeled data in discovering the underlying factors of variations, however, preparing a rich set of labeled data is often prohibitive in practice. The unsupervised setup <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b7">[8]</ref> aims to learn the factors solely from the observed data without any supervision. To make the latent variables exclusively responsible for the variation of a unique aspect in the observed data, often referred to as the goal of latent disentanglement, most of the approaches augment the objective function of the VAE <ref type="bibr" target="#b17">[18]</ref> with an additional regularization term that encourages factorization/independence of the prior induced from the variational encoder. However, it was recently proved that the unsupervised setup suffers from unidentifiability unless proper inductive bias or regularity conditions are imposed <ref type="bibr" target="#b9">[10]</ref>.</p><p>The semi-supervised setup seeks to remedy the limitations of the previous two extremal setups. We particularly focus on recent work that aimed at modeling specific factors and separating them from the others, similar and closely related to ours. In <ref type="bibr" target="#b15">[16]</ref>, the samples of the reference factor values (e.g., neutral expression for the facial emotion factor) are exploited as weak supervision to capture and learn the visual differences in the images due to the changes in the factor values. In <ref type="bibr" target="#b14">[15]</ref>, the weak supervision is given as partial labels of specific class categories, in which the goal is to make the factor with labels orthogonal to other latents. However, instead of directly modeling the latent vectors, one for each class value, they considered a deep image-to-factor network that takes an image as input and returns a latent vector as output, essentially performing single instance inference instead of the group inference our OL-VAE and ML-VAE adopt. In addition, to separate the labeled factor from others, they formed an objective function as a mix of the conditional GAN loss and the VAE loss, which potentially incurs the sensitivity issue in selecting the hyperparameter that trades off between the two loss terms. Some recent works aim to deal with ordinal-valued labels in the VAE framework. However, their goals and setups are inherently different from ours. Among others, in <ref type="bibr" target="#b21">[22]</ref>, they consider a setup where some ordinal paired data instances are available in the training data. More specifically, in addition to the unlabeled data instances, they have some pairs {(x (a) , x (b) )} such that for a particular factor of interest (denoted by f ), their factor values are ordered f (x (a) ) &gt; f (x (b) ). In <ref type="bibr" target="#b22">[23]</ref>, they deal with an ordinal label problem setup while the idea is to introduce a variational posterior for the ordinal label, which is modeled as an ordinal regressor. The consequence is that, unlike our approach, they do not explicitly enforce the layout of the latent vectors to be aligned with the ordinal constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We demonstrate OL-VAE's capability of separating the content factor from other sources of variation, namely style. We highlight the importance of modeling the ordinal structure of the content factor, a unique property of our OL-VAE, by comparing it to non-ordinal content-style disentangled models, specifically the ML-VAE <ref type="bibr" target="#b16">[17]</ref>. We describe below the datasets we use, detailing the specific choice of the content factor, how the labels are collected and defined.</p><p>• Sprites dataset <ref type="bibr" target="#b23">[24]</ref>. The dataset consists of binary images of sprites with variations in the shape (oval, square, and heart) and four geometric factors: scale (6 variation modes), rotation (40), and X, Y translation (32 modes each). From the original dataset, we form two datasets to test the content-style disentanglement: 1) Rotation-Sprites takes the rotation as the content while the rest four factors (X/Y-pos, shape, and scale) as style, and 2) Scale-Sprites regards the scale as content and the rest as style. Both rotation and scale factors clearly entail ordinal semantics. For both datasets, we use K = 6 ordinal levels by merging adjacent content groups together if needed. We also collect only the images of rotation angles between 0 and 45 degrees to avoid redundancy in the datasets. • IMDB-WIKI dataset <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The dataset contains face images of celebrities (actors/actresses) collected from the IMDB and Wikipedia websites. For each image, the age of the subject is annotated; it is calculated/estimated from the date when the photo was taken and the birth date of the subject. To have tightly cropped face images, we use the face detector outputs comprised of both detection confidence scores and the bounding boxes that are provided in the dataset.  <ref type="bibr" target="#b26">[27]</ref>, which contains clip-art style images of pizza.</p><p>The images are synthetically generated by controlling the ingredients to be placed (ten ingredients, e.g., pepperoni, bacon, black olive, and basil) as well as the view angle, background, and the position of pizza in the image. There are 5,468 images, with 90% / 10% split for train/test using the original protocol. We used the image size (64×64) pixels. We add the calorie content of the pizza to <ref type="bibr" target="#b26">[27]</ref> by computing the calories, albeit not very precisely, using look-up from a standard nutrition table <ref type="foot" target="#foot_2">3</ref> for known pizza ingredients. After computing the calorie values, we discretized them into five groups by roughly equal size binning: lowest, low, medium, high, and highest calorie cohorts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Competing Models</head><p>In the empirical study, we focus on demonstrating the utility of ordinal content prior modeling facilitated by our OL-VAE model, highlighting the improvement over the nonordinal content-style disentangled approaches, namely ML-VAE <ref type="bibr" target="#b16">[17]</ref>. To this end, we make all the setups including the model architectures equal for both models, but the prior modeling part. In particular, for the encoder/decoder architectures, we adopt models similar to those in ML-VAE: compositions of four/five convolutional or transposed convolutional layers and two/three fully connected layers, whereas for the decoder model P (x|v, s), we set the output distribution to be Bernoulli; the normalized pixel value x ∈ [0, 1] is the mean of a Bernoulli process.</p><p>For baselines, we also compare with the VAE model <ref type="bibr" target="#b17">[18]</ref>, and the disentanglement encouraging β-VAE <ref type="bibr" target="#b3">[4]</ref>. These are unsupervised models, unable to utilize the content labels during the model training. For these models, we use similar encoder/decoder architectures, but for fair comparison with  the models with style and content latent vectors, the latent dimensions are set to dim(s) + dim(v), the sum of the style and content dimensions of OL-VAE. More specifically, the latent dimensions are: dim(s) = dim(v) = 50 for IMDB-WIKI and Pizza datasets, and dim(s) = dim(v) = 10 for the sprites datasets. The batch size is 256 for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent to Content Prediction (Quantitative)</head><p>For a well-trained model, we anticipate the content latent v only contains the content information of x, while the style latent vector s should ideally not entail content, instead capturing style. To test this capability, we form a classification problem: for each paired data (x, c), we encode x to have its style/content latent representation, that is, s ∼ Q s (•|x) and v ∼ Q c (•|x), then either s or v serve as input covariates for classification toward the target content class label c. Note that v should be highly predictive of c, while s should not, leading to chance level classifier performance.</p><p>To form the content vector v, we use 20 samples from the same content group (i.e., x's with the same label c) for a group inference as in <ref type="bibr" target="#b15">(16)</ref>. The classifier is trained with the IMDB dataset in the IMDB-WIKI and the preset training split for the Pizza, and we test on the WIKI in the IMDB-WIKI and the preset test split for the Pizza. At test time, we infer the covariates v using M test samples per content class c via the product of experts rule. We vary M from 1 to 20. This setup is applied to OL-VAE and in the same manner. For the baseline VAE and β-VAE, which only have a single latent vector z, we similarly accumulate M group samples at the test time to reinforce the latent inference (i.e., the instance-wise variational posteriors Q(z|x) are accumulated via the product of experts rule to have the covariates z). As the target label c has ordinal scale, we report the absolute error |c -ĉ|, instead of the 0/1 loss.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_3">3</ref> for the IMDB-WIKI and Pizza datasets, and Fig. <ref type="figure" target="#fig_4">4</ref> for Rotation-and Scale- Sprites. For our OL-VAE, prediction using the style latent vector is significantly worse than that when using the content latent, indicating the style vector s carries little information about the content. For example, on IMDB-WIKI the mean abs error (MAE) of the mean age index guess of 5.89 (a strawman predictor) is 2.03, indicating that s is not suggestive of age. Similarly, for the Pizza dataset, the mean guess of 2 would yield the MAE of 1.2, suggesting the lack of dependence between s and the calorie content. More importantly, our OL-VAE, throughout all datasets, achieves the lowest classification error (as M increases) for predicting content using the content latent vector. The classification error is dramatically reduced as we increase the number of samples M in the content group. This means that more evidence is helpful for inferring a correct content latent vector. On the other hand, the performance of ML-VAE trails that of OL-VAE and the strawman alike, often unaffected by the sample size M (e.g., the Pizza dataset), implying that the learned content latent is not salient enough to predict the content value. In summary, these results signify that for high separation between the style and content factors, it is crucial to both preserve the ordinal structure in the latent embedding and align the content vectors with the content labels, as done by our OL-VAE model. For the baseline VAE and β-VAE, increasing the sample size lowers the classification error. However, as there is mixing of content-style in the latent z, predictive value of z is lower than what the isolated content latent could offer (in OL-VAE or ML-VAE); this is reflected in higher content prediction error for those unsupervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of Learned Content Vectors</head><p>To verify our claim that the learned content latent vectors are well aligned with the ordinal labels, we visualize the distance map of pairwise distances ||v i -v j || for i, j = 1, . . . , K. Specifically, for the test sample instances from each content group c = i, we infer v i 's for ML-VAE and OL-VAE using Q c (v|x Gi ). We then compute the pairwise distances to form a (K × K) distance map. The maps are shown in Fig. <ref type="figure" target="#fig_5">5</ref> for the IMDB-WIKI dataset. For OL-VAE, we see the expected low values of distances on the diagonal and increasingly high values in the corners (mimicking a desired absolute loss |c i -c j |, indicated in the ideal map in the left-most image). This pattern is less pronounced for the ML-VAE, which exhibits a deformed heat map, indicating poor alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Swapping Content Values (Qualitative)</head><p>Next, we demonstrate the content swapping capability of our OL-VAE model. More specifically we seek to qualitatively control the content aspect of an image by synthesizing new images of different content values while preserving the original style. This capability is only possible when the model achieves high degree of separation between style and content latents as in our OL-VAE.</p><p>Suppose we use two reference images x st and x co . We encode each of them to obtain their style and content embeddings. More formally,</p><formula xml:id="formula_28">s st ∼ Q s (s|x st ), v st ∼ Q c (v|x st ) (20) s co ∼ Q s (s|x co ), v co ∼ Q c (v|x co )<label>(21)</label></formula><p>As we have a learned decoder model, we can reconstruct the original image using the corresponding pair of style and content latent vectors, namely P (x|v st , s st ). Extending the idea, one can consider swapping of content latents as a task of hallucinating a new image using the style vector s from one source and the content vector v from another.</p><p>For instance, we can generate a new image that possesses the style of x st and the content of x co :</p><p>x co+st ∼ P (x|v co , s st ).</p><p>The better the model separates the content from the style, the more visually clear the chosen style and content will be preserved in the hallucinated image. IMDB-WIKI dataset. To demonstrate this qualitative performance, we collect some random images x st from the test set (i.e., the WIKI dataset), while selecting 17 images x co from the training set (i.e., the IMDB dataset). The swapping results for OLVAE and MLVAE are shown in Fig. <ref type="figure" target="#fig_6">6(a)</ref>. Results of our OL-VAE show that the hallucinated images largely preserve the style aspects of the reference style images (e.g., identity and pose), while conforming to the content values of the reference content images accurately (age).</p><p>Pizza dataset. Pizza calorie (content) swapping results are shown in Fig. <ref type="figure" target="#fig_6">6(b</ref>). The leftmost column contains reference images of style, the top row has content images, calories from low (left) to high (right). Intuitively, swapping content and style here means creating lower or higher calorie visual versions of the original pizza. Although the generated images look slightly blurred, the OL-VAE clearly learns important visual cues about calories such as using green-ish ingredients (vegetables and greens) for low calorie meals and red/black-ish (pepperoni/bacon and olives) for high calorie pizzas. The style aspects are relatively well preserved, notably the size and the location of the pizzas in images.</p><p>Sprites datasets. The results are shown in Fig. <ref type="figure">7</ref> (scale as content) and Fig. <ref type="figure">8</ref> (rotation as content). On the scale sprites dataset, the OL-VAE swapping results visually have higher quality than ML-VAE. In the rotation content case, both OL-VAE and ML-VAE are equally good visually, but the classification performance in the previous section indicates OL-VAE's content-style separation is more significant.</p><p>V. CONCLUSION Isolating content from style is a key task in deep representational learning. In this paper we have shown that a critical step toward that goal is to provide adequate priors of the content latent space, while aligning that space with the available content label. To that end, we focused on a specific setting where the content is ordinal (e.g., age of a person in an image). We proposed a new class of representational models, OL-VAE, characterized by a novel ordinal content space prior, the conditional Gaussian spacing model. The prior effectively enforces the ordinal space constraints as reflected in improved separation of content from style, at the same time allowing computationally tractable learning. To demonstrate the benefits of this model, we used challenging real and synthetic datasets. Quantitative content-prediction metrics and qualitative evaluations based on content-style swapping all indicate that the proposed model offers significant advantages over models that do not take into account the ordinal nature of the underlying content space.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model representation for the proposed OL-VAE. Observed variables, the image/data instance x and the content label c, are shaded.</figDesc><graphic coords="2,429.84,161.45,111.09,56.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Left) K = 4 latent vectors (points) aligned in a poset formation in R 2 . For each vector vi, the adjoined horizontal and vertical lines specify the feasible quadrant where its superiors vj for j &gt; i can be positioned. (Right) Conditional spacing model P (v1, v2, v3) = P (v1)P (v2|v1)P (v3|v2) for K = 3. Our choice of Gaussian conditional densities (6-9) together with the constraints (10) guarantees that the samples from P (v1, v2, . . . , vK ) satisfy v1 ≤ v2 ≤ • • • ≤ vK with high probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>--&gt; calorie group) OL-VAE (s) OL-VAE (c) ML-VAE (s) ML-VAE (c) VAE Beta-VAE Strawman (b) Classification on Pizza</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification results on IMDB-WIKI and Pizza datasets. For OL-VAE and ML-VAE, s and c in the parenthese indicate that style and content latent vectors are used as classification covariates, respectively. Age and calorie variables indicate respective group indexes (thus, error of 1 on e.g., IMDB-WIKI represents 5 years.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Classification results on (a) Scale-Sprites and (b) Rotation-Sprites datasets. For each, (Top) shows classification errors for all models, and (Bottom) highlights the best three models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (IMDB-WIKI dataset) The distance maps of the learned content latent vectors for (b) our OL-VAE and (c) ML-VAE. Each(i, j) entry of the (K ×K) map with K = 17 encodes the distance between two content vectors, ||vi -vj||. Gradual and smooth increase of the distance from diagonals to off-diagonals, which looks more evident in our OL-VAE, indicates good alignment between the ordinal content labels and the positions of the latent vectors. As a reference, in (a) we also visualize an ideal distance map where the latent vectors are equally spaced, that is, vi = vi-1 + δ for all i = 2, . . . , K, with some constant vector δ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Swapping results of OL-VAE. (Top: IMDB-WIKI) The K = 17 content sample images on the top are the samples with increasing content values: c = 1 to c = 17 (also with the age range for each group). (Bottom: Pizza) The K = 5 content sample images on the top are the samples with increasing content (calorie) values: c = 1 to c = 5. For both datasets, the content vectors were inferred with M = 20 samples per group.</figDesc><graphic coords="9,235.62,383.00,141.08,303.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) OL-VAE (b) ML-VAE Figure 7: Scale-Sprites swapping results of OL-VAE and ML-VAE. This is for content vector inference with M = 20 samples per content (scale) group. The 6 content sample images on the top are the samples with increasing content values: c = 1 to c = 6. (a) OL-VAE (b) ML-VAE Figure 8: Rotation-Sprites swapping results of OL-VAE and ML-VAE. This is for content vector inference with M = 20 samples per content (rotation angle) group. The 6 content sample images on the top are the samples with increasing content values: c = 1 to c = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We discarded images containing multiple faces, which was done by collecting only those images with high first face detection score and zero second detection score (not detected). After further removing faulty images contained in the original dataset, we ended up with 224,418 cropped face images (184,363 from IMDB and 40,055 from Wikipedia), out of the original 523,051 images. We used the image size (64×64) pixels, and set IMDB as the training set and Wikipedia as the test set. We let the age serve as the content factor. To this end, we partition age values ranging from 5 to 90 into 17 age groups. Each group covers non-overlapped 5-year segments. The average age group index is 5.89 ± 2.62 (about 34 years old) for both Wikipedia and IMDB. All other factors of variation, including subject ID and head pose, are considered as style (unlabeled). The data is highly noisy and biased, including a mix of gray/color images, cartoon images, and even stamp portraits.• Synthetic Pizza Calorie Dataset. We use the dataset in</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The inequality constraints<ref type="bibr" target="#b9">(10)</ref> can be easily incorporated in the conventional unconstrained optimizer modules such as PyTorch and Ten-sorFlow through trivial reparametrizations, e.g., σ i := ∆ i 3 sigmoid(σ i ) and ∆ i := exp(∆ i ), where ∆ i and σ i are the unconstrained optimization variables, and sigmoid(x) = 1/(1 + exp(-x)). Furthermore, we fix σ 1 = 1 to make the optimization numerically more stable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We utilize the inverse() and cholesky() functions in PyTorch 1.1, which also allow auto-differentiations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We used the nutrition tables in https://modpizza.com/nutrition/, which contain nutrition facts specifically tailored to pizzas. We assume standard ingredient amounts.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of the Joint Gaussian form</head><p>We provide full derivations for the joint Gaussian forms <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref> of our conditional Gaussian spacing model <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref> in the main paper. From the conditional model in the main paper, we have, for i = 2, . . . , K,</p><p>We rewrite the above as a stochastic equation form as follows, by introducing the random variables i ∼ N (0, σ 2 i ), each of which accounts for the randomness of v i conditioned on v i-1 :</p><p>Note that 's are independent with each other, and in particular Cov( i , j ) = 0 if i = j. From the recursion (24), we can write v i as (for i ≥ 2):</p><p>where we use v 1 = µ 1 + 1 with 1 ∼ N (0, σ 2 1 ) as the initial equation. Now, the mean of v i is straightforwardly derived as:</p><p>Moreover, assuming i ≤ j without loss of generality,</p><p>where from ( <ref type="formula">28</ref>) to (29), we use the bilinearity of the covariance operator and mutual independence of 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Content Group Inference via Product of Experts</head><p>As described in the paper, the content latent vector can be inferred from a set of observations {x n } n∈G of the same content group G, via the product of experts rule:</p><p>We derive a full formula for Q c v|{x n } n∈G for the Gaussian encoder network Q c (v|x) = N (v; m(x), S(x)) where S(x) is a diagonal covariance matrix function.</p><p>It is well known that the product of two Gaussian distributions is a Gaussian up to a constant factor, that is,</p><p>where</p><p>2 µ 2 . The proof for the above is from straightforward algebra that reduces to combining two quadratic exponents from the two Gaussians. Extension to product of multiple (more than two) Gaussians is also straightforward, which simply amounts to applying the bi-product result recursively. The posterior of the group inference can then be written as follows:</p><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variational Lower Bound (ELBO)</head><p>We show that the variational lower bound (ELBO) for the data log-likelihood, can be expressed as (up to some constant):</p><p>To this end, we begin with the KL divergence between the true posterior distribution and our variational approximation.</p><p>With the full joint distribution of our model ((3) in the paper),</p><p>The last expectation term becomes identical to the first term of (36) due to the decomposition of the variational density Q. Using the fact that KL divergence (37) is non-negative, we have (36) as a lower bound of the data log-likelihood.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.056441" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005">2017. 1, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning independent features with adversarial nets for non-linear ICA</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.050501" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variation inference of disentangled latent concepts from unlabeled observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018, International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangling disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weaklysupervised disentangling with recurrent transformations for 3D view synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding visual concepts with continuation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016, International Conference on Learning Representation, Workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representations using adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning disentangled representations with reference-based variational autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno>abs/1901.08534</idno>
		<ptr target="http://arxiv.org/abs/1901.085341" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-level variational autoencoder: Learning disentangled representations from grouped observations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conferenceon Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Learning Representations</title>
		<meeting>the Second International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Learning Representations (ICLR</title>
		<meeting>the Second International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The variational homoencoder: Learning to learn high capacity generative models from few examples</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation using few labels</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.012584" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1258">1905.01258, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust ordinal VAE: Employing noisy pairwise comparisons for disentanglement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05898.5</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A novel variational autoencoder with applications to generative modelling, classification, and ordinal regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jaskari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07352.5</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/5" />
		<title level="m">dSprites: Disentanglement testing Sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2005">July 2016. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How to make a pizza: Learning a compositional layer-based gan model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">June 2019. 5</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
