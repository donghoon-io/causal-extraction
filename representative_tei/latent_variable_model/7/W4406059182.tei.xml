<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes</title>
				<funder>
					<orgName type="full">Solution Seeker Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-18">18 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bjarne</forename><surname>Grimstad</surname></persName>
							<email>bjarne.grimstad@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Solution Seeker AS</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kristian</forename><surname>Løvland</surname></persName>
							<email>kristian.lovland@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Solution Seeker AS</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lars</forename><forename type="middle">S</forename><surname>Imsland</surname></persName>
							<email>lars.imsland@ntnu.no</email>
							<affiliation key="aff0">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
								<address>
									<settlement>Trondheim</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vidar</forename><surname>Gunnerud</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Solution Seeker AS</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-18">18 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2407.13310v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many industrial processes, an apparent lack of data limits the development of data-driven soft sensors. There are, however, often opportunities to learn stronger models by being more data-efficient. To achieve this, one can leverage knowledge about the data from which the soft sensor is learned. Taking advantage of properties frequently possessed by industrial data, we introduce a deep latent variable model for semi-supervised multi-unit soft sensing. This hierarchical, generative model is able to jointly model different units, as well as learning from both labeled and unlabeled data.</p><p>An empirical study of multi-unit soft sensing is conducted using two datasets: a synthetic dataset of single-phase fluid flow, and a large, real dataset of multi-phase flow in oil and gas wells. We show that by combining semi-supervised and multi-task learning, the proposed model achieves superior results, outperforming current leading methods for this soft sensing problem. We also show that when a model has been trained on a multi-unit dataset, it may be finetuned to previously unseen units using only a handful of data points. In this finetuning procedure, unlabeled data improve soft sensor performance; remarkably, this is true even when no labeled data are available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern process industries are increasingly concerned with sustainable development to enhance product quality and process efficiency while minimizing adverse environmental impacts <ref type="bibr" target="#b0">[1]</ref>. Sustainable development is helped by novel technologies for process monitoring and control, enabled by the ongoing digitalization in the process industries <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. One example is the development of cost-efficient soft sensors as an alternative to hardware sensors for process monitoring <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>A soft sensor is a mathematical model that operates on process measurements (x) to make timely inferences about a variable of interest (y). Often, y is a key quality or performance indicator that is hard to measure directly and thus is measured occasionally. This may be the case if a measurement requires an expensive or disruptive experiment. Process measurements, x, from existing sensors are comparatively cheap and frequent. If x is informative of y, a discriminative model p(y | x) can be developed and applied to predict y conditionally on x. With this model, soft sensing enables indirect monitoring of y in periods where only x is measured.</p><p>In data-driven soft sensing, the model is learned from historical data using statistical modeling or machine learning techniques. The attraction to datadriven soft sensing is two-fold: first, it may exploit the increasing amount of data to improve models; second, it promises to reduce costs by simplifying modeling. This is opposed to soft sensing based on mechanistic models or first principles, which may require substantial investments and domain expertise to develop and maintain <ref type="bibr" target="#b3">[4]</ref>. In recent years, deep learning has been widely applied to data-driven soft sensing with varying degrees of success <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>The main challenge in data-driven soft sensing is arguably to learn a model when the data volume, measurement frequency, variety, or quality is low <ref type="bibr" target="#b3">[4]</ref>. The challenge is closely tied to the motivation for implementing soft sensing: to infer a key variable that is measured infrequently. To achieve a good predictive performance in information-poor environments, it is imperative to be data efficient. In some circumstances, there is an opportunity to improve data efficiency by modeling on more data. Semi-supervised learning (SSL) and multitask learning (MTL) are two learning paradigms which have been employed in soft sensing to this end. Consider the multi-unit setting in Figure <ref type="figure">1</ref>, where a soft sensor is developed on data collected from multiple process units. In this example, the units may for instance represent similar machines that perform the same operation in multiple production lines. A model learned by SSL and MTL may utilize the whole dataset, learning across units and from unlabeled data. In many cases, one would expect that such a model will be able to exploit the similarity between units as well as the additional unlabeled data and outperform models developed per unit which only use labeled data.</p><p>There is an inherent opportunity for SSL in many soft sensor applications, due to the higher measurement frequency of x (compared to y). With an SSL method, the model is learned from both labeled and unlabeled data <ref type="bibr" target="#b5">[6]</ref>. Many SSL methods are based on a generative model of the joint distribution p(x, y). As made clear by the factorization p(x, y) = p(y | x)p(x), generative models are more ambitious in terms of their modeling scope, as they approximate both the conditional distribution p(y | x) and the marginal distribution p(x). SSL methods can provide a powerful regularization mechanism to the model via p(x), given that an a priori dependence between p(y | x) and p(x) is allowed, Figure <ref type="figure">1</ref>: Illustration of multi-unit data generation. The units are similar, and can be thought of as realizations of a common "prototype" unit. Each unit gives rise to a large amount of unlabeled data (red boxes), and a small amount of labeled data (green boxes). and that the true data generating system exhibits such a dependence <ref type="bibr" target="#b6">[7]</ref>. This is not always true; for instance, it was shown in <ref type="bibr" target="#b7">[8]</ref> that semi-supervised learning will not outperform supervised learning if x is a cause of (and not caused by) y.</p><p>In soft sensing problems, there is arguably often not a clear direction of causality, and for dynamical systems and their steady-states, directions of causality may even be interpreted as going both ways <ref type="bibr" target="#b8">[9]</ref>. In that case, one can only conclude that semi-supervised learning may be advantageous <ref type="bibr" target="#b7">[8]</ref>.</p><p>In settings where p(x) does contain information about p(y | x), the advantage of using SSL is largest when the ratio of unlabeled to labeled data is large. However, the additional factor p(x) brings with it additional modeling assumptions, and model misspecification can be harmful to the model performance <ref type="bibr" target="#b9">[10]</ref>. Still, SSL has been successfully used to develop soft sensors for a wide range of processes, including: polymerization process <ref type="bibr" target="#b10">[11]</ref>, debutanizer column process <ref type="bibr" target="#b11">[12]</ref>, CTC fermentation process <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, ammonia synthesis process <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, biochemical process <ref type="bibr" target="#b17">[18]</ref>, with more references given in <ref type="bibr" target="#b2">[3]</ref>.</p><p>A second opportunity for improved data efficiency is to jointly model multiple process units, as opposed to developing a separate model per unit. If multiple units of the same type are installed in different locations (as illustrated in Figure <ref type="figure">1</ref>), then it may be possible to learn across their operational data. The MTL paradigm formalizes learning across tasks; in this example, a task would be to learn a soft sensor model for a single process unit. MTL methods may improve data efficiency by sharing statistical strength between learning tasks. In cases where there is little data per task, but many tasks, MTL may significantly outperform single-task learning methods <ref type="bibr" target="#b18">[19]</ref>. However, because MTL applied to multi-unit data requires data from multiple units, it has been used less than SSL in soft sensing applications. A survey on transferability in soft sensing is given in <ref type="bibr" target="#b19">[20]</ref>, and more recently in <ref type="bibr" target="#b20">[21]</ref>. Transfer learning and MTL have recently been used in the development of soft sensors for different processes, including: polyethylene process <ref type="bibr" target="#b21">[22]</ref>, virtual flow metering <ref type="bibr" target="#b22">[23]</ref>, solid waste incineration process <ref type="bibr" target="#b23">[24]</ref>, and a sulfur recovery unit process <ref type="bibr" target="#b24">[25]</ref>.</p><p>This paper shows how to leverage the two learning paradigms of MTL and SSL to obtain a data-efficient method for data-driven soft sensing. We model the data using a deep latent variable model (DLVM), a type of generative model for which conditional probability distributions are represented by deep neural networks <ref type="bibr" target="#b25">[26]</ref>. The generic model relies on two hierarchical levels of latent variables, c and z, to explain the observations (x, y). Building on the framework of the variational auto-encoder <ref type="bibr" target="#b26">[27]</ref>, we show how to make inferences for latent variables and estimate the model parameters from data. The main use case of our model and the focus of this paper is online prediction, for which accurate and timely inferences of y are required. In addition to the use we consider here, the generative model has additional applications relevant to soft sensing, which are not admitted by discriminative models. First, it enables data generation, which can be used in what-if scenario analyses. Second, it can be used to impute missing data. A third use is sensor surveillance, where data deemed improbable by the model may indicate sensor faults.</p><p>Our method is based on the variational autoencoder (VAE), which was first adapted to semi-supervised learning in <ref type="bibr" target="#b27">[28]</ref>. Several variants of semi-supervised VAE have later been developed <ref type="bibr" target="#b28">[29]</ref>. Most of these are specialized to semisupervised classification. There are some notable works on the combination of SSL and MTL for both classification and regression problems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Of these, the SAML method for classification, presented in <ref type="bibr" target="#b31">[32]</ref>, is perhaps closest to our method. Within data-driven soft sensing, the combination of SSL and transfer learning across operating conditions of a single process unit in <ref type="bibr" target="#b32">[33]</ref> is close in spirit to our method. Our work is different since it combines SSL with MTL in a multi-unit soft sensor setting. To the authors' knowledge, no previous work have explored this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Organization of the paper</head><p>The paper is structured as follows. Section 2 presents the problem statement. The model is presented in Section 3 and the learning method in Section 4. A comparison to related methods is given in Section 5. An empirical study of the method is given in Section 6 and the results are discussed in Section 7. Finally, some concluding remarks are given in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notation</head><p>Sets of variables are compactly represented by bold symbols, e.g. c = {c i } i . For double-indexed variables we write x = {x ij } ij and x i = {x ij } j , which permits us to also write x = {x i } i . When the indices are irrelevant to the discussion, we may write x instead of x ij .</p><p>For a positive integer K, we denote the K-vector of zeros by 0 K and the K × K identity matrix by I K . For a K-vector σ = (σ 1 , . . . , σ K ), log σ denotes the element-wise logarithm, and diag(σ 2 ) denotes the diagonal (K × K)-matrix with diagonal elements (σ 2 1 , . . . , σ 2 K ). The normal distribution is denoted by N (µ, Σ), where µ and Σ is the mean and covariance matrix, respectively. For a random variable X ∼ p(x), we denote the expectation operator by E[X]. We will sometimes write E X [X] or E p [X] to indicate that the expectation is of a specific random variable or probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>Consider a set of M &gt; 1 distinct, but related units indexed by i ∈ {1, . . . , M }. The units may be a set of objects, e.g. pumps, valves, or solar panels, or a set of complex systems, such as distillation columns, oil wells, or robots. The units are assumed to be related so that it is reasonable to expect, a priori to seeing data, that transfer learning between units is beneficial.</p><p>We assume that the same explanatory variables x ∈ R Dx and target variables y ∈ R Dy can be considered for all units. In soft sensor applications, x typically represents cheap and abundant measurements from which we wish to infer a target variable y which is expensive or difficult to measure. For each unit i, we have at our disposal a set of N l i labeled data D l i = {(x l ij , y l ij )} j and a set of N u i unlabeled data D u i = {x u ij } j . We collect the N i = N l i + N u i data points of unit i in the dataset D i = D l i ∪ D u i . We assume that D i consists of independent and identically distributed (i.i.d.) observations drawn from a probability distribution p i over R Dx×Dy .</p><p>Our goal is to efficiently learn the behavior of the units, as captured by p 1 , . . . , p M , from the data collection D = {D 1 , . . . , D M }. In particular, we wish to exploit that some structures or patterns are common among units. Furthermore, to fully utilize the information in D, we wish to learn from unlabeled data, which are overrepresented in most soft sensor applications.</p><p>Invariably, distinct units will differ in various ways, e.g., they may be of different design or construction, they may be in different condition, or they may operate under different conditions. The properties or conditions that make units distinct are referred to as the context of a unit. An observed context may be included in the dataset as an explanatory variable. An unobserved context must be treated as a latent variable to be inferred from data and it is only possible to make such inferences by studying data from multiple, related units. We assign the letter c to latent context variables.</p><p>We will assume that there is some underlying process p at population level, such that p i = p(• | c i ), where c i ∈ R K is a latent variable drawn from p(c). The variable c i represents unobserved properties or context of unit i. We can now restate our goal as learning from D a generative model p(x, y) which approximates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A generative model for multi-unit soft sensing</head><p>We consider a hierarchical model for generating observations of a population of units. Observations are split semantically so that x labels cheap and frequent observations, and y labels expensive and occasional observations. The model has three levels of latent variables that explain the observations. At observation level, z represents a latent state which is (partly) observed by x and y. At unit level, the latent variable c represents the context or specificity of a unit. At the universal top-level, θ represents properties which are shared by all units and observations. The hierarchical structure allows the model to capture variations at the level of units and observations. A graphical illustration is given in Figure <ref type="figure" target="#fig_0">2</ref>. In the figure, θ is treated as a parameter.</p><p>The model assumes the following generative process for a collection D of observations from M units. For each unit i = 1, . . . , M :</p><formula xml:id="formula_0">c i ∼ p(c) = N (0 K , I K ), z ij ∼ p(z) = N (0 D , I D ), j = 1, . . . , N i , x ij | z ij , c i ∼ p θ (x | z ij , c i ), j = 1, . . . , N i , y ij | z ij , c i ∼ p θ (y | z ij , c i ), j = 1, . . . , N i .<label>(1)</label></formula><p>At the unit level, latent context variables c i ∈ R K are generated from a common prior distribution p(c) = N (0 K , I K ). At the level of observation, latent variables z ij ∈ R D are generated from a common prior p(z) = N (0 D , I D ). The standard normal priors on the latent variables allow for easy sampling and reparameterization, and is commonly used in variational autoencoders <ref type="bibr" target="#b25">[26]</ref>. Observations x and y are generated conditionally independent given the latent variables z and c. That is,</p><formula xml:id="formula_1">p θ (x, y | z, c) = p θ (x | z, c)p θ (y | z, c</formula><p>). For the continuous variables x and y, an appropriate likelihood function may be the multivariate normal</p><formula xml:id="formula_2">p θ (x, y | z, c) = N (µ θ (z, c), Σ θ (z, c)),</formula><p>where µ θ (z, c) and Σ θ (z, c) are neural networks with parameters θ. The resulting model is a type of deep latent variable model and provides an expressive family of conditional distributions for modeling x, y | z, c.</p><p>Several simplifying assumptions are made in the above model: 1) the dimensions of the latent variables, K and D, are fixed and considered to be choices of design; 2) θ is treated as a parameter vector to be estimated, not as a latent variable with a prior p(θ); 3) no hyper-priors are put on the parameters of p(z) and p(c). In a fuller Bayesian approach, it would be natural to remove assumption 2 and 3, and include a hyper-prior also for p(θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Missing target values and grouping of variables</head><p>As indicated in Figure <ref type="figure" target="#fig_0">2</ref> by the partly shaded circle for y, targets may be observed or missing. For unit i, we denote an observed target by y l ij and a missing target by y u ij . Corresponding observations of x are denoted by x l ij and x u ij . We group these variables in the sets x l i , y l i , x u i , and y u i . Note that x i = x l i ∪ x u i and y i = y l i ∪ y u i . We group observed variables for unit i in</p><formula xml:id="formula_3">D i = {D l i , D u i }, where D l i = {(x l ij , y l ij )} j is the labeled data and D u i = {x u ij } j</formula><p>is the unlabeled data. The remaining variables for unit i are the latent variables which we group in</p><formula xml:id="formula_4">U i = {y u i , z i , c i }.</formula><p>When considering all units, the subscript i is dropped and we write D for all observed variables and U for all latent variables. Any one variable in x, y, z, or c will be either in D or U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint distribution and marginal likelihood</head><p>The joint distribution of all variables in (1) factorizes as follows</p><formula xml:id="formula_5">p θ (D, U) = M i=1 p(c i ) Ni j=1 p θ (x ij | z ij , c i )p θ (y ij | z ij , c i )p(z ij ).</formula><p>(</p><formula xml:id="formula_6">)<label>2</label></formula><p>The marginal likelihood of D is obtained by marginalizing out all latent variables U in (2):</p><formula xml:id="formula_7">p θ (D) = M i=1   (x l ,y l )∈D l i p θ (x l , y l | c) x u ∈D u i p θ (x u | c)p(c)dc   ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_8">p θ (x, y | c) = p θ (x | z, c)p θ (y | z, c)p(z)dz, p θ (x | c) = p θ (x, y | c)dy = p θ (x | z, c)p(z)dz.</formula><p>The calculation in ( <ref type="formula" target="#formula_7">3</ref>) is intractable due to the integration over high-dimensional and complex conditional distributions that are parameterized by deep neural networks. Maximum likelihood estimation of θ is thus not possible. In the following section, we derive an approximate inference procedure to enable the estimation of θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning method</head><p>Application of Bayes' theorem to the model in <ref type="bibr" target="#b1">(2)</ref> results in an alternative expression for the marginal log-likelihood:</p><formula xml:id="formula_9">log p θ (D) = log p θ (D, U) -log p θ (U | D).<label>(4)</label></formula><p>The above expression includes the posterior distribution of the latent variables, p θ (U | D). Since the model in ( <ref type="formula" target="#formula_6">2</ref>) is tractable by design, it follows from the intractability of the marginal distribution in (3) that the posterior must be intractable. Exact inference of the latent variables is thus unachievable. This is a general issue with DLVMs and motivates the use of approximate inference methods.</p><p>In variational inference, the posterior is approximated by an inference model q ϕ (U | D) with variational parameters ϕ. The inference model is used to derive the following lower bound on the marginal log-likelihood:</p><formula xml:id="formula_10">log p θ (D) ≥ L θ,ϕ (D) := E q ϕ [log p θ (D, U) -log q ϕ (U | D)] ,<label>(5)</label></formula><p>where the expectation is over values of latent variables U drawn from q ϕ . A derivation and discussion of this bound, which often is referred to as the evidence lower bound (ELBO), can be found in <ref type="bibr" target="#b33">[34]</ref>. Maximization of the ELBO with respect to the parameters (θ, ϕ) achieves two goals: i) it maximizes the marginal likelihood, and ii) it minimizes the KL divergence between the approximation q ϕ (U | D) and the posterior p θ (U | D). Another advantage with this strategy is that it permits the use of stochastic gradient descent methods which can scale the learning process to large datasets.</p><p>The next sections are spent on the derivation of the inference model and optimization method. Implementation details are provided at the end of the chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inference model</head><p>Our choice of inference model removes some of the complicating dependencies in the posterior. To motivate our simplifications, we consider the following factorization of the posterior distribution:</p><formula xml:id="formula_11">p θ (U | D) = p θ (z | D, y u , c)p θ (y u | D, c)p θ (c | D).<label>(6)</label></formula><p>The first factor in ( <ref type="formula" target="#formula_11">6</ref>) can be written as:</p><formula xml:id="formula_12">p θ (z | D, y u , c) = M i=1 Ni j=1 p θ (z ij | x ij , y ij , c i ). (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>From this expression, we see that the latent variable z ij is inferred from x ij , y ij and c i . Evidently, inference of z ij requires access to y ij , which may be observed (y ij ∈ y l ) or inferred (y ij ∈ y u ). The posterior factors in <ref type="bibr" target="#b6">(7)</ref> are approximated by a variational distribution q ϕz (z | x, y, c) with parameters ϕ z . The second factor in ( <ref type="formula" target="#formula_11">6</ref>) can be written as:</p><formula xml:id="formula_14">p θ (y u | D, c) = M i=1 N u i j=1 p θ (y u ij | x u ij , c i ).<label>(8)</label></formula><p>A variational distribution q ϕy (y | x, c) with parameters ϕ y is used to approximate the posterior factors in <ref type="bibr" target="#b7">(8)</ref>. Notice the resemblance to a discriminative model, where y is conditioned on an input x and context c. The third factor in (6) can be written as:</p><formula xml:id="formula_15">p θ (c | D) = M i=1 p θ (c i | D i ). (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>The latent variable c i is inferred from all observations in D i and can be thought of as a vector of summary statistics of the dataset. Conditioning the latent variable on the full dataset can be challenging for large datasets and it requires a model that can handle different dataset cardinalities. The inference is simplified by allowing an unconditional approximation of the form q ϕci (c i ), where ϕ ci are parameters related to variable c i . An argument for this simplification is that the latent variable is likely to change little when new data points are added to the dataset; i.e., as evidence is accumulated for the context variable c i , the variance of p θ (c i | D i ) is expected to shrink.</p><p>To summarize, we have motivated the following inference model to approximate (6): where we have denoted the variational parameters by ϕ = {ϕ z , ϕ y , ϕ c } and ϕ c = {ϕ c1 , . . . , ϕ cM }. The inference model is shown graphically in Figure <ref type="figure" target="#fig_1">3</ref>.</p><formula xml:id="formula_17">q ϕ (U | D) = M i=1 q ϕci (c i ) N u i j=1 q ϕy (y u ij | x u ij , c i ) Ni j=1 q ϕz (z ij | x ij , y ij , c i ),<label>(10)</label></formula><p>In the following, the variational distributions in <ref type="bibr" target="#b9">(10)</ref> are chosen to be meanfield normal:</p><formula xml:id="formula_18">q ϕci (c i ) = N (m ci , diag(s 2 ci )), for i = 1, . . . , M,<label>(11)</label></formula><formula xml:id="formula_19">q ϕy (y | x, c) = N (m y , diag(s 2 y )),<label>(12)</label></formula><formula xml:id="formula_20">q ϕz (z | x, y, c) = N (m z , diag(s 2 z )),<label>(13)</label></formula><p>where we have used the symbols m and s for the mean and standard deviation, respectively. The conditional distributions of y and z are computed as (m y , log s y ) = g ϕy (x, c) and (m z , log s z ) = g ϕz (x, y, c), where g ϕy and g ϕz are neural networks. The inference networks amortize the cost of inference by utilizing a set of global variational parameters (ϕ y and ϕ z ), instead of computing variational parameters per data point. In variational autoencoders, an inference network is usually called an encoder.</p><p>The parameters of the unconditional variational distribution for c i is simply computed as (m ci , log s ci ) = ϕ ci . Thus, the distribution of the context variable c i ∈ R K has 2K parameters for each task i.</p><p>With the inference model fully described, a leaner notation is afforded and subsequently we refer to the variational distributions by q ϕ (•), where the arguments indicate included factors and parameters. For example, we write q ϕ (y, z | x, c) = q ϕz (z | x, y, c)q ϕy (y | x, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Objective function</head><p>With the inference model in <ref type="bibr" target="#b9">(10)</ref>, the ELBO in (5) becomes:</p><formula xml:id="formula_21">L θ,ϕ (D) = M i=1 L θ,ϕ (D i ), (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>where the terms related to unit i are collected in</p><formula xml:id="formula_23">L θ,ϕ (D i ) := x u ∈D u i E q ϕ (y,z,ci | x u ) log p θ (x u , y, z | c i ) q ϕ (y, z | x u , c i ) + x l ,y l ∈D l i E q ϕ (z,ci | x l ,y l ) log p θ (x l , y l , z | c i ) q ϕ (z | x l , y l , c i ) -D KL (q ϕ (c i ) ∥ p θ (c i )) . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>Notice that the expectations in L θ,ϕ (D i ) are over the context variable c i of unit i. The final term is the Kullback-Leibler divergence of the variational distribution q ϕ (c i ), which can be computed analytically when both q ϕ (c i ) and p θ (c i ) are normal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization method</head><p>We wish to optimize the ELBO in ( <ref type="formula" target="#formula_21">14</ref>) using a gradient-based method. This requires us to compute</p><formula xml:id="formula_25">∇ θ,ϕ L θ,ϕ (D) = ∇ θ,ϕ E q ϕ [log p θ (D, U) -log q ϕ (U | D)] ,<label>(16)</label></formula><p>where the operator ∇ θ,ϕ produces the gradient with respect to the parameters θ and ϕ. The gradient computation in ( <ref type="formula" target="#formula_25">16</ref>) is problematic since the expectation on the right-hand side depends on ϕ. A common trick to circumvent this problem is to reparameterize the latent variables, here y u , z, and c. Utilizing the reparameterization trick, a one-sample Monte-Carlo approximation of the ELBO is derived:</p><formula xml:id="formula_26">L θ,ϕ (D) ≃ Lθ,ϕ (D) = M i=1 Lθ,ϕ (D i ),<label>(17)</label></formula><p>where Lθ,ϕ (D i ) :=</p><formula xml:id="formula_27">x u ∈D u i log p θ (x u , ỹ, z | ci ) q ϕ (ỹ, z | x u , ci ) + x l ,y l ∈D l i log p θ (x l , y l , z | ci ) q ϕ (z | x l , y l , ci ) -D KL (q ϕ (c i ) ∥ p θ (c i )) .<label>(18)</label></formula><p>Algorithm 1 SGVB estimator for the proposed model Require: data D, generative model p θ , and inference model q ϕ . 1: L ← 0 2: for i = 1, . . . , M do 3:</p><formula xml:id="formula_28">ε c ∼ N (0 K , I K ) 4: ci ← m ci + s ci ⊙ ε c 5: for x u ∈ D u i do ▷ Unlabeled data 6:</formula><p>(m y , log s y ) ← g ϕy (x u , ci )</p><formula xml:id="formula_29">7:</formula><p>ỹ ← m y + s y ⊙ ε y 8:</p><p>(m z , log s z ) ← g ϕz (x u , ỹ, ci )</p><formula xml:id="formula_30">9: z ← m z + s z ⊙ ε z 10: L ← L + log p θ (x u ,ỹ,z | ci) q ϕ (ỹ,z | x u ,ci)</formula><p>11:</p><p>end for 12:</p><p>for (x l , y l ) ∈ D l i do ▷ Labeled data 13:</p><p>(m z , log s z ) ← g ϕz (x l , y l , ci )</p><p>14:</p><formula xml:id="formula_31">z ← m z + s z ⊙ ε z 15: L ← L + log p θ (x l ,y l ,z | ci) q ϕ (z | x l ,y l ,ci) 16:</formula><p>end for 17:</p><formula xml:id="formula_32">L ← L -D KL (q ϕ (c i ) ∥ p θ (c i ))</formula><p>▷ Computed analytically 18: end for 19: Compute ∇ θ,ϕ L using back-propagation 20: return ∇ θ,ϕ L</p><p>In <ref type="bibr" target="#b17">(18)</ref>, the context variable is sampled once (per unit) by computing ci = m ci +s ci ⊙ε c , where ε c ∼ N (0 K , I K ). The latent variables z and y u are sampled per data point using the inference networks:</p><p>(m y , log s y ) = g ϕy (x, ci ),</p><formula xml:id="formula_33">ε y ∼ N (0 Dy , I Dy ), ỹ = m y + s y ⊙ ε y , (m z , log s z ) = g ϕz (x, ỹ, ci ), ε z ∼ N (0 D , I D ), z = m z + s z ⊙ ε z .<label>(19)</label></formula><p>Note that in the second summation in (18) both x and y are observed, and z is sampled by first evaluating (m z , log s z ) = g ϕz (x, y, ci ).</p><p>The gradient of the estimator in ( <ref type="formula" target="#formula_26">17</ref>) is known as the Stochastic Gradient Variational Bayes (SGVB) estimator <ref type="bibr" target="#b26">[27]</ref>. It is an unbiased estimator of the ELBO gradient, ∇ θ,ϕ L θ,ϕ (D), and can be computed efficiently using backpropagation due to the reparameterization. Pseudocode for the estimator is given in Algorithm 1, and information flow through the model is illustrated in Figure <ref type="figure">4</ref>.</p><p>Figure <ref type="figure">4</ref>: Illustration of model architecture when y is unobserved. When y is observed, the encoder q φ (y | x, c) is unused, and y is fed straight into the encoder q φz (z | x, y, c). In either case, the output is an approximation of p θ (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Augmenting the objective function to emphasize inference of y</head><p>As can be seen by inspecting <ref type="bibr" target="#b14">(15)</ref>, the inference model q ϕy (y | x, c) only enters in terms with unlabeled data. If the inference model is to be used for prediction, it is unfortunate that it will not be trained directly on labeled data. This issue, which persists for many different generative models developed for semisupervised learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, is commonly resolved by augmenting the ELBO with a labeled data log-likelihood term for the inference model. For our model, this corresponds to adding the following term to the ELBO:</p><formula xml:id="formula_34">J θ,ϕ (D) = 1 N L θ,ϕ (D) + α 1 N l M i=1 x l ,y l ∈D l i E q ϕ (ci) log q ϕ (y l | x l , c i ) .<label>(20)</label></formula><p>Above, α was introduced as a weighting factor to balance the influence of labeled data relative to unlabeled data on the objective function. Notice that the two terms are scaled by the respective number of data points used in their computation, N and N l . The stochastic gradient variational Bayes method in Section 4.3 also applies to the augmented objective in <ref type="bibr" target="#b19">(20)</ref>. We only need to form a Monte-Carlo approximation of the log-likelihood term using the same reparameterization trick to draw context variables, ci . The gradient of both terms can then be computed using back-propagation.</p><p>The augmented objective function gives us a new perspective on the ELBO. If we consider our main objective to be the learning of a discriminative model, q ϕ (y|x, c), so that we can make inferences of y in online prediction, then the ELBO term in (20) can be viewed as nothing more than an advanced regularization term. With this interpretation, the z-encoder and decoder, which are used to form the ELBO, can be discarded after training as their only purpose is to regularize the y-encoder (i.e. the discriminative model) during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to related generative models</head><p>In this section, we compare the model proposed in Section 3 to some closely related generative models found in the literature. The discussed models extend the variational autoencoder (VAE), originally presented for unsupervised learning, to new modeling paradigms. To simplify the comparisons, we focus more on the architectures of the generative models and less on the related inference methods. We also briefly consider the connection between our proposed model and other hierarchical models for multi-task learning.</p><p>VAE for unsupervised learning. The basic VAE assumes the following generative model:</p><formula xml:id="formula_35">p θ (x, z) = p θ (x | z)p(z),</formula><p>where x is the data and z is a latent variable <ref type="bibr" target="#b25">[26]</ref>. A simple normal prior p(z) = N (0, I) for the latent variables is combined with an expressive conditional distribution, p θ (x | z). Expressiveness is obtained by parameterizing the conditional distribution by a neural network. The generative model, p θ (x, z), is thus a deep latent variable model (DLVM). The VAE is used for unsupervised learning since the marginal distribution of x is approximated by p θ (x) = p θ (x, z)dz.</p><p>VAE for semi-supervised learning. The VAE was first applied to semisupervised classification by <ref type="bibr" target="#b27">[28]</ref>. The generative model of the VAE was extended to include an occasionally observed label y. The resulting model, originally named M2, assumed the following process z → x ← y.</p><p>Several works have later proposed different extensions of the VAE to enable semi-supervised learning; cf. <ref type="bibr" target="#b28">[29]</ref>. Figure <ref type="figure" target="#fig_2">5a</ref> shows one extension where the data is modeled by a common latent variable z. The model, which assumes the generative process x ← z → y, was used in the soft sensing method of <ref type="bibr" target="#b10">[11]</ref>.</p><p>Multi-level VAE for grouped data. The VAE assumes i.i.d. data and is not suitable for grouped data. To handle non-i.i.d. grouped data, the multilevel VAE (ML-VAE) adds a second level to the VAE <ref type="bibr" target="#b36">[37]</ref>. At the second level, a latent group/context variable c is introduced, as shown in Figure <ref type="figure" target="#fig_2">5b</ref>. The ML-VAE then models p θ (x, z | c) = p θ (x | z, c)p(z), where the conditioning on c allows the i.i.d. assumption on within-group data.</p><p>The ML-VAE can be viewed as a conditional VAE by considering that the model p θ (x, z | c) factorizes as above under the independence assumption z ⊥ ⊥ c <ref type="bibr" target="#b37">[38]</ref>. Several variations of the multi-level VAE exist in the literature; one example is the model in <ref type="bibr" target="#b38">[39]</ref>.</p><p>Semi-supervised learning on grouped data. As illustrated in Figure <ref type="figure" target="#fig_2">5</ref>, the SSMTL model is obtained by combining the structures of the SSL-VAE and ML-VAE. The observation-level structure of the SSMTL model is similar to the SSL-VAE. And like the ML-VAE, an additional level with latent variables c is introduced to model variations among groups/units. The resulting structure of the SSMTL model enables semi-supervised learning on grouped data.</p><p>Multi-task learning. The learning problem solved by the SSMTL model can also be framed as a multi-task learning problem. In this frame, the tasks are to model the data for each of the M groups/units. The hierarchical structure of the SSMTL model allows commonalities among tasks to be captured by the θ parameters, while task specificities are modeled by the context variables, c. In the MTL framework, the tasks are simultaneously solved, as in the inference method of the SSMTL model.</p><p>In the following numerical study, we will compare our model to a simpler model for supervised MTL. The MTL model, shown in Figure <ref type="figure" target="#fig_3">6</ref>, is given as p θ (y | x, c)p(c). This is a discriminative model since p(x) is not modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case study: Virtual Flow Metering</head><p>In this section, the proposed SSMTL method is evaluated on the soft sensing problem of Virtual Flow Metering (VFM), which is introduced in Section 6.1. In Section 6.2, the datasets used in the case study are described. Experimental results are presented in Section 6.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Virtual Flow Metering</head><p>In the petroleum industry, many critical decisions rely on measurements of flow. However, the flow from petroleum wells consists of multiple phases (oil, gas and water), which is difficult to measure precisely without first separating the phases. Since flow from multiple wells are typically combined before reaching the processing facility where this is done, information about flow from individual wells is often unavailable. The traditional method for performing multi-phase flow rate measurements in petroleum wells is illustrated in Figure <ref type="figure" target="#fig_4">7</ref>, and consists of routing flow to a test separator where gravity separates the phases into three single-phase flows, each of which can be reliably measured <ref type="bibr" target="#b39">[40]</ref>. This method, which is called well testing, requires a re-routing intervention which tends to decrease production. Well tests are consequently conducted quite rarely, typically every 7-60 days. Noninvasive physical measurement devices known as Multi-Phase Flow Meters (MPFM) do exist, and can be installed on individual wells. However, these are typically costly, while also being less precise, requiring regular maintenance, and suffering from sensor drift over time <ref type="bibr" target="#b40">[41]</ref>.</p><p>In summary, well flow measurements are often scarce. Measurements like pressure and temperature, however, are generally abundant. This motivates the application of a soft sensor. In the context of well flow estimation, soft sensors are called Virtual Flow Meters (VFM). The VFM literature has mainly been concerned with discriminative models, which have been shown to work well in many settings <ref type="bibr" target="#b41">[42]</ref>. However, most of the available data from wells are typically unsupervised due to the low frequency of well tests. As a consequence, current data-driven VFM approaches leave large amounts of the available data unused.</p><p>Thus, the VFM problem aligns well with the motivation for semi-supervised learning presented in Section 1. In the remainder of this section we investigate whether our proposed SSMTL methodology can leverage these unlabeled data, improving data-driven VFM performance. We do this both on a simple, simulated flow estimation problem, and on a large dataset containing production data from real petroleum wells.</p><p>In settings with real multi-unit well data, models based on MTL have repeat- edly been shown to outperform competing methods, both purely data-driven and physics-informed VFMs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21]</ref>. Based on this, we choose to benchmark our proposed method against a discriminative MTL method like the one which is illustrated in Figure <ref type="figure" target="#fig_3">6</ref>. To investigate the significance of the multi-unit nature of the data, we also consider a simple strategy for single-task learning (STL), where an independent model is trained for each unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Datasets</head><p>Synthetic data Any semi-supervised learning algorithm can fail to yield performance improvements over supervised methods if the learning problem at hand does not lend itself to semi-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Thus, it is useful to evaluate the proposed SSMTL method on a learning problem where one can be confident that unlabeled data can be of use. Motivated by this, a simulated system inspired by the VFM problem was considered. Just as the petroleum production system which is illustrated in Figure <ref type="figure" target="#fig_4">7</ref>, this system consists of a flow from a pressurized reservoir, and the soft sensing objective is to predict the flow rate Q given other available measurements. In this case the available measurements are a single pressure measurement p and a percent-wise choke valve opening u, meaning the regressor and target are defined as follows:</p><formula xml:id="formula_36">x ij = [u ij , p ij ] (<label>21</label></formula><formula xml:id="formula_37">)</formula><formula xml:id="formula_38">y ij = Q ij (<label>22</label></formula><formula xml:id="formula_39">)</formula><p>In contrast to the real petroleum production system, the simulated flow only consists of a single fluid, and it is modelled by simple equations for flow with friction through a vertical pipe ending in a choke valve. By generating data from simulated units with different physical parameters, the model can be learned across units by leveraging MTL methodology. This idealized setting, where data for each unit are i.i.d. and have no observation noise, is suited to inform us whether the proposed SSMTL can work, given that the problem admits semisupervised learning. An example of synthetic data is shown in Figure <ref type="figure" target="#fig_5">8</ref>. From this figure one can see that the data from the different units are qualitatively similar, but quantitatively different. This motivates the use of a hierarchical model for grouped data or a multi-task learning method.</p><p>Real data To evaluate the real-life utility of the proposed modelling approach, a dataset consisting of historical production data from real petroleum wells was considered. For this dataset, the regressor and target are defined as</p><formula xml:id="formula_40">x ij = [u ij , p UC ij , p DC ij , T UC ij , η OIL ij , η GAS ij , Q GL ij ]<label>(23)</label></formula><formula xml:id="formula_41">y ij = Q TOT ij (<label>24</label></formula><formula xml:id="formula_42">)</formula><p>were u is the choke valve opening, p UC and p DC are pressures upstream and downstream the choke valve, T UC is temperature upstream the choke valve, η OIL and η GAS are volumetric fractions of oil and gas, Q GL is volumetric gas lift rate, and Q TOT is total volumetric flow rate. All of these measurements are illustrated in Figure <ref type="figure" target="#fig_4">7</ref>. For a more in-depth discussion of this system and the VFM problem in general, see <ref type="bibr" target="#b41">[42]</ref>.</p><p>In the dataset, each data point is calculated as an average over a period of steady-state operation, detected using the technology described in <ref type="bibr" target="#b43">[44]</ref>. Thus, we only model steady-state flow, as is common in the VFM literature <ref type="bibr" target="#b41">[42]</ref>. As illustrated in Figure <ref type="figure" target="#fig_4">7</ref>, η OIL and η GAS arise from from the three singlephase flow measurements which together make up the multi-phase flow measurement. Thus, these will be subject to the same data limitations as the label y = Q TOT . To account for this, we assume that fractions change slowly over time, and approximate them using their value from the previous available flow measurement.</p><p>Accurate evaluation of model generalization requires access to sufficient amounts of labeled test data. This is hard to achieve in the label-scarce setting described in Section 6.1. To gain trustworthy estimates of model generalization error, we therefore make use of data from wells where both well tests and MPFM measurements are available. For these wells, labeled data are available at the same rate as unlabeled data, and we can generate sizeable test sets. To reproduce the data scarcity which motivates this work (and which is very real on poorlier instrumented wells), we then remove the labels from the majority of the training data. To mimic the setting in which a VFM would be used in practice, the split into training and test set is done chronologically. The result is a dataset where test data and unlabeled training data are abundant, while labeled training data are scarce. Figure <ref type="figure" target="#fig_6">9</ref> and Figure <ref type="figure">10</ref> illustrate the availability of the different types of data for the datasets which are used in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments</head><p>For both datasets, we investigate model performance in two data availability regimes: 1) a multi-unit learning setting where partly labeled data from many units are available, and 2) a single-unit finetuning setting where the same multiunit data are available, but where the objective is to model a previously unseen unit with little data. In both of these settings, our goal is to answer the following question: Can our proposed SSMTL model use unlabeled data to improve soft sensor accuracy? Figure <ref type="figure">10</ref>: Data used for the finetuning experiment. Across all units, a total of 56 labeled and 589 unlabeled and 283 data points are available. The labeled data points are equally distributed among the units, while the unlabeled and test data are not. The ratio N u /N l of unlabeled to labeled data varies between units, and across time intervals; on average, it is about 10. A rectangular marker indicates the availability of a data point. The x-axis is shifted for each unit such that t = 0 coincides with the start of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-unit learning</head><p>The first case study addresses the abilities of the models to learn from partly labeled multi-unit data. Here, models based on singletask learning (STL), multi-task learning (MTL) and semi-supervised multi-task learning (SSMTL) are considered.</p><p>For this experiment, synthetic data were generated for 50 different units, each unit having different physical constants like pipe height, pipe diameter and friction coefficient. For each of the units, a training dataset with 5 labeled and 100 unlabeled data points was generated, in addition to a test set with 100 data points for each unit. The real dataset consisted of historical data from 64 wells, where 20 labeled and 400 unlabeled and 20 test data points were available for each well. For both datasets the effect of increasing the amount of unlabeled data was investigated by considering three different unlabeled datasets, where the ratio N u /N l of unlabeled to labeled data was 1, 5 and 20, respectively. The real dataset for N u /N l = 20 is illustrated for seven of the 64 real wells in Figure <ref type="figure" target="#fig_6">9</ref>.</p><p>The SSMTL method was trained using the SGVB estimator of Algorithm 1, while the MTL and STL models were trained using stochastic gradient descent on the log-likelihood objective. Early stopping was used to decide which iteration of the training to use for model evaluation. To reduce variance, results for the MTL and SSMTL models were generated by repeating the experiment 20 times with different random initialization of the neural networks, and averaging results for each unit over experiment repetitions. For further implementation details, see Appendix A.</p><p>Table <ref type="table" target="#tab_0">1</ref> and 2 show the mean absolute percentage error (MAPE) on the test set for the synthetic and real dataset, respectively. For both synthetic and real data, a significant decrease in error is achieved when going from STL to MTL. The performance improvement when going from MTL to SSMTL is comparatively lower, but still evident in both cases. The improvement is consistent across the synthetic and real data, and across different amounts of unlabeled data. For the synthetic data, model performance also increases as the amount of unlabeled data increases. For the real data, this does not seem to be the case.</p><p>The performance improvement when going from zero to some unlabeled data, is however evident on both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-unit calibration</head><p>The second case study considers a setting where a previously unseen unit is to be modelled. For this unit, data points are appearing incrementally, and can be used to update the model which was learned in the previous case study in an online manner. If the base training set consists of data from sufficiently many units, one can motivate a simplified learning procedure wherein the neural network parameters are frozen, and only the context parameters are learned. This method was used here. For further motivation of this methodology, see <ref type="bibr" target="#b20">[21]</ref>.</p><p>For the synthetic dataset, data from 20 new units were generated. For each unit, N l labeled and N u = 10N l unlabeled observations were available for N l = 1, 2, . . . , 10 labeled observations, in addition to 100 test data points for each unit. For the real dataset, data from seven units which were not part of the multi-unit learning experiment were used. To mimic a setting where new data are incrementally observed, a test set with a duration of eight weeks was fixed for each unit, while labeled data points were chosen to appear every other Number of labeled data points  For the synthetic data case, the ratio of unlabeled to labeled data is N u /N l = 10. For the real data case, the ratio varies between wells, but is also, on average, close to 10.</p><p>week, incrementally and backwards in time, for N l = 1, 2, . . . 8. The number of unlabeled data points was only limited by the original dataset. For the largest number of weeks considered (16 weeks), a total of 56 labeled, 589 unlabeled and 283 test data points were available, meaning the average unlabeled to labeled ratio N u /N l was approximately 10. Data availability in this case is illustrated in Figure <ref type="figure">10</ref>.</p><p>For each new labeled datapoint that appeared (as well as its associated ≈ 10 unlabeled data points), a pre-trained SSMTL model was finetuned to the available data. This was done both in a supervised (only labeled data), an unsupervised (only unlabeled data) and a semi-supervised (both labeled and unlabeled data) fashion. In each case, learning was done by taking a fully trained model from the multi-unit learning experiment, freezing its neural network parameters, and learning the parameters of a freshly initialized context module from the data available for the given unit. For further details, see Appendix A.</p><p>Figure <ref type="figure" target="#fig_9">11</ref> shows the resulting model performance. In both the synthetic and real data case, the model is able to learn from unsupervised data; test set error decreases as purely unlabeled data become available (red line), and when labeled data are available, the SSMTL model which has access to unlabeled data (blue line) consistently outperforms the one which does not (green line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>The results presented in Section 6 suggest that the proposed SSMTL model is able to learn from unlabeled data, improving performance in the soft sensing problem of data-driven VFM. This conclusion holds across datasets and case studies: unlabeled data improve performance for classical multi-unit modelling and single-unit finetuning, both for synthetic and real data.</p><p>In terms of pure numbers, the performance improvement gained from semisupervised learning in the case study considered here is arguably quite modest. However, we emphasize that the question of interest to us is whether unlabeled data can be helpful at all; the degree of performance improvement which is attainable is limited by the learning problem and the data at hand <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Considering the fact that VAEs have proven capable of modelling highdimensional data <ref type="bibr" target="#b25">[26]</ref>, we expect that a further performance improvement from semi-supervised learning can be gained in settings with more complex, highdimensional regressors, for instance settings with additional sensors, sensors that provide higher-dimensional features, or for modelling sequences of observations. All of these model extensions provide interesting avenues for further research.</p><p>In light of this, it is remarkable that the SSMTL method is able to achieve a measurable performance increase at all in the setting considered here, where the regressor x has dimension 7, each feature being selected to be as informative as possible regarding y. We also emphasize the fact that in the single-unit calibration case study, a performance improvement was seen even in purely unsupervised settings, where no labels were available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Concluding remarks</head><p>We have proposed a deep probabilistic model which performs semi-supervised learning from multi-unit data, and demonstrated on real data from a significant industrial problem that the model can use unlabeled data to improve soft sensor performance. The proposed model provides a general approach to multi-unit soft sensing problems, and by combining the advantages of multi-task and semisupervised learning, the method is able to learn from very few data points. This makes it is especially powerful in information-poor environments, where data efficiency is essential. MTL models. The MTL model was implemented using a feed-forward neural network. The likelihood of an observation (x ij , y ij ) from unit i was modeled as</p><formula xml:id="formula_43">y ij ∼ N (f θ (x ij , c i ), σ 2 )</formula><p>, where f θ is a neural network with parameters θ. The context variables were modeled by a standard normal prior and a variational distribution c i ∼ N (m i , diag(s 2 i )) for i = 1, . . . , M . The parameters θ, σ, {m i } i , and {s i } i were learned from data. Note that the θ and σ parameters were shared by all units. In the synthetic data case, f θ had two hidden layers, each with 200 nodes. In the real data case, f θ had two hidden layers, each with 400 nodes. The dimension of the context variable was K = 4 in both cases.</p><p>To regularize the MTL model, L 2 regularization was also implemented. However, initial experiments indicated that the highest performance of the MTL model was achieved without explicit neural network parameter regularization (we hypothesize that this is due to early stopping). Consequently, L 2 regularization was not used in the case studies presented here. SSMTL models. The proposed model for semi-supervised multi-task learning (SSMTL) was implemented using three feed-forward neural networks: one for the likelihood, p θ (x, y | z, c), and two for the inference models, q ϕz (z | x, y, c) and q ϕy (y | x, c). For the synthetic data case, all four networks had two hidden layers, each with 200 nodes. For the real data case, all four networks had two hidden layers, each with 400 nodes. Variable dimensions used in the two cases are shown in Table <ref type="table" target="#tab_2">3</ref>. Notice that we used an overcomplete VAE with D &gt; D x + D y , where regularization is applied via the prior on the latent variables.</p><p>On the synthetic data case, we set α = 1. On the real data case, we found that the performance was improved by setting α = 10, putting more weight on log-likelihood of the inference model on labeled data.</p><p>Model architecture and optimization. All neural networks were implemented in PyTorch <ref type="bibr" target="#b45">[46]</ref>. We used ReLU activation functions and initialized the parameters using He initialization <ref type="bibr" target="#b46">[47]</ref>.</p><p>For the multi-unit learning case study, we used the Adam optimizer <ref type="bibr" target="#b47">[48]</ref> to train the SSMTL and MTL models. We used a five-sample SGVB estimate for the SSMTL model due to the additional variance introduced by the z variable. The learning rate was manually tuned to 5×10 -4 . The other hyperparameters of Adam were kept at their standard values. We used early stopping on randomly selected validation data to prevent overfitting.</p><p>For the single-unit finetuning experiment, plain stochastic gradient descent (SGD) was used. Based on manual tuning, 100 epochs of SGD with a learning rate of 10 -4 were performed for each unit, for all models. For every new data point that became available, training was done from scratch. For the synthetic data, context parameter means were initialized at their prior mean, i.e. at zero. For real data, the context parameter mean was set to the average of context parameter means on the same field. For both datasets, the variance of q φ (c i ) was fixed at 0.1.</p><p>All models were trained on a desktop computer with a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Generation of synthetic data</head><p>We base the generative process of our synthetic data on the on the following equations which, respectively, model single-phase fluid flow through a choke valve and pressure drop in a vertical pipe with friction:</p><formula xml:id="formula_44">Q = Cu 2(p -p o ) ρ<label>(25)</label></formula><formula xml:id="formula_45">p = p r -ρgh -f D 8ρh π 2 D 5 Q 2<label>(26)</label></formula><p>Here, Q is the single-phase flow rate, u ∈ [0, 1] is the choke valve position, p is the pressure upstream the choke valve, C is the choke CV coefficient, ρ is the fluid density, p o is the outlet pressure, p r is the bottomhole pressure, g is the gravitational acceleration, h is the height of the pipe, f D is the Darcy friction coefficient, an D is the diameter of the pipe. Q, u and p are measured variables, while all other values entering the equations are parameters.</p><p>The parameter space can be simplified by combining all parameters which enter together. We define constants</p><formula xml:id="formula_46">c 1 = 2C 2 ρ p o , c 2 = 2C 2 ρ , c 3 = p r -ρgh, c 4 = f D 8ρh π 2 D 5 .</formula><p>The equations can then be written as</p><formula xml:id="formula_47">Q = -c 1 u 2 + c 2 u 2 p (<label>27</label></formula><formula xml:id="formula_48">)</formula><formula xml:id="formula_49">p = c 3 -c 4 Q 2 (<label>28</label></formula><formula xml:id="formula_50">)</formula><p>Gaussian noise is added to measurements of p and Q. Pseudocode for the data generating process is given below. u ij ∼ U(0, 100) 5:</p><formula xml:id="formula_51">µ p ← c3+c1c4u 2 ij 1+c2c4u 2 ij 6:</formula><p>p ij ∼ N (µ p , σ 2 p )</p><p>7:</p><formula xml:id="formula_52">µ Q ← -c 1 u 2 ij + c 2 u 2 ij µ p 8: Q ij ∼ N (µ Q , σ 2 Q ) 9:</formula><p>end for 10: end for 11: return u, p, Q</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A generative model for multi-unit soft sensing. Random variables are encircled. A grey (white) circle indicates that the variable is observed (latent). The nested plates (rectangles) group variables at different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A graphical representation of the inference model. Arrows indicate which parameters (ϕ y , ϕ z , ϕ c ) and which variables (encircled) that are used in the inference of the latent variables (y u , z, c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Related generative models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Discriminative MTL model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Petroleum production system subject to well testing. The multiphase flow of produced liquid is either routed directly to a processing facility without being measured, or to a test separator which splits the flow into three single-phase flows which are easier to measure. The observations of pressure, temperature and choke opening are continuously available, while measurements of flow rate and phase fractions are scarce due to the cost of well testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Synthetic data from five different units. Data points from the same unit have the same color.</figDesc><graphic coords="18,133.77,124.80,343.71,100.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Data used for the multi-unit semi-supervised learning experiment. This figure shows data from seven of the 64 wells, for the ratio N u /N l = 20 of unlabeled to labeled data. A total of 1280 labeled, 25600 unlabeled and 1280 test data points are available, distributed equally among the units. A rectangular marker indicates the availability of a data point. The x-axis is shifted for each unit such that t = 0 coincides with the start of the test set.</figDesc><graphic coords="19,133.77,124.80,343.71,71.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Finetuned model performance on real and synthetic data. For the synthetic data case, the ratio of unlabeled to labeled data is N u /N l = 10. For the real data case, the ratio varies between wells, but is also, on average, close to 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 2 1 : 3 :</head><label>213</label><figDesc>Synthetic data generation Require: Parameter distribution p(c 1 , c 2 , c 3 , c 4 ). Draw parameters c 1 , c 2 , c 3 , c 4 2: for i = 1, . . . , M do for j = 1, . . . , N i do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Prediction error on synthetic test data. The average, the 10th, the 50th and the 90th percentile of the MAPE is reported. The lowest value of each error statistic is set in boldface.</figDesc><table><row><cell>Model</cell><cell>Unlabeled</cell><cell>Mean</cell><cell>P 10</cell><cell>P 50</cell><cell>P 90</cell></row><row><cell></cell><cell>ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STL</cell><cell>-</cell><cell>22.302</cell><cell>15.638</cell><cell>22.356</cell><cell>27.894</cell></row><row><cell>MTL</cell><cell>-</cell><cell>3.671</cell><cell>2.676</cell><cell>3.374</cell><cell>5.076</cell></row><row><cell>SSMTL</cell><cell>1</cell><cell>3.534</cell><cell>2.720</cell><cell>3.363</cell><cell>4.380</cell></row><row><cell>SSMTL</cell><cell>5</cell><cell>3.375</cell><cell>2.602</cell><cell>3.208</cell><cell>4.177</cell></row><row><cell>SSMTL</cell><cell>20</cell><cell>3.310</cell><cell>2.434</cell><cell>3.178</cell><cell>4.311</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Prediction error on real test data. The average, the 10th, the 50th and the 90th percentile of the MAPE is reported. The lowest value of each error statistic is set in boldface.</figDesc><table><row><cell>Model</cell><cell>Unlabeled</cell><cell>Mean</cell><cell>P 10</cell><cell>P 50</cell><cell>P 90</cell></row><row><cell></cell><cell>ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STL</cell><cell>-</cell><cell>14.726</cell><cell>4.416</cell><cell>11.620</cell><cell>27.059</cell></row><row><cell>MTL</cell><cell>-</cell><cell>11.737</cell><cell>3.114</cell><cell>7.219</cell><cell>25.729</cell></row><row><cell>SSMTL</cell><cell>1</cell><cell>10.815</cell><cell>3.068</cell><cell>6.729</cell><cell>24.917</cell></row><row><cell>SSMTL</cell><cell>5</cell><cell>10.774</cell><cell>2.887</cell><cell>6.887</cell><cell>24.908</cell></row><row><cell>SSMTL</cell><cell>20</cell><cell>10.679</cell><cell>3.433</cell><cell>7.182</cell><cell>24.741</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dimensions of variables in the SSMTL model for the synthetic and real data case.</figDesc><table><row><cell>Case</cell><cell>K</cell><cell>D</cell><cell>D x</cell><cell>D y</cell></row><row><cell>Synthetic data</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>1</cell></row><row><cell>Real data</cell><cell>4</cell><cell>10</cell><cell>7</cell><cell>1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">Solution Seeker Inc.</rs></p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Implementation details STL models. Single-task learning was performed by training one model per unit in the datasets. The models were trained to minimize the mean squared error on labeled data only. For these tasks, we used support vector regression with a radial basis function kernel <ref type="bibr" target="#b44">[45]</ref>. To avoid overfitting, the L 2 -regularization factor was found by performing a hyperparameter optimization on randomly selected validation data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The role of artificial intelligence-driven soft sensors in advanced sustainable process industries: A critical review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yasith</surname></persName>
		</author>
		<author>
			<persName><surname>Perera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.engappai.2023.105988</idno>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">105988</biblScope>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Deep Supervised Learning Framework for Data-Driven Soft Sensor Modeling of Industrial Processes</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2957366</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4737" to="4746" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning for Data-Driven Soft Sensors</title>
		<author>
			<persName><forename type="first">Qingqiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2021.3053128</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9329169/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<idno type="ISSN">1551-3203</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5853" to="5866" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven Soft Sensors in the process industry</title>
		<author>
			<persName><forename type="first">Petr</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibylle</forename><surname>Strandt</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compchemeng.2008.12.012</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="795" to="814" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Review on Soft Sensors for Monitoring, Control, and Optimization of Industrial Processes</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSEN.2020.3033153</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="12868" to="12881" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9780262033589.001.0001</idno>
		<editor>Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien</editor>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Taxonomy for Semi-Supervised Learning Methods</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="15" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012 2 (2012</date>
			<biblScope unit="page">6471</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causal modeling &amp; dynamical systems: A new perspective on feedback</title>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Bongers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jesper</surname></persName>
		</author>
		<author>
			<persName><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><surname>Hoos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-019-05855-6</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised Variational Autoencoders for Soft Sensor Modeling With Missing Data</title>
		<author>
			<persName><forename type="first">Ruimin</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2019.2951622</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<idno type="ISSN">1551-3203</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2820" to="2828" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble deep relevant learning framework for semi-supervised soft sensor modeling of industrial processes</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moreira</forename><forename type="middle">De</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Meneghetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ugulino</forename><forename type="middle">De</forename><surname>Araujo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.07.086</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="page" from="154" to="168" />
			<date type="published" when="2021-10">Oct. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evolutionary optimization based pseudo labeling for semi-supervised soft sensor development of industrial processes</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huaiping</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ces.2021.116560</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">116560</biblScope>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised ensemble support vector regression based soft sensor for key quality variable estimation of nonlinear industrial processes with limited labeled data</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cherd.2022.01.026</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Research and Design</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2022-03">Mar. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear probabilistic latent variable regression models for soft sensor application: From shallow to deep structure</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conengprac.2019.104198</idno>
	</analytic>
	<monogr>
		<title level="j">Control Engineering Practice</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">104198</biblScope>
			<date type="published" when="2019-03">March 2019 (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Learning for Industrial KPI Prediction: When Ensemble Learning Meets Semi-Supervised Data</title>
		<author>
			<persName><forename type="first">Qingqiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2020.2969709</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<idno type="ISSN">1551-3203</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Deep Dynamic Probabilistic Latent Variable Model for Multimode Process Soft Sensor Application</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2022.3183211</idno>
		<ptr target="https://ieeexplore.ieee.org/document/9797056/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<idno type="ISSN">1551-3203</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6056" to="6068" />
			<date type="published" when="2023-04">Apr. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for data-driven soft-sensing of biological and chemical processes</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Esche</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ces.2022.117459</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Science</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="page">117459</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Survey on Multi-Task Learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3070203</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">4347</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Soft sensor transferability: A survey</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Curreri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Patanè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gabriella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibilia</forename></persName>
		</author>
		<idno type="DOI">10.3390/app11167710</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-unit soft sensing permits few-shot learning in virtual flow metering</title>
		<author>
			<persName><forename type="first">Bjarne</forename><surname>Grimstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Løvland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">S</forename><surname>Imsland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15828</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation transfer learning soft sensor for product quality prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chemolab.2019.103813</idno>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and Intelligent Laboratory Systems</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page">103813</biblScope>
			<date type="published" when="2019-04">April (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning for virtual flow metering</title>
		<author>
			<persName><forename type="first">Anders</forename><forename type="middle">T</forename><surname>Sandnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjarne</forename><surname>Grimstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odd</forename><surname>Kolbjørnsen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107458</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page">107458</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Multitask Learning Model for the Prediction of NOx Emissions in Municipal Solid Waste Incineration Processes</title>
		<author>
			<persName><forename type="first">Junfei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIM.2022.3225056</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<idno type="ISSN">0018-9456</idno>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1557" to="9662" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling Task Relationships in Multivariate Soft Sensor With Balanced Mixture-of-Experts</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TII.2022.3202909</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<idno type="ISSN">1551-3203</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6556" to="6564" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000056</idno>
		<idno type="arXiv">arXiv:1906.02691</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="307" to="392" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014-01">January (2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Survey on Deep Semi-supervised Learning</title>
		<author>
			<persName><forename type="first">Xiangli</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00550</idno>
		<ptr target="http://arxiv.org/abs/2103.00550" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-Supervised Multitask Learning</title>
		<author>
			<persName><forename type="first">Qiuhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-Supervised Multi-Task Regression</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04174-7_40</idno>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="617" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation Learning via Semi-Supervised Autoencoder for Multi-task Learning</title>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Deep Probabilistic Transfer Learning Framework for Soft Sensor Modeling With Missing Data</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2021.3085869</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<idno type="ISSN">2162-237</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2162" to="2388" />
			<date type="published" when="2022-12">Dec. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning disentangled representations with semisupervised deep generative models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining deep generative and discriminative models for Bayesian semi-supervised learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.107156</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">107156</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11867</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2095" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards a Neural Statistician</title>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno>eprint: 1606.02185</idno>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Handbook of Multiphase Flow Metering. The Norwegian Society for Oil and Gas Measurement</title>
		<author>
			<persName><surname>Sidsel Corneliussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiphase flow metering: current trends and future developments</title>
		<author>
			<persName><forename type="first">Gioia</forename><surname>Falcone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPE Annual Technical Conference and Exhibition? SPE</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">71474</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">First principles and machine learning Virtual Flow Metering: A literature review</title>
		<author>
			<persName><forename type="first">Timur</forename><surname>Bikmukhametov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Jäschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Petroleum Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">106487</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Passive learning to address nonstationarity in virtual flow metering applications</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Hotvedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjarne</forename><forename type="middle">A</forename><surname>Grimstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">S</forename><surname>Imsland</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2022.118382</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems With Applications</title>
		<idno type="ISSN">0957-4174</idno>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page">118382</biblScope>
			<date type="published" when="2022-02">February (2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A simple data-driven approach to production estimation and optimization</title>
		<author>
			<persName><surname>Grimstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPE intelligent energy international conference and exhibition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">181104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:STCO.0000035301.49549.88</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbrc.2018.01.076</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
