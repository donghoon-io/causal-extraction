<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2014-05-12">12 May 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-05-12">12 May 2014</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1309.5124v2[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer graph analysis for dynamic social networks</head><p>Brandon Oselio, Student Member, IEEE, Alex Kulesza, Alfred O. Hero, III, Fellow, IEEE Abstract-Modern social networks frequently encompass multiple distinct types of connectivity information; for instance, explicitly acknowledged friend relationships might complement behavioral measures that link users according to their actions or interests. One way to represent these networks is as multi-layer graphs, where each layer contains a unique set of edges over the same underlying vertices (users). Edges in different layers typically have related but distinct semantics; depending on the application multiple layers might be used to reduce noise through averaging, to perform multifaceted analyses, or a combination of the two. However, it is not obvious how to extend standard graph analysis techniques to the multi-layer setting in a flexible way. In this paper we develop latent variable models and methods for mining multi-layer networks for connectivity patterns based on noisy data. Index Terms-Hypergraphs, multigraphs, mixture graphical models, Pareto optimality Multi-layer networks arise naturally when there exists more than one source of connectivity information for a group of users. For instance, in a social networking context there is often knowledge of direct communication links, i.e., relational information. Examples of relational information include the frequency with which users communicate over social media, or whether a user has sent or received emails from another user in a given time period. However, it is also possible to derive behavioral relationships based on user actions or interests. These behavioral relationships are inferred from information that does not directly connect users, such as individual preferences or usage statistics. In this paper we show how to deal with multiple layers of a social network when performing tasks like inference, clustering, and anomaly detection.</p><p>We propose a generative hierarchical latent-variable model for multi-layer networks, and show how to perform inference on its parameters. Using techniques from Bayesian Model Averaging <ref type="bibr" target="#b0">[1]</ref>, the layers of the network are conditionally decoupled using a latent selection variable; this makes it possible to write the posterior probability of the latent variables given the multi-layer network. The resulting mixture can be viewed as a scalarization of a multi-objective optimization problem <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. When the posterior probability functions are convex, the scalarization is both optimal and consistent with the Bayesian principle of model-averaged inference <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>We then step back from the Bayesian setting and discuss</p><p>The authors are with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA. Tel: 1-734-763-0564. Fax: 1-734-763-8041. Emails: {boselio, kulesza, hero}@umich.edu.</p><p>This work was partially supported by ARO grant number W911NF-12-1-0443. Parts of this paper were presented in the Proceedings of the IEEE Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), St. Martin, Dec. 2013.</p><formula xml:id="formula_0">Adjacency matrices A 1 W 2 W 1 A 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observed matrices</head><p>Fig. <ref type="figure" target="#fig_5">1</ref>. Adjacency and Observation Matrices. This graphical model depicts how the latent adjacency matrices can affect the observervation matrices. Note that the observation matrices are dependent on all adjacency matrices in general.</p><p>how multi-objective optimization can be used to perform MAP estimation of the desired latent variables. Using the concept of Pareto optimality <ref type="bibr" target="#b3">[4]</ref>, an entire front of solutions is defined; this allows a user to define a preference over optimization functions and tune the algorithm accordingly. The result is a level of supervised optimization and inference that utilizes the structure of multi-layer networks without scalarization. Experiments on a simulated example show that our method yields improved clustering performance in noisy conditions. The developed framework is then combined with the dynamic stochastic block model (DSBM) <ref type="bibr" target="#b5">[6]</ref>, which captures a variety of complex temporal network phenomena. Finally, the multilayer DSBM is applied to a real-world data set drawn from the ENRON email corpus. This example illustrates how we can combine two layers of a network to explore complex connections through both time and layer mixing parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. MULTI-LAYER NETWORKS</head><p>A multi-layer graph G = (V, E) comprises vertices V = {v 1 , . . . , v p }, common to all layers, and edges E = (E 1 , . . . , E L ) on L layers, where E i is the edge set for layer i.</p><p>In the real-world network setting, we will assume that the observed data are noisy reflections of a true underlying multilayer graph. For convenience we will work with adjacency representations, letting A i ∈ R p×p be the true adjacency matrix of layer i, and W i ∈ R p×p the corresponding observed adjacency matrix. Figure <ref type="figure" target="#fig_5">1</ref> depicts the model graphically.</p><p>In some cases W i might be binary, reflecting merely the presence or absence of a connection-for instance, whether two users were seen to communicate. In other settings, such as measuring temporal or content correlation scores between users, the entries of W i could be real-valued. The goal is to estimate A 1 , . . . , A L given the observations W 1 , . . . , W L . Using standard parametric methods this will require computing the posterior distribution of A 1 , . . . , A L , which can be difficult given the number of parameters. Specifically, the influence of A 1 , . . . , A L on a single W i is difficult to measure, as the dependencies are unspecified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HIERARCHICAL MODEL DESCRIPTION</head><p>A hierarchical model is proposed that simplifies this inference procedure by conditionally decoupling W 1 , . . . , W L . For simplicity, we specialize to the case where L = 2. This also allows us to view the networks in the setting described in the introduction: one layer of the network represents the observed extrinsic relationships between users, and the other layer represents their correlated intrinsic behaviors.</p><p>We introduce a latent variable denoted Y (see Figure <ref type="figure" target="#fig_0">2</ref>) that conditionally decouples the posterior distributions of the two layers:</p><formula xml:id="formula_1">P (W 1 , W 2 |A 1 , A 2 , Y ) = P (W 1 |A 1 , Y )P (W 2 |A 2 , Y ) (1) P (W 1 , W 2 |A 1 , A 2 ) = P (W 1 , W 2 |A 1 , A 2 , Y )P (Y |A 1 , A 2 )dY .<label>(2)</label></formula><p>Shifting the focus from the adjacency matrices A 1 , A 2 , to the latent variable Y , using Y as a compact description of how these adjacencies combine to form the multi-layer network structure. It is possible to write down the posterior distribution for Y as</p><formula xml:id="formula_2">P (Y |W 1 , W 2 ) = A1,A2 P (Y |A 1 , A 2 )P (A 1 , A 2 |W 1 , W 2 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. POSTERIOR MIXTURE MODELING</head><p>Consider the graphical model shown in Figure <ref type="figure">3</ref>. We have collapsed the A 1 , A 2 variables with the observed data W 1 , W 2 , because we are mainly interested in inferring W , and W i can be considered a representation of the real connectivity.</p><p>Following the previous model, we have decomposed Y = (W, Z), where W ∈ R p×p is a latent adjacency or similarity matrix describing the underlying connections between vertices, and Z ∈ {1, 2} is a model selection variable, P (Z = 1) = α, and P (Z = 2) = 1 -α. Here there is the implicit assumption a common connectivity structure W informs all layers of the network. In a sense, the model produces observed matrices that correspond to multiple views of the latent variable W . The model selection variable Z will decouple the posterior distribution of W given both layers into a weighted sum of marginalized posteriors given each individual layer.</p><p>The prior for W is P (W ), left unspecified for now. The distributions P (W 1 |W, Z) and P (W 2 |W, Z) are in general taskdependent (e.g., they could be Gaussian, Wishart, Bernoulli, etc.), but we will make the simplifying assumption that Z acts as a selector variable, so that W and W 1 are conditionally independent given Z = 2, and likewise W and W 2 are conditionally independent when Z = 1. Formally, using the notation P z to denote conditioning on Z = z, we have</p><formula xml:id="formula_3">P 2 (W 1 |W ) = P 2 (W 1 ) (3) P 1 (W 2 |W ) = P 1 (W 2 ) .<label>(4)</label></formula><p>We are interested in the posterior distribution of the latent variable W given the observed variables W 1 , W 2 :</p><formula xml:id="formula_4">P (W |W 1 , W 2 ) (5) = P (W, Z = 1|W 1 , W 2 ) + P (W, Z = 2|W 1 , W 2 ) (6) = P (W |W 1 , W 2 , Z = 1)P (Z = 1|W 1 , W 2 ) + P (W |W 1 , W 2 , Z = 2)P (Z = 2|W 1 , W 2 ) (7) = ξP (W |W 1 , W 2 , Z = 1) + (1 -ξ)P (W |W 1 , W 2 , Z = 2) ,<label>(8)</label></formula><p>where ξ = P (Z = 1|W 1 , W 2 ). Let's consider the first term. We have</p><formula xml:id="formula_5">P (W |W 1 , W 2 , Z = 1) = P (W, W 1 , W 2 , Z = 1) Ŵ P ( Ŵ , W 1 , W 2 , Z = 1)<label>(9)</label></formula><formula xml:id="formula_6">= P (W )P 1 (W 1 |W )P 1 (W 2 ) Ŵ P ( Ŵ )P 1 (W 1 | Ŵ )P 1 (W 2 ) .<label>(10)</label></formula><p>Since P 1 (W 2 ) does not depend on W , it factors out of the sum in the denominator and cancels; thus <ref type="bibr" target="#b9">(10)</ref> becomes</p><formula xml:id="formula_7">P (W |W 1 , W 2 , Z = 1) = P (W )P 1 (W 1 |W ) P 1 (W 1 ) . (<label>11</label></formula><formula xml:id="formula_8">)</formula><p>Performing the same computation on the other side and combining, we have</p><formula xml:id="formula_9">P (W |W 1 , W 2 )<label>(12)</label></formula><formula xml:id="formula_10">= ξ P (W )P 1 (W 1 |W ) P 1 (W 1 ) + (1 -ξ) P (W )P 2 (W 2 |W ) P 2 (W 2 ) (13) = P (W ) [γ 1 P 1 (W 1 |W ) + γ 2 P 2 (W 2 |W )] ,<label>(14)</label></formula><p>where γ 1 = ξ/P 1 (W 1 ) and γ 2 = (1-ξ)/P 2 (W 2 ) are constants with respect to W . If we assume the prior on W is uniform, then the MAP estimate of W is also the maximum likelihood estimate, which can be written as</p><formula xml:id="formula_11">argmax W [γ 1 P 1 (W 1 |W ) + γ 2 P 2 (W 2 |W )] .<label>(15)</label></formula><p>The above solutions describe not just one MAP estimate of W , but rather a family of MAP estimates, based on the priors that we implicitly assign to each model by choosing a specific value of α (which affects ξ and γ in turn). Qualitatively, this can be viewed as determining a relative confidence parameter between the networks; if W 1 is more trusted than W 2 , then α would be greater than 0.5.</p><p>As an example, assume that both P (W 1 |W ) and P (W 2 |W ) are isotropic Gaussians, i.e.,</p><formula xml:id="formula_12">P (W 1 |W ) = N (W, σ 2 1 I p ) (16) P (W 2 |W ) = N (W, σ 2 2 I p ) . (<label>17</label></formula><formula xml:id="formula_13">) Latent variables W 2 W 1 Z W Observed matrices</formula><p>Fig. <ref type="figure">3</ref>. Model with Similarity Matrix and Selection Variable. We introduce the similarity matrix W and the selection variable Z to describe our latent variable model. Conditioning on W and Z, we assume that the two layers are independent from each other.</p><p>Then the solution for Ŵ has the form</p><formula xml:id="formula_14">Ŵ = βW 1 + (1 -β)W 2 ,<label>(18)</label></formula><p>for some choice of 0 ≤ β ≤ 1.</p><p>A proof of this is given in Appendix A. In the non-isotropic, non-Gaussian case the solution will not have such a simple form. However, numerical methods can be used to compute the solution <ref type="bibr" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION EXAMPLE</head><p>We use simulations to show that clustering of nodes in a weighted graph can be improved using the MAP estimate of W . This simulation example uses the Bayesian posterior representation, where P (W 1 |W ) and P (W 2 |W ) are isotropic multivariate Gaussian distributions with (posterior) mean W . Two weighted random graphs with 500 nodes are constructed with 10 known clusters of equal size. The weights between nodes in the same cluster are independently generated from the normal distribution N (5, 0.5), and the edge weights between nodes that are not in the same cluster are independently generated from the normal distribution N (4.7, 0.5). The dichotomy between these edge weights is to simulate the underlying community structure with variability. The networks are then corrupted with i.i.d. Gaussian noise on each edge weight with zero mean and different variances. Specifically, the first network layer is corrupted with additive noise distributed as N (0, σ 1 ) and the second layer is corrupted with additive noise distributed as N (0, σ 2 ). This setup corresponds to the form of Ŵ that is derived in <ref type="bibr" target="#b17">(18)</ref>. For various choices of mixing parameters β, the combined network Ŵ is calculated and then clustered using a spectral clustering algorithm <ref type="bibr" target="#b6">[7]</ref>. The spectral clustering algorithm finds the eigenvectors of the graph Laplacian L = D-A, where D, A are the degree and adjacency matrix obtained from Ŵ . The Adjusted Rand Indices (ARI) <ref type="bibr" target="#b7">[8]</ref> are computed in comparison to the true clustering structure; this gives us a measure of the quality of the clustering. For each of several different levels of noise variance, this experiment is run 50 times, and the results are averaged. Figure <ref type="figure" target="#fig_1">4</ref> computes the solution <ref type="bibr" target="#b14">(15)</ref>, and shows that using <ref type="bibr" target="#b13">(14)</ref> to estimate the mixture of networks improves clustering when compared to using only one layer of the network, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PARETO SUMMARIZATIONS</head><p>Of course, in practice it may be difficult to effectively set the prior parameter α. In such cases we can generate a  family of MAP estimates and apply multiple-objective ranking techniques. In particular, one can view the maximization (15) of the combined posterior distributions as a particular scalarization of a multi-objective optimization problem. However, there are other solutions to multiple objective optimization that do not use linear scalarization, such as Pareto front analysis <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>Consider the multi-objective optimization problem</p><formula xml:id="formula_15">Ŵ = argmin W [f 1 (W ), f 2 (W )] ,<label>(19)</label></formula><p>where the minimization in <ref type="bibr" target="#b18">(19)</ref> is in the sense of multi-objective minimization, to be made clear below. For the model derived in Section III, we have <ref type="bibr" target="#b18">(19)</ref> being interpreted in terms of linear scalarization using weighting coefficients γ and 1 -γ:</p><formula xml:id="formula_16">f 1 (W ) = -P 1 (W |W 1 ) and f 2 (W ) = -P 2 (W |W 2 ), with</formula><formula xml:id="formula_17">Ŵ = argmin W [γf 1 (W ) + (1 -γ)f 2 (W )] .<label>(20)</label></formula><p>An alternative to the scalarization approach is a ranking approach that seeks to find a family of solutions W that would be highly ranked by any scalarization, linear or non-linear. This leads to the idea of Pareto optimization. A solution to a multi-objective optimization problem is said to be weakly Pareto optimal (or weakly non-dominated) if it is not possible to improve any single objective function without lowering some other objective function <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. More formally, we say that a solution W 1 dominates a solution W 2 if f i (W 1 ) ≤ f i (W 2 ) for every objective function f i and there exists some j such that f j (W 1 ) &lt; f j (W 2 ). The first Pareto front is the set of weakly non-dominated points. In terms of finding Pareto optimal points, the linear scalarization technique discussed above can identify the complete Pareto front when the solution space is a convex set and the individual objective functions are convex functions on the solution space <ref type="bibr" target="#b4">[5]</ref>. However, if these convexity conditions are not met, the scalarization technique will not find the entire Pareto front. Often, the posterior distributions in <ref type="bibr" target="#b18">(19)</ref> are not convex. Figure <ref type="figure" target="#fig_2">5</ref> shows an example of the Pareto front of a multiobjective optimization, where f 1 and f 2 are the two dimensional pdfs of normal distributions, as shown below:</p><formula xml:id="formula_18">f i (W ) = (2π) -n/2 |Σ i | -1 2 e -1 2 (W -Wi) T Σ -1 i (W -Wi)<label>(21)</label></formula><formula xml:id="formula_19">W 1 = 10 8 , W 2 = 8 10 , Σ 1 = Σ 2 = 2I 2 . (<label>22</label></formula><formula xml:id="formula_20">)</formula><p>Even this relatively simple distribution has a non-convex Pareto front; note that minimizing a linear combination of f 1 and f 2 can only find optima at the extremes of the curve, and does not explore the interior, which may be more useful for some applications. This example motivates further research into generating MAP estimates in this manner, as finding the Pareto front could give us an advantage when attempting to infer parameters of the model as we do above, or perform some other common task; see for instance <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. STOCHASTIC BLOCK MODELS AND THE DSBM</head><p>Consider a single layer network. Often we are interested in networks that are expected to have some community structure. A community is defined as a subset of nodes that behave similarly to each other, where similarity is determined according to some fixed criterion. This allows for a more interesting community structure than just using the density of connections in a group, i.e., creating communities based on high intraconnectivity between nodes. For instance, one group may exhibit strong interconnection with another group, but only moderate connectivity within themselves. A Stochastic Block Model (SBM) is one way to model such community structure. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Consider a network with N nodes that we expect to fall in K classes, where c ∈ R N is a known class membership vector. In this setup we are considering binary relationships between nodes, and so a connectivity matrix A = {a xy } ∈ R N ×N is observed. The parameters for a standard SBM are prior probabilities of edges occurring between nodes within and across classes. Specifically, let Θ be a matrix of class probabilities called the Bernoulli parameter matrix, where θ ij is the probability of a link forming between a node in class i and class j. While the graph adjacency matrix will be N × N , Θ ∈ R K×K and is symmetric.</p><formula xml:id="formula_21">Letting S i = {x | c(x) = i}, it can be shown ([6]) that the MLE of θ ij is θij = m ij n ij<label>(23)</label></formula><formula xml:id="formula_22">m ij = x∈Si y∈Sj a xy<label>(24)</label></formula><formula xml:id="formula_23">n ij = |S i ||S j |, , i = j |S i |(|S i | -1), , i = j . (<label>25</label></formula><formula xml:id="formula_24">)</formula><p>This estimate of Θ (which we call Y ) can be used to explore the structure of the network. When the class membership vector c is unknown, the SBM can be modified to simultaneously estimate c and the Bernoulli matrix Θ <ref type="bibr" target="#b14">[15]</ref>.</p><p>The SBM accounts for community structure, but does not account for temporal changes in the network. One solution to this problem would be to fit a SBM to every time step in the sequence. This approach, however, fails to take advantage of information from previous time steps, and it does not encourage the class membership to evolve smoothly over time. Recently, the Dynamic SBM (DSBM) has been introduced to account for some of these effects <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>The DSBM of <ref type="bibr" target="#b5">[6]</ref> employs an extended Kalman filter (EKF) to track temporal changes in the network. Two types of DSBM were introduced in <ref type="bibr" target="#b5">[6]</ref>: one that is given the class membership a priori, and another that estimates the class memberships along with the other SBM parameters. For the benefit of the reader, the a priori DSBM is briefly reviewed below. The DBSM is based on the following simple linear model observation:</p><formula xml:id="formula_25">Y t = Θ t + z t ,<label>(26)</label></formula><p>where z t is i.i.d. zero-mean Gaussian noise and Θ t is an unknown matrix of Bernoulli parameters at time t. Because the elements of Θ t must be between 0 and 1, the DSBM uses a logistic transform to map them onto the real line:</p><formula xml:id="formula_26">ψ ij = log(θ ij ) -log(1 -θ ij ) ∈ (-∞, +∞) . (<label>27</label></formula><formula xml:id="formula_27">)</formula><p>Since the logistic transform is invertible, (26) can be written as</p><formula xml:id="formula_28">Y t = h(ψ t ) + z t .<label>(28)</label></formula><p>A linear state space model for the time evolution of the logistically transformed parameters ψ t is assumed. With this state space model for ψ t and the observation model ( <ref type="formula" target="#formula_28">28</ref>), an extended Kalman filter estimator can be implemented to produce state estimates ψt|t-1 from which the SBM parameters can be tracked over time:</p><formula xml:id="formula_29">ψt|t-1 = F t ψt-1|t-2 + K t|t-1 η t ,<label>(29)</label></formula><p>where η t = Y t -H t ψt|t-1 is the Kalman innovation process, K t|t-1 is the ( ψt|t-1 -dependent) Kalman gain, and H t is the Jacobian of h( ψt|t-1 ). Once the inference is complete, the Kalman estimate is then mapped back into Bernoulli parameters.</p><p>When the class memberships are unknown, the DSBM can be modified to estimate these memberships and the probability parameters simultaneously <ref type="bibr" target="#b6">[7]</ref>. For the ENRON data experiment described below, we implemented a multi-layer extension of the a priori DSBM in <ref type="bibr" target="#b6">[7]</ref> using a simple random walk state space model (F t = I, the identity matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ENRON EXAMPLE</head><p>The proposed dynamic SBM multi-layer community detection approach of Section VI is illustrated on real-world ENRON email data set 1 . This data set consists of approximately a half million email messages sent or received by 150 senior employees of the ENRON Corporation. These emails were made publicly available as a result of the SEC investigation of the company in 2002, and constitute one of the largest publicly available email repositories. This dataset represents a unique opportunity to examine private email messages in a corporate setting. This is rare due to privacy concerns and proprietary information, but the ENRON dataset is for the most part untouched, except for a few emails that were specifically requested to be removed. In addition to the raw emails, the dataset also contains the job title of the employees that are included. This is useful to separate the employees into classes, so that we may examine their behavior using the DSBM and its related techniques.</p><p>To explore the multi-level structure, two layers are extracted from the ENRON dataset. As discussed previously, one layer represents the extrinsic, "relational" information between users, and the other represents intrinsic, "behavioral" information between users. The network layers are extracted from the data as follows. First, a relational network is recovered from the headers of emails by identifying the sender and receiver(s) of each message, including Cc and Bcc recipients. For each week in the dataset, a separate network of employees is constructed from the emails sent during that week.</p><p>A second set of behavioral networks are recovered using the contents of email messages. On the same weekly basis the contents of all emails originating from each user are combined to form long "documents". Only emails that are sent by the user are considered, which is different from the relational case. This is to obtain a better representation of each user's individual writing habits, as opposed to the writing habits of them and their peers. These emails combine to produce a dictionary of words from which term frequency-inverse document frequency (TF-IDF) scores are calculated <ref type="bibr" target="#b16">[17]</ref>. TF-IDF scores are commonly used for identifying important words in text analysis, and are 1 <ref type="url" target="http://www.cs.cmu.edu/">http://www.cs.cmu.edu/</ref> ∼ enron computed using</p><formula xml:id="formula_30">tf(t, d) = f (t, d) max t f ( t, d) (30) idf(t) = log |D| N (t, D) (31) score(t, d) = tf(t, d)idf(t) ,<label>(32)</label></formula><p>where f (t, d) is the frequency of term t in document d, N (t, D) is the number of documents in which the term t appears, and |D| is the size of the document corpus, which in this case is the number of active network nodes. For each active user (document), a TF-IDF score is computed for each word in the dictionary. Using the vector of TF-IDF scores for each user, we measure the cosine similarity of each user by taking dot products in order to obtain a similarity matrix W . Again, this is done for every week in the relevant time period, creating a second dynamic network with weighted edges. However, since we started in the SBM framework, it is necessary to transform the weighted edge network into a binary network. To do this, the similarity scores are thresholded. To be roughly consistent with the density of the relational network, we keep the top 15% greatest correlations between users at each time step, setting all other connections to 0. This allows us to create networks of similar sparsity level.</p><p>The above procedure yields a two-layer binary dynamic network that we can use to obtain insight into the structural dynamics of the ENRON data. To do so, we extended the dynamic stochastic block model (DSBM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref> to the multi-layer setting. We group employees by their role in the company (CEO, President, Director, etc.). Thus, the DSBM class memberships are known a priori, and the a priori DSBM described in Section VI can be implemented to estimate the Bernoulli parameters, which predict the likelihood of an edge between users from any pair of groups.</p><p>Figure <ref type="figure">6</ref>(a) and Figure <ref type="figure">6</ref>(b) shows some of the estimated Bernoulli parameters for different classes when the DSBM is run on the two layers separately. Figure <ref type="figure">6</ref>(a) represents the evolution of the relational layer, while the Figure <ref type="figure">6(b)</ref> represents the behavioral layer. The DSBM was run over a 120 week period, from December 6th, 1999 to March 27th, 2002. The vertical lines represent important events in the ENRON time line. Line 1 corresponds to ENRON releasing a code of ethics policy. It is also the first time that the company's stock reached above $90. Line 2 corresponds to their stock closing below $60. This was a critical point in the timeline, because the company began losing many partnerships, including one to create a video-on-demand system. In this same month, a few of the employees had begun to communicate the uneasiness with ENRON's accounting practices. Line 3 is the week of Jeffrey Skilling's resignation. A mere month after his resignation as CEO, the SEC began their official inquiry into ENRON. These events are chosen as a baseline to compare the two layers of the network.</p><p>For the relational DSBM parameters, the most interesting results come from the CEO's activity. Note that the CEO group combines all past and present CEO's. This evolution of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To directors To CEOs To presidents To VPs To managers</head><p>To traders To others Fig. <ref type="figure">7</ref>. Combined DSBM Results. These graphs show the results of combining the two layers of the network with a parameter α = 0.5. Therefore, we should see attributes from both the behavioral and relational DSBM, and maybe some new, interesting results that result from combining the two layers.</p><p>different behavior than the relational layer. The Vice Presidents appear much more active during the entire period when compared with the relational layer. Because of the nature of the TF-IDF and thresholding process, there could be a number of reasons for this. One possible reason could be that the weeks in which the Vice Presidents were active, they could have been sending a lot of forwarded emails, acting as a conduit of information between parties. This would cause the TF-IDF scores for the Vice President group to rise. Another interesting phenomenon in the behavioral layer is that of the CEOs. Specifically, it is interesting how their activity drops off significantly, and in fact one event that is very much apparent in the relational parameters completely disappears. This can only happen if the document content for the CEOs during those weeks are completely orthogonal to the other groups. Because we consider only text that the sender has written, and we only consider sent emails, one explanation could be that the CEO's forwarded many emails without adding any additional text. This would cause the list of words for the CEO to become very small. However, a more likely explanation after some examination of the dataset shows that there is a large amount of activity in the relational dataset because many of the employees were emailing the CEO in a petition-like fashion, creating much activity. However, the CEO group actually sent very few emails during that time.</p><p>Combining the two networks as in Section III, we run the DSBM for different levels of the mixing parameter α. This was the probability of the selection variable choosing W 1 over W 2 . Because of the use of binary networks in this example, the α parameter is used as the probability that the combined data will choose to use the relational network when the two layers disagree with each other. The objective in this particular example is to show that using this method we can not only reduce noise, but also discover interesting multifaceted behavior that is not obvious from one layer alone. We expect that this form of combination will emphasize traits or attributes that occur in both networks; however, attributes that exist mostly in one network but are strong enough will also be retained. We can study these effects through various network measures; in this case we look at betweenness and degree centrality.</p><p>Figure <ref type="figure">7</ref> shows the DSBM parameters for mixing parameters α = 0.5. Smaller values of α should be chosen because the relational network seems to be less noisy and more stable. This makes sense as the extrinsic relational interactions are directly measured. One interesting phenomenon that occurs is that much of the behavior that we saw in the relational layer is present, including the high level of CEO activity. However, the period of inactivity that is experienced in the behavioral layer for the CEO group has an effect by dampening the some of the strong peaks that we saw towards the end of the time period.</p><p>Figure <ref type="figure">8</ref> shows the betweeness centrality of the Directors group over time as the mixing parameter is varied. In general, the betweeness rises roughly monotonically as α is varied; however, from week 95 to week 115, betweenness centrality is significantly increased when using a combined dynamic network-that is, an intermediate value of α. This time corresponds to the beginning of the company's upheaval and public disclosure of troubles. It may be concluded that by examining both network layers simultaneously we have removed some of the edges between other classes, and thus the centrality score of this particular group increased. It is true that during this time, when overall email usage increased, the betweenness centrality measure went down, as there were more shortest paths through users from other groups. Using the combination of layers, however, there appears to be an increase in the number of shortest paths through the Directors group.</p><p>On the other hand, we can also see well-behaved monotonic behavioral correlations in some cases. Figure <ref type="figure">9</ref> shows a transition of degree centrality for the class of CEOs (of which there were four during this time period). The behavioral network shows more connectivity for the CEO class. This phenomenon makes sense, as the behavioral data takes into account all written documents, which could be correlated with those of other users, while the relational network only takes into account direct communication between the CEOs and others. In reality, much of that communication is performed through third parties (such as assistants), and thus CEOs probably do not send as much email as the average employee. Increasingly anomalous behavior occurs toward the end of the time period. We hypothesize that this is due to a larger volume of unusual emails sent directly to the CEO during this tumultuous period. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>The literature on single layer networks is large, with contributions coming from many different fields. There are many results on structural and spectral properties of a singlelayer network, including community detection <ref type="bibr" target="#b18">[19]</ref>, random walk return times <ref type="bibr" target="#b19">[20]</ref>, and percolation theory results <ref type="bibr" target="#b20">[21]</ref>. Diffusion or infection models have also been studied in the context of complex networks (see <ref type="bibr" target="#b21">[22]</ref>, for instance).</p><p>Estimation of community structure in a network of agents is an active area of research in its own right. Specifically, the stochastic block model (SBM) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b12">[13]</ref> is used to model community structure within a network by assuming identical statistical behavior for disjoint subsets of nodes. These communities are more flexible than simple cliques because it is not required that they be heavily interconnected, but only that they interact with nodes in other subcommunities uniformly. More recently, the SBM has been extended to track temporal changes in the network, appropriately called a Dynamic SBM, or DSBM. We follow the development in <ref type="bibr" target="#b5">[6]</ref>, but there have been other extensions of the classic SBM. In particular, <ref type="bibr" target="#b14">[15]</ref> uses Gibbs sampling and probabilistic simulated annealing to estimate the Bernoulli parameters and class memberships over time. <ref type="bibr" target="#b15">[16]</ref> also fits a DSBM, but with a mixed membership model for the agents. The DSBM in <ref type="bibr" target="#b5">[6]</ref> uses an extended Kalman filter to track temporal changes between nodes, which will result in a smoothed and potentially insightful evolution of the estimated parameters.</p><p>Recently, there has been a growing interest in the multi-level network problem. Some basic network properties have been extended to the multilevel structure <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> as well as some results that serve as an extension of single layer concepts, such as multi-level network growth <ref type="bibr" target="#b24">[25]</ref> and spreading of epidemics <ref type="bibr" target="#b25">[26]</ref>. The metrics that have been proposed attempt to incorporate the dependence of the layers into the statistical framework, which allows for a much richer view of the network. In the same vein, the approach described in this paper performs parameter inference on a multi-level network, incorporating some of the dependence information that the multi-level structure allows.</p><p>Bayesian model averaging is also related to this work; ideas from BMA are used to create conditional independence between the layers of a network <ref type="bibr" target="#b0">[1]</ref>. This framework accounts for the interdependent relationships between the multiple layers into latent variables, which can then be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>We introduced a novel method for inference on multilayer networks. A hierarchical model was used to jointly describe the noisy observation matrices and MAP estimation was performed on the relevant latent variable. A simulation example using clustering demonstrated that the mixture of layers under the correct circumstances can lead to better results, and possibly a better understanding of the underlying structure between users. A real-life example was also discussed using the ENRON email dataset. The approach developed here can be extended to nonlinear multi-objective optimization techniques to explore other ways of inferring multi-layer networks, such as Pareto ranking <ref type="bibr" target="#b11">[12]</ref> or posterior Pareto ranking <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. ACKNOWLEDGEMENTS</head><p>We would like to thank Kevin Xu for providing the code for the DSBM model and his suggestions for utilizing it, as well as his general comments on the content of the paper. XI. APPENDIX A: SOLUTION OF TWO GAUSSIAN DISTRIBUTIONS Theorem 1: Let W ∈ R n The solution to the maximization problem</p><formula xml:id="formula_31">Ŵ = argmax W f (W ) = [γ 1 P 1 (W 1 |W ) + γ 2 P 2 (W 2 |W )] ,<label>(33)</label></formula><p>with P (W i |W ) of the multivariate Normal distribution</p><formula xml:id="formula_32">P (W 1 |W ) = N (W, σ 2 1 I n ) (34) P (W 2 |W ) = N (W, σ 2 2 I n ) ,<label>(35)</label></formula><p>is of the form</p><formula xml:id="formula_33">Ŵ = βW 1 + (1 -β)W 2 , beta ∈ [0, 1].<label>(36)</label></formula><p>Proof: The proof is separated into two steps. First, we show that for any arbitrary point W ∈ R n , the point W which is the projection of W onto the line g(W ) = W 1 + β(W -W 1 ) increases the value of f , that is</p><formula xml:id="formula_34">f (W ) ≤ f (W ) .<label>(37)</label></formula><p>Then we show that for all points on the line g(W ), f is maximized for some point on the line segment between W 1 and W 2 , corresponding to β ∈ [0, 1] Let W ∈ R n . There exists a unique decomposition of x into a vector parallel to g(W ) and one perpendicular to g(W ):</p><formula xml:id="formula_35">W = W + W ⊥ .<label>(38)</label></formula><p>Plugging W into f (W ), we have  (40)</p><formula xml:id="formula_36">f (W ) = (2π) -n/2 |σ 2 1 I n | -1 2 e -</formula><p>The exponent can be decomposed as follows:</p><formula xml:id="formula_37">(W +W ⊥ -W 1 ) T (W + W ⊥ -W 1 )<label>(41)</label></formula><p>= (W -W 1 ) T (W -W 1 )</p><formula xml:id="formula_38">+ 2W ⊥ (W -W 1 ) + x T ⊥ W ⊥ (42) = (W -W 1 ) T (W -W 1 ) + W T ⊥ W ⊥<label>(43)</label></formula><p>≥ (W -W 1 ) T (W -W 1 ) .</p><p>Note that since W 1 is on the line g(W ), and W ⊥ is orthogonal to all points on g(W ), W T ⊥ W 1 = 0 and so the cross term goes to 0. The same can be shown for the other exponential term with W 2 . Since the term with x is greater than with just x , so</p><formula xml:id="formula_40">f (W ) ≥ f (W ) .<label>(45)</label></formula><p>Finally, let us show that the maximum for f must be between W 1 and W 2 . This can easily be seen by the fact that both summation terms in f decrease as the distance between W and the means W 1 and W 2 increases. When on the line g, but outside the line segment between W 1 and W 2 , moving closer to the means will increase both terms. Therefore, the maximum of f must be on the line g, with β restricted between 0 and 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. General Latent Variable Model. This model represents a latent variable model, in which a set of variables Y control the distributions of the adjacency matrices and through them the observation matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Clustering Simulation. This surface plot shows the ARI for different simulations of σ 2 and β. Note that for all levels of σ 2 , a β that is around 0.5 tends to produce the best clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Pareto front for two Gaussians. A convex Pareto front would bulge toward the lower left corner, but this plot demonstrates that even relatively simple objective equations can have extremely non-convex Pareto fronts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Betweenness Centrality for Directors. This centrality is a measure of how connected a node is to the rest of the network. Larger centrality scores often occur for intermediate values of α, particularly between time 95 and 115.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 2W</head><label>1</label><figDesc>(W -W1) T (σ 2 1 In) -1 (W -W1) +W ⊥ -W2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.</head><label></label><figDesc></figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>parameters seems to indicate that during some of the important milestones in ENRON's demise, the CEO's were talking to each other more often, as well as sending out emails to the other employees in the network. This suggests that they were at least somewhat aware of what was happening with the company during these events, and had maybe discussed matters among themselves. From the relational layer, it also appears that the CEO's were the most active in communicating with other groups, where as the Directors showed very little connectivity. One explanation for this is that because the subset of employees that were studied were higher up in the company, the Director group didn't communicate with them as much, instead managing the lower level employees. Another interesting result is that the President group had much more activity towards the end of the time period, suggesting that as the legal situation worsened, their activity increased.</p><p>The behavioral DSBM parameters appear to be more noisy than their relational counterparts. In addition, they show very</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian model selection in social research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="111" to="164" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiobjective optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ehrgott</surname></persName>
		</author>
		<ptr target="http://search.proquest.com.proxy.lib.umich.edu/docview/208128027?accountid=14667" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiobjective Optimization</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470640425.ch18</idno>
		<ptr target="http://dx.doi.org/10.1002/9780470640425.ch18" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>John Wiley and Sons, Inc</publisher>
			<biblScope unit="page" from="231" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pareto multi objective optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ngatchou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zarei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Sharkawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on</title>
		<meeting>the 13th International Conference on</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
	<note>Intelligent Systems Application to Power Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proper efficiency and the theory of vector maximization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Geoffrion</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0022247X68902011" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="618" to="630" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic stochastic blockmodels: Statistical models for time-evolving networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O H</forename><surname>Iii</surname></persName>
		</author>
		<idno>abs/1304.5974</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-007-9033-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s11222-007-9033-z" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01908075</idno>
		<ptr target="http://dx.doi.org/10.1007/BF01908075" />
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-objective machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multicriteria gene screening for analysis of differential expression with dna microarrays</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mears</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swaroop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-criteria anomaly detection using pareto depth analysis</title>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
		<ptr target="http://books.nips.cc/papers/files/nips25/NIPS20120395.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="854" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-objective optimization for multi-level networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Oselio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-05579-4_16</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-05579-416" />
	</analytic>
	<monogr>
		<title level="m">Social Computing, Behavioral-Cultural Modeling and Prediction</title>
		<title level="s">ser. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Kennedy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8393</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels for directed graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">397</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive evolutionary clustering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O H</forename><surname>Iii</surname></persName>
		</author>
		<idno>abs/1104.1990</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1104.html#abs-1104-1990" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting communities and their evolutions in dynamic social networks-a bayesian approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5214-7</idno>
		<ptr target="http://dx.doi.org/10.1007/s10994-010-5214-7" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="157" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evolving cluster mixed-membership blockmodel for time-varying networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf</title>
		<meeting>14th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tf-sidf: Term frequency, sketched inverse document frequency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baena-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carmona-Cejudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morales-Bueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1044" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0378873383900217" />
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast algorithm for detecting community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.69.066133</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevE.69.066133" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66133</biblScope>
			<date type="published" when="2004-06">Jun 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random walks on complex networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rieger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.92.118701</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevLett.92.118701" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">118701</biblScope>
			<date type="published" when="2004-03">Mar 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.74.47</idno>
		<ptr target="http://link.aps.org/doi/10.1103/RevModPhys.74.47" />
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="47" to="97" />
			<date type="published" when="2002-01">Jan 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information diffusion in online social networks: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hacid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
		<idno type="DOI">10.1145/2503792.2503797</idno>
		<ptr target="http://doi.acm.org.proxy.lib.umich.edu/10.1145/2503792.2503797" />
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Metrics for the analysis of multiplex networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Battiston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical mechanics of multiplex networks: Entropy and overlap</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bianconi</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.87.062806</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevE.87.062806" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">62806</biblScope>
			<date type="published" when="2013-06">Jun 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Growing multiplex networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bianconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barthelemy</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.111.058701</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevLett.111.058701" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">58701</biblScope>
			<date type="published" when="2013-07">Jul 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Epidemic spreading on interconnected networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saumell-Mendiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boguñá</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.86.026106</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevE.86.026106" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">26106</biblScope>
			<date type="published" when="2012-08">Aug 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pareto-optimal methods for gene ranking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fleury</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/vlsisp/vlsisp38.html#HeroF04" />
	</analytic>
	<monogr>
		<title level="j">VLSI Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
