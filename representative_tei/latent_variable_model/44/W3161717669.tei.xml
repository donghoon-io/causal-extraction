<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSR-GAN: MULTI-SEGMENT RECONSTRUCTION VIA ADVERSARIAL LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-02-18">18 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mona</forename><surname>Zehni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE and CSL</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE and CSL</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MSR-GAN: MULTI-SEGMENT RECONSTRUCTION VIA ADVERSARIAL LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-18">18 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.09494v1[eess.SP]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-segment reconstruction</term>
					<term>adversarial learning</term>
					<term>unsupervised learning</term>
					<term>Gumbel-Softmax approximation</term>
					<term>categorical distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-segment reconstruction (MSR) is the problem of estimating a signal given noisy partial observations. Here each observation corresponds to a randomly located segment of the signal. While previous works address this problem using template or momentmatching, in this paper we address MSR from an unsupervised adversarial learning standpoint, named MSR-GAN. We formulate MSR as a distribution matching problem where the goal is to recover the signal and the probability distribution of the segments such that the distribution of the generated measurements following a known forward model is close to the real observations. This is achieved once a min-max optimization involving a generator-discriminator pair is solved. MSR-GAN is mainly inspired by CryoGAN [1]. However, in MSR-GAN we no longer assume the probability distribution of the latent variables, i.e. segment locations, is given and seek to recover it alongside the unknown signal. For this purpose, we show that the loss at the generator side originally is non-differentiable with respect to the segment distribution. Thus, we propose to approximate it using Gumbel-Softmax reparametrization trick. Our proposed solution is generalizable to a wide range of inverse problems. Our simulation results and comparison with various baselines verify the potential of our approach in different settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The problem of recovering a signal from a set of noisy partial observations appear in a wide range of applications including genomic sequence assembly <ref type="bibr" target="#b2">[2]</ref>, puzzle solving <ref type="bibr" target="#b3">[3]</ref>, tomographic reconstruction <ref type="bibr" target="#b4">[4]</ref> and cryo-electron microscopy (Cryo-EM) <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>, to name a few. In this paper, we focus on multi-segment reconstruction (MSR) <ref type="bibr">[7]</ref>, where the unknown is a 1D sequence and the measurements are noisy randomly located partial observations (segments) of this sequence. A schematic illustration of MSR is provided in Fig. <ref type="figure" target="#fig_0">1</ref>. MSR is a general form of multi-reference alignment (MRA) <ref type="bibr" target="#b8">[8]</ref> problem in which the measurements are noisy randomly shifted versions of the signal. While in MRA the length of each measurement is the same as the signal, in MSR the measurements can be shorter.</p><p>Current efforts devoted to MSR is studied in two broad categories, 1) alignment-based, 2) alignment-free. In one form of alignment-based methods, the segment location corresponding to each observation is estimated. Then, the observations are aligned accordingly and averaged. While these methods have low computational and sample complexity, low signal-to-noise ratio (SNR) of the observations adversely affect their performance. Examples of alignment-based methods applied to MRA and tomographic reconstruction are found in <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>. In other forms of alignment based methods, the segment locations and the 1D sequence are jointly updated using alternating steps. An example would be the maximum likelihood formulation of MSR, solved using expectationmaximization (EM). Despite the robustness of EM to different noise regimes, it suffers from high computational complexity. This is due to the complexity of the E-step, requiring a whole pass through the measurements at every iteration. This is significantly timeconsuming, especially in the presence of large number of observations.</p><p>Alignment-free solutions specifically designed for MRA sidestep the estimation of the random shifts by introducing a set of invariant features. These features constitute the moments of the signal and are estimated from the measurements. The signal is then estimated from the features via an optimization-based framework <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11]</ref>, tensor decomposition <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref> using Jennrich's algorithm <ref type="bibr" target="#b14">[14]</ref> or spectral decomposition <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. As these works are specialized for MRA, they do not address the challenges associated with MSR, such as observing only shorter segments of the signal. In <ref type="bibr">[7]</ref>, we showed how for MSR, we can estimate the invariant features from the measurements and how the recovery of the signal is tied to the segment length. Compared to alignment-based solutions, in alignment-free methods, we only have one pass through the measurements to estimate the features, thus computationally more efficient. The estimated features then serve as a compact representation of the measurements which are functions of the unknown signal and segment location distribution.</p><p>In this paper, we propose an alignment-free adversarial learning based method for MSR. Our goal is to find the unknown 1D signal and the distribution of the segment locations such that the measurements generated from the estimated signal match the real measurements in a distribution sense. Therefore, we train a generator discriminator pair, where the discriminator tries to distinguish between the measurements output by the generator and the real ones. Our approach is inspired by CryoGAN <ref type="bibr" target="#b1">[1]</ref> in which the goal is to reconstruct a 3D structure given 2D noisy projection images from unknown projection views. Unlike CryoGAN, we assume the distribution of the latent variables, i.e. the segment locations in MSR, is unknown and we seek to recover it alongside the signal. For this purpose, we modify the loss at the generator side using Gumbel-Softmax approximation of categorical distribution, to accommodate gradient-based updates of the segment location distribution. Our simulation results and comparison with several baselines confirm the feasibility of our approach in various segment length and noise regimes. Our code is available at <ref type="url" target="https://github.com/MonaZI/MSR-GAN">https://github.com/MonaZI/MSR-GAN</ref>.  In addition, the randomly located segment of the signal is contaminated by additive white Gaussian noise εj with zero mean and covariance σ 2 Im (Im denoting the identity matrix with size m × m). Our goal here is to recover x and p given the noisy partial observations {ξj} N j=1 . Note that the distribution of the observations depends on both the signal x and the distribution of the segment locations p. Thus, it is possible to estimate x and p by matching the distribution of the observations generated by x and p following (1) to the real measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>We use an unsupervised adversarial learning approach to solve MSR. Our method is unsupervised as it only relies on the given observations and does not use large paired datasets for training. Similar to <ref type="bibr" target="#b1">[1]</ref>, our method aims to find x and p such that the distribution of the partial noisy measurements generated from (1) matches the real measurements {ξ j real } N j=1 . To this end, we use a generative adversarial network (GAN) <ref type="bibr" target="#b17">[17]</ref>. Unlike common GAN models, we use the known forward model in <ref type="bibr" target="#b1">(1)</ref> to map the signal and segment distribution to the measurements. Thus, the generator acts upon x and p and simulates noisy measurements {ξ j sim } M j=1 . The discriminator's task is then to distinguish between the real and fake measurements from the generator. An illustration of MSR-GAN is provided in Fig. <ref type="figure" target="#fig_1">2</ref>. Here we use Wasserstein GAN <ref type="bibr" target="#b18">[18]</ref> with gradient penalty (WGAN-GP) <ref type="bibr" target="#b19">[19]</ref>, to benefit from its favorable convergence behaviour. In WGAN, the output of the discriminator is a score, where the more the input resembles ξreal, the higher the score. The min-max formulation of the problem is: Sample a batch of simulated measurements using estimated signal and PMF, i.e. {ξ b sim } B b=1 where</p><formula xml:id="formula_0">x, p = arg min x,p max φ L(φ, x, p)<label>(2)</label></formula><formula xml:id="formula_1">L(φ, x, p) = B b=1 D φ (ξ b real ) -D φ (ξ b sim ) -λ GP(ξ b int )<label>(3)</label></formula><formula xml:id="formula_2">ξ b sim = Msx + ε b , ε b ∼ N (0, σIm) 5: Generate interpolated samples {ξ b int } B b=1 , ξ b int = α ξ b real + (1 -α) ξ b</formula><p>sim with α ∼ Unif(0, 1)</p><formula xml:id="formula_3">6:</formula><p>Update the discriminator using gradient ascent steps with,</p><formula xml:id="formula_4">∇ φ LD(φ) = ∇ φ B b=1 D φ (ξ b real ) -D φ (ξ b sim ) + λGP(ξ b int ) 7:</formula><p>end for 8:</p><p>Sample a batch of {q i,b } B b=1 using (8)</p><p>9:</p><p>Update x and p using gradient descent steps with the following gradients,</p><formula xml:id="formula_5">∇x,pLG(x, p) = ∇x,p - B b=1 d-1 s=0 q i,b D φ (Msx + ε b ) 10: end while GP(ξ b int ) = ∇ ξ D φ (ξ b int ) -1 2<label>(4)</label></formula><p>where L denotes the loss which is a function of the discriminator's parameters φ, the signal and the PMF. Also, B is the batch size, D φ denotes the discriminator parameterized by φ and ξsim = Msx + ε, s ∼ p and ε ∼ N (0, σ 2 Im). The weight of the gradient penalty term (GP) is λ and ξint is a sample generated by linear interpolation between a real and simulated measurement, i.e. ξint = α ξreal + (1α) ξsim, α ∼ Unif(0, 1). To solve the min-max optimization in (2), following common practice, we take alternating steps to update the discriminator's parameters φ and the generator, i.e. x and p, using their gradients.</p><p>To update p, we need to take gradients of (3) with respect to p. However, this loss function is related to p through a sampling operator which is non-differentiable (we are sampling the segment locations based on the p distribution). This would be problematic at the generator update steps. Therefore, it is crucial to devise a way to have a meaningful gradient with respect to p. First, let us take a closer look at the loss function that is minimized at the generator side:</p><p>LG</p><formula xml:id="formula_6">(x, p) = - B b=1 D φ (Ms b x + ε b ), s ∼ p, ε ∼ N (0, σ 2 Im) (5) = - B b=1 d-1 s=0 δ(s -s b )D φ (Msx + ε b ) (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where δ is the Kronecker delta and δ(s -s b ) denotes the one-hot representation of a sample drawn from a categorical distribution with PMF p. Jang et al. in <ref type="bibr" target="#b20">[20]</ref> proposed a Gumbel-Softmax reparametrization trick to approximate samples from a categorical distribution with a differentiable function. We use this idea and LG(x, p)</p><formula xml:id="formula_8">≈ B b=1 d-1 s=0 q s,b D φ (Msx + ε b )<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">q s,b = exp ((g b,s + log(p[s]))/τ ) d-1 i=0 exp ((g b,i + log(p[i]))/τ )</formula><p>, g b,s ∼ Gumbel(0, 1). <ref type="bibr" target="#b8">(8)</ref> Note that ( <ref type="formula">8</ref>) is a continuous approximation of the arg max function, τ is the softmax temperature factor and q s,b → δ(sarg maxs (g b,s + log p[s])) as τ → 0. Note that drawing samples from arg maxs (g b,s + log p[s]), g b,s ∼ Gumbel(0, 1) is an efficient way of sampling from p distribution <ref type="bibr" target="#b20">[20]</ref>. Furthermore, to obtain samples from the Gumbel distribution <ref type="bibr" target="#b21">[21]</ref>, it suffices to transform samples from a uniform distribution using g = -log(-log(u)), u ∼ Unif(0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLEMENTATION DETAILS</head><p>We present the pseudo-code for MSR-GAN in Alg. 1. In all our experiments, we use a batch-size of B = 200 and keep the number of real measurements as N = 3 × 10 4 unless otherwise mentioned.</p><p>We have three separate learning rates for the discriminator, the signal and the PMF denoted by α φ , αx and αp, while in most experimental settings we keep αx = αp. We reduce the learning rates by a factor of 0.9, with different schedules for different learning rates. We use SGD <ref type="bibr" target="#b22">[22]</ref> as the optimizer for the discriminator and the signal x with a momentum of 0.9. We also update p using gradient descent steps after normalizing the corresponding gradients. We clip the gradients of the discriminator to have norm 1. Similar to common practice, we train the discriminator ndisc = 4 times per updates of x and p.</p><p>To have stabilized updates with respect to p, we choose τ = 0.5 in our experiments. We also use spectral normalization to stabilize the training <ref type="bibr" target="#b23">[23]</ref>.</p><p>Our architecture of the discriminator consists of three fully connected (FC) layers with , /2, and 1 output sizes, where is determined accordingly for different experiments. We use ReLU <ref type="bibr" target="#b24">[24]</ref> for the non-linear activations between the FC layers. We initialize the layers with weights drawn from normal distribution with mean zero and 0.01 standard deviation. We train MSR-GAN for 30, 000 and 50, 000 iterations for high and low SNR regimes, respectively. To enforce p to have non-negative values while adding up to one, we set it to be the output of a Softmax layer. Our implementation is in PyTorch and runs on single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NUMERICAL RESULTS</head><p>In this section we first provide details on our evaluation metrics and baselines. Next, we discuss our results. Evaluation metrics and baselines: The SNR of the observations is defined as the variance of the clean measurements divided by the variance of the noise. As the signal and PMF are reconstructed up to a random global shift, we align the reconstructions before comparing them to the ground truths. We use relative error (rel-error) between the aligned estimated signal x and the ground truth x as In this experiment the signal length d = 60, the signal is generated randomly and σ = 0.01. The success rate is computed based on 10 random initializations for each segment length value. All three methods are initialized with the same random x and p.</p><p>the quantitative measure of the performance, defined as rel-error</p><formula xml:id="formula_10">= mins x-Rs x 2 x 2</formula><p>, where Rs shifts its input by s ∈ {0, ..., d -1}. To assess the quality of the estimated PMF, we use total variation (TV) distance, defined as TV = 1 2 mins p -Rs p 1 <ref type="bibr" target="#b25">[25]</ref>. We also define success rate by running MSR solutions with 10 different initializations. The ratio of the initializations that lead to a relative-error less than a threshold 0.02 is reported as the success-rate.</p><p>We compare MSR-GAN to two baselines: 1) Estimating shiftinvariant features, i.e. moments up to the third order, from the measurements and recovering x and p by solving a non-convex optimization problem <ref type="bibr">[7]</ref>. We use up to third order moments as the features. We call this baseline MSR via shift-invariant features (MSR-SIF). We use Riemannian trust-regions method <ref type="bibr" target="#b26">[26]</ref> implemented in Manopt <ref type="bibr" target="#b27">[27]</ref> to solve the optimization problem. 2) Expectation maximization (EM). In this baseline, we formulate MSR as a maximum marginalized likelihood estimation problem and solve it via EM <ref type="bibr" target="#b8">[8,</ref><ref type="bibr">7]</ref>.</p><p>Effect of knowledge of PMF on the MSR-GAN results: Figure <ref type="figure" target="#fig_4">3</ref> shows the results of MSR-GAN on different signals with d = 64 and m = 24 in three different scenarios: 1) p in known (first column), 2) p is not known but fixed with a uniform distribution during training (second column), 3) p is not known and we recover it along side x (third and fourth columns). Note that for all three scenarios, we are using Alg. 1 with the same discriminator architecture and = 100. However, for the first and the second scenarios, we do not update p (skip step 9-update p), rather keep it fixed with the true and the uniform distribution, respectively.</p><p>Note that when the PMF is known, the results of MSR-GAN closely match the ground truth signal. When the PMF is unknown, if we fix p to be a uniform distribution (see second column of Fig. <ref type="figure" target="#fig_4">3</ref>), we observe that although the reconstructed signal is close to the GT, it has larger relative error compared to the scenarios where the PMF is given (see the first column of Fig. <ref type="figure" target="#fig_4">3</ref>) or the PMF is updated jointly with the signal (see the third column of Fig. <ref type="figure" target="#fig_4">3</ref>). Updating p jointly with x (Fig. <ref type="figure" target="#fig_4">3</ref>-third column) leads to more accurate reconstruction of the signal. This shows the importance of recovering the distribution of the segments. length regimes using shift invariant features is more challenging, as the number of equations provided by the moments for smaller segment lengths can be less than the number of unknowns. Similarly, the EM algorithm fails at shorter segments, i.e. m ≤ 25, where the success rate is less than 50%. EM is more likely to get stuck at a local optimal solution when the segment length becomes smaller. However, as MSR-GAN solves the inverse problem by matching the distribution of real measurements and stochastic gradient descent, it achieves higher success rates for smaller segment lengths. In particular, even at m = 15, MSR-GAN achieves a success rate close to 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of segment length and comparison with baselines:</head><p>Effect of noise and comparison with baselines: In Fig. <ref type="figure" target="#fig_6">5</ref>, we investigate the effect of noise on the performance of MSR-GAN compared to the baselines. For this experiment d = 60, m = 18 and for the discriminator's architecture we set = 300. Note that in different noise regimes MSR-GAN outperforms MSR-SIF and EM.</p><p>Here we have a short segment length, thus as mentioned earlier solving MSR is more challenging and both baselines get stuck in local minima that is not close to the ground truth solution. Note that if we increase the segment length we observe an improved reconstruction error and success rate for MSR-SIF and EM (as also observed in Fig. <ref type="figure" target="#fig_5">4</ref>). This suggests that MSR-GAN is a better solution compared to the baselines in short segment length regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we focused on the multi-segment reconstruction (MSR) problem, where we are given noisy randomly located segments of an unknown signal and the goal is to recover the signal and the distribution of the segments. We proposed a novel adversarial learning based approach to solve MSR. Our approach relies on distribution matching between the real measurements and the ones generated by the estimated signal and segment distribution. We formulated our problem in a Wasserstein GAN based framework. We showed how the generator loss term is a non-differentiable function of the segments distribution. To facilitate updates of the distribution through its gradients, we approximate the loss function at the generator side using Gumbel-Softmax reparametrization trick. This allowed us to update both the signal and the segment distribution using stochastic gradient descent. Our simulation results and comparisons to various baselines verified the ability of our approach in accurately solving MSR in various noise regimes and segment lengths.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multi-segment reconstruction (MSR) problem.</figDesc><graphic coords="1,315.21,183.77,243.78,50.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of MSR-GAN pipeline.</figDesc><graphic coords="2,54.43,72.00,243.77,133.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2. SYSTEM MODELWe consider the following observation model,ξj = Ms j x + εj, j ∈ {1, 2, ..., N }(1) where x ∈ R d is the underlying signal and ξj ∈ R m , m ≤ d is the j-th observation. We often refer to m as the segment length. The cyclic masking operator Ms captures m consecutive entries of x starting from index s. In other words, Ms : R d → R m and (Msx) [n] = x[n + s mod d]. We also assume the segment location s ∈ {0, 1, ..., d -1} to be unknown and randomly drawn from a categorical distribution with p as its probability mass function (PMF) where P {s = sj} = p[sj].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>MSR-GAN Require: α φ , αx, αp: learning rates for the discriminator, the image and projection angle distribution. λ: gradient penalty weight. ndisc: the number of iterations of the discriminator (critic) per generator iteration. Require: Initialize x randomly and p with a uniform distribution, i.e. p 0 [s] = 1/d. Output: Estimates I and p given {ξ j real } N j=1 . 1: while not converged do 2: for t = 0, ..., ndisc-1 do 3: Sample a batch from real data, {ξ b real } B b=1 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison between MSR-GAN in different noise regimes for 1) known PMF (first column), 2) unknown PMF but fixed with uniform distribution during training (second column), 3) unkown PMF and recovered during training (third column). The last column plots the ground truth PMF (green dashed curve) alongside the estimated PMFs from MSR-GAN (the same experiment as the third column) in blue and red. Each row corresponds to different signals and PMFs. The relative error of the reconstruction for SNR = ∞ and SNR = 1 is written in blue (SNR = ∞) and red (SNR = 1) underneath each subplot. For all experiments in this figure we are using the same architecture for the discriminator with = 100 and the number of measurements is N = 5 × 10 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b4">4</ref>. Effect of segment length on the success rate of 1) MSR-GAN (blue curve), 2) MSR-SIF (red curve), 3) EM (green curve). In this experiment the signal length d = 60, the signal is generated randomly and σ = 0.01. The success rate is computed based on 10 random initializations for each segment length value. All three methods are initialized with the same random x and p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison between MSR-GAN with different baselines in terms of relative-error versus SNR of the observations. In this experiment d = 60 and m = 18. All three methods have been initialized with the same signal and PMF and the reported results are the median across 10 different initializations and noise realizations for the observations.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cryogan: A new reconstruction paradigm for single-particle cryo-em via deep adversarial learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information theory of DNA shotgun sequencing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Motahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N C</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="6273" to="6289" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Solving multiple square jigsaw puzzles with missing pieces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The evolution of image reconstruction for ct-from filtered back projection to artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Willemink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noël</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Radiology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2018">10 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rapid solution of the cryo-em reconstruction problem by frequency marching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Greengard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pataki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spivak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1170" to="1195" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building proteins in a day: Efficient 3D molecular structure estimation with electron cryomicroscopy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Punjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="706" to="718" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-segment reconstruction using invariant features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zehni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4629" to="4633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bispectrum inversion with application to multireference alignment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bendory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1037" to="1050" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The projected power method: An efficient algorithm for joint alignment from pairwise differences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feasibility of tomography with unknown view angles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1107" to="1122" />
			<date type="published" when="2000-06">Jun 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Heterogeneous multireference alignment: a single pass approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bendory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal rates of estimation for multi-reference alignment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Statistics and Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="75" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM REVIEW</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Foundations of the parafac procedure: Models and conditions for an &quot;explanatory&quot; multi-modal factor analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UCLA Working Papers in Phonetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A spectral method for stable bispectrum inversion with application to multireference alignment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zehni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="911" to="915" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sample complexity of the boolean multireference alignment problem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1316" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A * sampling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3086" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COMPSTAT</title>
		<meeting>of COMPSTAT</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the total variation distance of labelled markov chains</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<idno>CSL-LICS &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Meeting of the Twenty-Third EACSL Annual Conference on Computer Science Logic (CSL) and the Twenty-Ninth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)</title>
		<meeting>the Joint Meeting of the Twenty-Third EACSL Annual Conference on Computer Science Logic (CSL) and the Twenty-Ninth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trust-region methods on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gallivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="330" />
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Manopt, a matlab toolbox for optimization on manifolds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Boumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page" from="1455" to="1459" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
