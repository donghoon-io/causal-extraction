<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HANDLING UNCERTAIN OBSERVATIONS IN UNSUPERVISED TOPIC-MIXTURE LANGUAGE MODEL ADAPTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ekapol</forename><surname>Chuangsuwanich</surname></persName>
							<email>ekapolc@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<email>watanabe@merl.com</email>
						</author>
						<author>
							<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
							<email>iwata.tomoharu@lab.ntt.co.jp</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HANDLING UNCERTAIN OBSERVATIONS IN UNSUPERVISED TOPIC-MIXTURE LANGUAGE MODEL ADAPTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>language model</term>
					<term>latent topic model</term>
					<term>topic tracking</term>
					<term>confusion network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an extension to the recent approaches in topic-mixture modeling such as Latent Dirichlet Allocation and Topic Tracking Model for the purpose of unsupervised adaptation in speech recognition. Instead of using the 1-best input given by the speech recognizer, the proposed model takes confusion network as an input to alleviate recognition errors. We incorporate a selection variable which helps reweight the recognition output, thus creating a more accurate latent topic estimate. Compared to adapting based on just one recognition hypothesis, the proposed model show WER improvements on two different tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Adaptive and topic-mixture models have been explored by researchers in order to describe text corpora. Cache-based models exploit the property that words that appear earlier in the document are more likely to appear again <ref type="bibr" target="#b1">[1]</ref>. Another popular approach is to model the underlying topic mixtures and interpolate between topic dependent word distributions <ref type="bibr" target="#b2">[2]</ref>. One example of such an approach is Latent Dirichlet Allocation (LDA) which identifies topics from an unlabeled corpus in an unsupervised manner <ref type="bibr" target="#b3">[3]</ref>. The topic mixtures can be used for document retrieval or document classification. These techniques can also be applied to help automatic speech recognition by adapting the Language Model (LM). For the task of speech recognition in academic lectures, Hsu and Glass <ref type="bibr" target="#b4">[4]</ref> used a Hidden Markov Model with LDA (HMM-LDA) <ref type="bibr" target="#b5">[5]</ref> which can model content words as well as syntactic words. In our previous work, we developed the Topic Tracking Language Model (TTLM) to explicitly capture the time evolution of topics throughout a recording session <ref type="bibr">[6]</ref>. In <ref type="bibr" target="#b7">[7]</ref>, LDA is used with a class-based cache model to also incorporate topic history. However, the aforementioned approaches adapt the language model using the 1-best recognition results. § Shinji Watanabe is now with Mitsubishi Electric Research Laboratories.</p><p>Unlike text corpora where word observations are considered certain, ASR output is actually a set of uncertain observations with associated posterior probabilities. Despite error rates as high as 30-40% in some large vocabulary tasks, previous works typically use the most likely output from the speech recognizer. The improvement from the adaptation tends to diminish compared to the perplexity gains on text corpora, and sometimes it becomes even worse than the baseline.</p><p>In this work, we expand on our previous work by introducing a latent selection variable into existing methods such as TTLM and LDA to deal with confusion network inputs instead of the conventional bag-of-word inputs. The model then "selects" the word that best suits the current model parameters within a Gibbs sampling framework. Since topic-mixture models are capable of improving recognition results, incorporating latent topics should be able to reliably reweight the network. By using this model extension, we were able to improve the topic tracking capability and our ultimate recognition results in two different speech recognition tasks.</p><p>The rest of this paper is organized as follows. In the next section, we give a brief overview of TTLM which we will use as the basis to explain our proposed model. We then explain our extension of TTLM to cope with possible recognition errors. In Section 4, we explain our experiments and discuss the performance of our model. Finally, in Section 5 we provide some concluding remarks and describe some future plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TOPIC TRACKING LANGUAGE MODEL</head><p>In our previous work <ref type="bibr">[6]</ref>, we proposed TTLM which, unlike the original LDA, can capture the time evolution of topics. A long session of speech input is divided into chunks t = 1, 2, ..., T that is modeled by different topic distributions φ φ φ t = {φ tk } K k=1 where K is the number of topics. The current topic distribution depends on the topic distribution of the past H chunks and precision parameters α th as follows:</p><formula xml:id="formula_0">P (φ φ φ t |{ φ φ φ t-h , α th } H h=1 ) ∝ K k=1 φ (α * φk )t-1 tk (1)</formula><p>where (f * g) TTLM can be applied on the 1-best hypothesis to recover the latent topic probability distributions. With the topic distribution, the unigram probability of a word w m in the chunk can be recovered using the topic and word probabilities P (w m ) K k=1 φtk θ kwm , where θ kwm is the unigram probability of word w m in topic k. The updated unigram can be used to scale n-grams via Minimum Discrimination Information (MDI) adaptation <ref type="bibr" target="#b8">[8]</ref>. The adapted n-gram can be used for a 2 nd pass recognition for better results.</p><p>The original TTLM can also adapt the topic dependent unigrams. However, it was found that adapting the unigrams can cause degradations due to data sparsity when the chunk size is small. Due to this concern and for simplicity, we omit any discussions on unigram adaptation in this paper. The set of topic dependent unigrams, denoted by θ, is trained using LDA on the training corpus and kept fixed throughout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TOPIC TRACKING LANGUAGE MODEL USING CONFUSION NETWORK INPUTS</head><p>Similar to TTLM, we start by breaking the speech input into chunks. However, we will use the resulting confusion network from the first pass recognition instead of the 1-best hypothesis. Consider a confusion network with M word slots. Each word slot m can contain different number of arcs A m , with each arc containing a word w ma and a corresponding arc posterior d ma . As mentioned earlier, since the TTLM can help produce better transcriptions, there should be merit in using the latent topics discovered in the model to reweight the recognition outputs. Instead of fully trusting the 1-best hypothesis, the goal is to have the model be able to select the best arcs that can describe the chunks according to the latent topics. To accomplish this, we associate a binary selection parameter s ma , where s ma = 1 indicates that the arc is selected. S m represents the arc index a where s ma = 1. Selected arcs are considered to be correct words which should be generated from the current topic. The other arcs are considered errors generated from a separated error unigram θ e . We denote W t , Z t , S t as the sequence of words, topics, and selections in chunk t respectively, while the subscript m denotes the index within the chunk. A graphical representation of the Topic Tracking Language Model with Confusion Network inputs (TTLMCN) is show in Figure <ref type="figure" target="#fig_0">1</ref>. The generative process of the TTLMCN can be summarized as follows:</p><p>1. Draw φ φ φ t from Dirichlet((α * φk ) t )</p><p>2. For each word slot m in chunk t From this generation process, for each chunk t, we can write the joint distribution of words, latent topics and arc selections conditioned on the topic probabilities, unigram probabilities, and arc posteriors as follows:</p><formula xml:id="formula_1">P (W t , Z t ,S t |θ e , θ, φ φ φ t , D t ) = N m φ tzm Am a d sma ma θ sma zmwma θ 1-sma ewma (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inference</head><p>We infer latent topics and "correct" words based on collapsed Gibbs sampling <ref type="bibr" target="#b9">[9]</ref>. We start from the joint distribution and substituting Eqs. 1 and 2 :</p><formula xml:id="formula_2">P (W t , Z t , S t | φ φ φ t-1 , D t , α t , θ, θ e ) = P (W t , Z t , S t |θ e , θ, φ φ φ t , D t )P (φ φ φ t | φ φ φ t-1 , α t )dφ t = Γ(α t ) k Γ((α * φk ) t ) k Γ(n tk + (α * φk ) t ) Γ(n t + H h=1 α th ) × m a d sma ma θ sma zmwma θ 1-sma ewma (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where Γ() is the Gamma function, n t denotes the number of word slots in chunk t, and n tk denotes the number of words assigned to topic k in chunk t.</p><p>From Eq. 3, we can derive the Gibbs sampling equations of Z t and S t . (Appendixes of <ref type="bibr">[6]</ref> can be used as an outline for the derivations.)</p><formula xml:id="formula_4">P (z) tmkj P (z m = k|S m = j, ...) ∝ n tk\m + (α * φk ) t n t\m + H h=1 α th θ kwmj (4) P (s) tmkj P (S m = j|z m = k, ...) ∝ d mj θ kwmj (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where \m implies counts excluding the m th word slot. These two equations make sense intuitively: the probability of picking a topic depends on how often the topic occurs (with some influence from previous chunks). The probability of selecting an arc depends on the posterior and the corresponding topic dependent unigram. α th can be updated by the maximum likelihood estimation of the joint distribution in Eq. 3 as follows:</p><formula xml:id="formula_6">α th ← α th k φt-hk (Ψ(n tk + (α * φk ) t ) -Ψ((α * φk ) t )) Ψ(n t + h α th ) -Ψ( h α th )<label>(6)</label></formula><p>where Ψ() is the Digamma function. By iterating Eqs. ( <ref type="formula">4</ref>)-( <ref type="formula" target="#formula_6">6</ref>), we can obtain Z t and α th which can then be used to estimate the means of the topic distribution as follows:</p><formula xml:id="formula_7">φtk = n tk + (α * φk ) t n t + h α th (7)</formula><p>These means can be used to update the n-gram just like in the original TTLM. Extending to LDA is very similar in concept. One thing of note is the handling of epsilon transitions which indicates a deletion of the word slot. However, unigrams trained on text corpora would not naturally include epsilon transitions. We incorporate this by adding an additional entry to the unigrams with probability p and rescale the rest of the probability masses accordingly. p is considered a tuning parameter indicating the trade-off between insertions and deletions. The word counts should also exclude word slots that select epsilon transition arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Posterior Interpolation</head><p>Even though arc selection based on latent topics makes intuitive sense, basing these selections on unigrams instead of n-grams causes many incorrect arcs to be selected. One possible fix is to have a high pruning threshold for generating the confusion network. This leaves only the most confusable choices which helps reduce selection errors. However, the correct arcs might also be pruned out, limiting the usefulness of this model. Thus, we use a simple interpolation scheme to reinforce the original posterior by modifying the Gibbs sampling of the arc selection in Eq. 5 as follows:</p><formula xml:id="formula_8">P (S m = j|z m = k, ...) = P (z) tmkj P (s) tmkj + (1 -P (z) tmkj )d mj (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We conducted experiments on two different speech recognition tasks; the MIT OpenCourseWare (MIT-OCW) <ref type="bibr" target="#b10">[10]</ref> and the Corpus of Spontaneuos Japanese (CSJ) <ref type="bibr" target="#b11">[11]</ref>. The TTLM implementation followed the framework described in  [6]. Each recording was divided into chunks that were then processed sequentially. The error unigram θ e was set to be uniform. For both tasks, we show the performance of the existing baselines: unadapted LM (BASE), LDA, and TTLM. We also show the performance of LDA and TTLM with our proposed extensions to accept confusion network inputs (LDACN and TTLMCN). The posterior interpolation method was also examined for the TTLMCN case (TTLMCNI). For the MIT-OCW task, we also conducted an oracle experiment (ORACLE) where the means of the latent topic were learned from the original transcripts to indicate the best case scenario for language model adaptation based on TTLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MIT-OpenCourseWare Corpus</head><p>MIT-OCW is mainly composed of lectures given at MIT. Each lecture is typically two hours long. We segmented the lectures using Voice Activity Detectors into utterances averaging two seconds each. In order to be consistent with our previous work in <ref type="bibr">[6]</ref>, we set the size of each chunk to 64 utterances, although our more recent work <ref type="bibr" target="#b12">[12]</ref> incorporates multiple chunk sizes for additional performance gain. The training data consists of 147 lectures (128h of speech, 6.2M word). The development set contains one lecture (1 hour), while the evaluation was done on three lectures (3.5 hours). Table <ref type="table" target="#tab_1">1</ref> summarizes the recognition results on this task. Incorporating confusion network inputs improved performance for both LDA and TTLM algorithms. The posterior interpolation also improved the performance slightly. Figure  2 shows the performance of each algorithm for each individual lecture. One important thing to note is that on Test1, TTLM actually did worse than the non-adapted LM. This was partly due to the high WER of the ASR. Another cause was that the chunks in Test1 typically contained fewer words than the other lectures, which intensified the WER problem. Incorporating confusion network inputs alleviated the problem and improved the recognition results compared to the baseline. However, there was still a sizable gap from the oracle experiments indicating room for possible improvements. We also looked into the difference between the topic probability distributions estimated by each model such as one shown in Figure <ref type="figure" target="#fig_2">3</ref>. We can see that the topic probability of TTLMCNI is more similar to the oracle experiment than TTLM, especially in the low probability regions. The average KL divergence between the distributions obtained in TTLM and ORACLE was 3.3, while the KL divergence between TTLMCNI and ORACLE was 1.3, a noticeable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Corpus of Spontaneous Japanese</head><p>CSJ is mainly composed of conference presentations. The acoustic model was trained on 234 hours of speech, while the LM was trained on a larger set of 6.8M words. We used "CSJ testset 2" for development and "CSJ testset 1" for testing. Each set contains ten presentations where each presentation averages around 15 minutes. We used one utterance as one chunk to simulate a more real-time friendly scenario. Unlike in our previous work <ref type="bibr">[6]</ref> where the Minimum Discrimination Information n-gram scaling factor was fixed at 0.5, we also tuned this parameter using the development set.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the performance on the CSJ task. The confusion network extensions continued to show similar improvements even though the baseline WER was already low compared to the MIT-OCW task. The TTLMCNI showed the best improvement of 6.7% compared to the baseline LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We described an extension for the TTLM in order to handle errors in speech recognition. The proposed model used a confusion network as input instead of just one ASR hypothesis which improved performance even in high WER situations. Experiments on MIT-OCW and CSJ tasks showed improvements on the WER. The gain in word error rate was not very large since the LM typically contributed little to the performance of LVCSR. However, the topic probability estimates improved considerably. As future work, we would like to explore other ways of reincorporating the n-gram into the selection which might decrease the gap in performance between our model and the oracle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical representation of TTLMCN. Shaded and unshaded nodes indicate observed and latent variables, respectively. Note that φ can depend on the φs of several preceding chunks. However, for figure clarity, we depict only the case when H = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. WER performance on each individual lecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A heat map of topic distributions over time on an excerpt of TEST3. Blue pixels represent lower probability, while red pixels represent higher probability. Y-axis corresponds to different LDA topics. X-axis corresponds to 4 successive time chunks. System Dev WER Test WER % change BASE 17.7 20.8 -LDA 16.5 19.8 4.8 LDACN 16.4 19.6 5.8 TTLM 16.3 19.7 5.2 TTLMCN 16.1 19.5 6.5 TTLMCNI 16.1 19.4 6.7</figDesc><graphic coords="4,151.26,76.26,62.87,87.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Test set performance on the MIT-OCW task. P1 and P3 denote 1-gram and 3-gram perplexity respectively.</figDesc><table><row><cell>System</cell><cell>P1</cell><cell>P3</cell><cell>WER</cell><cell>% change</cell></row><row><cell>BASE</cell><cell>611.5</cell><cell>208.2</cell><cell>41.4</cell><cell>-</cell></row><row><cell>LDA</cell><cell>543.2</cell><cell>187.8</cell><cell>41.3</cell><cell>0.2</cell></row><row><cell>LDACN</cell><cell>549.2</cell><cell>202.4</cell><cell>41.0</cell><cell>1.0</cell></row><row><cell>TTLM</cell><cell>521.7</cell><cell>184.9</cell><cell>41.0</cell><cell>1.0</cell></row><row><cell>TTLMCN</cell><cell>504.4</cell><cell>179.4</cell><cell>40.6</cell><cell>2.0</cell></row><row><cell>TTLMCNI</cell><cell>503.4</cell><cell>178.88</cell><cell>40.5</cell><cell>2.2</cell></row><row><cell>ORACLE</cell><cell>482.9</cell><cell>171.9</cell><cell>39.4</cell><cell>4.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>4 successive time chunks. Performance on the CSJ task.</figDesc><table><row><cell>System</cell><cell>Dev WER</cell><cell>Test WER</cell><cell>% change</cell></row><row><cell>BASE</cell><cell>17.7</cell><cell>20.8</cell><cell>-</cell></row><row><cell>LDA</cell><cell>16.5</cell><cell>19.8</cell><cell>4.8</cell></row><row><cell>LDACN</cell><cell>16.4</cell><cell>19.6</cell><cell>5.8</cell></row><row><cell>TTLM</cell><cell>16.3</cell><cell>19.7</cell><cell>5.2</cell></row><row><cell>TTLMCN</cell><cell>16.1</cell><cell>19.5</cell><cell>6.5</cell></row><row><cell>TTLMCNI</cell><cell>16.1</cell><cell>19.4</cell><cell>6.7</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A cache-based natural language model for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Style &amp; topic language model adaptation using HMM-LDA</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">Advance in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
	<note>Integrating topics and syntax</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topic tracking language model for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="440" to="461" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dirichlet class language models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE trans. on Audio, Speech, and Language processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="482" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language model adaptation using dynamic marginals</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1971" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences</title>
		<meeting>of the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recent progress in the MIT spoken lecture processing project</title>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2553" to="2556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Japanese national project on spontaneous speech corpus and processing technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASR, 2000</title>
		<meeting>ASR, 2000</meeting>
		<imprint>
			<biblScope unit="page" from="244" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model adaptation for automatic speech recognition based on multiple time scale evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1081" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
