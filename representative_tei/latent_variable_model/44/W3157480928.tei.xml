<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Information based Method for Unsupervised Disentanglement of Video Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Sreekar</surname></persName>
							<email>paditya.sreekar@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ujjwal</forename><surname>Tiwari</surname></persName>
							<email>ujjwal.t@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Information based Method for Unsupervised Disentanglement of Video Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Prediction is a challenging but interesting task of predicting future frames from a given set context frames that belong to a video sequence. Video prediction models have prospective applications in maneuver planning, healthcare, autonomous navigation and simulation. One of the major challenges in future frame generation is the high dimensional nature of visual data. To handle this, we propose a Mutual Information Predictive Auto-Encoder (MIPAE) framework that reduces the task of predicting high dimensional video frames by factorising video representations into content and low dimensional pose latent variables. Our approach leverages the temporal structure in the latent generative factors of video sequences by applying a novel mutual information loss to learn disentangled video representations. A standard LSTM network is used to predict these low dimensional pose representations. Content and the predicted pose representations are decoded to generate future frames. We also propose a metric based on mutual information gap (MIG) to quantitatively assess the effectiveness of disentanglement on DSprites and MPI3D-real datasets. MIG scores corroborate the visual superiority of frames predicted by MIPAE. We also compare our method quantitatively on LPIPS, SSIM and PSNR evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Humans through experience learn to predict possible future states of the visual world by conditioning on the past. This key foresight has helped us in planning our actions ahead of time in highly dynamic real-world settings <ref type="bibr" target="#b0">[1]</ref>. In this work, our goal is to design an intelligent system that is capable of predicting future frames of a video sequence. That is, given a sub-sequence of context frames, a generative model is trained to predict plausible future frames. This problem of video frame prediction has been studied in variety of different contexts such as anomaly detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, robotics <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and healthcare <ref type="bibr" target="#b5">[6]</ref>. Recent advancements in predictive auto-encoder based generative modelling methods such as the advent of GANs <ref type="bibr" target="#b6">[7]</ref> and VAE <ref type="bibr" target="#b7">[8]</ref> have substantially improved the visual quality of the predicted frames. However, modelling the predictive distribution of future frames is challenging due to the highdimentional nature of video frames.</p><p>Concurrent video prediction methods overcome the challenge of making predictions in high dimensional pixel space by factorising video representations into a low dimensional temporally varying component and another temporally consistent component. Tulyakov et al. <ref type="bibr" target="#b8">[9]</ref> factorised video representations into time dependent pose and time independent content representations. Similarly, Vondrick-Pirsiavash et al. <ref type="bibr" target="#b9">[10]</ref>  decomposed video into salient (foreground) and non-salient (background) regions. In this paper, we introduce a novel predictive auto-encoder, referred to as MIPAE, that factorises the latent space representation of a video into two components, time dependent pose and time independent content latent variables. Our interpretation of the factorised latent space is similar to that of DRNET <ref type="bibr" target="#b10">[11]</ref>.</p><p>DRNET <ref type="bibr" target="#b10">[11]</ref> disentangled video representation into time dependent pose and time independent content latent representations. In their work, an adversarial loss is applied on the pose latent variables to achieve pose/content disentanglement. The adversarial objective enforces the pose latent variables to be indiscriminate for different video sequences, ensuring that they do not encode any content information. In the proposed work, the application of adversarial loss on pose latent representations has been formalised as reducing mutual information between pose representations across time. This ensures that pose latent factors contain minimum mutual information, thus enforcing that they should not contain any content information (Fig. <ref type="figure" target="#fig_0">1</ref>). We train a LSTM model conditioned on the content latent representation of the last observed frame to predict the low dimensional pose representation for future frames. The predicted pose and content representation is used to generate the next frame of the video sequence.</p><p>Mutual Information (MI) is a fundamental measure in Information theory that quantifies the amount of dependency between two random variables. By virtue of the recent advancements in deep learning, a number of methods <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> use Neural Estimators to approximate MI. The ability of these methods in approximating MI between two random variables, given samples from the joint and marginal distributions has inspired multiple applications of mutual information based loss arXiv:2011.08614v1 [cs.CV] 17 Nov 2020 term for learning disentangled data representations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. In the proposed MIPAE framework, Jenson-Shannon lower bound estimate of MI <ref type="bibr" target="#b13">[14]</ref> is penalised between pose latent representations across time for proper pose/content factorisation. The contributions of this paper are as follows:</p><p>• To the best of our knowledge, this work is the first to propose and empirically validate that minimising mutual information between pose latent variables across time in predictive auto-encoder setting for the task of frame prediction leads to factorised latent space representation of videos. • We adopt and present a metric based on Mutual Information Gap <ref type="bibr" target="#b14">[15]</ref> to quantitatively measure the effectiveness of disentanglement between the factorized latent variables -pose and content. This score has also been used for conclusive evaluation of our disentanglement method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Video Prediction</head><p>Previously reported methods predicted future frames by minimising reconstruction error in predictive auto-encoder or recurrent network setting <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. These methods assume the process of video generation to be deterministic. This assumption leads to blurry frame predictions on real-world videos exhibiting scene with stochastic dynamics as the model predicts an average of the possible future frames. <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> used VAE-based latent variable models to account for the inherent stochasticity in real world video sequences. While, these networks can model distributions over possible futures, the predictive distribution is still fully factorized over pixels which tends to produce blurry predictions. To alleviate this issues of blurry frame prediction, SAVP <ref type="bibr" target="#b22">[23]</ref> used adversarial training in a VAE-GAN setting to produce sharp and stochastic predictions. The aim of our MIPAE framework is to not only obtain visually sharp frame predictions but also produce proper disentanglement of video representations that significantly reduces the complexity of visual frame prediction.</p><p>Transformation based methods predict subsequent frames by transforming previous frame through a constrained geometric transformation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>. These methods primarily focus on modeling motion rather than on reconstructing appearances by exploiting the temporal consistency in video sequences. Finn-Goodfellow et al. <ref type="bibr" target="#b3">[4]</ref> modeled motion of masked out groups of pixels in a Conv-LSTM framework by predicting transformation kernels at each time step in an action conditioned setting. Vondrick-Torrabla et al. <ref type="bibr" target="#b23">[24]</ref> generated transformations for each pixel using adversarial training to constraint the set of plausible transformations. Learning non-linear transformations between consecutive frames in a video sequence that exhibit stochastic dynamics is extremely challenging due to the highdimentionality of video frames. Complementary to these methods, our framework predicts a single low dimensional pose vector rather than predicting non-linear transformations between consecutive frames.</p><p>Disentangling representation for the task of video prediction ha been explored by <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. MCNet <ref type="bibr" target="#b24">[25]</ref> disentangled video into content and motion representations by explicitly modelling flow using image difference and use a single content encoding for future frame prediction. Similarly, MOCOGAN <ref type="bibr" target="#b8">[9]</ref> disentangled video by sampling content representations once for the whole video sequence and motion representation for each frame, realism of the predicted frames is enforced using adversarial training. DRNET <ref type="bibr" target="#b10">[11]</ref> disentangled video into content and pose representations by using an adversarial loss term which aims to confuse a discriminator classifying pose vectors between same and different video sequences. DDPAE <ref type="bibr" target="#b25">[26]</ref> decomposed the video into constituent set of objects and learn disentangled representations of content and pose for these objects. In contrast to these methods we learn to factorise video into content/pose by adopting the DRNET video generation architecture and penalising mutual information (MI) between pose latent variables across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Disentangling Representations</head><p>Unsupervised learning of disentangled data representations is a well explored area in AI research <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. InfoGAN <ref type="bibr" target="#b16">[17]</ref> maximized MI between latent code and observations to disentangle the latent generative factors. Recent research has also used different variations of Evidence lower bound (ELBO) proposed by Kingama <ref type="bibr" target="#b7">[8]</ref> to enforce such disentanglement. β-VAE <ref type="bibr" target="#b28">[29]</ref> proposed a method based on using a high multiplicative constant for KL-divergence in ELBO to limit the capacity of latent information channel, thus enforcing highly factorised latent space representations. Total correlation based penalty between latent variables to attain disentanglement effect has also been explored by β-TCVAE <ref type="bibr" target="#b14">[15]</ref> and FactorVAE <ref type="bibr" target="#b15">[16]</ref>. However, <ref type="bibr" target="#b29">[30]</ref> theoretically explained that it is impossible to disentangle latent factors of data generation without exploiting the structure of the true generative factors. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref> exploit such known structural relationships between factors of data generation to learn factorised latent representations. In our proposed MIPAE framework, video representations are factorized by considering the temporal structure of the content and pose generative factors, that is, the time independence of content and time dependence of pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mutual Information Estimation</head><p>Mutual Information(MI) features prominently in ICA literature, such as <ref type="bibr" target="#b33">[34]</ref>, but they failed to provide a general method for computationally estimating mutual information from samples of two random variables. <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> proposed a non-parametric method to estimate mutual information between two random variables using k-nearest neighbour approach. But these methods do not scale well for large and high dimensional data.</p><p>Recent works have used variational estimation coupled with deep neural networks for tractable estimation of MI. <ref type="bibr" target="#b36">[37]</ref> proposed variational lower bound to estimate f-divergences, F-GAN <ref type="bibr" target="#b37">[38]</ref> utilised this to estimate common instances of f-divergence like KL-divergence or Jensen-Shannon divergence given samples from two different distributions. MINE <ref type="bibr" target="#b11">[12]</ref> used Donsker-Vardhan <ref type="bibr" target="#b38">[39]</ref> dual formulation for estimating MI, which is expressed as KL-divergence between joint distribution and the product of marginal distributions over a couple of random variables. Methods based on variational lower bounds use a deep neural network as critic to estimate MI. However, updating the critic using gradients of the F-GAN formulation is unstable. Hence, <ref type="bibr" target="#b13">[14]</ref> proposed to use GAN objective instead of using gradients from F-GAN to train the critic, we use this estimate of MI in our work. For comprehensive overview on variational bounds of MI, readers are requested to refer to <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_0">III. OUR APPROACH Our task is to generate the next T frames, xC+1:C+T = (x C+1 , xC+2 . . . xC+T ) of a video sequence conditioned on a sub-sequence of C context frames, x 1:C = (x 1 , x 2 , . . . x C ).</formula><p>Making predictions in the high dimensional image space pose a major challenge to auto-regressive models. We propose some simplifying assumptions to overcome this challenge of learning a predictive distribution over possible futures in image-pixel space. It is assumed that the identity of the constituent objects in the video sequence do not change with time, whereas the pose of these objects keep varying throughout the video sequence. Hence, the true generative factors for a video sequence can be considered to be time independent content factor, f c , and time dependent pose factors, f 1:C+T p . On top of the aforementioned assumption, we also consider that the video sequences are generated by a two step random process: <ref type="bibr" target="#b0">(1)</ref>  Here z c is considered to be consistent for all the future frames. We also train a pose encoder E p which is applied to all frames independently to get the pose representations z 1:C+T p . As the second step of the generative process under consideration, a decoder D is trained to reconstruct xt from z c and z t p , where z t p denotes the pose representation for the frame x t . It is essential to note that in context of video prediction, proper pose/content disentanglement refers to a situation where any change in the true generative factors f c or f t p should lead to change only in the learned content z c or pose z t p representations, respectively. However, Locatello-Bauer et al. <ref type="bibr" target="#b29">[30]</ref> showed that disentangling the generative factors of data in a completely unsupervised manner is impossible. This is because for any true generative factor f , one can always specify another equivalent set of generative factors f by trivially applying a deterministic function on f such that f = g(f ). Here, f is completely entangled with f . In unsupervised learning where only data samples are given, the learned latent representations z cannot distinguish the true generative factors f from their transformed versions f . Hence, it will be entangled with either f or f . In view of the above, it is proposed to disentangle video representations into pose z 1:C+T p and content z c latent representations by exploiting the temporal structure of the true underlying generative factors -time independence of content f c and time dependence of the pose f t p . Formally, in MIPAE framework we enforce this temporal structure by applying similarity loss L sim between the content latent representations z c of different frames from a given sequence and by minimizing the proposed Mutual Information Loss L M I between the pose latent representations z t p across time. L sim in Fig. <ref type="figure" target="#fig_1">2</ref> (Left), is the l 2 loss minimised between encoded frames z c t , z c t+k acquired by encoding frames x t , x t+k by the content encoder E c . L sim , enforces the content encoder E C to extract temporally invariant content features from the video sequence. However, this alone does not ensure proper pose/content disentanglement, as the pose latent representation of frames can still encode some aspect of the temporally invariant content information. Any content information in pose representations can be modeled as the mutual information between them. The pose encoder E p is restrained from encoding any content information by penalizing the mutual information between z t p and z t+k p for some random offset k. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> (Left), to calculate the mutual information between z t p and z t+k p , we acquire the joint (z t p,i , z t+k p,i ) and marginal samples (z t p,i , z t+k p,j ), where i and j denote they belong to two distinct video sequences. A critic, C, is trained to discriminate between the joint and marginal samples. We use the critic outputs to estimate mutual information using L M I as described in Eq. 3. Application of L M I is essential for proper pose/content factorisation of video representation, as the pose encoder E p and content encoder E c are explicitly constrained to encode complementary set of information for proper frame reconstruction. To ensure proper reconstruction, l 2 reconstruction error L recon is minimised between the ground truth and decoded frame. Similarity loss L sim , reconstruction loss L recon and proposed mutual information loss L M I have been explained in section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models and Architecture</head><p>The proposed architecture and loss terms are similar to DRNET except for their adversarial loss which is replaced with our mutual information objective. The different loss terms are explained below as follows: Similarity Loss: To enforce time invariance of content representation we penalize change in content representation between two different frames from the same video sequence that are separated by random offset k ∈ [0, K] time steps:</p><formula xml:id="formula_1">L sim = E P (x t ,x t+k ) E c (x t ) -E c (x t+k ) 2 2 (1)</formula><p>Mutual Information Loss: We now introduce the proposed Mutual Information loss term which is applied on the pose latent variables across time to achieve proper pose/content factorisation. For estimating the mutual information between z t p and z t+k p , we first train a critic C to classify whether z t p and z t+k p are sampled from joint distribution P (z t p , z t+k p ) or the product of marginal distributions P (z t p )P (z t+k p ) for some random offset k ∈ [0, K] using standard GAN discriminator objective <ref type="bibr" target="#b6">[7]</ref>. In practice, samples of (z t p , z t+k p ) from P (z t p , z t+k p ) are acquired by encoding frames x t and x t+k belonging to the same video sequence, whereas samples from the P (z t p )P (z t+k p ) is acquired by encoding frames from two different video sequences, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> (Left). The GAN discriminator objective is maximized for the optimal critic, C * (z t p , z t+k p ) = log(P (z t p , z t+k p )/P (z t p )P (z t+k p )). The expectation of C * with respect to P (z t p , z t+k p ) is the definition of MI. The critic objective is described below where σ(.) is the sigmoid function which is applied on critic output:</p><formula xml:id="formula_2">L C = E P (x t ,x t+k ) σ(C(E p (x t ), E p (x t+k ))) +E P (x t )P (x t+k ) 1 -σ(C(E p (x t ), E p (x t+k )))<label>(2)</label></formula><p>Unlike <ref type="bibr" target="#b15">[16]</ref>, instead of estimating MI using monte-carlo estimation of the expectation of C * with respect to P (z t p , z t+k p ) we use a variational lower bound of MI, I JS proposed in <ref type="bibr" target="#b13">[14]</ref>. I JS exhibits lower variance in comparison to F-GAN and monte-carlo estimates:</p><formula xml:id="formula_3">L M I = E P (z t p ,z t+k p ) C(z t p , z t+k p ) -E P (z t p )P (z t+k p ) exp(C(z t p , z t+k p ))<label>(3)</label></formula><p>By minimizing this MI estimate, E p is restricted from encoding any content information.</p><p>Reconstruction Loss: We use pixel-wise l 2 loss between decoded frame D(E c (x t ), E p (x t )) and the ground truth frame x t .</p><formula xml:id="formula_4">L recon = E P (x t ) D(E c (x t ), E p (x t )) -x t 2 2 (4)</formula><p>Training Methodology: The overall training objective for E c , E p and D is as follows:</p><formula xml:id="formula_5">min Ec,Ep,D L recon + αL sim + βL M I<label>(5)</label></formula><p>Training object for the critic C is given by: </p><formula xml:id="formula_6">max C L C<label>(6</label></formula><formula xml:id="formula_7">z t-1 p acquired from E p if t -1 ∈ [1, C], else zt-1 p is ẑt-1</formula><p>p , the pose representation predicted by L for the frame t -1. </p><formula xml:id="formula_8">ẑt p = L(z C c , zt-1 p ) where zt p = E p (x t ) t &lt; C + 1 L(z C c , zt-1 p ) t ≥ C + 1<label>(7</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metric</head><p>A popular method to evaluate disentanglement is latent traversal where change in image reconstruction is observed against change in one dimension of the latent space by holding other dimensions constant. This evaluation method is effective in finding methods that are unable to disentangle the generative factors of data but does not provide any quantitative measure of the effectiveness of disentanglement. Previous methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref> propose various metrics to quantitatively evaluate the effectiveness of disentanglement for datasets with known generative factors. Methods in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref> trained a classifier to predict ground truth factors from latent variables and use the classification accuracy as an indicator of effective disentanglement. As pointed out by <ref type="bibr" target="#b14">[15]</ref>, these methods fail to detect cases where latent representations and true factors are not axis-aligned. These methods also depend extensively on weight initialisation and training hyperparameters. H. Kim et al. <ref type="bibr" target="#b15">[16]</ref> used a majority vote classifier to avoid the dependency on hyperparameters and also deal with the axially unaligned case. However, these classification based evaluation techniques generally need further improvements.</p><p>T. Q. Chen et al. <ref type="bibr" target="#b14">[15]</ref> proposed a generic metric Mutual Information Gap metric (MIG). In this formulation, mutual information is calculated between each pair of dimensions of the true factors and learned representations. MIG can be used in scenarios where mutual information can be calculated (i.e where factors of data generation are known a priori). The generic MIG metric is modified to evaluate the effectiveness of disentanglement for video prediction methods. In our adaptation of the MIG metric for video prediction, mutual information is calculated between generative factors and the learned pose, content representations, instead of independent dimensions. We use this adopted version of MIG metric to quantitatively evaluate the effectiveness of disentanglement between the learned pose and content latent representations. Any subsequent reference to the mutual information gap (MIG) metric refers to the modified version described below:</p><formula xml:id="formula_9">M IG = 0.5 H(f c ) I(f c , z c ) -I(f c , z p ) + 0.5 H(f p ) I(f p , z p ) -I(f p , z c )<label>(8)</label></formula><p>In the above formulation of MIG, H(.) refers the entropy of a random variable and I(., .) denotes mutual information between two random variables. Any model that fails to learn proper pose/content disentanglement of video representation will exhibit low score in the proposed MIG metric. The first term will have small value when pose latent variables partially encode content descriptions. Similarly, if content latent variable encodes pose descriptions, the second additive term will have small value. In either case of improper pose/content disentanglement, the MIG score will exhibit low score. In our experiments, true generative factors f c and f p are discrete random variables whereas learned representations z c and z p are continuous random variable, so we use <ref type="bibr" target="#b35">[36]</ref> to estimate the mutual information between them, while <ref type="bibr" target="#b39">[40]</ref> is used to calculate entropy of the discrete generative factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our MIPAE framework quantitatively and qualitatively on two synthetic datasets, moving MNIST, Dspites <ref type="bibr" target="#b40">[41]</ref>, and one real world moving MPI3D-Real <ref type="bibr" target="#b41">[42]</ref> dataset. We use the LPIPS distance <ref type="bibr" target="#b42">[43]</ref> measure between predicted and ground truth frames to substantiate the visual superiority of predicted frames by our method on all three datasets. Previous works have evaluated the fidelity of their frame predictions by using PSNR and SSIM scores. PSNR and SSIM metric based evaluations show poor correspondence with the visual quality of the predicted frames <ref type="bibr" target="#b22">[23]</ref>. However, for the purpose of completeness, PSNR and SSIM scores have also been calculated for comparative evaluation f0of MIPAE. We provide quantitative comparison on the effectiveness of disentanglement achieved by MIPAE with DRNET by using the proposed M IG score on datasets with known generative factors namely, DSprites and MPI3D-Real.</p><p>Code for MIPAE and the experiments are available at <ref type="url" target="https://cvit.iiit.ac.in/projects/mutualInfo/">https: //cvit.iiit.ac.in/projects/mutualInfo/</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>For moving MNIST and DSprites datasets, E c , and E p all use DCGAN <ref type="bibr" target="#b43">[44]</ref> architecture with z c = 128 and z t p = 5. D is the mirrored version of the encoder where sub-sampling convolutional layers are replaced with deconvolutional layers.</p><p>For moving MPI3D-Real, E p is ResNet-18 <ref type="bibr" target="#b44">[45]</ref> architecture and E c and D are VGG-16 <ref type="bibr" target="#b45">[46]</ref> architecture with z c = 128 and z t p = 10. Decoder D is the mirrored version of the content encoder E c . In the decoder network, spatial upsampling layers are used instead of pooling layers. We also provide skip-connection from content encoder to decoder in U-Net <ref type="bibr" target="#b46">[47]</ref> style architecture.</p><p>In all experiments, the critic C is a multi-layer perceptron with two hidden layers of 512 units each and RELU activation function. We use Adam optimizer <ref type="bibr" target="#b47">[48]</ref> with learning rate 0.002 and β 1 = 0.5. We choose α = 1 and β = 0.0001 for our training objective as described in equation 5.</p><p>Recurrent pose prediction network L is a two layer LSTM network with 256 cells each, with linear input and output embedding layers. The proposed MIPAE is trained to observe 5 context frames to predict 10 future frames. For fair comparison, both MIPAE and DRNET are trained for the same number of video frames and hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Moving MNIST</head><p>Moving MNIST dataset consists of two MNIST digits bouncing independently in a 64x64 image. MNIST digits move with constant velocity and undergo mirror reflection upon collision with frame boundaries. This dataset has been widely used by previous works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b25">[26]</ref> to substantiate their disentanglement claims. Fig. <ref type="figure" target="#fig_5">3</ref>(a) and Fig. <ref type="figure" target="#fig_5">3(b</ref>) qualitatively demonstrate the pose/content disentanglement of DRNET and our model respectively. In these figures new frames in the grid are generated by taking position latent variables z 1:C+T p from the sequence highlighted in green and content latent variables z c from the images highlighted in red. In our predictions, digits in the generated sequences imitate the pose of digits in the source sequence (highlighted in green) irrespective of the input content. This indicates that that our pose representations are truly content agnostic. As mentioned in <ref type="bibr" target="#b10">[11]</ref>, DRNET requires additional information in the form of colored digits to learn disentangled representations. Fig. <ref type="figure" target="#fig_5">3</ref> shows that DRNET produces blurry results when trained without color, in contrast to our method which produces sharp digits even without color information. This is indicative of the fact that our model attains a better pose/content disentanglement even without additional color coding of the MNIST digits. Fig. <ref type="figure" target="#fig_5">3(c</ref>) shows frame prediction by our model and DRNET on two sequences. Our produces predictions closer to ground truth for longer horizons, as depicted in the first predicted sequence, indicating that higher pose/content disentanglement helps in sustained long range prediction. This can be verified quantitatively in Fig. <ref type="figure" target="#fig_7">5</ref>, that frames predicted by our model has lower LPIPS distance with ground truth frames over longer horizon. Further, in Fig. <ref type="figure" target="#fig_5">3(c</ref>) it is noteworthy that our model is able to generate distinct digits in video sequences containing multiple instances of the same digit, whereas DRNET is unable to keep track and confuses between these mulitple instances due to lack of colour information in the content representaion of MNIST digits. We are unable to evaluate the effectiveness of disentanglement using the proposed M IG metric due to partial knowledge of the generative factors for MNIST videos, specifically, the true content generative factors f c of MNIST digits are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Moving DSprites</head><p>DSprites <ref type="bibr" target="#b40">[41]</ref> is a procedurally generated dataset with known generative factors. The dataset contains 3 different shapes, at 6 scales and 40 rotational orientations. Video sequences form this dataset are generated in a controlled manner by moving the shapes with constant velocity which undergo mirror reflection upon collision with frame boundaries. Fig. <ref type="figure" target="#fig_6">4</ref>(a) and Fig. <ref type="figure" target="#fig_6">4</ref>(b) shows qualitative disentanglement results of DRNET and our model respectively. DRNET is unable to produce accurate reconstruction of the shapes, where as our method produces sharp and accurate reconstruction of different objects in the frames. This shows that our method is able to capture better content representations z c due to effective pose/content disentanglement. Our disentanglement claim is validated by the proposed MIG metric as the true generative factors for this dataset are known apriori. MIG metric score of our method and DRNET can be found in Tab.I along with the estimated MI between generative factors and learned representations. Our method has a higher MIG score as compared to DRNET indicating better pose/content disentanglement. This finding is further supported by visual comparison of generated future frames by both methods, as depicted in Fig. <ref type="figure" target="#fig_6">4</ref>(c), it can be seen that DRNET is unable to sustain frame prediction over longer horizons. MIPAE outperforms DRNET on PSNR, SSIM and LPIPS metrics, shown in Fig. <ref type="figure" target="#fig_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Moving MPI3D-Real</head><p>MPI3D-Real <ref type="bibr" target="#b41">[42]</ref> is a real world dataset with known generative factors. It contains images of different objects mounted on a rotating robotic arm at different angular positions. Video sequences are generated by rotating the robotic arm with constant angular velocity which undergoes mirror reflection upon collision with ground. Fig. <ref type="figure" target="#fig_8">6</ref>(a) and Fig. <ref type="figure" target="#fig_8">6</ref>(b) show the qualitative disentanglement results of DRNET and our method respectively. It can be seen that DRNET is unable to reconstruct accurate object shape, specifically in the third and fourth objects from the top where it reconstructs a cube as a cylinder, where as our method is able to produce sharp reconstruction of the cube. The quantitative comparison of future frames predicted can be found in Fig. <ref type="figure" target="#fig_7">5</ref>. M IG scores in Tab.I also indicate that our method has attains better disentanglement than DRNET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this work, we propose MIPAE framework, that reduces the task of predicting high dimensional future frames by disentangling video representations into content and low dimensional pose latent factors which are easier to predict. This pose/content factorisation is achieved by penalising mutual information between pose latent variables across time. We also adopt Mutual Information Gap (MIG) metric to quantitatively compare the effectiveness of disentanglement of the proposed method with DRNET to conclusively demonstrate that our method learns substantially better disentangled latent representations, which in turn leads to visually sharp and realistic frame prediction. These improvement over DRNET are achieved by replacing their adversarial loss with our mutual information loss without making any significant training procedure or architectural changes. The simplicity of this mutual information loss based disentanglement approach lends itself to easy integration with other video prediction models. In future we would like to extend our approach to VAE-based stochastic video prediction models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. pose/content disentanglement by MIPAE framework: Position latent variables form the target sequence(top) are combined with content representation of object from the second sequence. The object in the generated sequence (bottom) follows the pose description of the target sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left: Shows the training procedure for content encoder Ec, Ep and D along with various training objectives. To calculate MI, pose latent variables z t p,i , z t+k p,i and z t+k p,j are taken from sampled videos x 1:C+T i</figDesc><graphic coords="3,66.31,50.54,462.66,159.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>samples of the generative factors -content f c and pose f 1:C+T p are drawn from their true underlying joint distribution P (f c , f 1:C+T p ); (2) after having obtained these factors, video frames are generated from the true conditional distribution P (x 1:C+T |f c , f 1:C+T p ). Based on these simplifying assumptions and two step video generative process, the latent space representation of video sequences are factorised into two factors, time independent content, z c , and time dependent pose, z 1:C+T p . This factorisation simplifies the task of making predictions in high dimensional pixel-space to predicting low dimensional pose representation ẑC+1:C+T p of the sequence. Future frames xC+1:C+T are reconstructed from the predicted pose ẑC+1:C+T p and content z c representations. In the proposed framework, the time independent content factor z c is acquired by training a content encoder E c such that z c is acquired from a single frame x C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>) 1 p 1 p</head><label>11</label><figDesc>LSTM training procedure: The LSTM L is trained separately after training the main network, E c , E p and D. To predict a future frame xt , first, the LSTM L predicts ẑt p from previous frame's pose ztand content representation z C c of the last known frame x C . Where ztis the pose representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>)The training objective for L is to minimize the l 2 loss between predicted poses, ẑ2:C+T p , and poses inferred from ground truth frames, z 2:C+T p . Decoder D is used to generate the future frame xt from the content z c and the predicted pose representation ẑt p of the future frame, such that xt = D(z C c , ẑt p ). Note that content representation z C c is fixed from the last known frame x C , while pose representations are predicted in a recurrent manner. Fig. 2 (Right) depicts the future frame generation process. Model architecture for E p , E c , C, L and D are given in Section IV-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 .</head><label>3</label><figDesc>Qualitative comparison on moving MNIST dataset. Figures (a) and (b) demonstrate pose-content disentanglement in DRNET and our model respectively. Each image in the grid is generated by taking pose latent variable from sequence highlighted in green and content latent variable from images highlighted in red. It can be seen that our model generates sharp frames in contrast to blurry predictions by DRNET in frames involving complex interactions between MNIST digits due to better content/pose disentanglement. Figure (c) show future frame predictions by our model and DRNET on two sequences. It is visually evident that our model produces future frames closer to ground truth frames over longer horizon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 .</head><label>4</label><figDesc>Qualitative comparison of disentanglement on moving DSprites. Figures (a) and (b) demonstrate pose-content disentanglement in DRNET and our model respectively. Each image in the grid is generated by taking pose latent variable from sequence highlighted in green and content latent variable from images highlighted in red. It can be seen that DRNET is unable to produce accurate shape of different objects , where as our model produces sharp reconstructions capturing true object shapes due to proper pose/content factorisation. Figure (c) shows future frames generated by our method and DRNET on moving DSprites dataset. It can be seen that in comparison to DRNET, MIPAE generates coherent and stable long range predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Quantitative comparison on SM-MNIST, Dsprites and MPI3D-Real datasets of future frame prediction over long range. Left graph shows the LPIPS distance (lower is better) between ground truth frames and predicted frames. Middle and Right graphs show the PSNR and SSIM metric (higher is better) between ground truth and predicted frames.</figDesc><graphic coords="6,39.99,252.03,189.09,141.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6 .</head><label>6</label><figDesc>Qualitative comparison of disentanglement on MPI 3D Real. Figures (a) and (b) demonstrate pose-content disentanglement in DRNET and our model respectively. Each image in the grid is generated by taking pose latent variable from sequence highlighted in green and content latent variable from images highlighted in red. It can be seen that DRNET reconstructs cube as cylinder (the magnified part) where as our method reconstructs correctly.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prediction, cognition and the brain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bubic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Von Cramon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schubotz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2010.00025</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fnhum.2010.00025" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Anomaly det0ection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep visual foresight for planning robot motion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2786" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual robot task planning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Barnoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Katyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8832" to="8838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1609.02612</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03319</idno>
		<title level="m">Data-efficient mutual information neural estimator</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/poole19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ser. Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05983</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stochastic video generation wi?th a learned prior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07687</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11252</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1020" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08033</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02991</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representation using adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5040" to="5048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1878" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Independent component analysis: principles and practice</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Everson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mutual information between discrete and continuous data sets</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time. iv</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Entropy estimates from insufficient samplings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno>arXiv preprint physics/0307138</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ð</forename><surname>Miladinović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Volchkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Akpo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03292</idno>
		<title level="m">On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
