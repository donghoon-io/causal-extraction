<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised COVID-19 CT image segmentation using deep generative models</title>
				<funder ref="#_sPrs5fc">
					<orgName type="full">Asia Pacific Bioinformatics Conference</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-28">28 April 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Judah</forename><surname>Zammit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<postCode>R3T 2N2</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daryl</forename><forename type="middle">L X</forename><surname>Fung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<postCode>R3T 2N2</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<postCode>R3T 2N2</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biochemistry and Medical Genetics</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<addrLine>Room 308 -Basic Medical Sciences Building 745 Bannatyne Avenue</addrLine>
									<postCode>R3E 0J3</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai-Sang</forename><surname>Carson</surname></persName>
						</author>
						<author>
							<persName><surname>Leung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<postCode>R3T 2N2</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pingzhao</forename><surname>Hu</surname></persName>
							<email>pingzhao.hu@umanitoba.ca</email>
							<idno type="ORCID">0000-0002-9546-2245</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<postCode>R3T 2N2</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biochemistry and Medical Genetics</orgName>
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<addrLine>Room 308 -Basic Medical Sciences Building 745 Bannatyne Avenue</addrLine>
									<postCode>R3E 0J3</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CancerCare Manitoba Research Institute</orgName>
								<address>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised COVID-19 CT image segmentation using deep generative models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-28">28 April 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1186/s12859-022-04878-6</idno>
					<note type="submission">Received: 3 August 2022 Accepted: 3 August 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised learning</term>
					<term>Convolutional network</term>
					<term>Image segmentation</term>
					<term>COVID-19</term>
					<term>Computed tomography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: A recurring problem in image segmentation is a lack of labelled data. This problem is especially acute in the segmentation of lung computed tomography (CT) of patients with Coronavirus Disease 2019 . The reason for this is simple: the disease has not been prevalent long enough to generate a great number of labels. Semi-supervised learning promises a way to learn from data that is unlabelled and has seen tremendous advancements in recent years. However, due to the complexity of its label space, those advancements cannot be applied to image segmentation. That being said, it is this same complexity that makes it extremely expensive to obtain pixellevel labels, making semi-supervised learning all the more appealing. This study seeks to bridge this gap by proposing a novel model that utilizes the image segmentation abilities of deep convolution networks and the semi-supervised learning abilities of generative models for chest CT images of patients with the COVID-19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We propose a novel generative model called the shared variational autoencoder (SVAE). The SVAE utilizes a five-layer deep hierarchy of latent variables and deep convolutional mappings between them, resulting in a generative model that is well suited for lung CT images. Then, we add a novel component to the final layer of the SVAE which forces the model to reconstruct the input image using a segmentation that must match the ground truth segmentation whenever it is present. We name this final model StitchNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>We compare StitchNet to other image segmentation models on a high-quality dataset of CT images from COVID-19 patients. We show that our model has comparable performance to the other segmentation models. We also explore the potential limitations and advantages in our proposed algorithm and propose some potential future research directions for this challenging issue.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Modern deep learning based image segmentation techniques tend to require vast amounts of pixel-level labels to be effective. However, to obtain these labels, it is necessary to have someone sit down and categorize every pixel in an image. This requires a massive amount of human effort. In the case of biomedical images, this is made worse by the fact that it is often necessary to have a panel of experts do the labelling. Therefore, any technique that has the potential to reduce the number of labelled images needed has immense value.</p><p>This issue can be seen in the diagnosis and prognosis of patients suspected to have the Coronavirus Disease 2019 (COVID-19), using computed tomography (CT) scans of their lungs. A pixel-wise segmentation of these scans, identifying healthy tissue as well as parts of the lungs affected by either common pneumonia or novel coronavirus pneumonia, can be a powerful tool for diagnosis as well as for identifying how much risk the patient is in, or will be in. Obtaining these segmentations, however, is immensely time-consuming for medical professionals to do by hand. In response to this, there has been work <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> in using deep learning models for image segmentation to automate this process. Despite there being massive datasets of CT images, these models can only be trained on CT images that have been hand labelled by skilled radiologists, severely limiting the amount of usable data.</p><p>Semi-supervised learning has the potential to alleviate this issue. A semi-supervised model has the ability to learn from both unlabelled and labelled images simultaneously, drastically reducing the number of labelled images needed to achieve satisfactory performance. For this reason, there has been a surge of research into semi-supervised learning in recent years. We will discuss some notable previous work in image segmentation, starting with several fully-supervised models and following with several semi-supervised models.</p><p>U-Net <ref type="bibr" target="#b3">[4]</ref> is a deep learning based image segmentation model that has seen great success on medical imagery tasks. It utilizes an encoder-decoder style architecture with skip connections between the encoder and the decoder. SegNet <ref type="bibr" target="#b4">[5]</ref> has a similar encoderdecoder style architecture, however instead of skip connections it uses max unpooling layers in the decoder. The MobileNetV2 <ref type="bibr" target="#b5">[6]</ref>, an image classification network, can be used as the U-Net's encoder. When compared to much larger encoder networks, the Mobile-NetV2 achieves only slightly worse performance while being much faster. Zhang et al. <ref type="bibr" target="#b0">[1]</ref> have used several deep learning based, supervised segmentation models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> to predict a segmentation for a CT image of a patient's lungs. Fan et al. <ref type="bibr" target="#b2">[3]</ref> and Chen et al. <ref type="bibr" target="#b7">[8]</ref> both propose novel supervised segmentation models that have been handcrafted to perform well on chest CT images. Though impressive, these models are still limited by the number of CT images with pixel-level labels.</p><p>Moving away from fully supervised models, there is a plethora of papers proposing deep learning models that use image-level labels as a supervisory signal for the task of image segmentation. They do not utilize completely unlabelled images. This task is sometimes referred to as pure or true semi-supervision, and there are precious few published papers that tackle it <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>.</p><p>The above purely semi-supervised models tend to tackle the problem using some form of adversarial training, self-training, clustering or multi-view training. Many papers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> use an adversarially trained discriminator deep convolutional network to ensure that the prediction of some segmentation model is realistic. This scheme allows them to train their network on unlabelled photos by leveraging the fact that, even if you do not know the ground truth, it should at least belong to the same distribution as the ground truth for the labelled images. The main drawback to this technique is that it can be very difficult to get a model with an adversarial component to converge to a solution.</p><p>Other papers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> use the fact that images-labelled or unlabelled-that have been determined to be similar by some deep learning-based, unsupervised clustering algorithm should also be close to each other in various latent and feature spaces. These techniques are dependent on how you define "close" which can be quite difficult for data that is as high dimensional as images, causing the performance of these models to be underwhelming.</p><p>Pseudo-labelling <ref type="bibr" target="#b19">[20]</ref> is a commonly used semi-supervised learning technique where a fully supervised deep network is trained and then used to make predictions on some unlabelled data. The network is then retrained using the model's most confident predictions as labels. However, if this prediction is of low quality, then this scheme will continuously reinforce this bad behaviour to disastrous effect. As a result, pseudolabelling is typically considered the least effective, but simplest, semi-supervised technique. There are several papers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> that use this general scheme with some significant modifications.</p><p>As with this paper, many papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> seek to utilize unlabelled CT images from COVID-19 patients. Shan et al. <ref type="bibr" target="#b1">[2]</ref> use an intriguing human-in-the-loop strategy. This strategy entails training a deep learning-based, segmentation network on a small dataset of pixel-wise labelled data, then using this network to make prediction on a large unlabelled dataset. These predictions are then refined by a skilled radiologist and included in the pixel-level labelled dataset. The network is retrained, and this process repeats until satisfactory performance is achieved. Though the labelling effort is significantly reduced, this technique still requires some manual labelling effort and many research groups will simply not have access to a radiologist.</p><p>Fan et al. <ref type="bibr" target="#b2">[3]</ref> use pseudo-labelling in its most rudimentary form. Despite this, they achieved a sizable increase in segmentation performance compared to their fully supervised baseline. This makes it quite motivating to employ a more sophisticated semisupervised technique, as pseudo-labelling is far from capable of making full use of these unlabelled images. Fung et al. <ref type="bibr" target="#b20">[21]</ref> proposed a model that does just this. They add a selfsupervised pre-training step to Fan et al. 's InfNet model. During this step, the CT images are obscured with a black rectangle and the model is trained to reconstruct the full CT image. Though this method was able to improve on the InfNet, it is trained in two separate steps and a semi-supervised technique that can be trained end-to-end may improve the performance even further.</p><p>Deep generative models offer an elegant framework for semi-supervised models. In essence, they treat the image and label as two random variables in a graphical model and seek to model both using recent advances in variational inference. Some notable examples are the M2 variational autoencoder <ref type="bibr" target="#b21">[22]</ref> and the auxiliary deep generative model <ref type="bibr" target="#b22">[23]</ref>. Unfortunately, the vast majority of this research has been in the domain of image classification, where the label is simply a single category for each image. The assumption can be made that each of these categories are equally likely to occur. Even though this assumption is very close to the reality, it still allows for easy to compute, closed-form calculations. Modern deep generative-based, semisupervised techniques rely heavily on this fact.</p><p>A similar assumption cannot be made in the case of image segmentation. This is for several critical reasons. First, due to the fact that each pixel has a label, the number of unique segmentations is exponentially larger than the number of unique image-level labels. Furthermore, very few of these unique segmentations are realistic. For example, a set of pixels that have been give the dog label but are in the shape of a human is not a realistic segmentation. This is important because it completely removes our ability to assume that each unique segmentation is equally likely to occur. Finally, the label for each pixel is heavily dependent on the labels of the other pixels in the image, removing the possibility of making any independence assumptions. For these reasons, modern semi-supervised techniques tend to fall flat when used for image segmentation.</p><p>Though problematic, the issues mentioned above are not at all new. The same issues are encountered while trying to find a distribution capable of modelling images. The variational approach <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> handles this by finding a latent representation of the image as well as a deep learning-based, functional mapping between the image and its latent representation. You are then free to make simplifying assumptions about the latent space's distribution without making any assumptions about the images' distribution. In this study, we utilize this approach by finding a latent representation of both the original CT image, and its segmentation. Because we want our model to learn to segment images even when the ground-truth segmentation is not present, we assume that the original CT image is dependent on the segmentation. By doing this, in the absence of the ground-truth segmentation, the model learns to predict a segmentation that is useful for the reconstruction of the original CT image.</p><p>Though the original variational autoencoder (VAE) <ref type="bibr" target="#b23">[24]</ref> could be used for this task, it lacks the expressivity to sufficiently model large datasets. This is particularly true when the dataset is one of images. The ladder variational autoencoder (LVAE) <ref type="bibr" target="#b25">[26]</ref> greatly increases the expressivity of the VAE by introducing a hierarchy of latent variables and a novel way of training such a hierarchy. In this study, we modify the LVAE by sharing several key weights across the inference and generative network. Additionally, we replace all functional mappings in the LVAE with deep convolutional networks that have been handcrafted to work well on CT images. We name the resultant model the shared variational autoencoder (SVAE).</p><p>In their original forms, the VAE, LVAE and SVAE are designed to take an image as input and reconstruct that image as its output. We modify the SVAE to output both a segmentation mask and four CT images, one for each of the segmentation labels. Then, we reconstruct the original CT image by stitching together the four CT images based on the segmentation. We name the resultant model StitchNet. In summary, we develop a novel deep generative model called the SVAE. Then, using the SVAE, we create a semi-supervised model called StitchNet and test it on a high-quality dataset of CT images from COVID-19 patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>For the evaluation of StitchNet, the Zhang et al. 's <ref type="bibr" target="#b0">[1]</ref> China Consortium of Chest CT Image Investigation (CC-CCII), with several modifications, was used. CC-CCII contains CT images from 2,778 patients, totalling 444,034 images in total. Eighty-five percent of the patients were from the Chinese cities of Yichang, Hefei or Guangzhou and the remainder were from an international cohort. The patients either had common pneumonia (CP), novel-coronavirus pneumonia (NCP), or were part of the control group. In our dataset, we excluded the patients with CP.</p><p>CC-CCII contained 750 segmentation masks, which correspond to 150 patients with NCP. The segmentation was completed by five senior radiologists with over 25 years of experience. They segmented three labels: health lung field, ground-glass opacity and consolidation.</p><p>We will now discuss the data pre-processing and cleaning procedure we employed. We segmented the lung field in each CT image using the U-Net semantic segmentation model. The opening and closing morphological transformations were used for noise reduction. The images were then cropped to only include lung field. The result is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Before being used in our models, all images are resized to a resolution of 352⨯352 and the pixels values are scaled to be between zero and one. We randomly separate 60% of the labelled data into the training set, 20% into the testing set and 20% validation set. We do this by patient, not by image, so that all of a single patient's CT images will be in exactly one of the three sets, thus avoiding data leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>For each image, we employ the following four evaluation metrics: the Intersectionover-Union, F1-Score, Recall and Precision.</p><p>(1) The Intersection-over-Union (IoU): The Intersection-over-Union was used to measure the overlap between the ground-truth infected region (T) and the predicted infected region (P) in a way that controls for the size of the infected region. (2) The F1-Score (F1): The F1-Score, also called the Dice Coefficient, was used to measure the overlap between the ground-truth infected region (T) and the predicted infected region (P).</p><p>(3) Recall (Rec.): The Recall, also called Sensitivity or True Positive Rate, was used to measure what proportion of the ground-truth infected region (T) was present in the predicted infected region (P).</p><p>(4) Precision (Prec.): The Precision was used to measure what proportion of the predicted infected region (P) was present in the ground-truth infected region (T). These four metrics are defined in Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><p>where | | is the operator that calculates the number of pixels in the given region, ∩ is the intersection operator, and ∪ is the union operator.</p><p>We calculate the above metrics for each CT image and average the results. The Mean and Standard Deviation (STD) are defined as follows:</p><p>Let M((x, y)) be the value of the relevant evaluation metric calculated for the data point (x, y). Then let Metric = {M((x i , y i ))} (x i ,y i ) ∈ D Val , where D Val is the validation dataset.</p><p>(1) Mean: Then the Mean is simply</p><formula xml:id="formula_0">Metric M((x i ,y i ))</formula><p>|Metric| .</p><p>(2) STD: The STD is</p><formula xml:id="formula_1">1 |Metric| Metric (M((x i , y i )) -µ) 2</formula><p>, where µ is the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparison</head><p>We compared StitchNet to SegNet and to U-Net with a MobileNetV2 encoder. The hyperparameters used to train StitchNet can be found in Additional file 1: Table <ref type="table" target="#tab_1">S1</ref>. The results on the test set are shown in Table <ref type="table" target="#tab_1">1</ref> with some example prediction shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The results on the validation and training set can be found in Additional file 1: Tables <ref type="table" target="#tab_2">S2</ref> and<ref type="table">S3</ref>. Although performance of StitchNet and U-Net are comparable when predicting the ground glass opacity label, StitchNet's precision is higher whereas U-Net's recall is higher. This seems to indicate that StitchNet makes more conservative predictions than U-Net. The SegNet fails to predict any lesions, predicting only the background class. This is likely due to the fact that its backbone is based off the outdated VGG16 network <ref type="bibr" target="#b26">[27]</ref>, whereas StitchNet and U-Net's backbone uses the more sophisticated MobileNetV2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The (1)</p><formula xml:id="formula_2">IoU = |T ∩ P| |T ∪ P| , F1 = 2 • |T ∩ P| |T | + |P| , Rec. = |T ∩ P| |T | , Prec. = |T ∩ P| |P| ,</formula><p>Based on these two observations, when trained on both labelled and unlabelled data, StitchNet should learn to predict styles that are associated with lesions, for both the labelled data and the unlabelled data. StitchNet achieves this on the labelled data, however, on the unlabelled, all the styles are identical, and the segmentations are the exact same for every image. This seems to indicate that the fundamental idea is sound, but that further work needs to be done before StitchNet can outperform the supervised network.</p><p>Furthermore, we note that the standard deviation of StitchNet model is consistently lower than that of U-Net model. This is due to the fact that the model is able to reinforce the predictions it makes on the labelled data by training on the unlabelled data, resulting in a model that performs more consistently on unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In conclusion, we proposed a novel generative model called the shared variational autoencoder (SVAE), making a theoretical contribution to the field of generative modelling by introducing shared weights between the encoder and the decoder. We used this model to propose StitchNet, a model capable of tackling the challenging task of semisupervised CT image segmentation. While the theoretical foundation of StitchNet is sound, further work will be needed before it can make full use of unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared variational autoencoder</head><p>In this section we will introduce the theory, implementation and optimization of the SVAE. Suppose we have a dataset, D = {x (i) } N -1 i=0 , of N images. Assuming that these images are independent and identically distributed (i.i.d). samples from some groundtruth distribution, p(x), we wish to approximate that ground truth distribution. This allows us to sample from our approximation, synthesizing new images. The Ladder Variational Autoencoder (LVAE) <ref type="bibr" target="#b25">[26]</ref> is a recently proposed model that has been shown to be highly effective at modelling such distributions. Here we will briefly summarize their work and discuss some potential issues. The LVAE assumes that the data is generated in a hierarchical sampling process. Specifically, it assumes that, to generate an image, we first take a sample from a unit Gaussian distributed-latent variable, z n . A function is then applied to that sample, outputting the parameters to a diagonal Gaussian distributedlatent variable, z n-1 . This will repeat for n levels, with the last output distribution being a distribution for each pixel in the image (their work used a Bernoulli distribution). This allows them to assume independence between the pixels when conditioned on the latent variables. This model is denoted by p θ .</p><p>The LVAE uses variational inference to learn both the model p θ and an approximate posterior to p θ , q φ . In previous work, q φ will infer the value for z 1 from x and z 2 from z 1 . The LVAE differs from this in that their q φ completes a deterministic down pass, and then each z is inferred from the intermediate layers of this down pass. The dependencies between latent variables are recovered by combining the inferred distributions' parameters for the latent variables with the generative model's predicted distributions' parameters. This is depicted in Fig. <ref type="figure">3a</ref>.</p><p>Though the LVAE is quite interesting, it was not designed to work well on large, complex datasets such as of CT images. In this study, we seek to modify the LVAE so that it will work well on such a dataset. We do this by replacing the mappings between latent variables in the LVAE with deep convolutional layers that have been handcrafted to work well on CT images. Now, to find z 1 given z 2 , we apply a decon- volutional layer to d 2 to get d 1 and then apply many convolutional layers to d 1 to get z 1 . We note that, in both p θ and q φ , we have a mapping between d n and z n . We hypothesize that this mapping serves the exact same purpose in both, and that having both share weights would increase performance. With this final change, we arrive at the shared variational autoencoder (depicted in Fig. <ref type="figure">3b</ref>) Here, we will describe the deep convolutional layers used in the SVAE model. Several building blocks of the model are described in Fig. <ref type="figure">4</ref>. When the number of filters is greater than 64, linear bottleneck convolutions <ref type="bibr" target="#b27">[28]</ref> are used instead of the traditional convolution. Batch Normalization <ref type="bibr" target="#b28">[29]</ref> followed by the ReLU <ref type="bibr" target="#b29">[30]</ref> activation follows every convolutional layer and is suppressed for clarity. SVAE has five layers of latent variables, opposed to the two depicted in Fig. <ref type="figure">3b</ref>. The dimensionality of these latent variables and their deterministic expansion is shown in Table <ref type="table" target="#tab_2">2</ref>. We use the intermediate and output layers of MobileNetV2 <ref type="bibr" target="#b27">[28]</ref> with the image, x, as input to obtain d 1 , d 2 , ...d 5 . The mappings between variables are depicted in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-Image Ground-Truth U-Net SegNet StitchNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StitchNet</head><p>In this section we will introduce the theory, implementation and optimization of the StitchNet. Suppose we have a dataset, D UN = {(x (i) )} N -1 i=0 , of N CT images, where x (i) denotes the i th CT image in the dataset. We will assume that x (i) is a high-dimen- sional vector with entries ranging from zero to one. Suppose that we have a dataset, D LAB = {(x (i) , y (i) )} M-1 i=0 , of M CT images along with their associated segmentation, y (i) . We will assume that y (i) is of the same dimension as x (i) and has entries that  belong to the set {0, 1, 2, 3} . Here, if the n th entry of y (i) is equal to zero, then this indi- cates that the n th entry of x (i) is part of the background of the CT image Furthermore, one, two and three correspond to the healthy tissue, ground-glass opacity and consolidation class, respectively. This is depicted in Fig. <ref type="figure" target="#fig_5">6a</ref>.</p><p>We wish to obtain a model capable of taking a CT image (x) and outputting an accurate segmentation (y). In other words, we wish to approximate the ground-truth p(x|y) conditional distribution. Though not typically phrased in these terms, supervised deep-learning techniques do this by introducing the following approximation to this distribution:</p><p>where CAT is the Categorical distribution and f θ is some complex function. Due to their tremendous success on image data, f θ is typically chosen to be a convolutional neural-network.</p><p>(2) p θ (y|x) = CAT(y|f θ (x)), (a) An illustration of the function mapping di to di-1 These supervised techniques then aim to find the parameters θ that best explains the data we are given. This is done by maximizing the following objective using a numerical approximation algorithm such as gradient descent:</p><formula xml:id="formula_3">d i (h, w, f ) Conv 3x3 (h, w, f 2 ) Light Smooth ( h , w , f 2 ) ( h , w , f 2 ) Conv 3x3 (h, w, f 4 ) Light Smooth (h, w, f 4 ) Conv 3x3 ( h , w , 1 ) Conv 3x3 (1, 1, f 4 ) Light Smooth (h, w, f 4 ) Conv 3x3 ( h , w , 1 )</formula><p>Phrased in this way, the drawback to these supervised techniques is obvious. They can only use the labelled dataset D LAB . To remedy this, instead of approximating p(y|x), we can model joint distribution p(x, y) and derive the conditional distribution p(y|x) from it. This allows us to use both, D LAB and D UN by treating y as a latent variable in the latter case.</p><p>To effectively model p(x, y), we will assume that x and y are dependent on the hierarchy of latent variables from the SVAE, which here we will simply denote as z. Now we will model p(x, y, z). Furthermore, we will assume that each of the data points, (x (i) , y (i) , z (i) ) , were generated in the following way: where p(z) are assumed to follow the distribution from the SVAE and p θ (•) is assumed to be some distribution parameterized by θ (depicted in Fig. <ref type="figure" target="#fig_6">7</ref>).</p><p>To generate x we will first generate four stylistic representations-referred to as b, h, g and c-of x. These stylistic representations of x show you what the image would look like if the entire lung were background, healthy, ground-glass opacity and consolidation, respectively. We then reconstruct x by choosing the pixel from the style associated with the label predicted by y. Examples of these styles are shown in Fig. <ref type="figure" target="#fig_5">6b</ref>.</p><p>We will use this, as well as the following definitions, to define p θ :</p><p>(3) J = D LAB log p θ (y (i) |x (i) ).</p><p>(4) z (i) ∼ p(z), y (i) ∼ p θ (y|z (i) ), x (i) ∼ p θ (x|z (i) , y (i) ), where BETA denotes the Beta distribution and α θ (z) and β θ (z) are some complex func- tions parameterized by θ which outputs the parameters of the beta distribution. Finally, we define p θ as where π θ (z) is some complex function parameterized by θ . We now have a genera- tive model that is well suited to CT image segmentation (depicted in Fig. <ref type="figure" target="#fig_6">7b,</ref><ref type="figure">c</ref>). What remains is outlining an effective means for finding the values of θ that best explains our observed data. Concretely, we wish to solve</p><p>The existence of latent variable, and, by extension, the need to integrate over them, makes this objective completely intractable. We instead optimize a variational lower bound on the log likelihood of p θ . Concretely, we optimize, Though q φ can be any function of the latent variables, this lower bound is exactly equal to the true log likelihood when q φ is equal to p ′ θ s posterior, p θ (z|x, y) . Therefore, q φ has the interpretation of being an approximation to the posterior. When we implement the q φ , we will keep this fact in mind. <ref type="bibr" target="#b4">(5)</ref> p θ ({b, h, g, c}|z) = BETA({b, h, g, c}|α θ (z), β θ (z)), �(y, b, h, g, c) =</p><formula xml:id="formula_4">     b if y = 0, h if y = 1, g if y = 2, c if y = 3,<label>(6)</label></formula><p>p θ (y|z) = CAT(y|π θ (z)), p θ (x|y, b, h, g, c) = BETA(x|�(y, b, h, g, c)), <ref type="bibr" target="#b6">(7)</ref> max θ D UN log p θ (x (i) ) + D LAB log p θ (x (i) , y (i) )</p><p>= D UN log z y p θ (x (i) , y, z)dz + D LAB log z p θ (x (i) , y (i) , z)dz. <ref type="bibr" target="#b7">(8)</ref> log p θ (x (i) ) ≥ E q φ (z,k|x (i) ) log p θ (x (i) ,y (i) ,z) q φ (z|x (i) )</p><p>, log p θ (x (i) ) ≥ E q φ (z|x (i) ) log y p θ (x (i) ,y,z) q φ (z|x (i) )</p><p>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Before (top) and after (bottom) data pre-processing</figDesc><graphic coords="5,127.60,556.10,340.08,151.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Visual comparison of the segmentation results, where the green and blue labels indicate GGO and Consolidation, respectively</figDesc><graphic coords="8,127.59,88.51,340.10,239.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SVAE: Shared weights are depicted with blue arrows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 AFig. 4 A</head><label>34</label><figDesc>Fig. 3 A comparison between the LVAE and our SVAE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref> An illustration of the mappings between the SVAE's variables. We denote the output of each block as (height, width, filters)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Visualization of the data and StitchNet's outputs. For segmentations, ground glass opacity is shown in green, consolidation in blue and healthy tissue in black</figDesc><graphic coords="11,141.83,88.57,316.22,145.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Hierarchical graphical models. Latent, partially observed and observed variables are shown with clear, half-filled and filled, respectively. Arrows and diamond nodes represent functional mappings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>performance of StitchNet is comparable to that of the fully supervised U-Net model on the ground-glass opacity and consolidation lesion. This indicates that StitchNet is learning from the labelled data, but not the unlabelled data. When trained on only the labelled data, StitchNet predicts styles that are clearly associated with the appropriate lesions, effectively allowing you to see what a CT image would look like if it were entirely</figDesc><table /><note><p>filled with the associated lesion. Because of this, StitchNet seems to perform exactly as expected on the labelled data. When trained on only the unlabelled data, StitchNet learns unique and meaningful styles, learning a meaningful clustering of the data. This is exactly what we would expect from training with no labelled data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Quantitative results of ground-glass opacity (GGO), consolidation (CON), background, and the overall average on the test dataset</figDesc><table><row><cell>Lesion</cell><cell>Method</cell><cell>IoU</cell><cell>F1</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>GGO GGO GGO CON CON CON Background Background Background Overall</cell><cell>U-Net SegNet StitchNet U-Net SegNet StitchNet U-Net SegNet StitchNet U-Net</cell><cell>0.391 ± 0.280 0.004 ± 0.027 0.358 ± 0.257 0.404 ± 0.331 0.021 ± 0.113 0.318 ± 0.315 0.983 ± 0.023 0.97 ± 0.044 0.985 ± 0.021 0.593</cell><cell>0.499 ± 0.32 0.007 ± 0.044 0.471 ± 0.303 0.49 ± 0.368 0.027 ± 0.137 0.397 ± 0.361 0.992 ± 0.012 0.984 ± 0.024 0.992 ± 0.011 0.66</cell><cell>0.608 ± 0.358 0.012 ± 0.087 0.517 ± 0.331 0.616 ± 0.378 0.057 ± 0.227 0.539 ± 0.411 0.987 ± 0.02 0.999 ± 0.009 0.992 ± 0.011 0.737</cell><cell>0.47 ± 0.326 0.009 ± 0.071 0.489 ± 0.328 0.485 ± 0.38 0.021 ± 0.114 0.387 ± 0.369 0.996 ± 0.006 0.971 ± 0.043 0.993 ± 0.014 0.65</cell></row><row><cell>Overall</cell><cell>SegNet</cell><cell>0.332</cell><cell>0.339</cell><cell>0.356</cell><cell>0.334</cell></row><row><cell>Overall</cell><cell>StitchNet</cell><cell>0.554</cell><cell>0.62</cell><cell>0.683</cell><cell>0.623</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>The dimensionality of the five latent variables</figDesc><table><row><cell>Level</cell><cell></cell><cell>z</cell><cell></cell><cell></cell><cell>d</cell></row><row><cell>0</cell><cell></cell><cell cols="2">(352,352,1)</cell><cell></cell><cell>NA</cell></row><row><cell>1</cell><cell></cell><cell cols="2">(176,176,1)</cell><cell></cell><cell>(176,176,32)</cell></row><row><cell>2</cell><cell></cell><cell cols="2">(88,88,1)</cell><cell></cell><cell>(88,88,64)</cell></row><row><cell>3</cell><cell></cell><cell cols="2">(44,44,1)</cell><cell></cell><cell>(44,44,128)</cell></row><row><cell>4</cell><cell></cell><cell cols="2">(22,22,1)</cell><cell></cell><cell>(22,22,256)</cell></row><row><cell>5</cell><cell></cell><cell cols="2">(11,11,1)</cell><cell></cell><cell>(11,11,512)</cell></row><row><cell>Level 0 denotes the input image x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>di</cell><cell>(h, w, f )</cell><cell>Conv Transpose 4x4</cell><cell>(2h, 2h, f ) Light Smooth</cell><cell>(2h, 2w, f )</cell><cell>di-1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We can further increase tractability by approximating the calculation of the expectation over q φ . We do this by taking a Monte-Carlo sample from q φ and evaluating the expectation with just this sample. This approximation can be made more precise by taking multiple samples and averaging the expectation, but, for our work, we used only one. With this, we arrive at our final objective, which can be optimized via any gradient descent algorithm.</p></div>
<div><head>Acknowledgements</head><p>We greatly thank <rs type="person">Dr. Guangyu Wang</rs> for creating, and making publicly available, the high quality CC-CCII dataset.</p></div>
<div><head>About this supplement</head><p>This article has been published as part of <rs type="programName">BMC Bioinformatics Volume 23 Supplement 7, 2022 Selected</rs> articles from the 20th <rs type="funder">Asia Pacific Bioinformatics Conference</rs> (<rs type="grantNumber">APBC 2022</rs>): bioinformatics. The full contents of the supplement are available online at <ref type="url" target="https://bmcbi">https:// bmcbi</ref> oinfo rmati cs. biome dcent ral. com/ artic les/ suppl ements/ volume-23-suppl ement-7.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>Not applicable.</p></div>
			</div>


			<listOrg type="funding">
				<org type="funding" xml:id="_sPrs5fc">
					<idno type="grant-number">APBC 2022</idno>
					<orgName type="program" subtype="full">BMC Bioinformatics Volume 23 Supplement 7, 2022 Selected</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The CC-CCII dataset used for our analysis can be found at <ref type="url" target="http://ncov-ai">http:// ncov-ai</ref>. big. ac. cn/ downl oad? lang= en. All code necessary for the implementation of StitchNet and the replication of our results can be found at <ref type="url" target="https://github">https:// github</ref>. com/ Judah Zammit/ stitc hnet.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCP Novel-coronavirus pneumoniaSupplementary Information</head><p>The online version contains supplementary material available at <ref type="url" target="https://doi">https:// doi</ref>. org/ 10. 1186/ s12859-022-04878-6.</p><p>Additional file 1: Table <ref type="table">S1</ref>. The chosen hyperparameters used to train StitchNet. Table <ref type="table">S2</ref>. Quantitative results of ground-glass opacity (GGO), consolidation (CON), background, and the overall average on the validation dataset. Table <ref type="table">S3</ref>. Quantitative results of ground-glass opacity (GGO), consolidation (CON), Background, and the overall average on the training dataset.</p><p>Author contributions JZ: Designed and implemented the algorithm, drafted the manuscript; DLXF and QL: performed data analysis and participated in algorithm design; CK-SL and PH supervised the project and revised the manuscript. All authors read and approved the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Ethics approval and consent to participate Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p><p>E q φ (z|x (i) ) log p θ (x (i) ,y (i) ,z) q φ (z|x (i) )</p><p>≈ q φ (z (i) |x (i) ) log p θ (x (i) ,y (i) ,z (i) )</p><p>q φ (z|x (i) )</p><p>≡ J LAB , E q φ (z|x (i) ) log y p θ (x (i) ,y,z)</p><p>≈ q φ (z (i) |x (i) ) log y p θ (x (i) ,y,z (i) )</p><p>where z (i) ∼ q φ (z|x (i) ).</p><p>• fast, convenient online submission</p><p>• thorough peer review by experienced researchers in your field</p><p>• rapid publication on acceptance</p><p>• support for research data, including large and complex data types</p><p>• gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>At BMC, research is always in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learn more biomedcentral.com/submissions</head><p>Ready to submit your research Ready to submit your research ? Choose BMC and benefit from: ? Choose BMC and benefit from:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clinically applicable AI system for accurate diagnosis, quantitative measurements, and prognosis of COVID-19 pneumonia using computed tomography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyn</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2020.04.045</idno>
		<ptr target="https://doi.org/10.1016/j.cell.2020.04.045" />
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1423" to="143311" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abnormal lung quantification in chest CT images of COVID-19 patients with deep learning and its application to severity prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.14609</idno>
		<ptr target="https://doi.org/10.1002/mp.14609" />
	</analytic>
	<monogr>
		<title level="j">Med Phys</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1633" to="1645" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inf-Net: automatic COVID-19 lung infection segmentation from CT images</title>
		<author>
			<persName><forename type="first">D-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.2996645</idno>
		<ptr target="https://doi.org/10.1109/TMI.2020" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2020">2020. 29966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SegNet: a deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2644615</idno>
		<ptr target="https://doi.org/10.1109/TPAMI" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016">2017. 2016. 26446</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Residual attention u-net for automated multi-class segmentation of COVID-19 chest CT images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05645</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MR brain image segmentation based on unsupervised and semi-supervised fuzzy clustering methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Dmour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><forename type="middle">-</forename><surname>Ani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on digital image computing: techniques and applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Revisiting cyclegan for semi-supervised segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11569</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for network-based cardiac MR image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tarroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data distillation: towards omni-supervised learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep adversarial networks for biomedical image segmentation utilizing unannotated images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredericksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for fully convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1369" to="1379" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107269</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th British machine vision conference (BMVC) 2018, p</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">D-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 workshop: challenges in representation learning (WREPL)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning model for COVID-19 lung CT image segmentation highlighting putative causal relationship among age, underlying disease and COVID-19</title>
		<author>
			<persName><forename type="first">Dlx</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zammit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ck-S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12967-021-02992-2</idno>
		<ptr target="https://doi.org/10.1186/s12967-021-02992-2" />
	</analytic>
	<monogr>
		<title level="j">J Transl Med</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M ;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<meeting><address><addrLine>Red Hook</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International conference on machine learning. proceedings of machine learning research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33rd International conference on machine learning. proceedings of machine learning research<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1445" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International conference on learning representations (ICLR 2014) Conference track proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of machine learning research</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>machine learning research</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1409.1556</idno>
		<ptr target="https://doi.org/10.48550/arxiv.1409.1556" />
	</analytic>
	<monogr>
		<title level="m">rd International conference on learning representations, ICLR 2015-conference track proceedings</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international conference on machine learning. proceedings of machine learning research</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd international conference on machine learning. proceedings of machine learning research</meeting>
		<imprint>
			<publisher>PMLR, Lille</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on international conference on machine learning. ICML&apos;10</title>
		<meeting>the 27th international conference on international conference on machine learning. ICML&apos;10</meeting>
		<imprint>
			<publisher>Madison: Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International conference on learning representations, ICLR 2015-Conference track proceedings</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
