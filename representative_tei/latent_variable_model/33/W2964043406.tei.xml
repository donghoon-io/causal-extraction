<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint selection</title>
				<funder ref="#_ykXGTuD">
					<orgName type="full">Fondation INSA-Lyon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-09-25">25 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Emre</forename><surname>Dogan</surname></persName>
							<email>edogan@gsu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<country>CNRS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 5205</orgName>
								<orgName type="institution" key="instit1">INSA-Lyon</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Comp. Eng</orgName>
								<orgName type="institution">Galatasaray University</orgName>
								<address>
									<addrLine>36 Ciragan Cd</addrLine>
									<postCode>34349</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gonen</forename><surname>Eren</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Comp. Eng</orgName>
								<orgName type="institution">Galatasaray University</orgName>
								<address>
									<addrLine>36 Ciragan Cd</addrLine>
									<postCode>34349</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<country>CNRS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 5205</orgName>
								<orgName type="institution" key="instit1">INSA-Lyon</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Lombardi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<country>CNRS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<country>CNRS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 5205</orgName>
								<orgName type="institution" key="instit1">INSA-Lyon</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<postCode>F-69621</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint selection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1751-8644</idno>
						<imprint>
							<date type="published" when="2017-09-25">25 Sep 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1709.08527v1[cs.CV]</idno>
					<note type="submission">This paper is a preprint of a paper submitted to IET Computer Vision. If accepted, the copy of record will be available at the IET Digital Library. This paper is a preprint of a paper submitted to IET Computer Vision. If accepted, the copy of record will be available at the IET Digital Library.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new method for human pose estimation which leverages information from multiple views to impose a strong prior on articulated pose. The novelty of the method concerns the types of coherence modelled. Consistency is maximised over the different views through different terms modelling classical geometric information (coherence of the resulting poses) as well as appearance information which is modelled as latent variables in the global energy function. Moreover, adequacy of each view is assessed and their contributions are adjusted accordingly. Experiments on the HumanEva and UMPM datasets show that the proposed method significantly decreases the estimation error compared to single-view results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation is a building block in many industrial applications such as human-computer interaction, motion capture systems, etc. Whereas the problem has been almost solved for easy instances, such as cooperative settings in close distance and depth data without occlusions, other realistic configurations still present a significant challenge. In particular, pose estimation from RGB input in non-cooperative settings remains a difficult problem. Methods range from unstructured and purely discriminative approaches in simple tasks on depth data, which allow real-time performance on low-cost hardware, up to complex methods imposing strong priors on pose. The latter are dominant on the more difficult RGB data but also increasingly popular on depth. These priors are often modelled as kinematic trees (as in the proposed method) or, using inverse rendering as geometric parametric models (see section 2 for related works).</p><p>In this paper, we leverage the information from multiple (RGB) views to impose a strong prior on articulated pose, targetting applications such as video surveillance from multiple cameras. Activity recognition in this context is frequently preceded by articulated pose estimation, which -in a non-cooperative environment such as surveillance -can strongly depend on the optimal viewpoint. Multi-view methods can often increase robustness w.r.t. occlusions.</p><p>In the proposed approach, kinematic trees model independent pose priors for each individual viewpoint, and additional terms favour consistency across views. The novelty of our contribution lies in the fact that consistency is not only forced geometrically on the solution, but also in the space of latent variables across views.</p><p>More precisely, a pose is modelled as mixtures of parts, each of which is assigned to a position. As in classical kinematic trees, deformation terms model relative positions of parts w.r.t. neighbours in the tree. In the lines of <ref type="bibr" target="#b0">[1]</ref>, the deformations and the appearance terms depend on latent variables which switch between mixture components. This creates a powerful and expressive model with low-variance mixture components which are able to model precise relationships between appearance and deformations. Intuitively, and as an example, we could imagine relative positions of elbow and forearm to depend on a latent variable, which itself depends on the appearance of the elbow. It is easy to see that a stretched elbow requires a different relative position than a bent elbow.</p><p>In the proposed multi-view setting, positions, as well as latent variables, are modelled for each individual view. A global energy term favours a consistent pose over the complete set of views, including consistency of the latent part type variables which select mixture components. Here the premise is that appearance may certainly differ between viewpoints, but that a given pose is translated into a subset of consistent latent mixture components which can be learned.</p><p>An overview of the proposed method can be seen in Fig. <ref type="figure" target="#fig_0">1</ref> which depicts the iterative nature of the multi-view pose estimation process. Basically, one of the single-view estimations is selected as the support pose and provides additional information to the other view. On each iteration, support and target poses are swapped so that both predictions improve over iterations. The optimisation loop continues until convergence, where a final pose is produced for each view.</p><p>As a summary, our main contributions are the following:</p><p>• We propose a global graphical model over multiple views including multiple constraints: geometrical constraints and constraints over the latent appearance space.</p><p>• We propose an iterative optimization routine.</p><p>• We introduce an adaptive viewpoint selection method, which removes viewpoints if the expected error is too high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human pose estimation from RGB images has received increasing attention, we therefore restrict this section by excluding techniques that exploit depth images and focus on part-based models, generative and discriminative probabilistic models, tracking methods and deep neural networks.</p><p>Pictorial structures (PS) -are a dominant family of models. Based on the original idea in <ref type="bibr" target="#b1">[2]</ref>, they model an object as a combination of parts related to a star-shaped or tree-shaped structure and deformation costs. The problem is formulated as an energy function with terms for appearance similarity plus deformation terms between parts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Although efficient for inference, tree-structured models are known to suffer from the double-counting problem, especially for limb parts. To address this issue loopy constraints are commonly used, but they require diverse approximate inference strategies <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. The relationship between the non-adjacent body parts is discussed in <ref type="bibr" target="#b7">[8]</ref> where a fully-connected model is proposed. Due to midlevel representations conveyed by poselets, unary and binary terms are updated during test time and the model is reduced to a classical PS, which can be solved directly. <ref type="bibr" target="#b8">[9]</ref> proposes a PS-inspired model, with binary random variables for each part, they model the presence and absence for every position, scale and orientation. However, this results in a high number of variables which forces them to approximate inference.</p><p>Flexible Mixtures of Parts (FMP) -have been introduced by <ref type="bibr" target="#b0">[1]</ref>, to tackle the limited expressiveness of tree-shaped models. Instead of orientations of parts, they proposed mixtures, which are obtained by clustering appearance information. A detailed review of the method can be found at section 3. Among extensions, <ref type="bibr" target="#b9">[10]</ref> proposed appearance sharing between independent samples to cluster similar background and foregrounds. <ref type="bibr" target="#b10">[11]</ref> presented a method to estimate 3D pose from a single image where they use FMP and camera parameter estimation, in conjunction with anthropomorphic constraints. <ref type="bibr" target="#b11">[12]</ref> proposed an improvement by aggregation of multiple hypotheses from the same view using kernel density approximation.</p><p>Multi-view settings -allow various methods to emerge, particularly for 3D pose estimation. <ref type="bibr" target="#b12">[13]</ref> proposed a generalised version of PS, which also exploits temporal constraints. They employ graphs spanning multiple frames and inference is performed with nonparametric belief propagation. Among 3D extensions of PS, the burden of the infeasible 3D search space is handled by reducing it with discretisation <ref type="bibr" target="#b13">[14]</ref>, supervoxels <ref type="bibr" target="#b14">[15]</ref>, triangulation of corresponding body parts from different viewpoints <ref type="bibr" target="#b15">[16]</ref> or by using voxel-based annealing particle filtering <ref type="bibr" target="#b16">[17]</ref>. Recently, <ref type="bibr" target="#b17">[18]</ref> proposed a strategy similar to 3D-PS, but with a realistic body model, and inference is carried out with particle-based max-product belief propagation. Inferring 3D pose from multiple 2D poses is also common, with various underlying strategies such as hierarchical shape matching <ref type="bibr" target="#b18">[19]</ref>, random forests <ref type="bibr" target="#b19">[20]</ref> and optical flow <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b21">[22]</ref> introduced a scheme where PS is employed to estimate 2D poses, then incorporates them to obtain a 3D pose with geometrical constraints, colour and shape cues. Although being somewhat analogous to our proposition, <ref type="bibr" target="#b21">[22]</ref> does not consider cases where some viewpoints are more beneficial than others, which we leverage with adaptive viewpoint selection.</p><p>Temporal strategies -are commonly used for pose estimation and articulated tracking from videos. Using spatiotemporal links between the individual parts of consecutive frames seems promising, but intractability issues arise. To this end, <ref type="bibr" target="#b6">[7]</ref> opt for approximation with distance transform <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b5">[6]</ref> reduce the graph by combining symmetrical parts of human body and generating part-based tracklets for temporal consistency. <ref type="bibr" target="#b23">[24]</ref> uses a spatiotemporal And/Or Graph to represent poses where only temporal links exist between parts. Recently <ref type="bibr" target="#b24">[25]</ref> proposed synthesising hypotheses by applying geometrical transformations to initially annotated pose and match next frame with the nearest neighbour search.</p><p>Discriminative approaches -learn a direct mapping from feature space to pose, often by avoiding any explicit body models (although models can be integrated). Silhouettes <ref type="bibr" target="#b25">[26]</ref> and edges <ref type="bibr" target="#b26">[27]</ref> are frequently used as image features in conjunction with learning strategies for probabilistic mapping, such as regression <ref type="bibr" target="#b25">[26]</ref>, Gaussian Processes <ref type="bibr" target="#b27">[28]</ref> and mixtures of Bayesian Experts <ref type="bibr" target="#b4">[5]</ref>. Previous work shows that these approaches are usually computationally efficient and perform well in controlled environments, but they are highly dependent on the training data and therefore tend to generalise poorly in unconstrained settings.</p><p>Deep neural networks -have received remarkable attention recently which inevitably affected the pose estimation challenge. <ref type="bibr" target="#b28">[29]</ref> address the problem by first obtaining 2D pose candidates with PS, then utilising a deep model the determine the final pose. <ref type="bibr" target="#b29">[30]</ref> feeds both local patches and their holistic views into the Convolutional Neural Networks (CNN), while <ref type="bibr" target="#b30">[31]</ref> proposes a new architecture where deep CNNs are used in conjunction with Markov Random Fields. <ref type="bibr" target="#b31">[32]</ref> on the other hand, follow a more direct approach and employ a cascade of Deep Neural Network regressors to handle the pose estimation task. <ref type="bibr" target="#b32">[33]</ref> uses a kinematic tree, where the same deep network learns unary terms as well as data dependent binary terms. Similar to our adaptive viewpoint selection scheme, predicting the estimation error during test time is explored by <ref type="bibr" target="#b33">[34]</ref>; however they employ a CNN to learn iterative corrections that converge towards the final estimation. Part mixtures is adopted in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, where message passing is implemented as additional layers. State-of-theart results on single-view benchmarks are achieved by <ref type="bibr" target="#b36">[37]</ref>, where multiple hourglass layer modules are stacked end-to-end.</p><p>Our method is based on FMP, which allows imposing a strong prior on pose, generalising it to multiple views. Compared to existing multi-view approaches, our method is not restricted to geometric coherence terms. We enforce coherence also in the space of latent variables (the mixture component selectors), which further improves performance. Additionally, we leverage the consistency between views by predicting the fitness of each view during test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Single view pose estimation</head><p>In the lines of <ref type="bibr" target="#b0">[1]</ref>, an articulated pose in a single 2D image is modelled as a flexible mixture of parts (FMP). Related to deformable part models introduced by <ref type="bibr" target="#b37">[38]</ref>, part-based models for articulated pose estimation are classically tree structured. Similarly, our model is a kinematic tree on which a global energy is defined including data attached unary terms and pairwise terms acting as a prior on body pose. The underlying graph is written as G = (V, E), where vertices are parts and edges are defined on adjacency between parts.</p><p>Let p i = (x, y) be the pixel coordinates for part i ∈ {1, . . . , K} in image I. The values of p i are the desired result values over which we would like to optimise. Additional latent variables t i ∈ {1, . . . , T } model a type of this part, which allows to model terms in the energy function for given types, effectively creating a powerful mixture model. In practice, the part types are set to clusters of appearance features during training time. In the single-view version, the energy function corresponds to the one given in <ref type="bibr" target="#b0">[1]</ref>. Defined over a full pose p = {p i }, input image I and latent variables t = {t i }, it is given as follows:</p><formula xml:id="formula_0">S(I, p, t) = i∈V w ti i φ(I, p i ) + ij∈E w ti,tj ij ψ(p i -p j ) + i∈V b ti i + ij∈E b ti,tj ij<label>(1)</label></formula><p>The expression in the first sum corresponds to data attached terms, where φ(I, p i ) are appearance features extracted at p i (HoG, see section 7). Note that the corresponding trained parameters w ti i depend on the latent part type t i .</p><p>The pairwise terms in the next expression model the prior over body pose using a classical second degree deformation ψ(p ip j ) = [dx dx 2 dy dy 2 ] T where dx = x i -x j and dy = y i -y j . They control the relative positions of the parts and act as a "switching" spring model, the switching controlled by the latent part types t i .</p><p>The last two sums define a prior over part types including unary part type biases b ti i and pairwise part type terms b ti,tj ij . Although scale information is not specified in the equations, a pyramid is used in the implementation to address the various sizes of the subjects in the image. Inference in this model (and in our generalisation to multi-view problems) will be addressed in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-view pose estimation</head><p>We generalise the single-view model to multiple views and show that geometrical consistency constraints can be leveraged to improve estimation quality. Without loss of generality let us fix the number of views to two and consider a setup with calibrated cameras. In this case, a global energy function models the pose quality over a pair views A and B, taking as input images I A and I B and estimating pose variables p A and p B while additionally optimising over latent part types t A and t B :</p><formula xml:id="formula_1">S(I A , I B , p A , p B , t A , t B ) = S(I A , p A , t A ) + S(I B , p B , t B ) +α i∈V a i ξ(p A i , p B i ) + β i∈V a i λ(t A i , t B i )<label>(2)</label></formula><p>Here, S is the single pose energy from equation 1. The two additional terms ξ and λ ensure consistency of the pose over the two views, α and β are the hyperparameters that controls the amount of influence of these terms, and a i are binary variables activating or deactivating consistency. All terms and symbols are described in the corresponding sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Geometric constraints</head><p>Assuming temporal synchronisation, images I A and I B show the same articulated pose from two different viewpoints, which can be exploited. In particular, given calibrated cameras, points in a given view correspond to epipolar lines in the second view. The geometric term ξ leverages this as follows:</p><formula xml:id="formula_2">ξ(p A i , p B i ) = -d(p A i , e(A, p B i )) -d(p B i , e(B, p A i ))<label>(3)</label></formula><p>where e(A, p B i ) is the epipolar line in view A of point p i in view B and d is the Euclidean squared distance between a point and a line. Thus, geometric constraints translate as additional energy to particular locations for both views in the global energy function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Appearance constraints</head><p>The geometric constraints above are imposed on the solution (positions p i ). The term λ adds additional constraints on the latent part type variables t i , which further pushes the result to consistent solutions. Recall that the latent variables are clusters in feature space, i.e. they are related to types of appearance. Appearances might, of course, be different over views as a result of the deformation due to viewpoint changes. However, some changes in appearances will likely be due to the viewpoint change, whereas others will not. Intuitively, we can give the example of an open hand in view A, which will certainly have a different appearance in view B; however, the image will not likely be the one of a closed hand.</p><p>We model these constraints in a non-parametric way as a discrete distribution learned from training data, i.e. λ(t A i , t B i ) = p(t A i , t B i ) (see section 5). Figure <ref type="figure" target="#fig_1">2</ref> illustrates this term using three filter examples shown for the learned model of part right shoulder. The λ term is high between (2a) and (2b), but low between (2a) and (2c). Intuitively, (2a) and (2b) look like the same 3D object seen from different angles, whereas (2a) and (2c) do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaptive viewpoint selection</head><p>Geometric and appearance constraints rely on the accuracy of the initial single-view pose estimates. In certain cases, the multi-view scheme can propagate poorly estimated part positions over views, eventually deteriorating the multi-view result. To solve this problem, we would like to estimate beforehand, whether an additional view can contribute, i.e. increase performance, or whether it will deteriorate good estimations from a better view.</p><p>We propose an adaptive viewpoint selection mechanism and introduce a binary indicator vector (over parts) that switches on and off geometric and appearance constraints for each part during inference. If an indicator is switched off for a part, then the support pose does not have an effect on the optimised pose for this part. The binary indicator vector a is given as follows:</p><formula xml:id="formula_3">a i = 0 if σ i (p A , θ) &gt; τ i or σ i (p B , θ) &gt; τ i 1 else (4)</formula><p>where τ i is a threshold obtained from median part errors on the training set and σ i (p A , θ) is a function with parameters θ that estimates the expected error committed by the single-view method for part i, given an initial estimate of the full pose p A . σ is a mapping learned as a deep CNN taking image tiles cropped around the initial (single-view) detection p A as input. Training the network requires to minimise a loss over part estimation errors, i.e. an error over errors, as follows:</p><formula xml:id="formula_4">min θ || σ(p A , θ) -e || 2 (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where e is the vector of ground truth errors obtained for the different parts by the single-view method, and || • || 2 is the L 2 norm which is here taken over a vector holding estimations for individual parts. θ are the parameters of the deep network.</p><p>We argue that such a network is suitable to anticipate whether an individual part is useful for multi-view scheme, by implicitly learning multi-level features from an image tile. For example, selfoccluded parts or other poor conditions would most likely to be associated with high error rates, whereas unobstructed views would yield low errors. Thresholding the output of the network, namely the error estimations σ i , can provide the decision whether the support view has an influence for part i or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>Single-view parameters: Appearance coefficients w ti i , deformation coefficients w ti,tj ij and part type prior coefficients b ti i and b ti,tj ij are learned as in <ref type="bibr" target="#b0">[1]</ref>: we proceed by supervised training with positive and negative samples, where the optimisation of the objective function is formulated as a structural SVM. Part type coefficients are learned w.r.t. their relative positions to their parents by clustering. This mixture of parts approach ensures the diversity of appearances of part types where their appearance is associated with their placement with reference to their parents; for example a left-oriented hand is usually seen on the left side of an elbow, while a upward facing hand is likely to occur above an elbow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency parameters:</head><p>The discrete distribution λ(t A i , t B i ) = p(t A i , t B i ) related to the appearance constraints between views is learned from training data as co-occurrences of part types between the viewpoint combinations. We propose a weakly-supervised training algorithm which supposes annotations of the pose (positions p i ) only, and which does not require ground truth of part types t i . In particular, the single-view problem is solved on the images of two different viewpoints and the resulting poses are checked against the ground truth poses. If the error is small enough, the inferred latent variables t i are used for learning. The distribution p(t A i , t B i ) is thus estimated by histogramming eligible values for t A i and t B i . Fig 2 <ref type="figure">shows</ref> an example of learned filters and their compatibility.</p><p>The hyper-parameters α and β weighting the importance of the consistency prior are learned through cross-validation over a holdout set (see section 7).</p><p>Viewpoint selection parameters: As seen in Section 4.3, σ is a mapping that estimates error of a single-view pose estimation, given an image tile cropped around the bounding box. To determine σ, we use regression of the expected error and train a deep CNN. We use a VGG-16 network <ref type="bibr" target="#b38">[39]</ref> pre-trained on ImageNet and remove all the top fully connected layers and replace them with a single small hidden layer for regression. We finetune the last convolutional block of VGG and learn the weights of the newly added fully connected layers with augmented data (see section 7 for further details).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference</head><p>Inference of the optimal pose pair requires maximising equation ( <ref type="formula" target="#formula_1">2</ref>) over both poses p A i and p B i and over the full set of latent variables t A i and t B i . Tractability depends on the structure of the graph, and on the clique functionals. Whereas the graph G = (V, E) for the single-view problem (the graph underlying equation 1) is a tree, the graph of the multi-view problem contains cycles. This can be seen easily, as it is constructed as a union of two identical trees with additional edges between corresponding nodes, which are due to the consistency terms. Compared to the single-view problem, maximisation cannot be carried out exactly and efficiently with dynamic programming.</p><p>Several strategies are possible to maximise equation ( <ref type="formula" target="#formula_1">2</ref>): approximative message passing (loopy belief propagation) is applicable for instance, which jointly optimises the full set of variables in an approximative way, starting from an initialisation. We instead chose an iterative scheme which calculates the exact solution for a subset of variables keeping the other variables fixed, and then alternates. In particular, as shown in figure <ref type="figure" target="#fig_0">1</ref>, we optimise for a given view while keeping the variables of the other view (the "support view") fixed. Removing an entire view from the optimisation space ensures that the graph over the remaining variables is restricted to a tree, which allows solving the sub-problem efficiently using dynamic programming.</p><p>Let us write kids(i) for the child nodes of part i. The score of a part location p i for a given part type t i is computed as follows:  </p><formula xml:id="formula_6">score i (t A i , t B i , p A i , p B i ) = b t A i i + w ti i • φ(I A , p A i ) + b t B i i + w ti i • φ(I B , p B i ) + a i (αξ(p A i , p B i ) + βλ(t A i , t B i )) + k∈kids(i) m k (t A i , t B i , p A i , p B i )<label>(6)</label></formula><formula xml:id="formula_7">Configuration U-R Arm U-L Arm L-R Arm L-L Arm U-R Leg U-L Leg L-R Leg L-L Leg</formula><p>FMP <ref type="bibr" target="#b0">[1]</ref>  with the message that part i passes to its parent j is defined as:</p><formula xml:id="formula_8">m i (t A j , t B j , p A j , p B j ) = max t A i ,t B i b t A i ,t A j ij + b t B i ,t B j ij + max p A i ,p B i score i (t A i , t B i , p A i , p B i ) + w t A i ,t A j ij • ψ(p A i , p A j ) + w t B i ,t B j ij • ψ(p B i , p B j )<label>(7)</label></formula><p>As mentioned, one of the two sets A and B is kept constant at each time, which simplifies the equations (6, 7) to a single-view form, similar to <ref type="bibr" target="#b0">[1]</ref>. Messages from all children of part i are collected and summed with the bias term and filter response, resulting in the score for that pixel position and mixture pair. As classically done in deformable parts based models, the optimisation can be carried out with dynamic programming and the inner maximisation in equation ( <ref type="formula" target="#formula_8">7</ref>) with min-convolutions (a distance transform, see <ref type="bibr" target="#b22">[23]</ref>). The algorithm is initialised by solving the single-view problem independently for each viewpoint. The pose with the lowest estimated error (see section 4.3) is chosen as initial support pose, the pose of the other viewpoint being optimised in the first iteration. The iterative process is repeated on until convergence or a maximum number of iterations is reached. Optimising each sub-problem is classical, where the message passing scheme iterates from the leaf nodes to the root node. After thresholding to eliminate weak candidates and non-maximum suppression to discard similar ones, backtracking obtains the final pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluated our work on two datasets, HumanEva I <ref type="bibr" target="#b39">[40]</ref> and Utrecht Multi-Person Motion (UMPM) <ref type="bibr" target="#b40">[41]</ref>. Both datasets have been shot using several calibrated cameras. Ground truth joint locations were recorded with a motion capture system, with 20 and 15 joints, respectively.</p><p>For HumanEva set we only use three cameras (C1, C2 and C3), which are placed with 90 degrees offset. Three subjects (S1, S2 and S3) perform following activities: walking, boxing, jogging, gestures and throw-catch. There are three takes for each sequence, used for training, validation and test. Since the creators of HumanEva favour online evaluation, original test set does not contain ground truth joint positions. Following <ref type="bibr" target="#b21">[22]</ref>, we divided the original training set into training and validation sets and used the original validation set for testing. All hyper-parameters have been optimised over our validation set.</p><p>For UMPM set, all available cameras (F, L, R and S) were used. We considered all available sequences with one subject, which includes object interactions such as sitting on a chair, picking up a small object, leaning and lying on a table. The training, validation and test partitions were divided using 60%, 20% and 20% of the all available data, respectively. The HumanEva test set consists of 4493 images per camera, while UMPM test set has 6074 images per camera. The number of distinct images used in the tests sums up to 13479 and 24296, respectively.</p><p>Since our model is trained with 26 parts, we used a linear function to convert our box centres to the 20 joint locations for HumanEva and 15 joints locations for UMPM.</p><p>The data attached terms φ(., .) in this work were based on HoG features from <ref type="bibr" target="#b41">[42]</ref>. Other features are possible, in particular learned deep feature extractors as in <ref type="bibr" target="#b32">[33]</ref> or <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. This does not change the setup, and can be performed with finetuning of a pre-trained model for this case, where the amount of training data is relatively low.</p><p>We evaluate our multi-view approach against the single-view method given in <ref type="bibr" target="#b0">[1]</ref>. We use two poses as input and evaluate on one of these two poses, varying over multiple configurations.</p><p>Parameters of the single-view model (Eq. 1) are learned on all activities of S1 for HumanEva. We took 100 frames with equal time intervals for every activity from three cameras for training, which sums up to 1500 images. The remainder of the data was set as the validation set. For UMPM, nearly 400 consecutive frames for each sequence were used as positive samples. As for the negative samples, This paper is a preprint of a paper submitted to IET Computer Vision. If accepted, the copy of record will be available at the IET Digital Library.</p><p>1.0 0.8 0.6 0.4 0.2 0.0  background images from corresponding datasets were used in addition to the INRIA Person Database <ref type="bibr" target="#b41">[42]</ref>. Hyper-parameters α and β of equation ( <ref type="formula" target="#formula_1">2</ref>) were learned on validation sets.</p><formula xml:id="formula_9">H E A D S H O U L D E R S U -R A R M U -L A R M L -R A R M L -L A R M T O R S O H I P S U -R L E G U -L L E G L -R L E G L -L L E G<label>(</label></formula><formula xml:id="formula_10">H E A D S H O U L D E R S U -R A R M U -L A R M L -R A R M L -L A R M T O R S O H I P S U -R L E G U -L L E G L -R L E G L -L L E G<label>(</label></formula><p>To learn the weights of the error estimating CNN σ i (•), training data sets were augmented with horizontal flip, Gaussian blur and additive noise. As mentioned earlier, we used a finetuned version of VGG-16 <ref type="bibr" target="#b38">[39]</ref> model using pre-trained weights on ImageNet to estimate the part-based error of the single-view pose. We removed the fully connected layers and introduced our top model with a hidden layer of 1024 nodes, an output layer of K nodes and parametric ReLU as non-linearity. First, weights of the complete VGG-16 network were frozen so that they are unaffected by the backpropagation and weights of the top model were roughly learnt with a high learning rate. Then, the top model were initialised with these weights, and the last convolutional blocks (namely the last three conv3-512 layers) were unfrozen for finetuning. We preferred stochastic gradient descent as optimisation algorithm with small learning rate to ensure that the weights of the last convolutional block are marginally updated. To prevent overfit to augmented data sets we applied strong regularisation and also employed Dropout <ref type="bibr" target="#b44">[45]</ref> with a probability of 0.5.</p><p>For each multi-view arrangement, i.e. pair combinations of cameras, two pose estimations are produced. Since each view belongs to several multi-view arrangements, we end up with several pose candidates for the same viewpoint, e.g. we obtain two pose candidate for C1, once from the C1-C2 pair and once from the C3-C1 pair. These candidates are simply averaged and obtained 2D poses are triangulated non-linearly to obtain 3D pose for a single time frame. Following the literature on 3D pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> we use the percentage of correctly detected parts in 3D (PCP3D), which is calculated as</p><formula xml:id="formula_11">ŝn -sn + ên -en 2 ≤ γ ŝn -ên<label>(8)</label></formula><p>where sn and en are the estimated start and end 3D coordinates of the n'th part segment, and ŝn and ên are the ground truth 3D coordinates for the same part segment. By convention we take γ = 0.5 in all our computations, unless specified otherwise.</p><p>Performancesare shown in Table <ref type="table" target="#tab_0">1</ref> as PCP3D scores on train subject S1 only and over all subjects; while table 2 shows PCP3D scores on UMPM test set. We provide three versions: geometric constraints only, geometric and appearance constraints combined, and both constraints with adaptive viewpoint selection. It is clear that in all cases and both data sets, the multi-view scheme significantly improves performance. Depending on the performed action, gains can be significant up to 9.2% in HumanEva and 10.1% in UMPM. The last columns of tables 1 and 2 show that the additional coherence terms decrease the error. Fig. <ref type="figure">4</ref> demonstrates that this error is distributed over all different parts of the body: we improve most on wrists and elbows, which are important joints for gesture and activity recognition, as seen in table <ref type="table">3</ref>. Plots for overall PCP3D curves w.r.t. various thresholds are also given in Fig. <ref type="figure">6</ref>.</p><p>The adaptive viewpoint selectioneffectively prevents erroneous consistency terms for certain parts dynamically, due to poor initial single-view estimations as discussed in Section 4.3, and shown in table <ref type="table" target="#tab_0">1</ref>.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> depicts intermediate poses and epipolar lines throughout the course of algorithm while Fig. <ref type="figure">5</ref> shows several examples from the test set, where faulty poses are corrected with the multi-view approach. Note that limbs are in particular subject to correction by geometrical and appearance based constraints, since they are considerably susceptible to be mistaken for their respective counterpart. It should be also noted that in case of poor initial detections, a faulty part location can be propagated through the constraints and deteriorate a correct part estimation in other views. Performance tables show that our adaptive viewpoint selection scheme successfully prevents this by considerably decreasing the number of deterioration cases. Particularly, Fig. <ref type="figure" target="#fig_6">7</ref> depicts the the amount of improvements and deteriorations w.r.t the baseline, with and without the adaptive viewpoint selection scheme, which efficiently discards the erroneous single-view part detections.</p><p>Comparison to the state of the art -We compare to the original FMP <ref type="bibr" target="#b0">[1]</ref>, to Schick et al.'s voxel carving based 3D PS method <ref type="bibr" target="#b14">[15]</ref> and to pre-trained Stacked Hourglass Networks (SHN) <ref type="bibr" target="#b36">[37]</ref>.</p><p>[15] report 78% PCP3D for HumanEva and for all sequences of S1 and S2 (ours: 83.42%) and they report 75% for UMPM and for all sequences of P1 (ours: 80.04%).</p><p>SHN <ref type="bibr" target="#b36">[37]</ref> requires a cropped input image that is centred around the person with specific scale requirements. Similar to our scheme, 2D poses from different views were triangulated to obtain 3D pose. Table <ref type="table">4</ref> depicts our estimation performance and two versions of <ref type="bibr" target="#b36">[37]</ref>: First one is with unrestricted images, i.e. same input to our method; and second one with pre-processing steps that require the ground truth. Please note that the SHN heavily depends on the preprocessing, and fails if the person is not centred on image. Our method, which does not require such supervision, obtains similar or better performance to SHN with pre-processed input.</p><p>Fast parallel implementation: Our implementation is based on our port of the Matlab/C++code from the single-view method by <ref type="bibr" target="#b0">[1]</ref> to 100% pure C++, where crucial parts have also been ported to GPU processing using NVIDIA's CUDA library. This sped up runtime from 3000ms/frame to 880ms/frame on a computer equipped with a 2.4Ghz Xeon E5-2609 processor and an NVIDIA 780 Ti GPU for the single-view algorithm (for a 172x224 image with 32 levels of down-sampling). The multi-view algorithm is slower as 5.73 iterations are performed in average before the results are stable. We are currently working on additional optimisations of computational complexity using approximative parallel implementations of the distance transform on GPUs.</p><p>Table <ref type="table">4</ref> Comparison of our performance to SHN <ref type="bibr" target="#b36">[37]</ref> on subject 1 of HumanEva dataset, in terms of PCP3D score (%). First reported result is calculated with unrestricted images (i.e. same input for our method) which is dubbed as Standard, while the second one is calculated with cropped input images around the person with a scale requirement, which is dubbed as Preprocessed. (U-L: upper left, U-R: upper right, L-L: lower left, L-R: lower right)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body Part</head><p>Ours  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a novel multi-view method to estimate articulated body pose from RGB images. Experiments show that combining appearance constraints with geometrical constraints and adaptively applying them on individual parts yields better results than the original single-view model. We also show that our algorithm performs more accurately regardless of the view combinations, and it generalises well in a way to handle unseen subjects and activities.</p><p>This paper is a preprint of a paper submitted to IET Computer Vision. If accepted, the copy of record will be available at the IET Digital Library.</p><p>We plan to extend and evaluate our method in settings with three or more viewpoints, which should be straightforward. Generally, a graph modelling the possible interactions could possibly have highorder cliques, where each clique contains nodes corresponding to the possible views. In practice, it is unsure whether high-order interactions should provide more powerful constraints then (sub)-sets of pairwise constraints. A straightforward algorithm should be similar to the one proposed in the paper: optimizations are carried out over a single-view including pairwise terms involving the different (multiple) support views.</p><p>Another improvement would be the extension to a non-calibrated setting, by exploring the self-calibration and epipolar line estimation techniques, which would allow our method to be used in multi-agent robotic systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Overview of the model: a) Initial pose estimation running the single-view model on each view separately. The pose with the highest confidence score is selected as the support pose. b) Joint estimation loop with geometrical and appearance constraints provided by support pose. The newly obtained pose becomes the support pose at the end of each iteration and provides constraints for the other view. c) After convergence, the last two poses are returned as the final results. Best viewed in colour.</figDesc><graphic coords="2,68.03,55.84,459.22,215.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Illustration of the learned multi-view consistency term over latent appearance (part types). The picture shows HoG filters of the same part (the shoulder). Filter pair (a -b) has been learned to be highly compatible (eventually across different viewpoints), whereas compatibility of pair (a -c) has been learned to be low, according to training data.</figDesc><graphic coords="3,53.59,76.25,73.14,73.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Illustration of the iterative optimisation process. The first and last rows are two respective viewpoints, the middle row shows epipolar lines overlaid over the respective viewpoint. Diagonal arrows show the pose that the epipolar lines are based on. Each column is an iteration and vertical arrows shows the resulting pose and epipolar lines used in joint estimation. Final poses are marked with green borders. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 3</head><label>3</label><figDesc>PCP3D scores (%) for all limb parts with PCP threshold 0.5, compared to FMP[1] on HumanEva and UMPM datasets.(U-L: upper left, U-R: upper right, L-L: lower left, L-R: lower right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :Fig. 5 :Fig. 6 :</head><label>456</label><figDesc>Fig. 4: PCP3D scores (%) for individual parts obtained by FMP[1] (red) and ours (green) on both datasets. (U-L: upper left, U-R: upper right, L-L: lower left, L-R: lower right)</figDesc><graphic coords="6,212.95,303.77,121.61,91.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Illustration of the effect of adaptive viewpoint selection. The histograms show differences of errors (px) compared to the baseline<ref type="bibr" target="#b0">[1]</ref>. Negative differences (red) indicate that our method performs worse, positive differences (green) indicate that our method yielded a better pose. The adaptive mechanism reduces deteriorations while keeping improvements.</figDesc><graphic coords="7,308.98,454.17,243.78,132.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>HumanEva -PCP3D scores (%) of our model trained on subject 1, evaluated on subject 1 and all subjects combined, with PCP threshold 0.5. Performance of is compared to Flexible Mixture of Parts (FMP)<ref type="bibr" target="#b0">[1]</ref> method.</figDesc><table><row><cell cols="3">Subject Sequence FMP [1]</cell><cell cols="3">----Ours ----</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Geom. +App. +Adap.</cell></row><row><cell>S1</cell><cell>Box</cell><cell>77.34</cell><cell>82.70</cell><cell>83.87</cell><cell>85.31</cell></row><row><cell>All</cell><cell>Box</cell><cell>67.14</cell><cell>69.45</cell><cell>70.23</cell><cell>71.57</cell></row><row><cell>S1</cell><cell>Gestures</cell><cell>78.91</cell><cell>84.27</cell><cell>84.08</cell><cell>88.14</cell></row><row><cell>All</cell><cell>Gestures</cell><cell>74.68</cell><cell>77.38</cell><cell>78.81</cell><cell>80.34</cell></row><row><cell>S1</cell><cell>Jog</cell><cell>84.91</cell><cell>86.75</cell><cell>86.70</cell><cell>86.86</cell></row><row><cell>All</cell><cell>Jog</cell><cell>77.52</cell><cell>80.16</cell><cell>79.84</cell><cell>80.97</cell></row><row><cell>S1</cell><cell>Walking</cell><cell>84.65</cell><cell>86.71</cell><cell>86.50</cell><cell>87.68</cell></row><row><cell>All</cell><cell>Walking</cell><cell>78.49</cell><cell>81.69</cell><cell>81.96</cell><cell>83.17</cell></row><row><cell>S1</cell><cell>Overall</cell><cell>82.02</cell><cell>85.43</cell><cell>85.49</cell><cell>87.24</cell></row><row><cell>All</cell><cell>Overall</cell><cell>74.86</cell><cell>77.62</cell><cell>78.11</cell><cell>79.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">UMPM -PCP3D scores (%) on all</cell></row><row><cell cols="5">sequences with PCP threshold 0.5, compared to Flex-</cell></row><row><cell cols="4">ible Mixture of Parts (FMP) [1] method.</cell><cell></cell></row><row><cell cols="2">Sequence FMP [1]</cell><cell cols="3">----Ours ----</cell></row><row><cell></cell><cell></cell><cell cols="3">Geom. +App. +Adap.</cell></row><row><cell>Chair</cell><cell>74.72</cell><cell>78.09</cell><cell>77.54</cell><cell>79.94</cell></row><row><cell>Grab</cell><cell>74.23</cell><cell>76.25</cell><cell>77.18</cell><cell>81.92</cell></row><row><cell>Orthosyn</cell><cell>72.47</cell><cell>74.65</cell><cell>75.22</cell><cell>76.48</cell></row><row><cell>Table</cell><cell>70.30</cell><cell>73.49</cell><cell>74.18</cell><cell>77.86</cell></row><row><cell>Triangle</cell><cell>73.69</cell><cell>77.26</cell><cell>77.81</cell><cell>83.81</cell></row><row><cell>Overall</cell><cell>73.07</cell><cell>75.91</cell><cell>76.37</cell><cell>80.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>SHN Standard SHN Pre-processed</figDesc><table><row><cell>Head</cell><cell>100.0</cell><cell>7.40</cell><cell>25.17</cell></row><row><cell cols="2">Shoulders 99.47</cell><cell>49.08</cell><cell>100.0</cell></row><row><cell>U-R Arm</cell><cell>94.52</cell><cell>15.13</cell><cell>96.37</cell></row><row><cell>U-L Arm</cell><cell>88.31</cell><cell>45.51</cell><cell>98.48</cell></row><row><cell>L-R Arm</cell><cell>76.09</cell><cell>7.73</cell><cell>73.98</cell></row><row><cell>L-L Arm</cell><cell>71.53</cell><cell>41.88</cell><cell>84.54</cell></row><row><cell>Torso</cell><cell>100.0</cell><cell>52.58</cell><cell>100.0</cell></row><row><cell>Hips</cell><cell>60.63</cell><cell>0.0</cell><cell>65.52</cell></row><row><cell>U-R Leg</cell><cell>100.0</cell><cell>52.64</cell><cell>100.0</cell></row><row><cell>U-L Leg</cell><cell>100.0</cell><cell>51.65</cell><cell>100.0</cell></row><row><cell>L-R Leg</cell><cell>82.69</cell><cell>51.65</cell><cell>99.41</cell></row><row><cell>L-L Leg</cell><cell>73.58</cell><cell>48.15</cell><cell>98.15</cell></row><row><cell>Overall</cell><cell>87.24</cell><cell>35.28</cell><cell>86.80</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partly supported by a grant of type BQR by <rs type="funder">Fondation INSA-Lyon</rs>, project <rs type="projectName">CROME</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ykXGTuD">
					<orgName type="project" subtype="full">CROME</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T on PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="422" to="429" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T on PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2131" to="2143" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<editor>NIPS.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1337" to="1344" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human pose estimation in videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2012" to="2020" />
			<pubPlace>Santiago, Chile</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2361" to="2368" />
			<pubPlace>Columbus, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="588" to="595" />
			<pubPlace>Portland, Oregon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="331" to="346" />
			<pubPlace>Zurich, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Appearance sharing for collective human pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<editor>ACCV.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="138" to="151" />
			<pubPlace>Daejeon, Korea</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2369" to="2376" />
			<pubPlace>Columbus, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate human pose estimation by aggregating multiple pose hypotheses using modified kernel density approximation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="449" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Loose-limbed people: Estimating 3D human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3618" to="3625" />
			<pubPlace>Portland, Oregon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d pictorial structures for human pose estimation with supervoxels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<meeting><address><addrLine>Hawaii, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Voxel based annealed particle filtering for markerless 3d articulated motion capture</title>
		<author>
			<persName><surname>Canton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pardas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
			<pubPlace>Potsdam, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The stitched puppet: A graphical model of 3d human shape and pose</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3537" to="3546" />
			<pubPlace>Boston, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-view 3d human pose estimation combining single-frame recovery, temporal integration and model adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2214" to="2221" />
			<pubPlace>Miami, Florida</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-view body part recognition with random forests</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Bristol, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Joint camera pose estimation and 3d human pose estimation in a multi-camera setup</title>
		<author>
			<persName><forename type="first">J</forename><surname>Puwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<editor>ACCV.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="473" to="487" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Bristol, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance transforms of sampled functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="428" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName><surname>Xiaohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1293" to="1301" />
			<pubPlace>Boston, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with tiny synthetic videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T on PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast algorithms for large scale conditional 3d prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
			<pubPlace>Anchorage, Alaska</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sparse probabilistic regression for activity-independent human pose inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
			<pubPlace>Anchorage, Alaska</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2337" to="2344" />
			<pubPlace>Columbus, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1347" to="1355" />
			<pubPlace>Boston, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<editor>NIPS.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1799" to="1807" />
			<pubPlace>Montreal, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks&apos;</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1653" to="1660" />
			<pubPlace>Columbus, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1736" to="1744" />
			<date type="published" when="2014">2014</date>
			<pubPlace>Columbus, Ohio</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4733" to="4742" />
			<pubPlace>Las Vegas, Nevada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3073" to="3082" />
			<pubPlace>Las Vegas, Nevada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4715" to="4723" />
			<pubPlace>Las Vegas, Nevada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="483" to="499" />
			<pubPlace>Amsterdam, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T on PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Umpm benchmark: A multi-person dataset with synchronized video and motion capture data for evaluation of articulated human motion and interaction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Van Der Aa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Giezeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICV / ICCV 2011</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1264" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005">2005</date>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hand pose estimation through weakly-supervised learning of a rich intermediate representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
		<idno>arxiv:151106728</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Pre-print</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-task, multi-domain learning: application to semantic segmentation and pose regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trãl'meau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
