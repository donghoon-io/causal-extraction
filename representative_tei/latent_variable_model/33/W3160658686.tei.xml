<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SWITCHING VARIATIONAL AUTO-ENCODERS FOR NOISE-AGNOSTIC AUDIO-VISUAL SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mostafa</forename><surname>Sadeghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Nancy Grand-Est</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inria Grenoble Rhône-Alpes &amp; Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ieee</forename><forename type="middle">Senior</forename><surname>Member</surname></persName>
						</author>
						<title level="a" type="main">SWITCHING VARIATIONAL AUTO-ENCODERS FOR NOISE-AGNOSTIC AUDIO-VISUAL SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio-visual speech enhancement</term>
					<term>robustness</term>
					<term>variational auto-encoder</term>
					<term>variational inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, audio-visual speech enhancement has been tackled in the unsupervised settings based on variational autoencoders (VAEs), where during training only clean data is used to train a generative model for speech, which at test time is combined with a noise model, e.g. nonnegative matrix factorization (NMF), whose parameters are learned without supervision. Consequently, the proposed model is agnostic to the noise type. When visual data are clean, audio-visual VAE-based architectures usually outperform the audio-only counterpart. The opposite happens when the visual data are corrupted by clutter, e.g. the speaker not facing the camera. In this paper, we propose to find the optimal combination of these two architectures through time. More precisely, we introduce the use of a latent sequential variable with Markovian dependencies to switch between different VAE architectures through time in an unsupervised manner: leading to switching variational auto-encoder (SwVAE). We propose a variational factorization to approximate the computationally intractable posterior distribution. We also derive the corresponding variational expectation-maximization algorithm to estimate the parameters of the model and enhance the speech signal. Our experiments demonstrate the promising performance of SwVAE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio-visual speech enhancement (AVSE) refers to the task of removing background noise from a noisy speech with the help of visual information (lip movements) of the unknown speech <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. Several deep neural network (DNN)-based methods have been proposed for AVSE in the past. The majority of these methods are supervised, where the underlying idea is to learn a DNN that maps noisy speech and its associated visual data (video frames of mouth area) to clean speech <ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr">[5]</ref>. To have a good generalization performance, a huge dataset with different noise types and various signal-tonoise ratio (SNR) levels is usually required.</p><p>Xavier Alameda-Pineda acknowledges ANR JCJC ML3RI project (ANR-19-CE33-0008-01). This work has been partially supported by MIAI @ University Grenoble Alpes, (ANR-19-P3IA-0003) Recently, some unsupervised AVSE methods have been proposed that do not need noise signals for training <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref>, meaning that their training is agnostic to the noise type. This approach builds upon the audio-only speech enhancement counterpart <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref> consisting of two main steps. First, modeling the probabilistic generative process of clean speech using VAEs <ref type="bibr" target="#b11">[11]</ref>. Second, combining it with a noise model, e.g. NMF, to perform speech enhancement from noisy speech.</p><p>One critical issue with AVSE methods, shared with other AV-processing tasks such as speaker localisation and tracking <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>, is how to robustly handle noisy visual data at test time, e.g., when mouth area is heavily occluded or nonfrontal. Exploiting such noisy visual data by an AVSE model trained on clean data may degrade the performance. In the supervised settings, this problem is usually addressed by proper data augmentation and efficient audio-visual fusion strategies during model training. For example, <ref type="bibr" target="#b14">[14]</ref> proposes to combine speaker embedding with visual cues to achieve more robustness to occluded visual stream. Moreover, during training, some artificial occlusions are added to video frames. In the VAE-based unsupervised settings, a totally different perspective is pursued owning to its probabilistic nature. In this regard, a robust generative model has been proposed in <ref type="bibr" target="#b7">[7]</ref> which is a mixture of trained audio-based (A-VAE) and audiovisual based (AV-VAE) model. As such, following a variational inference approach, for noisy visual data the A-VAE model is chosen, whereas for clean visual data the AV-VAE model is used, thus providing robustness.</p><p>In this paper, we build upon <ref type="bibr" target="#b7">[7]</ref> and introduce a new model and associated robust AVSE algorithm, where a Markovian dependency is assumed to switch between different VAE-based generative models, and term them switching variational auto-encoder (SwVAE). Alternatively, the proposed model can be understood as a hidden Markov model (HMM) <ref type="bibr" target="#b15">[15]</ref> with emission probabilities given by the decoder of several VAEs. Furthermore, we propose a variational factorization of the posterior distribution of the latent variables, enabling efficient inference and algorithm initialization. Experimental results demonstrate the superior performance of the proposed method compared to <ref type="bibr" target="#b7">[7]</ref>.</p><p>The rest of the paper is organized as follows. Section 2 introduces the proposed SwVAE. The inference and speech enhancement methodologies, and the relation of the present work to <ref type="bibr" target="#b7">[7]</ref> are also detailed in this section. Section 3 presents and discusses the experiments.</p><formula xml:id="formula_0">m t t → t + 1 z t s t (a) Graphical model x t λ, τ ξ mt , Λ mt Σ mt v t W, H x t z t s t m t t ↔ t + 1 (b) Variational approximation</formula><p>Fig. <ref type="figure">1</ref>: Graphical model (left) and proposed variational inference (right) assotiacted to switching variational autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SWITCHING VARIATIONAL AUTOENCODERS</head><p>In this section, we present a generative model for short-time Fourier transform (STFT) time frames of clean speech consisting of audio-only and audio-visual VAE models plus a switching variable deciding which model to be used for each audio frame. The switching variable is modeled with an HMM. We also discuss how to structure the variance of the background noise via NMF. Then, a variational approximation is proposed to estimate the model parameters and infer the latent variables, including the clean speech signal, from the noisy mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The generative model of SwVAE</head><p>We define s t ∈ C F as the vector of clean speech STFT coefficients at time frame t ∈ {1, ..., T }. In the following, N c and N stand for complex-and real-valued Gaussian distributions, respectively. The main methodological contribution of this paper is the use of a switching variable m t ∈ {1, . . . , M } modeled with a Markov chain in combination with a set of M non-linear generative models (i.e. VAE) to model clean speech. The full generative model describes the probabilistic relationship between the switching variable m t , the clean speech s t , and the latent code z t ∈ R L , describing some hidden characteristics of s t , given the associated visual data representation v t ∈ R V . There are two possible, equivalent interpretations of this model. First, a hidden Markov model with emission probabilities given by the decoder of M VAEs. Second, a set of M VAEs switched by a selecting variable modeled with Markovian dependencies. More formally:</p><formula xml:id="formula_1">       p(m 1 , . . . , m T ) ∼ MC(λ, τ ), p(z t |m t ; v t ) ∼ N ξ mt (v t ), Λ mt (v t ) , p(s t |z t , m t ; v t ) ∼ N c 0, Σ mt (z t , v t ) ,<label>(1)</label></formula><p>where MC(λ, τ ) is short for a Markov chain with initial distribution λ and transition distribution τ , and ξ mt (.), Λ mt (.), and Σ mt (., .) are non-linear transformations of their inputs indexed by m t ∈ {1, . . . , M } and realized as DNNs. For each generative model, the associated DNNs are trained by approximating the intractable posterior p(z t |s t , m t ; v t ) by another DNN-based parameterized Gaussian distribution called the encoder <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b11">11]</ref>. So, there are M different distributions for the prior of z t and for the likelihood of s t . Importantly, the switching variable m t selects which one of the M models is used at each time step t, while ensuring temporal smoothing in the choice of this transformation. To complete the definition of the probabilistic model, we use an NMF structure for the additive noise <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>:</p><formula xml:id="formula_2">p(x t |s t ) ∼ N c s t , diag Wh t ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">W ∈ R F ×K + , H ∈ R K×T +</formula><p>, and h t denotes the t-th column of H. The graphical representation of the full model is shown in Fig. <ref type="figure">1 (a)</ref>. The set of HMM and NMF parameters, i.e. {λ, τ, W, H} are then estimated following a variational inference method detailed in the next section, and represented in Fig. <ref type="figure">1 (b)</ref>. While for the generative model the dependencies are forward in time, at inference time, the latent code and spectrogram at any time t depend on the past and future noisy observations. It should be emphasized that the DNN parameters of (1), trained according to <ref type="bibr" target="#b6">[6]</ref>, are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Variational Inference</head><p>In the proposed formulation, the problem of speech enhancement is cast into the computation of the posterior probability p(s|x, v), which is the marginal of the full posterior p(s, z, m|x, v), where we define x = {x t } T t=1 and analogously s, z, m, v. The full posterior being intractable, we propose the following variational factorization:</p><formula xml:id="formula_4">p(s, z, m|x, v) ≈ r s (s|m)r z (z|m)r m (m).<label>(3)</label></formula><p>It is easy to see that r s and r z further factorize over time, meaning that: r s (s|m) = t r s (s t |m t ) and analogously for r z (z|m). Moreover, as a variational approximation, the posterior of the latent code z t is assumed to follow a Gaussian distribution r z (z t |m t ) = N (c tm , Ω tm ), where the mean vector c tm and the diagonal covariance matrix Ω tm are to be estimated along with r s and r m . To this end, we optimize the following lower-bound of the data log-likelihood log p(x, v), as done in variational inference:</p><formula xml:id="formula_5">E r s r z r m log p(x, v, s, z, m) r s (s|m)r z (z|m)r m (m) ≤ log p(x, v). (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">E-s step</head><p>Optimizing (4) over r s provides the following expression:</p><formula xml:id="formula_6">r s (s t |m t ) ∝ p(x t |s t ) • exp E r z log p(s t |z t , m t ; v t ) .</formula><p>Approximating the intractable expectation with a Monte-Carlo estimate, we obtain a Gaussian distribution: r s (s</p><formula xml:id="formula_7">t |m t ) = N c (η mt t , diag[ν mt t ]</formula><p>), where:</p><formula xml:id="formula_8">η mt f t = γ mt f t γ mt f t + (WH) f t •x f t , ν mt f t = γ mt f t • (WH) f t γ mt f t + (WH) f t ,<label>(5)</label></formula><formula xml:id="formula_9">γ mt f t = 1 D D d=1 Σ -1 mt,f f (z (d) mt , v t ) -1 ,<label>(6)</label></formula><p>in which, Σ mt,f f denotes the (f, f )-th entry of Σ mt (similarly for the rest of the variables), and {z</p><formula xml:id="formula_10">(d)</formula><p>mt } D d=1 is a sequence sampled from r z (z t |m t ). The result in (5) must be interpreted as a Wiener filter, averaged over the latent variable z t for a given VAE generative model m t . The enhanced speech signal is the marginalisation over the switching variable at time t, and naturally writes:</p><formula xml:id="formula_11">ŝt = E r m (mt) E r s (st|mt) [s t ] = mt r m (m t )η mt t , ∀t. (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">E-z step</head><p>After doing some derivations, the set of parameters of r z (z t |m t ) is estimated by solving:</p><formula xml:id="formula_12">max ctm,Ωtm E r m (mt) E r z (zt|mt) E r s (st|mt) log p(s t |z t , m t ; v t ) -KL(r z (z t |m t ) p(z t |m t ; v t )) .<label>(8)</label></formula><p>where, KL denotes the Kullback-Leibler divergence. In <ref type="bibr" target="#b8">(8)</ref>, the expectation over r m and r s can be evaluated in closedform. This is also the case for the KL term as both the distributions are Gaussian. However, the expectation over r z is intractable. Like in standard VAE, here we approximate this expectation with a single sample drawn from r z . Furthermore, to be able to back-propagate through the posterior parameters, the reparametrization trick is utilized <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">E-m step</head><p>For r m (m), we obtain:</p><formula xml:id="formula_13">r m (m) ∝ p(m) • T t=1 exp(-g t (m t ))<label>(9)</label></formula><p>with: • The parameters of r s (s|m) using ( <ref type="formula" target="#formula_8">5</ref>).</p><formula xml:id="formula_14">g t (m t ) =E r z KL(r s (s t |m t ) p(s t |z t , m t ; v t )) -<label>(10)</label></formula><p>• The posterior r m (m) uniformly.</p><p>• The parameters W, H, τ and λ (randomly).</p><p>3: While stop criterion not met do:</p><p>• E-z step: Using <ref type="bibr" target="#b8">(8)</ref>.</p><p>• E-s step: Using ( <ref type="formula" target="#formula_8">5</ref>).</p><p>• E-m step: Compute q mt = exp(-gt(mt))</p><formula xml:id="formula_15">m t</formula><p>exp(-gt(mt)) using <ref type="bibr" target="#b10">(10)</ref>, and run the forward backward algorithm <ref type="bibr" target="#b15">[15]</ref> to obtain the posterior probability r m (m t ) and the joint posterior probability ζ m (m t-1 , m t ).</p><p>• M step: Update W, H using ( <ref type="formula" target="#formula_17">12</ref>) and ( <ref type="formula" target="#formula_16">11</ref>), and λ, τ using the standard formulae with r m and ζ m <ref type="bibr" target="#b15">[15]</ref>.</p><p>4: End while 5: Speech enhancement: Using <ref type="bibr" target="#b7">(7)</ref>.</p><p>compute <ref type="bibr" target="#b10">(10)</ref>. In order to compute the marginal variational posterior r m (m t ) required in the E-s and E-z steps, we realize that ( <ref type="formula" target="#formula_13">9</ref>) has the same structure as standard HMM if we consider exp(-g t (m t )) as the emission probability of the HMM. We therefore use the forward-backward algorithm <ref type="bibr" target="#b15">[15]</ref> to efficiently compute r m (m t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">M step</head><p>After performing the E steps, the NMF parameters are updated by optimizing (4). The update formulas for W and H are then obtained by using standard multiplicative rules <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_16">H ← H W V (WH) -2 W (WH) -1 ,<label>(11)</label></formula><formula xml:id="formula_17">W ← W V (WH) -2 H (WH) -1 H , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where</p><formula xml:id="formula_19">V = mt r m (m t )(|x f t -η mt f t | 2 +ν mt f t ) (f,t)</formula><p>, and signifies entry-wise operation. The parameters of the HMM, i.e. λ and τ , are updated by the standard formulae using the joint posterior probabilities computed by the forwardbackward algorithm in the E-m step. The complete inference and enhancement algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Novelty of SwVAE w.r.t. [7]</head><p>The closest work to ours is <ref type="bibr" target="#b7">[7]</ref>, which uses a mixture model, comprising an A-VAE and an AV-VAE, as the generative Though sharing some similarities, there are several crucial differences between the two methods. First, here we assume a Markovian dependency on the switching variable that ensures smoothness over time. Second, in <ref type="bibr" target="#b7">[7]</ref> the following variational factorization is proposed: p(s, z, m|x) ≈ r s (s)r z (z)r m (m), where r s and r z are not conditioned on m. This is in contrast to our proposed factorization given in <ref type="bibr" target="#b3">(3)</ref>, which provides a more effective approximation and a robust initialization for the latent codes z, as required by the inference algorithm. More precisely, in the proposed framework, the parameters of r s (s|m) are initialized using its respective set of latent codes z, which themselves are initialized by the corresponding encoders (see Section 3), as opposed to <ref type="bibr" target="#b7">[7]</ref> where a weighted combination of the latent codes (coming from different models) is used for initializing the parameters of r s (s). This might not be effective given that latent initialization is important in VAE-based AVSE <ref type="bibr" target="#b8">[8]</ref>. Finally, the proposed posterior approximation r z (z t |m t ) = N (c tm , Ω tm ) makes sampling, needed by <ref type="bibr" target="#b6">(6)</ref>, more efficient than the method of <ref type="bibr" target="#b7">[7]</ref> which relies on the computationally demanding Metropolis-Hastings algorithm <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Protocol We evaluate the performance of SwVAE and compare it with <ref type="bibr" target="#b7">[7]</ref> using the same experimental protocol. We used two VAE models (A-VAE and AV-VAE) 1 from <ref type="bibr" target="#b6">[6]</ref>, trained on the NTCD-TIMIT dataset <ref type="bibr" target="#b17">[17]</ref>. The test set includes 9 speakers, along with their corresponding lip region of interest, with different noise types: Living Room (LR), White, Cafe, Car, Babble, and Street, and noise levels: {-5, 0, 5, 10, 15} dB. From each speaker, we randomly selected 150 examples per noise level for evaluation. The parameters for the algorithm of <ref type="bibr" target="#b7">[7]</ref> where set as their proposed values. Both of the algorithms were run for 200 iterations, on the same test set. For optimizing <ref type="bibr" target="#b8">(8)</ref>, the Adam optimizer <ref type="bibr" target="#b18">[18]</ref> was used with a learning rate of 0.05 for 10 iterations. Moreover, we used D = 20 samples to compute <ref type="bibr" target="#b6">(6)</ref> and <ref type="bibr" target="#b10">(10)</ref>. The c tm , Ω tm parameters of r z were, respectively, initialized with the means and variances at the output of the respective VAE encoders by giving (x t , v t ) as their inputs. The parameters of r s are then initialized using ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula" target="#formula_9">6</ref>). 1 For A-VAE, the prior of zt is a standard normal distribution, and Σm t is a function of only zt; see <ref type="bibr" target="#b1">(1)</ref>.</p><p>The two AVSE algorithms were run on the test set with both clean visual data as well as artificially generated noisy versions, where about one third of the total video frames per test instance were occluded. Similarly to <ref type="bibr" target="#b7">[7]</ref>, the occlusions were simulated by random patches of standard Gaussian noise added to randomly selected sub-sequences of 20 consecutive video frames. We used three standard speech enhancement scores, i.e., signal-to-distortion ratio (SDR) <ref type="bibr" target="#b19">[19]</ref>, perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b20">[20]</ref>, and short-time objective intelligibility (STOI) <ref type="bibr" target="#b21">[21]</ref>. SDR is measured in decibels (dB), and PESQ and STOI values lie in the intervals [-0.5, 4.5] and [0, 1], respectively (the higher the better).</p><p>Results Table <ref type="table" target="#tab_1">1</ref> summarizes the results, averaged over all the test samples, for the three performance measures, and clean as well as noisy visual data. From this table, we can see that in terms of PESQ and SDR, SwVAE outperforms <ref type="bibr" target="#b7">[7]</ref>, with the performance difference being more significant in high SNR values. In terms of the intelligibility measure, i.e., STOI, the proposed method exhibits much better performance than <ref type="bibr" target="#b7">[7]</ref>. These observations are consistent for both clean and noisy visual data. Furthermore, the two algorithms show robustness to noisy visual data, which is especially noticeable in terms of STOI. However, for the algorithm of <ref type="bibr" target="#b7">[7]</ref> the performance drop due to noisy visual data is higher than SwVAE. Supplementary materials are available online<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we proposed a noise-agnostic audio-visual speech generative model based on a sequential mixture of trained A-VAE and AV-VAE models, combined with an NMF model for the noise variance. The switching variable allows us to seamlessly use either of the auto-encoders for speech enhancement, without requiring supervision. We detailed a variational expectation-maximization approach to estimate the parameters of the model as well as to enhance the noisy speech. The proposed algorithm, called switching VAE (SwVAE), exhibits promising performance when compared to the previous work <ref type="bibr" target="#b7">[7]</ref> on robust AVSE. In the future, we would like to explore the use of Dynamical VAEs <ref type="bibr" target="#b22">[22]</ref> for unsupervised AVSE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Trained A-VAE and AV-VAE models, noisy STFT frames {x t }</figDesc><table><row><cell cols="2">T t=1 , and visual embeddings {v t } T t=1 .</cell></row><row><cell>2: Initialize:</cell><cell></cell></row><row><cell>• The latent codes {z</cell><cell>(d) mt } D d=1 via the VAE encoders.</cell></row></table><note><p><p>E r s log p(x t |s t ) + KL(r z (z t |m t ) p(z t |m t ; v t ))</p>Again, the KL terms and the expectation over r s can be computed in closed-form. However, we approximate the expectation over r z by a Monte-Carlo estimate. This allows us to Algorithm 1 SwVAE 1: Input:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average PESQ, SDR and STOI values of the enhanced speech signals. Here, "clean" and "noisy" refer to visual data.</figDesc><table><row><cell>Measure</cell><cell></cell><cell></cell><cell>PESQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SDR (dB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STOI</cell><cell></cell><cell></cell></row><row><cell>SNR (dB)</cell><cell>-5</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>-5</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>-5</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell></row><row><cell>Input</cell><cell cols="9">1.44 1.67 2.04 2.30 2.72 -12.30 -7.30 -3.45 1.88</cell><cell>6.73</cell><cell cols="5">0.22 0.32 0.45 0.56 0.68</cell></row><row><cell>[7] -clean</cell><cell cols="5">1.70 1.92 2.29 2.48 2.66</cell><cell>-3.51</cell><cell>1.67</cell><cell>5.38</cell><cell cols="7">9.22 12.07 0.24 0.35 0.47 0.55 0.65</cell></row><row><cell cols="6">SwVAE -clean 1.67 1.97 2.39 2.62 2.83</cell><cell>-3.59</cell><cell>2.00</cell><cell cols="8">6.24 10.73 14.12 0.25 0.36 0.51 0.61 0.72</cell></row><row><cell>[7] -noisy</cell><cell cols="5">1.66 1.91 2.22 2.41 2.51</cell><cell>-3.78</cell><cell>1.50</cell><cell>5.18</cell><cell cols="7">8.72 10.88 0.23 0.34 0.45 0.53 0.63</cell></row><row><cell cols="6">SwVAE -noisy 1.65 1.94 2.36 2.60 2.81</cell><cell>-3.97</cell><cell>1.84</cell><cell cols="8">6.14 10.51 14.06 0.24 0.35 0.50 0.59 0.67</cell></row><row><cell cols="2">model of clean speech.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://team.inria.fr/perception/research/ swvae/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio-visual enhancement of speech in noise</title>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3007" to="3020" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An overview of deep-learningbased audio-visual speech enhancement and separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09586</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-visual speech enhancement using multimodal deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jen-Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Hui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiu-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3244" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual speech enhancement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1170" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio-visual speech enhancement using conditional variational auto-encoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust unsupervised audio-visual speech enhancement using a mixture of variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mixture of inference networks for vae-based audio-visual speech enhancement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10647</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variance modeling framework based on variational autoencoders for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Machine Learning for Signal Processing</title>
		<meeting>IEEE International Workshop on Machine Learning for Signal essing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="716" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Active-speaker detection and localization with microphones and cameras embedded into a robotic head</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Sanchez-Riera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE-RAS Humanoids</publisher>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting the complementarity of audio and visual data in multi-speaker tracking</title>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">My lips are concealed: Audio-visual speech enhancement through obstructions</title>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NTCD-TIMIT: A new database and baseline for noise-robust audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">A.-H</forename><surname>Abdelaziz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference of the International Speech Communication Association</title>
		<meeting>Conference of the International Speech Communication Association</meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3752" to="3756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
	<note>Hekstra</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of timefrequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Diard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12595</idno>
		<title level="m">Dynamical variational autoencoders: A comprehensive review</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
