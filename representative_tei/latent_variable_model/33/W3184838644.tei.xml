<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Block Nonlinear Neural Dynamical Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elliott</forename><surname>Skomski</surname></persName>
							<email>elliott.skomski@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soumya</forename><surname>Vasisht</surname></persName>
							<email>soumya.vasisht@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colby</forename><surname>Wight</surname></persName>
							<email>colby.wight@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Tuor</surname></persName>
							<email>aaron.tuor@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ján</forename><surname>Drgoňa</surname></persName>
							<email>jan.drgona@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Draguna</forename><surname>Vrabie</surname></persName>
							<email>draguna.vrabie@pnnl.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<settlement>Richland</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Block Nonlinear Neural Dynamical Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network modules conditioned by known priors can be effectively trained and combined to represent systems with nonlinear dynamics. This work explores a novel formulation for data-efficient learning of deep control-oriented nonlinear dynamical models by embedding local model structure and constraints. The proposed method consists of neural network blocks that represent input, state, and output dynamics with constraints placed on the network weights and system variables. For handling partially observable dynamical systems, we utilize a state observer neural network to estimate the states of the system's latent dynamics. We evaluate the performance of the proposed architecture and training methods on system identification tasks for three nonlinear systems: a continuous stirred tank reactor, a two tank interacting system, and an aerodynamics body. Models optimized with a few thousand system state observations accurately represent system dynamics in open loop simulation over thousands of time steps from a single set of initial conditions. Experimental results demonstrate an order of magnitude reduction in open-loop simulation mean squared error for our constrained, block-structured neural models when compared to traditional unstructured and unconstrained neural network models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Most systems are inherently nonlinear in nature, which makes the study of nonlinear models of systems of utmost importance to engineers, physicists, and mathematicians. In contrast with linear systems, where observations are proportional to changes in the input and state variables, nonlinear systems appear unpredictable, counter-intuitive, or even chaotic <ref type="bibr" target="#b0">[1]</ref>. Identifying mathematical models for such complex dynamical systems from input-output data is central to most engineering systems for modeling, prediction and control.</p><p>Most works in deep learning-based system identification train large, complex, monolithic networks with strong regularization to avoid overfitting. However, greater data efficiency and stability in optimization of more precise and less opaque systems models can be achieved by embedding prior knowledge and intuition of general system characteristics such as structure and dissipation <ref type="bibr" target="#b1">[2]</ref>. In addition, modern neural network approaches don't typically provide constraints enforcement within physically realizable and operationally safe bounds, which can lead to unexpected model behavior in unobserved regions of the system state space. To address these shortcomings, we propose a "domain intuitive" framework for constructing constrained block nonlinear neural network dynamics models which incorporate general knowledge and intuition about system behavior and can train efficiently from small data sets.</p><p>We present a generalized family of neural state space model architectures in the form of constrained block-structured nonlinear neural state space models. Our approach combines the benefits of classical system identification techniques with the advances in constrained deep learning to learn deep structured models. In particular, we build modular representations of nonlinear systems with identifiable neural modules that may be constrained and influenced by structural priors. This allows for inclusion of stability constraints through eigenvalue regularizations of the network weights, state and input constraints, and other boundary condition constraints.</p><p>We investigate the benefits of the proposed method on three nonlinear system identification tasks, and analyze, through an ablation study, the value of adding structure and constraints on the modeling outcome. Compared to their unstructured unconstrained counterparts, the presented constrained blockstructured models show 68-91% reduction in open-loop MSE from the ground truth dynamics. Our experimental results empirically demonstrate the potential to learn physically representative neural network dynamics models from limited recorded systems measurements, and further underscore the significance of including structure and constraints in the identification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>Various methods exist for identifying nonlinear systems differing in their purpose and application <ref type="bibr" target="#b2">[3]</ref>. When a whitebox approach of mathematical modeling using first principles is not feasible, data-driven gray and black-box methods have been proposed including Volterra series, neuro-fuzzy models, block-oriented models, and NARMAX models <ref type="bibr" target="#b3">[4]</ref>. These methods have been successful in many areas <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>.</p><p>In the current work we focus on block-oriented models which are composed of two basic building blocks: linear dynamic models and static nonlinearities <ref type="bibr" target="#b2">[3]</ref>. Such blockstructured nonlinear models, represent a nonlinear system as a collection of static or dynamic components, either linear or simpler nonlinear sub-systems <ref type="bibr" target="#b3">[4]</ref>. This enables a more accurate description of the process behavior and reduces the hard problem of identifying high-order nonlinear systems to the identification of lower order subsystems and their interactions. Two such simple models are the Hammerstein and the Wiener models. Hammerstein systems are useful to describe linear dynamic processes driven by a nonlinear actuator. Wiener systems describe linear dynamic processes equipped with a nonlinear sensor. Despite their simplicity, such models have been successfully used in many engineering fields (signal processing, identification of biological systems, modeling of distillation columns, modeling of hydraulic actuators, etc.) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>, providing reasonable approximations and insight into the system structure <ref type="bibr" target="#b12">[13]</ref>.</p><p>Neural networks have a long and productive history in data-driven physical modeling. Neural networks were used as early as 1990 to model discrete-time nonlinear dynamics <ref type="bibr" target="#b13">[14]</ref>. Recurrent neural networks (RNN) and longshort term memory (LSTM) networks are dynamics-inspired architectures which have been studied heavily in the context of machine learning applications <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>. In recent years, RNNs and LSTMs are becoming increasingly popular for their potential in tackling nonlinear problems in black-box system identification <ref type="bibr" target="#b18">[19]</ref>. For instance, an LSTM-based architecture in <ref type="bibr" target="#b19">[20]</ref> was used to model nonlinear effects in unmanned surface vehicle dynamics. Several extensions to straightforward applications of recurrent architectures have since been proposed. For instance, several variations of Neural Kalman filters <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[23]</ref> have been proposed for control oriented systems modeling, and neural Koopman operators <ref type="bibr" target="#b22">[24]</ref> have been presented. In addition, several recent advances have extended neural networks to model nonlinear continuous time dynamics <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some recent works interpret deep neural networks through the optics of differential equations <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> creating stronger theoretical links between dynamical systems and neural networks. Others show that structural priors via factored parametrizations of linear maps can be used, for instance to constrain the eigenvalues of the layer Jacobians <ref type="bibr" target="#b31">[32]</ref>. Additionally, it has been demonstrated that regularizing the eigenvalues of successive linear transformations alleviates the vanishing and exploding gradient problem, allowing to train very deep recurrent architectures <ref type="bibr" target="#b33">[33]</ref>.</p><p>Neural state space models (SSM) <ref type="bibr" target="#b34">[34]</ref>- <ref type="bibr" target="#b37">[37]</ref> represent structural modifications of vanilla RNNs. For example, authors in <ref type="bibr" target="#b38">[38]</ref> compared different neural network architectures including strictly feed-forward and recurrent Hammerstein models for identifying nonlinear dynamical systems. Our present work is a generalization of several prior works on structured neural SSMs, such as Hammerstein <ref type="bibr" target="#b39">[39]</ref>, or Hammerstein-Wiener models <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>Consider the non-autonomous partially observable nonlinear dynamical system</p><formula xml:id="formula_0">d dt x(t) = f (x(t), u(t)),<label>(1a)</label></formula><formula xml:id="formula_1">y(t) = g(x(t)), (1b) x(t 0 ) = x 0 , u(t 0 ) = u 0 (1c)</formula><p>where y(t) ∈ R ny are measured outputs, and u(t) ∈ R nu represents controllable inputs or measured disturbances affecting the system dynamics. A. Neural network dynamical models 1) Unstructured nonlinear dynamical models: In the absence of prior knowledge of the full dynamics of the system and the interactions between its components, we use the following generic form of a neural state space model:</p><formula xml:id="formula_2">x t+1 = f xu ([x t ; u t ]) (2a) ŷt+1 = f y (x t+1 ) (2b)</formula><p>where f xu is a neural network which models the interaction between state and input/disturbance vectors, and f y is a linear map. Although this formulation provides a good substrate for learning system dynamics with unknown properties, it lacks structure for incorporating functional priors for specific system components.</p><p>2) Block-nonlinear dynamical models: Given prior assumptions of the interactions between the components of a block-structured nonlinear model, we can instead use a formulation with a structured representation of the state and input/disturbance dynamics of a system:</p><formula xml:id="formula_3">x t+1 = f x (x t ) + f u (u t ) (3a) ŷt+1 = f y (x t+1 )<label>(3b)</label></formula><p>where f x and f u are nonlinear maps, and f y is a linear map. This formulation allows each component of the state dynamics to assume separate priors, and for interactions between components to be specified directly. By changing the linearity of model components f x , f u , or f y the formulation can be adjusted to follow the structure of other block-oriented state space models, which may be more appropriate to model particular systems. Table <ref type="table">I</ref> lists the possible model class configurations and the linearity of their respective components.</p><p>3) State observer model: To model partially observable dynamical systems, we assume that the states x t of our neural state space models represent latent dynamics. Hence, we include an additional neural network representing a state observer:</p><formula xml:id="formula_4">x 0 = f o ([y 1-Np ; . . . ; y 0 ])<label>(4)</label></formula><p>where f o is a nonlinear map, and N p is a lookback horizon capturing potential time lag of observed system outputs y t . 4) Overall Model Architecture: Fig. <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of the block structured neural state model (3) with a state estimator model <ref type="bibr" target="#b3">(4)</ref>. Please note that different neural network component blocks can be parametrized by structured linear maps and learnable activation functions explained in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Problem</head><p>We assume access to a limited set of system measurements in the form of tuples, each of which corresponds to the inputoutput pairs along sampled trajectories with temporal gap ∆. That is, we form a dataset</p><formula xml:id="formula_5">S = {(u (i) t , y (i) t ), (u (i) t+∆ , y (i) t+∆ ), . . . , (u (i) t+N ∆ , y (i) t+N ∆ )},<label>(5)</label></formula><p>where i = 1, 2, . . . , n represents up to n different batches of input-output trajectories with N -step time horizon length.</p><p>For each batch, i, of observations, the neural state estimator generates an initial state x 0 and the neural state space model, in open-loop, recurrently generates N predictions, ŷ(i) 1 , ..., ŷ(i) N . The neural network state space model is then fit to the unknown dynamics (1) by minimizing the mean squared error, L y , between predicted values and the ground truth measurements for the N -step prediction horizon:</p><formula xml:id="formula_6">L y = 1 n • N • n y n i=1 N t=1 ny j=1 (ŷ (i) t,j -y (i) t,j ) 2 (6)</formula><p>where n y is the dimension of the y t vectors of observations and predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neural network block components</head><p>We consider three different classes of neural networks as options for the state observer and nonlinear block components of the state space models.</p><p>1) Multi-layer Perceptron: The MLP is a universal function approximator which composes successive affine linear transformations with a nonlinear activation function g. (1)  (7a)</p><formula xml:id="formula_7">f = f (L) • f (L-1) • . . . • f</formula><formula xml:id="formula_8">f (k) (x) = g(W (k) x + b (k) ) (7b)</formula><p>where W and b are weight and bias terms, respectively. The number of hidden layers of the MLP is the number of nonlinear compositions, and the number of nodes for each layer is the dimension of the projected space for the linear map for that layer. Deeper (more layers) and wider (more nodes) networks allow for the approximation of arbitrary multivariable vector-valued functions.</p><p>2) Residual MLP: The residual variant of the MLP adds so called shortcut connnections from the output of the previous layer:</p><formula xml:id="formula_9">f (k) (x) = g W (k) x + b (k) + f (k-1) (x)<label>(8)</label></formula><p>Residual MLPs mitigate vanishing gradients during backpropagation, allowing for effective optimization of deeper neural networks <ref type="bibr" target="#b28">[29]</ref>.</p><p>3) Recurrent Neural Networks: To model sequential dependencies across time, we make use of RNNs which add a recurrent term to the standard MLP as an additional affine linear transformation of the previous transformed time step:</p><formula xml:id="formula_10">f (k) (x t ) = g W (k) x t + W (k) r f (k) (x t-1 ) + b (k) (9)</formula><p>where W r are the weights of the recurrent term. 4) Activation functions: We consider two prospective nonlinear activation functions for the aforementioned neural network block functions. The Gaussian Error Linear Unit (GELU) <ref type="bibr" target="#b41">[41]</ref> weights activations by their value rather than gating them by their sign and has demonstrated superior performance to other activations on some tasks. Bendable Linear Units (BLU) <ref type="bibr" target="#b42">[42]</ref> learns an activation from a family of smooth approximations to two piece linear functions such as encompassed by Parametric ReLU <ref type="bibr" target="#b43">[43]</ref> but which are C ∞ continuous, yielding a smoother error surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Linear maps</head><p>In addition to standard neural network weights, we use structured matrices and regularization strategies to constrain the models' constituent linear maps to satisfy various structural and spectral properties. Constrained linear maps provide a powerful means of embedding prior information into a dynamical model, and can improve the stability of both the underlying optimization problem and the learned dynamics model.</p><p>1) Perron-Frobenius Parametrization: We impose constraints on the eigenvalues of a linear map using insight from the Perron-Frobenius theorem: the row-wise minimum and maximum of any positive square matrix defines its dominant eigenvalue's lower and upper bound, respectively. To apply this, we introduce a second weight matrix M ∈ R nx×nx whose values are clamped between lower and upper eigenvalue bounds λ min and λ max , then elementwise multiply it with a weight matrix W ∈ R nx×nx with row-wise softmax applied such that each of its rows sum to 1:</p><formula xml:id="formula_11">M = λ max -(λ max -λ min ) • σ(M) (10a) Wi,j = exp(W i,j ) nx k=0 exp(W i,k ) • Mi,j<label>(10b)</label></formula><p>where σ(x) = 1/1-exp(-x) is the logistic sigmoid function.</p><p>2) Soft SVD Parametrization: This technique parametrizes a linear map as a factorization via singular value decomposition. The map is defined as two unitary matrices U and V initialized as orthogonal matrices, and singular values Σ initialized randomly. During optimization, U and V are constrained via regularization terms such that they remain orthogonal:</p><formula xml:id="formula_12">L U = ||I -UU T || 2 + ||I -U T U|| 2 (11a) L V = ||I -VV T || 2 + ||I -V T V|| 2 (11b) L reg = L U + L V (11c)</formula><p>Similar to Perron-Frobenius regularization, this regularization also enforces boundary constraints on the singular values Σ, and its eigenvalues by extension. This is again achieved by clamping and scaling Σ:</p><formula xml:id="formula_13">Σ = diag(λ max -(λ max -λ min ) • σ(Σ)) (12a) W = U ΣV (12b)</formula><p>where λ min and λ max are the lower and upper singular value bounds, respectively.</p><p>3) Spectral Parametrization: Similar to Soft SVD, this method proposed by <ref type="bibr" target="#b44">[44]</ref> uses an SVD factorization to parametrize a linear map to enforce spectral constraints. This is achieved structurally using Householder reflectors to represent unitary matrices U and V. The singular values Σ are also constrained to lie between λ min and λ max using a sigmoid clamping similar to Soft SVD. In this form, no regularization term is needed to maintain orthogonal structure or spectral properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Constraints 1) Output Constraints:</head><p>We apply inequality constraints on predictions during training in order to promote the boundedness and convergence of our dynamical models. We define output lower and upper bounds y and y, respectively, and derive corresponding slack variables s y and s y to include in the objective function as an additional term L con y :</p><formula xml:id="formula_14">s y = max(0, -ŷ + y)<label>(13a)</label></formula><formula xml:id="formula_15">s y = max(0, ŷ -y)<label>(13b)</label></formula><formula xml:id="formula_16">L con y = 1 n y ny i=1 (s y i + s y i ) (13c)</formula><p>Bounding system trajectories is a straightforward way of including prior knowledge by delineating a physically meaningful phase space of the learned dynamics.</p><p>2) Input Influence Constraints: For block nonlinear models, we apply additional constraints to control the influence of learned input dynamics f u on predictions. Like the state observation constraints previously described, these are inequality constraints and are derived for f u in the same manner as in Equation <ref type="formula" target="#formula_14">13</ref>to create another term L con fu .</p><p>3) Predicted State Smoothing: To promote continuous trajectories of our dynamics models, we optionally apply a state smoothing loss which minimizes the mean squared error between successive predicted states:</p><formula xml:id="formula_17">L dx = 1 (N -1)n x N -1 t=1 nx i=1 (x (i) t -x (i) t+1 ) 2<label>(14)</label></formula><p>4) Multi-objective loss function: We include constraints penalties as additional terms to the optimization objective 6, and further define coefficients, Q * as hyperparameters to scale each term in the multi-objective loss function:</p><formula xml:id="formula_18">L = Q y L y +Q reg L reg +Q dx L dx +Q con y L con y +Q con fu L con fu<label>(15)</label></formula><p>Note that in this work, L reg is only applied if Soft SVD linear map regularization is used. To minimize this objective, we use gradient descent via the AdamW optimizer <ref type="bibr" target="#b45">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL CASE STUDIES</head><p>In this section we outline the experimental setup, procedure, and resulting outcomes for three numerical case studies which apply our constrained neural block nonlinear modeling approach to three nonlinear system identification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We consider three systems with varying degrees and types of nonlinearities in states and inputs to illustrate the versatility of our neural block-structured framework.</p><p>In chemical engineering, the Continuous Stirred Tank Reactor (CSTR) is a common mathematical model for a chemical reactor. It is equipped with a mixing device to provide efficient mixing of materials. It is a non-dissipative system that exhibits nonlinear behavior during exothermic reactions with unstable, oscillatory modes that makes the identification process non-trivial. The model is described by the following set of ordinary differential equations:</p><formula xml:id="formula_19">r = k 0 e -E Rx 2 x 1 (16a) ẋ1 = q V (C af -x 1 ) -r (16b) ẋ2 = q V (T f -x 2 ) + H ρc p rA + A V ρc p (u -x 2 ) (16c)</formula><p>where the measured system states x 1 , and x 2 are the concentration of the product and temperature, respectively. The system has one control variable u that represents the temperature of cooling jacket. We assume that the structure of the differential equations and the parameters {k 0 , E, R, q, V, H, ρ, c p , A, T f , C af } are unknown. We generate a training dataset with an emulator based on the system equations. Random step inputs are used to excite the system to capture the system's stability region and transient response.</p><p>We generate the dataset by simulating the system for 10,000 time steps using the Scipy ODEInt solver. The Two Tank system we consider consists of two water tanks that are connected by a valve. Liquid inflow to the first tank is governed by a pump. The valve opening controls the flow between the tanks and can either be fully open or fully closed. The system can be described by the ordinary differential equations:</p><formula xml:id="formula_20">ẋ1 = (1 -u 1 )c 1 u 2 -c 2 √ x 1 , if x 1 ≤ 1 0 otherwise (17a) ẋ2 = c 1 u 1 u 2 + c 2 √ x 1 -c 2 √ x 2 , if x 2 ≤ 1 0 otherwise (17b)</formula><p>where the measured system states x 1 , and x 2 denote the liquid levels in first and second tank, respectively. The system has two control variables u 1 and u 2 representing the pump speed and valve opening, respectively. Similar to the CSTR system we generate a dataset of 10,000 time steps via simulation using the Scipy ODEInt solver.</p><p>The Aerodynamic Body exhibits oscillatory behavior when subject to symmetric disturbances. We use a model that predicts the acceleration and velocity of the body using the measurements of its velocities (translational and angular) and various angles related to its control surfaces. It has a large number of parameters, including 4 aerodynamic force coefficients, 11 aerodynamic momentum coefficients, 3 moments of inertia factors, and 4 other constant parameters. The system has 10 inputs, including the various angles of the control surfaces, angle of attack and sideslip, and the measured angular velocities about the three inertial axes. Its 5 outputs measure the angular velocities and accelerations about the lateral y and z direction. For further details about the model, we refer the reader to the dataset source <ref type="bibr" target="#b46">[46]</ref>. The dataset contains 501 input-output measurements sampled uniformly at 0.02 second intervals.</p><p>For each system, we group the time series of inputs and observed measurements into evenly split contiguous training (first 1 /3 of the data points), validation (next 1 /3 of the data points), and test sets (final 1 /3 of the data points). Models are optimized using the AdamW <ref type="bibr" target="#b45">[45]</ref> variant of gradient descent on the training set. For each system, we optimized at least 500 model configurations (obtained via directed and random hyperparameter searches) for 10,000 gradient descent updates. We select the best performing models on the open-loop MSE for the development set, and report performance results on the held-out test set. The hyperparameter space for the model configuration search ranged over i) block structure, ii) linear map prior, iii) nonlinear map type, iv) activation function type, v) weights on the multi-objective loss function, vi) gradient descent step size (from 3 × 10 -5 to 0.01) and Nstep prediction horizon, vii) estimator input history window, and viii) neural network width and depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussion</head><p>Table <ref type="table" target="#tab_1">II</ref> shows a comparison of our best performing systems models versus models trained with a single N -step objective loss (6) without constraints penalties with an unstructured neural network model. Table <ref type="table" target="#tab_2">III</ref> shows configuration details for the best performing models. The CSTR system shows an almost 200% improvement on open-loop MSE using a block nonlinear structured model and the constraint enforcing multiobjective loss <ref type="bibr" target="#b14">(15)</ref>. The Aerodynamic Body system shows  greater than 200% improvement using Spectral regularization for the linear maps of the neural network blocks, a learnable BLU activation function, and penalty constraints via multiobjective loss <ref type="bibr" target="#b14">(15)</ref>. The Two Tank system shows an order of magnitude improvement using the neural Hammerstein model with the multi-objective loss <ref type="bibr" target="#b14">(15)</ref>. For this system, the structured model generalizes well to the open loop evaluation task from the N -step training objective whereas the unstructured model does not. Figure <ref type="figure" target="#fig_2">3</ref> compares the traces of these learned models against the ground truth demonstrating that these quantitative gains translate to meaningful improvement in performance. The Two Tank and Aerodynamic Body systems are well-fit on the development set for both the constrained/structured and unconstrained/unstructured models. However, the unconstrained/unstructured models (second row) drift from the true trajectory on the unobserved test set. For the unstable highly nonlinear CSTR system, neither model reaches all the extreme values but the unconstrained/unstructured model has an evident bias for even the non-extreme states across the development and test sets.</p><p>As may be expected, shown in Figure <ref type="figure" target="#fig_1">2</ref>, particular systems favor particular block structured configurations, with the CSTR model preferring nonlinear maps for all block components, the Two Tank model preferring a Hammerstein model with a nonlinear map only on the input, and the Aerodynamic Body model preferring the model without block structure which allows for more direct nonlinear interaction between the input and state variables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>The above results suggest that constraint-enforcing objective terms and structural priors can have a dramatic effect on the ultimate model fit. In this section we perform studies to assess which modeling priors have greatest effect for each respective system. For each system, we conduct an ablation study in order to assess the effectiveness of constraint penalty terms and linear map parameterizations for modeling. For each system's best-observed architecture, we train 10 randomly initialized models with all the recorded constraint values, no constraints, and ablating each constraint in turn. Specifically, we train models setting Q con y = 0, Q dx = 0, Q con fu = 0, and with no inductive prior on the linear map. As an extreme case, we also show results for models trained without block structure or constraints penalties.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the test set open-loop MSE on the best performing models for each system given the different ablation conditions. Fully constrained and block-structured nonlinear models (All) perform better than unstructured unconstrained models (-All*). However, different combinations of constraints may improve performance depending on the system under consideration. For the Aerodynamic Body system, removing any one constraint degrades performance similar to removing all constraints with the exception of smoothing (Q dx ) which seems to have less effect. For the Two Tank system removing most individual constraints has little effect but removing the smoothing constraint can actually improve performance which aligns with the discontinuities in the underlying system. Performance on Two Tank appears to rely heavily on the Hammerstein block structure which results in a dramatic drop in performance when removed. The CSTR model performs worse when removing the input influence constraint (Q con fu ) or the block-nonlinear model structure, but can benefit when removing other constraints. Similar to the Two Tank model, the near discontinuities in the underlying system behavior make the smoothing term an ill-posed regularization constraint for this particular system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Eigenvalue Analysis</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> shows concatenated eigenvalue plots of the state transition maps of the best performing structured and constrained (top row) vs unstructured and unconstrained models (second row) for the CSTR, Two Tank, and Aerodynamic Body systems, respectively. We display only the eigenvalues of the linear maps for f x and f xu , respectively. Hence the damping effect of the used stable activation functions on the eigenvalue space is omitted in this analysis. Blue circles represent the stable regions of the complex plane. An interesting observation is that the best performing constrained and structured models (first row) always yield stable state transition weights. By intuition, the composition of stable functions yields the global stability of the learned system dynamics. A formal proof of this proposition will be subject to future work. On the other hand, the unstructured and unconstrained models (second row) sometimes learn weights with unstable eigenvalues, therefore, lose the global stability guarantee.</p><p>The eigenvalues of the linearized ground truth ODEs (bottom row) show the tendencies of each system over a range of steady-state conditions. The unstable oscillatory modes of CSTR are apparent in the original linearized system (bottom left) and explain why the stable eigenvalue spectrum of the learned block-structured model (top left) is not entirely capturing the oscillatory modes, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The small eigenvalues are expected for the Two Tank system (bottom center) because the system's energy drops rapidly without inlet flow. In contrast, the learned Hammerstein model (top center) has a dominant eigenvalue close to 1, which refers to the energy conversion property of the learned latent dynamics. However, thanks to the projection of the latent dynamics to the output space dynamics via linear f y map, the model is still accurately representing the actual system dynamics. The Aerodynamic Body model is linearized around its steady trimmed conditions and the resulting eigenvalues (bottom right) are very similar to the ones of the learned constrained block-structured model (top right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed constrained block nonlinear neural state space models for the identification of nonlinear dynamical systems, and a host of options for constructing such models. Analyses of three dynamical systems have exhibited the efficacy of this modeling approach by superior open-loop performance. In addition to learning accurate systems models from small measurement sets, our constrained and block-structured neural models can yield meaningful performance gains over their unconstrained and unstructured counterparts. Different model structures performed best for each test system, emphasizing the utility of our adaptable, modular deep system identification methodology. Our analysis of the learned system dynamics suggests that suitable constraints and structural priors can be chosen for effective data-driven modeling given some high-level intuition of the underlying system and lacking any specific knowledge of the governing equations or their functional forms. For future work, extended analysis of individual constraints and neural architecture choices on a broader set of non-autonomous and autonomous systems can provide further guidance on effective application of our presented methods for engineering practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block nonlinear neural state space model architecture for partially observable systems with Np-step past and N -step prediction horizons. Individual blocks can be parametrized by different structured linear maps with learnable activation functions.</figDesc><graphic coords="3,54.00,50.08,244.79,210.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance comparison across block structured models.</figDesc><graphic coords="5,313.20,50.08,244.80,155.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Open-loop traces of the best performing model configurations with constraints (top row) compared to unstructured models without constraints (bottom row) for the CSTR (left), Two Tank (center), and Aerodynamic Body (right) systems.</figDesc><graphic coords="6,75.71,163.48,151.20,113.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance comparison across models with ablated constraints.</figDesc><graphic coords="6,313.20,406.49,244.80,153.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Eigenvalue plots in the complex plane for the state transition map weights of the best performing models with constraints (first row) compared to unstructured and unconstrained (second row) for the CSTR (left), Two Tank (center), and Aerodynamic Body (right) systems. The third row plots eigenvalues of ground truth ODEs linearized across a range of initial conditions. Blue circles represent stable regions.</figDesc><graphic coords="7,128.64,223.96,115.92,86.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II TEST</head><label>II</label><figDesc>SET MSE OF BEST-OBSERVED STRUCTURED CONSTRAINED AND UNSTRUCTURED UNCONSTRAINED MODELS.</figDesc><table><row><cell>System</cell><cell>Structure</cell><cell>Constr.</cell><cell cols="2">N -step Open-loop</cell></row><row><cell>CSTR</cell><cell>Block-nonlinear Unstructured</cell><cell>Y N</cell><cell>0.00126 0.00477</cell><cell>0.00171 0.00496</cell></row><row><cell>TwoTank</cell><cell>Hammerstein Unstructured</cell><cell>Y N</cell><cell>0.00029 0.00047</cell><cell>0.00050 0.00542</cell></row><row><cell>Aero</cell><cell>Unstructured Unstructured</cell><cell>Y N</cell><cell>0.00144 0.00481</cell><cell>0.00153 0.00484</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CONFIGURATIONS</head><label>III</label><figDesc>OF BEST-OBSERVED MODELS FOR EACH SYSTEM.</figDesc><table><row><cell>System</cell><cell>Structure</cell><cell cols="2">State Est. LM</cell><cell>NLM</cell><cell cols="3">Layers Nodes Act.</cell><cell>LR</cell><cell>Q dx</cell><cell>Q con y</cell><cell>Q con f u</cell><cell>Qy</cell><cell>λ min</cell><cell>λmax</cell><cell>N</cell><cell>Np</cell></row><row><cell>CSTR</cell><cell cols="2">Block-nonlinear RNN</cell><cell>Linear</cell><cell cols="2">rMLP 6</cell><cell>60</cell><cell cols="2">GELU 1 × 10 -4</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell><cell>0.5</cell><cell>-</cell><cell>-</cell><cell>64</cell><cell>1</cell></row><row><cell cols="2">TwoTank Hammerstein</cell><cell>rMLP</cell><cell>Linear</cell><cell cols="2">rMLP 3</cell><cell>40</cell><cell cols="2">GELU 3 × 10 -4</cell><cell>0.3</cell><cell>0.3</cell><cell>0.1</cell><cell>0.5</cell><cell>-</cell><cell>-</cell><cell>64</cell><cell>4</cell></row><row><cell>Aero</cell><cell>Unstructured</cell><cell>RNN</cell><cell cols="2">Spectral rMLP</cell><cell>2</cell><cell>25</cell><cell>BLU</cell><cell>0.01</cell><cell>0.2</cell><cell>0.2</cell><cell>-</cell><cell>0.5</cell><cell>0.4</cell><cell>0.7</cell><cell>16 2</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nonlinear systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><surname>Khalil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Structural Priors in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yani</surname></persName>
		</author>
		<author>
			<persName><surname>Ioannou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlinear system identification: A useroriented road map</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schoukens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="28" to="99" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to Block-oriented Nonlinear Systems</title>
		<author>
			<persName><forename type="first">Er-Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fouad</forename><surname>Giri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Block-oriented Nonlinear System Identification</title>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<editor>
			<persName><forename type="first">Fouad</forename><surname>Giri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Er-Wei</forename><surname>Bai</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Volterra series and geometric control theory</title>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">W</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="176" />
			<date type="published" when="1976-03">March 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Application of a neuro-fuzzy model to landslide-susceptibility mapping for shallow landslides in a tropical hilly area</title>
		<author>
			<persName><forename type="first">Hyun-Joo</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswajeet</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1264" to="1276" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear gas turbine modeling using narmax structures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="893" to="898" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wiener-hammerstein systems modeling using diagonal volterra kernels coefficients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kibangou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Favier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="384" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the wiener and hammerstein models for power amplifier predistortion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gilabert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asia-Pacific Microwave Conference Proceedings</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4" to="2005" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frequency domain identification of hammerstein models</title>
		<author>
			<persName><forename type="first">Er-Wei</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="530" to="542" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identification of hammerstein models with cubic spline nonlinearities</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Westwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear system identification of hydraulic actuator. friction dynamics using a hammerstein model</title>
		<author>
			<persName><forename type="first">Byung-Jae</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Yagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP &apos;98 (Cat. No.98CH36181)</title>
		<meeting>the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP &apos;98 (Cat. No.98CH36181)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1933" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gray-box identification of block-oriented nonlinear models</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">K</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pottmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Process Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-linear system identification using neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1214" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A recurrent neural-networkbased real-time learning control strategy applying to nonlinear systems with unknown dynamics</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Tommy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on industrial electronics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear system identification for predictive control using continuous time recurrent neural networks and automatic differentiation</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Seyab</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Process Control</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="568" to="581" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear system identification using discrete-time recurrent neural networks with stable learning algorithms</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information sciences</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="131" to="147" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlinear system identification with recurrent neural networks and dead-zone kalman filter algorithm</title>
		<author>
			<persName><forename type="first">José</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2460" to="2466" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new concept using lstm neural networks for dynamic system identification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 American Control Conference (ACC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5324" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic model identification of unmanned surface vehicles using deep learning network</title>
		<author>
			<persName><forename type="first">Joohyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanwoo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nakwan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Ocean Research</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="123" to="133" />
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Neural Implementation of the Kalman Filter</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leif</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2062" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory kalman filters: Recurrent neural estimators for pose regularization</title>
		<author>
			<persName><forename type="first">Huseyin</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning deep neural network representations for koopman operators of nonlinear</title>
		<author>
			<persName><forename type="first">Enoch</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Hodas</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep kalman filters. stat, 1050:25, 2015. dynamical systems</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 American Control Conference (ACC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4832" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-driven discovery of coordinates and governing equations</title>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Lusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="22445" to="22451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Physics informed deep learning (part I): data-driven solutions of nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno>CoRR, abs/1711.10561</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01236</idno>
		<idno>arXiv: 1801.01236</idno>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note>nlin, physics:physics, stat</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">DeepXDE: A deep learning library for solving differential equations</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno>CoRR, abs/1907.04502</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">IMEXnet: A forward stable deep neural network</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keegan</forename><surname>Lensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Treister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903">1903.02639, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<idno>CoRR, abs/1705.03341</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Structured inference networks for nonlinear state space models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<idno>CoRR, abs/1811.04551</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning nonlinear state-space models using deep autoencoders</title>
		<author>
			<persName><forename type="first">D</forename><surname>Masti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bemporad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3862" to="3867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Syama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">W</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7785" to="7794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Nonlinear Systems Identification Using Deep Dynamic Neural Networks</title>
		<author>
			<persName><forename type="first">Olalekan</forename><surname>Ogunmolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01439</idno>
		<idno>arXiv: 1610.01439</idno>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nonlinear systems identification using deep dynamic neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Olalekan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejun</forename><surname>Ogunmolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Gans</surname></persName>
		</author>
		<idno>CoRR, abs/1610.01439</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A hammerstein-wiener recurrent neural network with universal approximation capability</title>
		<author>
			<persName><forename type="first">Jeen-Shing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2008-10">Oct 2008</date>
			<biblScope unit="page" from="1832" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR, abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An evaluation of parametric activation functions for deep learning</title>
		<author>
			<persName><forename type="first">Godfrey</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3006" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stabilizing gradients for deep neural networks via efficient svd parameterization</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5806" to="5814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<ptr target="https://www.mathworks.com/help/ident/ug/modeling-an-aerodynamic-body.html" />
		<title level="m">Modeling an Aerodynamic Body</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
