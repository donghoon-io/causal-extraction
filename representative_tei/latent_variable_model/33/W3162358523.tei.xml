<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIRECTIONAL ASR: A NEW PARADIGM FOR E2E MULTI-SPEAKER SPEECH RECOGNITION WITH SOURCE LOCALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-10-30">30 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shanmugam</forename><surname>Aswin</surname></persName>
						</author>
						<author>
							<persName><surname>Subramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD Tencent</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD Tencent</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DIRECTIONAL ASR: A NEW PARADIGM FOR E2E MULTI-SPEAKER SPEECH RECOGNITION WITH SOURCE LOCALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-30">30 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2011.00091v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>source localization</term>
					<term>source separation</term>
					<term>end-toend speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new paradigm for handling far-field multispeaker data in an end-to-end neural network manner, called directional automatic speech recognition (D-ASR), which explicitly models source speaker locations. In D-ASR, the azimuth angle of the sources with respect to the microphone array is defined as a latent variable. This angle controls the quality of separation, which in turn determines the ASR performance. All three functionalities of D-ASR: localization, separation, and recognition are connected as a single differentiable neural network and trained solely based on ASR error minimization objectives. The advantages of D-ASR over existing methods are threefold: (1) it provides explicit speaker locations, (2) it improves the explainability factor, and (3) it achieves better ASR performance as the process is more streamlined. In addition, D-ASR does not require explicit direction of arrival (DOA) supervision like existing data-driven localization models, which makes it more appropriate for realistic data. For the case of two source mixtures, D-ASR achieves an average DOA prediction error of less than three degrees. It also outperforms a strong far-field multi-speaker end-to-end system in both separation quality and ASR performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Source localization methods to estimate the direction of the sound sources with respect to the microphone array is an important frontend for many downstream applications. For example, they are an indispensable part of robot audition systems <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> that facilitates interaction with humans. Source localization is used as a pivotal component in the robot audition pipeline to aid source separation and recognition. Recently far-field systems are being designed to process multi-talker conversations like meeting <ref type="bibr" target="#b3">[3]</ref> and smart speaker scenarios <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5]</ref>. Incorporating source localization functionality can enrich such systems by monitoring the location of speakers and also potentially help improve the performance of the downstream automatic speech recognition (ASR) task. Assuming the direction of arrival (DOA) is known, the effectiveness of using features extracted from the ground-truth location for target speech extraction and recognition was shown in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>.</p><p>Similar to other front-end tasks, signal processing has been traditionally used to estimate the DOA <ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>. Wideband subspace methods like test of orthogonality of projected subspaces (TOPS) have shown promising improvements over narrowband subspace methods like multiple signal classification (MUSIC) but most existing signal processing approaches are not robust to reverberations <ref type="bibr" target="#b11">[11]</ref>. Supervised deep learning methods have been successful in making the DOA estimation more robust <ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref>. However, for real multi-source data like CHiME-5 <ref type="bibr" target="#b4">[4]</ref>, it is very difficult to get the parallel ground-truth DOA. In such cases, it might be possible to use labels from a different view that are easier to annotate and use it to indirectly optimize the localization model parameters.</p><p>There is growing interest in optimizing the front-end speech processing systems with applications-oriented objectives. For example, speech enhancement and separation systems have been trained based on ASR error minimization <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref>. The front-end in such systems was encompassed into the ASR framework to train them without parallel clean speech data by using the text transcription view as the labels. MIMO-Speech <ref type="bibr" target="#b20">[20]</ref> is a multichannel end-to-end (ME2E) neural network that defines source-specific time-frequency (T-F) masks as latent variables in the network which in turn are used to transcribe the individual sources.</p><p>Although MIMO-Speech might implicitly learn localization information, it might not be consistent across frequencies because of narrowband approximation. We propose to further upgrade the explainability factor of the MIMO-Speech system by realizing an explicit source localization function. We call our novel technique directional ASR (D-ASR), which is a new paradigm of joint separation and recognition explicitly driven by source localization. The masking network in MIMO-Speech is expected to directly predict an information rich T-F mask. It is hard to accurately estimate such a mask without the reference signals. In D-ASR, this masking network is replaced with a simpler component that discretizes the possible DOA azimuth angles and estimates the posterior of these angles for each source. This estimated angle can be in turn converted to steering vectors and T-F masks, thereby making the localization function tightly coupled with the other two functionalities of D-ASR. This streamlining makes this method more effective. The estimated angle is tied across the steering vector for all frequency bands and hence this method also has wideband characteristics.</p><p>Although D-ASR is trained with only the ASR objective, its evaluation is performed across all three of its expected functionalities. This is possible as the localization and separation intermediate outputs are interpretable in our formulation. The L1 prediction error is used as the DOA metric with MUSIC and TOPS as baselines for this comparison. Source separation and ASR performance are compared with the MIMO-Speech model <ref type="bibr" target="#b21">[21]</ref> with objective signal quality and transcription based metrics, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DIRECTIONAL ASR</head><p>A block diagram of the proposed D-ASR architecture is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The architecture has five blocks and a detailed formulation of these blocks is given in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Localization Subnetwork</head><p>The first block is the localization subnetwork and its goal is to estimate the azimuth angle of the sources. This block in turn has three components: (1) CNN-based phase feature extraction, (2) phase feature masking and, (3) temporal averaging.</p><p>Let Y1, Y2, • • • , YM be the M -channel input signal in the shorttime Fourier transform (STFT) domain, with Ym ∈ C T ×F , where T is the number of frames and F is the number of frequency components. The input signal is reverberated, consisting of N speech sources with no noise. We assume that N is known. The phase spectrum of the multichannel input signal is represented as P ∈ [0, 2π] T ×M ×F . This raw phase P is passed through the first component of the localization network based on CNN given by LocNet-CNN(•) to extract phase feature Z by pooling the channels as follows:</p><formula xml:id="formula_0">Z = LocNet-CNN(P) ∈ R T ×Q , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Q is the feature dimension. Phase feature Z will have DOA information about all the sources in the input signal. This is processed by the next component LocNet-Mask(•) which is a recurrent layer. This component is used to extract source-specific binary masks as follows,</p><formula xml:id="formula_2">[W n ] N n=1 = σ(LocNet-Mask(Z)),<label>(2)</label></formula><p>where W n ∈ [0, 1] T ×Q is the feature mask for source n and σ(•) is the sigmoid activation. This mask segments Z into regions that correspond to each source.</p><p>In this work, we assume that the DOA does not change for the whole utterance. So in the next step we use the extracted phase mask from Eq. ( <ref type="formula" target="#formula_2">2</ref>) to perform a weighted averaging of the phase feature from Eq. ( <ref type="formula" target="#formula_0">1</ref>) to get source-specific summary vectors. This summary vector should encode the DOA information specific to a source and that is why the mask is used as weights to summarize information only from the corresponding source regions.</p><formula xml:id="formula_3">ξ n (q) = T t=1 w n (t, q)z(t, q) T t=1 w n (t, q) ,<label>(3)</label></formula><p>where ξ n (q) is the summary vector for source n at dimension q, w n (t, q) ∈ [0, 1] and z(t, q) ∈ R are the extracted feature mask (for source n) and the phase feature, respectively, at time t and feature dimension q. The summary vector, represented in vector form as ξ n ∈ R Q is passed through a learnable AffineLayer(•), which converts the summary vector from dimension Q to the dimension 360/γ , where γ is the angle resolution in degrees to discretize the DOA angle. Based on this discretization, we can predict the DOA angle as a multi-class classifier with the softmax operation. From this, we can get the source-specific posterior probability for the possible angle classes as follows,</p><formula xml:id="formula_4">[Pr(θ n = αi|P)] 360/γ i=1 = Softmax(AffineLayer(ξ n )),<label>(4) θn</label></formula><formula xml:id="formula_5">= 360/γ i=1 Pr(θ n = αi|P)αi,<label>(5)</label></formula><formula xml:id="formula_6">αi = (γ * i) -(γ -1)/2 π/180 ,<label>(6)</label></formula><p>where θn is the DOA estimated for source n by performing a weighted sum using the estimated posteriors from Eq. ( <ref type="formula" target="#formula_4">4</ref>) and αi is the angle in radians corresponding to the class i. We define the composite function of Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_6">6</ref>) with learnable parameter Λloc as follows:</p><p>[ θn ] N n=1 = Loc(P; Λloc). ( <ref type="formula">7</ref>) Note that all of the functions in this section are differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Steering Vector &amp; Localization Mask</head><p>In this step, the estimated DOAs from Eq. ( <ref type="formula">7</ref>) is converted to a steering vector and also optionally to a time-frequency mask to be used for beamforming. First, the steering vector d n (f ) ∈ C M for source n and frequency f is calculated from estimated DOA θn . In this work we have used uniform circular arrays (UCA) and the steering vector is calculated as follows,</p><formula xml:id="formula_7">τ n m = r c cos( θn -ψm), m = 1 : M<label>(8)</label></formula><formula xml:id="formula_8">d n (f ) = [e j2πf τ n 1 , e j2πf τ n 2 , ..., e j2πf τ n M ],<label>(9)</label></formula><p>where τ n m is the signed time delay between the m-th microphone and the center for source n, ψi is the angular location of microphone m, r is the radius of the UCA and c is the speed of sound (343 m/s).</p><p>The estimated steering vectors can be sufficient to separate the signals. However, depending on the beamformer used subsequently, we can also use a time-frequency mask l n (t, f ) ∈ [0, 1) and it is estimated as,</p><formula xml:id="formula_9">a n (t, f ) = |d n (f ) H y(t, f )| 2 , (<label>10</label></formula><formula xml:id="formula_10">) [ν n (t, f )] N n=1 = Softmax([a n (t, f )] N n=1 ),<label>(11)</label></formula><formula xml:id="formula_11">l n (t, f ) = 1 (1 -κ) * ReLU(ν n (t, f ) -κ),<label>(12)</label></formula><p>where κ ∈ [0, 1) is a sparsity constant, H is conjugate transpose, y(t, f ) ∈ C M is the multichannel input at time t and frequency f . An initial estimate of the mask is extracted by passing the directional power spectrum a n (t, f ) through a softmax function over the source dimension using Eq. ( <ref type="formula" target="#formula_10">11</ref>). The mask is further refined using Eq. ( <ref type="formula" target="#formula_11">12</ref>) to create sparsity and we define this output as the localization mask. Again, all of the operations in this section are differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Differentiable Beamformer</head><p>As we have both steering vectors from Eq. ( <ref type="formula" target="#formula_8">9</ref>) and masks from Eq. <ref type="bibr" target="#b12">(12)</ref>, there are many possible beamformers that could be used. We experimented with three options. First, linearly constrained minimum power (LCMP) beamformer <ref type="bibr" target="#b22">[22]</ref> given by b n LCMP (f ) ∈ C M for source n and frequency f , is estimated by solving the following equation which requires only the steering vectors,</p><formula xml:id="formula_12">b n LCMP (f ) = Φy(f ) -1 G(f )[G(f ) H Φy(f ) -1 G(f )] -1 µ n , (<label>13</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">Φy(f ) = 1 T T t=1 y(t, f )y(t, f ) H ,<label>(14)</label></formula><p>where Φy(f ) ∈ C M ×M is the input spatial covariance matrix (SCM) at frequency f . G(f ) ∈ C M ×N is the constraint matrix whose columns are the estimated steering vectors at frequency f , such that the n-th column is d n (f ). µ n ∈ {0, 1} N is a one-hot vector with the n-th element as 1. Alternative to LCMP, we can use MVDR beamformer. The localization masks from Eq. ( <ref type="formula" target="#formula_11">12</ref>) are used to compute the source specific SCM, Φ n (f ) as follows:</p><formula xml:id="formula_15">Φ n (f ) = 1 T t=1 l n (t, f ) T t=1 l n (t, f )y(t, f )y(t, f ) H ,<label>(15)</label></formula><p>The interference SCM, Φ n intf (f ) for source n is approximated as <ref type="bibr" target="#b20">[20]</ref> (we experiment only with N = 2, so no summation in that case). From the computed SCMs, the M -dimensional complex MVDR beamforming filter for source n and frequency f ,</p><formula xml:id="formula_16">N i =n Φ i (f ) like</formula><formula xml:id="formula_17">b n MVDR (f ) ∈ C M is estimated as, b n MVDR (f ) = [Φ n intf (f )] -1 d n (f ) d n (f ) H [Φ n intf (f )] -1 d n (f ) . (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>We can also use the MVDR formulation based on reference selection <ref type="bibr" target="#b23">[23]</ref>, in which case the steering vectors will not be used explicitly.</p><p>We refer to this beamformer as MVDR-REF and it is estimated as,</p><formula xml:id="formula_19">b n MVDR-REF (f ) = [Φ n intf (f )] -1 Φ n (f ) Tr([Φ n intf (f )] -1 Φ n (f )) u,<label>(17)</label></formula><p>where u ∈ {0, 1} M is a one-hot vector to choose a reference microphone and Tr(•) denotes the trace operation.</p><p>Once we obtain the beamforming filters from either Eq. ( <ref type="formula" target="#formula_12">13</ref>), Eq. ( <ref type="formula" target="#formula_17">16</ref>) or Eq. ( <ref type="formula" target="#formula_19">17</ref>), we can perform speech separation to obtain the n-th separated STFT signal, x n (t, f ) ∈ C as follows:</p><formula xml:id="formula_20">x n (t, f ) = b n (f ) H y(t, f ),<label>(18)</label></formula><p>where b n (f ) can be either of LCMP, MVDR or MVDR-REF beamforming coefficients. Again, all the operations in this section are also differentiable. One added advantage of our approach is that we can use a different beamformer during inference to what was used during training. Irrespective of the beamformer used while training, we found that using MVDR-REF while inference gives better separation and ASR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Feature Transformation &amp; ASR</head><formula xml:id="formula_21">O n = MVN(Log(MelFilterbank( X n )))<label>(19)</label></formula><formula xml:id="formula_22">C n = ASR(O n ; Λasr).<label>(20)</label></formula><p>The separated signal for source n from Eq. ( <ref type="formula" target="#formula_20">18</ref>), represented in matrix form as X n ∈ C T ×F is transformed to a feature suitable for speech recognition by performing log Mel filterbank transformation and utterance based mean-variance normalization (MVN). The extracted feature O n for source n is passed to the speech recognition subnetwork ASR(•) with learnable parameter Λasr to get</p><formula xml:id="formula_23">C n = (c n 1 , c n 2 , • • • )</formula><p>, the token sequence corresponding to source n. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, all the components are connected in a computational graph and trained solely based on the ASR objective to learn both Λloc in Eq. <ref type="bibr" target="#b7">(7)</ref> and Λasr in Eq. ( <ref type="formula" target="#formula_22">20</ref>) with the reference text transcriptions [C i ref ] N i=1 as the target. The joint connectionist temporal classification (CTC)/attention loss <ref type="bibr" target="#b24">[24]</ref> is used as the ASR optimization criteria. We use the permutation invariant training (PIT) scheme similar to MIMO-Speech <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref> to resolve the prediction-target token sequence assignment problem. Optionally, a regularization cross entropy loss for the posteriors in Eq. ( <ref type="formula" target="#formula_4">4</ref>) with a uniform probability distribution as the target can be added. This is to discourage a very sparse posterior distribution and encourage interpolation in Eq. ( <ref type="formula" target="#formula_5">5</ref>), especially when high values of γ are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data &amp; Setup</head><p>We simulated 2-speaker mixture data using clean speech from the subset WSJ0 of the wall street journal (WSJ) corpus <ref type="bibr" target="#b25">[25]</ref>. For each utterance, we mixed another utterance from a different speaker within the same set, so the resulting simulated data is the same size as the original clean data with 12,776 (si tr s), 1,206 (dt 05), and 651 (et 05) utterances for the training, development, and test set respectively. The SMS-WSJ <ref type="bibr" target="#b26">[26]</ref> toolkit was used for creating the simulated data with maximum overlap. Image method <ref type="bibr" target="#b27">[27]</ref> was used to create the room impulse responses (RIR). Room configurations with the size (length-width-height) ranging from 5m-5m-2.6m to 11m-11m-3.4m were used. A uniform circular array (UCA) with a radius of 5 cm was used. The reverberation time (T60) was sampled uniformly between 0.15s and 0.5s. The two speech sources were placed randomly at a radius of 1.5-3m around the microphone-array center.</p><p>Three CNN layers with rectified linear unit (ReLU) activation followed by a feedforward layer were used as LocNet-CNN(•) defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Q was fixed as "2 × 360/γ ". One output gate projected bidirectional long short-term memory (BLSTMP) layer with Q cells was used as LocNet-Mask(•) defined in Eq. ( <ref type="formula" target="#formula_2">2</ref>). κ in Eq. ( <ref type="formula" target="#formula_11">12</ref>) was fixed as "0.5". The encoder-decoder ASR network was based on the Transformer architecture <ref type="bibr" target="#b28">[28]</ref> and it is initialized with a pretrained model that used single speaker training utterances from both WSJ0 and WSJ1. Attention/CTC joint ASR decoding was performed with score combination with a word-level recurrent language model from <ref type="bibr" target="#b29">[29]</ref> trained on the text data from WSJ. Our implementation was based on ESPnet <ref type="bibr" target="#b30">[30]</ref>.</p><p>The input signal was preprocessed with weighted prediction error (WPE) <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref> based dereverberation with a filter order of "10" and prediction delay "3", only during inference time . The second channel was fixed as the reference microphone for MVDR-REF in Eq. <ref type="bibr" target="#b17">(17)</ref>. The recently proposed MIMO-Speech with BLSTM front-end and Transformer back-end <ref type="bibr" target="#b21">[21]</ref> was used as the 2-source ASR baseline. We used the same ASR subnetwork architecture as D-ASR in MIMO-Speech for a fair comparison. Signal processing based DOA estimation was performed with "Pyroomacoustics" toolkit <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Source Localization Performance</head><p>The estimated azimuth angles (DOA) from our proposed D-ASR method are extracted as intermediate outputs from Eq. <ref type="bibr" target="#b7">(7)</ref>. We use two popular subspace-based signal processing methods MUSIC and TOPS as baselines. For both, the spatial response is computed and the top two peaks are detected to estimate the DOA. The average absolute cyclic angle difference between the predicted angle and the ground-truth angle in degrees is used as the metric. The permutation of the prediction with the reference that gives the minimum error is chosen.</p><p>The results with and without WPE preprocessing are given in Table <ref type="table" target="#tab_0">1</ref>. Results of the D-ASR method with all three possible beamformers while training and different angle resolutions (γ) are shown. All configurations of D-ASR outperforms the baselines. Specifically, LCMP version is significantly better. In the LCMP version of D-ASR, having a higher γ of 10 works best and it is also robust without WPE. The results of LCMP D-ASR with γ = 5 is shown with and without the additional regularization loss (cross-entropy with uniform distribution as target). Although this regularization doesn't change the performance much, it helps in faster convergence during training.   <ref type="formula" target="#formula_4">4</ref>) were 1-hot vectors, there would have been a discretization error of 5 • (for γ=10) but our prediction error is less than this. This shows that the network uses the posteriors as interpolation weights and hence it is not bounded by the discretization error. Figure <ref type="figure" target="#fig_1">2</ref> shows the localization output for a test example (note that only half of the possible angles are shown as in this case all the predictions were in the top semi-circle). TOPS predicts one of the sources which is at 148 • with good precision like the two D-ASR LCMP systems shown but has about a 15 • error in predicting the other source at 50 • , which the D-ASR systems can estimate again with good precision. The posteriors of D-ASR (γ = 5) trained without the regularization loss predicts a very sparse distribution while for the one with regularization loss and γ = 10, the distribution is very dense but the prediction with the weighted sum is a bit more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ASR &amp; Source Separation Performance</head><p>Word error rate (WER) is used as the metric for ASR. The signal-todistortion ratio (SDR) and perceptual evaluation of speech quality (PESQ) scores computed with the dry signal as the reference are used as the metric for source separation. The results are given in Table 2. The LCMP based D-ASR with γ = 10 is used for this evaluation as that performs the best in terms of DOA error. We observed the steering vector based LCMP beamformer to be good at training the D-ASR network as that ensures tighter coupling with the localization subnetwork but once the network parameters are learned it was better to replace the beamformer to be mask-based for inference. So during inference MVDR-REF beamformer was used as mentioned in Section 2.4. The results of the clean single-speaker data with the pre- trained ASR model is given in the first row to show a bound for the 2-mix ASR performance. The ASR results of the simulated mixture using the single-speaker model is also shown (WER more than 100% because of too many insertion errors). We perform oracle experiments by directly giving the reference masks to the beamformer. We define an ideal localization mask (ILM) which is basically Eq. ( <ref type="formula" target="#formula_11">12</ref>) calculated by using the ground-truth DOA. The ILM is compared with the ideal binary mask (IBM) that is calculated with the reference clean signal magnitude. The ILM system is slightly worse than IBM but the results show it is a good approximation for IBM, which justifies our approach of obtaining masks from the DOA. Our proposed D-ASR method outperforms MIMO-Speech with the added advantage of also predicting DOA information. The SDR scores of D-ASR is almost as good as the oracle IBM and it is far superior to MIMO-Speech. An example of the estimated masks comparing D-ASR with MIMO-Speech is shown in Figure <ref type="figure" target="#fig_3">3</ref>. In MIMO-Speech as the spectral masks are estimated directly, it is hard for the network to learn this without reference signals. Our approach can provide better masks because we simplify the front-end to just predict angles and then approximate the mask from it. Some audio examples for demonstration are given in <ref type="url" target="https://sas91.github.io/DASR.html">https://sas91. github.io/DASR.html</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper proposes a novel paradigm to drive far-field speech recognition through source localization. It also serves as a method to learn multi-source DOA from only the corresponding text transcriptions. D-ASR not only makes the existing approaches like MIMO-Speech more interpretable but also performs better in terms of both ASR and speech separation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Proposed D-ASR architecture for 2-speaker scenario. The DOA estimates θ1 and θ2 corresponding to the two sources are obtained as intermediate outputs. The yellow blocks have learnable parameters.</figDesc><graphic coords="2,319.57,72.00,235.08,74.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Localization output given by D-ASR &amp; TOPS for a test example. Only the first two quadrants are shown. D-ASR LCMP with γ = 5 here is without uniform regularization</figDesc><graphic coords="4,74.62,229.43,203.40,141.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Input overlapping speech (b) Reference source-1 (c) MIMO-Speech Source-1 mask (d) D-ASR Source-1 Mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Intermediate T-F mask corresponding to one of the sources, estimated by D-ASR and MIMO-Speech for a test 2-spkr mix utterance in (a). The D-ASR mask in (d) captures the corresponding reference in (b) far better than the MIMO-Speech mask in (c).</figDesc><graphic coords="4,318.35,508.54,113.39,70.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>DOA Prediction Error on our simulated 2-source mixture comparing our proposed D-ASR method with subspace methods</figDesc><table><row><cell>Method</cell><cell>Training Beamformer</cell><cell>γ</cell><cell>Uniform Regulariza-tion</cell><cell cols="4">Average L1 Error (degree) No WPE WPE Dev Test Dev Test</cell></row><row><cell>MUSIC</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell cols="4">22.5 24.0 15.6 14.4</cell></row><row><cell>MUSIC</cell><cell>-</cell><cell>10</cell><cell>-</cell><cell cols="4">23.2 21.7 13.5 12.6</cell></row><row><cell>TOPS</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell cols="4">20.0 18.7 11.7 10.1</cell></row><row><cell>TOPS</cell><cell>-</cell><cell>10</cell><cell>-</cell><cell cols="4">20.4 19.4 13.3 10.6</cell></row><row><cell>D-ASR</cell><cell>LCMP</cell><cell>1</cell><cell></cell><cell cols="2">20.3 20.8</cell><cell>8.3</cell><cell>7.0</cell></row><row><cell>D-ASR</cell><cell>LCMP</cell><cell>5</cell><cell></cell><cell>3.9</cell><cell>3.6</cell><cell>3.5</cell><cell>3.0</cell></row><row><cell>D-ASR</cell><cell>LCMP</cell><cell>5</cell><cell></cell><cell>8.6</cell><cell>7.7</cell><cell>3.7</cell><cell>3.5</cell></row><row><cell>D-ASR</cell><cell>LCMP</cell><cell>10</cell><cell></cell><cell>3.0</cell><cell>3.0</cell><cell>2.5</cell><cell>2.5</cell></row><row><cell>D-ASR</cell><cell>MVDR</cell><cell>5</cell><cell></cell><cell cols="2">11.5 10.4</cell><cell>7.4</cell><cell>7.1</cell></row><row><cell>D-ASR</cell><cell>MVDR</cell><cell>10</cell><cell></cell><cell cols="2">20.8 19.9</cell><cell>7.4</cell><cell>6.9</cell></row><row><cell cols="2">D-ASR MVDR-REF</cell><cell>5</cell><cell></cell><cell cols="4">14.2 14.5 10.2 10.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ASR &amp; Speech Separation Performance comparing our proposed D-ASR method with MIMO-Speech. For WER, lower the better and for SDR &amp; PESQ, higher the better.</figDesc><table><row><cell></cell><cell cols="2">WER (%)</cell><cell cols="2">SDR (dB)</cell><cell cols="2">PESQ</cell></row><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell cols="3">Test Dev Test</cell></row><row><cell>Clean</cell><cell>2.1</cell><cell>1.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Input mixture (ch-1)</cell><cell cols="3">105.0 107.1 -0.2</cell><cell>-0.2</cell><cell>1.9</cell><cell>1.9</cell></row><row><cell>Oracle binary Mask (IBM)</cell><cell>3.6</cell><cell>3.0</cell><cell cols="2">15.7 15.5</cell><cell>2.9</cell><cell>2.9</cell></row><row><cell>Oracle DOA Mask (ILM)</cell><cell>4.3</cell><cell>3.7</cell><cell cols="2">15.3 15.3</cell><cell>2.9</cell><cell>2.9</cell></row><row><cell>MIMO-Speech</cell><cell>6.6</cell><cell>5.1</cell><cell cols="2">11.2 11.7</cell><cell>2.6</cell><cell>2.7</cell></row><row><cell>D-ASR</cell><cell>5.1</cell><cell>4.1</cell><cell cols="2">15.2 15.2</cell><cell>2.9</cell><cell>2.9</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design and implementation of robot audition system &apos;HARK&apos;open source software for listening to three simultaneous speakers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="739" to="761" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time auditory and visual multiple-object tracking for humanoids</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hidai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mizoguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Okuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1425" to="1432" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Advances in online audio-visual meeting transcription</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abramovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aksoylar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The fifth CHiME speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1561" to="1565" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech processing for digital home assistants: Combining signal processing with deep-learning techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multichannel overlapped speech recognition with location guided speech extraction network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SLT Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Farfield location guided target speech extraction using end-to-end speech recognition objectives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7299" to="7303" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple emitter location and signal parameter estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on antennas and propagation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="280" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TOPS: new doa estimator for wideband signals</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Yeo-Sun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1977" to="1989" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust localization and tracking of multiple speakers in real environments for binaural robot audition</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-speaker DOA estimation using deep convolutional networks trained with noise signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="8" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>EUSIPCO</publisher>
			<biblScope unit="page" from="1462" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sound source localization using deep learning models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics and Mechatronics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Microphone array processing for robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Pittsburgh PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint noise adaptive training for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2504" to="2508" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multichannel end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2632" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beamnet: End-to-end training of a beamformer-supported multi-channel ASR system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanebrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5325" to="5329" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-toend multi-speaker speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4819" to="4823" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech enhancement using end-to-end speech recognition objectives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="page" from="229" to="233" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="237" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-toend multi-speaker speech recognition with transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6134" to="6138" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A consolidated perspective on multimicrophone speech enhancement and source separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovich-Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on ASLP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="730" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On optimal frequencydomain multichannel linear filtering for noise reduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Souden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on ASLP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4835" to="4839" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based CSR corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13934</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A comparative study on Transformer vs RNN in speech applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ASRU</publisher>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with word-based RNN language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE SLT Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech dereverberation based on variance-normalized delayed linear prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on ASLP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1717" to="1731" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NARA-WPE: A Python package for weighted prediction error dereverberation in Numpy and Tensorflow for online and offline processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITG Fachtagung Sprachkommunikation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyroomacoustics: A python package for audio room simulation and array processing algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bezzam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dokmanić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="351" to="355" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
