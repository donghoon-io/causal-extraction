<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Robust Adversarial Ensemble with Causal (Feature Interaction) Interpretations for Image Classification</title>
				<funder ref="#_UAffZnC">
					<orgName type="full">Center for Connected Multimodal Mobility</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-31">December 31, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automotive Engineering</orgName>
								<orgName type="institution">Clemson University</orgName>
								<address>
									<addrLine>4 Research Drive</addrLine>
									<postCode>29607</postCode>
									<settlement>Greenville</settlement>
									<region>South Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pierluigi</forename><surname>Pisu</surname></persName>
							<email>pisup@clemson.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Automotive Engineering</orgName>
								<orgName type="institution">Clemson University</orgName>
								<address>
									<addrLine>4 Research Drive</addrLine>
									<postCode>29607</postCode>
									<settlement>Greenville</settlement>
									<region>South Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gurcan</forename><surname>Comert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Physics &amp; Engineering</orgName>
								<orgName type="institution">Benedict College</orgName>
								<address>
									<addrLine>1600 Harden Street</addrLine>
									<postCode>29204</postCode>
									<settlement>Columbia</settlement>
									<region>South Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Negash</forename><surname>Begashaw</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Physics &amp; Engineering</orgName>
								<orgName type="institution">Benedict College</orgName>
								<address>
									<addrLine>1600 Harden Street</addrLine>
									<postCode>29204</postCode>
									<settlement>Columbia</settlement>
									<region>South Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Varghese</forename><surname>Vaidyan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Cyber Sciences</orgName>
								<orgName type="institution">Dakota State University</orgName>
								<address>
									<addrLine>820 N Washington Avenue South Dakota</addrLine>
									<postCode>57042</postCode>
									<settlement>Madison</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nina</forename><forename type="middle">Christine</forename><surname>Hubig</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Clemson University</orgName>
								<address>
									<addrLine>821 McMillan Road</addrLine>
									<postCode>29634</postCode>
									<settlement>Clemson</settlement>
									<region>South Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Robust Adversarial Ensemble with Causal (Feature Interaction) Interpretations for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-31">December 31, 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.20025v1[cs.CV]</idno>
					<note type="submission">Preprint submitted to arXiv</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pattern recognition</term>
					<term>image classification</term>
					<term>adversarial attacks</term>
					<term>generative classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based discriminative classifiers, despite their remarkable success, remain vulnerable to adversarial examples that can mislead model predictions. While adversarial training can enhance robustness, it fails to address the intrinsic vulnerability stemming from the opaque nature of these black-box models. We present a deep ensemble model that combines discriminative features with generative models to achieve both high accuracy and adversarial robustness. Our approach integrates a bottom-level pre-trained discriminative network for feature extraction with a top-level generative classification network that models adversarial input distributions through a deep latent variable model. Using variational Bayes, our model achieves superior robustness against white-box adversarial attacks without adversarial training. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate our model's superior adversarial robustness. Through evaluations using counterfactual metrics and feature interaction-based metrics, we establish correlations between model interpretability and adversarial robustness. Additionally, preliminary results on Tiny-ImageNet validate our approach's scalability to more complex datasets, offering a practical solution for developing robust image classification models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, various deep learning-based discriminative models have made significant progress in image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, such as VGG <ref type="bibr" target="#b2">[3]</ref>, ResNet <ref type="bibr" target="#b3">[4]</ref>, and ViT <ref type="bibr" target="#b4">[5]</ref>. Despite their great success, recent studies have uncovered that deep discriminative classifiers are vulnerable to adversarial examples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. These are welldesigned inputs with slight perturbations that can cause the model to produce incorrect classification results. This vulnerability partly stems from the opaque nature of the black-box deep neural network (DNN) models and their limited interpretability. Adversarial examples are typically modified slightly so that humans don't misclassify them, thus posing a non-trivial threat to many DNN-based applications.</p><p>Adversarial training has emerged as one of the most effective techniques for improving robustness <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. The core idea is to include adversarial examples in the training stage. However, training data augmentation doesn't address the root cause of adversarial vulnerability: the lack of model interpretability. Recent studies show that deep generative classifiers, despite their limited performance on image classification tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, can be more robust to adversarial examples <ref type="bibr" target="#b14">[15]</ref>. This adversarial robustness benefits from their ability to model the distribution of each class, essentially modeling how a specific class generates the input data. This approach leads to better model interpretability and allows them to better understand what adversarial inputs might look like. Discriminative classifiers, on the other hand, model decision boundaries between different classes and learn which input features contribute most to distinguishing between various classes. Consequently, they are more vulnerable to adversarial examples designed to create outliers for decision boundaries, thus confusing the classifier. This difference provides the underlying motivation and theoretical support for using generative classifiers to build more adversarially robust networks, as it's more challenging to shift feature distribution than to create outliers.</p><p>Building upon these insights, in this paper, we develop a generalized deep neural network architecture for image classification: an ensemble network consisting of discriminative features and generative classifiers. This novel architecture combines the strengths of both approaches to create an accurate and more robust classification model. Our methodology begins with constructing a latent variable model that models the relationships among input images, discriminative features, output labels, and latent variables. We then employ variational Bayes to formulate final prediction probabilities. Our contributions in this work can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a bottom-up discriminative-generative architecture featuring a generative component that can be attached to various pre-trained discriminative models, highlighting the generalizability of our approach; (2) We demonstrate our network's resistance to various types and strengths of white-box adversarial attacks, significantly decreasing attack success rates without adversarial training; (3) Leveraging counterfactual metrics and feature interaction-based metrics, we demonstrate that the proposed model has superior model interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial training has emerged as an effective approach to enhance model robustness. Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref> and Kurakin et al. <ref type="bibr" target="#b15">[16]</ref> demonstrated significant error rate reductions on MNIST and ImageNet datasets, respectively, through adversarial training. However, injecting adversarial examples into the training set can decrease accuracy on clean datasets, and the process is computationally expensive due to the construction of sophisticated adversarial examples <ref type="bibr" target="#b15">[16]</ref>. To address these issues, Shafahi et al. <ref type="bibr" target="#b9">[10]</ref> introduced a "free" adversarial training algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Recent improvements in adversarial training include using cyclic learning rates to accelerate training <ref type="bibr" target="#b10">[11]</ref> and employing diffusion models to augment adversarial training datasets <ref type="bibr" target="#b16">[17]</ref>. Nevertheless, it remains unrealistic to involve all types of adversarial examples in the training phase due to the variety of attacks. More importantly, adversarial training doesn't solve the intrinsic problem of adversarial vulnerability, which lies in the lack of model interpretability.</p><p>Recent research has begun to evaluate the adversarial robustness of generative classifiers, which offer improved interpretability. Li et al. <ref type="bibr" target="#b14">[15]</ref> found that deep generative classifiers can be robust on MNIST and CIFAR-binary datasets. Zhang et al. <ref type="bibr" target="#b17">[18]</ref> further improved deep generative classifier robustness by modeling adversarial perturbation from a causal perspective. However, both studies were unable to obtain satisfactory results on full image datasets, reporting that VAE-based generative classifiers achieved less than 50% clean test accuracy on CIFAR-10. Additionally, they didn't conduct an interpretability analysis to show the correlations between adversarial robustness and model interpretability. Mackowiak et al. <ref type="bibr" target="#b11">[12]</ref> showed that Invertible Neural Networks (INNs) are more interpretable than ResNet-50. However, they demonstrated that INNs do not fully prevent C&amp;W attacks <ref type="bibr" target="#b7">[8]</ref> on ImageNet, despite C&amp;W attacks prioritizing small perturbations over attack success rates.</p><p>To address these limitations, we develop a generalized deep generative classifier architecture for more complex datasets (full CIFAR-10 and CIFAR-100), using only clean data for training to reduce training time and computational costs. To demonstrate the generalizability of our approach, we conduct preliminary experiments on the Tiny-ImageNet dataset, showcasing the potential of our method to scale to larger datasets for real-world applications. Furthermore, we evaluate robustness against one of the strongest attacks (PGD <ref type="bibr" target="#b8">[9]</ref>), which prioritizes attack success rates over perturbation size. Importantly, we explore the correlations between adversarial robustness and model interpretability, showing that by design- ing causal graphs in generative classifiers, we can achieve better model interpretability, leading to improved adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The overall model architecture, illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, consists of two primary components: a bottom-level pretrained discriminative feature extraction network and a top-level generative classification network. While the discriminative features ensure high classification accuracy, the generative model provides adversarial robustness by modeling the distribution of adversarial inputs. The bottom-level discriminative network can be any pretrained Convolutional Neural Network (CNN) model, such as VGG or ResNet, making our approach generalizable to various classification tasks and user preferences.</p><p>Our work extends the approach presented in <ref type="bibr" target="#b17">[18]</ref> to handle more complex image classification tasks (e.g. full CIFAR-10, CIFAR-100 and Tiny-ImageNet datasets). The top-level generative classifier processes both the features extracted from the bottom-level pre-trained CNN and the original image. This dual-input approach leverages the generative model's ability to regenerate its inputs, enabling it to learn distributions from both feature representations and original data. Such comprehensive learning strategy enhances the model's capability to handle both adversarial images and adversarial features.</p><p>The generative component of our architecture consists of two main elements: (1) A latent variable model captures the relationships between input images, discriminative features, output labels, and latent variables; (2) A variational auto-encoder (VAE) formulates final prediction probabilities using the latent variable model and variational Bayesian inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Latent Variable Model</head><p>The foundation of our generative classifier is a deep latent variable model with a causal graph that captures the relationships between inputs, outputs, and latent variables, as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. This causal reasoning approach enables DNNs to learn causal relations rather than merely statistical correlations between inputs and outputs, thereby improving robustness and reducing overfitting. In our model, we define two types of inputs to the generative classifier which are the original image X 1 and image features extracted from a pre-trained feature extractor X 2 . The causal graph incorporates three key factors that influence the formation of image data: Y represents the predicted label containing the class of the object; M represents variables that can be modified artificially (adversarial perturbations); Z denotes all the other contributing factors.</p><p>Following Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref>, we model adversarial perturbations M as a specific type of noise affecting both X 1 and X 2 . In white-box attacks, these perturbations M are generated based on the labels Y, input data (X 1 , X 2 ), and network parameters θ. The formation of adversarial inputs (X 1 , X 2 ) is then caused by the combination of adversarial perturbations M, labels Y, and other factors Z. For simplicity, we do not consider cases where other factors Z influence labels Y. The causal model for input data formation can be expressed as:</p><formula xml:id="formula_0">X 1 , X 2 = P(M, Y, Z)<label>(1)</label></formula><p>where P represents the process of input data formation.</p><p>The generative classifier learns this causal relationship during the training phase and makes correct classifications based on its reasoning from these factors during the inference phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VAE-based Generative Classifier</head><p>Building upon the causal relations established in Section 3.1, we develop our generative classifier using amortized variational inference <ref type="bibr" target="#b18">[19]</ref>. The fundamental approach applies Bayes' theorem to estimate the conditional probability p(y|x), where y is the target label and x is the input. A generative classifier predicts the label y of an input x according to:</p><formula xml:id="formula_1">p(y|x) = p(x|y)p(y) p(x) = so f tmax C c=1 [log p(x, y c )] (2)</formula><p>where C is the total number of classes and the likelihood function log p(x, y c ) is maximized during training. The prediction process involves computing the log-likelihood for each class y = c followed by a softmax operation. After incorporating the latent variables m (adversarial perturbations) and z (other factors), Eq. ( <ref type="formula">2</ref>) can be reformulated as:</p><formula xml:id="formula_2">p(y|x) = so f tmax C c=1 [log p(x, y c , z, m) dm dz]<label>(3)</label></formula><p>Given that our input X contains both x 1 and x 2 , we can express the joint probability p(x, y c , z, m) as:</p><formula xml:id="formula_3">p(x 1 , x 2 , y, z, m) = p(x 1 , x 2 |y, z, m)p(y, z, m) = p(x 1 , x 2 |y, z, m)p(m)p(z)p(y)<label>(4)</label></formula><p>Substituting Eq. ( <ref type="formula" target="#formula_3">4</ref>) into Eq. ( <ref type="formula" target="#formula_2">3</ref>) yields:</p><formula xml:id="formula_4">p(y|x) = so f tmax C c=1 [log p(x 1 , x 2 |y, z, m)p(m)p(z)p(y) dm dz]<label>(5)</label></formula><p>Due to the intractability of the integral of the marginal log-likelihood, arising from intractable true posterior densities p(z|•) and p(m|•) for latent variables, we introduce an approximate distribution q(z, m; λ) with variational parameters λ to approximate the true posterior <ref type="bibr" target="#b19">[20]</ref>. Then the training objective of maximizing log-likelihood function in Eq. ( <ref type="formula" target="#formula_4">5</ref>) is equivalent to minimizing the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b20">[21]</ref> between the variational distribution and true posterior D KL (q(z, m; λ)||p(z, m|•)). However, this divergence D KL is almost impossible to minimize to zero because the variational distribution is usually not capable enough to catch the complexity of the true posterior due to insufficient parameters. To address this challenge, we maximize the Evidence Lower Bound (ELBO), which is equivalent to minimizing the divergence <ref type="bibr" target="#b18">[19]</ref>. ELBO is a lower bound on the log marginal probability of the data and can be derived from Eq. ( <ref type="formula" target="#formula_4">5</ref>) using Jensen's inequality: log p(x, y) = log p(x 1 , x 2 |y, z, m)p(m)p(z)p(y) dm dz = log p(x 1 , x 2 |y, z, m)p(m)p(z)p(y) q(z, m; λ) q(z, m; λ) dm dz</p><formula xml:id="formula_5">= log E q(z,m;λ) p(x 1 , x 2 |y, z, m)p(m)p(z)p(y) q(z, m; λ) ≥ E q(z,m;λ) log p(x 1 , x 2 |y, z, m)p(m)p(z)p(y) q(z, m; λ)<label>(6)</label></formula><p>Following the causal graph (Fig. <ref type="figure" target="#fig_1">2</ref>), we design the probabilistic encoder network (i.e., variational posterior inference) as:</p><formula xml:id="formula_6">q(z, m; λ) = q λ (z, m|x 1 , x 2 , y) = q λ 1 (z|x 1 , x 2 , y, m)q λ 2 (m|x 1 , x 2 , y)<label>(7)</label></formula><p>where λ = {λ 1 , λ 2 } are the encoder network parameters. λ 1 is parameter for encoder network q λ 1 (z|x 1 , x 2 , y, m), and λ 2 is parameter for encoder network q λ 2 (m|x 1 , x 2 , y). The generative parameters of the probabilistic decoder network are defined as:</p><formula xml:id="formula_7">p θ (x 1 , x 2 , y, z, m) = p θ 1 (x 1 , x 2 |y, z, m)p(m)p(z)p(y) (8)</formula><p>where the generative parameters are θ = {θ 1 } and θ 1 is parameter for decoder network p θ 1 (x 1 , x 2 |y, z, m). The VAE architecture (Fig. <ref type="figure" target="#fig_2">3</ref>) implements both encoder and decoder networks with separate neural nets. Both probabilistic networks use multi-layer perceptrons (MLPs) with Gaussian outputs parameterized by mean µ and standard deviation σ.</p><p>To avoid the enlarged dataset and computational overhead of adversarial training, we train only on clean data (m = 0). Therefore, Eq. ( <ref type="formula" target="#formula_6">7</ref>) and Eq. ( <ref type="formula">8</ref>) can be simplified and the joint training of p θ and q λ networks maximizes the simplified ELBO:</p><formula xml:id="formula_8">max E q λ 1 log p(z)p(y c )p θ 1 (x 1 , x 2 |y, z, m) q λ 1 (z|x 1 , x 2 , y, m)<label>(9)</label></formula><p>We initialize p(z) as a Gaussian distribution (µ = 0, σ = 0) and p(y) as a uniform distribution based on the dataset's classes (i.e., 0.1 for CIFAR-10 and 0.01 for CIFAR-100). During inference, m is not set to 0 but sampled from q λ 2 (m|x 1 , x 2 , y c ) and z is sampled from q λ 1 (z|x 1 , x 2 , y c , m). The final prediction is:</p><formula xml:id="formula_9">p(y|x) = p(x|y)p(y) p(x) ≃ so f tmax C c=1        log K k=1 p(z k )p(y c )p θ 1 (x 1 , x 2 |y c , z, m) q λ 1 (z k |x 1 , x 2 , y c , m)       <label>(10)</label></formula><p>where K denotes the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. Generative classifier benchmarking typically focuses on MNIST, SVHN, or CIFAR datasets due to performance constraints <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. In this paper, we utilize the full CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b21">[22]</ref> for comprehensive evaluation of both clean accuracy and adversarial robustness. Additionally, we conduct clean accuracy experiments on Tiny-ImageNet <ref type="bibr" target="#b22">[23]</ref> to demonstrate the generalizability of our approach. Feature extractors. We employ VGG-16 and VGG-19 <ref type="bibr" target="#b2">[3]</ref> as our pre-trained feature extractors. Despite not being the most recent models, VGG remains widely used as backbones in state-of-the-art (SOTA) CNN and transformer-based vision models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and is capable enough to deal with CIFAR and Tiny-ImageNet data.</p><p>Rather than using the entire VGG model, we extract features from intermediate layers. We implement a shallow model (containing only convolution layers) and a deep model (containing both convolution and dense layers) for ablation study, with further details discussed in Section 4.4. Our bottom-up architecture design also allows seamless integration of alternative backbones, such as ResNet <ref type="bibr" target="#b3">[4]</ref>, without requiring modifications to the generative component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE-based classifier.</head><p>Each MLP estimating q λ 1 (z|x 1 , x 2 , y, m) and q λ 2 (m|x 1 , x 2 , y) consists of four hidden fully-connected layers with 500 neurons each and ReLU activation functions.</p><p>MLP estimating p θ 1 (x 1 , x 2 |y, z, m) consists of two hidden layers. We train the model using Adam optimizer with a learning rate of 5e-5 and a batch size of 150 for 350 total training iterations. Adversarial attacks. Following the evaluation framework in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>, we target white-box attacks and evalu-ate our model against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). FGSM <ref type="bibr" target="#b6">[7]</ref> is selected for its simplicity and representativeness as a classic attack. PGD <ref type="bibr" target="#b8">[9]</ref> represents SOTA attack performance and is consistently ranked among the strongest attacks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. For both attacks, we evaluate perturbation magnitudes ϵ ranging from 0 to 0.2, where ϵ = 0 represents clean inputs. The perturbation size is constrained to maintain visual similarity between original and adversarial inputs. We employ the l 2 -norm with 30 steps for PGD attacks. Interpretability. We use counterfactual explanations to evaluate interpretability, treating adversarial examples as a specific type of counterfactuals. We evaluate these based on proximity (perturbation size) and speed (attack iterations) <ref type="bibr" target="#b27">[28]</ref>. Lower proximity and higher speed indicate higher quality counterfactuals. Models more susceptible to generating high-quality counterfactuals demonstrate lower interpretability, and this indicates greater vulnerability to adversarial attacks.</p><p>Additionally, we employ feature attribution analysis through Remove and Retrain (ROAR) <ref type="bibr" target="#b28">[29]</ref>. Using integrated gradients <ref type="bibr" target="#b29">[30]</ref> for feature attribution, we replace 30% of the most important pixels with the mean of CI-FAR data, followed by model retraining and re-evaluation on the modified dataset. Greater accuracy degradation in this case indicates better interpretability, as it shows the model's reliance on more important features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Adversarial Robustness</head><p>We evaluate our model's robustness against adversarial attacks as shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a), 4(d), 5(a), and 5(d). The baseline models include three adversarially robustified generative classifiers (GBZ-CONV9 <ref type="bibr" target="#b14">[15]</ref>, GBY-CONV9 <ref type="bibr" target="#b14">[15]</ref>, and Deep CAMA <ref type="bibr" target="#b17">[18]</ref>) and one discriminative classifier (DBX-FC1 <ref type="bibr" target="#b14">[15]</ref>). Additionally, as our model employs VGG16 as a feature extractor, we include a pure VGG-based discriminative classifier for direct comparison. Results from both shallow and deep models are reported and ablation study is shown in Section 4.4.</p><p>Our proposed model demonstrates superior adversarial robustness across different attack types and strengths. For FGSM attacks, the model maintains remarkable robustness on both CIFAR-10 and CIFAR-100 datasets, with accuracy degradation limited to less than 0.1, effectively mitigating the FGSM attack's impact. Against the more challenging PGD attack, our model achieves impressive results on CIFAR-10, maintaining over 0.8 accuracy at ϵ = 0.05 and 0.5 accuracy at ϵ = 0.1, representing a significant robustness improvement. While the PGD attack results on CIFAR-100 show reduced effectiveness compared to CIFAR-10, our model still outperforms all baseline models. The generative classifiers GBZ-CONV9 and GBY-CONV9 also show robustness on both attacks and datasets, but the improvement is limited compared to our approach. Notably, Deep CAMA fails to generalize effectively to CIFAR-10 and CIFAR-100, achieving clean accuracy below 0.5 on both datasets, consistent with the limitations reported in their paper <ref type="bibr" target="#b17">[18]</ref>. The discriminative baselines (VGG16 and DBX-FC1) exhibit the worst robustness across both attacks and datasets. Their performance degrades significantly even with minimal perturbations (ϵ &lt; 0.05), with accuracy falling below 0.2. These results highlight the advantages of our hybrid approach over purely discriminative or generative classifiers.</p><p>Additionally, our model maintains high clean dataset accuracy (ϵ = 0) without the typical degradation observed in adversarial training <ref type="bibr" target="#b15">[16]</ref>. We attribute this to the enhanced interpretability (showed in Section 4.3), which appears to contribute to overall model performance. Traditional adversarial training typically requires 3-30 times longer training periods compared to non-robust equivalents <ref type="bibr" target="#b9">[10]</ref>. While our study doesn't include quantitative training time comparisons with adversarially trained baselines, our approach provides inherent efficiency advantages as it achieves robustness without dataset augmentation or enlargement through injecting adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Model Interpretability</head><p>For a focused interpretability analysis, we compare our best-performing model (shallow model) against the most robust generative baseline, GBZ-CONV9, and a pure VGG16 discriminative model. Our evaluation employs multiple complementary metrics to assess model interpretability. We first examine counterfactual proximity by measuring the perturbation size needed in adversarial examples to affect model predictions. Models with better interpretability typically require larger perturbation (low proximity) for successful attacks. The FGSM attack results, presented in Fig. <ref type="figure" target="#fig_4">4</ref>  that both our model and GBZ-CONV9 require larger perturbation compared to VGG16 to achieve effective attacks. This increased perturbation requirement leads to low counterfactual proximity, indicating our model's enhanced interpretability. For PGD attacks (Fig. <ref type="figure" target="#fig_4">4</ref>(e) and 5(e)), the differences between models are less apparent, likely because the iterative nature of PGD allows perturbations to accumulate over multiple steps (T = 30), potentially masking model-specific variations in perturbation size. Our analysis of counterfactual generating speed reveals additional insights into model interpretability. We define this metric as the minimum number of iterations required to reduce accuracy to 0.4, with slower attack speed (more iterations) indicating increased attack difficulty and better model interpretability. The results for Iterative-FGSM (I-FGSM) and PGD attacks, shown in Fig. <ref type="figure" target="#fig_4">4(c</ref>) and 5(c), reveal significant differences between models. VGG16 proves highly vulnerable to single-step attacks, with FGSM and PGD attacks executing fast. GBZ-CONV9 shows moderate resistance, requiring 5 iterations on CIFAR-10, though remaining vulnerable to single-step attacks on CIFAR-100. Our model demonstrates remarkable resistance to I-FGSM attacks, consistently requiring the maximum allowed iterations (30) on both datasets. For PGD attacks (Fig. <ref type="figure" target="#fig_4">4</ref>(f) and 5(f)), our model shows slower attack speed at small ϵ. These results indicate lower counterfactual generating speed and support our model's enhanced interpretability.</p><p>The ROAR results presented in Table <ref type="table" target="#tab_0">1</ref> provide additional validation of our model's interpretability. Higher ROAR scores indicate that removing important features significantly impacts model accuracy, suggesting better alignment between the model's decision-making process and attribution methods. Our model achieves the highest ROAR among all compared approaches, demonstrating that saliency maps more accurately reflect its interpretability. The consistently lower ROAR of discriminative models compared to generative approaches underscore the interpretability advantages of generative ar- chitectures. The comprehensive evaluation results reveal a clear correlation between adversarial robustness (discussed in Section 4.2) and model interpretability. Models displaying greater vulnerability to adversarial attacks consistently demonstrate lower interpretability across our metrics. This relationship suggests that our model's superior adversarial robustness stems from its enhanced interpretability, highlighting the importance of interpretable architectures in developing robust DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct an ablation study to investigate the impact of combining discriminative features and generative classifiers in our model architecture. Specifically, we examine how the depth of the feature extractor affects various aspects of model performance. By varying the number of layers in the feature extractor, we uncover important trade-offs between clean accuracy and adversarial robustness. The experimental results presented in Table <ref type="table" target="#tab_1">2</ref> demonstrate that deeper feature extractors lead to higher </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>ROAR VGG16 <ref type="bibr" target="#b2">[3]</ref> 0.4828 DBX-FC1 <ref type="bibr" target="#b14">[15]</ref> 0.4962 Deep CAMA <ref type="bibr" target="#b17">[18]</ref> 0.2462 GBZ-CONV9 <ref type="bibr" target="#b14">[15]</ref> 0.4278 GBY-CONV9 <ref type="bibr" target="#b14">[15]</ref> 0.5571 Ours (deep) 0.5393 Ours (shallow) 0.7405 clean accuracy, attributable to their ability to learn more sophisticated discriminative features. However, as shown in Figure <ref type="figure" target="#fig_5">5</ref> and Table <ref type="table" target="#tab_0">1</ref>, models with shallower feature extractors show superior robustness and better interpretability. This observation points to a fundamental trade-off in our architecture: more sophisticated discriminative fea- tures, while beneficial for clean accuracy, tend to compromise adversarial robustness. The relationship suggests that achieving optimal performance requires careful balancing of the feature extractor's depth.</p><p>To validate the generalizability of our approach, we extend our experiments to the more challenging Tiny-ImageNet dataset using a VGG19 backbone. While shallow models struggle to achieve satisfactory performance with clean accuracy below 0.5, deep models demonstrate SOTA performance with accuracy around 0.7. Based on previous findings, we argue that with appropriate architectural choices, our approach can be extended to develop Tiny-ImageNet models that achieve both acceptable accuracy and adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a generalized image classifier architecture that combines discriminative features and generative classifiers with built-in causal graphs to achieve both high classification accuracy and adversarial robustness. The experimental results on CIFAR-10 and CIFAR-100 datasets validate our model's superior adversarial robustness compared to SOTA adversarially trained generative classifiers. Notably, our model achieves this robustness while requiring no adversarial example augmentation during training. Through interpretability analysis using multiple evaluation metrics, our results reveal strong correlations between model interpretability and adversarial robustness, suggesting that enhanced interpretability contributes to improved robustness. The flexibility of our architecture is demonstrated through its extension to the more complex Tiny-ImageNet dataset. The generative component functions as an auxiliary network that can be integrated with various pre-trained CNNs, adapting to different dataset requirements. While our current work focuses primarily on white-box robustness, future research can include evaluating our approach against black-box attacks , and extending our methodology to other computer vision tasks such as object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Bottom-up discriminative generative architecture. The overall model consists of both a feature extractor and a generative classifier.</figDesc><graphic coords="3,83.43,124.80,205.78,208.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal graph. Solid lines represent the causal reasoning of input data.</figDesc><graphic coords="4,83.43,173.88,205.77,122.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: VAE architecture. Each individual neural net in the encoder and decoder estimates independent probabilities for q and p, respectively.</figDesc><graphic coords="5,310.61,124.80,228.64,119.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b) and 5(b), demonstrate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification accuracy, adversarial example proximity, and adversarial example speed for FGSM and PGD on CIFAR-10 dataset. Dashed lines represent discriminative classifiers while solid lines represent generative classifiers. ϵ controls the attack strength.</figDesc><graphic coords="7,385.52,253.96,144.85,97.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Classification accuracy, adversarial example proximity, and adversarial example speed for FGSM and PGD on CIFAR-100 dataset. Dashed lines represent discriminative classifiers while solid lines represent generative classifiers. ϵ controls the attack strength.</figDesc><graphic coords="8,385.52,253.96,144.85,97.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ROAR. Higher ROAR shows more accuracy degradation when important features are missing, indicating better interpretability.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on various layers of discriminative features.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Acc</cell></row><row><cell>CIFAR-10</cell><cell cols="2">Shallow (9 CONV) Deep (13 CONV+1 Dense) 0.9304 0.9248</cell></row><row><cell>CIFAR-100</cell><cell cols="2">Shallow (12 CONV) Deep (13 CONV+1 Dense) 0.7089 0.7011</cell></row><row><cell>Tiny-</cell><cell>Shallow (14 CONV)</cell><cell>0.4734</cell></row><row><cell>ImageNet</cell><cell cols="2">Deep (16 CONV+2 Dense) 0.6935</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">Center for Connected Multimodal Mobility</rs> (<rs type="grantNumber">C 2 M 2</rs> ) (<rs type="affiliation">U.S. DOT Tier 1 University Transportation Center</rs>) headquartered at <rs type="affiliation">Clemson University, Clemson, South Carolina</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UAffZnC">
					<idno type="grant-number">C 2 M 2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing vision transformers and convolutional neural networks for image classification: A literature review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maurício</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Domingues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5521</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Comparative analysis of image classification algorithms based on traditional machine learning and deep learning, Pattern recognition letters</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards evaluating the robustness of neural networks, in: 2017 ieee symposium on security and privacy (sp)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free!</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03994</idno>
		<title level="m">Fast is better than free: Revisiting adversarial training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative classifiers as a basis for trustworthy image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mackowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2971" to="2981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Understanding the limitations of conditional generative models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01171</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting discriminative vs. generative classifiers: Theory and implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="42420" to="42477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are generative classifiers more robust to adversarial attacks?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3804" to="3814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better diffusion models further improve adversarial training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="36246" to="36263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01095</idno>
		<title level="m">A causal view on robustness of neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advances in variational inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bütepage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2008" to="2026" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vgg-tswinformer: Transformer-based deep learning model for early alzheimer&apos;s disease prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page">107291</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object detection using deep learning, cnns and vision transformers: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Amjoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amrouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="35479" to="35516" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion-based adversarial sample generation for improved stealthiness and controllability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial attacks on faster r-cnn object detector</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causal interpretability for machine learningproblems, methods and evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moraffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark for interpretability methods in deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
