<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAR : CONTROLLABLE AUTOREGRESSIVE MODELING FOR VISUAL GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-07">7 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jialin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifeng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Southern University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<email>lilei@di.ku.dk</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CAR : CONTROLLABLE AUTOREGRESSIVE MODELING FOR VISUAL GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-07">7 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.04671v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Controllable generation, which enables fine-grained control over generated outputs, has emerged as a critical focus in visual generative models. Currently, there are two primary technical approaches in visual generation: diffusion models and autoregressive models. Diffusion models, as exemplified by ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas autoregressive models, despite showcasing impressive generative quality and scalability, remain underexplored in terms of controllability and flexibility. In this study, we introduce Controllable AutoRegressive Modeling (CAR), a novel, plug-and-play framework that integrates conditional control into multi-scale latent variable modeling, enabling efficient control generation within a pre-trained visual autoregressive model. CAR progressively refines and captures control representations, which are injected into each autoregressive step of the pre-trained model to guide the generation process. Our approach demonstrates excellent controllability across various types of conditions and delivers higher image quality compared to previous methods. Additionally, CAR achieves robust generalization with significantly fewer training resources compared to those required for pre-training the model. To the best of our knowledge, we are the first to propose a control framework for pretrained autoregressive visual generation models. Our code is publicly available at <ref type="url" target="https://github.com/MiracleDance/CAR">https://github.com/MiracleDance/CAR</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Controllable generation represents a pivotal aspect of visual generative models, enabling precise and fine-grained control over generated outputs. This capability is indispensable for tasks that demand a high degree of precision and adaptability, positioning it as a significant area of focus within the domain. Currently, there are mainly two primary paradigms that have substantially advanced the field of visual generation: diffusion models <ref type="bibr" target="#b38">(Rombach et al., 2022;</ref><ref type="bibr" target="#b40">Saharia et al., 2022)</ref> and autoregressive models <ref type="bibr" target="#b48">(Tian et al., 2024;</ref><ref type="bibr" target="#b9">Esser et al., 2021)</ref>. While the diffusion paradigm has already given rise to numerous widely adopted methods for controllable generation, the autoregressive approach remains underexplored, particularly in how to empower the strengths of this paradigm for controllable generation, which constitutes the central emphasis of this work.</p><p>Diffusion models utilize iterative denoising processes based on Markov chains to produce highquality outputs. These models have inspired the development of widely used controllable generation techniques such as ControlNet <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> and T2I-Adapter <ref type="bibr" target="#b29">(Mou et al., 2024)</ref>, which can provide granular control over the generation images by incorporating additional signals such as edge maps and human poses. However, challenges arise when integrating diffusion models into multimodal frameworks, particularly when interfacing with large language models (LLMs) <ref type="bibr" target="#b6">(Chiang et al., 2023;</ref><ref type="bibr" target="#b49">Touvron et al., 2023)</ref>. The representations in diffusion models are inconsistent with the embeddings used by LLMs, complicating their seamless integration. This discrepancy might hinder visual generation tasks that require direct collaboration between vision and language models in the future. As a result, these shortcomings necessitate the exploration of more unified approaches to controllable generative modeling.</p><p>On the other hand, autoregressive models, drawing inspiration from autoregressive language models <ref type="bibr" target="#b35">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown, 2020)</ref>, offer a compelling alternative for visual generation tasks. These approaches model image generation as a sequence prediction problem, which align with the representation used in LLMs and offer lower computational costs compared to diffusion models. By employing intricate and scalable designs, recent autoregressive models <ref type="bibr" target="#b46">(Sun et al., 2024;</ref><ref type="bibr" target="#b48">Tian et al., 2024)</ref> have demonstrated generative capabilities comparable to those of diffusion models. However, existing autoregressive models have not yet fully explored the potential of controllable visual generation. In an earlier attempt, IQ-VAE <ref type="bibr" target="#b58">(Zhan et al., 2022)</ref> introduced condition patches as prefix tokens to generate subsequent image patches. This approach results in overly long sequences, which significantly reduces efficiency. More recently, ControlVAR <ref type="bibr" target="#b22">(Li et al., 2024)</ref> models conditions and images simultaneously to guide the generation process. However, it restricts the ability to effectively utilize pre-trained models, thereby increasing the training resources needed and reducing flexibility and adaptability. These inefficiencies underscore the necessity for more versatile and streamlined methods for controllable autoregressive generation.</p><p>To address the challenges mentioned above, we propose Controllable AutoRegressive Modeling (CAR), a novel, end-to-end, plug-and-play framework designed to facilitate controllable visual autoregressive generation by leveraging pre-trained models. The core design of our framework involves integrating multi-scale latent variable modeling, where the control representation is progressively refined and injected into each step of a pre-trained autoregressive model. Specifically, we employ the "next-scale prediction" autoregressive model VAR <ref type="bibr" target="#b48">(Tian et al., 2024)</ref> as our pre-trained base, and we freeze its weights to maintain its strong generative capabilities. Inspired by Control-Net <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref>, we have also designed a parallel control branch to autoregressively model multi-scale control representation, which utilizes both the input condition signal and the embedding from the pre-trained base model. The prediction of each scale's image token map depends on the previous image tokens and the extracted control information. Through this approach, our CAR framework successfully captures multi-scale control representations and injects them into the frozen base model, ensuring that the generated image adheres to the specified visual conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of this work can be summarized as follows:</head><p>1. To the best of our knowledge, our proposed CAR is the first flexible, efficient and plugand-play controllable framework designed for the family of autoregressive models. We hope that our work will contribute to accelerating the development of this field.</p><p>2. CAR builds on pre-trained autoregressive models, not only preserving the original generative capabilities but also enabling controlled generation with limited resources-using less than 10% of the data required for pre-training. We design a general framework to capture multi-scale control representations, which are robust and can be seamlessly integrated into the pre-trained base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Extensive experiments demonstrate that our CAR achieves precise fine-grained visual control across various condition signals. CAR effectively learns the semantics of these conditions, enabling robust generalization even to unseen categories outside the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Diffusion Models Diffusion models have attracted significant attention for their ability to generate high-fidelity images through iterative noise reduction processes. These models operate by progressively transforming Gaussian noise into a data distribution, with each step in the Markov chain refining the image <ref type="bibr" target="#b43">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b44">Song et al., 2020)</ref>. The introduction of the Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020) marked a breakthrough, achieving stateof-the-art results in image synthesis. Following this, several approaches have aimed to improve the efficiency and quality of diffusion models <ref type="bibr">(Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr" target="#b38">Rombach et al., 2022;</ref><ref type="bibr" target="#b53">Watson &amp; Johnson, 2023)</ref>. In the past two years, diffusion models have nearly become the de facto approach in the realm of text-to-image and text-to-video generation <ref type="bibr" target="#b40">(Saharia et al., 2022;</ref><ref type="bibr" target="#b42">Singer et al., 2022;</ref><ref type="bibr" target="#b32">Peebles &amp; Xie, 2023;</ref><ref type="bibr" target="#b34">Podell et al., 2023;</ref><ref type="bibr" target="#b7">Dai et al., 2023;</ref><ref type="bibr">Blattmann et al., 2023a;</ref><ref type="bibr" target="#b3">b;</ref><ref type="bibr" target="#b10">Esser et al., 2023;</ref><ref type="bibr" target="#b47">2024)</ref>. More recently, some works have increasingly focused on integrating diffusion models into multimodal tasks <ref type="bibr">(Nichol et al., 2021;</ref><ref type="bibr" target="#b24">Lu et al., 2022;</ref><ref type="bibr" target="#b47">2024;</ref><ref type="bibr" target="#b54">Xie et al., 2024;</ref><ref type="bibr" target="#b60">Zhou et al., 2024)</ref>.</p><p>Autoregressive Models Autoregressive models have emerged as a scalable alternative to diffusion models in generative tasks, offering a more efficient architecture for image synthesis. Inspired by the success of autoregressive models in language tasks, such as GPT <ref type="bibr" target="#b35">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown, 2020)</ref>, their visual counterparts like DALL-E <ref type="bibr" target="#b36">(Ramesh et al., 2021)</ref> model image generation as a sequence prediction problem. This paradigm shift allows autoregressive models to generate highquality images while circumventing the iterative nature of diffusion models, thereby reducing computational overhead. A number of excellent works adhering to this paradigm have emerged <ref type="bibr" target="#b12">(Ge et al., 2023;</ref><ref type="bibr" target="#b27">Ma et al., 2024;</ref><ref type="bibr" target="#b25">Lu et al., 2024;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b48">Tian et al., 2024;</ref><ref type="bibr" target="#b47">Team, 2024;</ref><ref type="bibr" target="#b5">Chern et al., 2024;</ref><ref type="bibr" target="#b23">Liu et al., 2024;</ref><ref type="bibr" target="#b8">Dong et al., 2023;</ref><ref type="bibr" target="#b13">Ge et al., 2024)</ref>.</p><p>One of the major developments in this field is the application of discrete latent spaces, introduced by VQ-VAE <ref type="bibr" target="#b50">(Van Den Oord et al., 2017)</ref> and VQ-GAN <ref type="bibr" target="#b9">(Esser et al., 2021)</ref>, enabling efficient encoding and decoding of image data. Subsequent works have further enhanced the representational capacity of discrete visual encoders <ref type="bibr" target="#b56">(Yu et al., 2023a;</ref><ref type="bibr" target="#b3">b;</ref><ref type="bibr" target="#b26">Luo et al., 2024)</ref>. More recently, VAR <ref type="bibr" target="#b48">(Tian et al., 2024)</ref> provides a scaling-up modeling approach for discrete latent spaces, significantly enhancing generation. Nonetheless, while these models exhibit better efficiency and comparable generation quality to diffusion models, they still lack sophisticated controllable generation mechanisms. This limitation restricts their applicability in tasks requiring user-driven or signal-driven generation. Approaches such as ControlVAR <ref type="bibr" target="#b22">(Li et al., 2024)</ref> have made some progress; however, they remain inflexible and fail to fully exploit pre-trained models, often necessitating fine-tuning.</p><p>Controllable Generation Controllable generation, where the model is guided by various conditions during the generative process, has been an active area of research. Early works focused on conditional GANs <ref type="bibr" target="#b28">(Mirza &amp; Osindero, 2014)</ref> and VAEs <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2013)</ref>, where control was imposed through explicit conditioning variables such as class labels. However, the challenge of maintaining both high-quality generation and precise control persists across different generative frameworks. Diffusion-based methods like ControlNet <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> and T2I-Adapter <ref type="bibr" target="#b29">(Mou et al., 2024)</ref> have incorporated external control signals, such as pose or sketch, to achieve detailed manipulation of generated content. In contrast, controllable generation methods for autoregressive models, especially efficient ones similar to ControlNet or T2I-Adapter in the diffusion context, have not been fully explored. Actually, prior to our work, it was unknown whether similar capabilities could be achieved with purely autoregressive models. While methods such as IQ-VAE <ref type="bibr" target="#b58">(Zhan et al., 2022)</ref> and ControlVAR <ref type="bibr" target="#b22">(Li et al., 2024)</ref> allow for fine-grained control over the visual autoregressive generation process by integrating conditional tokens or patches, they cannot flexibly leverage pre-trained models, and increase computational complexity. Therefore, this paper aims to develop a more efficient and flexible controllable framework for autoregressive visual generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We propose Controllable AutoRegressive Modeling (CAR) to explore the potential of autoregressive models in handling controllable image generation task. We define the task as follows: given a conditional control image C ∈ R 3×H×W , where H and W represent the height and width, our goal is to generate a controllable image I ∈ R 3×H×W that aligns with the specified visual conditions. The overall objective can be formulated as modeling the conditional distribution p(I | C).</p><p>In Section 3.1, we introduce the preliminary foundational concepts of the "next-scale prediction" paradigm in visual autoregressive modeling. Following this, in Section 3.2, we explain how our proposed CAR framework controls visual generation through multi-scale latent variable modeling. By applying Bayesian inference, we identify that the learning objective of CAR is to obtain a robust control representation. Finally, in Section 3.3, we thoroughly discuss the control representation expression and the network optimization strategy.  <ref type="formula">2021</ref>) use a "next-token prediction" approach, tokenizing and flattening images into sequences (x 1 , x 2 , . . . , x T ). Here, T is the product of the height and width of the image feature map. Each token x t is predicted based on the preceding tokens (x 1 , x 2 , . . . , x t-1 ). The final token sequence is quantized and decoded to produce images.</p><p>However, a recent study <ref type="bibr" target="#b48">(Tian et al., 2024)</ref> notes that this paradigm can lead to mathematical inconsistencies and structural degradation, which is less optimal for generating highly-structured images.</p><p>To resolve this, it introduces a novel visual autoregressive modeling paradigm (VAR), shifting from "next-token prediction" to "next-scale prediction". In VAR, each unit predicts an entire token map at a different scale. Starting with a 1 × 1 token map r 1 , VAR predicts a sequence of multi-scale token maps (r 2 , . . . , r K ), increasing in resolution. The generation process is expressed as:</p><formula xml:id="formula_0">p(r 1 , r 2 , . . . , r K ) = K k=1 p(r k | r 1 , r 2 , . . . , r k-1 ),<label>(1)</label></formula><p>where r k ∈ [V ] h k ×w k represents the token map at scale k, with dimensions h k and w k , conditioned on previous maps (r 1 , r 2 , . . . , r k-1 ). Each token in r k is an index from the VQVAE codebook V , which is trained through multi-scale quantization and shared across scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONTROLLABLE VISUAL AUTOREGRESSIVE MODELING</head><p>As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, our proposed CAR framework addresses controllable image generation by modeling the conditional distribution p(I | C). The objective is to maximize the likelihood p(I | C), ensuring that the generated image I conforms to the visual conditions specified by C.</p><p>Following the "next-scale prediction" paradigm of VAR, the CAR model adopts a multi-scale latent variable framework, where latent variables (token maps) at each scale capture image structures at progressively higher resolutions. The control information provides additional observations to aid in inferring the latent variables at each scale.</p><p>Multi-scale Conditional Probability Modeling Suppose the total number of scales in the latent framework is K, our CAR model generates an image I in a multi-scale fashion by factorizing the conditional distribution p(I | C) as a product of conditional probabilities at each scale:</p><formula xml:id="formula_1">p(I | C) = p(r 1 , r 2 , . . . , r K | c 1 , c 2 , . . . , c K ) = K k=1 p(r k | (c 1 , r 1 ), (c 2 , r 2 ), . . . , (c k-1 , r k-1 ), c k ) = K k=1 p(r k | {(r i , c i )} k-1 i=1 , c k ),<label>(2)</label></formula><p>where r k ∈ R h k ×w k is the image token map at scale k, and c k ∈ R h k ×w k is the corresponding control map derived from the control image C. Each token map r k is generated conditioned on the previous token maps (r 1 , r 2 , . . . , r k-1 ) and the control information (c 1 , c 2 , . . . , c k ). This multiscale conditional modeling ensures that the control information at each scale guides the generation process in a recursive and hierarchical manner, progressively refining the latent representations of the image token maps across scales.</p><p>Posterior Approximation In the CAR framework, as formulated in Equation <ref type="formula" target="#formula_1">2</ref>, the previous scales' image and control token maps, {(r i , c i )} k-1 i=1 , along with the current scale's control token map c k , serve as a posterior approximation for the current scale's image token map. This means that r k is generated by leveraging the information from this posterior approximation. From a Bayesian perspective, the goal of the CAR model is to approximate the posterior distribution of the image tokens given the control information at each scale:</p><formula xml:id="formula_2">p(r k | {(r i , c i )} k-1 i=1 , c k ) ∝ p(r k | {(r i , c i )} k-1 i=1 )p(c k | r k ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">p(r k | {(r i , c i )} k-1 i=1</formula><p>) represents the autoregressive prior learned across scales from previous token maps, and p(c k | r k ) captures the likelihood of observing the control token map c k given the current image token map r k .</p><p>Based on the above Bayesian inference, we can clearly identify that the learning objective of CAR is to optimize c k so that this control representation aligns with the image representation r k . This objective can be learned through a neural network, supervised by the ground truth r k from the provided image dataset, allowing the network to progressively approximate the posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CONTROL REPRESENTATION AND OPTIMIZATION</head><p>Control Representation Expression VAR accumulates image tokens {r i } k-1 i=1 from all previous scales, interpolates them to match the resolution of h k × w k , and forms the input for inference at scale k, denoted as b k . Then in our CAR framework, at each scale k, the control information is injected by fusing the input image token map b k ∈ R h k ×w k ×d and the control map c k ∈ R h k ×w k ×d to form a combined representation:</p><formula xml:id="formula_4">s k = F(b k , c k ; θ F ),<label>(4)</label></formula><p>where F(•) is a fusion function parameterized by θ F . This fusion mechanism ensures that s k encapsulates both generated image features and the control conditions. By utilizing s k to predict r k , the control information is incorporated into the generation process, ensuring that the generated token map adheres to the control conditions c k , providing fine-grained guidance.</p><p>To ensure effective extraction of control representation and integration of control information, the fused representation s k is vectorized and transformed via a series of Transformer layers, yielding a refined conditional prior that guides the image generation at each scale k. Formally, s k is transformed into a vectorized representation ŝk through a learned mapping T (•) as follows:</p><formula xml:id="formula_5">ŝk = T (s k ; θ T ),<label>(5)</label></formula><p>where T (•) is parameterized by the CAR's Transformer θ T , which is designed as a parallel branch alongside the VAR's Transformer. The blocks in T (•) perform self-attention on the vectorized control representations, extracting relevant condition priors by modeling dependencies within the control information.</p><p>Once the refined conditional prior ŝk is extracted, it is injected into the image token map r k , which is predicted by the pre-trained VAR and represents the latent image features at scale k. This injection is achieved through a injection function G(•) parameterized by θ G , which combines ŝk and r k to ensure that the control information modulates the generated image tokens:</p><formula xml:id="formula_6">rk = G(ŝ k , r k ; θ G ),<label>(6)</label></formula><p>where rk represents the updated token map incorporating the control information. This mechanism enables the model to progressively condition the generation process on multi-scale control information, thereby producing images that are coherent across scales and adhere to the external visual condition specified by C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Optimization</head><p>To align the generated image I with the control conditions C, we minimize the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b20">(Kullback &amp; Leibler, 1951)</ref>  </p><p>where θ car represents the learnable parameters in our CAR framework, specifically {θ F , θ T , θ G }. Since p data (I | C) is constant with respect to θ car , minimizing L KL is equivalent to maximizing the log-likelihood log p(I | C; θ car ).</p><p>Using the fused representations s k , the conditional distribution p(I | C; θ car ) can be factorized as:</p><formula xml:id="formula_8">log p(I | C; θ car ) = K k=1 log p(r k | s k ; θ car ).<label>(8)</label></formula><p>Maximizing this log-likelihood during training ensures that the generated token maps rk closely match the ground truth r k , remaining consistent with both previous tokens and the control conditions encapsulated in s k . This process facilitates the learning of more effective control representations, as outlined in the posterior approximation in Equation <ref type="formula" target="#formula_2">3</ref>, ensuring that the generated images adhere to the control conditions C while preserving the original generative capabilities of pre-trained VAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUPS</head><p>Model Architecture Design We employ the pre-trained VAR <ref type="bibr" target="#b48">(Tian et al., 2024)</ref> as the base model, freezing it during training to preserve its generative ability and reduce training costs. For the learnable modules {θ F , θ T , θ G }, we experiment with various choices and empirically select the optimal design choices. For the fusion function F(•), we use a convolutional encoder to extract semantic features from the control input c k , and add them to the base model input b k . For T (•), we design a series of GPT-2-style Transformer <ref type="bibr" target="#b35">(Radford et al., 2019)</ref> blocks, with the depth being half of that of the pre-trained base model. For G(•), we inject ŝk into the base model output r k through concatenation, which is followed by a LayerNorm (Ba, 2016) to normalize the distribution of the two domain features, and a linear transformation to adjust the channel dimension.  Dataset We conduct experiments on the ImageNet <ref type="bibr" target="#b39">(Russakovsky et al., 2015)</ref> dataset. First, we pseudo-label five conditions for the training set: Canny edge <ref type="bibr" target="#b4">(Canny, 1986)</ref>, Depth map <ref type="bibr" target="#b37">(Ranftl et al., 2020)</ref>, Normal map <ref type="bibr" target="#b52">(Vasiljevic et al., 2019)</ref>, HED map <ref type="bibr" target="#b55">(Xie &amp; Tu, 2015)</ref>, and Sketch <ref type="bibr" target="#b45">(Su et al., 2021)</ref>, allowing CAR to be trained separately on different conditional controls. We randomly select 100 categories from the total 1000 for training CAR, and evaluate it on the remaining 900 unseen categories to assess its generalizable controllability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We utilize Fréchet Inception Distance (FID) <ref type="bibr" target="#b15">(Heusel et al., 2017)</ref>, Inception Score (IS) <ref type="bibr" target="#b41">(Salimans et al., 2016)</ref>, Precision and Recall <ref type="bibr" target="#b21">(Kynkäänniemi et al., 2019)</ref> metrics to assess the quality of the generated results. We also compare the inference speed with existing controllable generation methods, such as ControlNet <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> and T2I-Adapter <ref type="bibr" target="#b29">(Mou et al., 2024)</ref>.</p><p>Training Details We set the depth of the pre-trained VAR to 16, 20, 24, or 30, and initialize the control Transformer T (•) using the weights from the first half of the VAR to accelerate convergence. The CAR model is trained for 100 epochs with the Adam optimizer on 8 NVIDIA V100 GPUs, and the inference speed is evaluated on a single NVIDIA 4090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">QUANTITATIVE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Previous Methods</head><p>We compare our CAR model with two classic controllable generation baselines, ControlNet and T2I-Adapter. For a fair comparison, we retrained both models on the ImageNet dataset and trained each model separately on all five condition annotations. As shown in Table <ref type="table" target="#tab_0">1</ref>, our CAR demonstrates significant improvements, with FID reductions of <ref type="bibr">3.3, 2.3, 2.3, 3.0, and 5</ref>.1 on Canny, Depth, Normal, HED, and Sketch, respectively, compared to Con-trolNet. Similar improvements are observed in the IS metric. We attribute these gains to recent advancements in autoregressive models, which have surpassed diffusion models in image generation by progressively scaling up the resolution during generation. In addition to image quality, we also compare inference speed, with our CAR running over five times faster than both ControlNet and T2I-Adapter, further highlighting the efficiency advantage of CAR in practical applications. Overall, these promising quantitative results indicate that CAR can serve as a more efficient and scalable controllable generative paradigm than diffusion-based models like ControlNet.</p><p>As for different types of conditions, it is worth noting that HED Maps, Depth Maps, and Normal Maps demonstrate relatively superior metrics, likely due to the clarity of input conditions and welldefined objectives. These factors provide the model with more precise guidance, enhancing the generation of high-quality images. In contrast, the Sketch condition is often simplistic, consisting of basic outlines with fewer visual details compared to other conditions, making it less controllable and leading the model to generate more freely. This may result in fluctuations in image quality.</p><p>Scaling Laws We assess the image quality of our CAR model as its depth increases. As illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, with increasing model depth, the CAR produces higher-quality images across five different conditions, demonstrating a lower FID metric alongside elevated IS, Precision, and Recall scores, which align with the scaling laws of autoregressive generative modeling <ref type="bibr" target="#b18">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b14">Henighan et al., 2020;</ref><ref type="bibr" target="#b17">Hoffmann et al., 2022)</ref>. The most high metrics are observed in HED Maps, Depth Maps, and Normal Maps, and Canny Edge and Sketch are relatively low, which is consistent with the observation in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">QUALITATIVE RESULTS</head><p>Overall Controllability and Image Quality Figure <ref type="figure" target="#fig_5">4</ref> qualitatively demonstrates that our CAR model generates high-quality and diverse results based on the given conditional controls. The visual details of various conditional inputs are effectively reflected in the generated images, ensuring a strong alignment between the images and their corresponding conditions. Notably, the categories shown are not among the 100 categories used during training, yet CAR still achieves precise control over these unseen categories, demonstrating that our CAR learns the general semantic information from the given conditional controls, rather than overfitting to the training set. This advantage highlights the cross-category generalization and robust controllability of our CAR framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Data Distribution</head><p>We analyze the controllability of CAR from the perspective of data distribution. Specifically, HED Maps are used as a type of condition to guide the image generation process, with this condition extracted from ground truth images. An uncontrollable vanilla autoregressive model <ref type="bibr" target="#b48">(Tian et al., 2024)</ref> is employed to generate comparison samples. We apply t-SNE <ref type="bibr" target="#b51">(Van der Maaten &amp; Hinton, 2008)</ref> to visualize the first two principal components of the embedding features from all generated images. These embedding features are extracted using the backbone of the HED Map extraction method <ref type="bibr" target="#b55">(Xie &amp; Tu, 2015)</ref>.</p><p>As illustrated in Figure <ref type="figure" target="#fig_6">5</ref>, there is a significant misalignment between the generation distribution of the vanilla autoregressive model and the ground truth, as the vanilla model lacks condition control information. In contrast, the distribution of the CAR model's generated results closely aligns with the ground truth, demonstrating that our samples accurately capture the visual details of the HED Map, bringing the HED embedding features closer to the ground truth. This highlights that our CAR model enhances both the controllability and accuracy of generated results I based on the provided condition control C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDIES</head><p>We conduct ablation studies on the ImageNet validation set to explore the different function choices for each component in the CAR framework, including F(•), T (•), and G(•).</p><p>Different Function Choices for F (•) We explore the impact of different methods for introducing conditional control c k to form s k in F(•). Specifically, we compare two strategies: 1) using the pre-trained VQ-VAE encoder from the VAR model to directly map the condition image to token maps at various scales; 2) our approach, which resizes the condition images to different scales at the pixel level and employs a shared, learnable convolutional encoder for control feature extraction. The results are shown in Table <ref type="table" target="#tab_2">3</ref>, where the learnable encoder shows significant improvements in IS scores, indicating enhanced image quality. We hypothesize that the pre-trained VQ-VAE encoder, being designed for image reconstruction, may not effectively capture image semantics, making it less suitable for extracting control semantics. Similar visualization results are illustrated in Figure <ref type="figure" target="#fig_7">6</ref>, where using the VQ-VAE encoder results in image distortion and poor quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Function Choices for T (•)</head><p>We design an encoder for T (•) to extract accurate and effective control representations ŝk . Specifically, we compare two architectures: 1) a simple convolutional network, and 2) a GPT-2-style Transformer. As shown in Table <ref type="table" target="#tab_2">3</ref> and Figure <ref type="figure" target="#fig_7">6</ref>, the Transformer shows significantly higher image quality compared to the simple convolutional network baseline, due to its powerful representation ability. Meanwhile, the Transformer-based en- Different Function Choices for G(•) We compare different injection functions G(•), where the control representation ŝk is injected into the image representation r k of a pre-trained autoregressive model to update the image representation rk . Specifically, we compare three techniques: 1) applying zero convolutions <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> to the control representation, followed by the addition of control and image features; 2) applying cross normalization <ref type="bibr">(Peng et al., 2024)</ref>, which normalizes the control representation using the mean and variance of the image representation, then adds these two features; 3) our method, which concatenates the two representations, applies a learnable LayerNorm to normalize the distributions, followed by a linear transformation to adjust the channel dimension.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, adding the image and control features leads to a decrease in the IS metric, regardless of whether zero convolution and cross normalization are applied before the addition. This indicates that these operations result in a reduction in image quality compared to our approach. The generation results in Figure <ref type="figure" target="#fig_7">6</ref> also demonstrate that these two baselines perform worse than our approach in terms of image quality and naturalness. We attribute this to the incompatibility of two different domain representations. Although cross normalization tries to align the distribution across the domain gap, such an operation is insufficient. Therefore, concatenating the two representations, followed by LayerNorm, more effectively harmonizes the conditional and backbone features, addressing discrepancies in data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose Controllable AutoRegressive Modeling (CAR), which establishes a novel paradigm for controlling VAR generation. CAR captures robust multi-scale control representations, which can be seamlessly integrated into pre-trained autoregressive models. Our experimental results demonstrate that CAR outperforms existing methods in both controllability and image quality, while also reducing the required computational costs. CAR represents a significant step forward in autoregressive visual generation, offering a flexible, efficient, and scalable solution for various controllable generation tasks.</p><p>Discussion and Future Works While the proposed CAR framework demonstrates advancements in controllable visual generation, it still faces certain limitations inherent in the underlying VAR model. Specifically, the reliance on sequential token prediction can sometimes limit the model's efficiency, especially when dealing with long image sequences or when requiring precise fine-grained control at high resolutions. The multi-scale injection mechanism used in CAR could also be extended to explore alternative injection strategies, such as attention-based or adaptive injection, to further enhance control precision. Additionally, although the current design excels at injecting control signals in a recursive manner, extending the framework to handle more complex tasks, such as video generation, remains an open challenge for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Controllable generation using CAR under various conditions. Results are 512 × 512.</figDesc><graphic coords="1,304.63,377.84,58.62,58.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the proposed Controllable AutoRegressive Modeling (CAR) framework. CAR integrates multi-scale latent variable modeling, where control representation is progressively refined and injected into the generation process of a pre-trained autoregressive model. Previous image tokens are accumulated and upsampled to form b k , which serves as the input token for scale k. Each scale's token map r k is predicted based on previous tokens and the corresponding control input c k , ensuring that the generated image I adheres to the specified visual conditions C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3.1 PRELIMINARY KNOWLEDGE OF AUTOREGRESSIVE MODELINGTraditional autoregressive models<ref type="bibr" target="#b50">Van Den Oord et al. (2017)</ref>;Esser et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>between the model's conditional distribution p(I | C; θ car ) and the true data distribution p data (I | C): L KL = E I,C∼pdata [log p data (I | C) -log p(I | C; θ car )] ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scaling laws of our CAR model. It can be observed that as the model depth increases, the four image quality metrics improve across the five conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results are presented for five distinct conditions, where the top row shows the input conditions, and the following rows display the generated images. These categories are excluded from the training set, demonstrating that the CAR learns the general semantics from the input conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: T-SNE visualization of the distribution of generation results from our CAR model and the vanilla model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of different function choices given (a) control input: (b) using a pre-trained VQ-VAE encoder in F(•), (c) designing a convolutional network in T (•), (d) applying zero convolution in G(•), (e) using cross normalization in G(•), and (f) our final architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with previous controllable generation approaches on ImageNet. Our CAR surpasses these works by delivering higher image quality while being much more efficient in inference.</figDesc><table><row><cell>Methods</cell><cell cols="2">Canny Edge FID ↓ IS ↑</cell><cell cols="2">Depth Map FID ↓ IS ↑</cell><cell cols="2">Normal Map FID ↓ IS ↑</cell><cell cols="2">HED Map FID ↓ IS ↑</cell><cell cols="2">Sketch FID ↓ IS ↑</cell><cell>Time (s) ↓</cell></row><row><cell>T2I-Adapter</cell><cell>10.2</cell><cell>156.6</cell><cell>9.9</cell><cell>133.6</cell><cell>9.5</cell><cell>142.8</cell><cell>9.3</cell><cell>141.6</cell><cell>16.2</cell><cell>156.0</cell><cell>2.3</cell></row><row><cell>ControlNet</cell><cell>11.6</cell><cell>172.7</cell><cell>9.2</cell><cell>150.3</cell><cell>8.9</cell><cell>155.3</cell><cell>8.6</cell><cell>150.3</cell><cell>15.3</cell><cell>162.5</cell><cell>1.7</cell></row><row><cell>CAR (Ours)</cell><cell>8.3</cell><cell>167.3</cell><cell>6.9</cell><cell>178.6</cell><cell>6.6</cell><cell>175.9</cell><cell>5.6</cell><cell>182.2</cell><cell>10.2</cell><cell>161.6</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>User Studies We conduct the user studies with 30 participants to evaluate the genera-tion performance of our CAR in comparison with previous methods, ControlNet and T2I-Adapter. For each of the five types of condi-</cell><cell cols="4">: User studies are conducted to evaluate three controllable generative approaches based on three criteria: 1) IQ: image quality, 2) CF: condi-tion fidelity, and 3) ID: image diversity.</cell></row><row><cell>tions, we input 30 condition images and gener-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ate corresponding results for each method, pro-</cell><cell>Methods</cell><cell>IQ ↑</cell><cell>CF ↑</cell><cell>ID ↑</cell></row><row><cell>ducing 150 results per method. For each condi-tional input, participants are required to choose the best one based on three criteria: 1) image quality, 2) condition fidelity, and 3) image di-</cell><cell>T2I-Adapter ControlNet CAR (Ours)</cell><cell>23% 26% 51%</cell><cell>27% 31% 42%</cell><cell>19% 36% 45%</cell></row><row><cell>versity. As demonstrated in Table 2, our CAR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">outperforms ControlNet and T2I-Adapter in all three aspects, demonstrating the effectiveness of</cell></row><row><cell>proposed Controllable Autoregressive Modeling.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of IS metrics for different function choices, including F (•), T (•), and G(•).</figDesc><table><row><cell></cell><cell>Settings</cell><cell cols="5">Canny Depth Normal HED Sketch</cell></row><row><cell>F(•)</cell><cell cols="2">Pre-trained VQ-VAE Encoder Learnable Convolutional Encoder 166.2 131.3</cell><cell>139.2 177.9</cell><cell>141.0 173.0</cell><cell></cell><cell>149.7 123.5 181.9 159.3</cell></row><row><cell>T (•)</cell><cell>Convolutional Network Transformer Network</cell><cell>145.7 166.2</cell><cell>156.8 177.9</cell><cell>153.2 173.0</cell><cell></cell><cell>159.6 142.6 181.9 159.3</cell></row><row><cell></cell><cell>Zero Convolution &amp; Add</cell><cell>161.9</cell><cell>170.3</cell><cell>168.7</cell><cell></cell><cell>173.5 153.8</cell></row><row><cell>G(•)</cell><cell>Cross Normalization &amp; Add</cell><cell>160.7</cell><cell>170.1</cell><cell>169.3</cell><cell></cell><cell>173.1 154.2</cell></row><row><cell></cell><cell>Concat &amp; LayerNorm &amp; Linear</cell><cell>166.2</cell><cell>177.9</cell><cell>173.0</cell><cell></cell><cell>181.9 159.3</cell></row><row><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>y</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>20 0</cell><cell></cell><cell cols="3">Ground Truth Controllable Autoregressive Vanilla Autoregressive</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>20</cell><cell>x</cell><cell>0</cell><cell>20</cell><cell>40</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ba</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<biblScope unit="page" from="2023" to="2023" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023-03">2023b. 3</date>
			<biblScope unit="page" from="22563" to="22575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Tom B Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiadi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06135</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">April 2023. 2023</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Emu: Enhancing image generation models using photogenic needles in a haystack</title>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15807</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Runpei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunrui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11499</idno>
		<title level="m">Synergistic multimodal comprehension and creation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnathan</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parmida</forename><surname>Atighehchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7346" to="7356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Planting a seed of vision in large language model</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08041</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Seed-x: Multimodal models with unified multi-granularity comprehension and generation</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14396</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09750</idno>
		<title level="m">Controlvar: Exploring controllable visual autoregressive modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining</title>
		<author>
			<persName><forename type="first">Dongyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.02657</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unified-io: A unified model for vision, language, and multi-modal tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Marten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26439" to="26455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Open-magvit2: An open-source project toward democratizing auto-regressive visual generation</title>
		<author>
			<persName><forename type="first">Zhuoyan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengyuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.04410</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10797</idno>
		<title level="m">Star: Scale-wise text-to-image generation via auto-regressive representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Controlnext: Powerful and efficient control for image and video generation</title>
		<author>
			<persName><forename type="first">Bohao</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06070</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2006">2019. 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1623" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixel difference networks for efficient edge detection</title>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dewen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5117" to="5127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Autoregressive model beats diffusion: Llama for scalable image generation</title>
		<author>
			<persName><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyue</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06525</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Chameleon: Mixed-modal early-fusion foundation models</title>
		<author>
			<persName><forename type="first">Chameleon</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.09818</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Visual autoregressive modeling: Scalable image generation via next-scale prediction</title>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyue</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02905</idno>
		<imprint>
			<date type="published" when="2009">2024. 2, 3, 4, 6, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Igor</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew R Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00463</idno>
		<title level="m">A dense indoor and outdoor depth dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Real-time diffusion-based text-to-image generation using latent space sampling</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00110</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Jinheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Qinghong Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12528</idno>
		<title level="m">Show-o: One single transformer to unify multimodal understanding and generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Magvit: Masked generative video transformer</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023-03">2023a. 3</date>
			<biblScope unit="page" from="10459" to="10469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Language model beats diffusiontokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05737</idno>
		<imprint>
			<date type="published" when="2023-03">2023b. 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Auto-regressive image synthesis with integrated quantization</title>
		<author>
			<persName><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongliang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2023. 2, 3, 7</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Shamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.11039</idno>
		<title level="m">Transfusion: Predict the next token and diffuse images with one multi-modal model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
