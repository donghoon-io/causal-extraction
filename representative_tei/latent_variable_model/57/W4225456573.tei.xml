<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-TASK NEURAL PROCESSES</title>
				<funder>
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder ref="#_nJbpjC5">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_4tbh6Xx #_VXSa3UM">
					<orgName type="full">Institute of Information &amp; communications Technology Planning &amp; Evaluation</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-03-25">25 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Donggyun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Seongwoong</forename><surname>Cho</surname></persName>
							<email>seongwoongjo@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Wonkwang</forename><surname>Lee</surname></persName>
							<email>wonkwang.lee@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
							<email>seunghoon.hong@kaist.ac.kr</email>
						</author>
						<title level="a" type="main">MULTI-TASK NEURAL PROCESSES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-25">25 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.14953v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g., multiple attributes and multi-sensor data), it is beneficial to infer them jointly and exploit the underlying correlation to improve the predictive performance. To this end, we propose Multi-Task Neural Processes (MTNPs), an extension of NPs designed to jointly infer tasks realized from multiple stochastic processes. We build MTNPs in a hierarchical way such that inter-task correlation is considered by conditioning all per-task latent variables on a single global latent variable. In addition, we further design our MTNPs so that they can address multi-task settings with incomplete data (i.e., not all tasks share the same set of input points), which has high practical demands in various applications. Experiments demonstrate that MTNPs can successfully model multiple tasks jointly by discovering and exploiting their correlations in various real-world data such as time series of weather attributes and pixel-aligned visual modalities. We release our code at <ref type="url" target="https://github.com/GitGyun/multi_task_neural_processes">https://github.com/GitGyun/multi_task_neural_processes</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural Processes (NPs) <ref type="bibr">(Garnelo et al., 2018b)</ref> are a class of meta-learning methods that model a distribution of functions (i.e. a stochastic process). By considering a task as a function realized from the underlying stochastic process, they can flexibly adapt to various unseen tasks through inference on functions. The adaptation requires only one forward step of a trained neural network without any costly retraining or fine-tuning, and has linear complexity to the data size. NPs can also quantify their prediction uncertainty, which is essential in risk-sensitive applications <ref type="bibr" target="#b6">(Gal &amp; Ghahramani, 2016)</ref>. Thanks to such appealing properties, there have been increasing attempts to improve NPs in various domains, such as image regression <ref type="bibr" target="#b18">(Kim et al., 2019;</ref><ref type="bibr" target="#b9">Gordon et al., 2020)</ref>, image classification <ref type="bibr" target="#b30">(Requeima et al., 2019;</ref><ref type="bibr" target="#b36">Wang &amp; Van Hoof, 2020)</ref>, time series regression <ref type="bibr" target="#b29">(Qin et al., 2019;</ref><ref type="bibr" target="#b25">Norcliffe et al., 2021)</ref>, and spatio-temporal regression <ref type="bibr" target="#b31">(Singh et al., 2019)</ref>. In this paper, we explore extending NPs to a multi-task setting where correlated tasks are realized simultaneously from multiple stochastic processes. Many real-world data represent multiple correlated functions, such as different attributes or modalities. For instance, medical data <ref type="bibr" target="#b16">(Johnson et al., 2016;</ref><ref type="bibr" target="#b12">Harutyunyan et al., 2019)</ref> or climate data <ref type="bibr" target="#b37">(Wang et al., 2016)</ref> contain various correlated attributes on a patient or a region that need to be inferred simultaneously. Similarly, in multi-task vision data <ref type="bibr" target="#b22">(Lin et al., 2014;</ref><ref type="bibr">Zhou et al., 2017;</ref><ref type="bibr" target="#b39">Zamir et al., 2018)</ref>, multiple labels of different visual modalities are associated with an image. In such scenarios, it is beneficial to exploit functional correlation by modeling the functions jointly rather than independently, in terms of performance and efficiency <ref type="bibr">(Caruana, 1997)</ref>. Unfortunately, naive NPs lack mechanisms to jointly handle a set of multiple functions and cannot capture their correlations either. This motivates us to extend NPs to model multiple tasks jointly by exploiting the inter-task correlation. In addition to extending NPs to multi-task settings, we note that handling multi-task data often faces a practical challenge where observations can be incomplete (i.e. not all the functions share the common sample locations). For example, when we collect multi-modal signals from different sensors, the sensors may have asynchronous sampling rates, in which case we can observe signals from only an arbitrary subset of sensors at a time. To fully utilize such incomplete observations, the model should be able to associate functions observed in different inputs such that it can improve the predictive performance of all functions using their correlation. A multivariate extension of Gaussian Processes (GPs) <ref type="bibr">(Álvarez et al., 2012)</ref> can handle incomplete observations to infer multiple functions jointly. However, naive GPs suffer from cubic complexity to the data size and needs approximations to reduce the complexity. Also, their behaviour depend heavily on the kernel choice <ref type="bibr" target="#b18">(Kim et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published as a conference paper at ICLR 2022</head><p>To address these challenges, we introduce Multi-Task Neural Processes (MTNPs), a new family of stochastic processes that jointly models multiple tasks given possibly incomplete data. We first design a combined space of multiple functions, which allows not only joint inference on the functions but also handling incomplete data. Then we define a Latent Variable Model (LVM) of MTNP that theoretically induces a stochastic process over the combined function space. To exploit the inter-task correlation, we introduce a hierarchical LVM consists of (1) a global latent variable that captures knowledge about all tasks and (2) task-specific latent variables that additionally capture knowledge specific to each task conditioned on the global latent variable. Inducing each task conditioned on the global latent, the hierarchical LVM allows MTNP to effectively learn and exploit functional correlation in multi-task inference. MTNP also inherits advantages of NP, such as flexible adaptation, scalable inference, and uncertainty-aware prediction. Experiments in synthetic and real-world datasets show that MTNPs effectively utilize incomplete observations from multiple tasks and outperform several NP variants in terms of accuracy, uncertainty estimation, and prediction coherency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>2.1 BACKGROUND: NEURAL PROCESSES We consider a task f t : X → Y t as a realization of a stochastic process over a function space (Y t ) X that generates a data D t = (X D , Y t D ) = {(x i , y t i )} i∈I(D t ) , where I(D t ) denotes a set of data index. Neural Processes (NPs) use a conditional latent variable model to learn the stochastic process. Given a set of observations C t = (X C , Y t C ) = {(x i , y t i )} i∈I(C t ) , NP infers the target task f t through a latent variable z and models the data D t by a factorized conditional distribution p(Y p(y t i |x i , z)p(z|C t )dz.</p><p>(1)</p><p>We refer to the set of observations C t as a context data and the modeling data D t as a target data.</p><p>NP models the generative model p(Y t D |X D , z) and the conditional prior p(z|C t ) by two neural networks, a decoder p θ and an encoder q φ , respectively. Since the direct optimization of Eq.1 is intractable, the networks are trained by maximizing the following variational lower-bound.</p><p>log p θ (Y t D |X D , C t ) ≥ E q φ (z|D t ) [log p θ (Y t D |X D , z)] -D KL (q φ (z|D t )||q φ (z|C t )).</p><p>(2) Note that the decoder network q φ is also used as a variational posterior q φ (z|D). The parameter sharing between model prior and variational posterior gives us an intuitive interpretation of the loss function: the KL term acts as a regularizer for the encoder q φ such that the summary of the context is close to the summary of the target. This reflects the assumption that the context and target are generated by the same underlying data-generating process and aids effective test-time adaptation. After training, NP infers the target function according to the latent variable model (Eq.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EXTENDING TO MULTIPLE TARGET FUNCTIONS</head><p>Now we extend the setting to multi-task learning problems where multiple tasks f 1 , • • • , f T are realized from T stochastic processes simultaneously, each of which has its own function space (Y t ) X , ∀t ∈ T = {1, 2, • • • , T }. Let D = (X D , Y 1:T D ) = t∈T D t be a multi-task target data, where each D t corresponds to the data of task f t . Then the learning objective for the set of T realized tasks is to model the conditional probability p(Y 1:T D |X D , C) given the multi-task context C = (X C , Y 1:T C ) = t∈T C t , where each C t is a set of observations of task f t . The sets C and D can be arbitrarily chosen, but we assume C ⊂ D for simplicity. However, assuming the complete context C for all tasks is often challenged by many practical issues, such as asynchronous sampling across multiple sensors or missing labels in multi-attribute data. To address such challenges, we relax the assumptions on context C and let I(C t ) be different across t ∈ T . In this case, an input point x i can be associated with a partial set of output values {y t i } t∈Ti , T i T , which is referred incomplete observation. Next, we present two ways to use NPs to model the multi-task data and discuss their limitations. Single-Task Neural Processes (STNPs) A straightforward application of NPs to the multi-task setting is assuming independence across tasks and define independent NPs over the function spaces (Y 1 ) X , • • • , (Y T ) X . We refer to this approach as Single-Task Neural Processes (STNPs). Specifically, a STNP has T independent latent variables v 1 , • • • , v T , where each v t implicitly represents a task f t . Thanks to the independence assumption, STNPs can handle incomplete context by conditioning on each task-specific data C t independently. However, this approach can only model the marginal distributions for each task, ignoring complex inter-task correlation within the joint distribution of the tasks. Note that this is especially impractical for multi-task settings under the incomplete data since each task f t can be learned only from C t , ignoring rich contexts available in other data C t , ∀t = t.</p><formula xml:id="formula_0">p(Y 1:T D |X D , C) = T t=1 p(Y t D |X D , v t )p(v t |C t )dv t .<label>(3)</label></formula><p>Joint-Task Neural Process (JTNP) An alternative approach is to combine output spaces to a product space Y 1:T = t∈T Y t and define a single NP over the function space (Y 1:T ) X . We refer to this approach as Joint-Task Neural Processes (JTNPs). In this case, a single latent variable z governs all T tasks jointly.</p><formula xml:id="formula_1">p(Y 1:T D |X D , C) = p(Y 1:T D |X D , z)p(z|C)dz.<label>(4)</label></formula><p>JTNPs are amenable to incorporate correlation across tasks through the shared variable z. However, by definition, they require complete context and target for both training and inference. This is because any incomplete set of output values {y t i } t∈Ti for an input point x i such that T i = T is not a valid element of the product space Y 1:T . In addition, it relies solely on a single latent variable to explain all tasks, ignoring per-task stochastic factors in each function</p><formula xml:id="formula_2">f t .</formula><p>In what follows, we propose an alternative formulation for jointly handling multiple tasks on incomplete data, which (1) enables a probabilistic inference on the incomplete data and ( <ref type="formula">2</ref>) is more amenable for learning both task-specific and task-agnostic functional representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK NEURAL PROCESSES</head><p>In this section, we describe Multi-Task Neural Processes (MTNPs), a family of stochastic processes to model multiple functions jointly and handle incomplete data. We first formulate MTNPs using a hierarchical LVM. Then we propose the training objective and a neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FORMULATION</head><p>Our objective is to extend NPs to jointly infer multiple tasks from incomplete context. Discussions in Section 2.2 suggest that direct modeling of a distribution over functions of form f : X → t∈T Y t is achievable via JTNP (Eq. 4), yet it requires complete data in both training and inference. To circumvent this problem, we reformulate the functional form by h : X × T → t∈T Y t . Note that this functional form allows us to model the same set of functions as JTNP by f</p><formula xml:id="formula_3">(x i ) = (h(x i , 1), • • • , h(x i , T ))</formula><p>. However, by using the union form we can exploit incomplete data since any partial set of output values {y t i } t∈Ti now becomes a set of valid output values at different input points (x i , t), t ∈ T i . For notational convenience, we denote x t i = (x i , t) and assume input points in the context C and the target D are embedded by the task indices, i.e., C = (X</p><formula xml:id="formula_4">1:T C , Y 1:T C ) = t∈T C t where C t = (X t C , Y t C ) = {(x t i , y t i )} i∈I(C t )</formula><p>and the same for D. Next, we present a latent variable model that induces a stochastic process over functions of form h. To make use of both task-agnostic and task-specific knowledge, we define a hierarchical latent variable model (Figure <ref type="figure" target="#fig_0">1(c)</ref>). In this model, the global latent variable z captures shared stochastic factors across tasks using the whole context C, while per-task stochastic factors are captured by the task-specific latent variable v t using C t and z. It induces the predictive distribution on the target by:</p><formula xml:id="formula_5">p(Y 1:T D |X 1:T D , C) = T t=1 p(Y t D |X t D , v t )p(v t |z, C t ) p(z|C)dv 1:T dz,<label>(5)</label></formula><p>where v 1:T := (v 1 , • • • , v T ). Similar to Eq. 1, we assume the conditional independence on p(Y t D |X t D , v t ). Note that this hierarchical model can capture and leverage the inter-task correlation by sharing the same z across v 1:T . Also, it is amenable to fully utilize the incomplete data: since the global variable z is inferred from the entire context data C = t∈T C t and is conditioned to infer task-specific latent variable v t , each function f t induced by v t exploits the observations available for not only itself C t , but also for other tasks C t , ∀t = t. Next, we show that Eq. 5 induces a stochastic process over the functions of form h : X × T → t∈T Y t . Proposition 1. Consider the following generative process on data D and context C, which is a generalized form of Eq. 5.</p><p>z ∼ p(z|C), v t ∼ p(v t |z, t, C), y t i ∼ p(y t i |x t i , v t ), ∀t ∈ T , ∀i ∈ I(D).</p><p>(6) Then under some mild assumptions, there exists a stochastic process over functions of form h : X × T → t∈T Y t , where the data D is generated. Proof. We leave the proof in Appendix A.2. We refer to the resulting stochastic processes as Multi-Task Neural Processes (MTNPs). In the perspective of stochastic process, Eq. 5 allows us to learn functional posterior not only on each task via v t , but also across the tasks via z. Then optimizing Eq. 5 can be interpreted as learning to learn each task captured by v t together with the functional correlation captured by z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING AND INFERENCE</head><p>We use an encoder network q φ and a decoder network p θ to approximate the conditional prior and generative model in Eq. 5, respectively. Since the direct optimization of Eq. 5 is intracable, we train the networks via the following variational lower bound, where we use the same network q φ for both conditional prior and variational posterior as in NP:</p><formula xml:id="formula_6">log p θ (Y 1:T D |X 1:T D , C) ≥ E q φ (z|D) T t=1 E q φ (v t |z,D t ) [log p θ (Y t D |X t D , v t )]] -D KL q φ (v t |z, D t ) || q φ (v t |z, C t ) -D KL q φ (z|D) || q φ (z|C) ,<label>(7)</label></formula><p>We leave the derivation in Appendix A.3. The above objective reflects several desirable behaviors for our model. Similar to NP, the KL divergences encourage that both latent variables z and v t inferred from the context data are consistent with those inferred from the entire target data. On the other hand, we observe that minimizing the KL divergence on task-specific variables forces the global latent z to be informative across all tasks, such that it can induce the task-specific factors v t from the limited context C t . This makes the model encode correlated information across tasks in z and use it for inferring each task with v t , which is critically important for joint inference with incomplete context data. After training, MTNP infers the target functions according to the latent variable model (Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NEURAL NETWORK MODEL FOR MTNP</head><p>This section presents an implementation of MTNPs composed of an encoder q φ and a decoder p θ (Eq. 7). While our MTNP formulation is not restricted to a specific architecture, we adopt ANP <ref type="bibr" target="#b18">(Kim et al., 2019)</ref> as our backbone, which implements the encoder by attention layers <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> and the decoder by a MLP. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the overall architecture.</p><p>In the following, we denote a stacked multi-head attention block <ref type="bibr" target="#b27">(Parmar et al., 2018)</ref> by Attn(Q, K, V ) and a MLP by ψ(x). Also, we denote e t by a learnable task embedding for t ∈ T which is used to condition on the task index t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Encoder</head><p>The latent encoder samples global and per-task latent variables by aggregating the context C. For each context example (x t i , y t i ) ∈ C t , we first project it to a hidden representation s t i = ψ s (x i , y t i ) + e t . Then we aggregate them to a task-specific representation s t via self-attention followed by a pooling operation, which is further aggregated to a global representation s.</p><formula xml:id="formula_7">s t = pool(Attn({s t i } i∈I(C t ) , {s t i } i∈I(C t ) , {s t i } i∈I(C t ) )), ∀t ∈ T ,<label>(8)</label></formula><p>s = pool(Attn({s t } t∈T , {s t } t∈T , {s t } t∈T )).</p><p>(9) Note that the first attention is applied along the example axis (per-task) to encode information of each task, while the second one is applied along the task axis (across-task) to aggregate the information across the tasks. Then, we get the global and task-specific latent variables via ancestral sampling. Deterministic Encoder To further improve the expressiveness of model, we extend the deterministic encoder of <ref type="bibr" target="#b18">Kim et al. (2019)</ref> that produces local representation specific to both target example and task via attention mechanism. As in the latent encoder, we first project each context example</p><formula xml:id="formula_8">z ∼ q φ (z|C) = N (ψ (z,1) (s), ψ (z,2) (s)),<label>(10)</label></formula><formula xml:id="formula_9">v t ∼ q φ (v t |z, C t ) = N (ψ (v t ,1) (s t , z), ψ (v t ,2) (s t , z)), ∀t ∈ T .<label>(11)</label></formula><formula xml:id="formula_10">(x t i , y t i ) ∈ C t to a hidden representation d t i = ψ d (x i , y t i</formula><p>) + e t that serves as value embedding in cross-attention. Also, we use context and target input x t i as key and query embeddings for the crossattention, respectively. Then we apply cross-attention along the example axis (per-task) followed by self-attention along the task axis (across-task).</p><formula xml:id="formula_11">{u t i } i∈I(D t ) = Attn({x t i } i∈I(C t ) , {x t i } i∈I(C t ) , {d t i } i∈I(D t ) ), ∀t ∈ T , (<label>12</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">{r t i } t∈T = Attn({u t i } t∈T , {u t i } t∈T , {u t i } t∈T ), ∀i ∈ I(D). (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>Decoder Finally, the decoder produces predictive distributions for the target output y t i ∈ Y t D for each target input x t i . We first project the input to w t i = ψ w (x i ) + e t , then concatenate it with the corresponding latent variable v t and determinstic representation r t i . The output distribution is computed by MLPs, whose output depends on the type of the task.</p><formula xml:id="formula_15">y t i ∼ N (ψ (y,1) (w t i , v t , r t i ), ψ (y,2) (w t i , v t , r t i )), if y t i is continuous, Categorical(ψ (y,1) (w t i , v t , r t i )), if y t i is discrete, ∀i ∈ I(D t ), ∀t ∈ T . (14)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Stochastic Processes for Multi-Task Learning There exist several stochastic processes related to MTNPs that consider learning multiple tasks. Multi-Output Gaussian Processes (MOGPs) <ref type="bibr">(Álvarez et al., 2012)</ref> extend Gaussian Processes (GPs) to infer multiple tasks together, and also handle incomplete data. However, MOGPs are usually trained on a single set of tasks, thus require a lot of observations to produce accurate predictions. Recently there have been some attempts to combine meta-learning and GPs <ref type="bibr" target="#b5">(Fortuin et al., 2019;</ref><ref type="bibr" target="#b33">Titsias et al., 2020)</ref>, while they do not consider multitask settings. Sequential Neural Processes (SNPs) <ref type="bibr" target="#b31">(Singh et al., 2019;</ref><ref type="bibr" target="#b38">Yoon et al., 2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate MTNP on three datasets, including both synthetic and real-world tasks. In all experiments, we construct incomplete context data by selecting a complete subset C ⊂ D of size m = |I(C)| from the target, then randomly drop the output points independently according to the missing rate γ ∈ [0, 1] (γ = 0 means complete data). We repeat the procedure with five different random seeds and report the mean values of each evaluation metric.</p><p>Baselines In each experiment, we compare MTNP with two NP variants, STNP and JTNP. We adopt ANP <ref type="bibr" target="#b18">(Kim et al., 2019)</ref> as a backbone architecture for STNP and JTNP, which is a strong NP baseline. Since JTNP cannot handle incomplete data, we build a stronger baseline by the combination of STNP and JTNP (S+JTNP), where missing labels are imputed by STNP and then used to jointly infer the tasks by JTNP. In 1D regression tasks, we additionally compare two Multi-Output Gaussian Processes baselines, CSM <ref type="bibr" target="#b34">(Ulrich et al., 2015)</ref> and MOSM <ref type="bibr" target="#b28">(Parra &amp; Tobar, 2017)</ref>, and two metalearning baselines, MAML <ref type="bibr" target="#b4">(Finn et al., 2017)</ref> and Reptile <ref type="bibr" target="#b24">(Nichol et al., 2018)</ref>, where we slightly modify the meta-learning baselines to learn multiple tasks jointly from incomplete data. At training time, we set γ = 0.5 for all models but keeping γ = 0 for JTNP. At test time, we evaluate the models in various missing rates γ ∈ {0, 0.25, 0.5, 0.75}. We provide architectural and training details in Appendix B. We also provide ablation studies on architectural designs such as self-attentions and pooling, parameter sharing and task embedding, latent and deterministic encoders in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">1D CURVE REGRESSION ON SYNTHETIC DATA</head><p>Dataset and Metric We begin with 1D synthetic regression tasks where the target functions are correlated by shared parameters (e.g., scale, bias, phase) but have different shapes. Inspired by <ref type="bibr">Guo et al. (2020b)</ref>, we first randomly sample global parameters a, b, c, w ∈ R shared across the tasks, then generate four correlated tasks using different activation functions as follows.</p><formula xml:id="formula_16">y t i = a • act t (wx i + b) + c, act t ∈ {Sine, Tanh, Sigmoid, Gaussian}, x i ∼ U(-5, 5).<label>(15)</label></formula><p>To simulate task-specific stochasticity, we perturb the parameters (a, b, c, w) with small i.i.d. Gaussian noises per task. In this setting, the model has to learn per-task functional characteristics imposed by different activation functions and per-task noises, as well as how to share the underlying parameters unseen during training among the tasks. For evaluation, we generate training and testing sets via non-overlapping splits of the parameters, then measure mean squared error (MSE) normalized by the scale parameter a to aggregate results on functions with different scales. See Appendix C for details.</p><p>Results Table <ref type="table" target="#tab_2">1</ref> shows the quantitative results with γ = 0.5. More comprehensive results with different missing rates and standard deviations for the metrics are provided in Appendix D. As it shows, MTNP outperforms all baselines in all tasks and context sizes. This can be attributed to the ability of MTNP to (1) exploit all available context examples to infer inter-task general knowledge (i.e. a, b, c, w) and (2) translate it back to functional representations for each task. In contrast, STNP fails to predict multiple tasks accurately due to the independent task assumption. Although JTNP is designed to discover and utilize inter-task correlations, its performances do not show dramatic improvement over STNP since its observations are largely based on noisy imputations from STNP. We also observe that GP baselines (MOSM, CSM) perform even worse than STNP when the context size is small, despite their inherent ability to joint inference on incomplete data. We conjecture that it is because GPs lack a meta-training mechanism that allows NPs (and MTNPs) to quickly learn the tasks using a few examples. Gradient-based meta-learning baselines (MAML, Reptile) are also comparable to STNP and JTNP but perform worse than MTNP. This could be due to the lack of global inference on function space, which leads them to overfit the context points. As an illustrating example, we also plot predicted distributions from the models in a highly incomplete scenario (m = 10 and γ = 0.5) in Figure <ref type="figure" target="#fig_2">3</ref> (a). We observe that STNP generally suffers from inaccurate predictions due to limited context, while MTNP successfully exploits incomplete observations from different tasks to improve the predictive performance. The qualitative results for all baselines are provided in Appendix D.</p><p>We also perform an ablation study on the latent variable model to justify the effectiveness of our hierarchical formulation. We consider two variants of MTNP that consist of the global latent variable only (MTNP-G) and the task-specific latent variables only (MTNP-T). Then we evaluate the models in three synthetic datasets generated with different levels of inter-task correlation. Specifically, we construct partially correlated tasks as described before, totally correlated tasks by removing the task-specific noises, and independent tasks by sampling the parameters a, b, c, w independently for   <ref type="figure">-G</ref>). On the other hand, MTNP and MTNP-T do not suffer from such a negative transfer since each of the independent tasks can be addressed by per-task latent variables separately. The overall results demonstrate that incorporating both global and task-specific information is the most effective and robust against various levels of inter-task correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">1D TIME-SERIES REGRESSION ON WEATHER DATA</head><p>Dataset and Metric To demonstrate our method in a practical, real-world domain, we perform an experiment on weather data. Weather attributes are physically correlated with each other, and the observations are often incomplete due to different sensor configurations or coverage per station. Also, the observed attributes are highly stochastic, making MTNP's stochastic process formulation fits it well. We use a dataset gathered by Dark Sky API<ref type="foot" target="#foot_0">foot_0</ref> , consisting of 12 daily weather attributes collected at 266 cities for 258 days. We choose six attributes, namely low and high temperatures (TempMin, TempMax), humidity (Humidity), precipitation probability (Precip), cloud cover (Cloud), and dew point (Dew), which forms six correlated tasks. We normalize each attribute to be standard Gaussian and the time to be in [0, 1]. We divide the data into 200 training, 30 valid, and 33 test sets of time series, where each set corresponds to a unique city. We evaluate the prediction performance by MSE. Since the data is noisy, we also report negative log-likelihood as a metric of uncertainty estimation.</p><p>Results Table <ref type="table" target="#tab_3">2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">2D IMAGE COMPLETION ON FACE DATA Dataset and Metric</head><p>We further demonstrate our approach to more challenging 2D structured function regression tasks. Following <ref type="bibr">Garnelo et al. (2018a)</ref>, we interpret an RGB image as a function that maps a 2D pixel location x i ∈ [0, 1] 2 to its RGB values y i ∈ [0, 1] 3 , and extend its concept to pixel-aligned 2D spatial data for the multi-task setting. Specifically, we consider four pixel-aligned visual modalities with a resolution of 32 × 32 on celebrity faces as a set of tasks, namely RGB image (RGB) <ref type="bibr" target="#b23">(Liu et al., 2015)</ref>, semantic segmentation map (Segment) <ref type="bibr">(Lee et al., 2020)</ref>, Sobel edge (Edge) <ref type="bibr" target="#b17">(Kanopoulos et al., 1988)</ref>, and Projected Normalized Coordinate Code (PNCC) <ref type="bibr">(Zhu et al., 2016)</ref>. We then construct training and testing sets with non-overlapping splits of face images.</p><p>To evaluate the Segment task, we report mean Intersection-over-Union (mIoU). For the other tasks, we report MSE. We also measure prediction coherency across tasks to evaluate the task correlation captured by models. To measure the coherency between the predictions, we generate pseudo-labels by translating the RGB prediction into the other three modalities using image-to-image translation methods <ref type="bibr" target="#b17">(Kanopoulos et al., 1988;</ref><ref type="bibr">Guo et al., 2020a;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>, then measure errors (MSE or 1 -mIoU) between the pseudo-labels and predictions. Additional details are provided in Appendix F.</p><p>Results Table <ref type="table" target="#tab_4">3</ref> summarizes the quantitative comparison results. More comprehensive results with different missing rates are provided in Appendix G. Overall, we observe similar results with the 1D regression experiments where MTNP generates more accurate predictions over STNP and S+JTNP by effectively exploiting the incomplete data. We also observe that the MTNP produces more coherent predictions over the baselines, which shows that it indeed learns to exploit the correlation across tasks effectively. To further validate the results, we present qualitative comparison results in Figure <ref type="figure" target="#fig_4">5</ref>. We observe that STNP and S+JTNP produce inaccurate (red boxes) or incoherent (green box) outputs when the number of contexts is extremely small. On the other hand, MTNP (1) consistently regresses coherent functions regardless of the number of observable contexts, and (2) its predictions are more accurate than the baselines given the same number of contexts (green box). Finally, we investigate the discovery and exploitation of task correlations achieved by MTNP. We first partition tasks into source and target tasks. Then, we measure relative performance improvement on the target tasks before and after the model observes data from source tasks. We summarize the results in Table <ref type="table" target="#tab_5">4</ref>, where we average performance gains coming from all possible combinations of source tasks for each target task. By observing which task is the most beneficial to each of the other tasks, we observe that there are two groups of highly correlated tasks (RGB-Edge) and (Segment-PNCC). These  <ref type="table" target="#tab_33">S+JTNP 0.0421 0.0129 0.0046 0.0338 0.0190 0.0090 0.6316 0.4341 0.3171 0.0105 0.0021 0</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose Multi-Task Neural Processes (MTNPs), a new family of stochastic processes designed to infer multiple functions jointly from incomplete data, along with a hierarchical latent variable model.</p><p>Through extensive experiments, we demonstrate that the proposed MTNPs can leverage incomplete data to solve multiple heterogenous tasks by learning to discover and exploit task-agnostic and task-specific knowledge. Scaling up our method to large-scale datasets will be a promising research direction. To this end, our method can be improved in several aspects by (1) generalizing to unseen task space T and (2) allowing empty context data for some tasks such that we can generalize MTNPs in more diverse real-world scenarios such as zero-shot inference and semi-supervised learning.</p><p>Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.</p><p>Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan Z. Li. Face alignment across large poses: A 3d solution. In CVPR, 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A THEORETICAL JUSTIFICATIONS</head><p>In this section, we give a proof of Proposition 1 with a brief introduction of the Kolmogorov Extension Theorem <ref type="bibr" target="#b15">(Itô et al., 1984)</ref>, and derive training objectives of STNP, JTNP, and MTNP.</p><p>A.1 STOCHASTIC PROCESSES AND THE KOLMOGOROV EXTENSION THEOREM A stochastic process F : X × Ω → Y is a collection of random variables {Y x : Ω → Y} x∈X which is indexed by an index set X . Also, all the random variables are defined on a single probability space (Ω, F, P) and a value space Y. This can be interpreted as a distribution over a function space Y X , such that sampling a function</p><formula xml:id="formula_17">f corresponds to f (•) = F (•, ω), ω ∈ Ω. Another interpretation of F is a random function, since F (x, •) = Y x is a random variable.</formula><p>Suppose we have observed input and output sequences</p><formula xml:id="formula_18">X = (x 1 , x 2 , • • • , x n ) and Y = (y x1 , y x2 , • • • , y xn ) of a function f : X → Y. With a slight abuse of a notation, let p(Y |X) = ρ X (Y )</formula><p>be the marginal distribution of Y on a product space n i=1 Y, where each i-th space of the product is the output space of Y xi , 1 ≤ i ≤ n. Then by the Kolmogorov Extension Theorem, the data (X, Y ) induces a stochastic process</p><formula xml:id="formula_19">F such that ∃ ω ∈ Ω s.t. y xi = F (x i , ω) for all i = 1, 2, • • • , n, if the distribution p(Y |X)</formula><p>satisfies two conditions: consistency and exchangability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">(Consistency) For any</head><formula xml:id="formula_20">m such that 1 ≤ m &lt; n, p(Y |X)dY m+1:n = p(Y 1:m |X 1:m ),<label>(16)</label></formula><p>where</p><formula xml:id="formula_21">X i1:i2 = (x i1 , x i1+1 , • • • , x i2 ) and Y i1:i2 = (y i1 , y i1+1 , • • • , y i2 ) for all i 1 ≤ i 2 . 2. (Exchangability) For any permutation π on X 1:n (a permutation π on set S is a bijection π : S → S), p(π • Y |π • X) = p(Y |X), (17) where X 1:n = {x 1 , • • • , x n }, π • X = (π(x 1 ), π(x 2 ), • • • , π(x n )), and π • Y = (y π(x1) , y π(x2) , • • • , y π(xn) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MTNP IS A STOCHASTIC PROCESS</head><p>In the case of MTNP, we observe input and output sequences X</p><formula xml:id="formula_22">= ((x 1 , t 1 ), (x 2 , t 2 ), • • • , (x n , t n )) and Y = (y (x1,t1) , y (x2,t2) , • • • , y (xn,tn) ) of a function h : X × T → t∈T Y t .</formula><p>Note that in the main text, we abbreviate x t i = (x i , t) and y t i = y (xi,t) for visibility. Now we want to show the existence of a stochastic process H : X × T × Ω → t∈T Y t , where the data D = (X, Y ) is generated. This can be done by showing the following conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">(Consistency) For any</head><formula xml:id="formula_23">m such that 1 ≤ m &lt; n, p(Y |X, C)dY m+1:n = p(Y 1:m |X 1:m , C),<label>(18)</label></formula><p>where</p><formula xml:id="formula_24">X i1:i2 = ((x i1 , t i1 ), • • • , (x i2 , t i2 )) and Y i1:i2 = (y (xi 1 ,ti 1 ) , • • • , y (xi 2 ,ti 2 ) ).</formula><p>2. (Exchangability) For any permutation π on X 1:n × T ,</p><formula xml:id="formula_25">p(π • Y |π • X, C) = p(Y |X, C),<label>(19)</label></formula><p>where</p><formula xml:id="formula_26">X 1:n = {x 1 , • • • , x n }, π • X = (π((x 1 , t 1 )), • • • , π((x n , t n ))), and π • Y = (y π((x1,t1)) , • • • , y π((xn,tn)) ).</formula><p>Here</p><formula xml:id="formula_27">p(Y |X, C) = ρ X (Y |C) is the conditional distribution of Y given any context C. Note that C</formula><p>is conditioned since we are modeling functional posterior of h, rather than prior.</p><p>Now we provide the proof of Proposition 1, which states that the following generative model defines a stochastic process.</p><formula xml:id="formula_28">z ∼ p(z|C), v t ∼ p(v t |z, t, C), y (xi,t) ∼ p(y (xi,t) |x i , t, v t ) ∀t ∈ T , ∀x i ∈ X 1:n ,<label>(20)</label></formula><p>To show the conditions of Kolmogorov Extension Theorem, we need two assumptions on the data generating process (Eq. 20). First, we assume the distribution defined by the data generating process is finite so that the order of integral can be swapped. Also, we assume that the conditional distribution p(y (xi,t) |x i , t, v t ) can implicitly select the per-task latent variable v t among v 1:T using the given task index t, i.e., there exists a distribution p such that p(y</p><formula xml:id="formula_29">(xi,t) |x i , t, v 1:T ) = p(y (xi,t) |x i , t, v t ).</formula><p>This means no more than that the latent variables v 1:T are indeed task-specific, such that each v t corresponds to task f t . Note that our neural-network model of MTNP (Figure <ref type="figure" target="#fig_1">2</ref>) indeed satisfies the second assumption, since the decoder selects the corresponding per-task latent variable v t given the task index t ∈ T .</p><p>Proof. We first show the consistency condition. From the data generating process (Eq. 20),</p><formula xml:id="formula_30">p(Y |X, C)dY m+1:n (21) = n i=1 p(y (xi,ti) |x i , t i , v ti ) T t=1 p(v t |z, t, C) p(z|C)dv 1:T dzdY m+1:n (22) = m i=1 p(y (xi,ti) |x i , t i , v ti ) n i=m+1 p(y (xi,ti) |x i , t i , v ti )dY m+1:n T t=1 p(v t |z, t, C) p(z|C)dv 1:T dz (23) = m i=1 p(y (xi,ti) |x i , t i , v ti ) T t=1 p(v t |z, t, C) p(z|C)dv 1:T dz (24) = p(Y 1:m |X 1:m , C).<label>(25)</label></formula><p>Next, we show the exchangability condition. Let π 1 , π 2 be the values of first and second coordinate of π, such that π((</p><formula xml:id="formula_31">x i , t i )) = (π 1 ((x i , t i )), π 2 ((x i , t i ))). Then p(π • Y |π • X, C) (26) = n i=1 p(y π((xi,ti)) |π((x i , t i )), v 1:T ) T t=1 p(v t |z, t, C) p(z|C)dv 1:T dz (27) = n i=1 p(y π((xi,ti)) |π((x i , t i )), v π2((xi,ti)) ) T t=1 p(v t |z, t, C) p(z|C)dv 1:T dz (28) = n i=1 p(y (xi,ti) |x i , t i , v ti ) T t=1 p(v t |z, t, C) p(z|C)dv 1:T dz (29) = p(Y |X, C). (<label>30</label></formula><formula xml:id="formula_32">)</formula><p>Here we used the assumption about p(y</p><formula xml:id="formula_33">(xi,t) |x i , t, v t ) such that p(y π((xi,ti)) |π((x i , t i )), v 1:T ) = p(y π((xi,ti)) |π((x i , t i )), v π2((xi,ti))</formula><p>). Since 1 ≤ m &lt; n and π are arbitrarily chosen, the data generating process (Eq. 20) satisfies the conditions of the Kolmogorov Extension Theorem. Thus there exists a stochastic process H : X × T × Ω → t∈T Y t , whose realizations are functions of the form h :</p><formula xml:id="formula_34">X × T → t∈T Y t .</formula><p>Note that the latent variable model of MTNP (Eq. 5) is a special case of the data generating process (Eq. 20), where p(v t |z, t, C) = p(v t |z, C t ). Thus MTNP is a stochastic process over the functions of form h : X × T → t∈T Y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ELBO DERIVATION FOR MTNP</head><p>We derive the evidence lower bound (ELBO) for log p θ (Y</p><formula xml:id="formula_35">1:T D |X 1:T D , C). Recall that C = (X 1:T C , Y 1:T C ) = t∈T C t where C t = (X t C , Y t C ) = {(x t i , y t i )} i∈I(C t ) and D = (X 1:T D , Y 1:T D ) = t∈T D t where D t = (X t D , Y t D ) = {(x t i , y t i )} i∈I(D t ) .</formula><p>For simplicity, we assume C t ⊂ D t for all t so that C ⊂ D as well. Also, to avoid confusion, for this derivation we denote the conditional prior networks as p θ (z|C t ) and p θ (v t |z, C t ) and then replace them with q φ (z|C t ) and q φ (v t |z, C t ) respectivelly, when we introduce parameter-sharing between prior and variational posterior networks.</p><p>First, the conditional log-likelihood has a lower bound</p><formula xml:id="formula_36">log p θ (Y 1:T D |X 1:T D , C) (31) = E q φ (z|D) log p θ (Y 1:T D , z|X 1:T D , C) p θ (z|X 1:T D , Y 1:T D , C) (32) = E q φ (z|D) log p θ (Y 1:T D |X 1:T D , C, z)p θ (z|X 1:T D , C) p θ (z|D) (33) = E q φ (z|D) log p θ (Y 1:T D |X 1:T D , C, z)p θ (z|C) p θ (z|D) (34) = E q φ (z|D) log p θ (Y 1:T D |X 1:T D , C, z) + D KL q φ (z|D) || p θ (z|D) -D KL q φ (z|D) || p θ (z|C)<label>(35)</label></formula><formula xml:id="formula_37">≥ E q φ (z|D) log p θ (Y 1:T D |X 1:T D , C, z) -D KL q φ (z|D) || p θ (z|C) ,<label>(36)</label></formula><p>where p θ (Y 1:T D |X 1:T D , C, z) in Eq. 36 can be further expanded by</p><formula xml:id="formula_38">log p θ (Y 1:T D |X 1:T D , C, z) (37) = E T t=1 q φ (v t |z,D t ) log p θ (Y 1:T D , v 1:T |X 1:T D , C, z) p θ (v 1:T |X 1:T D , Y 1:T D , C, z) (38) = E T t=1 q φ (v t |z,D t ) log p θ (Y 1:T D |X 1:T D , C, z, v 1:T )p θ (v 1:T |X 1:T D , C, z) p θ (v 1:T |z, D) (39) = E T t=1 q φ (v t |z,D t ) T t=1 log p θ (Y t D |X t D , v t )p θ (v t |z, C t ) p θ (v t |z, D t ) (40) = T t=1 E q φ log p θ (Y t D |X t D , v t )p θ (v t |z, C t ) p θ (v t |z, D t ) (41) = T t=1 E q φ log p θ (Y t D |X t D , v t ) + D KL q φ (v t |z, D t ) || p θ (v t |z, D t ) -D KL q φ (v t |z, D t ) || p θ (v t |z, C t ) (42) ≥ T t=1 E q φ log p θ (Y t D |X t D , v t ) -D KL q φ (v t |z, D t ) || p θ (v t |z, C t ) . (<label>43</label></formula><formula xml:id="formula_39">)</formula><p>On Eq. 34 and Eq. 40, we use the conditional independence relation follows from the latent variable model (Eq. 5) By combining Eq. 36 and Eq. 43, and also by sharing the parameters of conditional priors p θ (z|C) and p θ (v t |z, C t ) with variational posteriors q φ (z|C) q φ (v t |z, C t ), we get Published as a conference paper at ICLR 2022 the following lower bound.</p><formula xml:id="formula_40">log p θ (Y 1:T D |X 1:T D , C) (44) ≥ E q φ T t=1 E q φ log p θ (Y t D |X t D , v t ) -D KL q φ (v t |z, D t ) || p θ (v t |z, C t ) -D KL q φ (z|D) || p θ (z|C) (45) = E q φ T t=1 E q φ log p θ (Y t D |X t D , v t ) -D KL q φ (v t |z, D t ) || q φ (v t |z, C t ) -D KL q φ (z|D) || q φ (z|C) .<label>(46)</label></formula><p>A.4 ELBO FOR STNP AND JTNP STNP is no more than a collection of independent Neural Processes (NPs), where each NP corresponds to each task. Using T encoders and decoders {(p θt , q φt )} T t=1 , the objective for STNP can be derived by summing up the NP objectives (Eq. 2). We omit the ELBO derivation for NP. Note that the parameter sharing is used for the conditional prior network p θt (v t |C t ) and the variational posterior network q φt (v t |D t ).</p><formula xml:id="formula_41">log p θ (Y 1:T D |X D , C)<label>(47)</label></formula><formula xml:id="formula_42">≥ T t=1 E q φ t (v t |D t ) log p θt (Y t D |X D , v t ) -D KL q φt (v t |D t ) || p θt (v t |C t ) (48) = T t=1 E q φ t (v t |D t ) log p θt (Y t D |X D , v t ) -D KL q φt (v t |D t ) || q φt (v t |C t ) . (<label>49</label></formula><formula xml:id="formula_43">)</formula><p>On the other hand, JTNP is a single NP that models all tasks jointly, by concatenating the output variables into a single vector. Using an encoder q φ and a decoder p θ , the objective for JTNP is the same as the NP objective. Again, the encoder q φ serves as both conditional prior and variational posterior.</p><formula xml:id="formula_44">log p θ (Y 1:T D |X D , C) (50) ≥ E q φ (z|D) log p θ (Y 1:T D |X D , z) -D KL q φ (z|D) || p θ (z|C) (51) = E q φ (z|D) log p θ (Y 1:T D |X D , z) -D KL q φ (z|D) || q φ (z|C) .<label>(52)</label></formula><p>Note that STNP and JTNP model functions with input space X , so there is no superscript t in the input variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ARCHITECTURAL AND TRAINING DETAILS</head><p>In this section, we provide architectural and training details about models used in the experiments (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ATTENTIVE NEURAL PROCESS</head><p>As a strong NP baseline, we adopt Attentive Neural Processes (ANPs) <ref type="bibr" target="#b18">(Kim et al., 2019)</ref> architecture for STNP and JTNP. The encoder of ANP consists of a latent path and a deterministic path, each computes a latent variable z and a deterministic representation r i specific to each target example x i .</p><p>Then the decoder produces a distribution for the target output y i , which is assumed to be Normal. </p><formula xml:id="formula_45">= {(x i , y 1:T i )} i∈I(C) where y 1:T i = (y 1 i , • • • , y T i ).</formula><p>Latent Encoder The latent encoder samples a global latent z. For each context example (x i , y 1:T i ) ∈ C, we first project it to a hidden representation s 1:T i = ψ s (x i , y 1:T i ) using a single MLP ψ s . Then we aggregate them to a global representation s via self-attention followed by a pooling operation.</p><formula xml:id="formula_46">s = pool(Attn({s 1:T i } i∈I(C) , {s 1:T i } i∈I(C) , {s 1:T i } i∈I(C) ). (<label>53</label></formula><formula xml:id="formula_47">)</formula><p>Then the global latent is sampled via two MLPs.</p><formula xml:id="formula_48">z ∼ q φ (z|C) = N (ψ (z,1) (s), ψ (z,2) (s)). (<label>54</label></formula><formula xml:id="formula_49">)</formula><p>Deterministic Encoder The deterministic encoder produces local representation r i for each i ∈ D.</p><p>We first project each context example (x i , y 1:T i ) ∈ C t to a hidden representation d 1:T i = ψ d (x i , y 1:T i ) that serves as value embedding in cross-attention. Then by using the context and target input x i as key and query embeddings, we apply a cross-attention along the example axis (per-task).</p><formula xml:id="formula_50">{r 1:T i } i∈I(D) = Attn({x i } i∈I(D) , {x i } i∈I(C) , {d 1:T i } i∈I(D) ). (<label>55</label></formula><formula xml:id="formula_51">)</formula><p>Decoders Finally, the decoder produces predictive distribution for each joint target output y 1:T i . We first project the input to w i = ψ w (x i ), then concatenate it with the global latent variable z and deterministic representation r 1:T i . To compute the output distribution, we first apply two MLPs on the triple (w i , z, r 1:T i ).</p><formula xml:id="formula_52">µ i = ψ (y,1) (w i , z, r 1:T i )<label>(56)</label></formula><formula xml:id="formula_53">σ 2 i = ψ (y,2) (w i , z, r 1:T i )).<label>(57)</label></formula><p>Then for each dimension, we construct the predictive distributions as Normal or Categorical, depending on the corresponding task type.</p><formula xml:id="formula_54">y t i ∼ N ((µ i ) Y t , (σ 2 i ) Y t ), if y t i is continuous, Categorical((µ i ) Y t ), if y t i is discrete,<label>(58)</label></formula><p>where</p><formula xml:id="formula_55">(µ i ) Y t (or (σ 2 i ) Y t )</formula><p>) denotes the projection of µ i (or σ 2 i ) into the task-specific output space Y t , by indexing the corresponding dimension from µ i . For example, if all tasks are one-dimensional, then this corresponds to selecting t-th coordinate of µ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ANP MODEL FOR STNP</head><p>This section presents a detailed description of the STNP architecture used in the experiments. The STNP consists of T independent ANPs, which consists of T latent encoders, T deterministic encoders, and T decoders. Then STNP produces target output distribution by conditioning on the context set C = t∈T C t where C t = {(x i , y t i )} t∈T . In the following, we deonte a stacked multi-head attention block <ref type="bibr" target="#b27">(Parmar et al., 2018)</ref> by Attn(Q, K, V ) and a MLP by ψ(x), as in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Encoders</head><p>The latent encoders sample per-task latents v 1:T = (v 1 , • • • , v T ). For each context example (x i , y t i ) ∈ C t , we first project it to a hidden representation s t i = ψ t s (x i , y t i ) using a MLP ψ t s specific to task f t . Then we aggregate them to a task-specific representation s t via self-attention followed by a pooling operation.</p><formula xml:id="formula_56">s t = pool(Attn({s t i } i∈I(C t ) , {s t i } i∈I(C t ) , {s t i } i∈I(C t ) )), ∀t ∈ T . (<label>59</label></formula><formula xml:id="formula_57">)</formula><p>Note that each attention is applied along the example axis (per-task) and independent to each task.</p><p>Then the per-task latent variables are sampled independently, via MLPs.</p><formula xml:id="formula_58">v t ∼ q φ (v t |C t ) = N (ψ (v t ,1) (s t ), ψ (v t ,2) (s t )), ∀t ∈ T . (<label>60</label></formula><formula xml:id="formula_59">)</formula><p>Deterministic Encoders The deterministic encoders produce local representation r t i for each i ∈ D and t ∈ T . We first project each context example (x i , y t i ) ∈ C t to a hidden representation d t i = ψ t d (x i , y t i ) that serves as value embedding in cross-attention. Then by using the context and target input x i as key and query embeddings, we apply T independent cross-attention along the example axis (per-task).</p><formula xml:id="formula_60">{r t i } i∈I(D t ) = Attn({x t i } i∈I(D t ) , {x t i } i∈I(C t ) , {d t i } i∈I(D t ) ), ∀t ∈ T .<label>(61)</label></formula><p>Decoders Finally, the decoders produce predictive distributions for each target output y t i . We first project the input to w t i = ψ t w (x i ), then concatenate it with the corresponding latent variable v t and deterministic representation r t i . The output distribution is computed similar to MTNP described in Section 3.3.</p><formula xml:id="formula_61">y t i ∼ N (ψ (y t ,1) (w t i , v t , r t i ), ψ (y t ,2) (w t i , v t , r t i )), if y t i is continuous, Categorical(ψ (y t ,1) (w t i , v t , r t i )), if y t i is discrete, ∀i ∈ I(D t ), ∀t ∈ T . (<label>62</label></formula><formula xml:id="formula_62">)</formula><p>We use the hidden dimension d = 128 for all models in synthetic and CelebA experiments and d = 64 and in weather experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ARCHITECTURAL HYPER-PARAMETERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 TRAINING DETAILS</head><p>For all three models, we schedule learning rate lr by lr = base_lr × 1000 0.5 × min(n_iters × 1, 000 -1.5 , n_iters -0.5 ),</p><p>where n_iters is the number of total iterations and base_lr is the base learning rate. We also introduce beta coefficient on the ELBO objective following <ref type="bibr" target="#b14">Higgins et al. (2017)</ref>, which is multiplied by each KL term. The beta coefficient is scheduled to be linearly increased from 0 to 1 during the first 10000 iters, then fixed to 1. We summarize the training hyper-parameters of models used in the experiments in Table <ref type="table" target="#tab_10">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 PARAMETER SHARING IN MTNP</head><p>The overall description of our neural network model for MTNP is provided in Section 3.3. We use different parameter sharing techniques in the datasets, depending on whether the tasks are homogeneous or not. In synthetic and weather tasks, all output values are one-dimensional. Thus we tie the parameters of the per-task paths in encoder and decoder, which makes more efficient parametrization compared to per-task encoders and decoders. In visual tasks, however, the tasks have different output dimensionalities. Thus in this case, we separate the parameters of all per-task paths.</p><p>As task identity is implicitly encoded by the separation of task-specific paths, we do not employ task embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 OTHER BASELINES</head><p>We include two Multi-Output Gaussian Process (MOGP) baselines, MOSM <ref type="bibr" target="#b28">(Parra &amp; Tobar, 2017)</ref> and CSM <ref type="bibr" target="#b34">(Ulrich et al., 2015)</ref>. To make use of training set of tasks, we consider pretraining MOGPs with respect to the kernel parameters using the same meta-training dataset with MTNP, and transfer the learned kernel parameters as prior in meta-testing. This allows both MOGPs and MTNPs to be trained and evaluated under the same setting. To prevent overfitting, we early-stopped the pretraining based on NLL. We observe that such pretraining is effective in synthetic tasks but not in weather tasks, thus we report the pretrained version for results on synthetic tasks and non-pretrained version for results on weather tasks.</p><p>We also include two gradient-based meta-learning baselines, MAML <ref type="bibr" target="#b4">(Finn et al., 2017)</ref> and Reptile <ref type="bibr" target="#b24">(Nichol et al., 2018)</ref> that use the same meta-train/meta-test data with our method. We chose these models as they are model-agnostic meta-learning methods that can be applied to our multi-task regression setting with incomplete data. Applied to our problem, the meta-training involves bi-level optimization where the inner loop optimizes the loss for context data and the outer loop optimizes the loss for target data. We employ a similar architecture to MTNP for the baselines that consists of a 4-layer MLP encoder network shared by all tasks and task-specific 4-layer MLP decoder networks.</p><p>For fair comparisons, we controlled the total number of parameters of the models similar to NP baselines (STNP, JTNP, MTNP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS OF 1D SYNTHETIC FUNCTION REGRESSION</head><p>In this section, we describe details of the data generating process and experimental settings of 1D function regression on synthetic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 SYNTHETIC DATASET</head><p>As discussed in the paper, we simulate synthetic tasks which are correlated by a set of parameters a, b, c, w ∈ R as follow:</p><formula xml:id="formula_64">f t (x i ) = a • act t (wx i + b) + c, act t ∈ {Sine, Tanh, Sigmoid, Gaussian},<label>(64)</label></formula><p>where Sine, Tanh, Sigmoid are sine, hyperbolic tangent, logistic sigmoid function, respectively, and Gaussian(x) is defined as exp(-x<ref type="foot" target="#foot_1">foot_1</ref> ). Rather than sharing the exactly same parameters a, b, c, w across tasks, we add a task-specific noise to each parameter, to control the amount of correlation across tasks as follow:</p><formula xml:id="formula_65">α t = α + , ∼ N (0, 0.1), ∀t ∈ T , ∀α ∈ {a, b, c, w}.<label>(65)</label></formula><p>Thus in fact the input-output pairs of each task is generated as follow:</p><formula xml:id="formula_66">f t (x i ) = a t • act t (w t x i + b t ) + c t , act t ∈ {Sine, Tanh, Sigmoid, Gaussian},<label>(66)</label></formula><p>We split the 1,000 functions into 800 training, 100 validation, and 100 test sets of four correlated tasks.</p><p>Then we construct a training dataset, a validataion dataset, and a test dataset using the corresponding set of generated tasks. For each training and validation data, we sample 200 input points uniformly within the interval [-5, 5], and applied the corresponding tasks to generate multi-task output values.</p><p>For each test data, we choose 1000 input points in the uniform grid of the interval [-5, 5], and generate the multi-task output values similarly. Finally, simulating the incomplete data is achieved by randomly dropping each output value y t i with probability γ ∈ [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EVALUATION PROTOCOL</head><p>For evaluation, we average the normalized MSE M SE = 1 n n i=1 (y t i -ŷt i ) 2 /a 2 on test dataset. 2 For prediction Ŷ 1:4 , we approximate the predictive posterior mean with Monte Carlo sampling. For example in MTNP,</p><formula xml:id="formula_67">p(y t i |x i , C) = p(y t i |x i , v t )p(v t |z, C t )p(z|C)dv t dz (67) ≈ 1 N M N k=1 M l=1 p(y t i |x i , v t k,l ), where v t k,l i.i.d. ∼ p(v t |z k , C t ), z k i.i.d. ∼ p(z|C).<label>(68)</label></formula><p>We use N = M = 5, resulting total 25 samples. For STNP (or JTNP), we sample each latent v t (or z) 5 times, since there is no hierarchy. Since all the output distributions are Gaussian, the posterior predictive mean can be computed by averaging the means of each sample distribution p(y t i |x i , v t k,l ). To plot the predictions in Figure <ref type="figure" target="#fig_2">3</ref> (a), we use the posterior means for both z and v t (which corresponds to the Maximum A Posteriori estimation) and plot the mean and variance of resulting p(y t i |x i , v t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS ON SYNTHETIC 1D FUNCTION REGRESSION D.1 ADDITIONAL RESULTS ON COMPLETE AND PARTIALLY INCOMPLETE DATA</head><p>In this section, we provide additional results on the synthetic experiment, with various missing rates γ and also with standard deviation from 5 different random seeds. When the data is incomplete and missing some task labels (i.e., γ = 0.25, 0.5, 0.75), we can see that MTNP clearly outperforms the baselines in almost all cases. When the complete data (γ = 0) is given, MTNP still outperforms almost all baselines while achieves at least competitive performance to JTNP. Figure <ref type="figure">6</ref> and 7 shows that MTNP is the most robust against both context size and quality (incompleteness).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 ADDITIONAL RESULTS ON TOTALLY INCOMPLETE DATA</head><p>In this section, we explore the totally incomplete setting where no two task output values are observed at the same input point (I(C t ) ∩ I(C t ) = ∅, ∀t = t ), to further validate the practical effectiveness of our method. To generate the totally incomplete dataset, we randomly sample context input points for each task independently, then compute the corresponding output points according to the Eq. 65 and Eq. 66. Note that in this case we do not drop any output points, so that the context size is</p><formula xml:id="formula_68">m = |I(C)| = t∈T |I(C t )|.</formula><p>We evaluate the baselines and ours in two different scenarios: (1) training on partially incomplete or complete dataset then testing on totally incomplete dataset and ( <ref type="formula">2</ref>) both training and testing on totally incomplete dataset. We observe that in both scenarios, MTNP outperforms the baselines. .0482 ± 0.0014 0.0270 ± 0.0024 0.0088 ± 0.0019 0.0745 ± 0.0009 0.0589 ± 0.0024 0.0449 ± 0.0031 S+JTNP 0.0412 ± 0.0025 0.0235 ± 0.0055 0.0089 ± 0.0033 0.1136 ± 0.0099 0.0669 ± 0.0044 0.0367 ± 0.0035 MTNP 0.0302 ± 0.0022 0.0092 ± 0.0010 0.0028 ± 0.0004 0.0668 ± 0.0041 0.0367 ± 0.0018 0.0191 ± 0.0003</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL RESULTS ON WEATHER TIME-SERIES REGRESSION</head><p>In this section, we provide additional results on the time-series regression experiment on weather data, with various missing rates γ and also with standard deviation from 5 different random seeds. In this section, we provide additional results on the synthetic experiment, with various missing rates γ and also with standard deviation from 5 different random seeds. When the data is highly incomplete (i.e., γ = 0.5, 0.75), we can see that MTNP clearly outperforms the baselines in general. When the complete or less incomplete data (γ = 0, 0.25) is given, MTNP still outperforms the gradient-based meta-learning baselines and MOSM while achieves at least competitive performance to CSM, STNP and JTNP.     STNP 0.0068 ± 0.0001 0.0009 ± 0.0000 0.0005 ± 0.0000 0.0317 ± 0.0005 0.0260 ± 0.0003 0.0209 ± 0.0002 JTNP 0.0073 ± 0.0001 0.0017 ± 0.0000 0.0007 ± 0.0000 0.0116 ± 0.0002 0.0146 ± 0.0001 0.0174 ± 0.0001 MTNP 0.0052 ± 0.0000 0.0007 ± 0.0000 0.0004 ± 0.0000 0.0098 ± 0.0001 0.0120 ± 0.0001 0.0136 ± 0.0001  STNP 0.0079 ± 0.0001 0.0011 ± 0.0000 0.0005 ± 0.0000 0.0334 ± 0.0003 0.0267 ± 0.0001 0.0218 ± 0.0002 S+JTNP 0.0082 ± 0.0001 0.0018 ± 0.0000 0.0008 ± 0.0000 0.0109 ± 0.0001 0.0141 ± 0.0001 0.0181 ± 0.0001 MTNP 0.0061 ± 0.0000 0.0009 ± 0.0000 0.0005 ± 0.0000 0.0098 ± 0.0001 0.0117 ± 0.0001 0.0135 ± 0.0002  STNP 0.0102 ± 0.0002 0.0015 ± 0.0000 0.0006 ± 0.0000 0.0382 ± 0.0005 0.0271 ± 0.0003 0.0232 ± 0.0002 S+JTNP 0.0104 ± 0.0002 0.0022 ± 0.0000 0.0009 ± 0.0000 0.0104 ± 0.0002 0.0134 ± 0.0001 0.0192 ± 0.0002 MTNP 0.0082 ± 0.0001 0.0012 ± 0.0000 0.0006 ± 0.0000 0.0098 ± 0.0001 0.0112 ± 0.0001 0.0132 ± 0.0001   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ABLATION STUDY</head><p>In this section, we provide results of ablation study on the implementations of MTNP. We conduct experiments on synthetic and weather datasets to analyze the effect of various design choices we made for MTNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 ABLATION STUDY ON SELF-ATTENTION LAYERS AND PARAMETRIC POOLING</head><p>In this study, we explore the of using self-attention layers before pooling operations and using PMA layer for pooling in MTNP. The variants of MTNP are as follows.</p><p>(1) MTNP-A: MTNP without self-attention and using average pooling, (2) MTNP-P: MTNP without self-attention and using PMA, (3) MTNP-SA: MTNP with self-attention and using average pooling, (4) MTNP-SP: MTNP with self-attention and using PMA. The results are summarized below. Note that we consistently use MTNP-SP architecture for the experiments in Section 5. As shown in the result, MTNP with selfattention outperforms the one without self-attention by a large margin, implying that self-attention is critical in MTNP. MTNP-A 0.0158 ± 0.0028 0.0082 ± 0.0012 0.0050 ± 0.0002 0.0711 ± 0.0054 0.0454 ± 0.0042 0.0321 ± 0.0018 MTNP-P 0.0119 ± 0.0021 0.0052 ± 0.0005 0.0033 ± 0.0003 0.0519 ± 0.0029 0.0270 ± 0.0031 0.0161 ± 0.0008 MTNP-SA 0.0071 ± 0.0018 0.0015 ± 0.0003 0.0005 ± 0.0001 0.0345 ± 0.0048 0.0132 ± 0.0007 0.0065 ± 0.0010 MTNP-SP 0.0066 ± 0.0019 0.0014 ± 0.0001 0.0006 ± 0.0001 0.0360 ± 0.0018 0.0132 ± 0.0008 0.0069 ± 0.0012 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 EFFECT OF PARAMETER-SHARING IN PER-TASK BRANCHES</head><p>In this study, we explore the effect of parameter-sharing the per-task encoder networks and decoder networks in STNP and MTNP. The variants of STNP and MTNP are as follows. (1) STNP-S: STNP with shared encoder and decoder, (2) STNP-TS: STNP with task-specific encoders and decoders, (3) MTNP-S: MTNP with shared encoder and decoder in per-task branches, (4) MTNP-TS: MTNP with task-specific encoders and decoders in per-task branches. Note that we consistently use STNP-TS and MTNP-S architectures for the 1D experiments (synthetic and weather) and STNP-TS and MTNP-TS architectures for the 2D experiments (CelebA) in Section 5</p><p>As can be seen in the tables, in the weather dataset, we observe that the models with parameter sharing (STNP-S &amp; MTNP-S) show comparable performances to their respective non-sharing baselines (STNP-TS &amp; MTNP-TS). On the other hand, the parameter sharing technique slightly improves the performances of the models in the 1-D synthetic case. We conjecture that the utilization of the same architecture and its parameter for all tasks acts as a good inductive bias to the models considering that all tasks share the same global parameter a,b,c,w. Nonetheless, we still find that MTNPs consistently outperform STNPs regardless of whether the parameters are shared or not, validating the effectiveness of MTNP to capture and exploit functional correlation for multi-task learning problems. STNP-S 0.0177 ± 0.0034 0.0050 ± 0.0009 0.0012 ± 0.0003 0.0801 ± 0.0124 0.0371 ± 0.0048 0.0181 ± 0.0028 STNP-TS 0.0203 ± 0.0034 0.0067 ± 0.0013 0.0025 ± 0.0005 0.0799 ± 0.0098 0.0409 ± 0.0041 0.0222 ± 0.0042 MTNP-S 0.0066 ± 0.0019 0.0014 ± 0.0001 0.0006 ± 0.0001 0.0360 ± 0.0018 0.0132 ± 0.0008 0.0069 ± 0.0012 MTNP-TS 0.0093 ± 0.0015 0.0031 ± 0.0010 0.0014 ± 0.0001 0.0426 ± 0.0025 0.0173 ± 0.0019 0.0100 ± 0.0006 Table <ref type="table" target="#tab_4">30</ref>: Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5. STNP-S 0.2675 ± 0.0104 0.9537 ± 0.1347 0.2629 ± 0.0043 0.7847 ± 0.0503 0.0084 ± 0.0007 -0.9877 ± 0.0312 STNP-TS 0.2607 ± 0.0082 1.1242 ± 0.2362 0.2631 ± 0.0044 0.8563 ± 0.0637 0.0086 ± 0.0008 -0.9815 ± 0.0283 MTNP-S 0.2276 ± 0.0028 0.6557 ± 0.0433 0.2215 ± 0.0043 0.6660 ± 0.0141 0.0073 ± 0.0003 -1.0331 ± 0.0147 MTNP-TS 0.2187 ± 0.0043 0.7213 ± 0.0953 0.2253 ± 0.0100 0.6663 ± 0.0283 0.0071 ± 0.0003 -1.0232 ± 0.0144</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 EFFECT OF LATENT AND DETERMINISTIC ENCODERS</head><p>In this study, we compare STNP, JTNP, and MTNP without using deterministic encoder, each corresponds to STNP-L, JTNP-L, and MTNP-L in the table. Note that these variants correspond to the direct NP implementations of STNP/JTNP/MTNP rather than ANP. The results are provided in the tables below. The overall trends are the same as the models with deterministic encoder, which demonstrates that the effectiveness of MTNP does not depend on a specific choice of architecture (vanilla NP or ANP).</p><p>Table <ref type="table" target="#tab_4">31</ref>: Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5. STNP-L 0.0213 ± 0.0025 0.0086 ± 0.0008 0.0045 ± 0.0006 0.0809 ± 0.0101 0.0405 ± 0.0063 0.0234 ± 0.0026 S+JTNP-L 0.0201 ± 0.0030 0.0106 ± 0.0008 0.0061 ± 0.0003 0.0687 ± 0.0056 0.0376 ± 0.0014 0.0227 ± 0.0019 MTNP-L 0.0096 ± 0.0022 0.0027 ± 0.0005 0.0012 ± 0.0002 0.0417 ± 0.0037 0.0169 ± 0.0010 0.0091 ± 0.0007 Table <ref type="table" target="#tab_4">32</ref>: Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5. STNP-L 0.3381 ± 0.0026 0.8792 ± 0.0008 0.3411 ± 0.0049 0.8798 ± 0.0066 0.0106 ± 0.0005 -0.8804 ± 0.0118 S+JTNP-L 0.2832 ± 0.0095 0.7347 ± 0.0179 0.2851 ± 0.0123 0.7692 ± 0.0111 0.0127 ± 0.0007 -0.8827 ± 0.0220 MTNP-L 0.2432 ± 0.0046 0.6267 ± 0.0220 0.2372 ± 0.0056 0.6659 ± 0.0163 0.0085 ± 0.0003 -1.0006 ± 0.0065</p><p>We also compare MTNP with deterministic encoder only, which corresponds to MTNP-D in the table below. To emphasize the benefits of generative modeling of MTNPs, we include MTNP evaluated on the best sample among 25 predictive samples, which corresponds to MTNP-best in the table. We observe that MTNP and MTNP-D are comparable in synthetic and weather datasets, which seems reasonable as we designed the deterministic encoder to mimic the latent encoder of MTNP (e.g., they employ both per-task and across-task inferences). However, we can see that MTNP-best clearly outperforms MTNP-D, which implies that MTNP can generate more accurate samples while MTNP-D cannot. We observe that MTNP with one-hot embedding is comparable to MTNP with learnable embedding. To further investigate the effect of learnable embedding, we visualize the learned task embedding by MTNP using the t-SNE algorithm. We include the visualization results in Figure <ref type="figure" target="#fig_0">18</ref> and 19. As shown in the figure, we find that the learned task embeddings are well-separated from each other and uniformly distributed on the embedding space. From the observations, we conjecture that well-separated task embeddings are sufficient task information for MTNP. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical models of three different stochastic processes for multiple functions. Gray and white circles represent observable and latent variables, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the neural network model for MTNP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Predictions from NP baselines and MTNP. Black line: true function. Black dots: context points. Black crosses: imputed points from STNP. Lighter colored lines: posterior predictive samples where different colors used for different tasks. Darker colored line: mean of the samples. (b) Relative performance of MTNP variants on synthetic tasks with different levels of inter-task correlation. Top: on totally correlated tasks. Middle: on partially correlated tasks. Bottom: on independent tasks. each task. Figure 3 (b) shows the result in m = 10 and γ = 0.5. When the tasks are correlated (first and second rows of the figure), we can see introducing global latent improves the overall performance, which is further improved by the hierarchical formulation. When the tasks are independent (third row of the figure), sharing all knowledge through a single global latent degrades the performance (MTNP-G). On the other hand, MTNP and MTNP-T do not suffer from such a negative transfer since each of the independent tasks can be addressed by per-task latent variables separately. The overall results demonstrate that incorporating both global and task-specific information is the most effective and robust against various levels of inter-task correlation.</figDesc><graphic coords="7,422.89,293.54,81.50,52.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of MTNP's internal knowledge transfer. By observing additional data from Cloud task (at red triangles) given upon a few context points (at blue dots), the predicted mean and variance of Precip task improve at the additionally observed region. transfer its knowledge from one task (Cloud) to another (Precip) given the incomplete observations. When the observation is sparse (Figure 4(a)), the model produces an inaccurate prediction with high uncertainty for unobserved input domains. However, when the additional observations are available for the other attribute (Figure 4(b),(c)), MTNP successfully transfers the knowledge to improve the prediction. It shows that MTNP can effectively learn to exploit the incomplete observation by transferring knowledge across tasks.</figDesc><graphic coords="8,317.77,186.73,174.76,69.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative results on 2D function regression. Performances of all models improve as the number of observable contexts (m) increases. However, under the limited number of observable contexts (e.g. m = 10), STNP and S+JTNP produce inaccurate outputs (e.g. mis-predicting hairs and poses as in the green box) or incoherent outputs (e.g. different head poses as in the red box). results demonstrate that MTNP successfully captured dependence among tasks considering that (1) RGB and Edge are composed of two correlated low-level signals (e.g. color intensity and its gradients)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 2 )</head><label>2</label><figDesc>while both Segment and PNCC contain high-level semantic information on facial landmarks. Note that discovering inter-task correlations is one of the actively studied topics in the machine learning literature, where the efforts often come at the cost of extra computations and resources due to hard-coded<ref type="bibr" target="#b39">(Zamir et al., 2018;</ref><ref type="bibr" target="#b32">Standley et al., 2020)</ref> or hand-crafted<ref type="bibr" target="#b26">(Pal &amp; Balasubramanian, 2019)</ref> algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :Figure 11 :</head><label>8911</label><figDesc>Figure 8: Qualitative results on synthetic task with γ = 0, m = 10. For latent variable models (STNP, JTNP, MTNP), we sample the latents 25 times and plot the mean prediction from each sample. For the other models, we plot the mean prediction.</figDesc><graphic coords="26,116.92,554.22,387.79,60.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Predicted mean and standard deviation of STNP, S+JTNP, and MTNP with MAP estimations.</figDesc><graphic coords="30,160.09,517.40,343.84,53.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Qualitative results on 2D function regression. (γ = 0)</figDesc><graphic coords="39,119.34,589.37,92.07,92.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Qualitative results on 2D function regression. (γ = 0.25)</figDesc><graphic coords="40,119.34,588.88,92.07,92.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Qualitative results on 2D function regression. (γ 0.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Qualitative results on 2D function regression. (γ = 0.75)</figDesc><graphic coords="42,118.67,588.88,92.07,92.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>± 0.0003 -1.1381 ± 0.0223 0.0066 ± 0.0006 -1.0527 ± 0.0241 0.0621 ± 0.0069 0.0240 ± 0.1210 STNP-TS 0.0046 ± 0.0004 -1.1514 ± 0.0181 0.0069 ± 0.0004 -1.0390 ± 0.0106 0.0632 ± 0.0072 0.1273 ± 0.1898 MTNP-S 0.0037 ± 0.0001 -1.1832 ± 0.0165 0.0054 ± 0.0001 -1.1049 ± 0.0154 0.0546 ± 0.0021 -0.1006 ± 0.0696 MTNP-TS 0.0036 ± 0.0002 -1.1818 ± 0.0076 0.0053 ± 0.0003 -1.0885 ± 0.0246 0.0519 ± 0.0013 -0.0662 ± 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>± 0.0199 0.3006 ± 0.0407 0.1254 ± 0.0159 0.1362 ± 0.0119 0.0585 ± 0.0109 0.0234 ± 0.0020 S+JTNP-L 0.4151 ± 0.0273 0.2560 ± 0.0184 0.1395 ± 0.0106 0.1055 ± 0.0171 0.0506 ± 0.0054 0.0224 ± 0.0023 MTNP-L 0.3003 ± 0.0147 0.1543 ± 0.0130 0.0755 ± 0.0080 0.0563 ± 0.0069 0.0162 ± 0.0020 0.0067 ± 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>± 0.0010 -1.0783 ± 0.0207 0.0102 ± 0.0008 -0.9202 ± 0.0205 0.0828 ± 0.0067 0.0341 ± 0.0401 S+JTNP-L 0.0085 ± 0.0016 -1.0632 ± 0.0252 0.0136 ± 0.0021 -0.8601 ± 0.0236 0.0966 ± 0.0108 0.0737 ± 0.0699 MTNP-L 0.0046 ± 0.0005 -1.1580 ± 0.0167 0.0072 ± 0.0005 -1.0203 ± 0.0171 0.0596 ± 0.0027 -0.1554 ± 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 Figure 19</head><label>1819</label><figDesc>Figure 18: t-SNE plot (with 2 components) of the learned task embeddings of MTNP in synthetic tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5.</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="7">0.2962 0.1582 0.0701 0.0991 0.0342 0.0131 0.0321</cell><cell>0.0119</cell><cell cols="4">0.0069 0.0696 0.0353 0.0174</cell></row><row><cell>Reptile</cell><cell cols="7">0.5164 0.2886 0.1414 0.1656 0.0557 0.0291 0.0619</cell><cell>0.0220</cell><cell cols="4">0.0181 0.1371 0.0679 0.0374</cell></row><row><cell>MOSM</cell><cell cols="7">0.7852 0.4410 0.0298 0.4912 0.1444 0.1618 0.0720</cell><cell>0.0127</cell><cell cols="4">0.0013 0.3329 0.0857 0.0190</cell></row><row><cell>CSM</cell><cell cols="7">0.8529 0.3587 0.1537 0.6884 0.3669 0.0726 0.2437</cell><cell>0.0730</cell><cell cols="4">0.0137 0.1525 0.0961 0.0407</cell></row><row><cell>STNP</cell><cell cols="7">0.5212 0.2609 0.0993 0.1307 0.0468 0.0159 0.0203</cell><cell>0.0067</cell><cell cols="4">0.0025 0.0799 0.0409 0.0222</cell></row><row><cell cols="8">S+JTNP 0.3848 0.2340 0.1114 0.1015 0.0418 0.0168 0.0163</cell><cell>0.0065</cell><cell cols="4">0.0032 0.0613 0.0318 0.0161</cell></row><row><cell>MTNP</cell><cell cols="7">0.2636 0.1137 0.0485 0.0435 0.0115 0.0040 0.0066</cell><cell cols="5">0.0014 0.0006 0.0360 0.0132 0.0069</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5.</figDesc><table><row><cell>task</cell><cell cols="2">TempMin</cell><cell cols="2">TempMax</cell><cell cols="2">Humidity</cell><cell cols="2">Precip</cell><cell cols="2">Cloud</cell><cell>Dew</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0067</cell><cell>-</cell><cell>0.0094</cell><cell>-</cell><cell>0.0705</cell><cell>-</cell><cell>0.3041</cell><cell>-</cell><cell>0.2987</cell><cell>-</cell><cell>0.0106</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0060</cell><cell>-</cell><cell>0.0078</cell><cell>-</cell><cell>0.0691</cell><cell>-</cell><cell>0.3160</cell><cell>-</cell><cell>0.3047</cell><cell>-</cell><cell>0.0096</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.0091 -0.0194 0.0124 -0.0259 0.0827</cell><cell>1.3831</cell><cell cols="6">0.3021 4.1009 0.3170 2.0663 0.0128 -0.0255</cell></row><row><cell>CSM</cell><cell cols="5">0.0069 -0.8839 0.0123 -0.8522 0.0906</cell><cell>0.6640</cell><cell cols="6">0.2895 3.1897 0.2983 1.2655 0.0118 -0.7243</cell></row><row><cell>STNP</cell><cell cols="5">0.0046 -1.1514 0.0069 -1.0390 0.0632</cell><cell>0.1273</cell><cell cols="6">0.2607 1.1242 0.2631 0.8563 0.0086 -0.9815</cell></row><row><cell cols="6">S+JTNP 0.0045 -1.1703 0.0068 -1.0681 0.0607</cell><cell>0.0169</cell><cell cols="6">0.2348 0.6792 0.2376 0.6812 0.0084 -0.9946</cell></row></table><note><p><p><p>summarizes quantitative results. More comprehensive results with different missing rates, context sizes, and standard deviations for the metrics are provided in Appendix E. MTNP outperforms all baselines in both accuracy and uncertainty estimation, which demonstrates that it generalizes well to real-world stochastic data. More interestingly, Figure</p>4</p>illustrates how MTNP Published as a conference paper at ICLR 2022 MTNP 0.0037 -1.1832 0.0054 -1.1049 0.0546 -0.1006 0.2276 0.6557 0.2215 0.6660 0.0073 -1.0331</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on 2D function regression (γ = 0.5). Upper rows show prediction performance and lower rows show prediction coherency, reported by MSE and (1-mIoU) for continuous and categorical data, respectively (lower-the-better).</figDesc><table><row><cell>Tasks</cell><cell></cell><cell>RGB</cell><cell></cell><cell></cell><cell>Edge</cell><cell></cell><cell></cell><cell>Segment</cell><cell></cell><cell></cell><cell>PNCC</cell><cell></cell></row><row><cell>m</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row></table><note><p><p>STNP</p>0.0440 0.0154 0.0054 0.0359 0.0256 0.0116 0.6637 0.4669 0.2958 0.0102 0.0015 0.00061</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Relative performance gain (%).</figDesc><table><row><cell cols="5">Source \ Target RGB Edge Segment PNCC</cell></row><row><cell>RGB</cell><cell>-</cell><cell>53.02</cell><cell>8.73</cell><cell>18.57</cell></row><row><cell>Edge</cell><cell>6.35</cell><cell>-</cell><cell>8.18</cell><cell>15.70</cell></row><row><cell>Segment</cell><cell cols="2">5.13 33.30</cell><cell>-</cell><cell>29.24</cell></row><row><cell>PNCC</cell><cell cols="2">5.58 31.88</cell><cell>15.88</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The general architecture of ANP follows<ref type="bibr" target="#b18">Kim et al. (2019)</ref>, while two major modifications have made as follows. First, we replace the average pooling operation in stochastic path by a Pooling by Multihead Attention (PMA) layer which is introduced in<ref type="bibr" target="#b21">Lee et al. (2019)</ref>. Next, since we have a categorical task (Segment) in 2D experiment, we modify the decoder for each Segment task to output logits for the Categorical distribution it models.B.2 ANP MODEL FOR JTNPThis section presents a detailed description of the JTNP architecture used in the experiments. The JTNP consists of a single ANP, which consists of a latent encoder, a deterministic encoder, and a decoder. Then JTNP produces target output distribution by conditioning on the context set C</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>summarizes the number of layers for each module in three models (STNP, JTNP, MTNP) used in the experiments. We use the same number of layers in all experiments.</figDesc><table><row><cell>Module</cell><cell cols="3">STNP JTNP MTNP</cell></row><row><cell>ψ s (or ψ t s ) ψ d (or ψ t d ) ψ w (or ψ t w ) ψ y (or ψ t y )</cell><cell>3 3 1 5</cell><cell>3 3 1 5</cell><cell>3 3 1 5</cell></row><row><cell>per-task Attn (in STNP, MTNP)</cell><cell>3</cell><cell>-</cell><cell>3</cell></row><row><cell>global Attn (in JTNP)</cell><cell>-</cell><cell>3</cell><cell>-</cell></row><row><cell>across-task Attn (in MTNP)</cell><cell>-</cell><cell>-</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="3">STNP JTNP MTNP</cell></row><row><cell>Synthetic</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Weather</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell>Face</cell><cell>128</cell><cell>128</cell><cell>128</cell></row></table><note><p><p><p><p>Number of layers of each module in STNP, JTNP, and MTNP used in the experiments.</p>Table</p>6</p>summarizes the hidden dimension dim hidden of all models used in each experiment. In our implementation, all layers (including the positional embedding) except the input and output layers have the dimension dim hidden .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hidden dimensions of the models used in the experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Training hyper-parameters used in the experiments.</figDesc><table><row><cell></cell><cell>n_iters</cell><cell cols="2">base_lr batch size</cell></row><row><cell cols="3">Synthetic 300000 0.00025</cell><cell>24</cell></row><row><cell>Weather</cell><cell cols="2">50000 0.00025</cell><cell>16</cell></row><row><cell>Face</cell><cell cols="2">300000 0.0005</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.</figDesc><table><row><cell>0.4 0.6 0.8 MSE</cell><cell></cell><cell>sine</cell><cell cols="2">MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell><cell>0.3 0.4 0.5 0.6 0.7 MSE</cell><cell></cell><cell>tanh</cell><cell cols="2">MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell><cell cols="2">0.10 0.15 0.20 0.25 MSE</cell><cell></cell><cell>sigmoid</cell><cell cols="2">MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell><cell>0.15 0.20 0.25 0.30 MSE</cell><cell>gaussian</cell><cell>MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1 0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05 0.10</cell></row><row><cell>0.0</cell><cell>5</cell><cell cols="2">10 context size</cell><cell>20</cell><cell>0.0</cell><cell>5</cell><cell cols="2">10 context size</cell><cell cols="2">20</cell><cell>0.00</cell><cell>5</cell><cell cols="2">10 context size</cell><cell cols="2">20</cell><cell>0.00</cell><cell>5</cell><cell>10 context size</cell><cell>20</cell></row><row><cell>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 MSE</cell><cell></cell><cell>sine Reptile MAML MOSM CSM STNP S+JTNP MTNP</cell><cell></cell><cell></cell><cell>0.1 0.2 0.3 0.4 0.5 0.6 0.7 MSE</cell><cell></cell><cell>tanh MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell><cell></cell><cell></cell><cell cols="2">0.05 0.10 0.15 0.20 MSE</cell><cell></cell><cell>sigmoid MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell><cell></cell><cell></cell><cell>0.05 0.10 0.15 0.20 0.25 MSE</cell><cell>gaussian MAML Reptile MOSM CSM STNP S+JTNP MTNP</cell></row><row><cell>0.0</cell><cell>0.00</cell><cell cols="2">0.25 missing rate 0.50</cell><cell>0.75</cell><cell>0.0</cell><cell>0.00</cell><cell cols="2">0.25 missing rate 0.50</cell><cell cols="2">0.75</cell><cell>0.00</cell><cell>0.00</cell><cell cols="2">0.25 missing rate 0.50</cell><cell cols="2">0.75</cell><cell>0.00</cell><cell>0.00</cell><cell>0.25 missing rate 0.50</cell><cell>0.75</cell></row><row><cell cols="17">Figure 7: Performance (normalized MSE) of models against various missing rates (γ). Context size</cell></row><row><cell cols="3">(m) is fixed to 10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Figure 6: Performance (normalized MSE) of models against various context sizes (m). Missing rate (γ) is fixed to 0.5.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.25.</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.2448 ± 0.0167 0.1047 ± 0.0069 0.0438 ± 0.0006 0.0746 ± 0.0095 0.0213 ± 0.0007 0.0109 ± 0.0009</cell></row><row><cell>Reptile</cell><cell cols="6">0.4222 ± 0.0304 0.2015 ± 0.0347 0.0737 ± 0.0030 0.1244 ± 0.0096 0.0329 ± 0.0012 0.0153 ± 0.0012</cell></row><row><cell>MOSM</cell><cell cols="6">0.6531 ± 0.0973 0.1514 ± 0.0562 0.0038 ± 0.0028 0.2490 ± 0.0505 0.1104 ± 0.0251 0.0701 ± 0.0766</cell></row><row><cell>CSM</cell><cell cols="6">0.5096 ± 0.0688 0.2387 ± 0.0524 0.0516 ± 0.0159 0.4401 ± 0.1387 0.1179 ± 0.0265 0.0163 ± 0.0033</cell></row><row><cell>STNP</cell><cell cols="6">0.3768 ± 0.0152 0.1547 ± 0.0145 0.0492 ± 0.0060 0.0711 ± 0.0077 0.0191 ± 0.0033 0.0070 ± 0.0013</cell></row><row><cell cols="7">S+JTNP 0.2906 ± 0.0241 0.1481 ± 0.0049 0.0738 ± 0.0066 0.0531 ± 0.0073 0.0169 ± 0.0008 0.0098 ± 0.0004</cell></row><row><cell>MTNP</cell><cell cols="6">0.1871 ± 0.0211 0.0705 ± 0.0027 0.0297 ± 0.0026 0.0300 ± 0.0029 0.0055 ± 0.0011 0.0023 ± 0.0002</cell></row><row><cell>task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.0255 ± 0.0033 0.0116 ± 0.0012 0.0087 ± 0.0011 0.0563 ± 0.0018 0.0283 ± 0.0033 0.0134 ± 0.0007</cell></row><row><cell>Reptile</cell><cell cols="6">0.0498 ± 0.0076 0.0175 ± 0.0019 0.0116 ± 0.0004 0.1112 ± 0.0072 0.0502 ± 0.0044 0.0217 ± 0.0022</cell></row><row><cell>MOSM</cell><cell cols="6">0.0243 ± 0.0056 0.0044 ± 0.0016 0.0002 ± 0.0002 0.1566 ± 0.0310 0.0459 ± 0.0147 0.0059 ± 0.0036</cell></row><row><cell>CSM</cell><cell cols="6">0.1315 ± 0.0736 0.0154 ± 0.0011 0.0010 ± 0.0004 0.1213 ± 0.0228 0.0737 ± 0.0071 0.0180 ± 0.0063</cell></row><row><cell>STNP</cell><cell cols="6">0.0121 ± 0.0038 0.0036 ± 0.0004 0.0013 ± 0.0001 0.0532 ± 0.0072 0.0260 ± 0.0037 0.0150 ± 0.0021</cell></row><row><cell cols="7">S+JTNP 0.0097 ± 0.0017 0.0038 ± 0.0002 0.0022 ± 0.0001 0.0403 ± 0.0055 0.0181 ± 0.0005 0.0112 ± 0.0006</cell></row><row><cell>MTNP</cell><cell cols="6">0.0040 ± 0.0017 0.0008 ± 0.0001 0.0004 ± 0.0000 0.0234 ± 0.0025 0.0087 ± 0.0012 0.0048 ± 0.0003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5. ± 0.0034 0.0067 ± 0.0013 0.0025 ± 0.0005 0.0799 ± 0.0098 0.0409 ± 0.0041 0.0222 ± 0.0042 S+JTNP 0.0163 ± 0.0024 0.0065 ± 0.0015 0.0032 ± 0.0004 0.0613 ± 0.0045 0.0318 ± 0.0021 0.0161 ± 0.0019 MTNP 0.0066 ± 0.0019 0.0014 ± 0.0001 0.0006 ± 0.0001 0.0360 ± 0.0018 0.0132 ± 0.0008 0.0069 ± 0.0012</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.2962 ± 0.0140 0.1582 ± 0.0052 0.0701 ± 0.0055 0.0991 ± 0.0085 0.0342 ± 0.0032 0.0131 ± 0.0023</cell></row><row><cell>Reptile</cell><cell cols="6">0.5164 ± 0.0167 0.2886 ± 0.0254 0.1414 ± 0.0431 0.1656 ± 0.0142 0.0557 ± 0.0033 0.0291 ± 0.0191</cell></row><row><cell>MOSM</cell><cell cols="6">0.7852 ± 0.1127 0.4410 ± 0.1269 0.0298 ± 0.0172 0.4912 ± 0.0706 0.1444 ± 0.0386 0.1618 ± 0.1999</cell></row><row><cell>CSM</cell><cell cols="6">0.8529 ± 0.2216 0.3587 ± 0.0395 0.1537 ± 0.0310 0.6884 ± 0.0841 0.3669 ± 0.0799 0.0726 ± 0.0251</cell></row><row><cell>STNP</cell><cell cols="6">0.5212 ± 0.0157 0.2609 ± 0.0382 0.0993 ± 0.0182 0.1307 ± 0.0134 0.0468 ± 0.0074 0.0159 ± 0.0028</cell></row><row><cell cols="7">S+JTNP 0.3848 ± 0.0203 0.2340 ± 0.0169 0.1114 ± 0.0084 0.1015 ± 0.0160 0.0418 ± 0.0066 0.0168 ± 0.0026</cell></row><row><cell>MTNP</cell><cell cols="6">0.2636 ± 0.0105 0.1137 ± 0.0078 0.0485 ± 0.0034 0.0435 ± 0.0047 0.0115 ± 0.0021 0.0040 ± 0.0002</cell></row><row><cell>task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.0321 ± 0.0053 0.0119 ± 0.0014 0.0069 ± 0.0006 0.0696 ± 0.0033 0.0353 ± 0.0013 0.0174 ± 0.0024</cell></row><row><cell>Reptile</cell><cell cols="6">0.0619 ± 0.0089 0.0220 ± 0.0016 0.0181 ± 0.0148 0.1371 ± 0.0087 0.0679 ± 0.0039 0.0374 ± 0.0202</cell></row><row><cell>MOSM</cell><cell cols="6">0.0720 ± 0.0160 0.0127 ± 0.0049 0.0013 ± 0.0005 0.3329 ± 0.1578 0.0857 ± 0.0105 0.0190 ± 0.0064</cell></row><row><cell>CSM</cell><cell cols="6">0.2437 ± 0.0753 0.0730 ± 0.0413 0.0137 ± 0.0167 0.1525 ± 0.0402 0.0961 ± 0.0151 0.0407 ± 0.0079</cell></row><row><cell>STNP</cell><cell>0.0203</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.75.</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.3972 ± 0.0122 0.2501 ± 0.0133 0.1401 ± 0.0104 0.1544 ± 0.0255 0.0722 ± 0.0054 0.0258 ± 0.0047</cell></row><row><cell>Reptile</cell><cell cols="6">0.6289 ± 0.0200 0.4404 ± 0.0175 0.2672 ± 0.0283 0.2200 ± 0.0391 0.1172 ± 0.0167 0.0426 ± 0.0053</cell></row><row><cell>MOSM</cell><cell cols="6">0.9726 ± 0.0788 0.8189 ± 0.0861 0.4288 ± 0.1540 0.7753 ± 0.1060 0.3867 ± 0.0620 0.1464 ± 0.0373</cell></row><row><cell>CSM</cell><cell cols="6">0.8747 ± 0.1166 0.7057 ± 0.0750 0.4091 ± 0.0384 0.9140 ± 0.1262 0.6870 ± 0.0831 0.3036 ± 0.0649</cell></row><row><cell>STNP</cell><cell cols="6">0.7329 ± 0.0581 0.5053 ± 0.0289 0.2770 ± 0.0286 0.1975 ± 0.0256 0.1128 ± 0.0111 0.0443 ± 0.0116</cell></row><row><cell cols="7">S+JTNP 0.5807 ± 0.0573 0.4115 ± 0.0348 0.2521 ± 0.0218 0.1654 ± 0.0195 0.0989 ± 0.0127 0.0426 ± 0.0115</cell></row><row><cell>MTNP</cell><cell cols="6">0.3784 ± 0.0395 0.2432 ± 0.0230 0.1295 ± 0.0172 0.0838 ± 0.0085 0.0340 ± 0.0020 0.0118 ± 0.0027</cell></row><row><cell>task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.0520 ± 0.0045 0.0227 ± 0.0031 0.0088 ± 0.0022 0.1045 ± 0.0050 0.0617 ± 0.0036 0.0332 ± 0.0031</cell></row><row><cell>Reptile</cell><cell cols="6">0.0857 ± 0.0078 0.0436 ± 0.0126 0.0191 ± 0.0037 0.1777 ± 0.0154 0.1215 ± 0.0119 0.0695 ± 0.0045</cell></row><row><cell>MOSM</cell><cell cols="6">0.1704 ± 0.0362 0.0658 ± 0.0132 0.0131 ± 0.0037 0.2663 ± 0.0440 0.2461 ± 0.0512 0.1005 ± 0.0116</cell></row><row><cell>CSM</cell><cell cols="6">0.3252 ± 0.0741 0.2176 ± 0.0392 0.0832 ± 0.0384 0.2413 ± 0.0461 0.1624 ± 0.0074 0.1066 ± 0.0176</cell></row><row><cell>STNP</cell><cell cols="6">0.0303 ± 0.0038 0.0191 ± 0.0030 0.0067 ± 0.0008 0.1232 ± 0.0037 0.0800 ± 0.0094 0.0469 ± 0.0037</cell></row><row><cell cols="7">S+JTNP 0.0260 ± 0.0044 0.0165 ± 0.0021 0.0070 ± 0.0005 0.0970 ± 0.0101 0.0608 ± 0.0035 0.0334 ± 0.0023</cell></row><row><cell>MTNP</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>0136 ± 0.0024 0.0059 ± 0.0004 0.0019 ± 0.0004 0.0643 ± 0.0048 0.0323 ± 0.0016 0.0155 ± 0.0018</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Average normalized MSE on synthetic tasks in totally incomplete scenario (1), with varying context size (m). All models are trained on partially incomplete data with γ = 0.5 but JTNP is trained with γ = 0. ± 0.2299 0.4228 ± 0.2466 0.2116 ± 0.1539 0.1753 ± 0.0890 0.0843 ± 0.0572 0.0344 ± 0.0262 S+JTNP 0.5162 ± 0.2061 0.3472 ± 0.1789 0.1976 ± 0.1154 0.1411 ± 0.0751 0.0772 ± 0.0544 0.0327 ± 0.0222 MTNP 0.3777 ± 0.1947 0.2045 ± 0.1290 0.1007 ± 0.0688 0.0890 ± 0.0685 0.0293 ± 0.0248 0.0089 ± 0.0064</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.6123 ± 0.0221 0.3715 ± 0.0231 0.1930 ± 0.0175 0.4117 ± 0.0413 0.1230 ± 0.0143 0.0308 ± 0.0068</cell></row><row><cell>Reptile</cell><cell cols="6">0.8089 ± 0.0378 0.5702 ± 0.0500 0.3482 ± 0.0447 0.4495 ± 0.0340 0.1557 ± 0.0197 0.0487 ± 0.0116</cell></row><row><cell>MOSM</cell><cell cols="6">1.0718 ± 0.0854 0.6073 ± 0.0297 0.3728 ± 0.1461 1.5199 ± 0.2179 0.9601 ± 0.0750 0.5937 ± 0.0805</cell></row><row><cell>CSM</cell><cell cols="6">1.0887 ± 0.0703 0.6557 ± 0.0794 0.3462 ± 0.0759 1.4156 ± 0.0876 1.1087 ± 0.1469 0.8813 ± 0.1811</cell></row><row><cell cols="2">STNP 0.6491 task</cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.2220 ± 0.0288 0.0456 ± 0.0066 0.0112 ± 0.0027 0.2281 ± 0.0147 0.0911 ± 0.0096 0.0365 ± 0.0071</cell></row><row><cell>Reptile</cell><cell cols="6">0.3012 ± 0.0328 0.0630 ± 0.0108 0.0210 ± 0.0053 0.3734 ± 0.0255 0.1435 ± 0.0194 0.0725 ± 0.0141</cell></row><row><cell>MOSM</cell><cell cols="6">0.4798 ± 0.1020 0.2471 ± 0.0184 0.3070 ± 0.1893 0.3245 ± 0.1298 0.2190 ± 0.0163 0.2023 ± 0.0318</cell></row><row><cell>CSM</cell><cell cols="6">0.5256 ± 0.0344 0.6013 ± 0.1190 0.4581 ± 0.0706 0.2086 ± 0.0420 0.1815 ± 0.0391 0.1456 ± 0.0151</cell></row><row><cell>STNP</cell><cell cols="6">0.0271 ± 0.0133 0.0136 ± 0.0092 0.0050 ± 0.0034 0.0962 ± 0.0325 0.0608 ± 0.0297 0.0366 ± 0.0198</cell></row><row><cell cols="7">S+JTNP 0.0236 ± 0.0129 0.0122 ± 0.0078 0.0053 ± 0.0028 0.0819 ± 0.0353 0.0470 ± 0.0240 0.0263 ± 0.0137</cell></row><row><cell>MTNP</cell><cell cols="6">0.0152 ± 0.0133 0.0049 ± 0.0045 0.0014 ± 0.0010 0.0535 ± 0.0268 0.0246 ± 0.0149 0.0113 ± 0.0058</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Average normalized MSE on synthetic tasks in totally incomplete scenario (2), with varying context size (m). All models are trained on totally incomplete data but JTNP is trained with γ = 0.</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.5906 ± 0.0249 0.3967 ± 0.0265 0.2347 ± 0.0205 0.3366 ± 0.0322 0.1018 ± 0.0133 0.0317 ± 0.0058</cell></row><row><cell>Reptile</cell><cell cols="6">0.7461 ± 0.0473 0.5945 ± 0.0616 0.3798 ± 0.0504 0.3690 ± 0.0318 0.1202 ± 0.0164 0.0412 ± 0.0089</cell></row><row><cell>MOSM</cell><cell cols="6">0.9080 ± 0.3979 0.6939 ± 0.4823 0.6504 ± 0.5446 0.7423 ± 0.5953 0.6695 ± 0.5833 0.6132 ± 0.5515</cell></row><row><cell>CSM</cell><cell cols="6">0.9501 ± 0.1520 0.7644 ± 0.2126 0.6486 ± 0.2249 0.7902 ± 0.3731 0.7175 ± 0.3494 0.7691 ± 0.2691</cell></row><row><cell>STNP</cell><cell cols="6">0.7683 ± 0.0064 0.5112 ± 0.0322 0.3111 ± 0.0086 0.2649 ± 0.0089 0.1642 ± 0.0103 0.0679 ± 0.0065</cell></row><row><cell cols="7">S+JTNP 0.7591 ± 0.0407 0.5058 ± 0.0202 0.2848 ± 0.0258 0.2348 ± 0.0278 0.1486 ± 0.0164 0.0593 ± 0.0070</cell></row><row><cell>MTNP</cell><cell cols="6">0.4860 ± 0.0129 0.3391 ± 0.0281 0.1808 ± 0.0176 0.1596 ± 0.0191 0.0563 ± 0.0074 0.0188 ± 0.0021</cell></row><row><cell>task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MAML</cell><cell cols="6">0.1999 ± 0.0296 0.0396 ± 0.0056 0.0148 ± 0.0029 0.1995 ± 0.0175 0.0862 ± 0.0061 0.0451 ± 0.0060</cell></row><row><cell>Reptile</cell><cell cols="6">0.2431 ± 0.0281 0.0474 ± 0.0072 0.0177 ± 0.0039 0.3176 ± 0.0261 0.1239 ± 0.0147 0.0694 ± 0.0124</cell></row><row><cell>MOSM</cell><cell cols="6">0.6438 ± 0.6566 0.6054 ± 0.6225 0.6009 ± 0.6179 0.7974 ± 0.6765 0.7498 ± 0.6541 0.7201 ± 0.6529</cell></row><row><cell>CSM</cell><cell cols="6">0.6620 ± 0.4619 0.6454 ± 0.4349 0.7356 ± 0.3281 0.9119 ± 0.4539 0.8470 ± 0.4587 0.9381 ± 0.3498</cell></row><row><cell>STNP</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="2">MAML 0.0044 ± 0.0003</cell><cell>-</cell><cell>0.0059 ± 0.0004</cell><cell>-</cell><cell>0.0517 ± 0.0020</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0038 ± 0.0002</cell><cell>-</cell><cell>0.0053 ± 0.0003</cell><cell>-</cell><cell>0.0510 ± 0.0023</cell><cell>-</cell></row><row><cell cols="6">MOSM 0.0056 ± 0.0005 -1.0074 ± 0.1020 0.0106 ± 0.0021 -0.4788 ± 0.1607 0.0735 ± 0.0096</cell><cell>0.7877 ± 0.1929</cell></row><row><cell>CSM</cell><cell cols="5">0.0056 ± 0.0019 -1.4274 ± 0.0863 0.0064 ± 0.0009 -1.1762 ± 0.0755 0.0572 ± 0.0052</cell><cell>0.0482 ± 0.1332</cell></row><row><cell>STNP</cell><cell cols="6">0.0034 ± 0.0002 -1.2159 ± 0.0113 0.0051 ± 0.0002 -1.1340 ± 0.0098 0.0487 ± 0.0013 -0.0937 ± 0.0760</cell></row><row><cell>JTNP</cell><cell cols="6">0.0034 ± 0.0002 -1.2128 ± 0.0076 0.0050 ± 0.0001 -1.1310 ± 0.0088 0.0461 ± 0.0021 -0.2421 ± 0.0455</cell></row><row><cell>MTNP</cell><cell cols="6">0.0032 ± 0.0002 -1.2169 ± 0.0123 0.0048 ± 0.0002 -1.1389 ± 0.0067 0.0487 ± 0.0015 -0.1999 ± 0.0376</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="2">MAML 0.2576 ± 0.0068</cell><cell>-</cell><cell>0.2576 ± 0.0086</cell><cell>-</cell><cell>0.0089 ± 0.0005</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.2599 ± 0.0077</cell><cell>-</cell><cell>0.2618 ± 0.0109</cell><cell>-</cell><cell>0.0080 ± 0.0004</cell><cell>-</cell></row><row><cell cols="7">MOSM 0.2957 ± 0.0131 4.1621 ± 1.0855 0.2997 ± 0.0203 1.5618 ± 0.1822 0.0113 ± 0.0017 -0.3663 ± 0.1571</cell></row><row><cell>CSM</cell><cell cols="6">0.2708 ± 0.0130 2.6255 ± 0.7511 0.2664 ± 0.0151 0.9500 ± 0.0802 0.0098 ± 0.0022 -1.0495 ± 0.1330</cell></row><row><cell>STNP</cell><cell cols="6">0.2411 ± 0.0041 2.1338 ± 0.2856 0.2415 ± 0.0062 0.8888 ± 0.0356 0.0071 ± 0.0004 -1.0475 ± 0.0210</cell></row><row><cell>JTNP</cell><cell cols="6">0.2166 ± 0.0031 0.6014 ± 0.0200 0.2202 ± 0.0023 0.6446 ± 0.0469 0.0069 ± 0.0006 -1.0568 ± 0.0230</cell></row><row><cell>MTNP</cell><cell cols="6">0.2194 ± 0.0019 0.6642 ± 0.0331 0.2144 ± 0.0066 0.6451 ± 0.0240 0.0068 ± 0.0004 -1.0672 ± 0.0142</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.25. ± 0.0047 0.6164 ± 0.0352 0.2289 ± 0.0050 0.6632 ± 0.0427 0.0074 ± 0.0004 -1.0430 ± 0.0272 MTNP 0.2216 ± 0.0037 0.6729 ± 0.0574 0.2177 ± 0.0058 0.6461 ± 0.0195 0.0072 ± 0.0005 -1.0563 ± 0.0193</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0052 ± 0.0006</cell><cell>-</cell><cell>0.0070 ± 0.0005</cell><cell>-</cell><cell>0.0558 ± 0.0044</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0042 ± 0.0002</cell><cell>-</cell><cell>0.0059 ± 0.0003</cell><cell>-</cell><cell>0.0555 ± 0.0050</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.0056 ± 0.0009 -0.9333 ± 0.1774 0.0106 ± 0.0020 -0.1676 ± 0.2981 0.0719 ± 0.0073</cell><cell>0.8968 ± 0.2437</cell></row><row><cell>CSM</cell><cell cols="5">0.0057 ± 0.0010 -1.3140 ± 0.0921 0.0087 ± 0.0027 -0.9624 ± 0.0974 0.0589 ± 0.0032</cell><cell>0.0951 ± 0.1031</cell></row><row><cell>STNP</cell><cell cols="6">0.0036 ± 0.0001 -1.1995 ± 0.0055 0.0055 ± 0.0002 -1.1124 ± 0.0116 0.0516 ± 0.0031 -0.0911 ± 0.0907</cell></row><row><cell cols="7">S+JTNP 0.0038 ± 0.0002 -1.1976 ± 0.0054 0.0056 ± 0.0004 -1.1118 ± 0.0133 0.0491 ± 0.0017 -0.1976 ± 0.0593</cell></row><row><cell>MTNP</cell><cell cols="6">0.0033 ± 0.0002 -1.2103 ± 0.0094 0.0051 ± 0.0003 -1.1252 ± 0.0143 0.0504 ± 0.0021 -0.1766 ± 0.0523</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.2759 ± 0.0058</cell><cell>-</cell><cell>0.2713 ± 0.0148</cell><cell>-</cell><cell>0.0097 ± 0.0012</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.2801 ± 0.0092</cell><cell>-</cell><cell>0.2755 ± 0.0154</cell><cell>-</cell><cell>0.0087 ± 0.0010</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="6">0.3058 ± 0.0252 4.8069 ± 0.9710 0.3034 ± 0.0102 1.6875 ± 0.2039 0.0108 ± 0.0017 -0.2865 ± 0.0982</cell></row><row><cell>CSM</cell><cell cols="6">0.2737 ± 0.0171 3.3485 ± 0.8053 0.2842 ± 0.0239 1.0663 ± 0.0729 0.0105 ± 0.0016 -0.9097 ± 0.1864</cell></row><row><cell>STNP</cell><cell cols="6">0.2459 ± 0.0077 1.3797 ± 0.3219 0.2436 ± 0.0127 0.8129 ± 0.0471 0.0076 ± 0.0006 -1.0168 ± 0.0316</cell></row><row><cell cols="2">S+JTNP 0.2221</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 :</head><label>16</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5. ± 0.0082 1.1242 ± 0.2362 0.2631 ± 0.0044 0.8563 ± 0.0637 0.0086 ± 0.0008 -0.9815 ± 0.0283 S+JTNP 0.2348 ± 0.0038 0.6792 ± 0.0314 0.2376 ± 0.0041 0.6812 ± 0.0231 0.0084 ± 0.0007 -0.9946 ± 0.0161 MTNP 0.2276 ± 0.0028 0.6557 ± 0.0433 0.2215 ± 0.0043 0.6660 ± 0.0141 0.0073 ± 0.0003 -1.0331 ± 0.0147</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0067 ± 0.0009</cell><cell>-</cell><cell>0.0094 ± 0.0017</cell><cell>-</cell><cell>0.0705 ± 0.0083</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0060 ± 0.0007</cell><cell>-</cell><cell>0.0078 ± 0.0004</cell><cell>-</cell><cell>0.0691 ± 0.0092</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.0091 ± 0.0015 -0.0194 ± 0.6391 0.0124 ± 0.0022 -0.0259 ± 0.2832 0.0827 ± 0.0093</cell><cell>1.3831 ± 0.2993</cell></row><row><cell>CSM</cell><cell cols="5">0.0069 ± 0.0015 -0.8839 ± 0.2680 0.0123 ± 0.0053 -0.8522 ± 0.1301 0.0906 ± 0.0288</cell><cell>0.6640 ± 0.5221</cell></row><row><cell>STNP</cell><cell cols="5">0.0046 ± 0.0004 -1.1514 ± 0.0181 0.0069 ± 0.0004 -1.0390 ± 0.0106 0.0632 ± 0.0072</cell><cell>0.1273 ± 0.1898</cell></row><row><cell cols="6">S+JTNP 0.0045 ± 0.0006 -1.1703 ± 0.0144 0.0068 ± 0.0003 -1.0681 ± 0.0112 0.0607 ± 0.0053</cell><cell>0.0169 ± 0.1437</cell></row><row><cell>MTNP</cell><cell cols="6">0.0037 ± 0.0001 -1.1832 ± 0.0165 0.0054 ± 0.0001 -1.1049 ± 0.0154 0.0546 ± 0.0021 -0.1006 ± 0.0696</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.3041 ± 0.0049</cell><cell>-</cell><cell>0.2987 ± 0.0132</cell><cell>-</cell><cell>0.0106 ± 0.0010</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>± 0.0087</cell><cell>-</cell><cell>0.3047 ± 0.0152</cell><cell>-</cell><cell>0.0096 ± 0.0008</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="6">0.3021 ± 0.0161 4.1009 ± 0.5731 0.3170 ± 0.0152 2.0663 ± 0.3535 0.0128 ± 0.0023 -0.0255 ± 0.2011</cell></row><row><cell>CSM</cell><cell cols="6">0.2895 ± 0.0103 3.1897 ± 0.7841 0.2983 ± 0.0130 1.2655 ± 0.2538 0.0118 ± 0.0016 -0.7243 ± 0.2799</cell></row><row><cell>STNP</cell><cell>0.2607</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 :</head><label>17</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.75.</figDesc><table><row><cell>task</cell><cell cols="2">TempMin</cell><cell cols="2">TempMax</cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0094 ± 0.0018</cell><cell>-</cell><cell>0.0141 ± 0.0031</cell><cell>-</cell><cell>0.0830 ± 0.0083</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0083 ± 0.0014</cell><cell>-</cell><cell>0.0122 ± 0.0032</cell><cell>-</cell><cell>0.0851 ± 0.0057</cell><cell>-</cell></row><row><cell>MOSM</cell><cell>0.0137 ± 0.0028</cell><cell>0.4165 ± 0.7186</cell><cell>0.0212 ± 0.0055</cell><cell>0.5358 ± 0.3826</cell><cell cols="2">0.0890 ± 0.0126 1.3986 ± 0.2961</cell></row><row><cell>CSM</cell><cell cols="6">0.0106 ± 0.0011 -0.3791 ± 0.5254 0.0174 ± 0.0064 -0.0270 ± 0.2872 0.0908 ± 0.0185 0.9146 ± 0.1250</cell></row><row><cell>STNP</cell><cell cols="6">0.0072 ± 0.0013 -1.0291 ± 0.0498 0.0104 ± 0.0029 -0.8627 ± 0.1556 0.0776 ± 0.0094 0.3167 ± 0.1057</cell></row><row><cell cols="7">S+JTNP 0.0073 ± 0.0017 -1.0810 ± 0.0558 0.0103 ± 0.0023 -0.9165 ± 0.0893 0.0803 ± 0.0095 0.3243 ± 0.1008</cell></row><row><cell>MTNP</cell><cell cols="6">0.0047 ± 0.0004 -1.1272 ± 0.0166 0.0075 ± 0.0010 -1.0004 ± 0.0566 0.0644 ± 0.0040 0.0454 ± 0.0473</cell></row><row><cell>task</cell><cell cols="2">Precip</cell><cell cols="2">Cloud</cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.3903 ± 0.0621</cell><cell>-</cell><cell>0.3482 ± 0.0154</cell><cell>-</cell><cell>0.0147 ± 0.0026</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.4059 ± 0.0720</cell><cell>-</cell><cell>0.3662 ± 0.0189</cell><cell>-</cell><cell>0.0143 ± 0.0022</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.3297 ± 0.0333 5.5436 ± 0.5968 0.3256 ± 0.0216 2.0465 ± 0.5114 0.0182 ± 0.0019</cell><cell>0.4010 ± 0.2871</cell></row><row><cell>CSM</cell><cell cols="6">0.3095 ± 0.0376 5.1712 ± 0.7329 0.3144 ± 0.0171 1.2520 ± 0.1105 0.0145 ± 0.0039 -0.4852 ± 0.2178</cell></row><row><cell>STNP</cell><cell cols="6">0.2877 ± 0.0082 1.3884 ± 0.2007 0.2884 ± 0.0117 0.8719 ± 0.0649 0.0122 ± 0.0007 -0.8116 ± 0.0539</cell></row><row><cell cols="7">S+JTNP 0.2702 ± 0.0145 0.8052 ± 0.0799 0.2587 ± 0.0131 0.7621 ± 0.0380 0.0106 ± 0.0011 -0.9028 ± 0.0447</cell></row><row><cell>MTNP</cell><cell cols="6">0.2433 ± 0.0105 0.7368 ± 0.1119 0.2339 ± 0.0053 0.7009 ± 0.0316 0.0090 ± 0.0009 -0.9388 ± 0.0656</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18 :</head><label>18</label><figDesc>Average MSE and NLL on weather tasks, with m = 20 and γ = 0.</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="2">MAML 0.0040 ± 0.0002</cell><cell>-</cell><cell>0.0055 ± 0.0003</cell><cell>-</cell><cell>0.0449 ± 0.0017</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0033 ± 0.0001</cell><cell>-</cell><cell>0.0047 ± 0.0002</cell><cell>-</cell><cell>0.0438 ± 0.0014</cell><cell>-</cell></row><row><cell cols="6">MOSM 0.0035 ± 0.0002 -1.4556 ± 0.0625 0.0057 ± 0.0006 -1.0477 ± 0.0962 0.0494 ± 0.0013</cell><cell>0.3296 ± 0.0594</cell></row><row><cell>CSM</cell><cell cols="6">0.0033 ± 0.0002 -1.6792 ± 0.0131 0.0049 ± 0.0004 -1.4526 ± 0.0338 0.0458 ± 0.0032 -0.2356 ± 0.0377</cell></row><row><cell>STNP</cell><cell cols="6">0.0030 ± 0.0001 -1.2349 ± 0.0038 0.0045 ± 0.0000 -1.1615 ± 0.0022 0.0418 ± 0.0013 -0.2829 ± 0.0509</cell></row><row><cell>JTNP</cell><cell cols="6">0.0031 ± 0.0001 -1.2255 ± 0.0060 0.0045 ± 0.0001 -1.1564 ± 0.0060 0.0431 ± 0.0007 -0.3034 ± 0.0238</cell></row><row><cell>MTNP</cell><cell cols="6">0.0029 ± 0.0001 -1.2309 ± 0.0039 0.0042 ± 0.0001 -1.1636 ± 0.0047 0.0452 ± 0.0014 -0.2809 ± 0.0205</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="2">MAML 0.2316 ± 0.0043</cell><cell>-</cell><cell>0.2231 ± 0.0013</cell><cell>-</cell><cell>0.0080 ± 0.0001</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.2337 ± 0.0055</cell><cell>-</cell><cell>0.2252 ± 0.0021</cell><cell>-</cell><cell>0.0068 ± 0.0001</cell><cell>-</cell></row><row><cell cols="7">MOSM 0.2638 ± 0.0073 2.6499 ± 0.4487 0.2520 ± 0.0065 1.0786 ± 0.0817 0.0073 ± 0.0004 -0.9203 ± 0.0998</cell></row><row><cell>CSM</cell><cell cols="6">0.2482 ± 0.0057 1.2779 ± 0.3799 0.2434 ± 0.0161 0.7473 ± 0.0361 0.0072 ± 0.0010 -1.2782 ± 0.0584</cell></row><row><cell>STNP</cell><cell cols="6">0.2195 ± 0.0055 1.0330 ± 0.1193 0.2156 ± 0.0054 0.6765 ± 0.0339 0.0062 ± 0.0002 -1.0897 ± 0.0101</cell></row><row><cell>JTNP</cell><cell cols="6">0.2116 ± 0.0020 0.5441 ± 0.0298 0.2133 ± 0.0024 0.5956 ± 0.0106 0.0064 ± 0.0001 -1.0772 ± 0.0078</cell></row><row><cell>MTNP</cell><cell cols="6">0.2134 ± 0.0018 0.5728 ± 0.0286 0.2084 ± 0.0052 0.5992 ± 0.0079 0.0065 ± 0.0003 -1.0939 ± 0.0064</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19 :</head><label>19</label><figDesc>Average MSE and NLL on weather tasks, with m = 20 and γ = 0.25.</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0043 ± 0.0002</cell><cell>-</cell><cell>0.0057 ± 0.0003</cell><cell>-</cell><cell>0.0467 ± 0.0014</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0037 ± 0.0002</cell><cell>-</cell><cell>0.0049 ± 0.0002</cell><cell>-</cell><cell>0.0457 ± 0.0011</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.0036 ± 0.0003 -1.3658 ± 0.1010 0.0064 ± 0.0016 -0.9779 ± 0.1294 0.0509 ± 0.0014</cell><cell>0.4595 ± 0.1306</cell></row><row><cell>CSM</cell><cell cols="6">0.0044 ± 0.0017 -1.6228 ± 0.0354 0.0057 ± 0.0007 -1.3557 ± 0.0260 0.0471 ± 0.0020 -0.1404 ± 0.1129</cell></row><row><cell>STNP</cell><cell cols="6">0.0032 ± 0.0001 -1.2247 ± 0.0050 0.0047 ± 0.0001 -1.1530 ± 0.0090 0.0439 ± 0.0017 -0.2663 ± 0.0519</cell></row><row><cell cols="7">S+JTNP 0.0033 ± 0.0001 -1.2175 ± 0.0064 0.0047 ± 0.0001 -1.1488 ± 0.0046 0.0450 ± 0.0005 -0.2727 ± 0.0277</cell></row><row><cell>MTNP</cell><cell cols="6">0.0030 ± 0.0001 -1.2257 ± 0.0036 0.0043 ± 0.0002 -1.1591 ± 0.0057 0.0465 ± 0.0013 -0.2713 ± 0.0175</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.2379 ± 0.0079</cell><cell>-</cell><cell>0.2287 ± 0.0018</cell><cell>-</cell><cell>0.0084 ± 0.0003</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.2397 ± 0.0098</cell><cell>-</cell><cell>0.2306 ± 0.0019</cell><cell>-</cell><cell>0.0071 ± 0.0002</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="6">0.2673 ± 0.0110 2.6371 ± 0.6555 0.2691 ± 0.0091 1.3464 ± 0.0967 0.0075 ± 0.0008 -0.8713 ± 0.0673</cell></row><row><cell>CSM</cell><cell cols="6">0.2554 ± 0.0034 1.4961 ± 0.3189 0.2464 ± 0.0096 0.8126 ± 0.0324 0.0076 ± 0.0009 -1.2384 ± 0.0567</cell></row><row><cell>STNP</cell><cell cols="6">0.2208 ± 0.0032 0.6287 ± 0.0876 0.2193 ± 0.0036 0.6448 ± 0.0227 0.0064 ± 0.0002 -1.0780 ± 0.0129</cell></row><row><cell cols="7">S+JTNP 0.2128 ± 0.0028 0.5581 ± 0.0376 0.2167 ± 0.0032 0.6086 ± 0.0134 0.0067 ± 0.0001 -1.0695 ± 0.0095</cell></row><row><cell>MTNP</cell><cell cols="6">0.2147 ± 0.0028 0.5722 ± 0.0251 0.2098 ± 0.0047 0.5980 ± 0.0125 0.0067 ± 0.0003 -1.0874 ± 0.0053</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20 :</head><label>20</label><figDesc>Average MSE and NLL on weather tasks, with m = 20 and γ = 0.5. ± 0.0022 0.5761 ± 0.0341 0.2224 ± 0.0082 0.6393 ± 0.0206 0.0071 ± 0.0003 -1.0577 ± 0.0064 MTNP 0.2169 ± 0.0022 0.5811 ± 0.0283 0.2129 ± 0.0039 0.6175 ± 0.0212 0.0066 ± 0.0003 -1.0834 ± 0.0080</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0046 ± 0.0002</cell><cell>-</cell><cell>0.0058 ± 0.0003</cell><cell>-</cell><cell>0.0505 ± 0.0028</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0041 ± 0.0003</cell><cell>-</cell><cell>0.0053 ± 0.0001</cell><cell>-</cell><cell>0.0495 ± 0.0025</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.0048 ± 0.0009 -1.0919 ± 0.1379 0.0083 ± 0.0012 -0.4804 ± 0.1996 0.0600 ± 0.0035</cell><cell>0.7389 ± 0.2151</cell></row><row><cell>CSM</cell><cell cols="5">0.0050 ± 0.0010 -1.4484 ± 0.0377 0.0069 ± 0.0012 -1.1374 ± 0.1509 0.0584 ± 0.0087</cell><cell>0.0686 ± 0.1651</cell></row><row><cell>STNP</cell><cell cols="6">0.0034 ± 0.0002 -1.2122 ± 0.0098 0.0051 ± 0.0002 -1.1368 ± 0.0092 0.0477 ± 0.0020 -0.2018 ± 0.0608</cell></row><row><cell cols="7">S+JTNP 0.0037 ± 0.0001 -1.2060 ± 0.0081 0.0051 ± 0.0003 -1.1320 ± 0.0017 0.0480 ± 0.0019 -0.2149 ± 0.0316</cell></row><row><cell>MTNP</cell><cell cols="6">0.0031 ± 0.0002 -1.2210 ± 0.0054 0.0045 ± 0.0001 -1.1538 ± 0.0058 0.0475 ± 0.0012 -0.2483 ± 0.0232</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.2585 ± 0.0080</cell><cell>-</cell><cell>0.2450 ± 0.0073</cell><cell>-</cell><cell>0.0089 ± 0.0009</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.2609 ± 0.0072</cell><cell>-</cell><cell>0.2491 ± 0.0115</cell><cell>-</cell><cell>0.0077 ± 0.0004</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="6">0.2904 ± 0.0177 3.5379 ± 0.7060 0.2883 ± 0.0166 1.5341 ± 0.2511 0.0089 ± 0.0010 -0.5127 ± 0.1146</cell></row><row><cell>CSM</cell><cell cols="6">0.2699 ± 0.0058 2.1170 ± 0.4957 0.2689 ± 0.0152 0.9841 ± 0.1305 0.0101 ± 0.0014 -0.9491 ± 0.1008</cell></row><row><cell>STNP</cell><cell cols="6">0.2387 ± 0.0086 0.7083 ± 0.1090 0.2371 ± 0.0081 0.7121 ± 0.0782 0.0070 ± 0.0003 -1.0455 ± 0.0112</cell></row><row><cell cols="2">S+JTNP 0.2211</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 21 :</head><label>21</label><figDesc>Average MSE and NLL on weather tasks, with m = 20 and γ = 0.75.</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell cols="2">TempMax</cell><cell cols="2">Humidity</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.0060 ± 0.0009</cell><cell>-</cell><cell>0.0087 ± 0.0012</cell><cell>-</cell><cell>0.0623 ± 0.0051</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.0056 ± 0.0008</cell><cell>-</cell><cell>0.0082 ± 0.0016</cell><cell>-</cell><cell>0.0615 ± 0.0055</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="3">0.0077 ± 0.0014 -0.1015 ± 0.3970 0.0107 ± 0.0030</cell><cell>0.1338 ± 0.1958</cell><cell>0.0727 ± 0.0056</cell><cell>1.3645 ± 0.1313</cell></row><row><cell>CSM</cell><cell cols="5">0.0068 ± 0.0020 -1.0450 ± 0.2197 0.0098 ± 0.0027 -0.7641 ± 0.0976 0.0663 ± 0.0062</cell><cell>0.3055 ± 0.1080</cell></row><row><cell>STNP</cell><cell cols="6">0.0043 ± 0.0005 -1.1555 ± 0.0317 0.0070 ± 0.0010 -1.0324 ± 0.0627 0.0576 ± 0.0070 -0.0438 ± 0.0543</cell></row><row><cell cols="7">S+JTNP 0.0051 ± 0.0007 -1.1630 ± 0.0350 0.0069 ± 0.0005 -1.0567 ± 0.0248 0.0598 ± 0.0041 -0.0294 ± 0.0833</cell></row><row><cell>MTNP</cell><cell cols="6">0.0036 ± 0.0002 -1.1960 ± 0.0115 0.0053 ± 0.0006 -1.1052 ± 0.0184 0.0533 ± 0.0047 -0.1471 ± 0.0571</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell cols="2">Cloud</cell><cell cols="2">Dew</cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MAML</cell><cell>0.3091 ± 0.0224</cell><cell>-</cell><cell>0.2715 ± 0.0071</cell><cell>-</cell><cell>0.0108 ± 0.0010</cell><cell>-</cell></row><row><cell>Reptile</cell><cell>0.3231 ± 0.0310</cell><cell>-</cell><cell>0.2816 ± 0.0075</cell><cell>-</cell><cell>0.0105 ± 0.0014</cell><cell>-</cell></row><row><cell>MOSM</cell><cell cols="5">0.2947 ± 0.0173 5.7383 ± 0.5086 0.3100 ± 0.0171 1.8804 ± 0.3589 0.0122 ± 0.0021</cell><cell>0.2336 ± 0.3153</cell></row><row><cell>CSM</cell><cell cols="6">0.2833 ± 0.0070 4.3286 ± 1.1360 0.2896 ± 0.0138 1.2055 ± 0.2523 0.0116 ± 0.0028 -0.5992 ± 0.3012</cell></row><row><cell>STNP</cell><cell cols="6">0.2602 ± 0.0080 0.9782 ± 0.2730 0.2590 ± 0.0146 0.7367 ± 0.0436 0.0093 ± 0.0007 -0.9411 ± 0.0460</cell></row><row><cell cols="7">S+JTNP 0.2377 ± 0.0100 0.6721 ± 0.0671 0.2354 ± 0.0052 0.6888 ± 0.0249 0.0086 ± 0.0004 -1.0001 ± 0.0183</cell></row><row><cell>MTNP</cell><cell cols="6">0.2248 ± 0.0081 0.6188 ± 0.0566 0.2193 ± 0.0041 0.6403 ± 0.0262 0.0078 ± 0.0008 -1.0210 ± 0.0187</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 22 :</head><label>22</label><figDesc>Average reconstruction errors (performance) and disagreement of predictions (coherency) on 2D function regression, with varying context size (m) and γ = 0. ± 0.0024 0.3961 ± 0.0014 0.2072 ± 0.0012 0.6503 ± 0.0026 0.5609 ± 0.0012 0.5197 ± 0.0009 JTNP 0.5746 ± 0.0031 0.3800 ± 0.0011 0.2339 ± 0.0024 0.5256 ± 0.0009 0.4959 ± 0.0016 0.5055 ± 0.0007 MTNP 0.5399 ± 0.0019 0.3413 ± 0.0017 0.1889 ± 0.0011 0.5033 ± 0.0014 0.4866 ± 0.0016 0.4928 ± 0.0006</figDesc><table><row><cell></cell><cell></cell><cell>task</cell><cell></cell><cell cols="2">RGB (performance)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>m</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell></cell><cell>STNP</cell><cell cols="4">0.0344 ± 0.0002 0.0101 ± 0.0000 0.0034 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell>JTNP</cell><cell cols="4">0.0304 ± 0.0002 0.0073 ± 0.0000 0.0021 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell cols="5">MTNP 0.0295 ± 0.0001 0.0066 ± 0.0000 0.0015 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Edge (performance)</cell><cell></cell><cell cols="2">Edge (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell>STNP</cell><cell cols="7">0.0342 ± 0.0001 0.0198 ± 0.0000 0.0066 ± 0.0000 0.0307 ± 0.0002 0.0238 ± 0.0001 0.0128 ± 0.0001</cell></row><row><cell>JTNP</cell><cell cols="7">0.0297 ± 0.0001 0.0124 ± 0.0000 0.0041 ± 0.0000 0.0175 ± 0.0002 0.0120 ± 0.0001 0.0138 ± 0.0000</cell></row><row><cell cols="8">MTNP 0.0283 ± 0.0001 0.0108 ± 0.0000 0.0025 ± 0.0000 0.0160 ± 0.0001 0.0067 ± 0.0000 0.0040 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Segment (performance)</cell><cell></cell><cell cols="2">Segment (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell cols="2">STNP 0.6155 task</cell><cell cols="2">PNCC (performance)</cell><cell></cell><cell cols="2">PNCC (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 23 :</head><label>23</label><figDesc>Average reconstruction errors (performance) and disagreement of predictions (coherency) on 2D function regression, with varying context size (m) and γ = 0.25. ± 0.0040 0.4243 ± 0.0021 0.2463 ± 0.0017 0.6587 ± 0.0012 0.5766 ± 0.0014 0.5246 ± 0.0011 S+JTNP 0.5940 ± 0.0024 0.4013 ± 0.0013 0.2741 ± 0.0018 0.5303 ± 0.0011 0.5037 ± 0.0008 0.5076 ± 0.0013 MTNP 0.5615 ± 0.0022 0.3677 ± 0.0009 0.2374 ± 0.0014 0.5103 ± 0.0028 0.4910 ± 0.0011 0.4935 ± 0.0010</figDesc><table><row><cell></cell><cell></cell><cell>task</cell><cell></cell><cell cols="2">RGB (performance)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>m</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell></cell><cell>STNP</cell><cell cols="4">0.0378 ± 0.0002 0.0121 ± 0.0001 0.0041 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell cols="5">S+JTNP 0.0342 ± 0.0003 0.0094 ± 0.0001 0.0031 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell>MTNP</cell><cell cols="4">0.0329 ± 0.0002 0.0084 ± 0.0000 0.0021 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Edge (performance)</cell><cell></cell><cell cols="2">Edge (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell>STNP</cell><cell cols="7">0.0349 ± 0.0001 0.0223 ± 0.0001 0.0085 ± 0.0000 0.0309 ± 0.0002 0.0254 ± 0.0002 0.0146 ± 0.0001</cell></row><row><cell cols="8">S+JTNP 0.0312 ± 0.0001 0.0152 ± 0.0000 0.0061 ± 0.0000 0.0184 ± 0.0002 0.0119 ± 0.0001 0.0139 ± 0.0000</cell></row><row><cell>MTNP</cell><cell cols="7">0.0298 ± 0.0001 0.0133 ± 0.0000 0.0040 ± 0.0000 0.0176 ± 0.0001 0.0079 ± 0.0001 0.0046 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Segment (performance)</cell><cell></cell><cell cols="2">Segment (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell cols="2">STNP 0.6315 task</cell><cell cols="2">PNCC (performance)</cell><cell></cell><cell cols="2">PNCC (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 24 :</head><label>24</label><figDesc>Average reconstruction errors (performance) and disagreement of predictions (coherency) on 2D function regression, with varying context size (m) and γ = 0.5. ± 0.0021 0.4669 ± 0.0009 0.2969 ± 0.0019 0.6710 ± 0.0021 0.5966 ± 0.0027 0.5353 ± 0.0012 S+JTNP 0.6322 ± 0.0024 0.4347 ± 0.0013 0.3176 ± 0.0018 0.5317 ± 0.0015 0.5169 ± 0.0019 0.5126 ± 0.0006 MTNP 0.6095 ± 0.0025 0.4010 ± 0.0016 0.2891 ± 0.0019 0.5224 ± 0.0021 0.4956 ± 0.0011 0.4948 ± 0.0004</figDesc><table><row><cell></cell><cell></cell><cell>task</cell><cell></cell><cell cols="2">RGB (performance)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>m</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell></cell><cell>STNP</cell><cell cols="4">0.0446 ± 0.0005 0.0154 ± 0.0001 0.0054 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell cols="5">S+JTNP 0.0422 ± 0.0004 0.0129 ± 0.0001 0.0046 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell>MTNP</cell><cell cols="4">0.0407 ± 0.0005 0.0113 ± 0.0000 0.0032 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Edge (performance)</cell><cell></cell><cell cols="2">Edge (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell>STNP</cell><cell cols="7">0.0359 ± 0.0001 0.0256 ± 0.0001 0.0116 ± 0.0000 0.0317 ± 0.0002 0.0271 ± 0.0002 0.0173 ± 0.0001</cell></row><row><cell cols="8">S+JTNP 0.0337 ± 0.0001 0.0191 ± 0.0000 0.0090 ± 0.0000 0.0200 ± 0.0002 0.0122 ± 0.0001 0.0142 ± 0.0001</cell></row><row><cell>MTNP</cell><cell cols="7">0.0324 ± 0.0001 0.0167 ± 0.0000 0.0060 ± 0.0000 0.0196 ± 0.0001 0.0097 ± 0.0002 0.0053 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Segment (performance)</cell><cell></cell><cell cols="2">Segment (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell cols="2">STNP 0.6641 task</cell><cell cols="2">PNCC (performance)</cell><cell></cell><cell cols="2">PNCC (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 25 :</head><label>25</label><figDesc>Average reconstruction errors (performance) and disagreement of predictions (coherency) on 2D function regression, with varying context size (m) and γ = 0.75. ± 0.0018 0.5351 ± 0.0021 0.3712 ± 0.0014 0.6804 ± 0.0015 0.6297 ± 0.0023 0.5601 ± 0.0014 S+JTNP 0.6611 ± 0.0013 0.4916 ± 0.0024 0.3778 ± 0.0031 0.5361 ± 0.0016 0.5385 ± 0.0004 0.5298 ± 0.0009 MTNP 0.6485 ± 0.0029 0.4573 ± 0.0025 0.3472 ± 0.0017 0.5307 ± 0.0012 0.5031 ± 0.0014 0.4962 ± 0.0010</figDesc><table><row><cell></cell><cell></cell><cell>task</cell><cell></cell><cell cols="2">RGB (performance)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>m</cell><cell>10</cell><cell>100</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell></cell><cell>STNP</cell><cell cols="4">0.0512 ± 0.0005 0.0224 ± 0.0001 0.0086 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell cols="5">S+JTNP 0.0496 ± 0.0005 0.0203 ± 0.0001 0.0079 ± 0.0000</cell></row><row><cell></cell><cell></cell><cell>MTNP</cell><cell cols="4">0.0483 ± 0.0002 0.0178 ± 0.0001 0.0058 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Edge (performance)</cell><cell></cell><cell cols="2">Edge (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell>STNP</cell><cell cols="7">0.0366 ± 0.0001 0.0302 ± 0.0001 0.0176 ± 0.0000 0.0323 ± 0.0003 0.0294 ± 0.0001 0.0222 ± 0.0001</cell></row><row><cell></cell><cell cols="7">0.0356 ± 0.0001 0.0251 ± 0.0001 0.0145 ± 0.0000 0.0214 ± 0.0002 0.0138 ± 0.0000 0.0157 ± 0.0001</cell></row><row><cell>MTNP</cell><cell cols="7">0.0344 ± 0.0001 0.0223 ± 0.0001 0.0103 ± 0.0000 0.0202 ± 0.0001 0.0130 ± 0.0001 0.0069 ± 0.0000</cell></row><row><cell>task</cell><cell></cell><cell cols="2">Segment (performance)</cell><cell></cell><cell cols="2">Segment (coherency)</cell></row><row><cell>m</cell><cell>10</cell><cell></cell><cell>100</cell><cell>512</cell><cell>10</cell><cell>100</cell><cell>512</cell></row><row><cell cols="2">STNP 0.6907 task</cell><cell cols="2">PNCC (performance)</cell><cell></cell><cell cols="2">PNCC (coherency)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 27 :</head><label>27</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5.</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MTNP-A</cell><cell cols="6">0.4157 ± 0.0174 0.2855 ± 0.0259 0.1913 ± 0.0148 0.1121 ± 0.0219 0.0549 ± 0.0108 0.0320 ± 0.0017</cell></row><row><cell>MTNP-P</cell><cell cols="6">0.3568 ± 0.0208 0.2090 ± 0.0088 0.1185 ± 0.0066 0.0780 ± 0.0131 0.0296 ± 0.0035 0.0153 ± 0.0013</cell></row><row><cell cols="7">MTNP-SA 0.2668 ± 0.0107 0.1299 ± 0.0118 0.0543 ± 0.0120 0.0452 ± 0.0085 0.0103 ± 0.0020 0.0033 ± 0.0003</cell></row><row><cell>MTNP-SP</cell><cell cols="6">0.2636 ± 0.0105 0.1137 ± 0.0078 0.0485 ± 0.0034 0.0435 ± 0.0047 0.0115 ± 0.0021 0.0040 ± 0.0002</cell></row><row><cell>task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 28 :</head><label>28</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5.</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell>Humidity</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="7">MTNP-A 0.0067 ± 0.0007 -1.0773 ± 0.0231 0.0119 ± 0.0012 -0.8658 ± 0.0405 0.0745 ± 0.0033 -0.0471 ± 0.0278</cell></row><row><cell>MTNP-P</cell><cell cols="6">0.0046 ± 0.0003 -1.1614 ± 0.0129 0.0071 ± 0.0002 -1.0166 ± 0.0145 0.0581 ± 0.0029 -0.1463 ± 0.0232</cell></row><row><cell cols="7">MTNP-SA 0.0039 ± 0.0001 -1.1881 ± 0.0074 0.0058 ± 0.0005 -1.0990 ± 0.0203 0.0542 ± 0.0024 -0.2083 ± 0.0283</cell></row><row><cell cols="7">MTNP-SP 0.0037 ± 0.0001 -1.1832 ± 0.0165 0.0054 ± 0.0001 -1.1049 ± 0.0154 0.0546 ± 0.0021 -0.1006 ± 0.0696</cell></row><row><cell>task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell>Dew</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="7">MTNP-A 0.2514 ± 0.0049 0.6655 ± 0.0212 0.2578 ± 0.0070 0.7186 ± 0.0369 0.0114 ± 0.0012 -0.9103 ± 0.0316</cell></row><row><cell>MTNP-P</cell><cell cols="6">0.2400 ± 0.0072 0.6205 ± 0.0210 0.2360 ± 0.0070 0.6532 ± 0.0166 0.0083 ± 0.0001 -1.0183 ± 0.0039</cell></row><row><cell cols="7">MTNP-SA 0.2262 ± 0.0041 0.6270 ± 0.0288 0.2242 ± 0.0042 0.6825 ± 0.0280 0.0074 ± 0.0004 -1.0409 ± 0.0200</cell></row><row><cell cols="7">MTNP-SP 0.2276 ± 0.0028 0.6557 ± 0.0433 0.2215 ± 0.0043 0.6660 ± 0.0141 0.0073 ± 0.0003 -1.0331 ± 0.0147</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 29 :</head><label>29</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5. ± 0.0157 0.2609 ± 0.0382 0.0993 ± 0.0182 0.1307 ± 0.0134 0.0468 ± 0.0074 0.0159 ± 0.0028 MTNP-S 0.2636 ± 0.0105 0.1137 ± 0.0078 0.0485 ± 0.0034 0.0435 ± 0.0047 0.0115 ± 0.0021 0.0040 ± 0.0002 MTNP-TS 0.2901 ± 0.0164 0.1297 ± 0.0079 0.0532 ± 0.0050 0.0618 ± 0.0069 0.0172 ± 0.0041 0.0063 ± 0.0006</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>STNP-S</cell><cell cols="6">0.5387 ± 0.0270 0.2596 ± 0.0351 0.0845 ± 0.0216 0.1255 ± 0.0143 0.0459 ± 0.0095 0.0123 ± 0.0019</cell></row><row><cell cols="2">STNP-TS 0.5212 task</cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 33 :</head><label>33</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5. ± 0.0039 0.0985 ± 0.0059 0.0447 ± 0.0034 0.0423 ± 0.0079 0.0110 ± 0.0015 0.0042 ± 0.0007 MTNP 0.2636 ± 0.0105 0.1137 ± 0.0078 0.0485 ± 0.0034 0.0435 ± 0.0047 0.0115 ± 0.0021 0.0040 ± 0.0002 MTNP-best 0.1239 ± 0.0087 0.0491 ± 0.0068 0.0176 ± 0.0032 0.0169 ± 0.0028 0.0032 ± 0.0009 0.0009 ± 0.0001 ± 0.0011 0.0015 ± 0.0004 0.0006 ± 0.0002 0.0281 ± 0.0018 0.0127 ± 0.0015 0.0073 ± 0.0012 MTNP 0.0066 ± 0.0019 0.0014 ± 0.0001 0.0006 ± 0.0001 0.0360 ± 0.0018 0.0132 ± 0.0008 0.0069 ± 0.0012 MTNP-best 0.0025 ± 0.0007 0.0004 ± 0.0001 0.0002 ± 0.0000 0.0146 ± 0.0021 0.0046 ± 0.0006 0.0021 ± 0.0005</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell cols="2">MTNP-D 0.1963 task</cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MTNP-D</cell><cell>0.0073</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 34 :</head><label>34</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5. ± 0.0001 -1.1892 ± 0.0079 0.0059 ± 0.0002 -1.0944 ± 0.0168 0.0535 ± 0.0023 -0.2142 ± 0.0359 MTNP 0.0037 ± 0.0001 -1.1832 ± 0.0165 0.0054 ± 0.0001 -1.1049 ± 0.0154 0.0546 ± 0.0021 -0.1006 ± 0.0696 MTNP-best 0.0032 ± 0.0001 -1.2143 ± 0.0118 0.0047 ± 0.0001 -1.1441 ± 0.0111 0.0489 ± 0.0019 -0.2320 ± 0.0460 ± 0.0035 0.6034 ± 0.0315 0.2260 ± 0.0038 0.6536 ± 0.0130 0.0077 ± 0.0003 -1.0434 ± 0.0150 MTNP 0.2276 ± 0.0028 0.6557 ± 0.0433 0.2215 ± 0.0043 0.6660 ± 0.0141 0.0073 ± 0.0003 -1.0331 ± 0.0147 MTNP-best 0.2160 ± 0.0026 0.5491 ± 0.0269 0.2091 ± 0.0041 0.6044 ± 0.0090 0.0065 ± 0.0003 -1.0763 ± 0.0102 H.4 EFFECT OF TASK EMBEDDINGS In this study, we compare to different types of task embeddings for MTNP. MTNP-Onehot uses one-hot encoded vector for task embedding e t while MTNP-learnable uses learnable vector for the task embedding. Note that we consistently use MTNP-learnable for the 1D experiments in Section 5</figDesc><table><row><cell>task</cell><cell></cell><cell>TempMin</cell><cell>TempMax</cell><cell></cell><cell>Humidity</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell cols="2">MTNP-D 0.0039 task</cell><cell>Precip</cell><cell>Cloud</cell><cell></cell><cell>Dew</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MTNP-D</cell><cell>0.2257</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 35 :</head><label>35</label><figDesc>Average normalized MSE on synthetic tasks, with varying context size (m) and γ = 0.5. Onehot 0.2507 ± 0.0174 0.1047 ± 0.0123 0.0426 ± 0.0075 0.0440 ± 0.0050 0.0110 ± 0.0014 0.0036 ± 0.0005 MTNP-Learnable 0.2636 ± 0.0105 0.1137 ± 0.0078 0.0485 ± 0.0034 0.0435 ± 0.0047 0.0115 ± 0.0021 0.0040 ± 0.0002 ± 0.0013 0.0014 ± 0.0003 0.0005 ± 0.0001 0.0338 ± 0.0017 0.0130 ± 0.0009 0.0062 ± 0.0008 MTNP-Learnable 0.0066 ± 0.0019 0.0014 ± 0.0001 0.0006 ± 0.0001 0.0360 ± 0.0018 0.0132 ± 0.0008 0.0069 ± 0.0012</figDesc><table><row><cell>task</cell><cell></cell><cell>Sine</cell><cell></cell><cell></cell><cell>Tanh</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MTNP-task</cell><cell></cell><cell>Sigmoid</cell><cell></cell><cell></cell><cell>Gaussian</cell><cell></cell></row><row><cell>m</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>MTNP-Onehot</cell><cell>0.0067</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 36 :</head><label>36</label><figDesc>Average MSE and NLL on weather tasks, with m = 10 and γ = 0.5. Onehot 0.0036 ± 0.0002 -1.1975 ± 0.0088 0.0055 ± 0.0001 -1.1046 ± 0.0090 0.0526 ± 0.0014 -0.0835 ± 0.1149 MTNP-Learnable 0.0037 ± 0.0001 -1.1832 ± 0.0165 0.0054 ± 0.0001 -1.1049 ± 0.0154 0.0546 ± 0.0021 -0.1006 ± 0.0696 ± 0.0034 0.7003 ± 0.0482 0.2244 ± 0.0044 0.6909 ± 0.0264 0.0070 ± 0.0004 -1.0479 ± 0.0155 MTNP-Learnable 0.2276 ± 0.0028 0.6557 ± 0.0433 0.2215 ± 0.0043 0.6660 ± 0.0141 0.0073 ± 0.0003 -1.0331 ± 0.0147</figDesc><table><row><cell>task</cell><cell>TempMin</cell><cell></cell><cell>TempMax</cell><cell></cell><cell>Humidity</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MTNP-task</cell><cell>Precip</cell><cell></cell><cell>Cloud</cell><cell></cell><cell>Dew</cell><cell></cell></row><row><cell>metric</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell><cell>MSE</cell><cell>NLL</cell></row><row><cell>MTNP-Onehot</cell><cell>0.2265</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/imantsm/COVID-19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We normalize the MSE for fair consideration of difference in amplitude a across different functions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work was supported by the <rs type="funder">Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP)</rs> (No. <rs type="grantNumber">2021-0-00537</rs> and <rs type="grantNumber">2019-0-00075</rs>) and the <rs type="funder">National Research Foundation of Korea (NRF)</rs> (No. <rs type="grantNumber">2021R1C1C1012540</rs>) funded by the <rs type="funder">Korea government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4tbh6Xx">
					<idno type="grant-number">2021-0-00537</idno>
				</org>
				<org type="funding" xml:id="_VXSa3UM">
					<idno type="grant-number">2019-0-00075</idno>
				</org>
				<org type="funding" xml:id="_nJbpjC5">
					<idno type="grant-number">2021R1C1C1012540</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>Recently, detecting and removing data bias have become essential problems towards producing fair machine learning models. We believe that our work can contribute to detect unintentional data bias present in multi-attribute data. MTNP can be seen as a universal correlation learner who learns arbitrary correlation across tasks purely data-driven way. Therefore, given potentially biased multi-attribute data (e.g., multiple personal attributes), MTNP may detect any biased relationship by learning the correlation between them. For example, we may perform the task-to-task transfer analysis on a trained MTNP as discussed in Section 5.2 and Section 5.3, then see which task (or attribute) has a high correlation with another task (or attribute).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibiltiy Statement</head><p>In this work, we present two major theoretical results (Proposition 1 and Eq. 7), a neural network model (Section 3.3), and experiments on three datasets (Section 5). We give a complete proof of Proposition 1 in Appendix A.2 and the ELBO derivation for Eq. 7 in Appendix A.3. We provide architectural details and training hyper-parameters of the models used in the experiments in Appendix B. Finally, details on the experimental settings and datasets are provided in Appendix C and Appendix F. For the other models, we plot the mean prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EXPERIMENTAL DETAILS OF IMAGE 2D FUNCTION REGRESSION</head><p>In this section, we describe details of the data generating process and experimental settings of the image function regression experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 DATASET</head><p>We use 30,000 RGB images from CelebA HQ dataset <ref type="bibr" target="#b23">(Liu et al., 2015)</ref> for RGB task and the corresponding semantic segmentation masks among 19 semantic classes from CelebA Mask-HQ dataset <ref type="bibr">(Lee et al., 2020)</ref> for Segment task. For Edge task, we apply the Sobel filter <ref type="bibr" target="#b17">(Kanopoulos et al., 1988)</ref> on the RGB images to generate continuous-valued edges. This corresponds to the Canny edge <ref type="bibr" target="#b2">(Canny, 1986)</ref> without non-maximum suppression, which is also used in Zamir et. al.</p><p>(2018) <ref type="bibr" target="#b39">(Zamir et al., 2018)</ref>. For PNCC task, we apply a pretrained 3D face reconstruction model on the RGB images to generate PNCC label maps <ref type="bibr">(Guo et al., 2020a)</ref>. At each pixel, the PNCC label consists of the (x, y, z) coordinate of the facial keypoint located at the pixel. In summary, labels for RGB are 3-dimensional, Edge are 1-dimensional, Segment are 19-dimensional, and PNCC are 3-dimensional vectors. We split the 30,000 images into 27,000 train, 1,500 valid, and 1,500 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 EVALUATION PROTOCOL</head><p>To evaluate the accuracy of the multi-task prediction, we average MSE M SE = 1 n n i=1 (y t i -ŷt i ) 2 on the test images for continuous tasks (RGB, Edge, PNCC), and average mean IoU on the test images for discrete task (Segment). The predictive posterior mean is computed by Monte Carlo sampling, the same as in the 1D experiment. For categorical outputs, we discretize the prediction with the argmax operator ŷi t = argmax k p(y t i = k|x i , v t ). To evaluate the consistency of predictions across tasks (coherency), we translate each RGB prediction to other task labels. For Edge and PNCC, we use the ground-truth label generation algorithm and the pretrained model used to generate the ground-truth labels for the translation, respectively. For Segment, we fine-tuned DeeplabV3+ <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> with ImageNet <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref> pretrained ResNet-50 <ref type="bibr" target="#b13">(He et al., 2016)</ref> backbone. We refer to the github repository of Yakubovskiy (2019) (Yakubovskiy, 2020) for the DeeplabV3+ model. After the translation, we measure MSE and 1 -mIoU for continuous and discrete tasks respectively, to evaluate the disagreement (as oppose to the coherency) between the predictions.</p><p>To examine the learned correlation across tasks by MTNP (Table <ref type="table">3</ref> in the main paper), we compare the performance before and after MTNP observes a set of source data. The source data consist of all examples labeled with the source tasks. For example, if the target task is RGB and the source tasks are Edge and Segment, we give Edge and Segment labels for all pixels, while no RGB or PNCC labels are given. Since MTNP requires at least one labeled example for each task, we give a single completely labeled example which is chosen randomly to MTNP as a base context, before MTNP observes the source data. There are total 4 1 + 4 2 + 4 3 = 14 different combinations of source tasks exist. By excluding the case where target task is in the set of source tasks, total 7 different combinations of source tasks remain for each target task. To measure the performance gain from task f 1 to f 2 , we average the performance gain of f 2 from all sets of source tasks that containing f 1 . For example, the performance gain from Edge to RGB is computed by averaging performance gains δ Edge→RGB , δ Edge,Segment→RGB , δ Edge,PNCC→RGB , and δ Edge,Segment,PNCC→RGB , where we denote δ A→B by the performance gain from source tasks A to target task B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ADDITIONAL RESULTS ON IMAGE 2D FUNCTION REGRESSION</head><p>In this section, we provide additional results on the image regression experiment, with various missing rates γ and also with standard deviation from 5 different random seeds.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kernels for vector-valued functions: A review</title>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rich Caruana. Multitask learning. Machine learning</title>
		<imprint>
			<date type="published" when="1986">1986. 1997</date>
		</imprint>
	</monogr>
	<note>PAMI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><surname>Rätsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08098</idno>
		<title level="m">Meta-learning mean functions for gaussian processes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eslami</forename><surname>Ali</surname></persName>
		</author>
		<title level="m">Conditional neural processes. In ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural processes</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional conditional neural processes</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to branch for multi-task learning</title>
		<author>
			<persName><forename type="first">Pengsheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitask learning and benchmarking with clinical time series data</title>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>David C Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An Introduction to Probability Theory</title>
		<author>
			<persName><forename type="first">Kiyosi</forename><surname>Itô</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman Li-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<title level="m">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Design of an image edge detection filter using the sobel operator</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Kanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagesh</forename><surname>Vasanthavada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSC</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, and Yee Whye Teh. Attentive neural processes</title>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural ode processes</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Norcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot task transfer</title>
		<author>
			<persName><forename type="first">Arghya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral mixture kernels for multi-output gaussian processes</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Tobar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent attentive neural process for sequential data</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Shenghao Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenshuo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Youngsung Son, and Sungjin Ahn. Sequential neural processes</title>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multi-task learning</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Francisco Jr</forename><surname>Michalis K Titsias</surname></persName>
		</author>
		<author>
			<persName><surname>Ruiz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03228</idno>
		<title level="m">Sotirios Nikoloutsopoulos, and Alexandre Galashov. Information theoretic meta learning with gaussian processes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gp kernels for cross-spectrum analysis</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kafui</forename><surname>Dzirasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational inference for neural processes with hierarchical latent variables</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Locally downscaled and spatially customizable climate data for historical and future periods for north america</title>
		<author>
			<persName><forename type="first">Tongli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Spittlehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Carroll</surname></persName>
		</author>
		<ptr target="https://github.com/qubvel/segmentation_models.pytorch" />
	</analytic>
	<monogr>
		<title level="j">Pavel Yakubovskiy. Segmentation models pytorch</title>
		<imprint>
			<date type="published" when="2016">2016. 2020</date>
		</imprint>
	</monogr>
	<note>PloS one</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robustifying sequential neural processes</title>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
