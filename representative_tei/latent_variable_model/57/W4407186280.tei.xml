<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Neural Process Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-04">4 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenzhe</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zehao</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan-Jakob</forename><surname>Sonke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
						</author>
						<title level="a" type="main">Geometric Neural Process Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-04">4 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.02338v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the challenge of Neural Field (NeF) generalization, where models must efficiently adapt to new signals given only a few observations. To tackle this, we propose Geometric Neural Process Fields (G-NPF), a probabilistic framework for neural radiance fields that explicitly captures uncertainty. We formulate NeF generalization as a probabilistic problem, enabling direct inference of NeF function distributions from limited context observations. To incorporate structural inductive biases, we introduce a set of geometric bases that encode spatial structure and facilitate the inference of NeF function distributions. Building on these bases, we design a hierarchical latent variable model, allowing G-NPF to integrate structural information across multiple spatial levels and effectively parameterize INR functions. This hierarchical approach improves generalization to novel scenes and unseen signals. Experiments on novel-view synthesis for 3D scenes, as well as 2D image and 1D signal regression, demonstrate the effectiveness of our method in capturing uncertainty and leveraging structural information for improved generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Fields (NeFs) <ref type="bibr">(Sitzmann et al., 2020b;</ref><ref type="bibr" target="#b63">Tancik et al., 2020)</ref> have emerged as a powerful framework for learning continuous, compact representations of signals across domains, including 1D signal <ref type="bibr" target="#b73">(Yin et al., 2022)</ref>, 2D images <ref type="bibr">(Sitzmann et al., 2020b)</ref>, and 3D scenes <ref type="bibr" target="#b51">(Park et al., 2019;</ref><ref type="bibr" target="#b42">Mescheder et al., 2019)</ref>. A notable advancement in 3D scene modeling is Neural Radiance Fields (NeRFs) <ref type="bibr" target="#b43">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b1">Barron et al., 2021)</ref>, which extend NeFs to map 3D coordinates and viewing directions to volumetric density and view-dependent radiance. By differentiable 1 University of Amsterdam 2 Carnegie Mellon University 3 The Netherlands Cancer Institute. Correspondence to: Jiayi Shen &lt;j.shen@uva.nl&gt;.</p><p>volume rendering along camera rays, NeRFs achieve photorealistic novel view synthesis. Although NeRFs achieve good reconstruction performance, they must be overfitted to each 3D object or scene, resulting in poor generalization to new 3D scenes with few context images.</p><p>In this paper, we focus on neural field generalization (also referred to as conditional neural fields) and the rapid adaptation of NeFs to new signals. Previous works on NeF generalization have addressed this challenge using gradient-based meta-learning <ref type="bibr" target="#b64">(Tancik et al., 2021)</ref>, enabling adaptation to new scenes with only a few optimization steps <ref type="bibr" target="#b64">(Tancik et al., 2021;</ref><ref type="bibr" target="#b50">Papa et al., 2024)</ref>. Other approaches include modulating shared MLPs through HyperNets <ref type="bibr">(Chen &amp; Wang, 2022;</ref><ref type="bibr" target="#b41">Mehta et al., 2021;</ref><ref type="bibr">Dupont et al., 2022a;</ref><ref type="bibr" target="#b32">Kim et al., 2023)</ref> or directly predicting the parameters of scene-specific MLPs <ref type="bibr" target="#b14">(Dupont et al., 2021;</ref><ref type="bibr" target="#b17">Erkoc ¸et al., 2023)</ref>. However, the deterministic nature of these methods cannot capture uncertainty in NeFs, when used with scenes with only limited observations are available. This is important as such sparse data may be interpreted in multiple valid ways.</p><p>To address uncertainty arising from having few context images, probabilistic NeFs <ref type="bibr" target="#b23">(Gu et al., 2023;</ref><ref type="bibr" target="#b24">Guo et al., 2023;</ref><ref type="bibr" target="#b36">Kosiorek et al., 2021)</ref> have recently been investigated. For example, VNP <ref type="bibr" target="#b24">(Guo et al., 2023)</ref> and PONP <ref type="bibr" target="#b23">(Gu et al., 2023)</ref> infer the NeFs using Neural Processes (NPs) <ref type="bibr" target="#b2">(Bruinsma et al., 2023;</ref><ref type="bibr" target="#b20">Garnelo et al., 2018b;</ref><ref type="bibr" target="#b69">Wang &amp; Van Hoof, 2020)</ref>, a probabilistic meta-learning method that models functional distributions conditioned on partial signal observations. These probabilistic methods, however, do not exploit potential structural information, such as the geometric characteristics of signals (e.g., object shape) or hierarchical organization in the latent space (from global to local). Incorporating such inductive biases can facilitate more effective adaptation to new signals from partial observations. To jointly capture uncertainty and leverage inherent structural information for efficient adaptation to new signals with few observations, we propose a probabilistic neural fields generalization framework called Geometric Neural Processes Fields (G-NPF). Our contributions can be summarized as follows: 1) Probabilistic NeF generalization framework. We formulate NeF generalization as a probabilistic modeling problem, allowing us to amortize a learned model over multiple signals and improve NeF learning and generalization. 2) Geometric bases. To encode structural inductive biases, we design geometric bases that incorporate prior knowledge (e.g., Gaussian structures), enabling the aggregation of local information and the integration of geometric cues. 3) Geometric neural processes with hierarchical latent variables. Building on these geometric bases, we develop geometric neural processes to capture uncertainty in the latent NeF function space. Specifically, we introduce hierarchical latent variables at multiple spatial scales, offering improved generalization for novel scenes and views. Experiments on 1D and 2D signals demonstrate the effectiveness of the proposed method for NeF generalization. Furthermore, we adapt our approach to the formulation of Neural Radiance Fields (NeRFs) with differentiable volume rendering on ShapeNet objects and NeRF Synthetic scenes to validate the versatility of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Neural (Radiance) Fields</head><p>Neural Fields (NeFs) <ref type="bibr">(Sitzmann et al., 2020b)</ref> are continuous functions f ω : x → y, parameterized by a neural network whose parameters ω we optimize to reconstruct the continuous signal y on coordinates x. As with regular neural networks, fitting Neural Field parameters ω relies on gradient descent minimization. Unlike regular networks, however, conventional Neural Fields are explicitly designed to overfit the signal y during reconstruction deterministically, without considering generalization <ref type="bibr" target="#b43">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b1">Barron et al., 2021)</ref>. The reason is that Neural Fields have been primarily considered in transductive learning settings in 3D graphics, whereby the optimization objective is to optimally reconstruct the photorealism of single 3D objects at a time. In this case, there is no need for generalization across objects. A single trained Neural Field network is optimized to "fill in" the specific shape of a specific 3D object under all possible view points, given input point cloud (coordinates x). For each separate 3D object, we optimize a separate Neural Field afresh. Beyond 3D graphics, Neural Fields have found applicability in a broad array of 1D <ref type="bibr" target="#b73">(Yin et al., 2022)</ref> and 2D <ref type="bibr">(Chen et al., 2023b)</ref> applications, for scientific <ref type="bibr" target="#b52">(Raissi et al., 2019)</ref> and medical data <ref type="bibr" target="#b11">(de Vries et al., 2023)</ref>, especially when considering continuous spatiotemporal settings.</p><p>Neural Radiance Fields (NeRF) <ref type="bibr" target="#b43">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b0">Arandjelović &amp; Zisserman, 2021)</ref> are Neural Fields specialized for 3D graphics, reconstructing the 3D shape and texture of a single objects. Specifically, each point p = (p x , p y , p z ) in the 3D space centered around the object has a color c(p, d), where d = (θ, ϕ) is the direction of the camera looking at the point p. Since objects might be opaque or translucent, points also have opacity σ(p). In Neural Field terms, therefore, our input comprises point co-ordinates and the camera direction, that is x = (p, d), and our output comprises colors and opacities, that is y = (c, σ).</p><p>Optimizing a NeRF is an inverse problem: we do not have direct access to ground-truth 3D colors and points of the object. Instead, we optimize solely based on 2D images from known camera positions o and viewing directions d. Specifically, we optimize the parameters ω of the NeRF function, which encodes the 3D shape and color of the object, allowing us to render novel 2D views from arbitrary camera positions and directions using ray tracing along r = (o, d). This ray-tracing process integrates colors and opacities along the ray, accumulating contributions from points until they reach the camera. The objective is to ensure that NeRF-generated 2D views match the training images. Since these images provide an object-specific context for inferring its 3D shape and texture, we refer to them as context data. In contrast, all other unknown shape and texture information is target data. For a detailed description of the ray-tracing integration process, see Appendix A. <ref type="bibr" target="#b50">(Papa et al., 2024)</ref> have recently gained popularity to avoid optimizing from scratch a new Neural Field for every new object. Conditional Neural Fields split parameters ω to a shared part ω D that is common between objects in the dataset D, and an object-specific part ω i that is optimized specifically for the i-th object. However, the optimization of ω i is still done independently per object using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Neural Fields</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Processes</head><p>Neural Processes (NPs) <ref type="bibr" target="#b20">(Garnelo et al., 2018b;</ref><ref type="bibr" target="#b33">Kim et al., 2019)</ref> extend the notion of Gaussian Processes (GPs) <ref type="bibr" target="#b53">(Rasmussen, 2003)</ref> by leveraging neural networks for flexible function approximation. Given a context set C = {(x C,n , y C,n )} N n=1 of N input-output pairs, NPs infer a latent variable z that captures function-level uncertainty. When presented with new inputs x T = {x T,m } M m=1 , the goal is to predict y T = {y T,m } M m=1 . Formally, NPs define the predictive distribution:</p><formula xml:id="formula_0">p y T | x T , C = p y T | x T , z p z | C dz.<label>(1)</label></formula><p>Here, p(z | C) is a prior over z derived solely from the context set. During training, an approximate posterior q(z | C, T ) (where T is the target set consisting of (x T , y T ) pairs) is learned via variational inference <ref type="bibr" target="#b20">(Garnelo et al., 2018b)</ref>. Through this latent-variable formulation, NPs capture both predictive uncertainty and function-level variability, enabling robust performance under partial observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Neural Process Fields</head><p>Despite their great reconstruction capabilities, Neural Fields are still limited by their lack of generalization. While Con- ditional Neural Fields offer an interesting path forward, they still suffer from the unconstrained nature of stochastic gradient descent and the over-parameterized nature of neural networks <ref type="bibr" target="#b50">(Papa et al., 2024)</ref>, thus making representation learning and generalization to few-shot settings hard, whether for 1-D (e.g., for time series), 2-D (e.g., for PINNs), or 3-D (e.g., for occupancy and radiance fields) data. We alleviate this by imposing geometric and hierarchical structure to the NeF and NeRF functions in 1-, 2-, or 3-D data, such that Neural Fields are constrained to the types of outputs that they predict. Further, we embed Conditional Neural Fields in a probabilistic learning framework using Neural Processes, so that the learned Neural Fields generalize well even with few-shot context data settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probabilistic Neural Process Fields</head><p>Conditional Neural Fields, defined in a deterministic setting, bear direct resemblance to Neural Processes and Gaussian Processes and their context and target sets, defined in a probabilistic setting. To make the point clearer, we will use the 2D image completion task as a running example, where the goal is to reconstruct an entire image from a sparse set of observed pixels (an occluded image).</p><p>In image completion task, the C = {(x C,n , y C,n )} N n=1 consists of N observed pixel coordinates x C and their corresponding intensity values y C , while the target set T = {x T } comprises all M pixel coordinates in the image, with y T denoting the unobserved intensities to be predicted. The objective is thus to infer the full image y T conditioned on C, effectively regressing pixel intensities across the entire spatial domain using only the sparse context observations. Although our approach is formulated as a general probabilistic framework, we present a novel 3D-specific extension for Neural Radiance Fields, detailed in Appendix F.</p><p>For probabilistic Neural Process Fields, we adopt the Neural Process decomposition from Eq. (1) for prior distribution,</p><formula xml:id="formula_1">p(y T |x T , x C , y C ) = (2) = p(y T |z, x T , x C , y C ) Conditional Neural Field p(z|x T , x C , y C )dz = M m=1 p(y T,m |z, x T,m , x C , y C )p(z|x T,m , x C , y C )dz,</formula><p>where in the last line of Eq. ( <ref type="formula">2</ref>) we use the fact that the M target output variables, which comprise the target object, are conditionally independent with respect to the latent variable z. In probabilistic Neural Process Fields, z encodes objectlevel information, similar to the object-specific parameters ω i in deterministic Conditional Neural Fields. However, by modeling z probabilistically, our approach enables generalization across different objects, whereas standard NeFs are limited to fitting a single object at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adding Geometric Priors to Probabilistic Neural Process Fields</head><p>With probabilistic Neural Process Fields, we are able to generalize conditional Neural Fields to account for uncertainty and thus be more robust to smaller training datasets and few-shot learning settings. Given that (conditional) Neural Fields are typically implemented as standard MLPs, they do not pertain to a specific structure in their output nor are they constrained in the type of values they can predict. This lack of constraints can have a detrimental impact on the generalization of the learned models, especially when considering Neural Radiance Fields, for which one must also make sure that there is consistency between the 2D observations and the 3D shape of the object.</p><p>To address this problem, we propose adding geometric priors to probabilistic Neural Process Fields. Specifically, we encode the context set C so that to represent it in terms of structured geometric bases B C = b R r=1 , rather than using C directly. Here R is the number of bases. These geometric bases must create an information bottleneck through which we embed structure to the context set C, thus R ≪ ∥C∥ = N .</p><p>Each geometric basis b r = N (µ r , Σ r ), ω r contains a Gaussian distribution N in the 2D spatial plane with covariance Σ r , centered around a 2D coordinate µ r . Note that when extending to the 3D data, N is a 3D Gaussian. Each geometric basis also contains a representation variable ω r , learned jointly to encode the semantics around the location of µ r . The probabilistic Neural Process Field in Eq. ( <ref type="formula">2</ref>) becomes</p><formula xml:id="formula_2">p(y T |x T , B C ) = p(y T |z, x T , B C ) Geometric Priors on Conditional Neural Fields p(z|x T , B C )dz</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adding Hierarchical Priors to Probabilistic Neural Process Fields</head><p>The decomposition in Eq. ( <ref type="formula">2</ref>) conditioning on the latent z allows generalizing conditional Neural Fields with uncertainty to arbitrary training sets, especially by introducing geometric priors in Eq. ( <ref type="formula">3</ref>). We note, however, that when learning probabilistic Neural Fields, our training must serve two slightly conflicting objectives. On one hand, the latent variable encodes the global appearance and geometry of the target object at x T , y T . On the other hand, the Neural Fields are inherently local, in that their inferences are coordinate-specific.</p><p>To ease the tension, we introduce hierarchical latent variables, having a single global latent variable z g , and M local latent variables {z l,m } M m=1 for the M target points x T , to condition the probabilistic Neural Process Fields. A graphical model of our method is provided in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_3">p(y T |x T , B C ) = p(y T |z g , z l , x T , B C ) Hierarchical Priors on Conditional Neural Fields p(z l |z g , x T , B C ) dz l . . . . . . p(z g |x T , B C ) dz g (4) = m p(y T,m |z g , z l,m , x T,m , B C ) . . . . . . p(z l,m |z g , x T , B C ) dz l,m p(z g |x T , B C )dz g .<label>(5)</label></formula><p>In Eq. ( <ref type="formula">4</ref>), we bring p(z g |x T , B C ) out of the inside integral, which marginalizes over the local latent variables z l . In Eq. ( <ref type="formula" target="#formula_3">5</ref>), we further decompose by using the fact that the target variables y T,m and the local latent variables z l,m are conditionally independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>We next describe the implementation of all individual components, and refer to the Appendix C for the full details.</p><p>Geometric basis functions. We implement the geometric basis functions using a transformer encoder, µ, Σ, ω <ref type="formula" target="#formula_3">5</ref>), the prior distribution of each hierarchical latent variable is conditioned on the geometric bases B C and target inputs x T . Since the geometric basis functions rely on Gaussians, we use an MLP with a Gaussian radial basis function to measure their interaction, that is</p><formula xml:id="formula_4">r = Encoder[x C , y C ]. In p(z l,m |z g , x T , B C ) of Eq. (</formula><formula xml:id="formula_5">⟨x T , B C ⟩ = MLP R r=1 exp(- 1 2 (x T -µ r ) T Σ -1 r (x T -µ r )) • ω r ,<label>(6)</label></formula><p>Global latent variables. We model the global latent variable z g as a Gaussian distribution:</p><formula xml:id="formula_6">µ g , σ g = MLP 1 M M m=1 ⟨x T , B C ⟩ ,<label>(7)</label></formula><p>where p(z g |x T , B C ) is parameterized by a Gaussian whose mean µ g and variance σ g are generated via an MLP. Eq (7)</p><p>aggregates representations across all target points to produce a global latent variable z g , thereby parameterizing the underlying object or scene. This formulation enables our model to capture object-specific uncertainty through the inferred distribution of z g .</p><p>Local latent variables. To infer the distribution of the local latent variables z l , we first compute the position-aware representation ⟨x T,m , B C ⟩ for each target point x T,m using Eq (6). The local latent variable z l,m is then derived by combining these representations with the global latent variable z g via a transformer: ) is obtained by propagating each target coordinate x T,m through the neural network, parameterized by z g and z l,m , to model the distribution of outputs y T,m . This process directly leverages the hierarchical prior distributions defined in Eq (5), ensuring consistency across scales. Implementation details of the conditioned network are provided in Appendix C.3.</p><formula xml:id="formula_7">µ l , σ l = Transformer (MLP [⟨x T,m , B C ⟩] ; ẑg ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training objective</head><p>To optimize the proposed G-NPF, we apply variational inference <ref type="bibr" target="#b20">(Garnelo et al., 2018b)</ref> to derive the evidence lower bound (ELBO). Specifically, we first introduce the hierarchical variational posterior: </p><formula xml:id="formula_8">q z g , {z l,m } | x T , B T = M m=1 q z l,m | z g , x T,m , B T q z g | x T , B T ,<label>(8)</label></formula><formula xml:id="formula_9">L = ||y T -y ′ T || 2 2 + α D KL p(z g |B C ) q(z g |B T ) + M m=1 D KL p(z l,m |z g , B C ) q(z l,m |z g , B T ) + β • D KL B C B T ,<label>(9)</label></formula><p>where y ′ T denotes predictions, and α, β balance the terms. The first term enforces local reconstruction quality, while the second ensures that the prior distributions are guided by the variational posterior using the Kullback-Leibler (KL) divergence. The third term, the KL divergence, aligns the spatial distributions of B C and B T , ensuring that the context bases capture the target geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">G-NPF in 1D, 2D, 3D</head><p>The proposed method generalizes seamlessly to 1D, 2D, and 3D signals by leveraging Gaussian structures of corresponding dimensionality. A single global variable consistently encodes the entire signal (e.g., a 3D object or a 2D image), ensuring unified representation. For local variables, we adopt a dimension-specific formulation: in 1D and 2D signals, local variables are associated with individual spatial locations; while in 3D radiance fields, we developed a mechanism where a unique local variable is assigned to each camera ray, detailed in Appendix F. This design preserves both global coherence and local adaptability across signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the generality of G-NPF, we validate extensively on five datasets, comparing in 2D, 3D, and 1D settings with the recent state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">G-NPF in 2D image regression</head><p>We start with experiments in 2D image regression, a canonical task <ref type="bibr" target="#b64">(Tancik et al., 2021;</ref><ref type="bibr">Sitzmann et al., 2020b)</ref> to evaluate how well Neural Fields can fit and represent a 2D signal. In this setting, the context set is an image and the task is to learn an implicit function that regresses the image pixels accurately. Following TransINR <ref type="bibr">(Chen &amp; Wang, 2022)</ref>, we resize each image into 178 × 178, and use patch size 9 for the tokenizer. The self-attention module remains the same as our baseline, VNP <ref type="bibr" target="#b24">(Guo et al., 2023)</ref>. For the Gaussian bases, we predict 2D Gaussians. The hierarchical latent variables are inferred in image-level and pixel-level. We evaluate the method on two real-world image datasets as used in previous works <ref type="bibr">(Chen &amp; Wang, 2022;</ref><ref type="bibr" target="#b64">Tancik et al., 2021;</ref><ref type="bibr" target="#b23">Gu et al., 2023)</ref>.</p><p>CelebA <ref type="bibr" target="#b40">(Liu et al., 2015)</ref>. CelebA encompasses approximately 202,000 images of celebrities, partitioned into train-  ing (162,000 images), validation (20,000 images), and test (20,000 images) sets.</p><p>Imagenette dataset <ref type="bibr" target="#b28">(Howard, 2020)</ref>. Imagenette is a curated subset comprising 10 classes from the 1,000 classes in ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>, consists of roughly 9,000 training images and 4,000 testing images.</p><p>Quantitative results. We give quantitative comparisons in Table <ref type="table" target="#tab_1">1</ref>. G-NPF outperforms baselines on both CelebA and Imagenette datasets significantly, generalizing better.</p><p>Qualitative results. Fig. <ref type="figure" target="#fig_3">3</ref> showcases G-NPF's ability to recover high-frequency details in image regression, producing reconstructions that closely match the ground truth with high fidelity. This highlights the effectiveness of our approach. Additional qualitative results, including image completion, are provided in Appendix E.1. Specifically, Fig. <ref type="figure" target="#fig_7">7</ref> in the Appendix demonstrates that G-NPF can reconstruct full signals from minimal observations, further validating its capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">G-NPF in 3D novel view synthesis</head><p>We continue with experiments in 3D novel view synthesis, a canonical task to evaluate 3D Neural Radiance Fields.</p><p>We follow the implementation of <ref type="bibr" target="#b24">(Guo et al., 2023;</ref><ref type="bibr">Chen &amp; Wang, 2022)</ref>. Briefly, our input context set comprises camera rays and their corresponding image pixels from one or two views. These are split into 256 tokens, each projected into a 512D vector via a linear layer and self-attention. Two MLPs predict 256 geometric bases: one generates 3D Gaussian parameters, and the other outputs 32D latent representations. From these, we derive object-and ray-specific modulating vectors (both 512D). Our NeRF function includes four layers-two modulated and two shared-with further details in Appendix C.1. ShapeNet <ref type="bibr" target="#b3">(Chang et al., 2015)</ref>. We follow the data setup of <ref type="bibr" target="#b64">(Tancik et al., 2021)</ref> We first compare with probabilistic Neural Field baselines, including NeRF-VAE <ref type="bibr" target="#b36">(Kosiorek et al., 2021)</ref>, PONP <ref type="bibr" target="#b23">(Gu et al., 2023)</ref>, and VNP <ref type="bibr" target="#b24">(Guo et al., 2023)</ref>. Like G-NPF, PONP <ref type="bibr" target="#b23">(Gu et al., 2023)</ref> and VNP <ref type="bibr" target="#b24">(Guo et al., 2023</ref>) also use Neural Processes, without, however, considering either geometric or hierarchical priors. Secondly, we also compare with well-established deterministic Neural Fields, including LearnInit <ref type="bibr" target="#b64">(Tancik et al., 2021)</ref> and TransINR <ref type="bibr">(Chen &amp; Wang, 2022)</ref>. We note that recent works <ref type="bibr" target="#b38">(Liu et al., 2023;</ref><ref type="bibr">Shi et al., 2023b)</ref> have shown that training on massive 3D datasets <ref type="bibr" target="#b12">(Deitke et al., 2023)</ref> is highly beneficial for Neural Radiance Fields. We leave massive-scale settings and comparisons to future work. Thirdly, to demonstrate the flexibility of G-NPF to handle complex scenes, we integrate with GNT <ref type="bibr" target="#b68">(Wang et al., 2022)</ref> and conduct experiments on the NeRF Synthetic dataset <ref type="bibr" target="#b43">(Mildenhall et al., 2021)</ref>.</p><p>Quantitative results. We show Peak Signal-to-Noise Ratio (PSNR) results in Table <ref type="table" target="#tab_2">2</ref>. G-NPF consistently outperforms all other baselines across all categories by a significant margin. On average, G-NPF outperforms the probabilistic Neural Field baselines such as VNP <ref type="bibr" target="#b24">(Guo et al., 2023)</ref>, by 0.87 PSNR, indicating that adding structure in the form of geometric and hierarchical priors leads to better generalization.  With two views for context, G-NPF improves significantly by about 1 PSNR.</p><p>Qualitative results. In Fig. <ref type="figure" target="#fig_5">4</ref>, we visualize the results on novel view synthesis of ShapeNet objects. G-NPF can infer object-specific radiance fields and render high-quality 2D images of the objects from novel camera views, even with only 1 or 2 views as context. More results and comparisons with other VNP are provided in Appendix E.</p><p>NeRF Synthetic <ref type="bibr" target="#b43">(Mildenhall et al., 2021)</ref>. We further evaluate on the NeRF Synthetic dataset against recent stateof-the-art, including GNT <ref type="bibr" target="#b68">(Wang et al., 2022)</ref>, MatchN-eRF <ref type="bibr">(Chen et al., 2023a)</ref>, and GeFu <ref type="bibr" target="#b39">(Liu et al., 2024)</ref>. For a fair comparison, we use the same encoder and NeRF network architecture while integrating our probabilistic framework into GNT. Following GeFu, we assess performance in 2-view and 3-view settings.</p><p>Quantitative results. We present results in Table <ref type="table" target="#tab_3">3</ref>. We observe that G-NPF surpasses GeFu by approximately 1 PSNR in the 3-view setting, validating the effectiveness of our probabilistic framework and geometric bases. Moreover, we consider a challenging 1-view setting to examine the model's robustness under extremely limited context. Both Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure">8</ref> indicate that G-NPF reconstructs novel views effectively also with only a single view for context, in contrast to GNT that fails in this setting. We furthermore test cross-category generalization for our model and GNT without retraining, training on the drums category and evaluating on lego. As shown in Figure <ref type="figure">9</ref>, G-NPF leverages the available context information more effectively, producing higher-quality generations with better color fidelity compared to GNT. We give additional details in Appendix E.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">G-NPF in 1D signal regression</head><p>Following the previous works' implementation <ref type="bibr" target="#b24">(Guo et al., 2023;</ref><ref type="bibr" target="#b33">Kim et al., 2019)</ref>, we conduct 1D signal regression experiments using synthetic functions drawn from Gaussian processes (GPs) with RBF and Matern kernels. This kernel selection, as advocated by <ref type="bibr" target="#b34">Kim et al. (2022)</ref>, ensures diverse function characteristics spanning smoothness, periodicity, and local variability. To evaluate performance, we adopt two key metrics: (1) context reconstruction error, quantifying the log-likelihood of observed data points (context set), and</p><p>(2) target prediction error, measuring the log-likelihood of extrapolated predictions (target set). We compare with three baselines, VNP <ref type="bibr" target="#b24">(Guo et al., 2023)</ref>, CNP <ref type="bibr">(Garnelo et al., 2018a)</ref>, and ANP <ref type="bibr" target="#b33">(Kim et al., 2019)</ref>.</p><p>Quantitative results. We present a quantitative comparison with baselines in Table <ref type="table" target="#tab_4">4</ref>. G-NPF consistently outperforms the baselines across two types of synthetic data, demonstrating its effectiveness and flexibility in different signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations</head><p>Geometric bases. We first ablate the effectiveness of the proposed geometric bases on a subset of the 3D Lamps scene synthesis task. As shown in Table <ref type="table" target="#tab_5">5</ref> (rows 1 and 5), with the geometric bases, GeomNP performs clearly better. This in- We further analyze the sensitivity to the number of geometric bases in CelebA image regression and Lamps NeRF tasks. Results in Table <ref type="table" target="#tab_6">6</ref> show that more bases lead to better accuracies and better generalization. We choose the number of bases by balancing the performance and computation.</p><p>Hierarchical latent variables. We ablate the importance of the hierarchical nature of G-NPF on a subset of the Lamps dataset. The last four rows of Table <ref type="table" target="#tab_5">5</ref> show that both global and local latent variables contribute to improved accuracy, with their combination yielding the best performance. Furthermore, the qualitative ablation study on hierarchical latent variables in Fig. <ref type="figure" target="#fig_3">13</ref> in the Appendix E.5 confirms that they effectively capture global and local structures, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Neural Fields (NeFs) and Generalization. Neural Fields (NeFs) map coordinates to signals, providing a compact and flexible continuous data representation <ref type="bibr">(Sitzmann et al., 2020b;</ref><ref type="bibr" target="#b63">Tancik et al., 2020)</ref>. They are widely used for 3D object and scene modeling <ref type="bibr" target="#b9">(Chen &amp; Zhang, 2019;</ref><ref type="bibr" target="#b51">Park et al., 2019;</ref><ref type="bibr" target="#b42">Mescheder et al., 2019;</ref><ref type="bibr" target="#b21">Genova et al., 2020;</ref><ref type="bibr" target="#b49">Niemeyer &amp; Geiger, 2021)</ref>. However, how to generalize to new scenes without retraining remains a problem. Many previous methods attempt to use meta-learning to achieve NeF generalization. Specifically, gradient-based meta-learning algorithms such as Model-Agnostic Meta Learning (MAML) <ref type="bibr" target="#b18">(Finn et al., 2017)</ref> and Reptile <ref type="bibr" target="#b48">(Nichol et al., 2018)</ref> have been used to adapt NeFs to unseen data samples in a few gradient steps <ref type="bibr" target="#b37">(Lee et al., 2021;</ref><ref type="bibr">Sitzmann et al., 2020a;</ref><ref type="bibr" target="#b64">Tancik et al., 2021)</ref>. Another line of work uses HyperNet <ref type="bibr" target="#b25">(Ha et al., 2016)</ref> to predict modulation vectors for each data instance, scaling and shifting the activations in all layers of the shared MLP <ref type="bibr" target="#b41">(Mehta et al., 2021;</ref><ref type="bibr">Dupont et al., 2022a;</ref><ref type="bibr">b)</ref>. Some methods use HyperNet to predict the weight matrix of NeF functions <ref type="bibr" target="#b14">(Dupont et al., 2021;</ref><ref type="bibr" target="#b74">Zhang et al., 2023)</ref>. Transformers <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref> have also been used as hypernetworks to predict column vectors in the weight matrix of MLP layers <ref type="bibr">(Chen &amp; Wang, 2022;</ref><ref type="bibr">Dupont et al., 2022b)</ref>. In addition, <ref type="bibr" target="#b54">(Reizenstein et al., 2021;</ref><ref type="bibr" target="#b68">Wang et al., 2022)</ref> use transformers specifically for NeRF. Such methods are deterministic and do not consider the uncertainty of a scene when only partially observed. Other approaches model NeRF from a probabilistic perspective <ref type="bibr" target="#b36">(Kosiorek et al., 2021;</ref><ref type="bibr" target="#b26">Hoffman et al., 2023;</ref><ref type="bibr" target="#b14">Dupont et al., 2021;</ref><ref type="bibr" target="#b44">Moreno et al., 2023;</ref><ref type="bibr" target="#b17">Erkoc ¸et al., 2023)</ref>. For instance, NeRF-VAE <ref type="bibr" target="#b36">(Kosiorek et al., 2021)</ref> learns a distribution over radiance fields using latent scene representations based on VAE <ref type="bibr" target="#b35">(Kingma &amp; Welling, 2013)</ref> with amortized inference. Normalizing flow <ref type="bibr" target="#b71">(Winkler et al., 2019)</ref> has also been used with variational inference to quantify uncertainty in NeRF representations <ref type="bibr" target="#b55">(Shen et al., 2022;</ref><ref type="bibr" target="#b70">Wei et al., 2023)</ref>. However, these methods do not consider potential structural information, such as the geometric characteristics of signals, which our approach explicitly models.</p><p>Neural Processes. Neural Processes (NPs) <ref type="bibr" target="#b20">(Garnelo et al., 2018b</ref>) is a meta-learning framework that characterizes distributions over functions, enabling probabilistic inference, rapid adaptation to novel observations, and the capability to estimate uncertainties. This framework is divided into two classes of research. The first one concentrates on the marginal distribution of latent variables <ref type="bibr" target="#b20">(Garnelo et al., 2018b)</ref>, whereas the second targets the conditional distributions of functions given a set of observations <ref type="bibr">(Garnelo et al., 2018a;</ref><ref type="bibr" target="#b22">Gordon et al., 2019)</ref>. Typically, MLP is employed in Neural Processes methods. To improve this, Attentive Neural Processes (ANP) <ref type="bibr" target="#b33">(Kim et al., 2019)</ref> integrate the attention mechanism to improve the representation of individual context points. Similarly, Transformer Neural Processes (TNP) <ref type="bibr" target="#b47">(Nguyen &amp; Grover, 2022)</ref> view each context point as a token and utilize transformer architecture to effectively approximate functions. Additionally, the Versatile Neural Process (VNP) <ref type="bibr" target="#b24">(Guo et al., 2023)</ref> employs attentive neural processes for neural field generalization but does not consider the information misalignment between the 2D context set and the 3D target points. The hierarchical structure in VNP is more sequential than global-to-local. Conversely, PONP <ref type="bibr" target="#b23">(Gu et al., 2023)</ref> is agnostic to neural-field specifics and concentrates on the neural process perspective. In this work, we consider a hierarchical neural process to model the structure information of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we addressed the challenge of Neural Field (NeF) generalization, enabling models to rapidly adapt to new signals with limited observations. To achieve this, we proposed Geometric Neural Processes (G-NPF), a probabilistic neural radiance field that explicitly captures uncertainty. By formulating neural field generalization in a probabilistic framework, G-NPF incorporates uncertainty and infers NeF function distributions directly from sparse context images. To embed structural priors, we introduce geometric bases, which learn to provide structured spatial information. Additionally, our hierarchical neural process modeling leverages both global and local latent variables to parameterize NeFs effectively. In practice, G-NPF extends to 1D, 2D, and 3D signal generalization, demonstrating its versatility across different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper contributes to the advancement of Machine Learning. While our work may have various societal implications, none require specific emphasis at this stage.</p><p>We again apply Jensen's inequality, but now w.r.t. q({z l,m } | z g , x T , B T ), factorizing over m:</p><formula xml:id="formula_10">log p y T , {z l,m } | z g , x T , B C q {z l,m } | z g , x T , B T q {z l,m } | z g , x T , B T dz l ≥ E q({z l,m }|zg,x T ,B T ) log p y T | z g , {z l,m }, x T , B C - M m=1 D KL q z l,m | z g , x T,m , B T p z l,m | z g , x T,m , B C .<label>(14)</label></formula><p>Putting this back into Eq. ( <ref type="formula">13</ref>), we arrive at the hierarchical ELBO:</p><formula xml:id="formula_11">log p y T | x T , B C ≥ E q(zg|x T ,B T ) M m=1 E q(z l,m |zg,x T ,m ,B T ) log p y T,m | z g , z l,m , x T,m , B C - M m=1 D KL q z l,m | z g , x T,m , B T ∥ p z l,m | z g , x T,m , B C -D KL q z g | x T , B T ∥ p z g | x T , B C .<label>(15)</label></formula><p>The first expectation over q(z g | x T , B T ) enforces global consistency and penalizes deviations from the prior p(z g | x T , B C ).</p><p>The second set of expectations over q(z l,m | z g , x T,m , B T ) shapes local reconstruction quality (via the log-likelihood) and penalizes deviations from the local prior p(z l,m | z g , x T,m , B C ).</p><p>Hence, the final ELBO (Equation <ref type="formula" target="#formula_11">15</ref>) combines these outer and inner regularization terms with the expected log-likelihood of the target data y T . This allows the model to learn coherent global structure as well as local (coordinate-specific) details in a principled way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Gaussian Construction</head><p>Since 3D Gaussians represent a special case involving quaternion-based transformations, we use them here as an illustrative example for constructing geometric bases. However, the method remains consistent with the construction of 1D and 2D Gaussian geometric bases.</p><p>Geometric Bases with 3D Gaussians. To impose geometric structure on the context variables, we encode the context set {x C , y C } into a set of M geometric bases:</p><formula xml:id="formula_12">B C = b r R r=1</formula><p>, where b r = N µ r , Σ r , ω r .</p><p>Each basis b r is thus defined by a 3D Gaussian N(µ r , Σ r ) and an associated semantic embedding ω r . The center µ r ∈ R 3 and covariance Σ i ∈ R 3×3 capture location and shape, while ω r ∈ R d B represents additional learned properties (e.g., color or texture). In our implementation, d B = 32.</p><p>Self-Attention Construction. We use a self-attention module, denoted Att, to extract these Gaussian parameters from the context data. Concretely,</p><formula xml:id="formula_14">µ i , Σ i = Att x C , y C , ω i = Att x C , y C ,<label>(17)</label></formula><p>where each call to Att produces M tokens of hidden dimension D. An MLP then maps each token into a 10-dimensional vector encoding: (i) the 3D center µ i , (ii) a 3D scaling vector s i , and (iii) a 4D quaternion q i that, together, define the rotation matrix R i . Following <ref type="bibr" target="#b31">Kerbl et al. (2023)</ref>, we obtain the covariance Σ i via</p><formula xml:id="formula_15">Σ i = R i S i S ⊤ i R ⊤ i ,<label>(18)</label></formula><p>where S i = diag(s i ) ∈ R 3×3 is the scaling matrix and R i ∈ R 3×3 is derived from q i . A separate MLP outputs the 32-dimensional embedding ω i . Consequently, each b i is a fully parameterized 3D Gaussian plus a semantic vector, allowing the model to represent rich geometric information inferred from the context set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Image Regression</head><p>We provide more image regression results to demonstrate the effectiveness of our method as shown in Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>Image Completion. In addition, we also conduct experiments of G-NPF on image completion (also called image inpainting), which is a more challenging variant of image regression. Essentially, only part of the pixels are given as context, while the INR functions are required to complete the full image. Visualizations in Fig. <ref type="figure" target="#fig_7">7</ref> demonstrate the generalization ability of our method to recover realistic images with fine details based on very limited context (10% -20% pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Comparison with GNT.</head><p>For fair comparison, we use GNT's image encoder and predict the geometric bases, and GNT's NeRF' network for prediction. Fig. <ref type="figure">8</ref> shows that our method is effective when very limited context information is given, while GNT fails. This indicates that our method can sufficiently utilize the given information.</p><p>Cross-Category Example. Additionally, we perform cross-category evaluation without retraining the model. The model is trained on drums category and evaluated on lego. As shown in Figure <ref type="figure">9</ref>, G-NPF leverages the available context information more effectively, producing higher-quality generations with better color fidelity compared to GNT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. More results on ShapeNet</head><p>In this section, we demonstrate more experimental results on the novel view synthesis task on ShapeNet in <ref type="bibr">Fig 10, comparison with VNP (Guo et al., 2023)</ref> in Fig. <ref type="figure" target="#fig_0">11</ref>, and image regression on the Imagenette dataset in Fig. <ref type="figure" target="#fig_6">6</ref>. The proposed method is able to generate realistic novel view synthesis and 2D images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Training Time Comparison</head><p>As illustrated in Fig. <ref type="figure" target="#fig_10">12</ref>, with the same training time, our method (GeomNP) demonstrates faster convergence and higher final PSNR compared to the baseline (VNP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Qualitative ablation of the hierarchical latent variables</head><p>In this section, we perform a qualitative ablation study on the hierarchical latent variables. As illustrated in Fig. <ref type="figure" target="#fig_3">13</ref>, the absence of the global variable prevents the model from accurately predicting the object's outline, whereas the local variable captures fine-grained details. When both global and local variables are incorporated, GeomNP successfully estimates the novel view with high accuracy.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GeomNP</head><formula xml:id="formula_16">p(Y T |X T , B C ) = p(Y T |f NeRF , X T )p(f NeRF |X T , B C )df NeRF ,<label>(23)</label></formula><p>where p(f NeRF |X T , B C ) is the prior distribution of the NeRF function, and p(Y T |f NeRF , X T ) is the likelihood term. Note that the prior distribution of the NeRF function is conditioned on the target points X T and the geometric bases B C . Thus, the prior distribution is data-dependent on the target inputs, yielding a better generalization on novel target views of new objects. Moreover, since B C is constructed with continuous Gaussian distributions in the 3D space, the geometric bases can enrich the locality and semantic information of each discrete target point, enhancing the capture of high-frequency details <ref type="bibr">(Chen et al., 2023b;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b46">Müller et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Geometric Neural Processes with Hierarchical Latent Variables</head><p>With the geometric bases, we propose Geometric Neural Processes (GeomNP) by inferring the NeRF function distribution p(f NeRF |X T , B C ) in a probabilistic way. Based on the probabilistic NeRF generalization in Eq. ( <ref type="formula">22</ref>), we introduce hierarchical latent variables to encode various spatial-specific information into p(f NeRF |X T , B C ), improving the generalization ability in different spatial levels. Since all rays are independent of each other, we decompose the predictive distribution in Eq. ( <ref type="formula" target="#formula_16">23</ref>) as:</p><formula xml:id="formula_17">p(Y T |X T , B C ) = N n=1 p(y r,n T |x r,n T , B C ),<label>(24)</label></formula><p>where the target input X T consists of N × P location points {x r,n T } N n=1 for N rays. Further, we develop a hierarchical Bayes framework for GeomNP to accommodate the data structure of the target input X T in Eq. ( <ref type="formula" target="#formula_17">24</ref>). We introduce an object-specific latent variable z o and N individual ray-specific latent variables {z n r } N n=1 to represent the randomness of f NeRF .</p><p>where ẑo is a sample from the prior distribution p(z o |X T , B C ). Similar to the object-specific latent variable, we also assume the distribution p(z n r |z o , x r,n T , B C ) is a mean-field Gaussian distribution with the mean µ r and variance σ r . We provide more details of the latent variables in Appendix C.2.</p><p>NeRF Function Modulation. With the hierarchical latent variables {z o , z n r }, we modulate a neural network for a 3D object in both object-specific and ray-specific levels. Specifically, the modulation of each layer is achieved by scaling its weight matrix with a style vector <ref type="bibr" target="#b24">(Guo et al., 2023)</ref>. The object-specific latent variable z o and ray-specific latent variable z n r are taken as style vectors of the low-level layers and high-level layers, respectively. The prediction distribution p(Y T |X T , B C ) are finally obtained by passing each location representation through the modulated neural network for the NeRF function. More details are provided in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Empirical Objective</head><p>Evidence Lower Bound. To optimize the proposed GeomNP, we apply variational inference <ref type="bibr" target="#b20">(Garnelo et al., 2018b)</ref> and derive the evidence lower bound as:</p><formula xml:id="formula_18">log p(Y T |X T B ) ≥ E q(zo|B T T ) N n=1 r |zo,x r,n T ,B T ) log p(y r,n T |x r,n T , z o , z n r ) -D KL [q(z n r |z o , x r,n T , B T )||p(z n r |z o , x r,n T , B C )] -D KL [q(z o |B T , X T )||p(z o |B C , X T )],<label>(29)</label></formula><p>where</p><formula xml:id="formula_19">q θ,ϕ (z o , {z i r } N i=1 |X T , B T ) = Π N i=1 q(z n r |z o , x r,n T , B T )q(z o |B T , X T )</formula><p>is the involved variational posterior for the hierarchical latent variables. B T is the geometric bases constructed from the target sets { X T , Y T }, which are only accessible during training. The variational posteriors are inferred from the target sets during training, which introduces more information on the object. The prior distributions are supervised by the variational posterior using Kullback-Leibler (KL) divergence, learning to model more object information with limited context data and generalize to new scenes. Detailed derivations are provided in Appendix F.5.</p><p>For the geometric bases B C , we regularize the spatial shape of the context geometric bases to be closer to that of the target one B T by introducing a KL divergence. Therefore, given the above ELBO, our objective function consists of three parts: a reconstruction loss (MSE loss), KL divergences for hierarchical latent variables, and a KL divergence for the geometric bases. The empirical objective for the proposed GeomNP is formulated as:</p><formula xml:id="formula_20">L GeomNP = ||y -y ′ || 2 2 + α • D KL [p(z o |B C )|q(z o |B T )] + D KL [p(z r |z o , B C )|q(z r |z o , B T )] + β • D KL [B C , B T ],<label>(30)</label></formula><p>where y ′ is the prediction. α and β are hyperparameters to balance the three parts of the objective. The KL divergence on B C , B T is to align the spatial location and the shape of two sets of bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5. Derivation of Evidence Lower Bound</head><p>Evidence Lower Bound. We optimize the model via variational inference <ref type="bibr" target="#b20">(Garnelo et al., 2018b)</ref>, deriving the evidence lower bound (ELBO):</p><formula xml:id="formula_21">log p(Y T | X T , B C ) ≥ E q(zg|X T ,B T ) M m=1 E q(z m l |zg,x m T ,B T ) log p(y m T | z g , z m l , x m T ) -D KL q(z m l |z g , x m T , B T ) p(z m l |z g , x m T , B C ) -D KL q(z g |X T , B T ) p(z g |X T , B C ) ,<label>(31)</label></formula><p>where the variational posterior factorizes as q(z g , {z m l } M m=1 |X T , B T ) = q(z g |X T , B T ) </p><formula xml:id="formula_22">M</formula><p>where q θ,ϕ (z o , {z i r } N i=1 |X T , B T ) = q(z n r |z o , x r,n T , B T )q(z o |B T , X T ) is the variational posterior of the hierarchical latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. More Related Work</head><p>Generalizable Neural Radiance Fields (NeRF) Advancements in neural radiance fields have focused on improving generalization across diverse scenes and objects. <ref type="bibr" target="#b68">(Wang et al., 2022)</ref> propose an attention-based NeRF architecture, demonstrating enhanced capabilities in capturing complex scene geometries by focusing on informative regions. <ref type="bibr" target="#b61">(Suhail et al., 2022)</ref> introduce a generalizable patch-based neural rendering approach, enabling models to adapt to new scenes without retraining. <ref type="bibr" target="#b72">(Xu et al., 2022)</ref> present Point-NeRF, leveraging point-based representations for efficient scene modeling and scalability. <ref type="bibr" target="#b67">(Wang et al., 2024)</ref> further enhance point-based methods by incorporating visibility and feature augmentation to improve robustness and generalization. <ref type="bibr" target="#b39">(Liu et al., 2024)</ref> propose a geometry-aware reconstruction with fusion-refined rendering for generalizable NeRFs, improving geometric consistency and visual fidelity. Recently, the Large Reconstruction Model (LRM) <ref type="bibr" target="#b27">(Hong et al., 2023)</ref> has drawn attention. It aims for single-image to 3D reconstruction, emphasizing scalability and handling of large datasets.</p><p>Gaussian Splatting-based Methods Gaussian splatting <ref type="bibr" target="#b31">(Kerbl et al., 2023)</ref> has emerged as an effective technique for efficient 3D reconstruction from sparse views. <ref type="bibr" target="#b62">(Szymanowicz et al., 2024)</ref> propose Splatter Image for ultra-fast single-view 3D reconstruction. <ref type="bibr" target="#b4">(Charatan et al., 2024)</ref> introduce pixelsplat, utilizing 3D Gaussian splats from image pairs for scalable generalizable reconstruction. <ref type="bibr" target="#b8">(Chen et al., 2025)</ref> present MVSplat, focusing on efficient Gaussian splatting from sparse multi-view images. Our approach can be a complementary module for these methods by introducing a probabilistic neural processing scheme to fully leverage the observation.</p><p>Diffusion-based 3D Reconstruction Integrating diffusion models into 3D reconstruction has shown promise in handling uncertainty and generating high-quality results. <ref type="bibr" target="#b45">(Müller et al., 2023)</ref> introduce DiffRF, a rendering-guided diffusion model for 3D radiance fields. <ref type="bibr" target="#b65">(Tewari et al., 2023)</ref> explore solving stochastic inverse problems without direct supervision using diffusion with forward models. <ref type="bibr" target="#b38">(Liu et al., 2023)</ref> propose Zero-1-to-3, a zero-shot method for generating 3D objects from a single image without training on 3D data, utilizing diffusion models. <ref type="bibr">(Shi et al., 2023a)</ref> introduce Zero123++, generating consistent multi-view images from a single input image using diffusion-based techniques. <ref type="bibr">(Shi et al., 2023c)</ref> present MVDream, which uses multi-view diffusion for 3D generation, enhancing the consistency and quality of reconstructed models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed G-NPF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical model for the proposed geometric neural processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where B T are target set-derived bases (available only at training). The variational posteriors are inferred from the target set T during training with the same encoder, which introduces more information on the object. The prior distributions are supervised by the variational posterior using Kullback-Leibler (KL) divergence, learning to model more object information with limited context data and generalize to new scenes. The details about the evidence lower bound (ELBO) and derivation are provided in the Appendix B Finally, the training objective combines reconstruction, hierarchical latent alignment, and geometric basis regulariza-tion:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualizations of image regression results on CelebA (left) and Imagenette (right).</figDesc><graphic coords="6,233.73,178.56,118.97,59.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, with objects from three ShapeNet categories: chairs, cars, and lamps. For each 3D object, 25 views of size 128 × 128 images are generated from viewpoints randomly selected on a sphere. The objects in each category are divided into training and testing sets, with each training object consisting of 25 views with known camera poses. At test time, a random input view is sampled to evaluate the performance of the novel view synthesis. Following the setting of previous methods (Chen &amp; Wang, 2022), we focus on the single-view (1-shot) and 2-view (2shot) versions of the task, where one or two images with their corresponding camera rays are provided as the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative results of the proposed G-NPF on novel view synthesis of ShapeNet objects. Both 1-view (top) and 2-view (bottom) context results are presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: More image regression results on the Imagenette dataset. Left: ground truth; Right: prediction.</figDesc><graphic coords="16,104.04,185.71,388.79,388.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Image completion visualization on CelebA using 10% (left) and 20% (right) context.</figDesc><graphic coords="17,56.76,168.17,80.15,80.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Qualitative comparison with GNT on 1-view setting.</figDesc><graphic coords="18,131.05,273.41,100.36,100.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: More NeRF results on novel view synthesis task on ShapeNet objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Training time vs. PSNR on the ShapeNet Car dataset. Our method (GeomNP) demonstrates faster convergence and higher final PSNR compared to the baseline (VNP).</figDesc><graphic coords="20,176.94,115.81,242.94,127.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :Figure 16 :</head><label>1316</label><figDesc>Figure 13: Qualitative ablation of the hierarchical latent variables (global and local variables).</figDesc><graphic coords="20,402.09,569.97,76.14,76.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>m=1 q(z m l |z g , x m T , B T ). Here, B T denotes geometric bases constructed from target data { X T , Y T } (available only during training). The KL terms regularize the hierarchical priors p(z g |B C ) and p(z m l |z g , B C ) to align with variational posteriors inferred from B T , enhancing generalization to context-only settings.The propose GeomNP is formulated as:p(YT |XT , BC ) = N n=1 p(y r,n T |x r,n T , BC , z n r , zo, )p(r n |zo, x r,n T , BC )dz n r p(zo|XT , BC )dzo,(32)where p(z o |B C , X T ) and p(z n r |z o , x r,n T , B C ) denote prior distributions of a object-specific and each ray-specific latent variables, respectively. Then, the evidence lower bound is derived as follows.log p(Y T |X T , B C ) r,n T |x r,n T , z o , z n r )p(z n r |z o , x r,n T , B C )dz n r p(z o C X T )dz o r,n T |x r,n T , z o , z n r )p(z n r |z o , x r,n T , B C ) q(z n r |z o , x r,n T , B T ) q(z n r |z o , x r,n T , B T ) dz n r p(z o |B C , X T ) q(z o |B T , X T ) q(z o |B T , X T , ) dz o ≥ E q(zo|B T ,X T ) N i=1 log p(y r,n T |x r,n T , z o , z n r )p(z n r |z o , x r,n T , B C ) q(z n r |z o , x r,n T , B T ) q(z n r |z o , x r,n T , B T ) dz n r -D KL (q(z o |B T , X T , )||p(z o |B C , X T )) ≥ E q(zo|B T ,X T ) N n=1 E q(z n r |zo,x r,n T ,B T ) log p(y r,n T |x r,n T , z o , znr ) -D KL [q(z n r |z o , x r,n T , B T )||p(z n r |z o , x r,n T , B C )] -D KL [q(z o |B T , X T )||p(z o |B C , X T )],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where ẑg is a sample from the global prior distribution p(z g | x T , B C ). Mirroring the global latent variable z g , we model the local prior distribution p(z l,m | z g , x T,m , B C ) as a mean-field Gaussian with parameters µ l and σ l . This hierarchical structure enables coordinate-specific uncertainty modeling while preserving global geometric consistency. Full architectural details are provided in Appendix C.2.</figDesc><table /><note><p>Predictive distribution. The hierarchical latent variables {z g , z l,m } condition the neural network to generate predictions that integrate global and local geometric uncertainty. Specifically, the neural field is conditioned jointly on the global latent variable z g , which encodes object-level structure, and the local latent variables z l,m , which capture coordinate-specific variations. The predictive distribution p(y T | x T , B C</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on 2D regression. G-NPF outperforms baseline methods consistently on both datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">CelebA Imagenette</cell></row><row><cell cols="3">Learned Init (Tancik et al., 2021)</cell><cell>30.37</cell><cell>27.07</cell></row><row><cell cols="3">TransINR (Chen &amp; Wang, 2022)</cell><cell>31.96</cell><cell>29.01</cell></row><row><cell cols="2">G-NPF (Ours)</cell><cell></cell><cell>33.41</cell><cell>29.82</cell></row><row><cell>Prediction</cell><cell>GT</cell><cell>Prediction</cell><cell>GT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Qualitative Comparison (PSNR) on Novel View Synthesis of ShapeNet Objects. G-NPF outperforms baselines across categories for both 1-view and 2-view contexts. PSNR ↑ is reported.</figDesc><table><row><cell>Method</cell><cell>Views</cell><cell>Car</cell><cell cols="2">Lamps Chairs</cell></row><row><cell>Learn Init</cell><cell>25</cell><cell cols="2">22.80 22.35</cell><cell>18.85</cell></row><row><cell>Tran-INR</cell><cell>1</cell><cell cols="2">23.78 22.76</cell><cell>19.66</cell></row><row><cell>NeRF-VAE</cell><cell>1</cell><cell cols="2">21.79 21.58</cell><cell>17.15</cell></row><row><cell>PONP</cell><cell>1</cell><cell cols="2">24.17 22.78</cell><cell>19.48</cell></row><row><cell>VNP</cell><cell>1</cell><cell cols="2">24.21 24.10</cell><cell>19.54</cell></row><row><cell>G-NPF (Ours)</cell><cell>1</cell><cell cols="2">25.13 24.59</cell><cell>20.74</cell></row><row><cell>Tran-INR</cell><cell>2</cell><cell cols="2">25.45 23.11</cell><cell>21.13</cell></row><row><cell>PONP</cell><cell>2</cell><cell cols="2">25.98 23.28</cell><cell>19.48</cell></row><row><cell>G-NPF (Ours)</cell><cell>2</cell><cell cols="2">26.39 25.32</cell><cell>22.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Qualitative Comparison on Novel View Synthesis of NeRF Synthetic. G-NPF outperforms baselines consistently.</figDesc><table><row><cell>GNT</cell><cell>1</cell><cell>10.25</cell><cell>0.583</cell><cell>0.496</cell></row><row><cell>G-NPF</cell><cell>1</cell><cell>20.07</cell><cell>0.815</cell><cell>0.208</cell></row><row><cell>GNT</cell><cell>2</cell><cell>23.47</cell><cell>0.877</cell><cell>0.151</cell></row><row><cell>MatchNeRF</cell><cell>2</cell><cell>20.57</cell><cell>0.864</cell><cell>0.200</cell></row><row><cell>GeFu</cell><cell>2</cell><cell>25.30</cell><cell>0.939</cell><cell>0.082</cell></row><row><cell>G-NPF</cell><cell>2</cell><cell>25.66</cell><cell>0.926</cell><cell>0.081</cell></row><row><cell>GNT</cell><cell>3</cell><cell>25.80</cell><cell>0.905</cell><cell>0.104</cell></row><row><cell>MatchNeRF</cell><cell>3</cell><cell>23.20</cell><cell>0.897</cell><cell>0.164</cell></row><row><cell>GeFu</cell><cell>3</cell><cell>26.99</cell><cell>0.952</cell><cell>0.070</cell></row><row><cell>G-NPF</cell><cell>3</cell><cell>27.85</cell><cell>0.958</cell><cell>0.068</cell></row></table><note><p><p>Models</p># Views PSNR (↑) SSIM (↑) LPIPS (↓)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on 1D signal regression. Log-likelihoods (↑) of the context set and target set are reported.</figDesc><table><row><cell></cell><cell cols="2">RBF kernel GP</cell><cell cols="2">Matern kernel GP</cell></row><row><cell>Method</cell><cell>Context</cell><cell>Target</cell><cell>Context</cell><cell>Target</cell></row><row><cell>CNP</cell><cell>1.023 ± 0.033</cell><cell cols="3">0.019 ± 0.015 0.935 ± 0.036 -0.124 ± 0.010</cell></row><row><cell cols="2">Stacked ANP 1.381 ± 0.001</cell><cell cols="2">0.400 ± 0.004 1.381 ± 0.001</cell><cell>0.183 ± 0.012</cell></row><row><cell>VNP</cell><cell cols="3">1.377 ± 0.004 0.651 ± 0.001 1.376 ± 0.004</cell><cell>0.439 ± 0.007</cell></row><row><cell>G-NPF</cell><cell cols="4">1.397 ± 0.006 0.741 ± 0.001 1.376 ± 0.004 0.545 ± 0.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation of geometric bases and hierarchical latent variablesB C z o z r PSNR (↑)</figDesc><table><row><cell>✗</cell><cell cols="2">✓ ✓</cell><cell>23.06</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>25.98</cell></row><row><cell>✓</cell><cell cols="2">✓ ✗</cell><cell>26.24</cell></row><row><cell>✓</cell><cell cols="2">✗ ✓</cell><cell>26.29</cell></row><row><cell>✓</cell><cell cols="2">✓ ✓</cell><cell>26.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Sensitivity to the number of geometric bases on NeRF and image regression.</figDesc><table><row><cell></cell><cell cols="3">Image Regression</cell><cell cols="2">NeRF</cell></row><row><cell># Bases</cell><cell>49</cell><cell>169</cell><cell>484</cell><cell>100</cell><cell>250</cell></row><row><cell cols="6">PSNR (↑) 28.59 33.74 44.24 24.31 24.59</cell></row><row><cell cols="6">dicates the importance of the structure information modeled</cell></row><row><cell cols="6">in the geometric bases. Moreover, the bases perform well</cell></row><row><cell cols="6">without hierarchical latent variables, demonstrating their</cell></row><row><cell cols="6">ability to construct 3D signals from limited 2D context.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of the number of parameters and PSNR on the ShapeNet Car dataset.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="2"># Parameters PSNR</cell><cell></cell></row><row><cell></cell><cell>VNP</cell><cell></cell><cell>34.3M</cell><cell>24.21</cell><cell></cell></row><row><cell></cell><cell cols="2">GeomNP</cell><cell>24.0M</cell><cell>25.13</cell><cell></cell></row><row><cell>Context</cell><cell>Prediction</cell><cell>GT</cell><cell cols="2">Context</cell><cell>Prediction</cell><cell>GT</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural Radiance Field Rendering</head><p>In this section, we outline the rendering function of NeRF <ref type="bibr" target="#b43">(Mildenhall et al., 2021)</ref>. A 5D neural radiance field represents a scene by specifying the volume density and the directional radiance emitted at every point in space. NeRF calculates the color of any ray traversing the scene based on principles from classical volume rendering <ref type="bibr" target="#b29">(Kajiya &amp; Von Herzen, 1984)</ref>. The volume density σ(x) quantifies the differential likelihood of a ray terminating at an infinitesimal particle located at x. The anticipated color C(r) of a camera ray r(t) = o + td, within the bounds t n and t f , is determined as follows: (10)</p><p>Here, the function T (t) represents the accumulated transmittance along the ray from t n to t, which is the probability that the ray travels from t n to t without encountering any other particles. To render a view from our continuous neural radiance field, we need to compute this integral C(r) for a camera ray traced through each pixel of the desired virtual camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hierarchical ELBO Derivation</head><p>Recall the hierarchical predictive distribution:</p><p>and its factorized version across M target points:</p><p>We introduce a hierarchical variational posterior:</p><p>where B T are target-derived bases (available only at training). We then write the log-likelihood as</p><p>We first apply Jensen's inequality w.r.t. q(z g | x T , B T ). This yields:</p><p>Inside the expectation over z g , we have</p><p>Transformer Encoder Linear Linear </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Hierarchical Latent Variables</head><p>At the object level, the distribution of the global latent variable z g is obtained by aggregating all location representations from (B C , x T ). We assume that p(z g | B C , x T ) follows a standard Gaussian distribution, and we generate its mean µ g and variance σ g using MLPs. We then sample a global modulation vector, ẑg , from its prior distribution p(z g | x T , B C ).</p><p>Similarly, as shown in Fig. <ref type="figure">5</ref>, we aggregate information for each target coordinate x T,m using B C , which is then processed through a Transformer along with ẑg to predict the local latent variable z l,m for each target point. The mean µ l,m and variance σ l,m of z l,m are obtained via MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Modulation</head><p>We use modulation to The latent variables for modulating the MLP are represented as [z g ; z l ]. Our approach to the modulated MLP layer follows the style modulation techniques described in <ref type="bibr" target="#b30">(Karras et al., 2020;</ref><ref type="bibr" target="#b24">Guo et al., 2023)</ref>. Specifically, we consider the weights of an MLP layer (or 1x1 convolution) as W ∈ R din×dout , where d in and d out are the input and output dimensions respectively, and w ij is the element at the i-th row and j-th column of W .</p><p>To generate the style vector s ∈ R din , we pass the latent variable z through two MLP layers. Each element s i of the style vector s is then used to modulate the corresponding parameter in W .</p><p>where w ij and w ′ ij denote the original and modulated weights, respectively. The modulated weights are normalized to preserve training stability,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>We train all our models with PyTorch. Adam optimizer is used with a learning rate of 1e -4. For NeRF-related experiments, we follow the baselines <ref type="bibr">(Chen &amp; Wang, 2022;</ref><ref type="bibr" target="#b24">Guo et al., 2023)</ref> to train the model for 1000 epochs. All experiments are conducted on four NVIDIA A5000 GPUs. For the hyper-parameters α and β, we simply set them as 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Complexity</head><p>The comparison of the number of parameters is presented in Table . 7. Our method, GeomNP, utilizes fewer parameters than the baseline, VNP, while achieving better performance on the ShapeNet Car dataset in terms of PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6. More multi-view reconstruction results</head><p>We integrate our method into GNT <ref type="bibr" target="#b68">(Wang et al., 2022)</ref>   Background on Neural Radiance Fields. We formally describe Neural Radiance Field (NeRF) <ref type="bibr" target="#b43">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b0">Arandjelović &amp; Zisserman, 2021)</ref> as a continuous function f NeRF : x → y, which maps 3D world coordinates p and viewing directions d to color and density values y. That is, a NeRF function, f NeRF , is a neural network-based function that represents the whole 3D object (e.g., a car in Fig. <ref type="figure">15</ref>) as coordinates to color and density mappings. Learning a NeRF function of a 3D object is an inverse problem where we only have indirect observations of arbitrary 2D views of the 3D object, and we want to infer the entire 3D object's geometry and appearance. With the NeRF function, given any camera pose, we can render a view on the corresponding 2D image plane by marching rays and using the corresponding colors and densities at the 3D points along the rays. Specifically, given a set of rays r with view directions d, we obtain a corresponding 2D image. The integration along each ray corresponds to a specific pixel on the 2D image using the volume rendering technique described in <ref type="bibr" target="#b29">(Kajiya &amp; Von Herzen, 1984)</ref>, which is also illustrated in Fig. <ref type="figure">15</ref>. Details about the integration are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Probabilistic NeRF Generalization</head><p>Deterministic Neural Radiance Fields Neural Radiance Fields are normally considered as an optimization routine in a deterministic setting <ref type="bibr" target="#b43">(Mildenhall et al., 2021;</ref><ref type="bibr" target="#b1">Barron et al., 2021)</ref>, whereby the function f NeRF fits specifically to the available observations (akin to "overfitting" training data).</p><p>Probabilistic Neural Radiance Fields As we are not just interested in fitting a single and specific 3D object but want to learn how to infer the Neural Radiance Field of any 3D object, we focus on probabilistic Neural Radiance Fields with the following factorization:</p><p>The generation process of this probabilistic formulation is as follows. We first start from (or sample) a set of rays X.</p><p>Conditioning on these rays, we sample 3D points in space X X. Then, we map these 3D points into their colors and density values with the NeRF function, Y = f NeRF (X). Last, we sample the 2D pixels of the viewing image that corresponds to the 3D ray Y|Y, X with a probabilistic process. This corresponds to integrating colors and densities Y along the ray on locations X.</p><p>The probabilistic model in Eq. ( <ref type="formula">21</ref>) is for a single 3D object, thus requiring optimizing a function f NeRF afresh for every new object, which is time-consuming. </p><p>As this paper focuses on generalization with new 3D objects, we keep the same sampling and integrating processes as in Eq. ( <ref type="formula">21</ref>). We turn our attention to the modeling of the predictive distribution p(Y T |X T , X C , Y C ) in the generalization step, which implies inferring the NeRF function.</p><p>Misalignment between 2D context and 3D structures It is worth mentioning that the predictive distribution in 3D space is conditioned on 2D context pixels with their ray { X C , Y C } and 3D target points X T , which is challenging due to potential information misalignment. Thus, we need strong inductive biases with 3D structure information to ensure that 2D and 3D conditional information is fused reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Geometric Bases</head><p>To mitigate the information misalignment between 2D context views and 3D target points, we introduce geometric bases</p><p>, which induces prior structure to the context set { X C , Y C } geometrically. M is the number of geometric bases.</p><p>Each geometric basis consists of a Gaussian distribution in the 3D point space and a semantic representation, i.e., b i = {N (µ i , Σ i ); ω i }, where µ i and Σ i are the mean and covariance matrix of i-th Gaussian in 3D space, and ω i is its corresponding latent representation. Intuitively, the mixture of all 3D Gaussian distributions implies the structure of Within the hierarchical Bayes framework, z o encodes the entire object information from all target inputs and the geometric bases {X T , B C } in the global level; while every z n r encodes ray-specific information from {x r,n T , B C } in the local level, which is also conditioned on the global latent variable z o . The hierarchical architecture allows the model to exploit the structure information from the geometric bases B C in different levels, improving the model's expressiveness ability. By introducing the hierarchical latent variables in Eq. ( <ref type="formula">24</ref>), we model GeomNP as:</p><p>where p(y r,n T |x r,n T , B C , z o , z i r ) denotes the ray-specific likelihood term. In this term, we use the hierarchical latent variables {z o , z i r } to modulate a ray-specific NeRF function f NeRF for prediction, as shown in Fig. <ref type="figure">16</ref>. Hence, f NeRF can explore global information of the entire object and local information of ray, leading to better generalization on views. A graphical model of our method is provided in Fig. <ref type="figure">17</ref>.</p><p>In the modeling of GeomNP, the prior distribution of each hierarchical latent variable is conditioned on the geometric bases and target input. We first represent each target location by integrating the geometric bases, i.e., &lt; x n T , B C &gt;, which aggregates the relevant locality and semantic information for the given input. Since B C contains M Gaussians, we employ a Gaussian radial basis function in Eq. ( <ref type="formula">26</ref>) between each target input x n T and each geometric basis b i to aggregate the structural and semantic information to the 3D location representation. Thus, we obtain the 3D location representation as follows:</p><p>where MLP[•] is a learnable neural network. With the location representation &lt; x n T , B C &gt;, we next infer each latent variable hierarchically, in object and ray levels.</p><p>Object-specific Latent Variable. The distribution of the object-specific latent variable z o is obtained by aggregating all location representations:</p><p>where we assume p(z o |B C , X T ) is a standard Gaussian distribution and generate its mean µ o and variance σ o by a MLP. Thus, our model captures objective-specific uncertainty in the NeRF function.</p><p>Ray-specific Latent Variable. To generate the distribution of the ray-specific latent variable, we first average the location representations ray-wisely. We then obtain the ray-specific latent variable by aggregating the averaged location representation and the object latent variable through a lightweight transformer. We formulate the inference of the ray-specific latent variable as:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nerf in detail: Learning to sample for view synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05264</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5855" to="5864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Requiema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buonomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14468</idno>
		<title level="m">Autoregressive conditional neural processes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Charatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="19457" to="19467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Tensorf</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="333" to="350" />
		</imprint>
	</monogr>
	<note>Tensorial radiance fields</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformers as meta-learners for implicit neural representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="170" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12294</idno>
		<title level="m">Explicit correspondence matching for generalizable neural radiance fields</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient 3d gaussian splatting from sparse multi-view images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><surname>Mvsplat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="370" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neurbf: A neural fields representation with adaptive radial basis functions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4182" to="4194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal physics-informed learning: A novel approach to ct perfusion analysis in acute ischemic stroke</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Van Herten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Hoving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Emmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Majoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Marquering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">102971</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Objaverse: A universe of annotated 3d objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deitke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13142" to="13153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04776</idno>
		<title level="m">Generative models as distributions of functions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">From data to functa: Your data point is a function and you can treat it like one</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12204</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Loya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><surname>Coin++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12904</idno>
		<title level="m">Neural compression across modalities</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperdiffusion: Generating implicit neural fields with weightspace diffusion</title>
		<author>
			<persName><forename type="first">¸</forename><surname>Erkoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14300" to="14310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local deep implicit functions for 3d shape</title>
		<author>
			<persName><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4857" to="4866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13556</idno>
		<title level="m">Convolutional conditional neural processes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalizable neural fields as partially observed neural processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5330" to="5339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Versatile neural processes for learning implicit neural representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08883</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<idno>ArXiv, abs/1609.09106</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">208981547</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probnerf: Uncertainty-aware inference of 3d shapes from 2d images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sountsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10425" to="10444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><surname>Lrm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04400</idno>
		<title level="m">Large reconstruction model for single image to 3d</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><surname>Imagenette</surname></persName>
		</author>
		<ptr target="https://github.com/fastai/imagenette" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Von Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH computer graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">3d gaussian splatting for real-time radiance field rendering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kopanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leimkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Drettakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalizable implicit neural representations via instance pattern composers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11808" to="11817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05761</idno>
		<title level="m">Attentive neural processes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural processes with stochastic attention: Paying more attention to the context dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05449</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nerf-vae: A geometry aware 3d scene generative model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mokrá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5742" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-learning sparse implicit neural representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11769" to="11780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-1-to-3: Zero-shot one image to 3d object</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9298" to="9309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geometry-aware reconstruction and fusionrefined rendering for generalizable neural radiance fields</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7654" to="7663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modulated periodic activations for generalizable local functional representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14214" to="14223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Nerf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Winckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Laser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05747</idno>
		<title level="m">Latent set representations for 3d generative modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rendering-guided 3d radiance field diffusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><surname>Diffrf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4328" to="4338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Instant neural graphics primitives with a multiresolution hash encoding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Transformer neural processes: Uncertainty-aware meta learning via sequence modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04179</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11453" to="11464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How to train neural field representations: A comprehensive study and benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valperga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Knigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kofinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Sonke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22616" to="22625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><surname>Deepsdf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Physicsinformed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer school on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sbordone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10901" to="10911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Conditional-flow nerf: Accurate 3d modelling with reliable uncertainty quantification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="540" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15110</idno>
		<title level="m">Zero123++: a single image to consistent multi-view diffusion base model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Zero123++: a single image to consistent multi-view diffusion base model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Mvdream</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.16512</idno>
		<title level="m">Multi-view diffusion for 3d generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Metasdf: Meta-learning signed distance functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10136" to="10147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalizable patch-based neural rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="156" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Splatter image: Ultra-fast single-view 3d reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Szymanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="10208" to="10217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learned initializations for optimizing coordinate-based neural representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2846" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Diffusion with forward models: Solving stochastic inverse problems without direct supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cazenavette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rezchikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="12349" to="12362" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14354</idno>
		<title level="m">Learning robust generalizable radiance field with visibility and feature augmented point representation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13298</idno>
		<title level="m">Is attention all that nerf needs? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational inference for neural processes with hierarchical latent variables</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10018" to="10028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16364</idno>
		<title level="m">Fg-nerf: Flow-gan based probabilistic neural radiance field for independence-assumption-free uncertainty estimation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning likelihoods with conditional normalizing flows</title>
		<author>
			<persName><forename type="first">C</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Point-nerf: Point-based neural radiance fields</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5438" to="5448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14855</idno>
		<title level="m">Continuous pde dynamics forecasting with implicit neural representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
