<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_zGNP8h5 #_FdRH8rt">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_rYVMnw7 #_9naAgTj">
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_nhsreeg #_AN39Zvw">
					<orgName type="full">Inria Sophia Antipolis -Méditerranée</orgName>
				</funder>
				<funder ref="#_4KndCuc #_GMTaQcX">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raphaël</forename><surname>Sivera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuman</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc-Michel</forename><surname>Rohé</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Loïc</forename><surname>Cadour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Demarcy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophie</forename><surname>Giffard-Roisin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pawel</forename><surname>Mlynarski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Cedilnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manon</forename><surname>Muntanter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yann</forename><surname>Thanwerdas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><surname>Antelmi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clement</forename><surname>Abi-Nader</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tania</forename><surname>Bacoyannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaume</forename><surname>Banus-Cobo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoit</forename><surname>Audelan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Santiago</forename><surname>Silva-Rincon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sara</forename><surname>Garbarino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Guigui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaëtan</forename><surname>Desrues</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Buntheng</forename><surname>Ly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology &amp; Innovation</orgName>
								<orgName type="institution" key="instit1">Yingyu Yang and many more</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>medical imaging</term>
					<term>image registration</term>
					<term>motion modeling</term>
					<term>artificial intelligence</term>
					<term>machine learning</term>
					<term>variational autoencoder</term>
					<term>sudden cardiac death</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This thesis presents new computational tools for quantifying deformations and motion of anatomical structures from medical images as required by a large variety of clinical applications. Generic deformable registration tools are presented that enable deformation analysis useful for improving diagnosis, prognosis and therapy guidance. These tools were built by combining state-of-the-art medical image analysis methods with cutting-edge machine learning methods.</p><p>First, we focus on difficult inter-subject registration problems. By learning from given deformation examples, we propose a novel agent-based optimization scheme inspired by deep reinforcement learning where a statistical deformation model is explored in a trial-and-error fashion showing improved registration accuracy.</p><p>Second, we develop a diffeomorphic deformation model that allows for accurate multiscale registration and deformation analysis by learning a low-dimensional representation of intra-subject deformations. The unsupervised method uses a latent variable model in form of a conditional variational autoencoder (CVAE) for learning a probabilistic deformation encoding that is useful for the simulation, classification and comparison of deformations.</p><p>Third, we propose a probabilistic motion model derived from image sequences of moving organs. This generative model embeds motion in a structured latent space, the motion matrix, which enables the consistent tracking of structures and various analysis tasks. For instance, it leads to the simulation and interpolation of realistic motion patterns allowing for faster data acquisition and data augmentation.</p><p>Finally, we demonstrate the importance of the developed tools in a clinical application where the motion model is used for disease prognosis and therapy planning. It is shown that the survival risk for heart failure patients can be predicted from the discriminative motion matrix with a higher accuracy compared to classical image-derived risk factors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé</head><p>Cette thèse présente de nouveaux outils informatiques pour quantifier les déformations et le mouvement de structures anatomiques à partir d'images médicales dans le cadre d'une grande variété d'applications cliniques. Des outils génériques de recalage déformable sont présentés qui permettent l'analyse de la déformation de tissus anatomiques pour améliorer le diagnostic, le pronostic et la thérapie. Ces outils combinent des méthodes avancées d'analyse d'images médicales avec des méthodes d'apprentissage automatique performantes.</p><p>Dans un premier temps, nous nous concentrons sur les problèmes de recalages inter-sujets difficiles. En apprenant à partir d'exemples de déformation donnés, nous proposons un nouveau schéma d'optimisation basé sur un agent inspiré de l'apprentissage par renforcement profond dans lequel un modèle de déformation statistique est exploré de manière itérative montrant une précision améliorée de recalage.</p><p>Dans un second temps, nous développons un modèle de déformation difféomorphe qui permet un recalage multi-échelle précis et une analyse de déformation en apprenant une représentation de faible dimension des déformations intra-sujet. La méthode non supervisée utilise un modèle de variable latente sous la forme d'un autoencodeur variationnel conditionnel (CVAE) pour apprendre une représentation probabiliste des déformations qui est utile pour la simulation, la classification et la comparaison des déformations.</p><p>Troisièmement, nous proposons un modèle de mouvement probabiliste dérivé de séquences d'images d'organes en mouvement. Ce modèle génératif décrit le mouvement dans un espace latent structuré, la matrice de mouvement, qui permet le suivi cohérent des structures ainsi que l'analyse du mouvement. Ainsi cette approche permet la simulation et l'interpolation de modèles de mouvement réalistes conduisant à une acquisition et une augmentation des données plus rapides.</p><p>Enfin, nous démontrons l'intérêt des outils développés dans une application clinique où le modèle de mouvement est utilisé pour le pronostic de maladies et la planification de thérapies. Il est démontré que le risque de survie des patients souffrant d'insuffisance cardiaque peut être prédit à partir de la matrice de mouvement discriminant avec une précision supérieure par rapport aux facteurs de risque classiques dérivés de l'image.</p><p>v Mots-clés: imagerie médicale, recalage d'images, modélisation du mouvement, intelligence artificielle, apprentissage profond, autoencodeur variationnel, mort cardiaque subite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vi</head><p>I would like to thank my thesis advisers Hervé Delingette, Tommaso Mansi and Nicholas Ayache whose guidance was invaluable for the success of this thesis. Thank you for always listening to my questions and constructive discussions when I encountered difficulties. Your passion, motivation and scientific expertise of the field are and will always be very inspiring to me. Tommaso, I am very happy and grateful you selected me for this opportunity. Thank you for your enthusiastic support during and also before my Ph.D. thesis, for always teaching me, motivating me and for pushing me when things seemed hopeless. Hervé, it has been a great pleasure to work with you. Thank you for the countless scientific and non-scientific discussions and for sharing your stimulating scientific ingenuity and deep insights. I would also like to thank Nicholas for your valuable advises, for accepting me in the Asclepios/Epione team with its outstanding and friendly research environment and for sending me to conferences and summer schools around the world. I am extremely grateful to Prof. Ivana Išgum and Prof. Daniel Rueckert for spending their valuable time on reviewing this manuscript and providing constructive feedback and insights. I am also thankful to Prof. Tom Vercauteren and Prof. Nikos Paragios for accepting to be members of my jury. Thank you, it has been a great honor for me to have such an outstanding jury.</p><p>A sincere thank to Dr. Hiroshi Ashikaga and Dr. Katherine Wu, with whom I worked on the clinical project. Hiroshi, I really enjoyed working with you. Thank you for helping me to understand what is important from the clinical point of view. I thank Dorin Comaniciu for supporting this project and giving me the chance to do four internships in his outstanding team in Princeton. I thank all my colleagues and interns at Siemens Healthineers who made my internships successful, enjoyable and unforgettable. I especially thank Shun Miao, Boris Mailhé, Bin Lou, Li Zhang, Rui Liao, Yue Zhang, Florin Ghesu, Guillaume Chabin, Serkan Çimen, Sasa Grbic, Dong Yang, Ingmar Voigt, Ali Kamen and Sebastien Piat for their help and support. In this chapter, we introduce the clinical context and objectives of the thesis. First, we discuss current needs in clinical routine that motivate our work. We show the steps from patient to diagnosis, to prognosis and therapy. Then, we explain and give concrete examples to these steps that raise the main objectives of this thesis:</p><p>Can we automatically derive relevant information from medical images to learn accurate deformation and motion models that can be helpful for diagnosis, prognosis and therapy planning?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Clinical Context</head><p>The typical clinical workflow consists of four main stages: diagnosis, prognosis, therapy planning and therapy. Starting point is the patient who feels sick and arrives at the hospital to get cured. The first main task of the physician is to collect relevant information such as symptoms, patient health history, vital signs, lab parameters and medical images.</p><p>The physician analyzes all these information with the help of multi-dimensional analysis tools to form a diagnosis. After a potential disease has been identified, a prognosis is made to evaluate the future impact of the disease such as duration and likely outcomes.</p><p>In the next step, therapy planning is done by taking into account all potential treatment measures given the previous information. Finally, the therapy which has the best chances of curing the patient or reducing the symptoms is carried out. These four stages will repeat until the endpoint is reached where the patient is healthy again (or has died).</p><p>Medical images are increasingly important to help clinicians at all four stages of the clinical workflow for a large variety of applications in healthcare. The importance and additional insights clinicians gain from medical images is also shown in the fact that more than one billion radiological exams are performed worldwide every year <ref type="bibr" target="#b92">[Krupinski, 2010]</ref>. In the US, medical imaging accounts for over 40% of all hospital procedures reported in the discharge report leading to a market volume at the size of $56 billion which is 0.5% of the GDP (in 2004) <ref type="bibr" target="#b92">[Krupinski, 2010]</ref>.</p><p>Today, many different medical image acquisition devices and protocols are used in healthcare. An overview of these systems and the physics behind is given in <ref type="bibr" target="#b179">[Webb, 2003]</ref> and more recently in <ref type="bibr" target="#b114">[Maier, 2018]</ref>.</p><p>With the rise of medical imaging systems comes the need for maximizing the insights clinicians can obtain from images. Automatic computational image analysis has a high value for diagnosis, prognosis and therapy as it can extract information in a fast and objective fashion that cannot be measured directly. This not only overcomes the problem of a high inter-rater variability but also allows the processing and comparison of a large amount of data which would be too time-consuming to be done manually. Thus, the last decades have seen large advances and progress in the automatic analysis of medical images using computer vision techniques.</p><p>One major need is to deal with multiple images of the same or overlapping body regions. Deformable registration and motion analysis tools aim at finding corresponding locations in images to define the mapping from one to the other image(s). This mapping describes the image deformations and is essential for the comparison, integration or fusion of medical images which can support diagnosis, prognosis and therapy of various diseases.</p><p>In general, multiple medical images are acquired to get more accurate information for a better understanding and examination of the human body. The images can be taken from various fields of view from the same image modality (mono-modal). For example in x-ray imaging, multiple images help to not oversee structures and abnormalities that are invisible in projections from certain directions. Sometimes, images are acquired from different modalities (multi-modal) to benefit from the advantages of each imaging principles, such as ultrasound and magnetic resonance images (MRI) of the same organ (e.g. prostate <ref type="bibr" target="#b136">[Puech, 2013;</ref><ref type="bibr" target="#b117">Marks, 2013]</ref>). Another example is fusing anatomical and functional features provided by Computer Tomography (CT) scans and Positron Emission Tomography (PET) scans respectively.</p><p>During a surgery, images need to be registered to other images taken before the surgery from the same or a different modality in order to guide the surgeons. In another example of registration, one would like to compare images of a patient with a reference image with known information (such as structure boundaries, anatomical landmarks or disease) or with images from a population of patients suffering from the same disease for prognostic or therapeutic reasons. This type of registration is known as inter-subject registration. Fusion of mono-or multi-modal and on the other hand of intra-or inter-subject information is required in numerous clinical applications such as in the investigation of organ function and pathologies. The medium of such analysis tasks are typically image deformations in the regions around the organ of interest or pathology. Using two images to extract such deformations is known as pairwise registration. Having multiple images to register is referred to as motion or group-wise registration.</p><p>Sequences of images that are acquired to track structural changes over time are of particular clinical interest to study. In longitudinal studies, for example, registered images acquired over longer time intervals allow to measure disease progression (e.g. neurodegenerative diseases) or tumor growth. Sequences of images are also acquired to analyze the motion of moving organs or to compensate for motion that introduces artifacts such as respiratory motion. One organ of particular interest for studying motion is the heart <ref type="bibr" target="#b191">[Zerhouni, 1988]</ref> as cardiovascular diseases are one of the most common disease groups around the world. An impaired heart function such as in heart failure (HF) patients can cause large implications and even lead to the death of a patient. Motion analysis can be very useful in HF as for example certain heart motion features (e.g. ventricular ejection fraction) that are computed manually from images are able to predict outcomes such as sudden cardiac death (SCD) <ref type="bibr" target="#b5">[Adabag, 2012]</ref>.</p><p>In conclusion, all these applications of deformable registration are examples where the integration or fusion of two or more images is an essential task for improving one or many of the four stages of the clinical routine: diagnosis, prognosis, therapy planning and therapy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Objectives and Organization of the Thesis</head><p>In this thesis, we present tools based on artificial intelligence for the study of deformations between two images and motion from a time series of images. These tools allow for robust image registration but also aim to improve the estimation of motion indices (such as ejection fraction or cardiac strain). This can help to directly guide the diagnosis, prognosis or therapy of diseases, not only but especially for dynamic organs. Given the context above, we first focus on the development of a robust registration tool for difficult registration scenarios by learning from existing examples of deformations.</p><p>Then, we study probabilistic deformation and motion models derived from a large database of images which capture population-specific representations of those deformation characteristics. The interest for such learned models is multiple as they allow to quantify, simulate and compare deformation and motion patterns of different patients. For example, this could support diagnosis by detecting similar patients with known diagnosis. Furthermore, predicting the disease progression in a patient could be used for therapy planning such as for survival risk prediction for cardiac diseases. In particular, we investigate the following research questions in the remaining chapters of this thesis:</p><p>• In many registration problems such as for inter-subject registration, a large variability in appearance and large deformations increase the difficulty for successful registration. Can we learn a robust registration algorithm from given examples that explores the solution space in small steps by trial-and-error to register images more accurately? (Chapter 3)</p><p>• Often, deformable registration is used for subsequent analysis tasks supporting diagnosis and prognosis. Can we learn a deformation model from images that inherently contains knowledge of physiological deformation patterns allowing for analysis tasks such as disease classification or simulation? (Chapter 4)</p><p>• Beyond pairwise registration, can we obtain a probabilistic motion model which is useful for consistent tracking of structures and motion simulation? Can we use the model to reconstruct motion from missing data? (Chapter 5)</p><p>• Having a compact motion model learned from images without supervision, does it capture discriminative factors that are useful for predicting disease outcomes? For example, can we predict the survival risk of heart failure patients? (Chapter 6)</p><p>The thesis is organized in the following way in accordance with the mentioned research questions:</p><p>In Chapter 2, the technical background of this thesis is discussed. We introduce a state-of-the-art of registration and motion methods including recent deep learning based approaches for deformable registration.</p><p>In Chapter 3, we investigate how a decision-making agent could help in difficult organspecific deformable registration problems. An artificial agent is trained to solve an intersubject registration task by exploring the parametric space of a statistical deformation model built from training data. Since it is difficult to extract trustworthy ground-truth deformation fields, we also present a training scheme with a large number of synthetically deformed image pairs requiring only a small number of real inter-subject deformations.</p><p>The proposed method has been evaluated on the difficult task of inter-subject prostate MR registration to solve motion compensation or atlas-based segmentation problems in prostate diagnosis. The method showed state-of-the-art registration accuracy in terms of structure overlaps and distance measures. The chapter was presented at MICCAI 2017, Quebec City, Canada <ref type="bibr" target="#b84">[Krebs, 2017]</ref>.</p><p>In Chapter 4, we propose to learn a low-dimensional probabilistic deformation model from data which can be used for registration and the analysis of deformations. The latent variable model maps similar deformations close to each other in an encoding space. It enables to compare deformations, generate normal or pathological deformations for any new image or to transport deformations from one image pair to any other image. Additionally, our framework is diffeomorphic and provides multi-scale velocity field estimations. We have applied our framework on cardiac intra-subject MR registration and demonstrate state-of-the-art registration accuracy, regularity and the model's potentials for disease clustering, deformation simulation and transport. The chapter is published in the journal IEEE TMI <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> and is based on the previous conference presentation at Deep Learning in Medical Image Analysis DLMIA (in conjunction with MICCAI 2018, Granada, Spain) <ref type="bibr" target="#b85">[Krebs, 2018]</ref>.</p><p>In Chapter 5, we extend our pairwise deformation model to a probabilistic latent motion model learned from a sequence of images for spatio-temporal registration problems. Our model encodes motion in a low-dimensional probabilistic space -the motion matrix -which enables various motion analysis tasks such as simulation and interpolation of realistic motion patterns allowing for faster data acquisition and data augmentation. Furthermore, the motion matrix allows to transport deformations from one subject to another simulating for example a pathological motion in a healthy subject without the need of inter-subject registration. The diffeomorphic motion model was analyzed by using cardiac cine-MRI showing state-of-the-art registration regularity and accuracy. Furthermore, motion simulation and interpolation are demonstrated. The chapter is based on the previous conference presentation at Statistical Atlases and Computational Models of the Heart STACOM (in conjunction with MICCAI 2019, Shenzhen, China) <ref type="bibr">[Krebs, 2020c]</ref> and has been submitted to IEEE TMI <ref type="bibr">[Krebs, 2020b]</ref>.</p><p>In Chapter 6, we present a learning-based method for personalized risk and survival prediction based on our motion model. We use the 4 chamber-view cine-MRI of a patient cohort suffering from heart failure to build a motion fingerprint, the motion matrix. We demonstrate the discriminative power of this compact representation by predicting risk scores from the fingerprint for disease outcomes. We show that such an imagederived risk score is a more predictive feature for HF endpoints such as hospitalization and sudden cardiac death than any relevant clinical factors. Based on the preliminary material presented in this chapter, a clinical journal submission is in preparation.</p><p>In Chapter 7, the main contributions of this thesis are summarized. Finally, potential future work and perspectives are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Publications and Awards</head><p>The described contributions led to the following peer-reviewed publications, patent applications and awards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal Articles</head><p>• <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> J. Krebs, H. Delingette, B. Mailhé, N. Ayache, and T. Mansi.</p><p>Learning a probabilistic model for diffeomorphic registration. In <ref type="bibr">IEEE Transactions on Medical Imaging, 38.9, 2019</ref><ref type="bibr">, pp. 2165</ref><ref type="bibr">-2176.</ref> (Selected as featured article on the front-page of ieee-tmi. org )</p><p>• <ref type="bibr">[Krebs, 2020b]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction</head><p>As shown before, the integration or fusion of medical images is essential for many diagnostic and interventional tasks. Therefore, research groups have been investigating deformable registration and motion modeling in great detail over the past 30 years. A tremendous number of methods and innovations have been proposed since then. However, the task of non-rigid registration is still mostly considered as an unsolved problem <ref type="bibr">[ElGamal, 2016]</ref>. Classifications and reviews of traditional deformable registration algorithms can be found in <ref type="bibr" target="#b122">[Modersitzki, 2004;</ref><ref type="bibr" target="#b162">Sotiras, 2013;</ref><ref type="bibr" target="#b129">Oliveira, 2014;</ref><ref type="bibr">ElGamal, 2016]</ref>. Recently, over the past 3-4 years many deep learning-based (DL) approaches have been proposed for image registration. Specific review papers aim to summarize the contributions in this new group of algorithms <ref type="bibr" target="#b64">[Haskins, 2020;</ref><ref type="bibr" target="#b55">Fu, 2019;</ref><ref type="bibr" target="#b24">Boveiri, 2020]</ref>.</p><p>In their recent paper, Boveiri et al. <ref type="bibr" target="#b24">[Boveiri, 2020]</ref> counted 80 contributions in DL-based image registration, combining rigid and non-rigid registration. In the remainder of this chapter, we aim to summarize and draw connections between both, traditional and DL-based registration. First, the general methodology for registration and motion modeling algorithms is introduced before we focus on the state-of-the-art of DL-based image registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Registration Algorithms</head><p>Registration is referred to as finding the spatial correspondences between two images where one is the moving M and the other the fixed image F . In order to be registered to F , the moving image M is deformed by applying a spatial transformation, the deformation field φ: M • φ where • denotes the warping functionality. The deformation field is defined by the sum of identity transform and displacement vector field u: φ(x) = x + u(x), x ∈ Ω for every position x in the image domain Ω. The registration process is illustrated in Fig. <ref type="figure" target="#fig_5">2</ref>.1. Typically, an optimization problem is solved in order to find the optimal deformation field φ ∈ T within a set of possible transformations T which best aligns M to the fixed image F . Traditionally, one seeks to minimize an objective function of the following form:</p><formula xml:id="formula_0">arg min φ∈T D(F, M • φ) + R(φ), (2.1)</formula><p>where D is a dissimilarity (or similarity) metric which measures how well the fixed and the deformed moving image are aligned and R denotes a regularizer enforcing pre-defined transformation properties such as the desired level of transformation smoothness. Due to the ill-posed nature of the high-dimensional registration problem, the deformation field φ needs to be regularized in order to obtain plausible transformations <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Many different metrics have been proposed for both terms as shown below. Most image registration algorithms consist of three parts: a deformation model determining the set of allowed transformations T , an objective function with suitable dissimilarity D and regularization R metrics and an appropriate optimization strategy to find its minimum <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. The choice for these elements is highly dependent on the registration problem to be solved. Some deformation models, dissimilarity and regularization metrics might be better suited for mono-modal than for multi-modal registration. On the other hand, intra-subject registration may require different models than inter-subject problems. Typically, the optimization problem is solved by iterative gradient descent, derivative-free optimizers or by statistical, machine-learning based strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Similarity Metrics</head><p>One can distinguish 2 main types of dissimilarity metrics. The first type, geometric methods, are based on the matching of corresponding features such as landmarks placed at anatomical meaningful locations. The difficulty hereby lies in the robust detection of landmarks. One way to automatically obtain landmarks is the SIFT algorithm and its variants <ref type="bibr" target="#b74">[Juan, 2007]</ref>. Because of the need for extrapolating the deformation field between sparse landmarks and therefore resulting in a decrease in accuracy, landmarkbased similarity metrics have lost popularity <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. However, with the rise of DL-based algorithms, they have gained popularity again due to the fact that in learning- The second type of dissimilarity metrics relies on intensity-based quantities such as sum-of-squared or absolute differences (SSD or SAD), cross-correlation (CC) or mutual information (MI). The choice depends on the assumed relation between the signal intensities. In mono-modal registration for example, the noise assumption and the assumed correspondence between intensities dictate the choice. SSD assumes Gaussian noise while CC assumes a linear relation between intensities. In multi-modal registration, these metrics would not be a good choice as the same structures may have very different intensities in images from different modalities. That is why information theoretic approaches have been proposed for multi-modal registration. The most popular metric is MI <ref type="bibr" target="#b175">[Viola, 1997;</ref><ref type="bibr" target="#b111">Maes, 1997]</ref> as it assumes a non-parametric statistical relationship between image intensities. However, its generality can turn into drawbacks that have been tried to tackle in numerous works as discussed in Sotiras et al. <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Besides SSD as one of the most commonly used similarity metrics for mono-modal registration, local cross-correlation (LCC) has been applied successfully due to its implicit estimation of the local affine scaling parameters as a good trade-off between SSD and MI <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>. In this work, LCC is defined as:</p><formula xml:id="formula_1">D LCC (F, M • φ) = Ω F (M • φ) F 2 • (M • φ) 2 (2.2)</formula><p>with the local mean images F obtained from a mean filter with kernel size k.</p><p>Besides geometric and intensity-based dissimilarity metrics, many approaches build on hybrid models that combine both criteria <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Most algorithms in the group of weakly supervised DL-based methods fall into this hybrid category and are discussed in 2.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Regularization and Deformation Models</head><p>The choices for a suitable deformation model and regularization metric determine the degrees of freedom (DoF) and complexity of the estimated deformation. The deformation model can limit the set of possible transformations T θ by parameterizing the transformations with parameters θ. These parameterizations can take very different forms and can range from a very small number of parameters (DoF), forming simple or very restricted deformation models to high-dimensional models including thousands or millions of parameters θ. Often no parameterization is used and the space of all dense deformation fields belongs to T . However, the more DoF a deformation model has the more the computational complexity rises and the need for a suitable regularization metric becomes necessary to obtain a well-posed problem <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolation-based Models</head><p>The number of parameters θ can be as small as 6 in the case of 3D rigid registration (3 rotation and 3 translation parameters). Affine registration adds another 3 scaling parameters. But in the case of deformable registration, the dimensionality of θ rises typically to thousands or millions. To keep the number reasonably low and constrain computational complexity, interpolation-based transformation models are commonly used. These models are for example based on radial basis functions <ref type="bibr">[Yang, 2011b;</ref><ref type="bibr" target="#b23">Bookstein, 1991]</ref>, elastic-body splines <ref type="bibr" target="#b41">[Davis, 1997]</ref> or, most commonly, free-from deformations (FFD, <ref type="bibr" target="#b150">[Rueckert, 1999;</ref><ref type="bibr" target="#b155">Schnabel, 2001;</ref><ref type="bibr" target="#b177">Wang, 2007]</ref>) where only displacements of sparse control points need to be predicted while the dense deformation field is obtained using interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physically-inspired Models</head><p>In contrast to interpolation-based methods, many methods are derived from physical models <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. In most of these models, the deformation model allows to estimate the full number of possible parameters determined by all values in the deformation field. However, depending on the underlying physical assumptions on how the image is allowed to deform, the estimated deformations are regularized. Typically, physical models are elastic- <ref type="bibr" target="#b39">[Davatzikos, 1997;</ref><ref type="bibr" target="#b133">Pennec, 2005]</ref>, fluid- <ref type="bibr" target="#b30">[Christensen, 1996]</ref> or diffusion-based <ref type="bibr" target="#b167">[Thirion, 1998;</ref><ref type="bibr" target="#b53">Fischer, 2002;</ref><ref type="bibr">Vercauteren, 2007a]</ref>. Diffusion-based models are based on the fact that the Gaussian kernel is the Green's function of the diffusion equation. Under this assumption, non-parametric registration regularization can be efficiently applied using Gaussian filtering of the deformation field <ref type="bibr" target="#b167">[Thirion, 1998]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Deformation Models</head><p>Another category of deformation models consists of statistically-constrained models <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Statistical deformation models (SDM) have the power to reduce the dimensionality of deformations tremendously allowing for a simpler subsequent deformation analysis. However, a statistical model needs to be trained from an existing database whereby it is limited to the observations in this training set. Before the era of DL-based registration, the size of such databases were typically relatively small due to limited computational powers. A broadly applied statistical dimensionality reduction method is principal component analysis (PCA) which has been used to learn an SDM from FFDs <ref type="bibr" target="#b152">[Rueckert, 2003]</ref>. In active shape models, the shape variability is learned from annotated points by using PCA <ref type="bibr" target="#b31">[Cootes, 1995]</ref>. PCA has been also used in a generative manner by generating intermediate images through sampling along the PCA axes. By doing so, the registration process can be initialized for instance by projecting the moving image to the closest target image <ref type="bibr" target="#b164">[Tang, 2009]</ref>. Similarly, Kim et al. <ref type="bibr" target="#b76">[Kim, 2012]</ref> estimated the intermediate target image by using support vector regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffeomorphims and other Deformation Constraints</head><p>In addition, to the presented deformation models and regularization energies, constraints on the transformations have been applied to obtain special properties that are important in medical image analysis problems. Among others, such properties are for example inverse consistency, deformation symmetry, and diffeomorphisms <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Since deformation are not inverse consistent in general, symmetric algorithms enforce symmetry by optimizing the objective function either 2 times in both directions (by exchanging moving and fixed image) or by constructing symmetry in the objective function, for example by registration to the midpoint between both images (cf. e.g. <ref type="bibr" target="#b174">[Vercauteren, 2008;</ref><ref type="bibr" target="#b105">Lorenzi, 2013]</ref>). Diffeomorphisms are topology-preserving and invertible transformations which makes them suitable for many medical registration problems in which foldings are physically implausible <ref type="bibr" target="#b175">[Vercauteren, 2009]</ref>. Popular parameterizations of diffeomorphisms include the Large Deformation Diffeomorphic Metric Mapping (LD-DMM) <ref type="bibr" target="#b18">[Beg, 2005;</ref><ref type="bibr" target="#b27">Cao, 2005;</ref><ref type="bibr">Zhang, 2015]</ref>, a symmetric normalization approach <ref type="bibr" target="#b7">[Avants, 2008]</ref> or stationary velocity fields (SVF) <ref type="bibr" target="#b6">[Arsigny, 2006;</ref><ref type="bibr" target="#b175">Vercauteren, 2009;</ref><ref type="bibr" target="#b105">Lorenzi, 2013]</ref>. SVFs provide an efficient formulation of diffeomorphisms while still maintaining the desirable properties of time-varying LDDMMs. An SVF is not able to capture all possible diffeomorphisms, however, in practice, SVFs are often chosen due to their computational efficiency. SVFs are described as the exponential map of the velocity field v: φ = exp(v) which can be efficiently computed by the scaling and squaring algorithm <ref type="bibr" target="#b6">[Arsigny, 2006]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Motion: Adding the Temporal Dimension</head><p>Estimating the deformations within a sequence of images is highly related to pairwise registration -the mapping between 2 images. Consistent temporal registration is useful for tracking moving structures or organs, for motion compensation and for detecting pathological motion patterns. Traditionally, one can separate proposed approaches for motion estimation by physically-motivated or interpolation-based and biomechanically-or biophysically-inspired motion models. In principle, the former group extends interpolation-based or physically-motivated methods for pairwise registration by an additional temporal dimension t denoted as 2D+t or 3D+t registration. Most approaches are based on FFDs due to their efficiency where applications range from intra-subject motion estimation <ref type="bibr" target="#b94">[LedesmaCarbayo, 2005;</ref><ref type="bibr" target="#b171">Vandemeulebroucke, 2011;</ref><ref type="bibr" target="#b42">De Craene, 2012]</ref>, to inter-subject sequence registration <ref type="bibr" target="#b134">[Perperidis, 2005;</ref><ref type="bibr" target="#b135">Peyrat, 2010]</ref> and group-wise registration with the purpose of defining a reference frame <ref type="bibr" target="#b118">[Metz, 2011]</ref>.</p><p>Another example for a spatio-temporal physically-motivated model (besides <ref type="bibr" target="#b135">[Peyrat, 2010]</ref>) computes cardiac strain from image sequences <ref type="bibr" target="#b116">[Mansi, 2011]</ref>.</p><p>On the other hand, biophysical models are exploiting anatomical and physiological knowledge. Many models apply finite element methods (FEMs) for different organs and applications, for instance for tumor growth modeling, breast imaging or the prostate and its surrounding <ref type="bibr" target="#b21">[Bharatha, 2001]</ref>. Also, a biomechanical model was used to generate synthetic training data for learning a statistical model of the prostate <ref type="bibr" target="#b123">[Mohamed, 2002]</ref>. Electromechanical models also exist in cardiac imaging where motion analysis can help in diagnosis and therapy planning of many diseases <ref type="bibr" target="#b156">[Sermesant, 2008]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep Learning-based Registration</head><p>The main difference between classical and learning-based, especially DL-based, registration is the transition from relying only on one pair of images to exploiting a large database of image pairs. Introducing this tremendous amount of data, the optimization strategy is mostly shifted to a training phase in order to retrieve rich implicit prior knowledge that allows to register a new image pair in almost real-time. This speed-up is regarded as one of the major benefits of using DL-based registration.</p><p>In general, a neural network is a function approximator which is parameterized by a large number of parameters ω, the network weights <ref type="bibr">[Goodfellow, 2016]</ref>. Applied to image registration, the deformation field φ can be obtained by a simple evaluation of such a trained function f ω that takes the image pair (F, M ) as input: In order to select the optimal network parameters ω * , the neural network is trained with respect to an objective function -the loss function. In DL-based registration, one can differentiate 3 classes of approaches <ref type="bibr" target="#b64">[Haskins, 2020]</ref> on how to choose the loss function in order to learn the parameterized registration function f ω : Supervised, unsupervised and weakly supervised approaches. Hereby, supervision refers to the fact that extra information such as ground-truth deformation fields or labels are required during training (but typically not during testing). The different classes of approaches are discussed in the following.</p><formula xml:id="formula_2">φ = f ω (F, M ). (<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Supervised</head><p>Supervised DL-based registration approaches aim at learning a similarity metric between the two images by providing a ground-truth deformation field φ GT . In this case, the learning objective turns into a regression problem of the following form: <ref type="bibr">.4)</ref> where p(F, M, φ GT ) is the empirical data distribution of image pairs and ground-truth deformation and D describes a distance metric such as SSD or CC. The idea of regressing deformation fields directly, originates from optical flow estimations in the computer vision community, where large datasets with ground-truth flow fields exist <ref type="bibr" target="#b43">[Dosovitskiy, 2015;</ref><ref type="bibr" target="#b182">Weinzaepfel, 2013]</ref>. Supervised approaches can be further differentiated as end-to-end or non-end-to-end depending on whether the learned similarity metric is used in a classical registration algorithm or directly applied for registration.</p><formula xml:id="formula_3">ω * = arg min ω E p(F,M,φ GT ) D(f ω (F, M ), φ GT ) , (<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-End-to-End</head><p>As one of the first DL-based approaches, <ref type="bibr" target="#b184">[Wu, 2015]</ref> learned application-specific features that were used in traditional registration methods instead of manually extracted features. In a similar way, learned features were used for estimating the registration error in <ref type="bibr">[Eppenhof, 2018b]</ref>. While these approaches are learning features that still need to be matched using a distance metric such as SSD or CC, Simonovsky et al. <ref type="bibr" target="#b158">[Simonovsky, 2016]</ref> proposed to learn a similarity metric for inter-subject brain MR T1-T2 registration which showed improved results compared to MI. Wright et al. <ref type="bibr" target="#b183">[Wright, 2018]</ref> used recurrent spatial co-transformer networks to iteratively register MR and US volumes showing a better quantified image similarity than self-similarity context descriptors for multi-modal registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End</head><p>To overcome the need of slow iterative registration procedures, supervised end-to-end approaches have been proposed that mostly have near real-time performance during testing. According to Eq. 2.4, approaches in this category require registered image pairs during training. In Fig. <ref type="figure" target="#fig_5">2</ref>.2, a graphical representation of the typical supervised approach is shown. Due to the difficulty of finding dense ground truth voxel mappings, supervised methods need to rely on deformation predictions either from existing algorithms <ref type="bibr">[Yang, 2017;</ref><ref type="bibr" target="#b146">Rohé, 2017;</ref><ref type="bibr" target="#b28">Cao, 2017]</ref>, simulations <ref type="bibr" target="#b161">[Sokooti, 2017;</ref><ref type="bibr" target="#b169">Uzunova, 2017;</ref><ref type="bibr">Eppenhof, 2018a]</ref> or a combination of both <ref type="bibr" target="#b112">[Mahapatra, 2018;</ref><ref type="bibr" target="#b84">Krebs, 2017]</ref>. Instead of predicting the deformation field φ, diffeomorphic approaches predict parameterizations based on patches of the initial momentum of LDDMMs <ref type="bibr">[Yang, 2017]</ref> or dense SVFs <ref type="bibr" target="#b146">[Rohé, 2017]</ref>.</p><p>In order to reduce the complexity but therefore limiting the use for large deformations, patch-wise approaches have been proposed <ref type="bibr" target="#b28">[Cao, 2017;</ref><ref type="bibr" target="#b161">Sokooti, 2017;</ref><ref type="bibr">Yang, 2017]</ref>.</p><p>In case of simulation-based approaches, Sokooti et al. <ref type="bibr" target="#b161">[Sokooti, 2017]</ref> used random transformations based on Gaussian kernels. Random transformations limit the realism and task-specificity of deformations such that, more sophisticated simulations were used by multi-scale, random transformations of aligned image pairs <ref type="bibr">[Eppenhof, 2018a]</ref> or applying a statistical deformation model for data augmentation <ref type="bibr" target="#b169">[Uzunova, 2017;</ref><ref type="bibr" target="#b84">Krebs, 2017]</ref>.</p><p>Another way of optimizing Eq. 2.4 is by using deep reinforcement learning (DRL) and implicitly quantifying image similarity through an agent <ref type="bibr" target="#b64">[Haskins, 2020]</ref>. Hereby, an agent takes consecutive decisions on actions to apply based on the current state and future reward. This strategy allows to follow a trajectory towards the optimal transformation parameters while allowing to recover from mistakes. Due to limitations on the action space, most approaches have considered rigid registration only <ref type="bibr">[Liao, 2017b;</ref><ref type="bibr" target="#b109">Ma, 2017</ref>; Miao, 2018]. However, by using a low-dimensional SDM, we have shown that DRL is useful for difficult inter-subject registration tasks by showing improved registration accuracy compared to state-of-the-art algorithms (cf. <ref type="bibr">Chapter 3, [Krebs, 2017]</ref>).</p><p>Supervised methods are free from the need of having to define a similarity metric (and most-often regularizer) manually, but the lack of ground-truth deformations either limits the approaches by the performance of existing algorithms or the realism of simulations. Furthermore, retrieving deformations from existing algorithms on a large database is time-consuming and increases the training complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Unsupervised</head><p>To overcome the limitations of supervised approaches, end-to-end DL-based approaches that do not require ground-truth deformations have been considered more recently. The introduction of spatial transformer networks (STN, <ref type="bibr" target="#b70">[Jaderberg, 2015]</ref>) allowed to integrate transformation models based on B-splines or linear interpolation for dense deformation fields in neural networks directly for an efficient and most importantly differentiable warping of the moving image. With a differentiable warping functionality, loss functions can be applied on the warped moving image, allowing to integrate the classical objective function for registration (Eq. 2.1) in the loss function of neural networks (cf. Fig. <ref type="figure" target="#fig_5">2</ref>.3): <ref type="bibr">.5)</ref> where p(F, M ) is the empirical data distribution of image pairs. The difference to classical registration is that the optimization is done over many training image pairs instead of one test image pair (F, M ). Similar learning approaches first appeared in the computer vision community <ref type="bibr" target="#b71">[Jason, 2016;</ref><ref type="bibr" target="#b99">Liang, 2017]</ref> and were recently applied to medical image registration <ref type="bibr">[Vos, 2017;</ref><ref type="bibr" target="#b57">Ghosal, 2017;</ref><ref type="bibr" target="#b189">Yoo, 2017;</ref><ref type="bibr">Balakrishnan, 2018;</ref><ref type="bibr" target="#b85">Krebs, 2018;</ref><ref type="bibr" target="#b35">Dalca, 2018;</ref><ref type="bibr" target="#b112">Mahapatra, 2018;</ref><ref type="bibr">Fan, 2018;</ref><ref type="bibr" target="#b165">Tanner, 2018;</ref><ref type="bibr" target="#b97">Li, 2018;</ref><ref type="bibr" target="#b87">Krebs, 2019b;</ref><ref type="bibr" target="#b176">Vos, 2019;</ref><ref type="bibr">Balakrishnan, 2019;</ref><ref type="bibr">Dalca, 2019a;</ref><ref type="bibr" target="#b154">Sandkühler, 2019]</ref>. These approaches cover dense or B-spline <ref type="bibr">[Vos, 2017;</ref><ref type="bibr" target="#b176">Vos, 2019]</ref> deformation models, diffeomorphic models <ref type="bibr" target="#b85">[Krebs, 2018;</ref><ref type="bibr" target="#b35">Dalca, 2018;</ref><ref type="bibr" target="#b87">Krebs, 2019b;</ref><ref type="bibr">Dalca, 2019a]</ref>, single or multi-scale models <ref type="bibr" target="#b176">[Vos, 2019;</ref><ref type="bibr" target="#b87">Krebs, 2019b]</ref>. Common similarity and regularization metrics as in classical methods are applied (cf. 2.2.1-2.2.2).</p><formula xml:id="formula_4">ω * = arg min ω E p(F,M ) D(F, M • f ω (F, M )) + R(f ω (F, M )) , (<label>2</label></formula><p>In an iterative fashion using recurrent networks, Sandkuhler et al. <ref type="bibr" target="#b154">[Sandkühler, 2019]</ref> obtained a more compact representation and a speedup of 15 compared to B-spline registration for 2D images. Dropping the need for choosing a pre-defined regularizer, Niethammer et al. proposed to learn a spatially adaptive regularizer using multi-Gaussian kernels <ref type="bibr" target="#b127">[Niethammer, 2019]</ref>.</p><p>In a different optimization scheme, adversarial approaches based on generative adversarial networks (GAN, <ref type="bibr" target="#b60">[Goodfellow, 2014]</ref>) were used for the difficult case of multi-modal registration <ref type="bibr" target="#b112">[Mahapatra, 2018;</ref><ref type="bibr">Fan, 2018;</ref><ref type="bibr" target="#b165">Tanner, 2018]</ref>. Besides, probabilistic approaches were proposed in <ref type="bibr" target="#b85">[Krebs, 2018;</ref><ref type="bibr" target="#b35">Dalca, 2018;</ref><ref type="bibr" target="#b87">Krebs, 2019b;</ref><ref type="bibr">Dalca, 2019a]</ref>. In our works <ref type="bibr" target="#b85">[Krebs, 2018;</ref><ref type="bibr" target="#b87">Krebs, 2019b]</ref>, deformations are encoded in a low-dimensional structured space, similar to an SDM, which allows for a variety of analysis tasks as particularly discussed in the later chapters of this thesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Weakly Supervised</head><p>All methods that use the unsupervised objective (Eq. 2.5) and additionally make use of some extra information during training such as labels or few ground-truth deformation fields fall in the category of weakly supervised algorithms.</p><p>In the latter case, Fan et al. <ref type="bibr" target="#b51">[Fan, 2019]</ref> combined supervised and unsupervised objective functions with dynamically changing weights between both, first focusing on learning from supervised deformation fields and later increasing the weight for the objective of Eq. 2.5 for fine-tuning.</p><p>On the other side, using the matching of extra labels such as landmarks or segmentation masks, has become popular in very recent approaches due to the fact that such anatomical guidance can improve registration performance in contrast to only intensity-based metrics. Furthermore, an advantage of DL-based approaches is unlike classical methods which are based on geometric similarity metrics, that such extra information are only necessary at the training stage, while test cases do not require labels. Following this principle, Hering et al. <ref type="bibr" target="#b65">[Hering, 2019]</ref> introduced a label and similarity metric based loss function for deformable registration of 2D cine-MR images. Hu et al. <ref type="bibr" target="#b67">[Hu, 2018]</ref> proposed to only optimize the matching of labels based on a multi-scale DICE loss and a deformation regularization duplicating the objectives of classical geometric approaches.</p><p>More recently, it has been proposed to learn a structure-enhanced representation from segmentations for helping with the registration of hard to register structures <ref type="bibr" target="#b96">[Lee, 2019]</ref>.</p><p>The assumption that segmentation and registration can facilitate each other has let to approaches predicting both by combining the unsupervised registration objective Eq. 2.5 and a segmentation loss <ref type="bibr" target="#b138">[Qin, 2018;</ref><ref type="bibr" target="#b98">Li, 2019]</ref>. The latter approach has been successfully applied on cardiac cine-MR sequences and showed solving registration and segmentation in a joint fashion helps to improve both tasks. In the previous chapter, we showed a state-of-the-art of deformable registration. This chapter focuses on inter-subject registration tasks which are difficult to solve using traditional methods because of the high variability in appearance and large deformations.</p><p>We try to overcome these difficulties by learning from known deformations and applying a decision-making process in order to optimize the parameters of a learned statistical deformation model. This approach can be classified as a supervised DL-based registration algorithm as it relies on simulated and ground-truth deformations. This chapter has been presented at the MICCAI 2017 conference <ref type="bibr" target="#b84">[Krebs, 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Registration of images with focus on the ROI is essential in fusion and atlas-based segmentation (e.g. <ref type="bibr" target="#b168">[Tian, 2015]</ref>). Traditional algorithms try to compute the dense mapping between two images by minimizing an objective function with regard to some similarity criterion. However, besides challenges of solving the ill-posed and non-convex problem many approaches have difficulties in handling large deformations or large variability in appearance. Recently, promising results using deep representation learning have been presented for learning similarity metrics <ref type="bibr" target="#b158">[Simonovsky, 2016]</ref>, predicting the optical flow <ref type="bibr" target="#b43">[Dosovitskiy, 2015]</ref> or the large deformation diffeomorphic metric mapping-momentum <ref type="bibr" target="#b187">[Yang, 2016]</ref>. These approaches either only partially remove the above-mentioned limitations as they stick to an energy minimization framework (cf. <ref type="bibr" target="#b158">[Simonovsky, 2016]</ref>) or rely on a large number of training samples derived from existing registration results (cf. <ref type="bibr" target="#b43">[Dosovitskiy, 2015;</ref><ref type="bibr" target="#b187">Yang, 2016]</ref>).</p><p>Inspired by the recent works in reinforcement learning <ref type="bibr" target="#b120">[Mnih, 2015;</ref><ref type="bibr" target="#b56">Ghesu, 2016]</ref>, we propose a reformulation of the non-rigid registration problem following a similar methodology as in 3-D rigid registration of <ref type="bibr">[Liao, 2017b]</ref>: in order to optimize the parameters of a deformation model we apply an artificial agent -solely learned from experience -that does not require explicitly designed similarity measures, regularization and optimization strategy. Trained in a supervised way the agent explores the space of deformations by choosing from a set of actions that update the parameters. By iteratively selecting actions, the agent moves on a trajectory towards the final deformation parameters. To decide which action to take we present a deep dual-stream neural network for implicit image correspondence learning. This work generalizes <ref type="bibr">[Liao, 2017b]</ref>  Inspired by <ref type="bibr">[Liao, 2017b]</ref>, we propose an alternative approach to optimize θ based on an artificial agent which decides to perform a simple action a t at each iteration t consisting in applying a fixed increment δθ at : θ t+1 = θ t + δθ at . If θ is a d-dimensional vector of parameters, we define 2d possible actions a ∈ A such that δθ 2i [j] = i δ j i and δθ 2i+1 [j] =i δ j i with i ∈ {0..d -1}. In other words the application of an action a t increases or decreases a specific parameter within θ t by a fixed amount where δ j i is an additional scaling factor per dimension that is set to 1 in our experiments but could be used e.g. to allow larger magnitudes first and smaller in later iterations for fine-tuning the registration.</p><p>The difficulty in this approach lies into selecting the action a t as function of the current state s t consisting of the fixed and current moving image: s t = (F, M t ). To this end, the framework models a Markov decision process (MDP), where the agent interacts with an environment getting feedbacks for each action. In reinforcement learning (RL) the best action is selected based on the maximization of the quality function a t = arg max a∈A Q (s t , a). In the most general setting, this optimal action-value function is computed based on the reward function defined between two states R(s 1 , a, s 2 ) which serves as the feed-back signal for the agent to quantify the improvement or worsening when applying a certain action. Thus, Q (s t , a) may take into account the immediate but also future rewards starting from state s t , as to evaluate the performance of an action a.</p><p>Recently, in RL powerful deep neural networks have been presented that approximate the optimal Q <ref type="bibr" target="#b120">[Mnih, 2015]</ref>. <ref type="bibr" target="#b56">Ghesu et al. [Ghesu, 2016]</ref> used deep reinforcement learning (DRL) for landmark detection in 2-D medical images. In the rigid registration approach by Liao et al. <ref type="bibr">[Liao, 2017b</ref>] the agent's actions are defined as translation and rotation movements of the moving image in order to match the fixed image.</p><p>In this work, the quality function y a (s t ) ≈ Q (s t , a) is learned in a supervised manner through a deep regression network. More precisely, we adopt a single-stage MDP for which Q (s t , a) = R(s t , a, s t+1 ), implying that only the immediate reward, i.e. the next best action, is accounted for. During training, a batch of random states, pairs of F and M, is considered with known transformation T θ GT (with F ≈ M • T θ GT ). The target quality is defined such that actions that bring the parameters closer to its ground truth value are rewarded:</p><formula xml:id="formula_5">Q (s t , a) = R(s t , a, s t+1 ) = θ GT -θ st 2 -θ GT -θ a s t+1 2 . (3.1)</formula><p>The training loss function consists of the sum of L 2 -norms between the explicitly computed Q-values (Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Image</head><p>Intra-Subj.</p><p>Inter-Subj. y a (s t ) per action. Having a training batch B with random states s b the loss is defined as:</p><formula xml:id="formula_6">L = s b ∈B a∈A y a (s b ) -Q (s b , a) 2 .</formula><p>In testing, the agent iteratively selects the best action, updates the parameter θ t and warps the moving image M t as to converge to a final parameter set representing the best mapping from moving to fixed image (see Fig. <ref type="figure" target="#fig_4">3</ref>.1b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Statistical Deformation Model</head><p>One challenge of the proposed framework is to find a low dimensional representation of non-rigid transformations to minimize the number of possible actions (equal to 2d), while keeping enough degrees of freedom to correctly match images. In this work, we base our registration method on statistical deformation models (SDM) defined from Free Form Deformations (FFD). Other parametrizations could work as well. Typically, the dense displacement field is defined as the summation of tensor products of cubic B-splines on a rectangular grid. Rueckert et al. <ref type="bibr" target="#b152">[Rueckert, 2003]</ref> proposed to further reduce the dimensionality by constructing an SDM through a principal component analysis (PCA) on the B-spline displacements.</p><p>We propose to use the modes of the PCA as the parameter vector θ describing the transformation T θ that the agent aims to optimize. The agent's basic increment per action i is normalized according to the mean value of each mode estimated in training. To have a stochastic exploration of the parameter space, predicted actions a t are selected in a stochastic manner among the 3 best actions with given fixed probabilities (see <ref type="bibr">[Liao, 2017b]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuzzy Action Control</head><p>Since parameters θ are the amplitudes of principal components, the deviation of θ 2m and θ 2m+1 from the mean µ m should stay within k-times the standard deviation σ m in testing. In order to keep θ inside this reasonable parametric space of the SDM, we propose fuzzy action controlling. Thus, actions that push parameter values of θ outside that space, are stochastically penalized -after being predicted by the network. Inspired by rejection sampling, if an action a moves parameter θ m to a value f m , then this move is accepted if a random number generated between [0, 1] is less than the ratio</p><formula xml:id="formula_7">N (f m ; µ m , σ m )/N (h; µ m , σ m ) where h m = µ m + kσ m , and N is the Gaussian distribution function. Therefore, if |f m -µ m | ≤ kσ m ,</formula><p>the ratio is greater than 1 and the action is accepted. If |f m -µ m | &gt; kσ m then the action is randomly accepted, but with a decreased likelihood as f m moves far away from µ m . This stochastic thresholding is performed for all actions at each iteration and rejection is translated into adding a large negative value to the quality function y a . The factor k controls the tightness of the parametric space and is empirically chosen as 1.5. By introducing fuzzy action control, the MDP gets more robust since the agent's access to the less known subspace of the SDM is restricted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training Data Generation</head><p>Since it is difficult to get trustworthy ground-truth (GT) deformation parameters θ GT for training, we propose to generate two different kinds of training pairs: Inter-and intra-subject pairs where in both moving and fixed images are synthetically deformed. The latter pairs serve as a data augmentation method to improve the generalization of the neural network.</p><p>In order to produce the ground truth deformations of the available training images, one possibility would be to apply existing registration algorithms with optimally tuned parameters. However, this would imply that the trained artificial agent would only be as good as those already available algorithms. Instead, we make use of manually segmented regions of interest (ROI) available for both pairs of images. By constraining the registration algorithms to enforce the correspondence between the 2 ROIs (for instance by artificially outlining the ROIs in images as brighter voxels or using point correspondences in the ROI), the estimated registration improves significantly around the ROI. From the resulting deformations represented on an FFD grid, the d principal components are extracted. Finally, these modes are used to generate the synthetic training samples by warping the original training images based on randomly drawn deformation samples according to the SDM. Amplitudes of the modes are bounded to not exceed the variations experienced in the real image pairs, similar to <ref type="bibr" target="#b152">[Rueckert, 2003]</ref>.</p><p>Intra-subject training pairs can be all combinations of synthetically deformed images of the same subject. Since the ground-truth deformation parameters are exactly known, it is guaranteed that the agent learns correct deformations. In the case of inter-patient pairs a synthetic deformed image i mb of one subject I m is allowed to be paired with any synthetic deformed image i nc of any other subject I n with b, c denoting random synthetic deformations (see <ref type="bibr">Fig. 3.1a)</ref>. Thereby, the GT parameters θ GT for image pair (i mb , i nc ) are extracted via composition of the different known deformations such that</p><formula xml:id="formula_8">((i mb • T i mb ,Im θ ) • T Im,In θ ) • T In,inc θ</formula><p>. Note the first deformation would require the inverse of a known deformation that we approximate by its opposite parameters for reasons of computational efficiency. The additional error due to this approximation, computed on a few pairs, remained below 2% in terms of the DICE score.</p><p>Mini-batches are created online -during training -via random image pairing where intra-and inter-subject pairs are selected with the same probabilities. Through online random pairing the experience of new pairs is enforced since the number of possible image combinations can be extremely high (e.g. 10 12 ) depending on the number of synthetic deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head><p>We focused on organ-centered registration of MR prostate images in 2-D and 3-D with the use case of image fusion and atlas-based segmentation <ref type="bibr" target="#b168">[Tian, 2015]</ref>. The task is very challenging since texture and anatomical appearance can vary a lot. 25 volumes were selected from the MICCAI challenge PROMISE12<ref type="foot" target="#foot_5">foot_5</ref> and 16 from the Prostate-3T database<ref type="foot" target="#foot_6">foot_6</ref> including prostate segmentations. Same images and the cases with rectal probes were excluded. Randomly 8 cases were chosen for testing (56 pairs), 33 for training. As preprocessing, translation-based registration for all pairs was carried out in 3-D using the elastix-framework <ref type="bibr" target="#b83">[Klein, 2010]</ref> with standard parameters followed by cropping and down sampling the images (to 100x100/75x75x20 pixels in 2-D/3-D respectively). For the 2-D experiments, the middle slice of each volume was taken. For the purpose of GT generation mutual information as similarity metric and a bending energy metric was used. The optimization function was further constrained by a Euclidean point correspondence metric. Therefore, equally distributed points were extracted from the given mask surfaces. elastix was used to retrieve the solution with the weights 1, 3 and 0.2 for the abovementioned metrics and a B-spline spacing of 16x16(x8) voxels. As a surrogate measure  For testing, the initial translation registration was done with elastix by registering each of the test images to an arbitrarily chosen template from the training base. Table <ref type="table">3</ref>.1 shows that our method reaches a median DICE coefficient of .88/.76 in 2-D/3-D and therefore shows similar performance as in <ref type="bibr" target="#b83">[Klein, 2010]</ref> with the best reported median DICE of .76</p><p>Tab. 3.1: Results of prostate MR registration on the 56 testing pairs. 2-D and 3-D results in comparison to elastix with B-spline spacing of 8 (e8) or 16 (e16) as proposed in <ref type="bibr" target="#b83">[Klein, 2010]</ref> and the LCC-Demons <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>  on a different data set. However, on our challenging test data our method outperformed the LCC-Demons <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref> algorithm with manually tuned parameters and elastix, using similar parameters as proposed for prostate registration <ref type="bibr" target="#b83">[Klein, 2010]</ref>  Regarding the results of elastix and LCC-Demons, a rising DICE score was observed while HD increased due to local spikes introduced in the masks (visible in Fig. <ref type="figure" target="#fig_4">3</ref>.2b) as we focused on the DICE scores during optimization for fair comparisons. In the 3-D* setting, DICE scores and HDs improved when applying fuzzy action control compared to not applying any constraints (see Table <ref type="table">3</ref>.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>In this work, we presented a generic learning-based framework using an artificial agent for approaching organ-focused non-rigid registration tasks appearing in image fusion and atlas-based segmentation. The proposed method overcomes limitations of traditional algorithms by learning optimal features for decision-making. Therefore, segmentation or handcrafted features are not required for the registration during testing. Additionally, we proposed a novel ground-truth generator to learn from synthetically deformed and inter-subject image pairs.</p><p>In conclusion, we evaluated our approach on inter-subject registration of prostate MR images showing first promising results in 2-D and 3-D. In future work, the deformation parametrization needs to be further evaluated. Rigid registration as in <ref type="bibr">[Liao, 2017b]</ref> could be included in the network or applied as preprocessing to improve results as shown in the experiments. Besides, the extension to multi-modal registration is desirable.  In chapter 3, we have shown the successful application of a simple learned statistical deformation model (based on PCA) for difficult registration problems. In this chapter, we focus on learning a more sophisticated statistical deformation model from data that allows deformation analysis tasks such as disease clustering and simulation. We propose to learn a generative deformation model based on a conditional variational autoencoder which can be seen as a non-linear generalization of PCA. This model is trained without requiring ground-truth deformation fields or labels and thus, can be classified as an unsupervised DL-based registration algorithm. The chapter is published in the journal IEEE TMI <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> and is based on the previous conference presentation at DLMIA 2018 <ref type="bibr" target="#b85">[Krebs, 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>Deformable image registration, the process of finding voxel correspondences in a pair of images, is an essential task in medical image analysis <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. This mappingthe deformation field -can be used for example in pre-op / post-op studies, to find the same structures in images from different modalities or to evaluate the progression of a disease. The analysis of geometric changes in successive images is important for instance for diagnosing cardiovascular diseases and selecting the most suited therapies. A possible approach is to register sequential images and analyze the extracted deformations for example by parallel transport <ref type="bibr" target="#b106">[Lorenzi, 2014]</ref> or by creating an adapted low-dimensional subspace <ref type="bibr" target="#b147">[Rohé, 2018]</ref>.</p><p>We propose a registration algorithm that learns a deformation model directly from training images. Inspired by recent generative latent variable models, our method learns a low-dimensional probabilistic deformation encoding in an unsupervised fashion. This latent variable space encodes similar deformations close to each other and allows the generation of synthetic deformations for a single image and the comparison and transport of deformations from one case to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Deformable Image Registration</head><p>Traditionally, deformable registration is solved by numerical optimization of a similarity metric which measures the distance between the fixed and the deformed moving image. The moving image is warped given a predefined deformation model in order to get closer to the fixed image. Unfortunately, this results in an ill-posed problem which requires further regularization based on prior assumptions <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Various regularization energies have been proposed including elastic- <ref type="bibr" target="#b39">[Davatzikos, 1997;</ref><ref type="bibr" target="#b25">Burger, 2013]</ref> or diffusion-based methods <ref type="bibr" target="#b167">[Thirion, 1998;</ref><ref type="bibr">Vercauteren, 2007b;</ref><ref type="bibr" target="#b105">Lorenzi, 2013]</ref> (cf. <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>). Diffeomorphic transforms are folding-free and invertible. The enforcement of these properties in many medical applications has led to the wide use of diffeomorphic registration algorithms. Popular parametrizations of diffeomorphisms include the Large Deformation Diffeomorphic Metric Mapping (LDDMM) <ref type="bibr" target="#b18">[Beg, 2005;</ref><ref type="bibr" target="#b27">Cao, 2005;</ref><ref type="bibr">Zhang, 2015]</ref>, a symmetric normalization approach <ref type="bibr" target="#b7">[Avants, 2008]</ref> or stationary velocity fields (SVF) <ref type="bibr" target="#b6">[Arsigny, 2006;</ref><ref type="bibr" target="#b174">Vercauteren, 2008]</ref>.</p><p>In recent years, learning-based algorithms -notably Deep Learning (DL) -have been proposed to avoid long iterative optimization at test time. In general, one can classify these algorithms as supervised and unsupervised. Due to the difficulty of finding ground truth voxel mappings, supervised methods need to rely on predictions from existing algorithms <ref type="bibr">[Yang, 2017;</ref><ref type="bibr" target="#b146">Rohé, 2017]</ref>, simulations <ref type="bibr" target="#b161">[Sokooti, 2017;</ref><ref type="bibr" target="#b169">Uzunova, 2017;</ref><ref type="bibr">Eppenhof, 2018a]</ref> or a combination of both <ref type="bibr" target="#b84">[Krebs, 2017;</ref><ref type="bibr" target="#b112">Mahapatra, 2018]</ref>. The latter can be achieved for example by projecting B-spline displacement estimations in the space of a statistical deformation model from which one can extract simulations by sampling of its components <ref type="bibr" target="#b84">[Krebs, 2017]</ref>. Diffeomorphic approaches predict patches of the initial momentum of LDDMMs <ref type="bibr">[Yang, 2017]</ref> or dense SVFs <ref type="bibr" target="#b146">[Rohé, 2017]</ref>. Supervised methods are either limited by the performance of existing algorithms or the realism of simulations.</p><p>Furthermore, retrieving deformations from existing algorithms on a large database is time-consuming and increases the training complexity.</p><p>Unsupervised approaches to registration aim to optimize an image similarity, often combined with a penalization or smoothing term (regularization). These learning approaches first appeared in the computer vision community <ref type="bibr" target="#b71">[Jason, 2016;</ref><ref type="bibr" target="#b99">Liang, 2017]</ref> and were recently applied to medical image registration <ref type="bibr">[Vos, 2017;</ref><ref type="bibr">Balakrishnan, 2018;</ref><ref type="bibr" target="#b51">Fan, 2019;</ref><ref type="bibr" target="#b35">Dalca, 2018;</ref><ref type="bibr" target="#b165">Tanner, 2018]</ref>. Unlike traditional methods, learning-based approaches also can include task-specific information such as segmentation labels during training while not requiring those labels at test time. Instead of using an image similarity, Hu et al. <ref type="bibr" target="#b67">[Hu, 2018]</ref> proposed to optimize the matching of labels based on a multi-scale DICE loss and a deformation regularization. Fan et al. <ref type="bibr" target="#b51">[Fan, 2019]</ref> proposed to jointly optimize a supervised and unsupervised objective by regressing ground-truth deformation fields (from an existing algorithm), while simultaneously optimizing an intensity-based similarity criterion. The disadvantage of these semi-supervised approaches is that their training complexity is higher since label information needs to be collected, and for example deformations outside the segmented areas are not guaranteed to be captured.</p><p>Most unsupervised approaches use B-spline grids or dense deformation fields, realized with spatial transformer layers (STN <ref type="bibr" target="#b70">[Jaderberg, 2015]</ref>) for an efficient and differentiable linear warping of the moving image. However, it has not been shown yet that these approaches lead to sufficiently regular and plausible deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Deformation Analysis and Transport</head><p>Understanding the deformation or motion of an organ goes beyond the registration of successive images. Therefore, it has been proposed to compare and characterize shape and motion patterns by normalizing deformations in a common reference frame <ref type="bibr" target="#b106">[Lorenzi, 2014;</ref><ref type="bibr">Duchateau, 2011]</ref> and for example by applying statistical methods to study the variation of cardiac shapes <ref type="bibr" target="#b12">[Bai, 2015]</ref>. In the diffeomorphic setting, various dimensionality reduction methods have been proposed. Vaillant et al. <ref type="bibr" target="#b170">[Vaillant, 2004]</ref> modeled shape variability by applying PCA in the tangent space to an atlas image. Qiu et al. used a shape prior for surface matching <ref type="bibr" target="#b140">[Qiu, 2012]</ref>. While these methods are based on probabilistic inference, dimensionality reduction is done after the estimation of diffeomorphisms. Instead Zhang et al. <ref type="bibr" target="#b193">[Zhang, 2014]</ref> introduced a latent variable model for principle geodesic analysis that estimates a template and principle modes of variation while infering the latent dimensionality from the data. Instead of having a general deformation model capable of explaining the deformations of any image pair in the training data distribution, this registration approach still depends on the estimation of a smooth template. Using the SVF parametrization for cardiac motion analysis, Rohé et al. <ref type="bibr" target="#b147">[Rohé, 2018]</ref> proposed to build affine subspaces on a manifold of deformations, the barycentric subspaces, where each point on the manifold represents a 3-D image and the geodesic between two points describes the deformation.</p><p>For uncertainty quantification, Wassermann et al. <ref type="bibr" target="#b179">[Wassermann, 2014]</ref> used a probabilistic LDDMM approach applying a stochastic differential equation and Wang et al. <ref type="bibr" target="#b178">[Wang, 2018]</ref> employed a low-dimensional Fourier representation of the tangent space of diffeomorphisms with a normal assumption. While both approaches contain probabilistic deformation representations, they have not been used for sampling and the representations have not been learned from a large dataset.</p><p>In the framework of diffeomorphic registration, parallel transport is a promising normalization method for the comparison of deformations. Currently used parallel transport approaches are the Schild's <ref type="bibr" target="#b104">[Lorenzi, 2011]</ref> or pole ladder <ref type="bibr" target="#b106">[Lorenzi, 2014;</ref><ref type="bibr" target="#b72">Jia, 2018]</ref> using the SVF parametrization or approaches based on Jacobi fields using the LDDMM parametrization <ref type="bibr" target="#b190">[Younes, 2007;</ref><ref type="bibr" target="#b108">Louis, 2017]</ref>. In general, these approaches aim to convert and apply the temporal deformation of one subject to another subject. However, this transport process typically requires multiple registrations, including difficult registrations between different subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Learning-based Generative Latent Variable Models</head><p>Alternatively and inspired by recently introduced learning-based generative models, we propose to learn a latent variable model that captures deformation characteristics just by providing a large dataset of training images. In the computer vision community, such generative models as Generative adversarial networks (GAN) <ref type="bibr" target="#b60">[Goodfellow, 2014]</ref>, stochastic variational autoencoders (VAE) <ref type="bibr" target="#b77">[Kingma, 2013]</ref> and adversarial autoencoders (AAE) <ref type="bibr" target="#b115">[Makhzani, 2016]</ref> have demonstrated great performance in learning data distributions from large image training sets. The learned models can be used to generate new synthetic images, similar to the ones seen during training. In addition, probabilistic VAEs are latent variable models which are able to learn continuous latent variables with intractable posterior probability distributions (encoder). Efficient Bayesian inference can be used to deduce the posterior distribution by enforcing the latent variables to follow a predefined (simple) distribution. Finally, a decoder aims to reconstruct the data from that representation <ref type="bibr" target="#b77">[Kingma, 2013]</ref>. As an extension, conditional variational autoencoders (CVAE) constrain the VAE model on additional information such as labels. This leads to a latent variable space in which similar data points are mapped close to each other. CVAEs are for example used for semi-supervised classification tasks <ref type="bibr">[Kingma, 2014b]</ref>.</p><p>Generative models also showed promising results in medical imaging applications such as in classifying cardiac diseases <ref type="bibr" target="#b22">[Biffi, 2018]</ref> or predicting PET-derived myelin content maps from multi-modal MRI <ref type="bibr" target="#b180">[Wei, 2018]</ref>. Recently, unsupervised adversarial training approaches have been proposed for image registration <ref type="bibr" target="#b112">[Mahapatra, 2018;</ref><ref type="bibr">Fan, 2018;</ref><ref type="bibr" target="#b165">Tanner, 2018]</ref>. <ref type="bibr" target="#b35">Dalca et al. [Dalca, 2018]</ref> developed a framework which enforces a multivariate Gaussian distribution on each component of the velocity field for measuring uncertainty. However, these approaches do not learn global latent variable models which map similar deformations close to each other in a probabilistic subspace of deformations.</p><p>To the best of the authors' knowledge, generative approaches for registration which allow the sampling of new deformations based on a learned low-dimensional encoding have not been proposed yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Probabilistic Registration using a Generative Model</head><p>We introduce a generative and probabilistic model for diffeomorphic image registration, inspired by generative latent variable models <ref type="bibr" target="#b77">[Kingma, 2013;</ref><ref type="bibr">Kingma, 2014b]</ref>. In contrast to other probabilistic approaches such as <ref type="bibr">[Yang, 2017;</ref><ref type="bibr" target="#b35">Dalca, 2018]</ref>, we learn a low-dimensional global latent space in an encoder-decoder neural network where the deformation of a new image pair is mapped to and where similar deformations are close to each other. This latent space, learned in an unsupervised fashion, can be used to generate an infinite number of new deformations for any single image from the data distribution and not only for a unique template as in the Bayesian inference procedure for model parameter estimation in <ref type="bibr" target="#b193">[Zhang, 2014]</ref>. From this abstract representation of deformations, diffeomorphic deformations are reconstructed by decoding the latent code under the constraint of the moving image. To the best of the author's knowledge, this method describes the first low-dimensional probabilistic latent variable model that can be used for deformation transport from one subject to another. Through applying a latent deformation code of one image pair on a new constraining image, deformation transport (and sampling from the latent space) is useful for instance for simulating cardiac pathologies or synthesizing a large number of pathological and healthy heart deformations for data augmentation purposes.</p><p>We use a variational inference method (a CVAE <ref type="bibr">[Kingma, 2014b]</ref>) with the objective of reconstructing the fixed image by warping the moving image. The decoder of the CVAE is conditioned on the moving image to ease the encoding task: by making appearance information easily accessible in the decoder (in the form of the moving image), the latent space is more likely to encode deformation rather than appearance information. This implicit decoupling of deformation and appearance information allows to transport deformations from one case to another by pairing a latent code with a new conditioning image. The framework provides multi-scale estimations where velocities are extracted at each scale of the decoding network. We use the SVF parametrization and diffeomorphisms are extracted using a vector field exponentiation layer, based on the scaling and squaring algorithm proposed in <ref type="bibr" target="#b6">[Arsigny, 2006]</ref>. This algorithm has been successfully applied in neural networks in our previous work <ref type="bibr" target="#b85">[Krebs, 2018]</ref> and in <ref type="bibr" target="#b35">[Dalca, 2018]</ref>. The framework contains a dense spatial transformer layer (STN) and can be trained endto-end with a choice of similarity metrics: to avoid asymmetry, we use a symmetric and normalized local cross correlation criterion. In addition, we provide a generic formulation to include regularization terms to control the deformation appearance (if required), such as diffusion regularization in form of Gaussian smoothing <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>.</p><p>During training, similarity loss terms for each scale and a loss term enforcing a prior assumption on the latent variable distribution are optimized by using the concept of deep supervision (cf. <ref type="bibr" target="#b95">[Lee, 2015]</ref>). During testing, the low-dimensional latent encoding, multi-scale estimations of velocities, deformation fields and deformed moving image are retrieved in a single forward path of the neural network.</p><p>We evaluate our framework on the registration of cardiac MRIs between end-diastole (ED) and end-systole (ES) and provide an intensive analysis on the structure of the latent code and evaluate its application for transporting encoded deformations from one case to another.</p><p>This paper extends our preliminary work <ref type="bibr" target="#b85">[Krebs, 2018]</ref> by adding:</p><p>• Detailed derivations of the probabilistic registration framework including a generic regularization model. • Deep supervision, multi-scale estimations and a normalized loss function to improve registration performances. • Analysis of size and structure of the latent variable space.</p><p>• Evaluation of the deformation transport by comparing it to a state-of-the-art algorithm <ref type="bibr" target="#b106">[Lorenzi, 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods</head><p>In image registration, the goal is to find the spatial transformation T z : R 3 → R 3 which is parametrized by a d-dimensional vector z ∈ R d . The optimal values of z are the ones which best warp the moving image M in order to match the fixed image F given the transformation T z . Both images F and M are defined in the spatial domain Ω ∈ R 3 . Typically, z is optimized by minimizing an objective function of the form:</p><formula xml:id="formula_9">arg min z F(z, M, F ) = D (F, M • T z ) + R(T z )</formula><p>, where D is a metric measuring the similarity between fixed F and warped moving image M • T z . R is a spatial regularizer <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. Recent unsupervised DL-based approaches <ref type="bibr" target="#b71">[Jason, 2016;</ref><ref type="bibr">Vos, 2017;</ref><ref type="bibr">Balakrishnan, 2018]</ref> try to learn to maximize such a similarity metric D using stochastic gradient descent methods and a spatial transformer layer (STN <ref type="bibr" target="#b70">[Jaderberg, 2015]</ref>) for warping the moving image M .</p><p>In extension, we propose to model registration by learning a probabilistic deformation parametrization vector z from a set of example image pairs (M, F ). Thereby, we constrain the low-dimensional z to follow a prior distribution p(z). In other words, our approach contains two key parts: a latent space encoding to model deformations and a decoding function that aims to reconstruct the fixed image F from this encoded transformationby warping the moving image M . In addition, this decoding function is generative as it allows to sample new deformations based on p(z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Probabilistic model for multi-scale registration</head><p>We assume a generative probabilistic distribution for registration p true (F |M ), capturing the deformation from M towards F . We aim at learning a parameterized model p θ (F |M ) with parameters θ which allows us to sample new F 's that are similar to samples from the unknown distribution p true . To estimate the posterior p θ we use a latent variable model parameterized by z. Following the methodology of a VAE <ref type="bibr" target="#b77">[Kingma, 2013]</ref>, we assume the prior p(z) to be a multivariate unit Gaussian distribution with spherical covariance I:</p><formula xml:id="formula_10">p(z) ∼ N (0, I). (4.1)</formula><p>Using multivariate Gaussians offers a closed-form differentiable solution, however, p(z) could take the form of other distributions. In this work, we parameterize deformation fields φ by stationary velocity fields (SVF), denoted by velocities v: φ = exp(v) <ref type="bibr" target="#b6">[Arsigny, 2006]</ref>. These transformation maps φ are given as the sum of identity and displacements u(x) for every position x ∈ Ω: φ(x) = x + u(x). In the multi-scale approach, we define velocities v s at scale s ∈ S where S is the set of different image scales (s = 1 describes the original scale for which we omit writing s and s = 2, 3, ... the scale, down-sampled by a factor of 2 s-1 ). For each scale s, a family of decoding functions f s v is defined, parameterized by a fixed θ s ⊂ θ and dependent on z and the moving image M s :</p><formula xml:id="formula_11">v s = f s v (z, M s ; θ s ). (4.2)</formula><p>In the training, the goal is to optimize θ s such that all velocities v s are likely to lead to warped moving images M * s that will be similar to F s in the training database. M * s is obtained by exponentiation of v s and warping of the moving image. Using Eq. 4.2, we can define the families of functions f s :</p><formula xml:id="formula_12">M * s := f s (z, M s ; θ s ) = M s • exp(f s v (z, M s ; θ s )). (4.3)</formula><p>In order to express the dependency of f s on z and M s explicitly, we can define a distribution p(F s |z, M s ; θ s ). The product over the different scales gives us the output distribution: By using the law of total probability, this leads to the following stochastic process for computing p θ (F |M ) which is also visualized in Fig. <ref type="figure">4</ref>.1a (cf. <ref type="bibr">[Kingma, 2014b]</ref>):</p><formula xml:id="formula_13">p θ (F |z, M ) = s∈S p(F s |z, M s ; θ s ). (<label>4</label></formula><formula xml:id="formula_14">p θ (F |M ) = z p θ (F |z, M )p(z)dz.<label>(4.5)</label></formula><p>The likelihood p θ (F |z, M ) can be any distribution that is computable and continuous in θ.</p><p>In VAEs, the choice is often Gaussian, which is equivalent to adopting a sum-of-squared differences (SSD) criterion (cf. <ref type="bibr" target="#b77">[Kingma, 2013]</ref>). We propose instead to use a local cross-correlation (LCC) distribution due to its robustness properties and superior results in image registration compared to SSD (cf. <ref type="bibr" target="#b105">[Lorenzi, 2013;</ref><ref type="bibr" target="#b9">Avants, 2011]</ref>). Thus, we use the following Boltzmann distribution as likelihood:</p><formula xml:id="formula_15">p s θ (F s |z, M s ) ∼ exp(-λD LCC (F s , M s , v s )),<label>(4.6)</label></formula><p>where v s = f s v (z, M s ; θ s ) are the velocities and λ is a scalar hyperparameter. The symmetric D LCC is defined as:</p><formula xml:id="formula_16">D LCC (F s , M s , v s ) = 1 P x∈Ω i F * s x i -F * s x M * s x i -M * s x 2 i F * s x i -F * s x 2 i M * s x i -M * s x 2 + τ -1, (4.7)</formula><p>with P pixels x ∈ Ω, the symmetrically warped images</p><formula xml:id="formula_17">M * s = M s • exp (v s /2) and F * s = F s • exp (-v s /2</formula><p>). The bar F x symbolizes the local mean grey levels of F x derived by mean filtering with kernel size k at position x. i is iterating through this k × k-window.</p><p>A small constant τ is added for numerical stability (τ = 1e -15 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning the constrained deformation encoding</head><p>In order to optimize the parameterized model over θ (Eq. 4.5), two problems must be solved: First, how to define the latent variables z, for example decide what information these variables represent. VAEs assume there is no simple interpretation of the dimensions of z but instead assert that samples of z are drawn from a simple distribution p(z).</p><p>Second, the integral over z is intractable since one would need to sample a too large number of z's to get an accurate estimate of p θ (F |M ). Instead of sampling a large number of z's, the key assumption behind VAEs is to sample only z's that are likely to have produced F and compute p θ (F |M ) only from those. To this end, one needs to compute the intractable posterior p(z|F, M ). Due to this intractability, in VAEs <ref type="bibr" target="#b77">[Kingma, 2013]</ref>, the posterior is approximated by learning an encoding distribution q ω (z|F, M ), using a neural network with parameters ω (the encoder). This approximated distribution can be related to the true posterior using the Kullback-Leibler divergence (KL) which leads (after rearranging the terms) to the evidence lower bound (ELBO) of the log marginalized likelihood log p θ (F |M ) (cf. <ref type="bibr" target="#b77">[Kingma, 2013;</ref><ref type="bibr">Kingma, 2014b]</ref>):</p><formula xml:id="formula_18">log p θ (F |M ) -KL [q ω (z|F, M ) p(z|F, M )] = E z∼q log p θ (F |z, M ) -KL [q ω (z|F, M ) p(z)] . (4.8)</formula><p>The KL-divergence on the left hand side gets smaller the better q ω (z|F, M ) approximates p(z|F, M ) and ideally vanishes if q ω is of enough capacity. Thus, maximizing log p θ (F |M ) is equivalent to maximizing the ELBO on the right hand side of the equation consisting of encoder q ω and decoder p θ which can be both optimized via stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimizing the ELBO</head><p>According to the right-hand side of Eq. 4.8, there are two terms to optimize, the KLdivergence of prior p(z) and encoder distribution q ω (z|F, M ) and the expectation of the reconstruction term log p θ (F |z, M ). Since the prior is a multivariate Gaussian, the encoder distribution is defined as</p><formula xml:id="formula_19">q ω (z|F, M ) = N (z|µ ω (F, M ), Σ ω (F, M ))</formula><p>, where µ ω and Σ ω are deterministic functions learned in an encoder neural network with parameters ω. The KL-term can be computed in closed form as follows (constraining Σ ω to be diagonal):</p><formula xml:id="formula_20">KL[N (µ ω (F, M ), Σ ω (F, M )) N (0, I)] = 1 2 tr(Σ ω (F, M )) + µ ω (F, M ) -k -log det(Σ ω (F, M )) , 4.2 Methods</formula><p>where k is the dimensionality of the distribution.</p><p>The expected log-likelihood E z∼q [log p θ (F |z, M )], the reconstruction term, could be estimated by using many samples of z. To save computations, we treat p θ (F |z, M ) as</p><formula xml:id="formula_21">E z∼q [log p θ (F |z, M )</formula><p>] by only taking one sample of z. This can be justified as optimization is already done via stochastic gradient descent, where we sample many image pairs (F, M ) from the dataset X and thus witness different values for z. This can be formalized with the expectation over F, M ∼ X :</p><formula xml:id="formula_22">E F,M ∼X E z∼q log p θ (F |z, M ) -KL [q ω (z|F, M ) p(z)] .</formula><p>To enable back-propagation through the sampling operation q ω (z|F, M ), the reparametrization trick <ref type="bibr" target="#b77">[Kingma, 2013]</ref> is used in practice, where</p><formula xml:id="formula_23">z = µ ω + Σ 1/2</formula><p>ω (with ∼ N(0, I)). Thus, for image pairs (F, M ) from a training dataset X the actual objective becomes:</p><formula xml:id="formula_24">E F,M ∼X E ∼N (0,I) log p θ (F |z = µ ω (F, M ) + Σ 1/2 ω (F, M ) * , M ) - KL [q ω (z|F, M ) p(z)] . (4.9)</formula><p>After insertion of Eq. 4.4, the log of the product over the scales s ∈ S results in the sum of the log-likelihood distributions:</p><formula xml:id="formula_25">E F,M ∼X E ∼N (0,I) s∈S log p θ s (F s |z = µ ω (F, M ) + Σ 1/2 ω (F, M ) * , M s ) - KL [q ω (z|F, M ) p(z)] . (4.10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Introducing regularization on velocities</head><p>So far, we have considered that at each scale s, a velocity field v s is generated by a decoding function f s v (z, M s ; θ s ) through a neural network. To have a better control of its smoothness, we propose to regularize spatially v s through a Gaussian convolution with standard deviation σ G :</p><formula xml:id="formula_26">vs = G σ G * v s (4.11)</formula><p>Gaussian smoothing was applied here, but it could be replaced by any quadratic Tikhonov regularizer or by any functional enforcing prior knowledge about the velocity field.</p><p>In the remainder, we show how the regularization of velocities can be inserted into the proposed probabilistic framework. To make the notation less cluttered, we drop the scale  s superscript in the velocity notations. Until now, the velocities v have been handled as fixed parameters v = f v (z, M ; θ). We can equivalently assume that velocities v are random variables with a Dirac posterior probability : p θ (v|z, M ) ∼ δ fv(z,M ;θ) (v).</p><p>We now introduce the random variable v * which represents the regularized velocities as shown in Fig. <ref type="figure">4</ref>.1b. This quantity is linked to the regular velocities v through a Gaussian distribution p(v|v * ) = G(v * , 1) such that v is close to v * in terms of L 2 norm. Furthermore, we define a diffusion-like regularization prior on v * <ref type="bibr" target="#b126">[Nielsen, 1997]</ref>:</p><formula xml:id="formula_27">log p(v * ) ∝ Ω ∞ i=1 σ 2i G 2 i i! ∂ i ∂Ω i v * 2 dΩ,</formula><p>taking into account the Taylor expansion of the Fourier transform of the Gaussian. The maximum a posteriori of the regularized velocities v is then obtained through Bayes law:</p><formula xml:id="formula_28">v = arg max v * log p(v * |v) = arg max v * logp(v|v * ) + logp(v * )</formula><p>which in this case is equivalent to solving the Heat equation <ref type="bibr" target="#b126">[Nielsen, 1997]</ref> and leads to a Gaussian convolution:</p><formula xml:id="formula_29">v = G σ G * v.</formula><p>Finally, we conveniently assume that the posterior probability of v * is infinitely peaked around its mode, i.e. p(v * |v) ∼ δ v(v * ) (assumption sometimes made for the Expectation-Maximization algorithm <ref type="bibr" target="#b93">[Kurihara, 2009]</ref>). In the decoding process, we can now marginalize out the velocity variables v and v * by integrating over both such that only v remains:</p><formula xml:id="formula_30">p θ (F |M ) = z v v * p(F |v * , M ) p(v * |v) p θ (v|z, M ) p(z) dv dv * dz = z p(F |v, M ) p(z) dz.</formula><p>(4.12)</p><p>Thus, the proposed graphical model leads to a decoder working with the regularized velocity field v instead of the v generated by the neural network. When combining regularized velocities vs at all scales, we get:</p><formula xml:id="formula_31">p θ (F |M ) = z s∈S p(F s |v s , M s ) p(z) dz. (4.13)</formula><p>This can be optimized as before and leads to Gaussian convolutions at each scale if considering diffusion-like regularization. Thus, the multi-scale loss function per training image pair (F ,M ) for one sample is defined as (cf. Eq. 4.10):</p><formula xml:id="formula_32">arg min ω,θ 1 2 tr(Σ ω ) + µ ω µ ω -k -log det(Σ ω ) -λ s∈S D LCC (F s , M s , vs ),<label>(4.14)</label></formula><p>where vs depends on v s and therefore on θ (cf. Eq. 4.2 and 4.11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Network architecture</head><p>The encoder-decoder neural network takes the moving and the fixed image as input and outputs the latent code z, velocities v, the deformation field φ and the warped moving image M * . The last three are returned at the different scales s. The encoder consists of strided convolutions while the bottleneck layers (µ, σ, z) are fully-connected. The deconvolution layers in the decoder were conditioned by concatenating each layer's output with sub-sampled versions of M. Making appearance information of the moving image easily accessible for the decoder, allows the network to focus on deformation information -the differences between moving and fixed image -that need to pass through the latent bottleneck. While it is not guaranteed that the latent representation contains any appearance information, it comes at a cost to use the small bottleneck for appearance information. At each decoding scale, a convolutional layer reduces the number of filter maps to three. Then, a Gaussian smoothing layer (cf. Eq. 4.11) with variance σ 2 G is applied on these filter maps. The resulting velocities v s (a SVF) are exponentiated by the scaling and squaring layer <ref type="bibr" target="#b85">[Krebs, 2018]</ref> in order to retrieve the diffeomorphism φ s which is used by a dense STN to retrieve the warped image M * s . The latent code z is computed according to the reparametrization trick. During training, the network parameters are updated through back-propagation of the gradients with respect to the objective Eq. 4.10, defined at each multi-scale output. Finally during testing, registration is done in a single forward path where z is set to µ since we want to execute registration deterministically. One can also think of drawing several z using σ and use the different outputs for uncertainty estimation as in <ref type="bibr" target="#b35">[Dalca, 2018]</ref> which we do not further pursue in this work. The network architecture can be seen in Fig. <ref type="figure">4</ref>.2a. Besides registration, the trained probabilistic framework can be also used for the sampling of deformations as shown in Fig. <ref type="figure">4</ref>.2b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We evaluate our framework on cardiac intra-subject registration. End-diastole (ED) frames are registered to end-systole (ES) frames from cine-MRI of healthy and pathological subjects. These images show large deformations. Additionally, we evaluate the learned encoding of deformations by visualizing the latent space and transporting encoded deformations from one patient to another. All experiments are in 3-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>We used the 334 ED-ES frame pairs of short-axis cine-MRI sequences. 184 cases were acquired from different hospitals and 150 cases were used from the Automatic Cardiac Diagnosis Challenge (ACDC) at STACOM 2017 <ref type="bibr" target="#b20">[Bernard, 2018]</ref>, mixing congenital heart diseases with images from adults. We used 234 cases for training and for testing 100 cases from ACDC, that contain segmentation and disease class information. The testing set contained 20 cases of each of the following cardiac diseases: dilated cardiomyopathy (DCM), hypertrophic cardiomyopathy (HCM), previous myocardical infarction (MINF), abnormal right ventricle (RV) and healthy (Normal). All images were resampled with a spacing of 1.5 × 1.5 × 3.15 mm and cropped to a size of 128 × 128 × 32 voxels, by equally removing voxels from all sides. These dimensions were chosen to save computation time and are not a limitation of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Our neural network consisted of four encoding convolutional layers with strides (2, 2, 2, 1) and three decoding deconvolutional layers. Each scale contained two convolutional layers and a convolutional Gaussian layer with σ G = 3mm (kernel size 15) in front of an exponentiation and a spatial transformer layer using trilinear interpolation (cf. Fig. <ref type="figure">4</ref>.2a).</p><p>The dimensionality of the latent code z was set to d = 32 as a compromise of registration quality and generalizability (cf. experiment on latent vector dimensionality). The number of trainable parameters in the network was ∼420k. LeakyReLu activation functions and L2 weight decay of 1 * 10 -4 were applied on all layers except the last convolutional layer in each scale where a tanh activation function was used in order to avoid extreme velocity values during training. All scales were trained together, using linearly down-sampled versions of the input images for the coarser scales. In all experiments, the number of iterations in the exponentiation layer was set to N = 4 (evaluated on a few training samples according to the formula in <ref type="bibr" target="#b6">[Arsigny, 2006]</ref>). During the training, the mean filter size of the LCC criterion was k = 9. The loss hyper parameter was empirically chosen as λ = 5000 such that the similarity loss was optimized while the latent codes roughly had zero means and variances of one. We applied a learning rate of 1.5 * 10 -4</p><p>with the Adam optimizer and a batch size of one. For augmentation purposes, training image were randomly shifted, rotated, scaled and mirrored. The framework has been implemented in Tensorflow using Keras<ref type="foot" target="#foot_12">foot_12</ref> . Training took ∼24 hours and testing a single registration case took 0.32s on a NVIDIA GTX TITAN X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Registration</head><p>We compare our approach with the LCC-demons (Dem, <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>) and the ANTs software package using Symmetric Normalization (SyN, <ref type="bibr" target="#b7">[Avants, 2008]</ref>) with manually tuned parameters (on a few training images) and the diffeomorphic DL-based method VoxelMorph <ref type="bibr" target="#b35">[Dalca, 2018]</ref> (VM) which has been trained using the same augmentation techniques as our algorithm. For the latter, we set σ = 0.05, λ = 50000 and applied a reduced learning rate of 5 * 10 -5 for stability reasons while using more training epochs.</p><p>Higher values for λ led to worse registration accuracy. We also show the improvement of using a multi-scale approach (with 3 scales, S3) compared to a single-scale objective (S1).</p><p>We measure registration performance with the following surrogates: intensity root mean square error (RMSE), DICE score, 95%-tile Hausdorff distance (HD in mm). To quantify deformation regularity, we show the determinant of the Jacobian qualitatively, while we also computed the mean magnitude of the gradients of the determinant of the Jacobian (Grad Det-Jac). We decided to report this second-order description of deformations to better quantify differences in smoothness among the different methods, which are not obvious by taking the mean of the determinant of the Jacobian as bigger and smaller values tend to cancel each other out. DICE and HD scores were evaluated on the following anatomical structures: myocardium (LV-Myo) and epicardium (LV) of the left ventricle, left bloodpool (LV-BP), right ventricle (RV) and LV+RV (Fig. <ref type="figure">4</ref>.5).</p><p>Table <ref type="table">4</ref>.1 shows the mean results and standard deviations of all algorithms. In terms of DICE scores, our algorithm using three scales (Our S3) shows the best performances on this dataset while the single-scale version (Our S1) performed similarly compared to the LCC-demons and the SyN algorithm. Hausdorff distances were significantly improved using both of our algorithms. Detailed registration results are shown in Fig. <ref type="figure">4</ref>.3. Interestingly, we found that the SyN algorithm showed marginally better DICE scores than the LCC-demons which has been also reported on brain data <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>.</p><p>Qualitative registration results of a pathological (HCM) and a healthy case (Normal) are presented in Fig. <ref type="figure">4</ref>.4a<ref type="foot" target="#foot_13">foot_13</ref> . The warped moving image (with and wihout grid overlay) and the determinant of the Jacobian (Det. Jac.) are shown. Displacements are visualized using the color encoding as typical for the optical flow in computer vision tasks. Middle and coarse scale outputs of our multi-scale method are shown in Fig. <ref type="figure">4</ref>.4b. We computed the determinant of the Jacobian using SimpleITK<ref type="foot" target="#foot_14">foot_14</ref> and found that for all methods no negative values were observed on our test dataset. Compared to the other algorithms, our approach produced smoother and more regular deformations as qualitatively shown by the determinant of the Jacobian in Fig. <ref type="figure">4</ref>.4a and quantitatively by the significantly smaller mean gradients of the determinant of the Jacobian (Table <ref type="table">4</ref>.1) <ref type="foot" target="#foot_15">4</ref> . Despite the fact of being diffeomorphic, the voxelmorph algorithm produced more irregular deformation fields compared to all other algorithms. Our single-scale approach resulted in slightly smoother deformations which is probably due to the fact that it performed less accurately in compensating large deformations. We applied the Wilcoxon signed-rank test with p &lt; 0.001 to evaluate statistical significance of the differences in the results of Fig. <ref type="figure">4</ref>.3. This method is chosen as a paired test without the assumption of normal distributions. For all metrics, the results of our multi-scale algorithm (Our S3) showed significant differences compared to the results of all other methods (including Our S1). With respect to our single-scale algorithm (Our S1), only the differences in DICE scores were not statistically significant in comparison with the LCC-demons (Dem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LV -BP RV LV -Myo LV LV+RV</head><p>Note, that higher DICE and HD scores can be achieved by choosing a higher latent dimensionality (cf. Experiment 4.3), which however comes at the cost of a more complex encoding space, making analysis tasks more difficult. We also tested the first version of voxelmorph <ref type="bibr">[Balakrishnan, 2018]</ref> on our dataset. We chose to show the results of the latest version <ref type="bibr" target="#b35">[Dalca, 2018]</ref> due to the fact that this version is diffeomorphic and that its DICE and HD results were better (cf. <ref type="bibr" target="#b85">[Krebs, 2018]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DICE</head><p>Hausdorff Distance (mm)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformation encoding</head><p>For evaluating the learned latent space, we investigated (a) the effects of the size of the latent vector on the registration accuracy, (b) the structure of the encoded space by visualizing the distribution of cardiac diseases and showing simulated deformations along the two main axes of variations and (c) we applied our framework on deformation transport and compare its performance with a state-of-the-art algorithm.</p><p>Latent Vector Size In Fig. <ref type="figure">4</ref>.6 we analyzed the influence of the size of the latent code vector with respect to registration accuracy in terms of DICE and HD scores. With a relatively small latent size of d = 8, competitive accuracy is achieved. With an increasing dimensionality, performance increases but reaches a plateau eventually. This behavior is expected, since CVAEs tend to ignore components if the dimensionality of the latent space is too high <ref type="bibr">[Kingma, 2014b]</ref>. For the cardiac use case, we chose d = 32 components as a trade-off between accuracy and latent variable size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease Distribution and Generative Latent Space</head><p>In this experiment, we used disease information and encoded z-codes of the test images to visualize the learned latent space.</p><p>Using linear CCA (canonical correlation analysis), we projected the z-codes (32-D) to a  We compare our approach with the pole-ladder algorithm (PL <ref type="bibr" target="#b106">[Lorenzi, 2014]</ref>). All intra-and inter-subject registrations required by the pole ladder were performed using the LCC-demons <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>. For the inter-subject pairs, we aligned the test data with respect to the center of mass of the provided segmentation and rotated the images manually for rigid alignment. This alignment step was done only for the pole ladder experiment <ref type="foot" target="#foot_17">5</ref> .</p><p>Qualitative results are shown in Fig. <ref type="figure">4</ref>.9 where the predicted deformations of one hypertrophy (A, HCM) and one cardiomyopathy (B, DCM) case (step 1) were transported to two healthy (Normal) subjects (step 2, targets C and D). Note that our algorithm automatically determines orientation and location of the heart. In Table <ref type="table">4</ref>.2, we evaluated the average ejection fraction (EF) of the ED-ES deformation prediction of the pathologies (step 1) and the average EF after transport to normal subjects (step 2). Hereby, we assume that EFs, as a relative measure, stay similar after successful transport (such that the absolute difference, EF step 1 -EF step 2, is small). The table shows the average of transporting 5 HCM and 5 DCM cases to 20 normal cases (200 transports). For our algorithm, the absolute differences in EFs are much smaller for DCM cases and similarly close in HCM cases in comparison to the pole ladder. All test subjects were not used during training. The EF is computed based on the segmentation masks (warped with the resulting deformation fields). Besides, it can be seen, that predictions done by the Step </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion and Conclusions</head><p>We presented an unsupervised multi-scale deformable registration approach that learns a low-dimensional probabilistic deformation model. Our method not only allows the accurate registration of two images but also the analysis of deformations. The framework is generative, as it is able to simulate deformations given only one image. Furthermore, it provides a novel way of deformation transport from one subject to another by using its probabilistic encoding. In the latent space, similar deformations are close to each other. The method enables the addition of a regularization term which leads to arbitrarily smooth deformations that are diffeomorphic by using an exponentiation layer for stationary velocity fields. The multi-scale approach, providing velocities, deformation fields and warped images in different scales, leads to improved registration results and a more controlled training procedure compared to a single-scale approach.</p><p>We evaluated the approach on end-diastole to end-systole cardiac cine-MRI registration and compared registration performance in terms of RMSE, DICE and Hausdorff distances to two popular algorithms <ref type="bibr" target="#b105">[Lorenzi, 2013;</ref><ref type="bibr" target="#b7">Avants, 2008]</ref> and a learning-based method <ref type="bibr" target="#b35">[Dalca, 2018]</ref>, which are all diffeomorphic. While the performance of our single-scale approach was comparable to the LCC-demons and the SyN algorithm, our multi-scale approach (using 32 latent dimensions) showed statistically significant improvements in terms of registration accuracy. Generally, our approach produced more regular deformation fields, which are significantly smoother than the DL-based algorithm. Using our method with a non-generative U-net style network <ref type="bibr" target="#b148">[Ronneberger, 2015]</ref> without a deformation encoding performed similarly compared to the proposed generative model.</p><p>Adding supervised information such as segmentation masks in the training procedure as in <ref type="bibr" target="#b67">[Hu, 2018;</ref><ref type="bibr" target="#b51">Fan, 2019]</ref> led to a marginal increase in terms of registration performance (∼1-2% in DICE scores), so we decided that the performance gain is not large enough in order to justify the higher training complexity. Theoretically, our method allows measurement of registration uncertainty as proposed in <ref type="bibr" target="#b35">[Dalca, 2018]</ref> which we did not further investigate in this work.</p><p>The analysis of the deformation encoding showed that the latent space projects similar deformations close to each other such that diseases can be clustered. Disease classification could be potentially enforced in a supervised way as in <ref type="bibr" target="#b22">[Biffi, 2018]</ref>. Furthermore, our method showed comparable quantitative and qualitative results in transporting deformations with respect to a state-of-the-art algorithm which requires the difficult step of inter-subject registration that our algorithm does not need.</p><p>It is arguable if the simple assumption of a multivariate Gaussian is the right choice for the prior of the latent space (Eq. 4.1). Possible other assumptions such as a mixture of Gaussians are subject to future work. The authors think that the promising results of the learned probabilistic deformation model could be also applicable for other tasks such as evaluating disease progression in longitudinal studies or detecting abnormalities in subject-to-template registration. An open question is how the optimal size of the latent vector changes in different applications. In future work, we plan to further explore generative models for learning probabilistic deformation models.</p><p>Xavier Pennec for the insightful discussions and Adrian Dalca for the help with the Voxelmorph <ref type="bibr" target="#b35">[Dalca, 2018]</ref> experiments. In chapter 4, we presented a pairwise probabilistic deformation model and showed its applicability to a variety of deformation analysis tasks. In this chapter, we extend the model to a generative motion model that learns population-specific motion patterns from a database of image sequences. Such a motion model enables consistent tracking of structures, the simulation and temporal interpolation of motion. A temporal conditional variational autoencoder is implemented using a novel Gaussian process prior assumption. This chapter is based on the conference presentation at STACOM 2019 <ref type="bibr">[Krebs, 2020b]</ref>. However, the presented version includes several methodological advancements and is currently under review as a journal paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction</head><p>Motion analysis is an important task in many medical image analysis problems such as organ tracking or longitudinal analysis of various diseases. For moving organs such as the heart, it is not only important to track anatomical structures but also to analyze motion indices that are useful for disease diagnosis or therapy selection <ref type="bibr" target="#b58">[Girija, 2017]</ref>.</p><p>Extracting motion patterns further allows to compensate for motion, handle missing data or do temporal super-resolution and motion simulation.</p><p>Motion in medical image sequences is typically analyzed by computing temporally consistent pairwise deformations where each frame in a sequence is registered to a target frame <ref type="bibr" target="#b58">[Girija, 2017]</ref>. The resulting series of deformation fields can be utilized to track structures throughout the sequence and to identify abnormal motion patterns, for example by computing clinically relevant variables such as the ejection fraction (EF) of the heart <ref type="bibr" target="#b147">[Rohé, 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">State-of-the-art</head><p>Registration algorithms typically seek to find the deformation field between two images by solving an optimization problem consisting of a similarity metric and a regularizer. The similarity metric measures the distance between the two images while the regularizer constrains the smoothness of the resulting deformation field. A large variety of registration algorithms using different similarity and regularizing metrics have been proposed <ref type="bibr" target="#b162">[Sotiras, 2013]</ref>. One group of registration methods aim to ensure diffeomorphic deformations due to their favorable properties. Diffeomorphisms are topology-preserving and invertible deformations which makes them suitable for many medical registration problems in which foldings are physically implausible <ref type="bibr" target="#b175">[Vercauteren, 2009]</ref>. This makes diffeomporphisms also appropriate for tracking anatomical structures in image sequences such as in cardiac imaging <ref type="bibr" target="#b135">[Peyrat, 2010]</ref> (assuming structures do not go out of the field of view). Many diffeomorphic registration algorithms have been proposed such as <ref type="bibr" target="#b18">[Beg, 2005;</ref><ref type="bibr">Zhang, 2015;</ref><ref type="bibr" target="#b174">Vercauteren, 2008;</ref><ref type="bibr" target="#b175">Vercauteren, 2009]</ref>, the SyN algorithm <ref type="bibr" target="#b7">[Avants, 2008]</ref> and the LCC-demons <ref type="bibr" target="#b105">[Lorenzi, 2013]</ref>. Recently, learning-based algorithms for pairwise diffeomorphic registration have been proposed. These are based on supervised ground-truth deformations <ref type="bibr">[Yang, 2017;</ref><ref type="bibr" target="#b146">Rohé, 2017]</ref> or on unsupervised learning <ref type="bibr" target="#b35">[Dalca, 2018;</ref><ref type="bibr" target="#b87">Krebs, 2019b]</ref>. The latter are trained by minimizing a loss function consisting of an image similarity and a deformation regularizer, similarly to the traditional optimization problem. In these two works, diffeomorphisms are guaranteed by using the stationary velocity field (SVF) parameterization based on the scaling-squaring algorithm <ref type="bibr" target="#b6">[Arsigny, 2006]</ref>.</p><p>For image sequences, one difficulty is to acquire temporally smooth deformations that are fundamental for consistent tracking. That is why registration algorithms with a temporal regularizer have been proposed <ref type="bibr" target="#b94">[LedesmaCarbayo, 2005;</ref><ref type="bibr" target="#b171">Vandemeulebroucke, 2011;</ref><ref type="bibr" target="#b42">De Craene, 2012;</ref><ref type="bibr" target="#b118">Metz, 2011;</ref><ref type="bibr" target="#b138">Qin, 2018;</ref><ref type="bibr" target="#b157">Shi, 2013]</ref>. In the computer vision community, temporal video super-resolution and motion compensation are a related research topic <ref type="bibr" target="#b26">[Caballero, 2017;</ref><ref type="bibr" target="#b75">Kappeler, 2016]</ref>.</p><p>However, while these methods are able to capture temporally consistent deformations along a sequence of images, they do not extract intrinsic motion parameters crucial for building a comprehensive motion model that can be used for analysis tasks such as motion simulation, transport or classification as it is for example done in bio-mechanical models such as <ref type="bibr" target="#b156">[Sermesant, 2008]</ref>. Yang et al. <ref type="bibr">[Yang, 2011a]</ref> generated a motion prior using manifold learning from low-dimensional shapes. Qiu et al. <ref type="bibr" target="#b139">[Qiu, 2011]</ref> proposed to build an eigenspace of initial momenta using PCA. In an image-driven fashion, Rohé et al. <ref type="bibr" target="#b147">[Rohé, 2018]</ref> introduced a parameterization, the Barycentric Subspaces, for cardiac motion analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Learning a Probabilistic Motion Model</head><p>In contrast, we propose a probabilistic motion model that is built in a fully data-driven way from image sequences. Instead of defining a motion parameterization explicitly or learning from pre-processed shapes, our model learns a low-dimensional motion matrix in an unsupervised fashion. The goal is not only to retrieve a compact representation of the motion but to obtain a structured and generative encoding that allows for temporal interpolation (to predict missing frames) and to simulate an indefinite number of new motion patterns. These features could be helpful for data augmentation and to speed-up image acquisition as the model reconstructs a full cyclic motion from missing frames. Besides, the learned probabilistic encoding could be useful for group-wise analysis as it enables to transport motion characteristics to a new subject, simulating for example a pathological motion in a healthy subject.</p><p>In this work, we introduce a novel Gaussian Process (GP) prior to extend a conditional variational autoencoder (CVAE <ref type="bibr">[Kingma, 2014b]</ref>), a latent variable model, for temporal sequences. A pairwise encoder-decoder neural network applies a temporal convolutional network (TCN) in its latent space in order to learn intrinsic temporal dependencies. Furthermore, we utilize a self-supervised training scheme based on temporal dropout (TD) to enforce temporal consistency and increase generalizability of the motion model. Smooth and diffeomorphic deformations are guaranteed by applying an exponentiation layer <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> and spatio-temporal regularization.</p><p>The proposed model demonstrates state-of-the-art registration accuracy measured on segmentation overlaps and distances and regularity for diffeomorphic tracking of cardiac cine-MRI. In addition, the potentials of the generated latent motion matrix for motion simulation, interpolation and transport are demonstrated. The main contributions are as follows:</p><p>• An unsupervised probabilistic motion model learned from medical image sequences • A conditional VAE model trained with a novel Gaussian process prior and selfsupervised temporal dropout using temporal convolutional networks • Demonstration of cardiac motion tracking, simulation, transport and temporal super-resolution This paper extends our preliminary conference paper <ref type="bibr">[Krebs, 2020c]</ref> by replacing the standard unit Gaussian of the CVAE with a novel Gaussian Process Prior. We add detailed derivations of the motion model and show improved tracking accuracy and temporal smoothness. Finally, we show a first generalization of the model to 3-D+t sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Methods</head><p>Typically, the motion of an image sequence I 0:T with T frames is described by deformation fields between one reference image, for example I 0 , and all other images in the sequence.</p><p>In order to extract consistent sequential deformations φ t with t ∈ [1, T ], we propose a temporal latent variable model that encodes the motion in a low-dimensional probabilistic space, the motion matrix z ∈ R D× T with T = T -1. Here, we define the reference image I 0 as moving image, while the other frames are fixed images I t . Each image pair (I 0 , I t ) is encoded by D latent variables, the z t -code, which are the columns of z. Each z t parameterizes the deformation field φ t while being conditioned on the moving image I 0 . The rows z d with length T of the motion matrix z represent the encoded deformation sequence per latent dimension d ∈ D.</p><p>Our motion model is learned from data by imposing a Normal prior distribution p(z) on the latent variables z that follows a Gaussian Process (GP) prior in the temporal dimension for each z d . In addition, we assume independence between the latent variables z d as in standard VAEs <ref type="bibr" target="#b77">[Kingma, 2013]</ref>. Note, when z is written as part of a distribution like p(z), z is used as a vector of size D T rather than a matrix for simpler notation.</p><p>During training, we follow the learning paradigms of conditional variational autoencoders (CVAE <ref type="bibr">[Kingma, 2014b;</ref><ref type="bibr">Kingma, 2014a]</ref>) with the exception of replacing the multivariate unit Gaussian prior with the proposed GP-prior. The approximated posterior is the output of a temporal convolutional neural network (TCN <ref type="bibr" target="#b13">[Bai, 2018]</ref>) allowing for temporal regularization. To further facilitate temporal dependencies and handle missing data, temporal dropout (TD) is applied during the training procedure. In the following, the different parts of the method are explained. First, the probabilistic motion model using a GP-prior is defined. Then, posterior and data likelihood distributions are modeled using a encoder-decoder neural network. Lastly, the concept of temporal dropout is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Generative Motion Model using a Gaussian Process Prior</head><p>The proposed motion model consists of an encoder q ω (z|I 0:T ) and a decoder p θ (I 1:T |z, I 0 ) which are parameterized by ω and θ respectively. The encoder first independently maps each image pair (I 0 , I t ) to a latent representation γ t which is then temporally regularized by mixing all time steps to retrieve the motion matrix z. The decoder p θ projects the z t -codes to the deformations φ t while being conditioned on the moving image I 0 . The output of the decoder are he reference image I 0 warped with the φ t deformation fields.</p><p>The encoder approximates the posterior distribution and the decoder the data likelihood of the latent variable model. Using a prior distribution p(z) over latent variables z, we define the following generative process:</p><formula xml:id="formula_33">p θ (I 1:T |I 0 ) = z p θ (I 1:T |z, I 0 )p(z) dz, (5.1)</formula><p>which is visualized in Fig. <ref type="figure" target="#fig_19">5</ref>.1a. In this work, encoder q ω and decoder p θ are approximated using neural networks where ω and θ represent the encoder and decoder networks' weights which are optimized using amortized Variational Inference <ref type="bibr" target="#b77">[Kingma, 2013]</ref>. The data likelihood p θ (I 1:T |z, I 0 ) can be seen as the fidelity of the reconstruction of the fixed images I 1:T by warping the moving image I 0 with appropriate deformations φ 1:T . An overview of the motion model can be seen in Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Process Prior</head><p>The prior follows a zero-centered multivariate Gaussian distribution: p(z) ∼ N (0|Σ) where the covariance matrix Σ is a diagonal block matrix of dimensions D T × D T :</p><formula xml:id="formula_34">Σ = Diag D d=1 (K l ). (5.2)</formula><p>Each diagonal element of Σ represents the temporal covariance matrix K l ∈ R T × T of a Gaussian time-continuous stochastic process whose kernels can be chosen by the user.</p><p>A typical choice in Gaussian Processes is the squared exponential kernel</p><formula xml:id="formula_35">K RBF l (τ, τ ) = σ 2 K exp (-|τ -τ | 2 /2l 2 )</formula><p>with length scale l and variance σ 2 K . However, due to the fact that we want to model data that varies at multiple time scales, we consider the Cauchy kernel <ref type="bibr" target="#b141">[Rasmussen, 2003;</ref><ref type="bibr" target="#b54">Fortuin, 2019]</ref>: <ref type="bibr">.3)</ref> with pre-defined σ K . This covariance matrix Σ allows temporally correlated latent variables while still assuming highest possible independence between the D latent dimensions. In other words, we extended the standard VAE latent space which only consists of the independence assumption between latent variables with a regularized temporal dimension. Latent variables are related over time according to the chosen kernel function K l while being independent of each other. An example of a covariance matrix can be seen in Fig. <ref type="figure" target="#fig_19">5</ref>.1b.</p><formula xml:id="formula_36">K Cauchy l (τ, τ ) = σ 2 K 1 - (τ -τ ) 2 l 2 -1 , (<label>5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Posterior and Likelihood Distributions</head><p>Similar to standard VAEs, the posterior q ω follows a multivariate Gaussian distribution q ω (z|I 0:T ) ∼ N (µ|Σ * (σ)) with data-driven predictions of mean vector µ ∈ R D T and variance vector σ ∈ R D . The full covariance matrix Σ * (σ) is defined as a block diagonal matrix of the following form: <ref type="bibr">.4)</ref> where 1 defines a vector of ones of size T and vec(•) describes the vectorization function.</p><formula xml:id="formula_37">Σ * (σ) = vec σ1 Σ =        σ 1 K l 0 • • • 0 0 σ 2 K l • • • 0 . . . . . . . . . . . . 0 0 • • • σ D K l        , (<label>5</label></formula><p>Mean and variance vectors (µ, σ) are the output of the encoder neural network. The kernel K l is kept the same as in the prior distribution and does not contain predicted parameters to guarantee a user-chosen temporal regularity.</p><p>Also, the likelihood p θ is assumed to follow a multivariate Gaussian distribution p θ (I 1:T |z, I 0 ) ∼ N (I 0 • φ 1:T (θ); 0|σ L * I D T ) where I D T is the identity matrix of size D T , f θ is the decoder neural network that outputs the diffeomorphisms φ 1:T and • denotes the image warping operation. The variance σ L is chosen to be a scalar constant, depicting for example the variance of intensity residuals of well registered images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning the Motion Model via Variational Inference</head><p>In order to optimize the parameterized motion model over ω and θ, the evidence lower bound (ELBO) of the log-marginalized likelihood p θ (I 1:T |I 0 ) that is conditioned on the moving image I 0 , must be maximized (see <ref type="bibr" target="#b77">[Kingma, 2013;</ref><ref type="bibr">Kingma, 2014b;</ref><ref type="bibr" target="#b87">Krebs, 2019b]</ref> for details):</p><p>E z∈qω(•|I 0:T ) log p θ (I 1:T |z, I 0 ) -KL q ω (z|I 0:T ) p(z) , <ref type="bibr">(5.5)</ref> with KL denoting the Kullback-Leibler Divergence (KL). The first term in Eq. 5.5 enforces that the moving image I 0 is well registered to the fixed images I 1:T by maximizing the log likelihood. The second term structures the latent motion encoding by enforcing the posterior distribution q ω (z|I 0:T )) to be close to the prior distribution p(z). Following the definition of the KL divergence between 2 multivariate Gaussian distributions, we obtain the closed-from solution (see Appendix A): <ref type="bibr">(5.6)</ref> with μi being the i-th segment of length T in µ.</p><formula xml:id="formula_38">KL q ω (z|I 0:T ) p(z) = 1 2 D i=1 σ 2 i T + μ i K -1 μi -log (σ 2 i ) -T ,</formula><p>Recall that the log likelihood p θ (I 1:T |z, I 0 ) is also Gaussian. Thus, log p θ (I 1:T |z,</p><formula xml:id="formula_39">I 0 ) = -1 2 T t=1 I t -I 0 • φ t 2 /σ L</formula><p>plus a constant which is equivalent to adopting a sumof-squared differences (SSD) criterion, commonly used as similarity metric in image registration (for example in <ref type="bibr">[Balakrishnan, 2018]</ref>).</p><p>During training of the model, parameters ω and θ are updated via stochastic gradient descent and back-propagation. In order to back-propagate through the sampling operation, the reparameterization trick is used <ref type="bibr" target="#b77">[Kingma, 2013]</ref>. For full-covariance Gaussian distributions, the covariance matrix must be positive-definite as we use the Cholesky decomposition for the reparameterization (cf. <ref type="bibr" target="#b81">[Kingma, 2019]</ref>). The details on how to efficiently compute the Cholesky decomposition of the covariance matrix Σ * in Eq. 5.4 can be found in Appendix B. Diffusion-like regularization in spatial and temporal dimensions is applied by Gaussian smoothing kernels. This regularization follows the derivations of <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> and is omitted in Fig. <ref type="figure" target="#fig_19">5</ref>.1a for reasons of clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Network Architecture</head><p>The encoder takes the image pairs (I 0 , I t ) as input and outputs the motion matrix z.</p><p>It consists of a feature extraction part and a temporal regularizer (TCN). The feature extraction part consists of convolutional and fully-connected layers for mean and variance predictions of the posterior <ref type="bibr">[Kingma, 2014b]</ref>. These layers are temporally independent and share weights across all image pairs of a sequence. As the output of the feature extraction networks, the extracted features γ t of size R 2D are merged across different time steps by using a temporal convolutional network (TCN) leading to temporally regularized mean and variance vectors (µ, σ) that define the posterior distribution q ω (z|I 0:T ) ∼ N (µ|Σ * (σ)). The size of 2D is chosen for γ t such that each σ value can be influenced by features from the whole sequence. Note, that samples from the posterior distribution are vectors of size D T which are reshaped to retrieve the motion matrix z with z t -columns.</p><p>Following the recommended architecture, the TCN consists of multiple 1-D convolutional layers with increasing dilation and skip connections allowing to learn temporal dependencies of the latent variables γ t that were time-independent before <ref type="bibr" target="#b13">[Bai, 2018]</ref>. We use zero-padding and non-causal convolutional layers to also take future time steps into account. The output tensor capturing (µ, σ) is of size R D T +D . Our TCN is shown in Fig. <ref type="figure" target="#fig_19">5</ref>.3a. TCNs can handle sequences of varying time lengths and are advantageous compared to recurrent neural networks (RNN) due to a flexible receptive field and more stable gradient computations <ref type="bibr" target="#b13">[Bai, 2018]</ref>. Another reason why the authors chose a TCN over RNNs is that RNNs are especially suitable to learn long-distance temporal relationships such as in natural language processing while the focus of this work is on rather short time sequences with higher local dependencies. One could use a cyclic padding instead of zero-padding for cyclic sequences, for example by linking the end of a sequence to its beginning. However, in the case of cardiac cine-MRI, 5-10% of the cardiac cycle are often omitted <ref type="bibr" target="#b20">[Bernard, 2018]</ref> such that we chose to not assume cyclic sequences explicitly.</p><p>The decoder takes as input samples z t from the posterior distribution and the moving image I 0 and outputs the diffeomorphisms φ 1:T and the accordingly warped moving image. Deconvolutional and convolutional layers are used in the decoder which are shared across all time steps. It is desired that the latent representation z encodes deformation information on a semantic level, independent of the given subject. That is why the decoder is further conditioned on the moving image I 0 by concatenating downsampled versions of I 0 with the outputs of the deconvolutional layers at different scales. By providing subject-specific appearance information in form of the moving image, the motion model is driven to encode subject-independent deformation information in the limited dimensionality of z <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>. In order to ensure smooth and diffeomorphic deformations, we utilize a Gaussian smoothing layer with standard deviations of σ G and σ T in temporal and spatial domains respectively and an exponentiation layer for the stationary velocity field parameterization of diffeomorphisms <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>. The linear warping functionality is realized using a spatial transformer network layer <ref type="bibr" target="#b70">[Jaderberg, 2015]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Missing Data and Temporal Dropout</head><p>To always predict a full sequence of T deformations, the size of the covariance matrix Σ * is kept identical across datasets with different time lengths T * . In case of shorter sequences, the features γ τ of all available image pairs (I 0 , I τ ) with τ ∈ T * are extracted and evenly distributed along T forming the matrix Γ ∈ R 2D× T . The remaining missing time steps are filled with a constant (typically zero). On the decoder side, the loglikelihood loss (first part of Eq. 5.5) is evaluated on all available time steps of the original sequence. If a sequence is longer than T , evenly distributed frames would be dropped to reach a lentgh of T . However, this should not happen normally as we assume to put T at least as the maximum experienced length in the data.</p><p>In addition, during training, further time steps (i.e. γ τ ) are dropped from Γ using temporal dropout (TD) in order to force the motion model to interpolate motion between available frames. To encourage the TCN to make use of its temporal connections and search for dependencies across time, our TC drops some of the γ τ while still trying to recover the deformations φ τ of all available image pairs (I 0 , I τ ). More precisely, in TD, instead of extracting features from an image pair (I 0 , I τ ), a vector of zeros is chosen as γ τ while still keeping the loss function on the decoder part for these time steps. A binary Bernoulli random variable r τ is used to randomly choose at each original time step τ if the zero vector is used instead of the extracted features given (I 0 , I τ ). All independent Bernoulli random variables r ∈ R T * have the success probability δ. The latent feature representation γ T D t using TD can thus be defined as:</p><formula xml:id="formula_40">γ T D τ = r τ * 0 + (1 -r τ ) * γ τ . (5.7)</formula><p>Note, TD is used only during training as a sort of self-supervision to encourage generalizability and consistent motion simulation and interpolation of missing data. When encountering missing data at test time, one just needs to place the available encoded frame pairs at the desired temporal positions of Γ in order to predict the full motion consisting of T time steps (cf. Fig. <ref type="figure" target="#fig_19">5</ref>.3b). A full motion simulation can be generated by setting all elements of Γ to zero. In this case, a sequence of deformations that are plausible with respect to the training data will be predicted given only the original image I 0 .</p><p>Optional Random Sub-Sequence Training: Since our motion model takes sequences of images as input and outputs a sequence of deformation fields, it comes naturally with high computational costs. This can lead to a model that may not be trainable on standard GPUs. Due to this limitation, we propose to train our model optionally with random subsequences. Let T be the maximum number of frames with which our model can be trained on a given GPU. In each training iteration, a random combination of T frames is selected from a training subject with T * frames in case T * &gt; T . After sorting this combination, the given frame pairs are encoded and placed at their relative temporal position in Γ. In contrast to the TD procedure, only the selected T time steps are reconstructed in the decoder to limit the requirements of GPU memory. In case of shorter training sequences with T * ≤ T , the full sequence is used. By sampling different sub-sequences in each training epoch, the network will eventually see all parts of a sequence during the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>In this paper, we evaluate the proposed motion model on cardiac cine-MRI. Besides accurate temporal tracking and registration, we show the model's capabilities for motion simulation, interpolation and transport. The improved temporal latent space using the GP prior is demonstrated. Extensive results are presented for 2D+T sequences with more</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>limited quantitative evaluations on 3D+T sequences due to their heavy computational requirements. In all experiments, the end-diastolic (ED) frame was used as the moving image I 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Databases</head><p>Two datasets forming 334 cardiac cine-MRI in total were used. First, 184 multi-centric short-axis sequences came from the EU FP7-funded project MD-Paedigree (Grant Agreement 600932), with congenital heart disease and healthy or pathological images from adults. In addition, 150 sequences originated from the Automatic Cardiac Diagnosis Challenge 2017 (ACDC <ref type="bibr" target="#b20">[Bernard, 2018]</ref>). The images were acquired in breath hold using 1R-R or 2R-R intervals mixing retrospective or prospective gating. The original sequence lengths varied from 13 to 35 frames. The 100 training cases from ACDC that contain ED-ES segmentation information were used for testing while all other sequences were used for training. Slices were resampled with a spacing of 1.5×1.5 mm and cropped to a size of 128×128 pixels. In case of 3D+T sequences, 18 slices were used by adding zero slices at the top and bottom in case of fewer original slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Implementation Details</head><p>The time-independent neural network parts, the feature extraction part of the encoder and the decoder, followed the architecture proposed in <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>. The feature extractor consisted of 4 convolutional layers with (2,2,2,1)-strides and (16,32,32,4)feature maps and a fully-connected layer of size 2D, outputting γ t . The decoder p θ consisted of a 3 deconvolutional and 1 convolutional layer with <ref type="bibr">(32,</ref><ref type="bibr">32,</ref><ref type="bibr">32,</ref><ref type="bibr">16</ref>)-feature maps. The TCN consisted of 4 1-D convolutional layers with (1,2,4,8)-dilations, same padding and skip connections (cf. Fig. <ref type="figure" target="#fig_19">5</ref>.3a). All (de-)convolutional layers used a kernel size of 3. The last convolutional layer of the decoder was followed by a spatio-temporal Gaussian layer with spatial σ G = 3mm and temporal standard deviation σ T = 1.5, an exponentiation layer using 6 scaling-squaring iterations <ref type="bibr" target="#b87">[Krebs, 2019b]</ref> and a linear warping layer <ref type="bibr" target="#b70">[Jaderberg, 2015]</ref>.</p><p>The latent dimensionality was set to D = 32 (as in <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>). We set the sequence length T to 35, the maximum sequence length found in the training data, resulting in a motion matrix z with D • T = 1088 elements. All sequences with fewer frames were handled as missing data as described in section 5.2.2. The number of trainable parameters (ω, θ) in the network summed up to ∼210k in 2D+T and ∼456k in 3D+T respectively. L2 weight decay of 1 • 10 -4 and LeakyReLu activation functions were applied on all layers except the last layer of the TCN and the last layer of the decoder. The former used no activation function for the µ-vector but used the exponential of the σ-vector to guarantee non-negative values close to 1. The last convolutional layer of the decoder p θ was followed by a tanh activation function for stability reasons during training. The Cauchy-kernel parameters were chosen as proposed in <ref type="bibr" target="#b54">[Fortuin, 2019]</ref> with l = 7 and σ K = 1.005. The variance of the data likelihood was set as the variance of intensity residuals of a few well-registered image sequences with σ L = 0.0045 in 2D+T and 0.00021 in 3D+T respectively.</p><p>For training, we used a first-order gradient-based method for stochastic optimization (Adam <ref type="bibr">[Kingma, 2014a]</ref>) with a batch size of one and fixed learning rate of 0.00015. The TD probability δ was 0.5. Random sub-sequence training was only applied for 3D+T with T = 18. Online data augmentation containing randomly shifted, rotated, scaled and mirrored images has been applied. The model was implemented using Keras <ref type="bibr" target="#b29">[Chollet, 2015]</ref> and Tensorflow <ref type="bibr" target="#b4">[Abadi, 2016]</ref>. The training time was ∼15h in 2D+T and 7 days for 3D+T sequences on a NVIDIA GTX TITAN X GPU. The LV volume curves extracted from the warped ED blood pool masks for 2 random test cases in ml, show the temporal smoothness and the distance to the ground-truth ED and ES volumes (marked with black points). The proposed algorithm (Our) shows slightly higher registration accuracy and temporally smoother deformations than the state-ofthe-art algorithms: SyN <ref type="bibr" target="#b7">[Avants, 2008]</ref>, LPR <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>, 4D-Elastix <ref type="bibr" target="#b118">[Metz, 2011]</ref> and the previous version of our method without GP prior (No-GP <ref type="bibr">[Krebs, 2020c]</ref>). ) for a test sequence. In 3D+T, smoother Jacobian determinants were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Registration and Motion Prediction</head><p>We compare our model in terms of registration accuracy and spatio-temporal deformation regularity with 3 state-of-the-art diffeomorphic methods: SyN <ref type="bibr" target="#b7">[Avants, 2008]</ref>, the learning-based probabilistic pairwise registration (LPR <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>) and the temporal B-spline algorithm in elastix (4D-Elastix <ref type="bibr" target="#b118">[Metz, 2011]</ref>). We also compare with the previous version of our method with Gaussian Process prior (No-GP <ref type="bibr">[Krebs, 2020c]</ref>). SyN and 4D-Elastix have been manually tuned on a few training images following the recommendations in the original papers. The LPR algorithm has been trained on a 2D single scale version using all image pairs of a sequence instead of only the enddiastolic/end-systolic (ED, ES) pairs. We measured registration accuracy using the root mean square error (RMSE) of intensities and segmentation-based DICE scores and 95%tile Hausdorff distances (HD, in mm) on the five anatomical structures available in ACDC: left ventricle myocardium (LV-Myo), epicardium (LV), left ventricle bloodpool (LV-BP), right ventricle (RV) and LV+RV. In terms of registration regularity, we report spatial (Spatial Grad.) and temporal gradients (Temp. Grad.) of the deformation fields φ t with t ∈ [1, T ]. The reported results in Table <ref type="table">5</ref>.1 were measured on all 2D test sequences containing at least one mask (resulting in 677 sequences from 100 test subjects). DICE scores and Hausdorff distances are only reported for the frames with available ground-truth segmentation (ES images). Detailed box plots of the results together with LV volume curves are shown in Fig. <ref type="figure" target="#fig_19">5</ref>.4. The LV volumes (in ml) were extracted by warping the ED mask according to the extracted deformation fields and computing the blood pool volume for all slices of one subject over time. The results indicate that our model achieves the same (RMSE) or slightly better (DICE and HD) registration accuracy compared to the reference methods while improving spatial and temporal regularity as shown by the deformation field gradients and the volume curves. In Table <ref type="table">5</ref>.2, we show the results on the 100 test sequences for our 3D+T model. In comparison to 4D-Elastix, our 3D+T model shows a similar registration accuracy but a significantly improved spatial and temporal regularity. In Fig. <ref type="figure" target="#fig_19">5</ref>.5, the warped moving image I 0 and the Jacobian determinant are visualized for one test sequence in 2D+T and 3D+T. One can see, the Jacobian determinants are smoother in 3D+T compared to 2D+T sequences.</p><p>The new Gaussian Process prior leads to smoother deformations compared to the previous time-independent prior (No-GP version) while using the same deformation field regularizer. This can be also seen in Fig. <ref type="figure" target="#fig_19">5</ref>.6 where the first 5 latent dimensions, the sequences z d with d ∈ [0, 4], are visualized for one test case. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No-GP Our</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Motion Simulation, Interpolation and Transport</head><p>To evaluate the performance on motion interpolation and simulation, we challenged our model to predict the motion for all time steps from a limited number of input frames. Thus, the goal was to predict motion patterns that are as close as possible to the observed motion of the full sequence (i.e. all registered frames obtained in the all frame model of the previous section 5. <ref type="bibr">3.3)</ref>. Just as in temporal dropout during training, all the missing frames were represented as zero columns γ t in the feature matrix Γ as shown in Fig. <ref type="figure" target="#fig_19">5</ref>.3b. We compared the motion predictions from various input frame subsets that are provided to the model. First, we provided every 2nd or every 5th frame for motion interpolation.</p><p>Then, we provided the first 5 frames or only the 10th frame (0th + 10th) to see if the model is able to complete typical cardiac motion patterns. Finally, we tested the full motion simulation by letting the model find a motion sequence given only the moving image I 0 (only 0th) and setting feature matrix Γ to zero everywhere. We compared the simulated motion, with linear and cubic interpolation of the deformation fields (which are taken from the all frame model at the selected time steps). In the top of For the cases of providing every 2nd and every 5th frame, our model interpolated the motion similarly well as linear or cubic interpolation, while providing better results in the cases of providing the 0th+10th and first 5 frames signaling an improved learned cardiac motion model. The full simulation (only 0th) did not result in well fitted volume curves, which is expected as the model has to simulate the full motion sequence from just the ED frame. However, it is observable that the model learned realistic cardiac specific motion patterns as the volume curves for example show the plateau phase before atrial systole which can be also seen in the completed motion for the cases where we provide the first 5 and 0th+10th frames. For the full simulation, our model often slightly under-estimated the motion (cf. case 3 in Fig. <ref type="figure" target="#fig_19">5</ref>.7) which can be related to the pathology distribution in the training dataset which contained many cases with reduced cardiac motion.</p><p>Furthermore, we demonstrate the model's capacity of motion transport in a qualitative way. Our model allows to transport motion patterns from one subject to another by taking the motion matrix z of one case and applying it on the moving image of another image sequence (ED frame). In this way, for example a pathological motion can be simulated in a healthy subject or vice versa. In Fig. <ref type="figure" target="#fig_19">5</ref>.8, we present 2 subjects from the ACDC dataset, from which one is classified as healthy and the other as a dilated myopathy case (DCM). We extracted the motion matrices for both and applied them on the ED frame of the other case, such that we simulated a DCM typical motion in the healthy case while curing the pathological case. This can be seen for example from the LV contraction strengths in the Jacobian determinants or the related ejection fraction (EF). Note, that this form of parallel transport does not require any additional inter-subject registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion and Conclusion</head><p>We presented a probabilistic motion model that can be useful for example for spatiotemporal registration, temporal super-resolution, data augmentation, shorter acquisition times and motion analysis. Based on a novel Gaussian Process prior conditional variational autoencoder, the model is learned in an unsupervised fashion from medical image sequences. Intrinsic motion patterns are encoded in a low-dimensional probabilistic space -the motion matrix -which allows for accurate diffeomorphic tracking, temporal interpolation, motion simulation and motion transport.</p><p>Our approach has shown state-of-the art registration accuracy and improved deformation regularity temporally and spatially in comparison to 3 state-of-the-art algorithms indicating that the low-dimensional motion encoding helps to regularize the registration problem of image sequences. We have shown that the novel Gaussian Process prior leads to a higher temporal consistency compared to the time-independent prior <ref type="bibr">[Krebs, 2020c]</ref> both, in latent and deformation space. A temporally smoother latent space is desirable as it brings more structure and interpretability and is consistent with the temporally smooth motion we experience in deformation space. We have demonstrated motion simulation and interpolation from a very limited number of frames indicating that data acquisition could be speed up as fewer frames are required in order to retrieve an accurate motion.</p><p>In case of full simulations, our model showed a slightly reduced cardiac motion compared to healthy subjects. The authors believe this is due to a bias introduced from the disease distribution in the training data. To not end up with such a mean motion that merges several pathological motion patterns, one could think of generating disease-specific models. This could be achieved by training different motion models with training sets separated by diseases. As another extension to our previous work, we have shown first results on 3D+T sequences which showed smoother Jacobian determinants than the 2D+T version which can be explained by out-of-plane deformations. However, a limitation is the high computational costs for 3D+T sequences with long training times even for relatively low-dimensional images.</p><p>In future work, we aim to reduce this complexity and work on the generalization of the approach to other applications such as respiratory motion estimation. Furthermore, the authors believe the motion matrix as a compact representation of organ motion can be helpful as a quantitative new tool to guide the diagnosis, prognosis or therapy of diseases of dynamic organs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">KL Divergence using the GP Prior</head><p>Given 2 multivariate Gaussian distributions with the same dimensionality, the KL divergence is defined in <ref type="bibr" target="#b45">[Duchi, 2007]</ref>. Suppose, we take our prior distribution p(z) with zero-mean 0 and covariance Σ of the form of Eq. 5.2 and our posterior distribution q ω with mean µ and covariance Σ * with dimensionality D T :</p><formula xml:id="formula_41">KL[q ω (z|I 0:T ) p(z)] = 1 2 tr(Σ -1 Σ * ) + µ Σ -1 µ -D T + ln det Σ det Σ * . (5.8)</formula><p>The determinants of the block diagonal matrices Σ, Σ * are det Σ = |K| D and det Σ * = |K| D D i=1 σ 2 i . Thus, the logarithm of the fraction of determinants in Eq. 5.8 becomes:</p><formula xml:id="formula_42">ln det Σ det Σ * = ln 1 D i=1 σ 2 i = - D i=1 ln σ 2 i (5.9)</formula><p>When taking the sum over the D latent dimensions over the remaining terms, Eq. 5.8 simplifies to:</p><formula xml:id="formula_43">KL[q ω (z|I 0:T ) p(z)] = 1 2 D i=1 σ 2 i T + μ i K -1 μi -T -ln (σ 2 i ) (5.10)</formula><p>with μi being the i-th segment of length T in µ. In the case of prior and posterior being identical, thus µ = 0 and σ = 1 the quantity in Eq. 5.10 becomes 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Cholesky Decomposition of Σ *</head><p>The Cholesky decomposition of a symmetric positive-definite matrix X equals the matrix product of a lower-diagonal L and its transposed: X = LL . The entries of L can be computed by the Cholesky-Banachiewicz algorithm:</p><formula xml:id="formula_44">L j,j = X j,j - j-1 k=1 L 2 j,k L i,j = 1 L j,j X i,j - j-1 k=1 L i,k Lj, k</formula><p>for i &gt; j. <ref type="bibr">(5.11)</ref> In case of the block diagonal matrix Σ * the lower triangular matrix L * equals a block diagonal matrix with lower triangular matrices that are resulting from the Cholesky decompositions of the diagonal block elements of Σ * . Thus, in order to compute L * , the Cholesky decompositions of the i ∈ D diagonal elements σ i K must be computed. From Eq. 5.11 it follows that c</p><formula xml:id="formula_45">• X = ( √ c • L)( √ c • L ). Thus, σ i K = ( √ σ i • L K )( √ σ i • L K ) and L * is: L * = Diag D d=1 ( √ σ d • L K ).<label>(5.12)</label></formula><p>Since the kernel matrix K is fixed in our framework, L K can be pre-computed using Eq. 5.11 and reused keeping the computational efforts minimal even for a large covariance matrix Σ * . This chapter is intended to show one specific clinical example on how the motion model developed in chapter 5 could be used to support prognosis and therapy planning. Using the motion model, the survival risks of heart failure patients can be predicted by obtaining a risk score from the latent motion matrix. Based on this estimated risk, an appropriate therapy can be chosen, for example, whether or not to implant a defibrillator. This demonstrates the discriminative power of the motion model trained on a cohort of heart failure patients. The chapter presents only preliminary results. A clinical journal submission is in preparation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Introduction</head><p>Sudden cardiac death (SCD) in heart failure (HF) patients is one of the leading causes of natural death. SCD occurs when the electrical system of the heart is malfunctioning causing irregular heartbeats (arrythmias). Emergency treatment includes electric shocks (defibrillation) to restore the normal heart rhythm. For patients with a high risk of SCD, an implantable cardioverter-defibrillator (ICD) can be inserted as a preventive treatment.</p><p>An ICD monitors the heart activity and can apply electric shocks in case of extreme arrythmias.</p><p>Selecting patients for ICD treatment is a challenging task. It is crucial to predict the risk for SCD to justify potential complications that come along with an ICD treatment such as surgery risks, false shocks and a shorter life expectancy. Accurate SCD risk prediction helps to select only patients for ICD who benefit from it.</p><p>Currently, the main quantitative measure used to predict risk for SCD is left ventricular ejection fraction (LVEF), an imaging feature of cardiac structure and function <ref type="bibr" target="#b124">[Myerburg, 2009]</ref>. However, among patients receiving a primary prevention ICD based on an LVEF ≤35% <ref type="bibr" target="#b168">[Tracy, 2013]</ref>, the rate of appropriate therapies is very low with 2.6% at 30 months of follow-up <ref type="bibr" target="#b153">[Sabbag, 2015]</ref>. In other words, many patients that receive ICD treatment do not require it. In addition, LVEF improvement occurs in up to 25-50% of patients and correlates with diminished SCD risk <ref type="bibr" target="#b137">[Punnoose, 2011]</ref>. Thus, LVEF is far from being a comprehensive feature to predict SCD. Recently, other imaging features of cardiac structure and function have been found to be independent predictors of SCD. Such factors are right ventricular (RV) and left atrial (LA) <ref type="bibr" target="#b144">[Rijnierse, 2017]</ref> function or the extent of heterogeneous myocardial tissue (gray zone) on late gadolinium enhancement (LGE) cardiac magnetic resonance images <ref type="bibr" target="#b69">[Jablonowski, 2017]</ref>. This motivates the assumption that more unidentified SCD predictors are inherently present in cardiac images.</p><p>Deep learning is capable of addressing the high-dimensional vector space and extracting unrecognized features from medical images. Lou et al. <ref type="bibr" target="#b107">[Lou, 2019]</ref> proposed to extract features from images to predict treatment outcomes in lung cancer patients by incorporating hand-crafted radiomics features in the training. Taking low-dimensional segmentations of the right ventricle as input, Bello et al. <ref type="bibr" target="#b19">[Bello, 2019]</ref> predicted the survival risk for patients with pulmonary hypertension. While these approaches rely on hand-crafted features extracted from images, we have shown in our previous work (Chapter 5) that a motion fingerprint containing inherent features of the LV motion can be generated from cine-MRI images using a latent variable model. This population-specific fingerprint can be learned in an unsupervised fashion by training a probabilistic motion model using a conditional variational autoencoder (CVAE) <ref type="bibr">[Krebs, 2020c]</ref>.</p><p>We propose a novel learning-based method for personalized survival risk prediction for SCD that utilizes automatically derived image features from 4 chamber view cine-MRI.</p><p>Our model generates fingerprints of inherent imaging features of the cardiac motion which are used to predict risk scores for outcomes of HF patients such as hospitalization or SCD. In clinical practice, these risk scores can be used to select high-risk patients for ICD treatment while postponing ICD treatment for low-risk patients. In particular, the novel risk predictor uses an automatically extracted personalized cardiac motion fingerprint in combination with a risk prediction neural network. The risk prediction network is based on a non-linear Cox proportional hazard loss to make use of right-censored survival outcome data.</p><p>On a non-ischemic cohort of HF patients with clinical criteria for primary prevention ICD, the derived motion risk factor showed the highest statistical significance as an independent predictor for hospitalization among other relevant clinical factors that are associated with HF endpoints.</p><p>The main contributions are:</p><p>• A novel risk prediction framework for HF patients based on a cardiac motion fingerprint extracted from image sequences in an unsupervised fashion.</p><p>• State-of-the-art predictive accuracy for HF hospitalization on a non-ischemic patient cohort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methods</head><p>The risk prediction model is composed of two elements: A. a motion fingerprint extractor from image sequences and B. a survival predictor that estimates the risk for a given endpoint (or outcome) from the motion fingerprint. In this work, we apply two independent neural networks for these tasks. First, a probabilistic encoder-decoder neural network <ref type="bibr">[Krebs, 2020c]</ref> is trained to learn motion characteristics from image sequences and extract a cardiac motion fingerprint in a fully unsupervised fashion. Second, an autoen-coder neural network is trained from the motion fingerprint by regressing HF outcomes.</p><p>To enable the use of censored data, a loss function inspired from the Cox proportional hazards model <ref type="bibr">[Cox, 1972]</ref> is utilized. The two steps, A. and B. are schematically shown in Fig. <ref type="figure">6</ref>.1 and are explained in detail in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Motion Fingerprint Extractor</head><p>The motion model used in this chapter has the same inputs and outputs as the one presented in chapter 5. However, it includes some methodological differences as it applies a multivariate unit Gaussian prior, time-independent sampling and explicit time dependence. Detailed derivations of the fingerprint extractor can be found in our conference paper <ref type="bibr">[Krebs, 2020c]</ref> and in the appendix 6.5.2.  <ref type="bibr">[Kingma, 2014b]</ref>). Instead of the left-ventricular motion as in <ref type="bibr">[Krebs, 2020c]</ref>, we learn a motion fingerprint of the full heart. Furthermore, in contrast to Chapter 5, a temporally independent unit Gaussian prior has been applied.</p><formula xml:id="formula_46">The</formula><p>First, the encoder q ω with network weights ω maps each of the image pairs (I 0 , I t ) independently to a latent space denoted by zt ∈ R D . To this end, the encoder approximates the posterior distribution q ω (z|I 0:T ) of the latent variable model. Second, as the key component of temporal modeling, these latent vectors zt are jointly mapped to the motion matrix or motion fingerprint z by conditioning them on all past and future time steps and on the normalized time t: p γ (z|z 1:T , t1:T ). This regularizing network p γ with weights γ is realized using a temporal convolutional network (TCN <ref type="bibr" target="#b13">[Bai, 2018]</ref>). Finally, the decoder p θ with trainable network weights θ aims to reconstruct the fixed image I t by warping the moving image I 0 with the deformation φ t . This deformation φ t is extracted from the temporally regularized z t -codes. The decoder is further conditioned on the moving image by concatenating the features at each scale with down-sampled versions of I 0 . It approximates the data likelihood p θ (I 1:T |z, I 0 ).</p><p>During training, a lower bound on the data likelihood is maximized with respect to a prior distribution p(z t ) of the latent space zt (cf. CVAE <ref type="bibr">[Kingma, 2014b]</ref>). The prior p(z t ) is assumed to follow a multivariate unit Gaussian distribution with spherical covariance I: p(z t ) ∼ N(0, I). The loss function of the motion fingerprint extractor results in optimizing the expected log-likelihood p θ and the Kullback-Leibler (KL) divergence enforcing the posterior distribution q ω to be close to the prior p(z t ) for all time steps:</p><formula xml:id="formula_47">L Motion (ω, γ, θ) = T t=1 -E zt∼pγ (•|z 1:T , t1:T ) log p θ (I t |z t , I 0 ) + KL [q ω (z t |I 0 , I t ) p(z)] . (6.1)</formula><p>Unlike the traditional CVAE model, the temporal regularized z t -code is used in the loglikelihood term p θ instead of the zt . We model p θ as a symmetric local cross-correlation Boltzmann distribution with the weighting factor ι. All network weights except the ones in the TCN are shared and thus independent of the time t. Their network architecture consists of convolutional and deconvolutional layers with fully-connected layers for mean and variance predictions in the encoder part <ref type="bibr">[Kingma, 2014b]</ref>. We use an exponentiation layer for a stationary velocity field parameterization of diffeomorphisms <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>, a linear warping layer and diffusion-like regularization with smoothing parameters σ G in spatial and σ T in temporal dimension. During training, we apply temporal dropout sampling as described in <ref type="bibr">[Krebs, 2020c]</ref> in order to further ensure learning temporal dependencies and increase generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Survival Predictor</head><p>The survival predictor takes the motion fingerprint z, the compact representation of the motion, as input and predicts the survival risk score r which is defined by the logarithm of the hazard ratio in the Cox regression analysis <ref type="bibr">[Cox, 1972]</ref>. This ratio contains the hazard h z (t) of a subject with fingerprint z with respect to the baseline hazard h 0 (t):</p><formula xml:id="formula_48">r = log h z (t) h 0 (t) , (<label>6.2)</label></formula><p>where the subject hazard h z (t) symbolizes the probability of the subject of dying at time t and the baseline hazard describes the survival without an influence of covariates z. The hazard ratio is assumed to be constant over time behind the semi-parametric proportional hazard model of Cox <ref type="bibr">[Cox, 1972]</ref>. Thus, the continuous risk score r allows to classify the outcome risk for a new patient at test time.</p><p>In contrast to standard Cox regression analysis, we define the risk r as a non-linear combination of input features z: r = r ν (e κ (z)) where r ν and e κ are two neural networks with network weights ν and κ. The full risk model is realized as autoencoder neural networks that reduce the fingerprint's dimensionality D T in order to retrieve the risk r.</p><p>The authors chose an encoder-decoder architecture in contrast to a direct prediction of r in order to constrain and regularize the risk predictor to avoid over-fitting <ref type="bibr" target="#b19">[Bello, 2019;</ref><ref type="bibr" target="#b107">Lou, 2019]</ref>.</p><p>The encoding and decoding branches of the risk autoencoder are denoted by e κ and d λ with network weights κ and λ respectively. A third network with weights ν is applied to obtain the risk score r ν (e κ (z)) from the latent space of the autoencoder e κ (z). In this work, the three networks consist of fully-connected layers due to the low dimensional fingerprints. In case of larger fingerprints, convolutional and deconvolutional layers in encoder respectively decoder networks could be used. The risk predictor is trained using multi-task learning by aiming to reconstruct the motion fingerprint and to predict the risk r at the same time. Thus, the loss function L Risk (κ, λ, ν) contains 2 terms, one for the fingerprint reconstruction L rec (κ, λ) and one for risk prediction L risk (κ, ν):</p><formula xml:id="formula_49">L Risk (κ, λ, ν) = L rec (κ, λ) + αL risk (κ, ν) (6.3)</formula><p>where α denotes a weighting factor between both terms. For risk prediction, we apply the negative log partial likelihood as survival function over N censored training samples following standard Cox regression analysis <ref type="bibr">[Cox, 1972]</ref>:</p><formula xml:id="formula_50">L risk (κ, ν) = - N i=1 δ i r ν (e κ (z i )) -log N j=1</formula><p>R ij exp(r ν (e κ (z j ))) , <ref type="bibr">(6.4)</ref> with z i being the fingerprint of the i-th training subject. The Boolean censoring indicator δ i equals 1 if the subject experienced SCD (or another endpoint of interest) at the given time τ . A subject is censored δ i = 0 if the patient was still alive at time τ but removed from the study afterwards. R is the risk matrix where R ij = 1 if τ j ≥ τ i and R ij = 0 if τ j &lt; τ i , based on N training samples per batch. This represents a non-linear Cox proportional hazard model (cf. to <ref type="bibr" target="#b107">[Lou, 2019]</ref>). The fingerprint reconstruction loss term is defined as the mean squared error between fingerprint z and reconstructed fingerprint z = d λ (e κ (z):</p><formula xml:id="formula_51">L rec (κ, λ) = 1 N N i=1 z i -d λ (e κ (z i ) 2 . (6.5)</formula><p>The two modules of fingerprint extractor and survival predictor are trained in 2 steps. First, the motion fingerprint is trained alone and afterwards the survival predictor while fixing the motion fingerprint network. This keeps the motion fingerprint independent of the survival analysis and allows for example the training on additional data for the motion extraction where no survival data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments</head><p>In the experiments, we used a non-ischemic cohort of 167 HF patients with clinical criteria (low LVEF) for primary prevention ICD. These cases were collected from 3 different sites.</p><p>We used 4 chamber-view cine-MRI which were taken prior to ICD implantation. For the preliminary experiments in this study, we used HF hospitalization as endpoint to evaluate the proposed outcome predictor. HF hospitalizations was defined as the time point when a patient came into hospital with a documented primary diagnosis related to heart failure. In future and once we get clearance for the data, we plan to add results for SCD risk prediction and other HF endpoints. Using HF hospitalization as endpoint, this cohort consisted of 36% of subjects (60 subjects) with event times and right censored data for the remaining ones. Besides the censored data of the endpoint, the following clinical features were used as comparison risk predictors in this study computed with standard clinical tools: graymass (GM), minimum and maximum LA index volume (VminI respectively VmaxI), LA strain rate during LV systole (SRmax), preatrial contraction (SpreA) and atrial contraction (SRA). These features were selected as they are associated for being predictive for HF hospitalization and SCD <ref type="bibr" target="#b68">[Issa, 2017;</ref><ref type="bibr" target="#b144">Rijnierse, 2017;</ref><ref type="bibr" target="#b69">Jablonowski, 2017]</ref>. From this cohort, 60 subjects (36%) experienced HF hospitalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Implementation Details</head><p>The 4 chamber-view cine MRI were resampled to an image size of 128 by 128 pixels with a spacing of 2.2 mm. The implementation of the fingerprint extractor followed the details in <ref type="bibr">[Krebs, 2020c]</ref>. We increased the latent dimensionality D to 64 motivated by the fact that 4 chamber view images contain more complex motion details than the LV motion alone. The survival predictor requires motion fingerprints z to have the same size for all patients. In order to retrieve same sized z, we interpolated the cine-MRI in temporal dimension to retrieve a fixed time length T . In this work, we used T = 25 as it represents the average sequence length in this cohort. We applied B-spline interpolation for resampling the image sequences that contained less or more than 25 frames. The Gaussian deformation field regularization was applied with σ G = 3mm and σ T = 1.5. The weighting factor between reconstruction and KL loss terms has been chose empirically as ι = 6 • 10 -4 .</p><p>In total, the neural network of the risk predictor contained 5 fully-connected layers. The encoder e κ consisted of two consecutive layers with 180 and respectively 10 units whose output created the latent space. On the one side, the decoder d λ used the latent code and applied two layers to retrieve the reconstructed fingerprint vector z . On the other side, a single dense layer was used to extract the scalar risk score r from the latent code. The output layer of the risk network r ν had a tanh activation function while the decoder's output did not apply an activation function. The remaining layers used relu activation functions. Furthermore, a dropout factor of 0.3 has been applied on the input layer. Dropout factor and number of units of the fully-connected layers were determined by a hyperparameter search using evolutionary optimization. The model has been trained using the Adam optimizer <ref type="bibr">[Kingma, 2014a]</ref> with a learning rate of 0.0001 and batch size of 16. The framework has been implemented using Keras <ref type="bibr" target="#b29">[Chollet, 2015]</ref> and Tensorflow <ref type="bibr" target="#b4">[Abadi, 2016]</ref>.</p><p>Tab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Results</head><p>We evaluated our risk prediction model in comparison to the other clinical factors by fitting linear univariate and multivariate proportional hazard Cox models <ref type="bibr">[Cox, 1972]</ref>. We used 6 fold stratified cross-validation, first for training the fingerprint extractor and second for the Cox models. The fingerprint extractor has been trained first, in a risk independent fashion. The extracted motion for 2 example cases, 1 with HF hospitalization event and one without, can be seen in the appendix 6.5.1. In Fig. <ref type="figure">6</ref>.3 of the appendix, we further compared the motion model with the 4D Elastix algorithm <ref type="bibr" target="#b118">[Metz, 2011]</ref> in terms of matching of intensities and deformation regularity.</p><p>For risk prediction, we report the mean concordance index (C) <ref type="bibr" target="#b62">[Harrell, 1982]</ref> over the 6 folds and compute hazard ratios (HR) including confidence intervals (CI) and statistical p-value by splitting all test results by their medium risk value, dividing the cohort in a low and high risk group. For the Cox analysis and HR computation, the python package lifelines <ref type="bibr" target="#b40">[DavidsonPilon, 2020]</ref> has been used.</p><p>In Table <ref type="table">6</ref>.1, the results for the Cox analysis are shown using the different clinical features and the fingerprint risk score independently. The last two rows in table 6.1 show multivariate Cox analysis results for the joint predictive power of the best-performing combination of clinical features and the combination of these clinical features and the fingerprint risk. We tested all combinations of the 6 clinical features and show only the best combination here denoted by Best Clinical Params. In terms of testing results on C and HR scores, this best combination was found to contain VminI, VmaxI and SpreA features. In case of multivariate models, the linear Cox model showed signs of over-fitting by resulting in much better training but worse testing scores.</p><p>With an HR of 2.93 (CI 2.05-4.18) and an C-index of 0.69 (CI 0.60-0.72), the novel fingerprint risk score extracted from the motion model shows the highest prediction accuracy as independent predictor of HF hospitalization. In combination with the best clinical features, the multivariate Cox analysis showed an improved cross-validated C-index of 0.70 (CI 0.63-0.75) and HR of 3.02 .</p><p>In Fig. <ref type="figure">6</ref>.2, we further show the Kaplan-Meier plots of 4 independent features and the 2 feature combinations. Kaplan-Meier estimates can be used to measure the fraction of subjects living for a certain amount of time <ref type="bibr" target="#b60">[Goel, 2010]</ref>. One can see the capability of different models to recognize low and high risk patients by analyzing the distance between high and low risk survival curves in the Kaplan-Meier plot. We split our cohort into low and high risk groups according to the median risk prognosticated by the Cox model (for test cases). It is shown that for example the gray mass (GM) is not a good predictor since low and high risk survival curves are highly overlapping. The best differentiation between both groups (characterized by a large gap between the survival lines) can be seen for the fingerprint risk score and the combination of fingerprint and clinical features. These results indicate that the proposed risk predictor based on an extracted motion fingerprint can more accurately predict HR hospitalization than other commonly used clinical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion and Conclusions</head><p>In this work, we have proposed a novel image-driven risk predictor for personalized survival analysis by using a learned motion fingerprint -a low-dimensional encoding of the motion from a sequence of images. The proposed method showed promising first results in terms of predicting the risks for hospitalization of HF patients. These findings could be the first step to lead to a better patient selection for ICD treatment.</p><p>Besides HF hospitalization, we plan to add other endpoints such as SCD to this study. Furthermore, the authors think, the performance could be further improved by adding more features to the risk prediction network. One possible way could be by complementing the motion fingerprint with a cardiac structure fingerprint, extracted in a similar fashion as the motion fingerprint from for example late gadolinum enhancement images. The authors think that if multivariate models are used (combination of multiple clinical and motion features) the experienced over-fitting might be resolved by using a bigger dataset for fitting the linear Cox proportional hazard model or by using sparse estimation of Cox proportional hazards models that select the most relevant features in the survival prediction such as in <ref type="bibr" target="#b49">[Evers, 2008;</ref><ref type="bibr" target="#b163">Su, 2016]</ref>.</p><p>The two modules of fingerprint extractor and survival predictor can be also trained in an end-to-end fashion where all loss terms in Eq. 6.1 and Eq. 6.3 are combined in a single weighted loss function as for multitask training. In this way, the motion fingerprint is fine-tuned for personalized outcome risk prediction. However, the weighting between the different loss terms is more difficult and additional data for training the fingerprint extractor is not easily usable.</p><p>In future work, the model's lack of interpretability could be explored. As the model already contains two clearly separated modules of motion fingerprint and risk predictor it would be interesting to see which features are especially used and relevant for predicting the HF risks. Another possible future direction is to investigate the neural network features in depth from a clinical research perspective to potentially find unknown motion features that can be associated with HF or SCD.</p><p>6.5 Appendix  The bottom shows boxplots of registration accuracy and deformation regularity in comparison to the 4D elastix algorithm in terms of root mean square (RMSE), local cross-correlation (LCC), gradient of the determinant of Jacobian (Grad. Det. Jac.), spatial and temporal gradients of the deformation field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Detailed Derivations of the Fingerprint Extractor</head><p>Due to the fact that the used motion model in this chapter is different from the one presented in chapter 5 (e.g. not utilizing a Gaussian process prior), we add the full derivations here.</p><p>The following sections are based on the method section in <ref type="bibr">[Krebs, 2020c]</ref>.</p><p>The motion observed in an image sequence with T + 1 frames is typically described by deformation fields φ t between a moving image I 0 and the fixed images I t with t ∈ [1, T ]. Inspired by the probabilistic deformation model of <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>   on conditional variational autoencoder (CVAE) <ref type="bibr">[Kingma, 2014b]</ref>, we define a motion model for temporal sequences. The model is conditioned on the moving image and parameterizes the set of diffeomorphisms φ t in a low-dimensional probabilistic space, the motion matrix z ∈ R D× T , where D is the size of the deformation encoding per image pair adn T = T -1. Each column's z t -code corresponds to the deformation φ t .</p><p>To take temporal dependencies into account, z t is conditioned on all past and future time steps. To learn this temporal regularization directly from data, we apply Temporal Convolutional Networks <ref type="bibr" target="#b13">[Bai, 2018]</ref> with explicit time dependence and temporal dropout sampling enforcing the network to fill time steps by looking at given past and future deformations. An illustration of the model is shown in Fig. <ref type="figure">6</ref>.4a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Motion Model</head><p>Our motion model consists of three distributions. First, the encoder q ω (z t |I 0 , I t ) maps each of the image pairs (I 0 , I t ) independently to a latent space denoted by zt ∈ R d . Second, as the key component of temporal modeling, these latent vectors zt are jointly mapped to the motion matrix z by conditioning them in all past and future time steps and on the normalized time t: p γ (z|z 1:T , t1:T ). Finally, the decoder p θ (I t |z t , I 0 ) aims to reconstruct the fixed image I t by warping the moving image I 0 with the deformation φ t . This deformation φ t is extracted from the temporally regularized z t -codes. The decoder is conditioned on the moving image by concatenating the features at each scale with down-sampled versions of I 0 .</p><p>The distributions q ω , p γ , p θ are approximated by three neural networks with trainable parameters ω, γ, θ. During training, a lower bound on the data likelihood is maximized with respect to a prior distribution p(z t ) of the latent space zt (cf. CVAE <ref type="bibr">[Kingma, 2014b]</ref>). The prior p(z t ) is assumed to follow a multivariate unit Gaussian distribution with spherical covariance I: p(z t ) ∼ N (0, I). The objective function results in optimizing the expected log-likelihood p θ and the Kullback-Leibler (KL) divergence enforcing the posterior distribution q ω to be close to the prior p(z t ) for all time steps: <ref type="bibr">.6)</ref> Unlike the traditional CVAE model, the temporal regularized z t -code is used in the loglikelihood term p θ instead of the zt . We model p θ as a symmetric local cross-correlation Boltzmann distribution with the weighting factor λ. Encoder and decoder weights are independent of the time t. Their network architecture consists of convolutional and deconvolutional layers with fully-connected layers for mean and variance predictions in the encoder part <ref type="bibr">[Kingma, 2014b]</ref>. We use an exponentiation layer for the stationary velocity field parameterization of diffeomorphisms <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>, a linear warping layer and diffusion-like regularization with smoothing parameters σ G in spatial and σ T in temporal dimension.</p><formula xml:id="formula_52">T t=1 E zt∼pγ (•|z 1:T , t1:T ) log p θ (I t |z t , I 0 ) -KL [q ω (z t |I 0 , I t ) p(z)] . (<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Convolutional Networks with Explicit Time Dependence</head><p>Since the parameters of encoder q ω and decoder p θ are independent of time, the temporal conditioning p γ plays an important role in merging information across different time steps. In our work, this regularization is learned by Temporal Convolutional Networks (TCN). Consisting of multiple 1-D convolutional layers with increasing dilation, TCN can handle input sequences of different lengths. TCN have several advantages compared to recurrent neural networks such as a flexible receptive field and more stable gradient computations <ref type="bibr" target="#b13">[Bai, 2018]</ref>.</p><p>The input of the TCN is the sequence of z concatenated with the normalized time t = t/T . Providing the normalized time explicitly, provides the network with information on where each z is located in the sequence. This supports the learning of a motion model from data representing the same type of motion with varying sequence lengths. The output of the TCN is the regularized motion matrix z. We use non-causal instead of causal convolutional layers to also take future time steps into account. We follow the standard implementation using zero-padding and skip connections. Each layer contains d filters. A schematic representation of our TCN is shown in Fig. <ref type="figure">6</ref>.4b. For cyclic sequences, one could use a cyclic padding instead of zero-padding, for example by linking zT to z0 . However, in case of cardiac cine-MRI, one can not assume the end of a sequence coincides with the beginning as 5-10% of the cardiac cycle are often omitted <ref type="bibr" target="#b20">[Bernard, 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training with Temporal Dropout Sampling</head><p>Using Eq. 6.6 for training could lead to learning the identity transform z ≈ z in the TCN p γ such that deformations of the current time step are independent of past and future time steps. To avoid this and enforce the model to search for temporal dependencies during the training, we introduce the concept of temporal dropout sampling (TDS). In TDS, some of the zt are sampled from the prior distribution p(z) instead of only sampling from the posterior distribution q ω (z t |I 0 , I t ) as typical for CVAE. At the time steps the prior has been used for sampling, the model has no knowledge of the target image I t and is forced to use the temporal connections within the TCN in order to minimize the objective.</p><p>More precisely, at each time step t, a sample from the prior distribution zprior t ∼ p(z t ) is selected instead of a posterior sample zpost t ∼ q ω (z t |I 0 , I t ) using a binary Bernoulli random variable r t . All independent Bernoulli random variables r ∈ R T have the success probability δ. The latent vector zt can be defined as:</p><formula xml:id="formula_53">zt = r t * zprior t + (1 -r t ) * zpost t .</formula><p>(6.7) Fig. <ref type="figure">6</ref>.4c illustrates the TDS procedure. At test time, for each time step independently, one can either draw zt from the prior or take the encoder's prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details and Training of the Fingerprint Extractor</head><p>The encoder q ω consisted of 4 convolutional layers with strides (2, 2, 2, 1) and dense layers of size D for mean and variance estimation of the VAE. The TCN consisted of four 1-D convolutional layers with dilations (1, 2, 4, 8), same padding, a kernel size of 3 and skip connections (cf. Fig. <ref type="figure">6</ref>.4b). The decoder p θ had 3 deconvolutional and 1 convolutional layer before the exponentiation and warping layers (Fig. <ref type="figure">6</ref>.4a). The loss weighting factor λ was chosen empirically as 6 • 10 4 . The dropout sampling probability δ was 0.5. We applied a first-order gradient-based method for stochastic optimization In this thesis, we presented computational frameworks for the analysis of medical image pairs and image sequences. We built upon state-of-the-art methods for designing accurate and reliable registration and motion analysis tools that can be applied in clinical research by facilitating diagnosis, prognosis and therapy of diseases.</p><p>The proposed methods utilize recent machine learning methods showing high computational efficiency. Furthermore, the use of artificial intelligence (AI) in this work demonstrates how powerful compact models can be learned from large datasets of images.</p><p>The developed tools were designed to find application in difficult inter-subject registration and in intra-subject motion tracking scenarios. For the latter, compact deformation and motion models from sequential images were proposed that enable a variety of analysis tools to quantify and compare deformations. The proposed algorithms were tested on publicly available datasets allowing to benchmark and compare results. While the first 5 chapters are intended for a broader range of applications focusing on the technical contributions of this thesis, in chapter 6, one potential clinical application is shown. It is demonstrated how the proposed motion model can directly support prognosis and therapy planning by predicting the survival risk of patients suffering from heart failure (HF). This could allow for a better patient selection for available therapies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Main Contributions</head><p>In chapter 3, we proposed a generic learning-based framework using an artificial agent for difficult inter-subject registration tasks appearing in organ-focused non-rigid image fusion and atlas-based segmentation. The proposed method overcomes limitations of traditional algorithms by learning optimal features for registration. Inspired, by deep reinforcement learning the registration problem was reformulated as the iterative optimization of deformation parameters through an artificial agent. Hereby, the agent (a neural network) optimized the parameters of a simple statistical deformation model (SDM) learned from data. In an iterative fashion, the optimal transformation parameters were approached on a trajectory of small deformations. To restrict the agent to a set of reasonable transformations, fuzzy action control has been introduced which sets limits to the parameters of the SDM. During training, a novel ground-truth generator was used. This generator relied on simulated deformations from an SDM and a few ground-truth inter-subject deformation fields that were enhanced by segmentations. We showed that the agent-based approach trained with data from the novel ground-truth generator outperformed three state-of-the art registration algorithms in terms of structure overlaps and distances.</p><p>We presented an unsupervised deformable registration approach that learns a lowdimensional probabilistic deformation model in chapter 4. The deformation model is based on a conditional variational autoencoder (CVAE). It not only allows for accurately registering two images but also for analyzing corresponding deformations efficiently by using a novel generative deformation encoding. In this encoded latent space, similar deformations are close to each other. This enables to cluster and simulate deformations for a given image. Furthermore, it provides a novel way of transporting deformations from one subject to another without requiring inter-subject registration. The model can be seen as a non-linear and richer generalization of a simple statistical deformation model such as PCA. The unsupervised method is based on variational inference. In addition, we introduced a novel exponentiation layer to make DL-based registration algorithms diffeomorphic utilizing the SVF parameterization. An extended version, allows to train the model in a multi-scale fashion which results in higher accuracy. We evaluated the approach on end-diastole to end-systole cardiac cine-MRI registration. In comparison to 3 state-of-the-art algorithms, our multi-scale model showed significantly improved registration accuracy and regularity. The latent encoding showed convincing generative and deformation transport capabilities and showed a 83% classification accuracy for differentiating 5 cardiac diseases.</p><p>Beyond pairwise registration, we proposed a probabilistic motion model in chapter 5. This model can be useful for spatio-temporal registration, temporal super-resolution, data augmentation, shorter acquisition times and other motion analysis tasks. Intrinsic motion patterns are encoded in a low-dimensional probabilistic space -the latent motion matrix -which allows for accurate tracking of structures, temporal interpolation, motion simulation and motion transport. The diffeomorphic motion model is trained as a temporal latent variable model utilizing a novel Gaussian process prior acting on the latent motion encoding and following the training principles of CVAEs. Applied on cardiac cine-MRI, our approach has shown state-of-the art registration accuracy and improved temporal and spatial deformation regularity in comparison to 3 state-of-the-art algorithms. These results indicated that the latent motion encoding helps to regularize the registration problem of image sequences. Besides, we demonstrated the model's applicability for motion analysis by simulating realistic motion patterns, by transporting the motion to simulate a pathology in a healthy case and by an improved motion reconstruction from sequences with missing frames.</p><p>In chapter 6, we presented how our low-dimensional motion model can be applied for risk estimation and disease outcome prediction in heart failure patients. We have proposed a neural network risk predictor based on a non-linear Cox regression loss to estimate different disease endpoints from a motion fingerprint. Hereby, the fingerprint (the motion matrix) was extracted by applying the motion model from the previous chapter to 4 chamber-view cine-MRI. We evaluated the risk predictor on a cohort of heart failure patients with known endpoints such as hospitalization and sudden cardiac death (SCD). We have shown that the risk score predicted from the motion fingerprint is the most predictive independent feature for survival in comparison to other clinical features that have been known to be independently predictive for HF endpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Perspectives and Future Applications</head><p>The proposed methods have proven to be accurate and suitable for the given applications in this thesis. However, one goal was to develop tools that are generalizable and applicable to other data and applications in medical image analysis. Therefore, we believe that the proposed tools could find further application for the study of registration and motion scenarios including different diseases, organs and imaging modalities. Due to the fact that the objective functions for the proposed deformation and motion model can include principally any differentiable similarity and regularization metric (as in traditional registration methods), it makes these models suitable to a large variety of applications including for example multi-modal registration. In addition, future work should focus on the interpretability of the proposed latent variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Motion Model for Cardiac Sequences from other Modalities</head><p>In a first step of generalization, one can think of applying the proposed motion model from chapter 5 to cardiac sequences from other modalities such as ultrasound or computer tomography images. Cardiac ultrasound (or echocardiography) is the most widely used and readily available imaging modality to assess cardiac function and structure. We already conducted preliminary experiments that show the applicability of our method to a publicly available database of cardiac ultrasound image sequences, the Echonet <ref type="bibr" target="#b130">[Ouyang, 2020]</ref>. This dataset contains 10.030 ultrasound videos. We extracted approximately one cardiac sequence given the annotated ED and ES frames. We followed the given division in 7550 training, 1287 validation, 1275 test splits and trained our motion model with the same hyperparameters as described for the cine-MRI. In Fig. in Fig. <ref type="figure">7</ref>.2. These first preliminary results are promising and suggest that the motion model is also applicable to echocardiography sequences of the heart without major adjustments or modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Interpretability and Causability in Deep Latent Variable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>In PCA, the different variables have a clear structure and the first components are often interpretable: they describe the direction of the largest possible variance of the data and each succeeding component has the highest possible variance while being orthogonal to the preceding components. On the other hand deep latent variable models, such as the ones presented in this thesis, do not offer equivalent means of interpretation for the individual latent variables. By making assumptions of certain prior distributions such as multivariate Gaussians, the latent variables are enforced to be more structured compared to standard autoencoder networks in the sense that similar data points are close to each other in the latent space <ref type="bibr" target="#b81">[Kingma, 2019]</ref>. This allows for example to interpolate between data points. However, a deeper interpretability of the latent variables is not available.</p><p>In future work, it is desirable to investigate the interpretability and causability of latent variable models as this can lead to an improved understanding of the latent encoded motion model and its reasoning <ref type="bibr" target="#b66">[Holzinger, 2019]</ref>. The goal is to not only provide a model that can be used for the tasks tackled in this manuscript but to also understand (in a human explainable way) which features and which characteristics of the images and its deformations are the most important ones for example for predicting disease outcomes. Providing such explainable decisions would make it easier for physicians to trust deep latent variable models and AI-based algorithms in general. A simple way to improve explainability is by looking at the model's feature maps and find distinctive patterns between different pathologies. In addition, it could be helpful to study the model's attention using saliency maps <ref type="bibr" target="#b159">[Simonyan, 2013]</ref>.</p><p>While looking at the network's attention, gives more insights about which parts of the data lead to a certain prediction, a more clinically motivated future direction is to integrate known features into the latent space. In terms of cardiac motion, one could incorporate classical clinical features such as ejection fraction or strain values and thus, enhance interpretability. For risk prediction, the inclusion of clinical features at different stages of a standard autoencoder network has been preliminary investigated by Ji et al. <ref type="bibr" target="#b73">[Jin, 2019]</ref>.</p><p>In another possible approach, one could think of interpreting all or some latent variables as the parameters of a known biomechanical model. In this case, the decoding part to retrieve dense deformation fields could be replaced by the biomechanical model and the motion model would predict optimal parameter values for the biomechanical model solely from a pair or sequence of given images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Beyond Predicting Heart Failure Disease Outcomes</head><p>We have shown the usefulness of the motion model for predicting disease outcomes such as hospitalization and SCD for heart failure patients. However, this is only the first step in the automatic image-driven feature analysis. The proposed risk prediction model and possible variants (e.g. using end-to-end learning) could be useful to identify and reveal unknown clinical features that are significantly predictive for disease outcomes such as the ones mentioned above. Closely related to the interpretability and causability of the model as mentioned above, these features could be extracted by introspection -by analyzing the neural networks' behavior in disease-specific cases. Besides, the diagnosis and prognosis for a single patient, this could impact clinical research directly and lead to a better understanding of heart failure and related risks.</p><p>Moreover, multiple other heart diseases that have been associated with an impaired cardiac motion could benefit from the proposed risk model. One example is pulmonary hypertension which is characterized by right ventricular dysfunction <ref type="bibr" target="#b52">[Farber, 2004]</ref>.</p><p>Here, an image-based automatic feature retrieval could also reveal new unknown cardiac motion factors that influence the disease. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Deformation Model for Studying Neurodegenerative Diseases</head><p>The thesis focused on the development of a deformation and motion model of the heart from images of the same patient determining intra-subject deformations. While this is helpful for analyzing moving body organs, one future direction could be to learn a deformation model across patients depicting inter-subject deformations. Such a model could be useful for determining disease progression in a patient. An active research topic that comes to mind is the analysis of neurodegenerative diseases. The progress of Alzheimer's disease and aging are known to cause morphological changes in the human brain <ref type="bibr" target="#b149">[Rosen, 2003;</ref><ref type="bibr" target="#b128">Ohnishi, 2001]</ref>. These changes can be extracted by image registration of a subject's brain MRI to a template. In combination with a learned template model (e.g. <ref type="bibr">[Dalca, 2019b]</ref>), a low-dimensional generative deformation model could provide novel insights in the analysis of brain aging and neurodegenerative diseases. Further on, one could potentially predict the disease progression in a patient by comparing the evolution of healthy and unhealthy brains <ref type="bibr" target="#b160">[Sivera, 2019;</ref><ref type="bibr" target="#b125">Nader, 2020]</ref> (cf. Fig. <ref type="figure">7</ref>.3). Thus, another way of applying the proposed motion model to neurodegenerative diseases is to learn the brain evolution in a patient. This could be done by learning a brain deformation model from longitudinal images where the temporal deformations depict structural brain changes over long time intervals rather than real-time organ motion as from the heart. Using such a low-dimensional temporal deformation model could help in characterizing the personalized disease progression in a patient and guide the therapy. However, modeling the more complex morphological changes in the brain may require an adaptation of the latent motion matrix in order to deal with this extra amount of deformation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.5">Respiratory Motion Model</head><p>Another potential area of application is the study of lung and respiratory motion. A crucial need in the analysis of images for lung diseases (such as CT) and for example for tumors in the abdomen is motion compensation <ref type="bibr" target="#b132">[Ozhasoglu, 2002]</ref>. In PET imaging for instance, respiratory motion causes artifacts in reconstructed images, which can lead to misinterpretations, imprecise diagnosis or the impairing of fusion with other modalities <ref type="bibr" target="#b142">[Reyes, 2007]</ref>. Often PET images need to be registered to CT images in order to map structural to functional images. In Fig. <ref type="figure">7</ref>.4, one can see the misalignment between CT and PET images induced from respiratory motion as the images were taken at different breathing states <ref type="bibr" target="#b26">[Callahan, 2014]</ref>. A comprehensive probabilistic motion model could help in compensating for these motion artifacts within a mono-modal sequence (CT) and allow for reliable multi-modal registration in a second step.</p><p>Furthermore, a compact motion model could help in the diagnosis and prognosis of lung diseases. It has been shown that an impaired lung function is associated with increased mortality rates <ref type="bibr" target="#b17">[Beaty, 1985]</ref>. The proposed motion model already demonstrated its capabilities for predicting disease outcomes from cardiac image sequences. Thus, we believe it can be also useful for motion-related lung diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.6">Deformation and Motion Modeling in Personalized Medicine</head><p>We conclude with a broader view on the positioning of this work in a long-term outlook.</p><p>In the past couple of years, machine-learning methods have been successfully applied to a wide variety of applications in medical image analysis <ref type="bibr" target="#b103">[Litjens, 2017]</ref>. Typically, these models are gathering experience from large databases in order to solve specific problems. The next logical step for personalized medicine is how to combine this specific knowledge to form something larger, a central system that is able to link information across applications. Already today, a physician has to take the patient's pre-existing conditions, his health history, his age and many other factors into account before reaching conclusions about diagnosis, prognosis and therapy. A system that helps in the analysis of the increasingly growing amount of information can be highly beneficial in the healthcare of tomorrow. Building a supporting system that combines a collection of computational models describing and simulating the human body of a patient has been termed virtual patient or digital twin. While such a model goes far beyond medical image analysis as it involves basically all available data of a patient, medical images, most certainly, will still be of crucial importance.</p><p>The models presented in this thesis are already designed to extract relevant information from medical images and create meaningful compact representations or task-specific fingerprints using modern machine learning techniques. Furthermore, we have shown that these personalized fingerprints enable a variety of analysis tasks such as predicting possible outcomes or simulating diseases. Thus, we believe that such personalized fingerprints of a patient can play an important role in the creation of a comprehensive digital twin. In terms of deformation and motion models, one could think of learning an ensemble of organ-specific and/or disease-specific models. This ensemble of fingerprints, could then be one part of the virtual patient helping and supporting the patient's health during all stages of the clinical workflow and whenever necessary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.8</head><p>Transporting the motion matrix z from one subject and combining it with the enddiastolic frame of another subject allows for simulating a disease (dilated myopathy, DCM, red motion) in a healthy subject and vice versa (green motion). Ejection fraction (EF) of the simulated cases are more similar to the transported motion. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.1</head><p>The outcome risk prediction model consisting of learning a motion fingerprint from </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 1. 3</head><label>33</label><figDesc>Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Objectives and Organization of the Thesis . . . . . . . . . . . . . . . . Publications and Awards . . . . . . . . . . . . . . . . . . . . . . . . . 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 1 :</head><label>21</label><figDesc>Fig. 2.1: The registration process: the moving image M is matched to the fixed image F by applying the deformation field φ. (MR image origin [Bernard, 2018])</figDesc><graphic coords="26,123.43,94.04,96.97,72.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 . 2 :</head><label>22</label><figDesc>Fig. 2.2: The supervised end-to-end registration model. During training, the known deformation field φ GT of an image pair (F, M ) is regressed (red arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 . 3 :</head><label>23</label><figDesc>Fig. 2.3: The unsupervised end-to-end registration model. During training, the similarity D between warped moving and fixed image are optimized together with a deformation regularizer R (red arrows).</figDesc><graphic coords="32,95.67,150.20,53.34,53.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.1: (a) Training Data Generation: Synthetic deformations (blue arrows) and inter-subject GT deformations (black) are used for intra-(green) and inter-subject (red) image pairs for training. (b) Dual-stream network used for Q-value prediction y a including complete single-stage Markov Decision Process for testing (blue background).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(a) 2</head><label>2</label><figDesc>-D: Moving, Fixed, elastix-e8 (.84), elastix-e16 (.70), ours (.94). (b) 3-D: Moving, Fixed, elastix-e8 (.49), elastix-e16 (.59), LCC-Demons (.67), ours (.79).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 . 2 :</head><label>32</label><figDesc>Fig. 3.2: 2-D and 3-D registration results of extreme cases with segmentation masks overlays (fixed: green, moving: orange) and DICE scores in parenthesis.</figDesc><graphic coords="42,92.13,169.55,415.24,66.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>using Bspline spacing of 8 and 16 pixels. We found that better rigid registration can significantly improve the algorithm's performance as shown in the experiments with perfect rigid alignment according to the segmentation (3-D*). Extreme results are visually shown in Fig. 3.2. More 2-D and 3-D examples are shown in the appendix, Fig. 3.3-3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 . 3 :Fig. 3 . 4 :Fig. 3 . 5 :</head><label>333435</label><figDesc>Fig. 3.3: 2-D registration results showing moving and fixed image masks as overlays in orange and green respectively. Final DICE scores for the 3 cases are .90, .93, .92 with initial overlaps of .65, .70, .72.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 4.1: (a) Generative process for registration representing the likelihood of the fixed image F given the latent variable vector z and moving M : p θ (F |z, M ), where ω and θ are fixed parameters. (b) Generative process for regularized image registration where the likelihood depends on the regularized velocities p θ (F |v * , M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 . 2 :</head><label>42</label><figDesc>Fig. 4.2: (a) Probabilistic multi-scale registration network based on a CVAE. An encoder maps deformations to latent variables z ∈ R d (with for example d = 32) from which a decoder extracts velocities and diffeomorphisms at different scales while being conditioned on the moving image M . (b) After training, the decoder network can be also used to sample and transport deformations: Apply z-code on any new image M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 . 3 :</head><label>43</label><figDesc>Fig. 4.3: Boxplots of registration results comparing the undeformed (Undef) case to the different algorithms: lcc-demons (Dem), SyN, voxelmorph (VM) and our single scale (S1) respectively multi-scale (S3) using RMSE, gradient of the determinant of the Jacobian, DICE scores (logit-transform) and Hausdorff distances (HD in mm). Mean values are denoted by red bars. Higher values are better.</figDesc><graphic coords="61,91.14,62.74,119.89,102.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 4 . 4 :</head><label>44</label><figDesc>Fig. 4.4: (a) Qualitative registration results showing a pathological (hypertrophy) and a normal case. Warped moving image M * , displacements u, warped moving image with grid overlay and Jacobian determinant are shown for LCC-demons (Dem), SyN, voxelmorph (VM) and our approach using 3 scales (Our S3). (b) Middle and coarse scale predictions of our multi-scale method (Our S3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 4 . 5 :</head><label>45</label><figDesc>Fig. 4.5: Cardiac structures used only for measuring registration accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 4 . 6 :</head><label>46</label><figDesc>Fig. 4.6: Showing the influence of the latent vector size d on the registration accuracy in terms of DICE and Hausdorff distances in mm of the different anatomical structures with the mean of all structures shown in the grey boxes. The performance of the LCC-demons (Dem) is shown as reference with dashed lines.</figDesc><graphic coords="65,136.61,211.31,252.20,129.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 4 . 7 :</head><label>47</label><figDesc>Fig. 4.7: Cardiac disease distribution after projecting the latent variables z of the test images on a 2-D CCA (canonical correlation analysis) space. Using an 8-D CCA and applying SVM with 10-fold cross-validation leads to a classification accuracy of 83%</figDesc><graphic coords="66,124.32,32.24,363.31,272.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 4 . 8 :</head><label>48</label><figDesc>Fig. 4.8: Reconstruction of simulated displacements and an accordingly warped random test image after generating z-codes by equally sampling along the two largest principal components within a range of ±2.5 sigma around their mean values (red box). The PCA was fitted using all training z-codes. The blue box indicates the image closest to the identity deformation. One can see that the horizontal eigenvalue influences large deformations while the vertical eigenvalue focuses on smaller ones, for example the right ventricle.</figDesc><graphic coords="67,88.13,63.78,206.14,206.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 4 . 9 :</head><label>49</label><figDesc>Fig. 4.9: Transport pathological deformation predictions (Step 1, hypertrophy HCM, myopathyDCM) to healthy (Normal) subjects by using the pole ladder (with LCC-demons) and our probabilistic method (Step 2). Note that the pole ladder algorithm requires the registration between pathological and normal subjects while our approach is able to rotate and translate deformations encoded in the latent space z automatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 4 .Fig. 4 .Fig. 4 . 2 )Fig. 4 .</head><label>44424</label><figDesc>Fig. 4.10: Qualitative registration results showing a dilated cardiomyopathy (DCM) and a hypertrophic cardiomyopathy (HCM) case. Warped moving image M * , displacements u, warped moving image with grid overlay and Jacobian determinant are shown for LCC-demons (Dem), SyN, voxelmorph (VM) and our approach using 3 scales (Our S3).</figDesc><graphic coords="71,340.27,544.82,52.58,51.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.1: (a) Generative process for the motion model representing the likelihood of fixed images I 1:T given the latent variables z and moving image I 0 : p θ (I 1:T |z, I 0 ), where ω and θ are fixed parameters and arrows denote dependencies between random variables. (b) Visualization of the covariance matrix Σ of the Gaussian prior p(z) with 5 latent dimensions, a sequence time length of 35 and a length scale of the Cauchy kernel of 7.</figDesc><graphic coords="80,296.82,57.95,252.65,189.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 5 . 2 :</head><label>52</label><figDesc>Fig. 5.2: Overview of the motion model including encoder and decoder neural networks. From sequential image pairs, temporally independent feature vectors γ t are extracted which are fed to a temporally convolutional network (TCN) to obtain the probabilistic motion matrix z. This compact representation is decoded to a sequence of diffeomorphic deformation fields φ t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 5 . 3 :</head><label>53</label><figDesc>Fig. 5.3: (a) The temporal convolutional network (TCN) allows for temporal regularization of the independently extracted features γ t per time step t, for retrieving mean vector µ and variance vector σ of the posterior distribution p θ . (b) Sequences with missing time steps (motion interpolation or simulation) are encoded by a full feature matrix Γ by setting the columns of missing time steps to zero. The TCN handles these missing columns and still predicts a full temporal motion sequence of T time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 5 . 4 :</head><label>54</label><figDesc>Fig. 5.4: Tracking results showing RMSE, spatial and temporal gradients of the displacement fields, DICE scores and Hausdorff distances for all 2D+T test sequences. The LV volume curves extracted from the warped ED blood pool masks for 2 random test cases in ml, show the temporal smoothness and the distance to the ground-truth ED and ES volumes (marked with black points).The proposed algorithm (Our) shows slightly higher registration accuracy and temporally smoother deformations than the state-ofthe-art algorithms: SyN<ref type="bibr" target="#b7">[Avants, 2008]</ref>, LPR<ref type="bibr" target="#b87">[Krebs, 2019b]</ref>, 4D-Elastix<ref type="bibr" target="#b118">[Metz, 2011]</ref> and the previous version of our method without GP prior (No-GP[Krebs, 2020c]).</figDesc><graphic coords="88,314.07,557.55,189.50,71.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 5 . 5 :</head><label>55</label><figDesc>Fig. 5.5: Showing 2D+T and 3D+T tracking results of the warped moving image I 0 with grid overlay and the Jacobian determinant (Det.-Jac.) for a test sequence. In 3D+T, smoother Jacobian determinants were obtained.</figDesc><graphic coords="89,173.37,329.77,60.94,61.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Tab. 5 . 2 :</head><label>52</label><figDesc>3D+T registration performance with mean and standard deviation scores of RSME, DICE, Hausdorff Distance (HD), spatial and temporal gradients of the deformation fields comparing the undeformed case (Undef), 4D-Elastix and the proposed method. 79.2 ±10 5.1 ±2.1 0.15 ±0.06 0.62 ±0.32 Our 0.16 79.5 ±09 5.4 ±2.1 0.07 ±0.02 0.09 ±0.03</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 5 . 6 :</head><label>56</label><figDesc>Fig. 5.6: First 5 latent dimensions of the same test sequence shows a temporally smoother motion matrix z for the proposed model trained with the Gaussian process prior compared to the No-GP version.</figDesc><graphic coords="91,108.11,221.12,376.33,188.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 5 . 7 :Fig. 5 . 8 :</head><label>5758</label><figDesc>Fig. 5.7: Predicted simulated and interpolated motion from a limited number of frames. Provided frames are decreasing from all frames to only the 0th frame (full motion simulation). The volume errors with respect to the all frame prediction are compared with linear and cubic interpolation of the deformation fields. Two random test subjects are shown in the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fig. 6 . 1 :</head><label>61</label><figDesc>Fig. 6.1: The outcome risk prediction model consisting of learning a motion fingerprint from 4-chamber view cine-MRI (A.) and a survival predictor neural network (B.) which estimates the outcome risk based on the motion fingerprint. The dashed black arrows symbolize training loss computations while the blue arrows symbolize the data flow during testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 6 . 2 :</head><label>62</label><figDesc>Fig. 6.2: Kaplan-Meier plots showing the average survival risk and its confidence interval for low and high risk patients depending on different predictors: gray mass (GM), SRmax, VminI, motion fingerprint and multivariate risks using clinical and the combination fingerprint and clinical features. The motion fingerprint helps to differentiate between low and high risk patients.</figDesc><graphic coords="107,105.17,378.86,199.33,149.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Fig. 6 . 3 :</head><label>63</label><figDesc>Fig. 6.3: The motion fingerprint extractor is able to learn motion patterns from 4 chamber view cine-MRI. The motion between end-diastolic (ED) and end-systolic (ES) frames are shown for two subjects, one with future HF hospitalization event and one without. The bottom shows boxplots of registration accuracy and deformation regularity in comparison to the 4D elastix algorithm in terms of root mean square (RMSE), local cross-correlation (LCC), gradient of the determinant of Jacobian (Grad. Det. Jac.), spatial and temporal gradients of the deformation field.</figDesc><graphic coords="109,107.89,302.94,368.48,150.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Fig. 6 . 4 :</head><label>64</label><figDesc>Fig. 6.4: Probabilistic motion model (a): The encoder q ω projects the image pair (I 0 , I t ) to a low-dimensional deformation encoding zt from which the temporal convolutional network p γ (b) constructs the motion matrix z ∈ R d×T conditioned on the normalized time t. The decoder p θ maps the motion matrix to the deformations φ t . The temporal dropout sampling procedure (c) randomly chooses to sample zt either from the encoder q ω or the prior distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Fig. 7 . 1 :</head><label>71</label><figDesc>Fig. 7.1: After training the motion model on the Echonet dataset [Ouyang, 2020], an example test sequence with extracted tracking results, Jacobian determinants and motioncompensated images is shown.</figDesc><graphic coords="117,109.18,410.36,380.29,76.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Fig. 7 . 2 :</head><label>72</label><figDesc>Fig. 7.2: Transported Motion from a case with hihg ejection fraction (EF) to one case with low EF and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Fig. 7 . 3 :</head><label>73</label><figDesc>Fig. 7.3: The Alzheimer's disease (AD) is assumed to have a faster morphological degeneration(aging) than healthy people<ref type="bibr" target="#b160">[Sivera, 2019]</ref>.</figDesc><graphic coords="120,133.65,61.85,343.17,220.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Fig. 7 . 4 :</head><label>74</label><figDesc>Fig. 7.4: Registration misalignment for CT and PET images induced from respiratory motion. Different breathing states are illustrated, free-breathing (FB), maximal inspiration (Insp) and maximal expiration (Exp) [Callahan, 2014].</figDesc><graphic coords="121,98.30,62.10,381.26,261.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>5 . 6 First 5</head><label>565</label><figDesc>latent dimensions of the same test sequence shows a temporally smoother motion matrix z for the proposed model trained with the Gaussian process prior compared to the No-GP version. . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Predicted simulated and interpolated motion from a limited number of frames. Provided frames are decreasing from all frames to only the 0th frame (full motion simulation). The volume errors with respect to the all frame prediction are compared with linear and cubic interpolation of the deformation fields. Two random test subjects are shown in the bottom. . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>4 - 2 Transported</head><label>42</label><figDesc>chamber view cine-MRI (A.) and a survival predictor neural network (B.) which estimates the outcome risk based on the motion fingerprint. The dashed black arrows symbolize training loss computations while the blue arrows symbolize the data flow during testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6.2Kaplan-Meier plots showing the average survival risk and its confidence interval for low and high risk patients depending on different predictors: gray mass (GM), SRmax, VminI, motion fingerprint and multivariate risks using clinical and the combination fingerprint and clinical features. The motion fingerprint helps to differentiate between low and high risk patients. . . . . . . . . . . . . . . . . .6.3The motion fingerprint extractor is able to learn motion patterns from 4 chamber view cine-MRI. The motion between end-diastolic (ED) and end-systolic (ES) frames are shown for two subjects, one with future HF hospitalization event and one without. The bottom shows boxplots of registration accuracy and deformation regularity in comparison to the 4D elastix algorithm in terms of root mean square (RMSE), local cross-correlation (LCC), gradient of the determinant of Jacobian (Grad. Det. Jac.), spatial and temporal gradients of the deformation field. . . . .6.4Probabilistic motion model (a): The encoder q ω projects the image pair (I 0 , I t ) to a low-dimensional deformation encoding zt from which the temporal convolutional network p γ (b) constructs the motion matrix z ∈ R d×T conditioned on the normalized time t. The decoder p θ maps the motion matrix to the deformations φ t . The temporal dropout sampling procedure (c) randomly chooses to sample zt either from the encoder q ω or the prior distribution. . . . . . . . . . . . . . . . . . . .7.1After training the motion model on the Echonet dataset<ref type="bibr" target="#b130">[Ouyang, 2020]</ref>, an example test sequence with extracted tracking results, Jacobian determinants and motion-compensated images is shown. . . . . . . . . . . . . . . . . . . . . . . 7.Motion from a case with hihg ejection fraction (EF) to one case with low EF and vice versa. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="92,140.59,511.04,316.45,159.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>J. Krebs, H. Delingette, N. Ayache, and T. Mansi. Learning a Generative Motion Model from Image Sequences based on a Latent Motion Matrix. Submitted to IEEE Transactions on Medical Imaging. J. Krebs and T. Mansi. Method and System for Deep Motion Model Learning in Medical Images. U.S. Patent Application No. 16/131,465, 2020.    </figDesc><table><row><cell>Patent Applications</cell></row><row><cell>• [Krebs, 2020a] • [Krebs, 2019a] J. Krebs, H. Delingette, N. Ayache, T. Mansi and S. Miao. Medi-</cell></row><row><cell>cal Imaging Diffeomorphic Registration based on Machine Learning. U.S. Patent</cell></row><row><cell>Application No. 16/233,174, 2019.</cell></row><row><cell>• [Liao, 2017a] R. Liao, S. Miao, P. de Tournemire, J. Krebs, L. Zhang, B. Georgescu,</cell></row><row><cell>S. Grbic, F. C. Ghesu, V. K. Singh, D. Xu, T. Mansi, A. Kamen, and D. Comaniciu.</cell></row><row><cell>Method and System for Image Registration Using an Intelligent Artificial Agent.</cell></row><row><cell>U.S. Patent Application No. 15/587,094, 2017.</cell></row><row><cell>Statistical Atlases and Computational Models of the Heart. Multi-Sequence CMR</cell></row><row><cell>Segmentation, CRT-EPiggy and LV Full Quantification Challenges. Springer, 2020, pp.</cell></row><row><cell>176-185.</cell></row></table><note><p><p><p><p><p>• Risk Prediction for Heart Failure Outcomes from Cardiac Motion Features. In preparation for submission to a clinical journal.</p>Conference Papers</p>•</p><ref type="bibr" target="#b84">[Krebs, 2017]</ref> </p>J. Krebs, T. Mansi, H. Delingette, L. Zhang, F. Ghesu, S. Miao, A. Maier, N. Ayache, R. Liao, and A. Kamen. Robust non-rigid registration through agent-based action learning. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer,2017, pp. 344-352. • [Krebs, 2018] J. Krebs, T. Mansi, B. Mailhé, N. Ayache, and H. Delingette. Unsupervised Probabilistic Deformation Modeling for Robust Diffeomorphic Registration. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, 2018, pp. 101-109. • [Krebs, 2020c] J. Krebs, T. Mansi, N. Ayache, and H. Delingette. Probabilistic Motion Modeling from Medical Image Sequences: Application to Cardiac Cine-MR. • [Bacoyannis, 2019] T. Bacoyannis, J. Krebs, N. Cedilnik, H. Cochet, M. Sermesant. Deep Learning Formulation of ECGI for Data-driven Integration of Spatiotemporal Correlations and Imaging Information. International Conference on Functional Imaging and Modeling of the Heart. Springer, 2019, pp. 20-28. Awards • The Best Oral Presentation Award was awarded at the Statistical Atlases and Computational Models of the Heart STACOM (workshop in conjunction with MICCAI) 2019, Shenzhen, China. For the paper and talk: Probabilistic Motion Modeling from Medical Image Sequences: Application to Cardiac Cine-MR. • An excellence prize Prix d'excellence 2019 was awarded from the University Côte d'Azur, Nice, France. For the paper: Probabilistic Motion Modeling from Medical Image Sequences: Application to Cardiac Cine-MR.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.5 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30</figDesc><table><row><cell>Robust Registration for Difficult</cell><cell>3</cell></row><row><cell>Deformations by Agent-based</cell><cell></cell></row><row><cell>Action Learning</cell><cell></cell></row><row><cell>Contents</cell><cell></cell></row><row><cell cols="2">3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21</cell></row><row><cell cols="2">3.2 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22</cell></row><row><cell cols="2">3.2.1 Training Artificial Agents . . . . . . . . . . . . . . . . . . . . . 22</cell></row><row><cell cols="2">3.2.2 Statistical Deformation Model . . . . . . . . . . . . . . . . . . 24</cell></row><row><cell cols="2">3.2.3 Training Data Generation . . . . . . . . . . . . . . . . . . . . 25</cell></row><row><cell>3.3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>algorithm (dem). T are the initial scores after translation registration with elastix. 3-D* are results with perfect rigid alignment T*. nfc are our results with no fuzzy action control (HD in mm).</figDesc><table><row><cell></cell><cell>2-D</cell><cell></cell><cell></cell><cell></cell><cell>3-D</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">T e16</cell><cell cols="2">e8 our</cell><cell cols="2">T e16</cell><cell cols="2">e8 dem</cell><cell>our</cell></row><row><cell>DICE Mean</cell><cell>.76</cell><cell>.74</cell><cell>.77</cell><cell>.87</cell><cell>.62</cell><cell>.63</cell><cell>.64</cell><cell>.67</cell><cell>.75</cell></row><row><cell>DICE Med.</cell><cell>.78</cell><cell>.79</cell><cell>.81</cell><cell>.88</cell><cell>.61</cell><cell>.71</cell><cell>70</cell><cell>.67</cell><cell>.76</cell></row><row><cell>DICE StD.</cell><cell>.10</cell><cell>.15</cell><cell>.13</cell><cell>.05</cell><cell>.11</cell><cell>.22</cell><cell>.20</cell><cell>.11</cell><cell>.06</cell></row><row><cell>HD Mean</cell><cell cols="3">11.6 15.2 14.5</cell><cell cols="6">7.7 16.1 21.2 25.3 15.9 11.8</cell></row><row><cell>HD Med.</cell><cell cols="3">11.7 13.2 13.0</cell><cell cols="6">7.2 15.2 18.0 21.7 15.8 11.2</cell></row><row><cell>HD StD.</cell><cell>4.3</cell><cell>6.8</cell><cell>6.7</cell><cell>2.5</cell><cell cols="3">3.9 10.7 10.9</cell><cell>3.9</cell><cell>2.9</cell></row><row><cell></cell><cell>3-D*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">T* e16</cell><cell cols="2">e8 dem</cell><cell cols="2">nfc our</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DICE Mean</cell><cell>.74</cell><cell>.72</cell><cell>.67</cell><cell>.79</cell><cell>.79</cell><cell>.80</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DICE Med.</cell><cell>.75</cell><cell>.77</cell><cell>.76</cell><cell>.80</cell><cell>.79</cell><cell>.81</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DICE StD.</cell><cell>.08</cell><cell>.17</cell><cell>.23</cell><cell>.07</cell><cell>.05</cell><cell>.04</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HD Mean</cell><cell cols="4">9.2 13.4 14.5 10.4</cell><cell>8.9</cell><cell>8.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HD Med.</cell><cell cols="4">9.0 11.6 13.5 10.8</cell><cell>8.8</cell><cell>7.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HD StD.</cell><cell>2.3</cell><cell>6.8</cell><cell>6.4</cell><cell>2.5</cell><cell>2.2</cell><cell>1.9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.1.1 Deformable Image Registration . . . . . . . . . . . . . . . . . 34 4.1.2 Deformation Analysis and Transport . . . . . . . . . . . . . . 35 4.1.3 Learning-based Generative Latent Variable Models . . . . . . . 36</figDesc><table><row><cell>Learning a Probabilistic Model for</cell><cell>4</cell></row><row><cell>Diffeomorphic Registration</cell><cell></cell></row><row><cell>Contents</cell><cell></cell></row><row><cell>4.1</cell><cell></cell></row></table><note><p>4.1.4 Probabilistic Registration using a Generative Model . . . . . . 37 4.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.2.1 Probabilistic model for multi-scale registration . . . . . . . . . 39 4.2.2 Introducing regularization on velocities . . . . . . . . . . . . . 42 4.2.3 Network architecture . . . . . . . . . . . . . . . . . . . . . . . 45 4.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.4 Discussion and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . 53 4.5 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Registration performance with mean and standard deviation scores (in brackets) of RMSE, DICE, Hausdorff Distance (HD in mm) and the mean gradient of the determinant of Jacobians (Grad Det-Jac, ×10 -2 ) comparing the undeformed case (Undef), LCCdemons (Dem), SyN, voxelmorph (VM) and our method.</figDesc><table><row><cell>Method</cell><cell>RMSE</cell><cell>DICE</cell><cell cols="2">HD Grad Det-Jac</cell></row><row><cell>Undef</cell><cell cols="3">0.37 (0.17) 0.707 (0.145) 10.1 (2.2)</cell><cell>-</cell></row><row><cell>Dem</cell><cell cols="2">0.29 (0.16) 0.799 (0.096)</cell><cell>8.3 (2.7)</cell><cell>2.9 (1.0)</cell></row><row><cell>SyN</cell><cell cols="2">0.32 (0.16) 0.801 (0.091)</cell><cell>8.1 (3.6)</cell><cell>3.4 (0.5)</cell></row><row><cell>VM</cell><cell cols="2">0.24 (0.08) 0.790 (0.096)</cell><cell>8.4 (2.6)</cell><cell>9.2 (0.5)</cell></row><row><cell>Our S1</cell><cell cols="2">0.31 (0.15) 0.797 (0.093)</cell><cell>7.9 (2.6)</cell><cell>1.2 (0.3)</cell></row><row><cell>Our S3</cell><cell cols="3">0.30 (0.14) 0.812 (0.085) 7.3 (2.7)</cell><cell>1.4 (0.3)</cell></row></table><note><p>Tab. 4.1:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>5.2.    </figDesc><table><row><cell>Input Sequence</cell><cell></cell><cell cols="2">𝐼 0 𝐼 1</cell><cell>𝐼 0 𝐼 2</cell><cell cols="2">…</cell><cell>𝐼 0 𝐼 𝑇</cell></row><row><cell cols="3">Feature Extraction</cell><cell></cell><cell></cell><cell cols="2">…</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Shared</cell></row><row><cell cols="2">Encoder 𝑞 𝜔</cell><cell>TCN</cell><cell>𝛾 1</cell><cell>𝛾 2</cell><cell cols="2">…</cell><cell>𝛾 𝑇</cell></row><row><cell cols="3">Motion Matrix</cell><cell>𝑧 1</cell><cell cols="2">𝑧 2 (𝜇, 𝜎)</cell><cell>𝑧 𝑇</cell></row><row><cell cols="2">Decoder 𝑝 𝜃</cell><cell>𝐼 0</cell><cell cols="2">𝐼 0</cell><cell cols="2">𝐼 0 Shared …</cell></row><row><cell cols="3">Deformations Warped 𝐼 0</cell><cell>𝜙 1</cell><cell>𝜙 2</cell><cell cols="2">…</cell><cell>𝜙 𝑇</cell></row><row><cell>Conv.</cell><cell cols="2">Dense</cell><cell>Deconv.</cell><cell cols="2">Smooth.</cell><cell>Exponentiation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Registration performance with mean and standard deviation scores of DICE (in %), Hausdorff Distance (HD in mm), spatial and temporal gradients of the deformation fields (×10 -2 ) comparing the undeformed case (Undef), SyN, learning-based pairwise registration (LPR), 4D-Elastix, our previous version without GP prior (No-GP) and the proposed method for all 2D+T sequences.</figDesc><table><row><cell>Tab. 5.1: Method</cell><cell>DICE</cell><cell cols="3">HD Spat. Grad. Temp. Grad.</cell></row><row><cell>Undef</cell><cell cols="2">72.8 ±14 9.70 ±4.20</cell><cell>-</cell><cell>-</cell></row><row><cell>SyN</cell><cell cols="3">82.7 ±12 7.02 ±4.34 0.23 ±0.06</cell><cell>0.43 ±0.19</cell></row><row><cell>LPR</cell><cell cols="3">82.1 ±10 6.60 ±3.07 0.16 ±0.06</cell><cell>0.32 ±0.13</cell></row><row><cell cols="4">4D-Elastix 83.7 ±11 6.27 ±3.91 0.15 ±0.06</cell><cell>0.33 ±0.15</cell></row><row><cell>No-GP</cell><cell cols="3">84.6 ±10 6.24 ±3.30 0.14 ±0.08</cell><cell>0.15 ±0.08</cell></row><row><cell>Our</cell><cell cols="3">85.2 ±09 6.11 ±3.28 0.10 ±0.03</cell><cell>0.12 ±0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 6.2.1 Motion Fingerprint Extractor . . . . . . . . . . . . . . . . . . . 86 6.2.2 Survival Predictor . . . . . . . . . . . . . . . . . . . . . . . . . 87 6.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 6.3.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . 89</figDesc><table><row><cell>Risk Prediction for Heart Failure</cell><cell>6</cell></row><row><cell>Outcomes from Cardiac Motion</cell><cell></cell></row><row><cell>Features</cell><cell></cell></row><row><cell>Contents</cell><cell></cell></row><row><cell>6.1</cell><cell></cell></row><row><cell></cell><cell>5.5 Appendix</cell></row></table><note><p>6.3.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 6.4 Discussion and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . 91 6.5 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 6.5.1 Motion Fingerprint Extraction . . . . . . . . . . . . . . . . . . 94 6.5.2 Detailed Derivations of the Fingerprint Extractor . . . . . . . . 94</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>motion fingerprint is learned in an encoder-decoder neural network which represents a latent variable model. The input of the network are a sequence of image pairs (I 0 , I t ) with t ∈ [1, T ] from image sequences of length T . The output are a sequence of dense deformation fields φ t (between (I 0 , I t )) and a compact deformation representation z t ∈ R D of dimensionality D per timestep t. The sequence of encoded z t are combined in the motion matrix z ∈ R D× T with T = T -1 and D latent dimensions depicting the cardiac motion. In this work, we consider 2-dimensional image sequences of four chamber cine-MRI with a single slice. The model is trained using a conditional variational autoencoder (CVAE,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Best Clinical Params. 3.02 (2.11-4.32) 0.70 (0.63-0.75) &lt;0.005</head><label></label><figDesc>6.1: Predictors of HF hospitalization using univariate and multivariate (for Clinical and Fingerprint+Clinical) Cox proportional hazard models. The results are obtained via 6-fold stratified cross-validation. HR, p-value (reject the null hypothesis that the HR equals one) and average concordance index (C) are reported including a 95% confidence interval (CI) in brackets. The motion fingerprint shows the highest prediction accuracy, independently and together with multiple clinical variables.</figDesc><table><row><cell>Feature</cell><cell>HR</cell><cell cols="2">C p-value</cell></row><row><cell>GM</cell><cell cols="2">0.92 (0.64-1.32) 0.52 (0.38-0.55)</cell><cell>0.66</cell></row><row><cell>VmaxI</cell><cell cols="3">1.85 (1.31-2.60) 0.63 (0.55-0.69) &lt;0.005</cell></row><row><cell>VminI</cell><cell cols="3">2.03 (1.44-2.87) 0.66 (0.59-0.72) &lt;0.005</cell></row><row><cell>SRmax</cell><cell cols="3">2.30 (1.63-3.26) 0.65 (0.59-0.71) &lt;0.005</cell></row><row><cell>SRA</cell><cell cols="3">2.02 (1.43-2.85) 0.62 (0.55-0.68) &lt;0.005</cell></row><row><cell>SpreA</cell><cell cols="3">1.98 (1.41-2.79) 0.63 (0.56-0.69) &lt;0.005</cell></row><row><cell>Fingerprint</cell><cell cols="3">2.93 (2.05-4.18) 0.69 (0.60-0.72) &lt;0.005</cell></row><row><cell>Best Clinical Params.</cell><cell cols="3">2.39 (1.69-3.39) 0.67 (0.61-0.74) &lt;0.005</cell></row><row><cell>Fingerp. +</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Main Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 7.2 Perspectives and Future Applications . . . . . . . . . . . . . . . . . . . 101 Respiratory Motion Model . . . . . . . . . . . . . . . . . . . . 106 7.2.6 Deformation and Motion Modeling in Personalized Medicine . 107</figDesc><table><row><cell>Conclusion</cell><cell>7</cell></row><row><cell>Contents</cell><cell></cell></row><row><cell cols="2">7.1 7.2.1 Motion Model for Cardiac Sequences from other Modalities . 101</cell></row><row><cell cols="2">7.2.2 Interpretability and Causability in Deep Latent Variable Models103</cell></row><row><cell cols="2">7.2.3 Beyond Predicting Heart Failure Disease Outcomes . . . . . . 104</cell></row><row><cell cols="2">7.2.4 Deformation Model for Studying Neurodegenerative Diseases 105</cell></row><row><cell>7.2.5</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1.2 Objectives and Organization of the Thesis</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>1.3 Publications and Awards</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>2.2 Registration Algorithms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>2.4 Deep Learning-based Registration</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>3.2 Method</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_5"><p>https://promise12.grand-challenge.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_6"><p>https://wiki.cancerimagingarchive.net/display/Public/Prostate-3T</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_7"><p>https://lasagne.readthedocs.io/en/latest3.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>Chapter 3 Robust Registration for Difficult Deformations by Agent-based Action Learning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>3.4 Conclusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>3.5 Appendix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p>4.2 Methods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_12"><p>https://keras.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_13"><p>Qualitative registration results for all five diseases are also presented in Fig.10-12 (available in the supplementary files /multimedia tab).4.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_14"><p>http://www.simpleitk.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_15"><p>Visualization of the gradients of the Jacobian determinant are presented in Fig.13(available in the supplementary files /multimedia tab).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_16"><p>4.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_17"><p>The pipeline for the parallel transport experiment using the pole ladder algorithm is presented in Fig.14(available in the supplementary files /multimedia tab).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_18"><p>4.4 Discussion and Conclusions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_19"><p>4.5 Appendix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_20"><p>5.2 Methods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_21"><p>Chapter 5 Learning a Generative Motion Model from Image Sequences based on a Latent Motion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_22"><p>5.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_23"><p>5.4 Discussion and Conclusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_24"><p>6.2 Methods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_25"><p>6.3 Experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_26"><p>6.4 Discussion and Conclusions</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_27"><p>6.5 Appendix</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_28"><p>7.2 Perspectives and Future Applications</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head></div>
<div><head>Acknowledgments</head><p>Data used in preparation of this article were obtained from the <rs type="funder">EU</rs> <rs type="programName">FP7</rs>-funded project <rs type="projectName">MD-Paedigree</rs> and the ACDC STACOM challenge 2017 <ref type="bibr" target="#b20">[Bernard, 2018]</ref>. This work was supported by the grant <rs type="grantNumber">AAP Santé 06 2017-260 DGA-DSH</rs>, and by the <rs type="funder">Inria Sophia Antipolis -Méditerranée</rs>, "NEF" computation cluster. The authors would like to thank</p></div>
<div><head>Acknowledgments</head><p>Data used in this article were obtained from the <rs type="funder">EU</rs> <rs type="programName">FP7</rs>-funded project <rs type="projectName">MD-Paedigree</rs> and the ACDC STACOM challenge 2017 <ref type="bibr" target="#b20">[Bernard, 2018]</ref>. This work has been supported by the <rs type="funder">French government</rs>, through the <rs type="institution">3IA Côte d'Azur Investments</rs> in the <rs type="projectName">Future</rs> project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs> and the grant <rs type="grantNumber">AAP Santé 06 2017-260 DGA-DSH</rs>, and by the <rs type="funder">Inria Sophia Antipolis -Méditerranée</rs>, "NEF" computation cluster.</p></div>
			</div>
			<div type="funding">
<div><p>Furthermore, this work has been supported by the <rs type="funder">French government</rs>, through the <rs type="institution">3IA Côte d'Azur Investments</rs> in the <rs type="projectName">Future</rs> project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs> and the grant <rs type="grantNumber">AAP Santé 06 2017-260 DGA-DSH</rs>, and by the <rs type="funder">Inria Sophia Antipolis -Méditerranée</rs>, "NEF" computation cluster.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zGNP8h5">
					<idno type="grant-number">AAP Santé 06 2017-260 DGA-DSH</idno>
					<orgName type="project" subtype="full">MD-Paedigree</orgName>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_FdRH8rt">
					<orgName type="project" subtype="full">MD-Paedigree</orgName>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_rYVMnw7">
					<orgName type="project" subtype="full">Future</orgName>
				</org>
				<org type="funding" xml:id="_4KndCuc">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
				<org type="funding" xml:id="_nhsreeg">
					<idno type="grant-number">AAP Santé 06 2017-260 DGA-DSH</idno>
				</org>
				<org type="funded-project" xml:id="_9naAgTj">
					<orgName type="project" subtype="full">Future</orgName>
				</org>
				<org type="funding" xml:id="_GMTaQcX">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
				<org type="funding" xml:id="_AN39Zvw">
					<idno type="grant-number">AAP Santé 06 2017-260 DGA-DSH</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>The registration process: the moving image M is matched to the fixed image F by applying the deformation field φ. (MR image origin <ref type="bibr" target="#b20">[Bernard, 2018]</ref>) . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>The supervised end-to-end registration model. During training, the known deformation field φ GT of an image pair (F, M ) is regressed (red arrows). . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.9</head><p>Transport pathological deformation predictions (Step 1, hypertrophy HCM, myopathy DCM) to healthy (Normal) subjects by using the pole ladder (with LCC-demons) and our probabilistic method (Step 2). Note that the pole ladder algorithm requires the registration between pathological and normal subjects while our approach is able to rotate and translate deformations encoded in the latent space z automatically. Warped moving image M * , displacements u, warped moving image with grid overlay and Jacobian determinant are shown for LCC-demons (Dem), SyN, voxelmorph (VM) and our approach using 3 scales (Our S3). . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.13</head><p>The gradient of the determinant of the Jacobian of a random test case for LCCdemons (Dem), SyN, voxelmorph (VM) and our approach using 1 and respectively 3 scales (Our S1, Our S3). Our single-scale approach shows the most regular </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4</head><p>Tracking results showing RMSE, spatial and temporal gradients of the displacement fields, DICE scores and Hausdorff distances for all 2D+T test sequences. The LV volume curves extracted from the warped ED blood pool masks for 2 random test cases in ml, show the temporal smoothness and the distance to the ground-truth ED and ES volumes (marked with black points). The proposed algorithm (Our) shows slightly higher registration accuracy and temporally smoother deformations than the state-of-the-art algorithms: SyN <ref type="bibr" target="#b7">[Avants, 2008]</ref>, LPR <ref type="bibr" target="#b87">[Krebs, 2019b]</ref>, 4D-Elastix <ref type="bibr" target="#b118">[Metz, 2011]</ref> and the previous version of our method without GP prior (No-GP <ref type="bibr">[Krebs, 2020c]</ref>). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5</head><p>Showing 2D+T and 3D+T tracking results of the warped moving image I 0 with grid overlay and the Jacobian determinant (Det.-Jac.) for a test sequence. In 3D+T, smoother Jacobian determinants were obtained. . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Tables</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Deformable Registration in Medical Image Analysis Contents 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2.2 Registration Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.1 Similarity Metrics . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.2 Regularization and Deformation Models . . . . . . . . .</orgName>
		</author>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">3 Motion: Adding the Temporal Dimension . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 Deep Learning-based Registration . . . . . . . . . . . . . . . . . . . . 14 2.4.1 Supervised . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.4.2 Unsupervised . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4.3 Weakly Supervised . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">We performed data augmentation on-the-fly by randomly shifting, rotating, scaling and mirroring images</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Barham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
	</analytic>
	<monogr>
		<title level="m">The training time was 15h on a NVIDIA GTX TITAN X GPU</title>
		<imprint>
			<date type="published" when="2015">2015. 2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Tensorflow: Large-scale machine learning on heterogeneous distributed systems. cit. on pp. 73, 90, 98</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sudden cardiac death in heart failure patients with preserved ejection fraction</title>
		<author>
			<persName><forename type="first">Lindsay</forename><forename type="middle">G</forename><surname>Adabag ; Adabag</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">K</forename><surname>Inder S Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">V</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><surname>Luepker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cardiac failure</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A log-euclidean framework for statistics on diffeomorphisms</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Commowick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Avants</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Brian B Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Avants</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A reproducible evaluation of ANTs similarity metric performance in brain image registration</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Brian B Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Learning Formulation of ECGI for Data-driven Integration of Spatiotemporal Correlations and Imaging Information</title>
		<author>
			<persName><forename type="first">Tania</forename><surname>Bacoyannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Bacoyannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Cedilnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Cochet</surname></persName>
		</author>
		<author>
			<persName><surname>Sermesant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Functional Imaging and Modeling of the Heart</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A bi-ventricular cardiac atlas built from 1000+ high resolution MR images of healthy subjects and an analysis of shape and motion</title>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>De Marvao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Unsupervised Learning Model for Deformable Medical Image Registration</title>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effects of pulmonary function on mortality</title>
		<author>
			<persName><forename type="first">; Th</forename><surname>Beaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Beaty</surname></persName>
		</author>
		<author>
			<persName><surname>Newill</surname></persName>
		</author>
		<author>
			<persName><surname>Bh Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chronic diseases</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computing large deformation metric mappings via geodesic flows of diffeomorphisms</title>
		<author>
			<persName><forename type="first">; M Faisal</forename><surname>Beg</surname></persName>
		</author>
		<author>
			<persName><surname>Beg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Trouvé</surname></persName>
		</author>
		<author>
			<persName><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplearning cardiac motion analysis for human survival prediction</title>
		<author>
			<persName><forename type="first">Ghalib</forename><forename type="middle">A</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">Jw</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature machine intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Learning Techniques for Automatic MRI Cardiac Multi-structures Segmentation and Diagnosis: Is the Problem Solved?</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName><surname>Zotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation of three-dimensional finite element-based deformable registration of pre-and intraoperative prostate imaging</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Bharatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Bharatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobuhiko</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><surname>Hata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Interpretable Anatomical Features Through Deep Generative Models: Application to Cardiac Remodeling</title>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><surname>Tarroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Thin-plate splines and the atlas problem for biomedical images</title>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
		<author>
			<persName><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991. 1991</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Medical Image Registration Using Deep Neural Networks: A Comprehensive Review</title>
		<author>
			<persName><forename type="first">;</forename><surname>Boveiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raouf</forename><surname>Boveiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Khayami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Reza</forename><surname>Javidan</surname></persName>
		</author>
		<author>
			<persName><surname>Mehdizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03401</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hyperelastic regularization energy for image registration</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Modersitzki</surname></persName>
		</author>
		<author>
			<persName><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highresolution imaging of pulmonary ventilation and perfusion with 68 Ga-VQ respiratory gated (4-D) PET/CT</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><surname>Aitken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2017. 2017. 2014</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">106</biblScope>
		</imprint>
	</monogr>
	<note>Real-time video super-resolution with spatio-temporal networks and motion compensation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic metric mapping of vector fields</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raimond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Winslow</surname></persName>
		</author>
		<author>
			<persName><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deformable image registration based on similarity-steered CNN regression</title>
		<author>
			<persName><surname>Cao ; Xiaohuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io.2015" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deformable templates using large deformation kinematics</title>
		<author>
			<persName><forename type="first">;</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Gary E Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Rabbitt</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cootes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active Shape Models-Their Training and Application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regression models and life-tables</title>
		<author>
			<persName><surname>David R Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dalca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning for fast probabilistic diffeomorphic registration</title>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</title>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guha</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning conditional deformable templates with convolutional networks</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Rakic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial transformation and registration of brain images using elastically deformable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><surname>Davidsonpilon</surname></persName>
		</author>
		<title level="m">CamDavidsonPilon/lifelines: v0.24.0. Version</title>
		<imprint>
			<date type="published" when="2020-02">2020. v0.24.0. Feb. 2020</date>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A physics-based coordinate transformation for 3-D image matching</title>
		<author>
			<persName><forename type="first">Malcolm</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><forename type="middle">P</forename><surname>Khotanzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Flamig</surname></persName>
		</author>
		<author>
			<persName><surname>Harms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal diffeomorphic free-form deformation: Application to motion and strain estimation from 3D echocardiography</title>
		<author>
			<persName><forename type="first">De</forename><surname>Craene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Mathieu De Craene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Camara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A spatiotemporal statistical atlas of motion for the quantification of abnormal myocardial tissue velocities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>Mathieu De Craene</publisher>
			<date type="published" when="2011">2015. 2011</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
	<note>FlowNet: Learning Optical Flow with Convolutional Networks</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Derivations for linear algebra and optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><surname>Duchi</surname></persName>
		</author>
		<editor>Berkeley, California</editor>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">80</biblScope>
		</imprint>
	</monogr>
	<note>ElGamal, 2016</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Current trends in medical image registration and fusion</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E A</forename><surname>El-Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elmogy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Egyptian Informatics Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deformable image registration using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Maxime</forename><forename type="middle">W</forename><surname>Eppenhof ; Koen Aj Eppenhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitko</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josien Pw</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Error estimation of deformable image registration of pulmonary CT scans using convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Eppenhof ; Koen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josien Pw</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparse kernel methods for high-dimensional survival data</title>
		<author>
			<persName><forename type="first">Ludger</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia-Martina</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><surname>Messow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial Similarity Network for Evaluating Image Alignment in Deep Learning Based Registration</title>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Pew-Thian Yap, and Dinggang Shen</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BIRNet: Brain image registration using dual-supervised fully convolutional networks</title>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Pew-Thian Yap, and Dinggang Shen</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pulmonary arterial hypertension</title>
		<author>
			<persName><forename type="first">Harrison</forename><forename type="middle">W</forename><surname>Farber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Farber</surname></persName>
		</author>
		<author>
			<persName><surname>Loscalzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast diffusion registration</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><surname>Modersitzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Multivariate time series imputation with variational autoencoders</title>
		<author>
			<persName><forename type="first">;</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04155</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep Learning in Medical Image Registration: A Review</title>
		<author>
			<persName><forename type="first">Yabo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tonghe</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12318</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An artificial agent for anatomical landmark detection in medical images</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><surname>Mansi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>MICCAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep deformable registration: Enhancing accuracy by fully convolutional neural net</title>
		<author>
			<persName><forename type="first">Ghosal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilanjan</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><surname>Girija</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">4D medical image registration: A survey</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Girija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P Chenna</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Intelligent Sustainable Systems (ICISS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Understanding survival analysis: Kaplan-Meier estimate</title>
		<author>
			<persName><forename type="first">Manish</forename><forename type="middle">Kumar</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pardeep</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jugal</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><surname>Kishore ; Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010">2010. 2010. 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Generative adversarial nets</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Harrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Evaluating the yield of medical tests</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Frank E Harrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerry</forename><forename type="middle">L</forename><surname>Pryor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Rosati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a survey</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingkun</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Enhancing label-driven deep deformable image registration with local distance metrics for state-of-the-art cardiac motion tracking</title>
		<author>
			<persName><forename type="first">Alessa</forename><surname>Hering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kuckertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattias</forename><forename type="middle">P</forename><surname>Heldmann</surname></persName>
		</author>
		<author>
			<persName><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bildverarbeitung für die Medizin</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heimo</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Convolutional Neural Networks for Multimodal Image Registration</title>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 19, 35, 54</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Left atrial size and heart failure hospitalization in patients with diastolic dysfunction and preserved ejection fraction</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><forename type="middle">G</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Peguero</surname></persName>
		</author>
		<author>
			<persName><surname>Podesta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cardiovascular echography</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">89</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cardiovascular magnetic resonance to predict appropriate implantable cardioverter defibrillator therapy in ischemic and nonischemic cardiomyopathy patients using late gadolinium enhancement border zone: comparison of four analysis methods</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jablonowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uzma</forename><surname>Jablonowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Pals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation: Cardiovascular Imaging</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName><forename type="first">; J</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Parallel Transport of Surface Deformations from Pole Ladder to Symmetrical Extension</title>
		<author>
			<persName><surname>Jia ; Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Duchateau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Moceri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ShapeMI MICCAI 2018: Workshop on Shape in Medical Imaging</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Joint analysis of clinical risk factors and 4D cardiac motion for survival prediction using a hybrid deep learning network</title>
		<author>
			<persName><forename type="first">Jin ; Shihao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Savioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>De Marvao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02951</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A comparison of sift, pca-sift and surf</title>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Gwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>Image Processing and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghwan</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A general fast registration framework by learning deformationappearance correlation</title>
		<author>
			<persName><forename type="first">Minjeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>Pew-Thian Yap, and Dinggang Shen</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">68</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Elastix: a toolbox for intensitybased medical image registration</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><surname>Staring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="28" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Robust non-rigid registration through agent-based action learning</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><surname>Delingette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="344" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised Probabilistic Deformation Modeling for Robust Diffeomorphic Registration</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Delingette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Medical Imaging Diffeomorphic Registration based on Machine Learning</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><surname>Miao</surname></persName>
		</author>
		<idno>App. 16/233</idno>
		<imprint>
			<date type="published" when="2019-07">2019. July 2019</date>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName><surname>Krebs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b]</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning a Probabilistic Model for Diffeomorphic Registration</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Mansi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 6</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Method and System for Deep Motion Model Learning in Medical Images</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><surname>Mansi</surname></persName>
		</author>
		<idno>App. 16/131</idno>
		<imprint>
			<date type="published" when="2020-03">2020. Mar. 2020</date>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning a Generative Motion Model from Image Sequences based on a Latent Motion Matrix</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Delingette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2020-03">2020. Mar. 2020</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 6, 61</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Probabilistic Motion Modeling from Medical Image Sequences: Application to Cardiac Cine-MRI</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Delingette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Atlases and Computational Models of the Heart. Multi-Sequence CMR Segmentation, CRT-EPiggy and LV Full Quantification Challenges</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-01">2020. Jan. 2020</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="84" to="87" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 6, 64</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Current perspectives in medical image perception</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Krupinski</surname></persName>
		</author>
		<author>
			<persName><surname>Krupinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Bayesian k-Means as a &quot;Maximization-Expectation&quot; Algorithm</title>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><surname>Kurihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Spatiotemporal nonrigid registration for ultrasound cardiac motion estimation</title>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">J</forename><surname>Ledesmacarbayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ledesma-Carbayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Kybic</surname></persName>
		</author>
		<author>
			<persName><surname>Desco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Image-and-Spatial Transformer Networks for Structure-Guided Image Registration</title>
		<author>
			<persName><forename type="first">Lee ; Matthew Ch</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Non-rigid image registration using self-supervised fully convolutional networks without training data</title>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A hybrid deep learning framework for integrated segmentation and registration: evaluation on longitudinal white matter tract changes</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiro</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Dual Motion GAN for Future-Flow Embedded Video Prediction</title>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Method and System for Image Registration Using an Intelligent Artificial Agent</title>
		<author>
			<persName><forename type="first">Shun</forename><surname>Liao ; Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><surname>De Tournemire</surname></persName>
		</author>
		<idno>App. 15/587,094</idno>
		<imprint>
			<date type="published" when="2017-11">2017. Nov. 2017</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">An Artificial Agent for Robust Image Registration</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><surname>De Tournemire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thijs</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Schild&apos;s ladder for the parallel transport of deformations in time series of images</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">LCC-Demons: a robust and accurate symmetric diffeomorphic registration algorithm</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">B</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><surname>Frisoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Efficient parallel transport of deformations in time series of images: from Schild&apos;s to pole ladder</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lorenzi</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical imaging and vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An imagebased deep learning framework for individualising radiotherapy dose: a retrospective analysis of outcome prediction</title>
		<author>
			<persName><forename type="first">Lou ; Bin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semihcan</forename><surname>Doken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingliang</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Parallel transport in shape analysis: a scalable numerical scheme</title>
		<author>
			<persName><forename type="first">Louis ; Maxime</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Bône</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Charlier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Durrleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Geometric Science of Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Disease Neuroimaging Initiative, et al</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Multimodal image registration with deep context reinforcement learning</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Multimodality image registration by maximization of mutual information</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Collignon</surname></persName>
		</author>
		<author>
			<persName><surname>Vandermeulen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
	<note>Guy Marchal, and Paul Suetens</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mahapatra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Deformable medical image registration using generative adversarial networks</title>
		<author>
			<persName><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavna</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahil</forename><surname>Garnavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI 2018)</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Vincent Christlein</surname></persName>
		</author>
		<author>
			<persName><surname>Hornegger</surname></persName>
		</author>
		<title level="m">Medical Imaging Systems: An Introductory Guide</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">11111</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>Adversarial Autoencoders</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">iLogDemons: A demons-based registration algorithm for tracking incompressible elastic biological tissues</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">MRIultrasound fusion for guidance of targeted prostate biopsy</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelena</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in urology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Nonrigid registration of dynamic medical imaging data using nD+ t B-splines and a groupwise optimization approach</title>
		<author>
			<persName><forename type="first">; Ct</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiro</forename><forename type="middle">J</forename><surname>Van Walsum</surname></persName>
		</author>
		<author>
			<persName><surname>Niessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Dilated fcn for multi-agent 2d/3d medical image registration</title>
		<author>
			<persName><surname>Miao ; Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Piat</surname></persName>
		</author>
		<author>
			<persName><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Numerical methods for image registration</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">J</forename><surname>Modersitzki</surname></persName>
		</author>
		<author>
			<persName><surname>Modersitzki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<publisher>Oxford University Press on Demand</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A combined statistical and biomechanical model for estimation of intra-operative prostate deformation</title>
		<author>
			<persName><forename type="first">Mohamed ; Ashraf</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Indications for implantable cardioverter-defibrillators based on evidence and judgment</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Myerburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Myerburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><surname>Castellanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Cardiology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s Disease Neuroimaging Initiative, et al. &quot;Monotonic Gaussian Process for spatio-temporal disease progression modeling in brain imaging data</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Nader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Abi Nader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Regularization, scale-space, and edge detection filters</title>
		<author>
			<persName><forename type="first">Mads</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Florack</surname></persName>
		</author>
		<author>
			<persName><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Metric learning for image registration</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois-Xavier</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><surname>Vialard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Changes in brain morphology in Alzheimer disease and normal aging: is Alzheimer disease an exaggerated aging process?</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Tabira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatake</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName><surname>Uno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Neuroradiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Medical image registration: a review</title>
		<author>
			<persName><forename type="first">;</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods in biomechanics and biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Video-based AI for beat-to-beat assessment of cardiac function</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">580</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Issues in respiratory motion compensation during external-beam radiotherapy</title>
		<author>
			<persName><forename type="first">Cihat</forename><surname>Ozhasoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Ozhasoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Radiation Oncology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note>Biology* Physics</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Riemannian elasticity: A statistical regularization framework for non-linear registration</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Stefanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Spatio-temporal free-form registration of cardiac MR image sequences</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Perperidis</surname></persName>
		</author>
		<author>
			<persName><surname>Perperidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Raad H Mohiaddin</surname></persName>
		</author>
		<author>
			<persName><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Registration of 4D cardiac CT sequences under trajectory constraints with multichannel diffeomorphic demons</title>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Peyrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Peyrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Prostate cancer diagnosis: multiparametric MR-targeted biopsy with cognitive and transrectal US-MR fusion guidance versus systematic biopsy-prospective multicenter study</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Puech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Puech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaele</forename><surname>Rouvière</surname></persName>
		</author>
		<author>
			<persName><surname>Renard-Penna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title/>
		<author>
			<persName><surname>Punnoose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Joint learning of motion estimation and segmentation for cardiac MR image sequences</title>
		<author>
			<persName><surname>Lynn R Punnoose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldrin F</forename><surname>Michael M Givertz</surname></persName>
		</author>
		<author>
			<persName><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011. 2018</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>Heart failure with recovered ejection fraction: a distinct clinical entity</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Principal component based diffeomorphic surface mapping</title>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Principal component based diffeomorphic surface mapping</title>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer School on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title/>
		<author>
			<persName><surname>Reyes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Model-based respiratory motion compensation for emission tomography image reconstruction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Koulibaly</surname></persName>
		</author>
		<author>
			<persName><surname>Angel González-Ballester</surname></persName>
		</author>
		<author>
			<persName><surname>Darcourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rijnierse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Usefulness of left atrial emptying fraction to predict ventricular arrhythmias in patients with implantable cardioverter defibrillators</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Mischa T Rijnierse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Kamali Sadeghian</surname></persName>
		</author>
		<author>
			<persName><surname>Schuurmans Stekhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of cardiology</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">SVF-Net: Learning Deformable Image Registration Using Shape Matching</title>
		<author>
			<persName><forename type="first">Marc-Michel</forename><surname>Rohé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Rohé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Lowdimensional representation of cardiac motion using Barycentric Subspaces: A new group-wise paradigm for estimation, analysis, and reconstruction</title>
		<author>
			<persName><forename type="first">Marc-Michel</forename><surname>Rohé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Rohé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Differential associations between entorhinal and hippocampal volumes and memory performance in older adults</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Rosen ; Allyson C Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">De</forename><surname>Prull</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral neuroscience</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Nonrigid registration using free-form deformations: application to breast MR images</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">I</forename><surname>Sonoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmel</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Automatic construction of 3-D statistical deformation models of the brain using nonrigid registration</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><forename type="middle">F</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1014" to="1025" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note>cit. on pp. 13, 24, 26</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Contemporary rates of appropriate shock therapy in patients who receive implantable device therapy in a real-world setting: From the Israeli ICD Registry</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Sabbag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Sabbag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avishag</forename><surname>Suleiman</surname></persName>
		</author>
		<author>
			<persName><surname>Laish-Farkash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart Rhythm</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Recurrent Registration Neural Networks for Deformable Image Registration</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Andermatt</surname></persName>
		</author>
		<author>
			<persName><surname>Bauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A generic framework for non-rigid registration based on non-uniform multilevel free-form deformations</title>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><surname>Quist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Toward patient-specific myocardial models of the heart</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marc</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phani</forename><surname>Peyrat</surname></persName>
		</author>
		<author>
			<persName><surname>Chinchapatnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart failure clinics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Temporal sparse free-form deformations</title>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jantsch</surname></persName>
		</author>
		<author>
			<persName><surname>Aljabar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A Deep Metric for Multimodal Registration</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Gutiérrez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>cit. on pp. 16, 21, 22</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">A model of brain morphological changes related to aging and Alzheimer&apos;s disease from cross-sectional assessments</title>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Sivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Nonrigid image registration using multi-scale 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">Hessam</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><surname>De Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Sotiras, 2013</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Deformable Medical Image Registration: A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>cit. on</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Sparse estimation of Cox proportional hazards models via approximated information criteria</title>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanjuan</forename><surname>Chalani S Wijayasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">RABBIT: rapid alignment of brains by building intermediate templates</title>
		<author>
			<persName><surname>Tang ; Songyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjeong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks for MR-CT Deformable Image Registration</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firat</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romy</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName><surname>Profanter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07349</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cit. on pp. 18, 35, 37</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Thirion</orgName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Image matching as a diffusion process: an analogy with Maxwell&apos;s demons</title>
		<author>
			<persName><forename type="first">J-P</forename><surname>Thirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">2012 ACCF/AHA/HRS focused update incorporated into the ACCF/AHA/HRS 2008 guidelines for device-based therapy of cardiac rhythm abnormalities: a report of the American College of Cardiology Foundation/American Heart Association Task Force on Practice Guidelines and the Heart Rhythm Society</title>
		<author>
			<persName><forename type="first">Lizhi</forename><surname>Tian ; Zhiqiang Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cynthia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawood</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><surname>Darbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Cardiology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">84</biblScope>
			<date type="published" when="2013">2015. 2015. 2013</date>
		</imprint>
	</monogr>
	<note>SPIE Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Training CNNs for Image Registration from Few Samples with Model-based Data Augmentation</title>
		<author>
			<persName><forename type="first">Hristina</forename><surname>Uzunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Uzunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinz</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Handels</surname></persName>
		</author>
		<author>
			<persName><surname>Ehrhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Statistics on diffeomorphisms via tangent space representations</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><surname>Trouvé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Spatiotemporal motion estimation for respiratorycorrelated imaging of the lungs</title>
		<author>
			<persName><forename type="first">Jef</forename><surname>Vandemeulebroucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vandemeulebroucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Rit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kybic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Clarysse</surname></persName>
		</author>
		<author>
			<persName><surname>Sarrut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Insight into efficient image registration techniques and the demons algorithm</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ezio</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><surname>Malis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Aymeric Perchant, and Nicholas Ayache</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Non-parametric diffeomorphic image registration with the demons algorithm</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note>Aymeric Perchant, and Nicholas Ayache</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Symmetric log-domain diffeomorphic registration: A demons-based approach</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Aymeric Perchant, and Nicholas Ayache</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Diffeomorphic demons: Efficient non-parametric image registration</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aymeric</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas Ayache ;</forename><surname>Perchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">62</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1997">2009. 2009. 1997</date>
		</imprint>
	</monogr>
	<note>NeuroImage</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">End-to-end unsupervised deformable image registration with a convolutional neural network&quot;. In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<author>
			<persName><forename type="first">Bob</forename><forename type="middle">D</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floris</forename><forename type="middle">F</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">A</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Išgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floris</forename><forename type="middle">F</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">A</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2017">2017. 2017. 2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>A deep learning framework for unsupervised affine and deformable image registration</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Nonrigid registration of brain MRI using NURBS</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzi</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Efficient Laplace Approximation for Bayesian Registration Uncertainty Quantification</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaomiao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Probabilistic diffeomorphic registration: Representing uncertainty</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wassermann ; Demian Wassermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Toews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">C</forename><surname>Kagadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Biomedical Image Registration</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2014. 2014. Webb, 2003. 2003</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Bibliography</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedetta</forename><surname>Bodini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">LSTM spatial co-transformer networks for registration of 3D fetal US and MR brain images</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishesh</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Driven Treatment Response Assessment and Preterm, Perinatal, and Paediatric Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Scalable High Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Munsell</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Prediction based collaborative trackers (PCT): A robust and accurate approach toward 3D medical object tracking</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Topology preservation evaluation of compact-support radial basis functions for image registration</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darong</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Fast Predictive Image Registration</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Quicksilver: Fast predictive image registration-A deep learning approach</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">ssemnet: Serial-section electron microscopy image registration using a spatial transformer network with learned features</title>
		<author>
			<persName><forename type="first">Inwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David Gc</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><forename type="middle">F</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chung</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Won-Ki</forename><surname>Jeong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Jacobi fields in groups of diffeomorphisms and applications</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of applied mathematics</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title/>
		<author>
			<persName><surname>Zerhouni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Human heart: tagging with MR imaginga method for noninvasive assessment of myocardial motion</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Elias A Zerhouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Parish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Finite-dimensional Lie algebras for fast diffeomorphic image registration</title>
		<author>
			<persName><forename type="first">Miaomiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fletcher</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaomiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fletcher</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Information Processing in Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">The Alzheimer&apos;s disease (AD) is assumed to have a faster morphological degeneration (aging) than healthy people</title>
		<author>
			<orgName type="collaboration">Sivera, 2019]. . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Registration misalignment for CT and PET images induced from respiratory motion. Different breathing states are illustrated, free-breathing (FB), maximal inspiration (Insp) and maximal expiration</title>
		<imprint>
			<publisher>Exp</publisher>
		</imprint>
	</monogr>
	<note>Callahan, 2014]. . . . . . . . . . . . . . 106 List of Figures</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">T are the initial scores after translation registration with elastix. 3-D* are results with perfect rigid alignment T*. nfc are our results with no fuzzy action control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>1 Results of prostate MR registration on the 56 testing pairs. 2-D and 3-D results in comparison to elastix with B-spline spacing of. HD in mm</note>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Registration performance with mean and standard deviation scores (in brackets) of RMSE, DICE, Hausdorff Distance (HD in mm) and the mean gradient of the determinant of Jacobians (Grad Det-Jac, ×10 -2 ) comparing the undeformed case (Undef), LCC-demons (Dem), SyN, voxelmorph (VM) and our method</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Mean Ejection fraction (EF in % with standard deviation in parentheses) of pathological deformation predictions (Step 1) should stay similar to the mean EF after the transport to healthy/normal subjects (Step 2)</title>
		<author>
			<orgName type="collaboration">PL). . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>Our algorithm shows smaller absolute differences compared to the pole ladder</note>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Registration performance with mean and standard deviation scores of DICE (in %), Hausdorff Distance (HD in mm), spatial and temporal gradients of the deformation fields (×10 -2 ) comparing the undeformed case (Undef), SyN, learning-based pairwise registration (LPR), 4D-Elastix, our previous version without GP prior (No-GP) and the proposed method for all</title>
		<author>
			<orgName type="collaboration">2D+T sequences. . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">2 3D+T registration performance with mean and standard deviation scores of RSME, DICE, Hausdorff Distance (HD), spatial and temporal gradients of the deformation fields comparing the undeformed case (Undef), 4D-Elastix and the proposed method</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Predictors of HF hospitalization using univariate and multivariate (for Clinical and Fingerprint+Clinical) Cox proportional hazard models. The results are obtained via 6-fold stratified cross-validation. HR, p-value (reject the null hypothesis that the HR equals one) and average concordance index (C) are reported including a 95% confidence interval (CI) in brackets. The motion fingerprint shows the highest prediction accuracy</title>
		<imprint/>
	</monogr>
	<note>independently and together with multiple clinical variables.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
