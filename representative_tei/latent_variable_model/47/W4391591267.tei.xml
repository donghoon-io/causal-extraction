<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions</title>
				<funder ref="#_WptUyAV">
					<orgName type="full">National Institutes of Health BRAIN initiative</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-30">30 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weihan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chengrui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yule</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anqi</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-30">30 May 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.02686v3[q-bio.NC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables with frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work establishes a connection between an LDS and a multioutput GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional representation, revealing communication directions across brain regions and separating oscillatory communications into different frequency bands.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The number of simultaneous neural recordings from various brain regions has increased recently. These recordings offer opportunities to explore the mechanisms through which inter-areal communication supports brain function <ref type="bibr" target="#b10">(Kohn et al., 2020)</ref>. Brain regions linked to sensory and cognitive functions often display interconnectedness, with signals transmitted bidirectionally and potentially simultaneously <ref type="bibr" target="#b5">(Harris &amp; Mrsic-Flogel, 2013;</ref><ref type="bibr" target="#b11">Miller et al., 2018;</ref><ref type="bibr" target="#b21">Wang et al., 2024)</ref>. However, the high-dimensional neural record-ings typically present a complex view of this concurrent communication-for example, neurons may concurrently represent overlapping neural activities within a certain region. Therefore, uncovering the interactions between different brain regions presents a challenging task.</p><p>Many statistical methodologies have been employed to address the challenge of understanding communications across multiple brain regions. <ref type="bibr" target="#b8">Hultman et al. 2018</ref> examined multiregion local field potential data and identified frequencybased interactions across brain regions using a Gaussian Process Factor Analysis model. Following a similar approach, <ref type="bibr" target="#b2">Gokcen et al. 2022;</ref><ref type="bibr">2023</ref> suggested that latent variables can be divided into across-and within-region components. This model was applied to disentangle the concurrent and bidirectional communications across brain regions with a multi-output Squared Exponential kernel. <ref type="bibr" target="#b1">Glaser et al. 2020</ref> developed a switching linear dynamic system to uncover low-dimensional interactions among multiple brain regions. This method captured regions responsible for transitioning between latent states by specifying a novel transition rule.</p><p>Broadly categorized into Gaussian Process (GP) and Linear Dynamical System (LDS) classes, these methods offer distinct advantages. The GP-based approach, leveraging the robust representational capability of multi-output kernels, performs well in discovering latent variables with crucial information, such as frequencies and directional communications. Conversely, the LDS-based approach, while computationally efficient with a linear cost in time points, lacks the powerful expressiveness of GP in latent representation.</p><p>Our goal is to combine the strengths of both methodologies by constructing an LDS that mirrors a GP. Several studies have explored this connection: Hartikainen &amp; Särkkä 2010 established a framework about converting a singleoutput GP with Matern or Squared Exponential kernel to an LDS, which relied on spectral factorization <ref type="bibr" target="#b15">(Sayed &amp; Kailath, 2001)</ref>. Building upon this, <ref type="bibr" target="#b18">Solin &amp; Särkkä 2014</ref> proposed the conversion for single-output periodic kernels, and <ref type="bibr" target="#b14">Särkkä et al. 2013</ref> extended the single-output conversions to a spatiotemporal GP. However, applying these conversions for GP-based multi-region methods is non-trivial because a gap exists in converting a multi-output GP to an LDS. One approach to bridge this gap is to assume the kernel is separable over spatial and temporal domain <ref type="bibr" target="#b19">(Solin et al., 2016)</ref>. This allows us to create a closed-form multi-output GP-LDS conversion following the framework proposed in single-output cases.</p><p>Consequently, choosing a separable kernel becomes essential in this study. An effective option is the complex-valued multi-region kernel <ref type="bibr" target="#b20">(Ulrich et al., 2015)</ref>, specifically designed to facilitate learning latent interactions encompassing frequencies and phase delays across brain regions. However, it is important to note that the connection between an LDS and a GP with a complex-valued multi-output kernel remains unknown.</p><p>We introduce the Multi-Region Markovian Gaussian Process (MRM-GP) to model latent representations, where Markovian means the discrete state space representation of a GP. Our work establishes a connection between an LDS and a multi-output GP that explicitly discovers frequency-based latent communications and their directionality via phase delays. By doing so, we can have three advantages: (1) utilizing the powerful representational capability of kernel functions; (2) employing the efficient inference algorithm to ensure a linear computational cost over time points; (3) extending the LDS to incorporate time-varying frequencies and delays by switching states.</p><p>We test MRM-GP using multi-region spike trains and local field potential recordings. The model proves its capability to produce understandable low-dimensional representations. These representations illustrate the direction of communication flow among regions and effectively disentangle oscillatory interactions into diverse frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We introduce the multi-region kernel, a multi-output kernel for modeling interactions across different brain regions. Then, we demonstrate how a Gaussian Process with this kernel can be employed to model latent communications across regions. It is worth noting that various mapping methods can be used to project latent representations onto neural recordings, and in this case, we opt for Factor Analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Region Kernel</head><p>The complex-valued multi-region kernel proposed in <ref type="bibr" target="#b20">(Ulrich et al., 2015)</ref> explicitly models communication frequencies and phase delays within the latent space of neural data:</p><formula xml:id="formula_0">K pp ′ (τ ) = R r=1 a r p a r p ′ exp - 1 2σ 2 τ 2 + iη(τ + ϕ pp ′ ) , (1) = exp - 1 2σ 2 τ 2 + iητ temporal R r=1 a r p a r p ′ exp(iηϕ pp ′ ) spatial .</formula><p>dimentions across-region latent variables 10ms -5ms</p><p>Region A Region B within-region latent variable within-region latent variable This kernel ensures separability over space and time, where p and p ′ are two brain regions, τ = t -t ′ is the time interval.</p><p>In the temporal part, σ signifies the length scale, i = √ -1 denotes the imagery unit, η represents the communication frequency between regions. In the spatial part, ϕ pp ′ represents the phase delay between region p and p ′ , a r p and a r p ′ are amplitudes, and R &gt; 1 denotes the rank number ensuring positive definiteness.</p><p>The separability is required to establish a connection between the multi-region kernel and a linear dynamic system (LDS). Moreover, the real part of this kernel Re</p><formula xml:id="formula_1">[K pp ′ (τ )] = R r=1 a r p a r p ′ exp(-1 2σ 2 τ 2 ) cos(η(τ +ϕ pp ′ ))</formula><p>, denoted as the Cross-Spectral Mixture (CSM) kernel <ref type="bibr" target="#b20">(Ulrich et al., 2015)</ref>, has been shown to effectively capture frequency-based communications among various brain regions <ref type="bibr" target="#b8">(Hultman et al., 2018)</ref>. However, due to CSM's non-separability, we work with the complex-valued kernel as represented in Eq. 1 to build an LDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Region Gaussian Process Factor Analysis</head><p>A Gaussian Process Factor Analysis model, utilizing the real part of the multi-region kernel in Eq. 1 and named CSM-GPFA, can identify latent variables that capture frequencies and phase delays across brain regions.</p><p>Given single region neural recording y p ∈ R n p ×T , p ∈ {1, . . . , P } is the brain region index, n p denotes the number of neurons in region p, and T represents time steps. Our goal is to find the M independent low-dimensional variables x p ∈ R M ×T for each region's neural data y p . These variables from P regions together form as x = [x 1 , . . . , x P ] ⊤ ∈ R M P ×T , representing a latent representation for multi-region recordings y = [y 1 , . . . , y P ] ⊤ ∈ R N ×T , where N = n 1 + • • • + n P is the total number of neurons over P regions. Besides, y is a linear mapping of x: y = Cx + d + ϵ, where C is a block diagonal matrix C = diag{C 1 , . . . , C p , . . . ,</p><formula xml:id="formula_2">C P } ∈ R N ×M P , d ∈ R N ×1 is bias, and ϵ ∼ N (0, V) is Gaussian noise with V ∈ R N ×N .</formula><p>Meanwhile, a widely used assumption of x p is to split it into across-and within-region parts <ref type="bibr" target="#b2">(Gokcen et al., 2022)</ref>:</p><formula xml:id="formula_3">x p = [x p,a , x p,w ] ⊤ , x p,a ∈ R ma×T , x p,w ∈ R mw×T ,</formula><p>where m a , m w are the number of dimensions for acrossor within-region part and m a + m w = M . The acrossregion variables x p,a describe neural activity that is shared across all brain regions, meaning that for the remaining P -1 regions, they have the latent variables with the same frequencies and dynamics except phase delays, while the within-region variables x p,w describe the neural activity of region p that is not related to other regions (see Figure <ref type="figure" target="#fig_0">1</ref>). Consequently, we model x p,a and x p,w separately with K in Eq. 1. For region p, there are m a dimensions of acrossregion variables, and each dimension x p,a m ∈ R T ×1 , m ∈ [1, m a ] has spatial correlations with the remaining P -1 regions. So, the m th dimension across-region variables over P regions: x a m = [x 1,a m , . . . , x P,a m ] ∈ R P ×T are considered as a group and modeled as the real part of a multi-output Complex Gaussian Process with K m . For within-region variables, each dimension x p,w m ∈ R T ×1 , m ∈ [1, m w ] is independently modeled as the real part of a single-output Complex Gaussian Process with K(τ</p><formula xml:id="formula_4">) m = R r=1 a m r 2 exp(-1 2σ m2 τ 2 + iη m τ )</formula><p>, where p = p ′ and ϕ pp ′ = 0. Furthermore, we also assume independence among different dimensions of across-and within-region variables, implying that different index m refers to distinct kernel parameters.</p><p>Unlike the approach in <ref type="bibr" target="#b20">(Ulrich et al., 2015)</ref>, which uses a mixture of frequencies, we employ a single frequency in Eq 1 to achieve frequency disentanglement. Specifically, each dimension of the across-region latent variable will have a single frequency peak. Consequently, the mixture of frequencies present in the data will be captured by multiple dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Modeling latent variables x p with Gaussian Process is inefficient with a O(T 3 ) time complexity. So, we want to build Markovian representations of these latent variables, indicating the state space representations of each dimension: across-region x a m and within-region x p,w m , where every Markovian representation follows a Linear Dynamical System (LDS) <ref type="bibr" target="#b19">(Solin et al., 2016)</ref>.</p><p>Spectral factorization has been used in multi-output GP cases <ref type="bibr" target="#b23">(Zhu et al., 2023)</ref>. However, for the complex-valued multi-output kernel, we need to develop a new spectral factorization-based method to first convert the complexvalued temporal part to an LDS (Section 3.1) and then use the kernel's separability to combine the complex-valued spatial part to get the final LDS (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Markovian Within-Region Latent Variables</head><p>The Markovian representation of region p's m th dimension within-region latent variable x p,w m ∈ R T ×1 follows a discrete-time LDS structure:</p><formula xml:id="formula_5">f p,w m,t = A w m f p,w m,t-1 + q t-1 , q t-1 ∼ CN (0, Q w m ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_6">f p,w m,t = [g p,w m,t , dg p,w m,t dt , . . . , d k-1 g p,w m,t dt k-1 ] T ∈ C k×T , de- noting the complex-valued dynamics g p,w</formula><p>m,t and its derivatives up to (k -1) th order at time t. Especially, within-region latent variable x p,w m is the real part of g p,w m,t . A w m ∈ C k×k represents the complex-valued transition matrix, and q t-1 is the sampling from a complex normal distribution CN (•) with the complex-valued measurement (Hermitian) matrix</p><formula xml:id="formula_7">Q w m ∈ C k×k . The key question now becomes how to associate single- output K(τ ) m = R r=1 a m r 2 exp(-1 2σ m2 τ 2 + iη m τ ) with A w m and Q w m .</formula><p>To achieve this, our approach involves two steps: forming a continuous-time LDS for withinregion variables in each region through spectral factorization <ref type="bibr" target="#b9">(Kailath et al., 2000)</ref>, and subsequently transforming it into the discrete-time version as specified in Eq. 2. This linkage is new and differs from previous connections <ref type="bibr" target="#b6">(Hartikainen &amp; Särkkä, 2010;</ref><ref type="bibr" target="#b18">Solin &amp; Särkkä, 2014)</ref> as the kernel is situated in the complex domain.</p><p>Forming a continuous-time LDS. Given single-output K m , the continuous-time LDS we want to form is:</p><formula xml:id="formula_8">df (t) p,w m dt = F w m f (t) p,w m + Lu(t),<label>(3)</label></formula><p>where</p><formula xml:id="formula_9">f (t) p,w m = [g(t) p,w m , dg(t) p,w m dt , . . . , d k-1 g(t) p,w m dt k-1</formula><p>] ⊤ ∈ C k×T , denoting the continuous-time version of g a m,t and its derivatives up to (k -1) th order. F w m ∈ C k×k is a continuous-time transition matrix, L = [0, . . . , 0, 1] ⊤ ∈ R k×1 signifies a constant vector, and u(t) denotes a singledimensional white noise with spectral density v. We need to obtain both F w m and v from K m . F w m takes a companion form of LDS <ref type="bibr" target="#b4">(Grewal &amp; Andrews, 2014)</ref>:</p><formula xml:id="formula_10">F w m =      0 1 0 1 . . . 1 -a 0 . . . -a k-2 -a k-1      , (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>where a 0 , . . . , a k-1 are the coefficients in a stochastic differential equation that is equivalent to Eq. 3:</p><formula xml:id="formula_12">d k g(t) p,w m dt k + a k-1 d k-1 g(t) p,w m dt k-1 + • • • + a 0 g(t) p,w m = u(t).<label>(5)</label></formula><p>To obtain F w m and v, we first apply Fourier transform on both sides of the continuous-time LDS in Eq. 3 to achieve a frequency domain representation (see Appendix A for derivation):</p><formula xml:id="formula_13">S(ω) = G(F w m -iωI) -1 LvL ⊤ (F w m + iωI) -T G ⊤ (6)</formula><p>where</p><formula xml:id="formula_14">S(ω) = √ 2πσ m exp(-(η m -ω) 2 2σ m ) is the spectral den- sity of single-output K m , G = [1, 0, . . . , 0] ∈ R 1×k</formula><p>represents a constant vector, I ∈ R k×k denotes an identity matrix, and, notably, v = √ 2πσ m . Now, we only need to solve Eq. 6 to obtain the coefficients a 0 , . . . , a k-1 in F w m . On the left-hand side, S(ω) follows an exponential family, which is infinitely differentiable. On the right-hand side, however, the finite number coefficients a 0 , . . . , a k-1 in F w m determine a finite polynomial function: G(F w m -iωI) -1 L. Therefore, we can only construct a finite polynomial approximation of S(ω). But one observation from Eq. 6 is that S(ω) can be factorized into two parts, i.e., a complex function multiplying its conjugate.</p><p>Previous established connections between real-valued kernel and LDS assume a symmetric S(ω), implying using a Taylor expansion to approximate S(ω) as a polynomial of ω <ref type="bibr" target="#b6">(Hartikainen &amp; Särkkä, 2010)</ref>. However, in the case of complex-valued K m , S(ω) is non-symmetric due to frequency η m , so our solution is to approximate it as a polynomial of iω:</p><formula xml:id="formula_15">1 S(ω) ≈ σ m 2π (b 0 + b 1 iω + b 2 (iω) 2 + • • • + b 2k (iω) 2k ), = T (iω),<label>(7)</label></formula><p>where</p><formula xml:id="formula_16">b 2k = 1 if k is even, b 2k = -1 if k is odd, b 0 , b 2 , . . . , b 2k-2 are real numbers, and b 1 , b 3 , . . . , b 2k-1</formula><p>are complex numbers that only have imagery parts. These coefficients' values depend on σ m , η m , and k. Figure <ref type="figure">2(A-C</ref>) shows the approximation of S(ω) when k = 2, 3, 4, demonstrating a reliable approximation even in the case of k = 2. Appendix D shows the effect of k on generated samples.</p><p>Now our target becomes solving the following equation for a 0 , . . . , a k-1 : where H(iω) is commonly referred to as the transfer function with a 0 , . . . , a k-1 acting as its coefficients. Its reciprocal 1 H(iω) is the function form of G(F w m -iωI) -1 L. Solving Eq. 8 is often referred to as spectral factorization, and an advantageous aspect of this factorization is that a 0 , . . . , a k-1 are a subset of complex-valued roots of T (iω), where they are all situated within the left-half complex plane <ref type="bibr" target="#b9">(Kailath et al., 2000)</ref> and can be found by QR algorithm with time complexity O(k). See Appendix A for derivation.</p><formula xml:id="formula_17">T (iω) = σ m 2π H(iω)H(-iω), H(iω) = a 0 + a 1 iω + • • • + a k-1 (iω) k-1 + (iω) k ,<label>(8)</label></formula><p>Forming a discrete-time LDS. Given F w m , L and v = 2π σ m in Eq. 6, the computation of A w m and Q w m in Eq. 2 are as follows <ref type="bibr" target="#b19">(Solin et al., 2016)</ref>:</p><formula xml:id="formula_18">dP ∞ dt = F w m P ∞ + P ∞ F w m H + LvL ⊤ = 0, A w m = expm(F w m ∆t), Q w m = P ∞ -A w m P ∞ A w m H ,<label>(9)</label></formula><p>where v is the spectral density of white noise u(t), expm(•) represents the matrix exponential function, and ∆t signifies the time interval in discrete-time LDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Markovian Across-Region Latent Variables</head><p>Region p's m th dimension across-region latent variable x a m ∈ R P ×T can be expressed as x a m = [x 1,a m , . . . , x p,a m , . . . , x P,a m ]. This indicates that they consist of P variables sharing the same η m and σ m for describing temporal features while using phase delays {ϕ m pp</p><formula xml:id="formula_19">′ } P p,p ′ =1</formula><p>to capture cross-spatial differences. It's important to note that the temporal and spatial components are separable in K m . Consequently, we can initially create a within-region Markovian representation, denoted as A w m , Q w m , for the temporal features in each x p,a m . Then, this representation is extended to the across-region Markovian representation for x a m through the incorporation of phase delays {ϕ m pp ′ } P p,p ′ =1 .</p><p>Therefore, the Markovian representation of m th dimension across-region latent variable x a m is:</p><formula xml:id="formula_20">f a m,t = A a m f a m,t-1 + q t-1 , q t-1 ∼ CN (0, Q a m ), A a m = I ⊗ A w m , Q a m = K m spatial ⊗ Q w m ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_21">f a m,t = [g a m,t , dg a m,t dt , . . . , d k-1 g a m,t dt k-1 ] ⊤ ∈ C P k×T , x a</formula><p>m,t is the real part of g a m,t , A a m ∈ C P k×P k is transition matrix, denoting the Kronecker product of identity matrix I ∈ R P ×P and A w m ∈ C k×k , and</p><formula xml:id="formula_22">Q a m ∈ C P k×P k is mea- surement matrix, denoting the Kronecker product of K m 's spatial part K m spatial = R r=1 a m,r p a m,r p ′ exp(iη m ϕ m pp ′ ) and Q w m ∈ C k×k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Region Markovian Gaussian Process</head><p>Given our assumption of independence among different dimensions of across-region variables and distinct dimensions of within-region variables, the Markovian representation for all variables x ∈ R M P ×T , both across-and within-region, spanning P brain regions can be expressed as:</p><formula xml:id="formula_23">f t = Af t-1 + q t-1 , q t-1 ∼ CN (0, Q),<label>(11)</label></formula><p>where</p><formula xml:id="formula_24">f t = [g t , dgt dt , . . . , d k-1 gt dt k-1 ] ⊤ ∈ C M P k×T , x t is the real part of g t , and A ∈ C M P k×M P k , Q ∈ C M P k×M P k are block diagonal matri- ces: A = diag{A a 1 , . . . , A a ma , A w 1 . . . A w mw }, Q = diag{Q a 1 , . . . , Q a ma , Q w 1 , . . . , Q w mw }. Meanwhile, the neural recordings y ∈ R N ×T can be reconstructed by y = C Re[Gf ] + d + ϵ, with C, d, ϵ from CSM-GPFA in Section 2.2,</formula><p>and G in Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-Region Markovian Gaussian Process with Switching States</head><p>After the link between the multi-region Gaussian Process and linear dynamical system (LDS) is established, we can seamlessly extend the across-region discrete-time LDS in Eq. 10 to incorporate switching states.</p><p>Integrating a Hidden Markov Model (HMM) into LDS leads to Switching LDS <ref type="bibr" target="#b0">(Fox et al., 2008)</ref>, and similarly, combining HMM with MRM-GP results in Switching MRM-GP. A significant advantage of this integration is the ability to link the across-region's transition and measurement matrices with distinct, discrete states z ∈ {1, . . . , Z}:</p><formula xml:id="formula_25">A a z = diag{A a 1,z , . . . , A a ma,z }, Q a z = diag{Q a 1,z , . . . , Q a ma,z</formula><p>}, which makes it easy to accommodate time-varying frequencies and delays in across-region latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>We have now established a connection between a Gaussian Process with a multi-region kernel and a linear dynamical system (LDS). The next step is to learn discrete states, model parameters, and latent variables. MRM-GP, as a discrete-time LDS, affords a significant advantage: the ability to learn its parameters with a cost linear in time steps: O(T ). To achieve this, we employ the variational Laplace EM inference algorithm proposed in the gen-eral recurrent state space framework for decision-making <ref type="bibr" target="#b24">(Zoltowski et al., 2020)</ref>.</p><p>If denoting the number of discrete states as Z, the parameters θ of MRM-GP can be categorized into two groups: (1) kernel parameters: {σ m,z , η m,z } ma,Z m=1,z=1 , {σ m,p , η m,p } mw,P m=1,p=1 , {ϕ m,z pp ′ } ma,P,Z m=1,p=1,p ′ =p+1,z=1 ; (2) emissions parameters: C, d, V. Additionally, the hyperparameters consist of the number of discrete states Z, the number of derivatives k, the kernel rank R, and the number of latent dimensions M . The value of Z depends on the data, k is discussed in Section 3.1 and Figure <ref type="figure">2</ref>, M is determined through a cross-validation strategy (Section 5.2), and the rank R is consistently set to 2 to ensure positive definiteness without introducing many amplitude parameters. Besides, there is no need to learn the amplitude parameters, denoted as {a m,r p } P,M,R p=1,m=1,r=1 , since the emissions parameter C fulfills a similar role in MRM-GP.</p><p>The variational Laplace EM inference algorithm alternatively updates discrete switching states z ∈ {1, . . . , Z}, latent dynamics f ∈ C M P k×T , and model parameters θ. The time complexity and memory storage of each step are all linear in time as follows: (1) updating z:</p><formula xml:id="formula_26">O(Z), O(ZT ); (2) updating f : O(T ), O(2M 2 P 2 k 2 T ); (3) updating θ: O(ZM k), O(M P kT ).</formula><p>Furthermore, to avoid the calculation of the complex number when updating f in our implementations, we rewrite the complex latent dynamics f in Eq. 11 to be a joint signal in the real domain, such that the latent dynamics becomes:</p><formula xml:id="formula_27">f r f i t = A r -A i A i A r f r f i t-1 + q r q i t-1 , q r q i t-1 ∼ N 0 0 , Q r -Q i Q i Q r ,<label>(12)</label></formula><p>where f r , f i , q r , q i , A r , A i , Q r , Q i are the real and imagery part of f, q, A, Q, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our code is available at <ref type="url" target="https://github.com/WeihanLikk/MRM-GP">https://github.com/ WeihanLikk/MRM-GP</ref>.</p><p>Datasets. We evaluate MRM-GP on three datasets:</p><p>• Synthetic Data: We generate simulated data incorporating both across-region communications and within-region neural activities, along with time-varying frequencies and phase delays introduced by various states. Baselines for comparison. We compare MRM-GP with two methods designed to discover the directional communications in the latent space of multi-region recordings:</p><p>• DLAG <ref type="bibr" target="#b2">(Gokcen et al., 2022)</ref>: A Gaussian Process Factor Analysis employs a multi-output Squared Exponential kernel. Its goal is to uncover simultaneous or bidirectional latent communications across different regions. The kernel function incorporates a time delay parameter to determine the directions for learned communications.</p><p>• CSM-GPFA: The Gaussian Process Factor Analysis, using a multi-region kernel as described in Section 2.2, is an extension of the model presented in <ref type="bibr" target="#b8">(Hultman et al., 2018)</ref>. This extension introduces a new classification assumption, distinguishing latent variables into across-region and withinregion types.</p><p>Metrics. For every model and dataset, we fit the model on the training set, denoted as y train , and test its performance on the test set y test . Specifically, we randomly select some trials as y train , while the remaining trials serve as y test .</p><p>Additionally, we randomly divide the test data y test into two parts: y held-in test with 90% neurons as held-in test data and y held-out test with 10% neurons as held-out test data. We infer x held-in test based on y held-in test , which is then used as the test latent variables when computing test log-likelihood (LL) p(y held-out test |x held-in test ; θ) <ref type="bibr" target="#b12">(Pei et al., 2021)</ref>, serving as the final metric in our experiments. To reduce the randomness when creating y held-in test and y held-out test , we also average p(y held-out test |x held-in test ; θ) over five distinct partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Data</head><p>This section aims to assess how well MRM-GP can identify switching states, latent variables, and parameters.</p><p>Experimental setup. We generate 50 independent trials for two brain regions P = 2, where each region has 30 neurons, m a = 1 dimension across-region variables, and m w = 1 dimension within-region variable. We also introduce the time-varying across-region frequencies and phase delays by two discrete states Z = {z 1 , z 2 }: (1) state 1: η z1,a = 1.0 rad/s, ϕ z1</p><p>1,2 = -10 ms, σ z1,a = 10, state 2: η z2,a = 0.25 rad/s, ϕ z2</p><p>1,2 = 10 ms, σ z2,a = 10. Different sign of ϕ 1,2 means the change of directions. We set η w = 0.75 rad/s and σ w = 10 for within-region variables. For the generative and inference process, we set hyperparameters k = 2, R = 2, and compare the test log likelihood when Z = 1, 2, 3.  Results. We fit an MRM-GP to the synthetic data, specifying m a = 1 dimension across-region variables, m w = 1 dimension within-region variable, and Z = 2 states. Figure <ref type="figure">3</ref>(A) shows single-trial latent variable estimations that accurately reflect the latent dynamics and communications influenced by discrete states over two brain regions. State z 1 (depicted in blue) exhibits a periodic signal with a higher frequency and forward communication from brain region 1 to brain region 2. In contrast, state z 2 (shown in purple) displays an oscillatory pattern with a lower frequency and feedback communication from region 2 to region 1.</p><p>For a quantitative assessment of learned parameters, Figure 3(B) displays the estimated phase delays, frequencies, and length scales across different initializations, demonstrating close alignment with ground truths. Furthermore, Figure <ref type="figure">3</ref>(C) illustrates the test log-likelihood for varying Z, revealing that both Z = 2 and Z = 3 provide similar and superior estimations compared to Z = 1 (see Appendix J for Z = 3 visualization). We also have synthetic experiments about parameter initialization and different parameter setting in Appendix F,G,H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Local Field Potential Recordings</head><p>This section aims to explore interactions between the mouse's primary visual area (V1) and the visual anteromedial area (VISam) in the presence of an 8Hz drifting grating. We also aim to compare the performance and inference time cost with other multi-region methods, namely DLAG and CSM-GPFA.</p><formula xml:id="formula_28">V1 VISam V 1 -V IS a m d e la y s V 1 a -v</formula><p>Experimental setup. We conduct experiments using two sessions, each comprising eight orientation directions, resulting in 16 datasets. Each dataset includes 15 trials (10 as a training set and 5 as a testing set) of continuous-time local field potential recordings from approximately 20 neurons in V1 and approximately 25 in VISam. The initial sampling rate is 1000Hz, and we downsample it to 100Hz, resulting in 200 time points with 10ms bin size. We set hyperparameters k = 2, R = 2.</p><p>To determine the dimensionalities of across-and withinregion latent variables, we adopt the approach outlined in <ref type="bibr" target="#b2">(Gokcen et al., 2022)</ref>. Initially, we apply Factor Analysis to identify the total number of latent variables required to elucidate the neural recordings for each region. A 5-fold crossvalidation was employed to select the configuration yielding the highest test LL. Subsequently, given the selected total number of latent variables (M ), we conduct a grid search for the dimensionalities of across-(m a ) and within-region (m w ), respectively. For each pair of (m a , m w ), we run 5fold cross-validation with MRM-GP and chose the setting with the highest test LL. Given this procedure, our final choice was m a = 1 across-region variables and m w = 3 within-region variables for both V1 and VISam. See Appendix B for the full comparison.</p><p>Results. We applied an MRM-GP to local field potential recordings with Z = 1 state. Figure <ref type="figure" target="#fig_2">4</ref> </p><formula xml:id="formula_29">K pp ′ (τ ) = exp(-1 2σ 2 (τ -δ pp ′ ) 2</formula><p>), which is independent of frequency and has a different interpretation from the phase delay (ϕ pp ′ ). In this context, ϕ pp ′ represents the delay in a specific frequency band. In contrast, δ pp ′ signifies the delay for a latent variable with a mixture of multiple frequencies, as evidenced by its power spectrum with three frequency peaks (Figure <ref type="figure" target="#fig_2">4(E)</ref>). Therefore, the divergence in values is acceptable if their directions align.</p><p>The left chart in Figure <ref type="figure" target="#fig_2">4</ref>(B) illustrates the estimated phase delays across all 16 datasets. Each data point represents the phase delay for an individual run on a specific dataset. The findings suggest consistent communication from V1 to VISam across all datasets, with the phase delays clustered around 7.5Hz (the left chart in Figure <ref type="figure" target="#fig_2">4</ref>(C)), which is consistent with external 8Hz stimulus.</p><p>To demonstrate that the MRM-GP itself is not the cause of delays, we first divide V1 randomly into two parts, V1a and V1b, each with channels of equal size. We then estimated the phase delays between them. The right chart in Figure <ref type="figure" target="#fig_2">4</ref>(B) illustrates that across 16 datasets, all phase delays hover around zero within frequency bands around 7.5Hz (the right chart in Figure <ref type="figure" target="#fig_2">4(C)</ref>). This suggests that the learned delays are a consequence of the data rather than the model. Figure <ref type="figure" target="#fig_2">4</ref>(D) shows that MRM-GP, a linear dynamics system approximation of CSM-GPFA, exhibits a similar test LL to CSM-GPFA. The higher test LL compared to DLAG suggests that the multi-region kernel (Eq. 1) outperforms DLAG's Squared Exponential kernel on these datasets. This is attributed to the former explicitly modeling frequencies through its kernel parameters and having a better frequency separation. Specifically, the across-region variable of the former has only one prominent frequency, whereas DLAG's across-region variable exhibits three peaks in Figure4(E), keeping consistent with the data spectrum in Appendix E. Lastly, in Figure <ref type="figure" target="#fig_2">4</ref>(F), we compare the inference time of MRM-GP, CSM-GPFA, and DLAG for 500 iterations. We achieved this by downsampling the recordings and creating four datasets with varying lengths of time points: 50, 100, 150, and 200. The results indicate that the time cost of MRM-GP increases linearly, whereas both CSM-GPFA and DLAG exhibit cubic growth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Neural Spike Trains</head><p>This section aims to evaluate MRM-GP's ability to identify switching states within the communications subspace while also discovering across-region communications using a distinct type of neural data.</p><p>Experimental setup. The simultaneous spike trains were obtained from the monkey's primary visual area (V1) and secondary visual cortex (V2) in the presence of a 6Hz moving grating. This dataset comprises four sessions, each featuring eight orientation directions, resulting in 32 datasets. Each dataset comprises 400 trials (64 time points with 20ms bin size for every trial), with 300 trials randomly selected as the training set and 100 trials as the testing set. In V1, there are approximately 90 neurons, while in V2, there are around 20 neurons. We set hyperparameters k = 2, R = 2.</p><p>Results. We fitted an MRM-GP to neural spikes trains with m a = 2 dimensions across-region variables, m w = 2 dimension within-region variable, and Z = 2 states. The configuration of m a and m w follows previous work <ref type="bibr" target="#b2">(Gokcen et al., 2022)</ref> and adopts the same strategy mentioned in Section 5.2.</p><p>Figure <ref type="figure">5</ref>(A) shows the across-region latent variables for one dataset (orientation 0 • , session 106r001p26, ten trials are displayed, all variables are scaled by the variance explained in each region), and the within-region variables are in Appendix B. These latent variables indicate time-varying forward and feedback communications between V1 and V2. Different states exhibit distinct phase delays and frequencies. The first dimension of across-region variables (denoted as x a 1 ) displays a periodic pattern caused by the external drifting grating stimulus, whereas the second dimension (denoted as x a</p><p>2 ) exhibits a non-periodic signal with a single peak shortly after the stimulus onset.</p><p>Figure <ref type="figure">5(B-C</ref>) presents the estimated phase delays and frequencies over multiple independent runs for 32 datasets. Each data point represents a dimension of across-region variables. Figure <ref type="figure">5</ref>(B) corresponds to state z 1 , indicating that most state z 1 dimensions exhibit across-region interactions within the 2Hz-8Hz range. Additionally, some dimensions display feedback communication with a large phase delay (&gt;10ms) from V2 to V1, corresponding to state z 1 of x a 2 in Figure <ref type="figure">5</ref>(A). However, there is variability across datasets for state z 1 with smaller phase delays (&lt;10ms). Some show forward communication from V1 to V2, akin to state z 1 of x a 1 in Figure <ref type="figure">5</ref>(A), while others indicate feedback communications from V2 to V1.</p><p>One explanation for this variability is that in certain datasets, x a 1 has a much weaker amplitude compared to x a 2 , making the weaker latent affected by the stronger one along with its delay. This leads to a feedback signal from V2 to V1 at state z 1 . On the other hand, in some datasets (e.g., orientation 0 • , session 106r001p26), x a 1 is not as weak, resulting in a forward signal from V1 to V2 at state z 1 .</p><p>Figure <ref type="figure">5</ref>(C) depicts the estimated phase delays and frequencies associated with state z 2 . The findings indicate a clear separation of oscillatory communications into two frequencies. One involves 6Hz communications with small phase delays (referring to state z 2 of x a 1 in Figure <ref type="figure">5</ref>(A)), while the other involves 1Hz communications with large phase delays (akin to state z 2 of x a 2 in Figure <ref type="figure">5</ref>(A)). The time-varying phase delays can be explained as follows:</p><p>(1) For x a 1 , V1 triggers V2 to have oscillatory dynamics during state z 1 , while in state z 2 , V2 is already engaged, causing both regions to oscillate synchronously, resulting in a smaller phase delay than in state z 1 . (2) For x a 2 , V2 consistently sends signals with a low frequency to V1, resulting in a larger phase delay due to the longer period as indicated by z 2 . During state z 1 , the stimulus onset triggers an intense signal from V2 to V1, leading to a smaller phase delay, which can be considered as an emergence of surprise or prediction error from V2 to V1 <ref type="bibr" target="#b13">(Rao &amp; Ballard, 1999)</ref>.</p><p>Similar to Section 5.2, we also perform a control experiment by learning the phase delays between V1a and V1b. In Figure <ref type="figure">5</ref>(D), the outcomes reveal zero-delay communications that are distributed across two frequencies (6Hz, 1Hz), suggesting that learned delays are a consequence of the data rather than the model. Finally, we compare the test LL of MRM-GP, CSM-GPFA, and DLAG in Figure <ref type="figure">5(E)</ref>. The results indicate that MRM-GP with Z = 2 states achieves the highest LL, while MRM-GP with Z = 1 state exhibits a similar LL compared to CSM-GPFA, and both outperform DLAG. This suggests that (1) switching states exist in these datasets; (2) the multiregion kernel is more appropriate than the Squared Exponential kernel for modeling signals with sinusoidal structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>MRM-GP establishes the connection between a linear dynamics system (LDS) and a multi-output Gaussian Process (GP) explicitly modeling frequency-based communications and their directionality via phase delays within the latent space of neural data.</p><p>Connecting a complex-valued GP with an LDS is non-trivial. Although a complex-valued GP can be written as a multioutput real-valued GP (by twice the dimension), the resulted multi-output GP cannot be converted to an LDS by spectral factorization because the separability of the resulted multioutput kernel is not guaranteed.  Figure <ref type="figure">5</ref>. Applying MRM-GP to neural spike trains. A: Visualize across-region latent variables with state z1 (in red) and state z2 (in blue). The results suggest forward and feedback communications between V1 and V2, along with time-varying delays. B-C: Visualize estimated parameters through phase delays on the x-axis and frequencies on the y-axis. D: Depict phase delays and frequencies learned in the V1a-V1b control experiment, with their representation on the x-axis and y-axis, respectively. E: Compare the test LL with discrete states Z = 1, 2 and other multi-region methods: DLAG and CSM-GPFA.</p><formula xml:id="formula_30">M R M -G P , Z = 1 M R M -G P , Z = 2 C S M -G P F A D L A G test LL (A) (B) (D) (E) (C)</formula><p>Once the link is established, we can harness several advantages: (1) using the powerful representational capability of kernels, such as applying a multi-region kernel to model latent variables with periodic patterns; (2) achieving a linear computational cost; (3) incorporating time-varying frequencies and delays by introducing different discrete states.</p><p>We test MRM-GP using two distinct types of neural data. The findings showcase its capability to discover statedependent latent communications across brain regions with a linear time inference cost.</p><p>Finally, the limitations of MRM-GP are twofold: (1) its reliance on separability for multi-output kernels, which restricts kernel selection options, and (2) its current model assumptions are unable to capture phenomena such as phase resetting and phase variability across different trials.</p><p>A. Spectral Factorization. Derivation for Eq. 6. Start with Eq. 3, taking Fourier transforms on both sides gives:</p><p>(iω)J(iω) p,w m = F w m J(iω) p,w m + LU(iω).</p><p>(13) Solving for J(iω) p,w m gives:</p><p>J(iω) p,w m = ((iω -F w m )I) -1 LU(iω).</p><p>Recall that f (t) p,w m in Eq. 3 is a Complex Gaussian Process with single-output kernel K(τ ) m = R r=1 a m r 2 exp(-1 2σ m 2 τ 2 + iη m (τ )) and its derivatives up to (k -1) th . So, the spectral density matrix of this process f (t) p,w m is:</p><formula xml:id="formula_32">S J (ω) = E[J(iω) p,w m J(-iω) p,w m ⊤ ].<label>(15)</label></formula><p>Bring Eq. 14 into Eq. 15 gives:</p><formula xml:id="formula_33">S J (ω) =(F w m -iωI) -1 LE[U(iω)U (-iω) ⊤ ]L ⊤ (F w m + iωI) -T , = (F w m -iωI) -1 LvL ⊤ (F w m + iωI) -T . (<label>16</label></formula><formula xml:id="formula_34">)</formula><p>Finally S(ω) in Eq. 6 is: </p><formula xml:id="formula_35">S(ω) = GS J (ω)G ⊤ .<label>(17)</label></formula><p>where the eigenvalues of this matrix are the roots for T (iω) <ref type="bibr" target="#b7">(Hom &amp; Johnson, 1985)</ref>. Notably, the companion matrix is structured as a Hessenberg matrix, suggesting that its eigenvalues can be obtained through the QR algorithm with Givens rotation. This process has a time complexity of O(k) for each iteration. Figure <ref type="figure" target="#fig_0">13</ref>. Determine the number of dimensions: (A-B) Factor Analysis is applied to V1 and VISam, revealing a gradual increase in test log-likelihood (LL) when the number of dimensions surpasses four (averaged over 5-fold cross-validation) . Considering both the inference time for all three models, we opt for four as the total dimensions for each region. (C) Conducting a grid search for across-and within-region variables using relative test LL as the metric-the difference between the model's test LL and Factor Analysis's test LL.</p><p>The outcomes indicate that one across-region and three within-region variables yield the highest LL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of two dimensions across-region latent variables and one dimension within-region latent variable. Brain region A and region B have bidirectional communications within different frequency bands. Each region also has a one-dimensional neural activity unrelated to the other region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Applying MRM-GP to LFP recordings. A: Compare the across-region latent variables with DLAG. B-C: Visualize the learned phase delays and frequencies from V1-VISam and V1a-V1b. D: Compare the test LL with other multi-region methods: DLAG and CSM-GPFA. E: The power spectrum of across-region latent variables in A. The latent variable from DLAG exhibits three frequency peaks, while MRM-GP's latent variable has only one peak. F: Inference time comparison: MRM-GP has a linear time cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(A) shows a comparison of single-trial across-region latent variables for one dataset (orientation 135 • , session 721123822), and the within-region variables are in Appendix B. Both latent variables demonstrate an oscillatory structure, capturing characteristics of the external 8Hz drifting grating stimulus. The MRM-GP's latent variable in this dataset is linked to a communication direction from V1 to VISam with an 8.2 ms phase delay, and the DLAG's latent variable shows a 1.1 ms time delay. Both delays fall within a single time bin (10ms) and are positive, suggesting consistent communication direction from V1 to VISam. The difference in their values arises because DLAG models time delay (δ pp ′ ) in the kernel equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Finding</head><label></label><figDesc>roots for T (iω) in Eq. 8. Using the T (iω)'s coefficients b 0 , b 1 , . . . , b 2k , we can create a companion matrix:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure3. Applying MRM-GP to synthetic data. (A): Compare the estimated latent variables and discrete states with the ground truth. MRM-GP accurately identifies two states with time-varying frequencies and phase delays, aligning with the ground truth.</figDesc><table><row><cell></cell><cell cols="4">across-region latent variables</cell><cell></cell><cell cols="3">within-region latent variables</cell></row><row><cell></cell><cell cols="3">estimated state 1 latent</cell><cell cols="3">estimated state 2 latent</cell><cell></cell><cell>estimated latent</cell></row><row><cell></cell><cell cols="3">true state 1 latent</cell><cell cols="3">true state 2 latent</cell><cell></cell><cell>true latent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>estimated latent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>true latent</cell></row><row><cell></cell><cell>state 1</cell><cell></cell><cell></cell><cell>state 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>phase delay (ms)</cell><cell>state 1 state 2</cell><cell>angular frequency (rad/s)</cell><cell cols="2">st a te w it h in -r e g io n 1 1 st a te 2 w it h in -r e g io n 2</cell><cell>length scale</cell><cell>st a te w it h in -r e g io n 1 1 st a te 2 w it h in -r e g io n 2</cell><cell>test LL</cell><cell>o n e st a te tw o st a te s th re e st a te s</cell></row></table><note><p>(B): Compare learned parameters with the ground truth indicated by dashed lines. (C): Examine the test LL with varying numbers of discrete states Z. The findings indicate that Z = 2 and Z = 3 exhibit similar LL, both larger than Z = 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>I. Cross-validating for the dimensionality of MRM-GP</figDesc><table><row><cell>(A)</cell><cell>Factor Analysis for V1</cell><cell>(B)</cell><cell>Factor Analysis for VISam</cell></row><row><cell>test LL</cell><cell></cell><cell>test LL</cell><cell></cell></row><row><cell></cell><cell>the number of dimensions</cell><cell></cell><cell>the number of dimensions</cell></row><row><cell></cell><cell>(C)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>rela�ve test LL</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">the number of across and within dimensions</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>School of Computational Science &amp; Engineering, Georgia Institute of Technology, Atlanta, USA. Correspondence to: Anqi Wu &lt;anqiwu@gatech.edu&gt;. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>235, 2024.  Copyright 2024 by the author(s).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is supported by <rs type="funder">National Institutes of Health BRAIN initiative</rs> (<rs type="grantNumber">1U01NS131810</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WptUyAV">
					<idno type="grant-number">1U01NS131810</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>The MRM-GP introduces an innovative and efficient method for investigating intricate interactions among brain regions. Its capacity to deliver an interpretable representation of multi-region neural data is poised to advance neuroscience, offering the potential for a more profound comprehension of brain function and disorders. This enhanced understanding of brain interactions has the prospect to drive advancements in neurotechnology, with potential benefits extending to fields such as brain-computer interfaces and personalized medicine. synthe�c data with a wider range of frequencies and delays </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian learning of switching linear dynamical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent switching dynamical systems models for multiple interacting neural populations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Whiteway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14867" to="14878" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disentangling the flow of signals between populations of neurons</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gokcen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zandvakili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Machens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="512" to="525" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncovering motifs of concurrent signaling across multiple neuronal populations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gokcen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Machens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Byron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyseventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Kalman filtering: Theory and Practice with MATLAB</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Andrews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cortical connectivity and sensory coding</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Mrsic-Flogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">503</biblScope>
			<biblScope unit="issue">7474</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kalman filtering and smoothing solutions to temporal gaussian process regression models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE international workshop on machine learning for signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge University Express</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">455</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Brain-wide electrical spatiotemporal dynamics encode depression vulnerability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hultman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ndubuizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Parise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="180" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Linear estimation. Number BOOK</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principles of corticocortical communication: proposed schemes and design considerations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gokcen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Machens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Byron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="725" to="737" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Working memory 2.0</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lundqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bastos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="475" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural latents benchmark&apos;21: evaluating latent variable models of neural population activity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Churchland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04463</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatiotemporal learning via infinite-dimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of spectral factorization methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical linear algebra with applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="467" to="496" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cortical areas interact through a communication subspace</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zandvakili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Machens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Survey of spiking in the mouse visual system reveals functional hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Graddis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Luviano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">592</biblScope>
			<biblScope unit="issue">7852</biblScope>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explicit link between periodic covariance functions and state space models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Särkkä</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="904" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Stochastic differential equation methods for spatio-temporal gaussian process regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gp kernels for cross-spectrum analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dzirasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extraction and recovery of spatio-temporal structure in latent dynamics alignment with diffusion model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous v1-v2 neuronal population recordings in anesthetized macaque monkeys</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zandvakili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kohn</surname></persName>
		</author>
		<idno type="DOI">10.6080/K0B27SHN</idno>
		<ptr target="https://doi.org/10.6080/K0B27SHN" />
	</analytic>
	<monogr>
		<title level="j">CRCNS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Markovian gaussian process variational autoencoders</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Balsells-Rodas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="42938" to="42961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A general recurrent state space framework for modeling neural dynamics during decision-making</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11680" to="11691" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
