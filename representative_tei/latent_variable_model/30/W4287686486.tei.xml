<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Doubly Stochastic Variational Inference for Neural Processes with Hierarchical Latent Variables</title>
				<funder>
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-10-30">30 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
						</author>
						<title level="a" type="main">Doubly Stochastic Variational Inference for Neural Processes with Hierarchical Latent Variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-30">30 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2008.09469v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce predictive distributions. However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while targetspecific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent decades, increasingly attention has been focused on deep neural networks, and the success of deep learning in computer vision, natural language processing and robotics control etc. can be attributed to the great potential of function approximation with high-capacity models <ref type="bibr" target="#b22">(LeCun et al., 2015)</ref>. Despite this, there still remain some limitations which incur doubts from industry when applying these models to real world scenarios. Among them, uncertainty quantification is long-standing and challenging, and instead of point estimates we prefer probabilistic estimates with meaningful confidence values in predictions.</p><p>With uncertainty estimates at hand, we can relieve some risk and make relatively conservative choices in cost-sensitive 1 Amsterdam Machine Learning Lab, University of Amsterdam, Amsterdam, the Netherlands. Correspondence to: Qi Wang &lt;q.wang3@uva.nl&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). decision-making <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016)</ref>.</p><p>Faced with such reality, Bayesian statistics provides a plausible schema to reason about subjective uncertainty and stochasticity, and marrying deep neural networks and Bayesian approaches together satisfies practical demands. Traditionally, Gaussian processes (GPs) <ref type="bibr" target="#b31">(Rasmussen, 2003)</ref> as typical non-parametric models can be used to handle uncertainties by placing Gaussian priors over functions. The advantage of introducing distributions over functions lies in the characterization of the underlying uncertainties from observations, enabling more reliable and flexible decision-making. For example, the uncertainty-aware dynamics model enjoys popularity in model-based reinforcement learning, and GPs deployed in PILCO enable propagation of uncertainty in forecasting future states <ref type="bibr" target="#b4">(Deisenroth &amp; Rasmussen, 2011)</ref>. Another specific instance can be found in demonstration learning; higher uncertainty in prediction would suggest the learning system to query new observations to avoid dangerous behaviors <ref type="bibr" target="#b43">(Thakur et al., 2019)</ref>. During the past few years, a variety of models inspired by GPs and deep neural networks have been proposed <ref type="bibr" target="#b34">(Salimbeni &amp; Deisenroth, 2017;</ref><ref type="bibr" target="#b37">Snelson &amp; Ghahramani, 2006;</ref><ref type="bibr" target="#b44">Titsias, 2009;</ref><ref type="bibr" target="#b45">Titsias &amp; Lawrence, 2010)</ref>.</p><p>However, GP induced predictive distributions are met with some concerns. One is high computational complexity in prediction due to the matrix inversion, and another is less flexibility in function space. Recognized as an explicit stochastic process model, the vanilla GP strongly depends on the assumption that the joint distribution is Gaussian, and such a unimodal property makes it tough to scale to more complicated cases. These issues facilitate the birth of adaptations or approximate variants for GP related models <ref type="bibr">(Garnelo et al., 2018a;</ref><ref type="bibr" target="#b29">b;</ref><ref type="bibr" target="#b24">Louizos et al., 2019)</ref>, which incorporate latent variables in modeling to account for uncertainties. Among them, Neural Processes (NPs) are relatively representative with advantages like uncertainty-aware prediction and efficient computations.</p><p>In this paper, we investigate NP related models and explore more expressive approximations towards general stochastic processes <ref type="bibr">(SPs)</ref>. The main focus is on a novel variational approximate model for NPs to solve learning problems in high-dimensional cases. To improve flexibility in predictive distributions, hierarchical latent variables are considered as part of the model structure. Our primary contributions can be summarized as follows.</p><p>• We systematically revisit NPs, SPs and other related models from a unified perspective with an implicit Latent Variable Model (LVM). Both GPs and NPs are studied in this hierarchical LVM.</p><p>• The Doubly Stochastic Variational Neural Process (DSVNP) is proposed to enrich the NP family, where both global and target specific local latent variables are involved in a predictive distribution.</p><p>• Experimental results demonstrate effectiveness of the proposed Bayesian model in high dimensional domains, including regression with multiple outputs and uncertainty-aware image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deep Latent Variable Model as Stochastic Processes</head><p>Generally, a stochastic process places a distribution over functions and any finite collections of variables can be associated with an implicit probability distribution. Here, we naturally formulate an implicit LVM <ref type="bibr" target="#b32">(Rezende et al., 2014;</ref><ref type="bibr" target="#b19">Kingma &amp; Welling, 2014)</ref> to characterize General Stochastic Function Processes (GSFPs). The conceptual generation paradigm for this LVM can be depicted in the following equations,</p><formula xml:id="formula_0">z i = φ(x i ) + (x i )<label>(1)</label></formula><formula xml:id="formula_1">y i = ϕ(x i , z i ) + ζ i (2)</formula><p>where terms and ζ respectively indicate the stochastic component in the latent space and random noise in observations. To avoid ambiguity in notation, the stochastic term is declared as an index dependent random variable (x i ), and ζ i is observation noise in the environment. Also, the transformations φ and ϕ are assumed to be Borel measurable, and the latent variables in Eq. (A.5) are not restricted. Note that they can be some set of random variables with statistical correlations without loss of generality. When Kolmogorov Extension Theorem <ref type="bibr" target="#b29">(Oksendal, 2013</ref>) is satisfied for (x i ), a latent SP can be induced. Eq. (A.5) decomposes the process into a deterministic component and a stochastic component in some models. The transformation ϕ in Eq. (B.2) is directly connected to the output. Such a generative process can simultaneously inject aleatoric uncertainty and epistemic uncertainty in modelling <ref type="bibr" target="#b17">(Hofer et al., 2002)</ref>, but inherent correlations in examples make the exact inference intractable mostly.</p><p>Another principal issue is about prediction with permutation invariance, which learns a conditional distribution in SP models. With the context C = {(x i , y i )|i = 1, 2, . . . , N } and input variables of the target x T , we seek a stochastic function f θ mapping from X to Y and formalize the distribution as p θ (y T |x C , y C , x T )<ref type="foot" target="#foot_0">foot_0</ref> invariant to the order of context observations. The definitions about permutation invariant functions (PIFs) and permutation equivariant functions (PEFs) are included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Gaussian Processes in the Implicit LVM</head><p>Let us consider a trivial case in the LVM when the operation ϕ is an identity map, ζ is Gaussian white noise, and the latent layer follows a multivariate Gaussian distribution. This degenerated case indicates a GP, and terms φ, are respectively the mean function and the zero-mean GP prior. Meanwhile, recall that the prediction at target input x T in GPs relies on a predictive distribution p(y</p><formula xml:id="formula_2">T |x C , y C , x T ),</formula><p>where the mean and covariance matrix are inferred from the context [x C , y C ] and target input x T .</p><formula xml:id="formula_3">µ(x T ; x C , y C ) = φ θ (x T ) + Σ T,C Σ -1 C,C y C -φ θ (x C ) Σ(x T ; x C , y C ) = Σ T,T -Σ T,C Σ -1 C,C Σ C,T<label>(3)</label></formula><p>Here φ θ and Σ in Eq. (B.4) are vectors of mean functions and covariance matrices. For additive terms, they embed context statistics and connect them to the target sample x T . Furthermore, two propositions are drawn, which we prove in Appendix B.</p><p>Proposition 1. The statistics of GP predictive distributions, such as mean and (co)-variances, for a specific point x * are PIFs, while those in p(y T |x C , y C , x T ) are PEFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Processes in the Implicit LVM</head><p>In non-GP scenarios, inference and prediction processes for the LVM can be non-trivial, and NPs are the family of approximate models for implicit SPs. Also, relationship between GPs and NPs can be explicitly established with deep kernel network <ref type="bibr" target="#b33">(Rudner et al., 2018)</ref>. Note that NPs translate some properties of GPs to predictive distributions, especially permutation invariance of context statistics, which is highlighted in Proposition 1. Here three typical models are investigated, respectively conditional neural process (CNP) <ref type="bibr">(Garnelo et al., 2018a)</ref>,vanilla NP <ref type="bibr">(Garnelo et al., 2018b)</ref> and attentive neural process (AttnNP) <ref type="bibr" target="#b18">(Kim et al., 2019)</ref>.</p><p>When approximate inference is used in NP family with Latent Variables, a preliminary evidence lower bound (ELBO) for the training process can be derived, which aims at pre- dictive distributions for most NP related models.</p><formula xml:id="formula_4">ln p(y T |x C , y C , x T ) ≥ E q φ (z T ) ln p θ (y T |x T , z T ) -D KL q φ (z T |x C , y C , x T , y T ) p(z T |x C , y C , x T )<label>(4)</label></formula><p>To ensure context information invariant to orders of points, CNP embeds the context point [x C , y C ] in an elementwise way and then aggregates them with a permutation invariant operation ⊕, such as mean or max pooling.</p><formula xml:id="formula_5">r i = h θ (x i , y i ), r C = N i=1 r i<label>(5)</label></formula><p>The latent variable in CNP is a deterministic embedding of the form </p><formula xml:id="formula_6">p θ (z C |x C , y C ) = r C (x C , y C ).</formula><formula xml:id="formula_7">z attn = N i=1 w(x i , x * )s i , z = [z attn , z G ]<label>(6)</label></formula><p>As a summary, AttnNP boosts performance with attention networks, which implicitly seeks more flexible functional translations for each target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Connection to Other Models</head><p>In some scenarios, when the latent layer in Eq. (A.5) is specified as a Markovian chain, the LVM degrades to classical state space model. If random variables in the latent layer of the LVM are independent, the resulted neural network is similar to the conditional variational auto-encoder <ref type="bibr" target="#b39">(Sohn et al., 2015)</ref> and no context information is utilized for prediction. Instead, the existence of correlations between latent variables in the hidden layer increases the model capacity.</p><p>The induced SP in Eq. (B.2) is a warped GP when the latent SP is a GP and the transformation ϕ is nonlinear monotonic <ref type="bibr" target="#b38">(Snelson et al., 2004)</ref>. In addition, several previous works integrate this idea in modelling as well, and representative examples are deep GPs <ref type="bibr" target="#b3">(Dai et al., 2016)</ref> and hierarchical GPs <ref type="bibr" target="#b47">(Tran et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Process under Doubly Stochastic Variational Inference</head><p>In the last section, we gain more insights about mechanism of GPs and NPs and disentangle these models with the implicit LVM. A conclusion can be drawn that the posterior inference conditioned on the context requires both approximate distributions with permutation invariance and some bridge to connect observations and the target in latent space.</p><p>Note that the global induced latent variable may be insufficient to describe dependencies, and critical challenge comes from non-stationarity and locality, which are even crucial Formally, the generative model as a SP is described as follows, where exact inferences for latent variables z G and z * are infeasible.</p><formula xml:id="formula_8">ρ x 1:N +M (y 1:N +M ) = N +M i=1 p(y i |z G , z i , x i ) p(z i |x i , z G )p(z G )dz 1:N +M dz G (7)</formula><p>Meanwhile, we emphasize that this generation method naturally induces an exchangeable stochastic process <ref type="bibr" target="#b0">(Bhattacharya &amp; Waymire, 2009)</ref>. (The proof is given in Appendix C.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Approximate Inference and ELBO</head><p>With the relationship between these variables clarified, we can characterize the inference process for DSVNP and then a new ELBO is presented. </p><formula xml:id="formula_9">q φ1,1 = N z G |µ(x C , y C , x T , y T ), Σ(x C , y C , x T , y T ) (8) q φ2,1 = N z * |µ(z G , x * , y * ), Σ(z G , x * , y * )<label>(9)</label></formula><p>where q φ1,1 and q φ2,1 are approximate posteriors in training process. The generative process is reflected in Eq. (D.5) for DSVNP, where g θ indicates a decoder in a neural network.</p><formula xml:id="formula_10">p(y * |x C , y C , x * ) = g θ (z G , z * , x * )<label>(10)</label></formula><p>Consequently, this difference between vanilla NP and DSVNP leads to another ELBO or negative variational free energy L as the right term,</p><formula xml:id="formula_11">ln p(y * |x C , y C , x * ) ≥ E q φ 1,1 E q φ 2,1 ln[p(y * |z G , z * , x * )] -E q φ 1,1 [D KL [q φ2,1 (z * |z G , x * , y * ) p φ2,2 (z * |z G , x * )] -D KL q φ1,1 (z G |x C , y C , x T , y T ) p φ1,2 (z G |x C , y C )<label>(11)</label></formula><p>where p φ1,2 (z G |x C , y C ) and p φ2,2 (z * |z G , x * ) parameterized with neural networks are used as prior distributions.</p><p>Here we no longer employ standard normal distributions with zero prior information, and instead these are parameterized with two diagonal Gaussians for the sake of simplicity and learned in an amortized way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scalable Training and Uncertainty-aware Prediction</head><p>Based on the inference process in DSVNP and the corresponding ELBO in Eq. (D.6), the Monte Carlo estimation for the lower bound is derived, in which we wish to maximize,</p><formula xml:id="formula_12">L M C = 1 K K k=1 1 S S s=1 ln[p(y * |x * , z (s) * , z<label>(k)</label></formula><p>G )]</p><formula xml:id="formula_13">-D KL [q(z * |z (k) G , x * , y * ) p(z * |z (k) G , x * )] -D KL q(z G |x C , y C , x T , y T ) p(z G |x C , y C ) (12)</formula><p>where latent variables are sampled as z</p><formula xml:id="formula_14">(k) G ∼ q φ1,1 (z G |x C , y C ) and z (s) * ∼ q φ2,1 (z * |z (k) G , x * , y * ).</formula><p>And the resulted Eq. ( <ref type="formula">12</ref>) is employed as the objective function in the training process. To reduce variance in sampling, the reparameterization trick <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014)</ref> is used for all approximate distributions and the model is optimized using Stochastic Gradient Variational Bayes <ref type="bibr" target="#b19">(Kingma &amp; Welling, 2014)</ref>. More details can be found in Algorithm (1).</p><p>The predictive distribution is of our interest. For DSVNP, prior networks as p(z G |x C , y C ) and p(z * |z G , x * ) are involved in prediction, and this leads to the integration over both global and local latent variables here as revealed in Eq. (13).</p><formula xml:id="formula_15">p(y * |x C , y C , x * ) = p(y * |z G , z * , x * )p φ1,2 (z G |x C , y C ) p φ2,2 (z * |z G , x * )dz G dz * (13)</formula><p>For uncertainty-aware prediction, there exist different approaches for Bayesian neural networks. Generally, once the model is well trained, the conditional distribution in neural networks can be derived. The accuracy can be evaluated through deterministic inference over latent variables, i.e.,</p><formula xml:id="formula_16">zG = E[z G |x C , y C ], z * = E[z * |z G , x * ], y * = arg max y p(y| z * , zG , x * ).</formula><p>The Monte Carlo estimation over Eq. ( <ref type="formula">13</ref>), which is commonly used for prediction, can be written in the following equation, Compute conditional probability distribution in Eq. (D.5); Update parameters by Optimizing Eq. ( <ref type="formula">12</ref>):</p><formula xml:id="formula_17">p(y * |x C , y C , x * ) ≈ 1 KS K k=1 S s=1 p θ (y * |x * , z (s) * , z (k) G )<label>(14</label></formula><formula xml:id="formula_18">φ 1 ← φ 1 + α∇ φ1 L M C φ 1 = [φ 1,1 , φ 1,2 ] φ 2 ← φ 2 + α∇ φ2 L M C φ 1 = [φ 2,1 , φ 2,2 ] θ ← θ + α∇ θ L M C end for</formula><p>where the global and local latent variables are sampled in prior networks through ancestral sampling as z</p><formula xml:id="formula_19">(k) G ∼ p φ1,2 (z G |x C , y C ) and z (s) * ∼ p φ2,2 (z * |z (k) G , x * ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">More Insights and Implementation Tricks</head><p>The global latent variable and local latent variables govern different variations in prediction and sample generation. This is a part of motivations for AttnNP and DSVNP. Interestingly, the inference for our induced SP integrates the aspects of vanilla NPs <ref type="bibr" target="#b8">(Eslami et al., 2018)</ref> and C-VAEs <ref type="bibr" target="#b39">(Sohn et al., 2015)</ref>.</p><p>Similar to β-VAE <ref type="bibr" target="#b16">(Higgins et al., 2017)</ref>, we rewrite the right term in Eq. (D.6) with constraints and these restrict the search for variational distributions. Equivalently, tuning the weights of divergence terms in Eq. (D.6) leads to varying balance between global and local information.</p><formula xml:id="formula_20">max φ1,φ2,θ E q φ 1,1 E q φ 2,1 ln[p θ (y * |z G , z * , x * )] D KL q(z G |x C , y C , x T , y T ) p(z G |x C , y C ) &lt; G E q φ 1,1 D KL [q(z * |z G , x * , y * ) p(z * |z G , x * )] &lt; L (15)</formula><p>Here, a more practical objective in implementations derived by weight calibrations in Eq. ( <ref type="formula">16</ref>).</p><formula xml:id="formula_21">L W M C = 1 K K k=1 1 S S s=1 ln[p(y * |x * , z (s) * , z (k) G )] -β 1 D KL [q(z * |z (k) G , x * , y * ) p(z * |z (k) G , x * )] -β 2 D KL q(z G |x C , y C , x T , y T ) p(z G |x C , y C ) (16)</formula><p>Also, training stochastic model with multiple latent variables is non-trivial, and there exist several works about KL divergence term annealing <ref type="bibr" target="#b40">(Sønderby et al., 2016)</ref> or dynamically adapting for the weights. Importantly, the target specific KL divergence term is sometimes suggested to assign more penalty to guarantee the consistency between approximate posterior and prior distribution <ref type="bibr" target="#b20">(Kohl et al., 2018;</ref><ref type="bibr" target="#b39">Sohn et al., 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we start with learning predictive functions on several toy dataset, and then high-dimensional tasks, including system identification on physics engines, multioutput regression on real-world dataset as well as image classification with uncertainty quantification, are performed to evaluate properties of NP related models. The dot-product attention is implemented in all AttnNPs here. All implementation details are attached in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Experiments</head><p>We initially investigate the episdemic uncertainty captured by NP related models on a 1-d regression task, and the function <ref type="bibr" target="#b30">(Osband et al., 2016)</ref> is characterized as y = x+ + sin(4(x + )) + sin(13(x + )). Observations as the training set include 12 points and 8 points respectively uniformly Further, we conduct curve fitting tasks in SP. The SP initializes with a zero mean Gaussian Process y (0) ∼ GP(0, k(., .)) indexed in the interval x ∈ [-2.0, 2.0], where the radial basis kernel k(x, x ) = σ 2 exp(-(x -x ) 2 /2l 2 ) is used with l-1 0.4 and σ 1.0. Then the transformation is performed to yield y = sin (y (0) (x) + x). The training process follows that in NP <ref type="bibr">(Garnelo et al., 2018b)</ref>. Predicted results are visualized in the second and the third rows of Fig.</p><p>(2). Note that CNP only predicts points out of the context in default settings. More evidence is reported in Table <ref type="table" target="#tab_7">(1)</ref>, where 2000 realizations are independently sampled and predicted for both interpolation and extrapolation.</p><p>After several repetitive observations, we find in terms of the interpolation accuracy, DSVNP works better than vanilla NP but the improvement is not as significant as that in At-tnNP, which is also verified in visualizations. All (C)NPs show higher uncertainties around index 0, where less context points are located, and variances are relatively close in other regions. For extrapolation results, since all models are trained in the dotted column lines restricted regions, it is tough to scale to regions out of training interval and all negative log-likelihoods (NLLs) are higher. When there exist many context points located outside the interval, the learned context variable may deteriorate predictions for all (C)NPs, and observations confirm findings in <ref type="bibr" target="#b13">(Gordon et al., 2020)</ref>. Interestingly, DSVNP tends to overestimate uncertainties out of the training interval but predicted extrapolation results mostly fall into the one σ confident region, this property is similar to CNP. On the other hand, vanilla NP and AttnNP tend to underestimate the uncertainty sometimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">System Identification on Physics Engines</head><p>Capturing dynamics in systems is crucial in control related problems, and we extend synthetic experiments on a classical simulator, Cart-Pole systems, which is detailed in <ref type="bibr">(Gal et al., 2016)</ref>. As shown in Fig.</p><p>(3), the original intention is to conduct actions to reach the goal with the end of a pole, but here we focus on dynamics and the state is a vector of the location, the angle and their first-order derivatives. Specifically, the aim is to forecast the transited state For each configuration of the simulator including training and testing environments, we sample 400 trajectories of horizon as 10 steps using a random controller, and more details refer to Appendix E. The training process follows Algorithm (1) with the maximum number of context points as 100. During the testing process, 100 state transition pairs are randomly selected for each configuration of the environment, working as the maximum context points to identify the configuration of dynamics. And the collected results are reported in Table (3), where prediction performance on 5600 trajectories from 14 configurations of environments are revealed. As can be seen, the negative log-likelihood values are not consistent with those of mean square errors, and DSVNP shows both better uncertainty quantification with lowest NLLs and approximation errors in MSEs. AttnNP improves NP in both metrics, while CNP shows relatively better NLLs but the approximation error is a bit higher than others.  For classification performance with NP related models, we observe the difference is extremely tiny on MNIST with all accuracies around 99%, while on CIFAR10 DSVNP beats all baselines with highest accuracy 86.3% and lowest in-distribution entropies (Refer to <ref type="bibr">Table (2)</ref> in Appendix E). The involvement of a deterministic path does not improve much, and in contrast, MC-Dropout and CNP achieve intermediate performance. A possible cause can be implicit kernel information captured by attention network in images is imprecise. The cumulative distributions about predictive entropies are reported in Fig. <ref type="figure" target="#fig_3">(4</ref>). For models trained on MNIST, we observe no significant difference on domain dataset, but DSVNP achieves best results on FM-NIST/KMNIST and MC-Dropout performs superior on Uniform/Gaussian noise dataset. Interestingly, AttnNP tends to underestimate uncertainty on FMNIST/KMNIST and the measure is close to the neural network without dropout. Those trained on CIFAR10 are are different from observations in the second row of Fig. <ref type="figure" target="#fig_3">(4</ref>). It can be noticed DSVNP shows lowest uncertainty on domain dataset (CIFAR10) and medium uncertainty on SVHN/Gaussian/Uniform Dataset. MC-Dropout and AttnNP seem to not work so well overall, but CNP well measures uncertainty on Gaussian/Uniform dataset. Results again verify SVHN as tough dataset for the task <ref type="bibr" target="#b28">(Nalisnick et al., 2019)</ref>. Also note that entropy distri-butions on Rademacher Dataset are akin to that on domain dataset, which means the Rademacher noise is more risky for CIFAR10 classification, and DSVNP is a better choice to avoid such adversarial attack in this case. Those evidences tell us that the deterministic path in AttnNP does not boost classification performance on domain dataset but weakens the ability of o.o.d. detection mostly, while local latent variables in DSVNP improve both performance. Maybe deterministic local latent variables require more practical attention information, but here only dot-product attention information is included. As a comparison, the local latent variable in DSVNP captures some target specific information during the training process and improves detection performance with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head><p>Scalability and Expressiveness in Stochastic Process. GPs are the most well known member of SPs family and have inspired a lot of extensions, such as deep kernel learning <ref type="bibr">(Wilson et al., 2016a;</ref><ref type="bibr" target="#b29">b)</ref> and sparse GPs <ref type="bibr" target="#b37">(Snelson &amp; Ghahramani, 2006)</ref> with better scalability. Especially, the latter incorporated sparse prior in function distribution and utilized a small proportion of observations in predictions. In multi-task cases, several GP variants were proposed <ref type="bibr" target="#b27">(Moreno-Muñoz et al., 2018;</ref><ref type="bibr" target="#b1">Bonilla et al., 2008;</ref><ref type="bibr" target="#b51">Zhao &amp; Sun, 2016)</ref>. Other works also achieve sparse effect but with variational inference, approximating the posterior in GPs and optimizing ELBO <ref type="bibr" target="#b14">(Hensman et al., 2015;</ref><ref type="bibr" target="#b35">Salimbeni et al., 2019;</ref><ref type="bibr" target="#b45">Titsias &amp; Lawrence, 2010)</ref>. Another branch is about directly capturing uncertainties with deep neural networks, which is revealed in NP related models. Other extensions include generative query network <ref type="bibr" target="#b8">(Eslami et al., 2018)</ref>, sequential NP <ref type="bibr" target="#b36">(Singh et al., 2019)</ref> and convolutional conditional NP <ref type="bibr" target="#b13">(Gordon et al., 2020)</ref>. Variational implicit process <ref type="bibr" target="#b25">(Ma et al., 2019)</ref> targeted at more general SPs and utilized GPs in latent space as approximation. Sun et al. proposed functional variational Bayesian neural networks <ref type="bibr" target="#b42">(Sun et al., 2019)</ref>, and variational distribution over functions of measurement set was used to represent SPs. The more recently proposed functional NPs <ref type="bibr" target="#b24">(Louizos et al., 2019)</ref> characterized a novel family of exchangeable stochastic processes, placing more flexible distributions over latent variables and constructing directed acyclic graphs with la- tent affinities of instances in inference and prediction.</p><p>Uncertainty Quantification and Computational Complexity. GPs can well characterize aleatoric uncertainty and epistemic uncertainty through kernel function and Gaussian noise. But SPs with non-Gaussian marginals are crucial in modelling. Apart from GPs, there exist some other techniques such as Dropout <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016)</ref> or other variants of Bayesian neural networks <ref type="bibr" target="#b23">(Louizos et al., 2017)</ref> to quantify uncertainty. In <ref type="bibr" target="#b5">(Depeweg et al., 2018)</ref>, uncertainties were further decomposed in Bayesian neural network. DSVNP can theoretically capture both uncertainties as an approximate prediction model for general SPs and approaches the problem in a Bayesian way. For the computational cost in prediction, the superior sparse GPs with K-rank covariance matrix approximations <ref type="bibr" target="#b2">(Burt et al., 2019)</ref> are with the complexity O((M + N )K 2 ), while the variants of CNPs or NPs mostly reduce the complexity O((N + M ) 3 ) in GPs to O(M + N ) in prediction process. And those for AttnNP and DSVNP are O((M + N )N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>In this paper, we present a novel exchangeable stochastic process as DSVNP, which is formalized as a latent variable model. DSVNP integrates latent variables hierarchically and improves the expressiveness of the vanilla NP model. Experiments on high-dimensional tasks demonstrate better capability in prediction and uncertainty quantification. Since this work mainly concentrates on latent variables and associated inference methods, future directions can be the enhancement in the representation of latent variables, such as the use of more flexible equivariant transformations over the context or the dedicated selection of proper context points. let ν(x 1 , x 2 , .., x k ) be some probability measure over (R d ) k . Suppose the used measure satisfies Kolmogorov Extension Theorem, then there exists a probability space (Ω, F, P), which induces a general stochastic function process (GSFP)</p><formula xml:id="formula_22">F : X × Ω → R d , keeping the property ν (x1,x2,..,x k ) (C 1 × C 2 × ... × C k ) = P(F(x 1 ) ∈ C 1 , F(x 2 ) ∈ C 2 , ..., F(x k ) ∈ C k ) for all x i ∈ X , d ∈ N and measurable sets C i ∈ R d .</formula><p>The Definition 2 presents an important concept for stochastic processes in high-dimensional cases, and this is a general description for the task to learn in mentioned related works. This includes but not limited to GPs and characterizes the distribution over the stochastic function family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 1</head><p>As we know, a Gaussian distribution is closed under marginalization, conditional probability computation and some other trivial operations. Here the statistical parameter invariance towards the order of the context variables in per sample predictive distribution would be demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a multivariate Gaussian as the context</head><formula xml:id="formula_23">X = [x 1 , x 2 , . . . , x N ] T ∼ N (X; [µ 1 , µ 2 , . . . , µ N ] T , Σ(x 1 , x 2 , . . . , x N )),</formula><p>and for any permutation operation over the order π : [1, 2, . . . , N ] → [π 1 , π 2 , . . . , π N ], there exist a permutation matrix</p><formula xml:id="formula_24">P π = [e T</formula><p>π1 , e T π2 , . . . , e T π N ], where only the π i -th position is one with the rest zeros. Naturally, it results in a permutation over the random variables in coordinates.</p><formula xml:id="formula_25">P π [x 1 , x 2 , . . . , x N ] T = [x π1 , x π2 , . . . , x π N ] T = X π (B.1)</formula><p>The random variable X π follows another multivariate Gaussian as X π ∼ N (X π ; P π µ, P π ΣP T π ) = N (X π ; µ π , Σ π ). In an elementwise way, we can rewrite the statistics in the form as follows.</p><formula xml:id="formula_26">E[x πi ] = µ πi σ π ls = e T l Σ π e s = e T l P π ΣP T π e s = e π l Σe πs = cov(x π l , x πs ) (B.2)</formula><p>Notice in Eq. (B.2), the statistics are permutation equivariant now.</p><p>As the most important component in GPs, the predictive distribution conditioned on the context D = [X 1:N , Y 1:N ] can be analytically computed once GPs are well trained and result in some mean function m θ .</p><formula xml:id="formula_27">p(y * |Y 1:N ) = N (y * |μ, σ2 ) μ = m θ (x * ) + Σ x * ,D Σ -1 D,D (y D -m θ (x D )) σ2 = σ 2 x * ,x * -Σ x * ,D Σ D,D Σ D,x * (B.3)</formula><p>Similarly, after imposing a permutation π over the order of elements in the context, we can compute the first and second order of statistics between</p><formula xml:id="formula_28">D π = [X π(1:N ) , Y π(1:N ) ] and per target point [x * , y * ]. Σ x * ,Dπ = Σ x * ,D P T π = P π Σ D,x * Σ -1 Dπ,Dπ = P π Σ -1 D,D P T π y Dπ -m θ (x Dπ ) = P π (y D -m θ (x D )) (B.4)</formula><p>Hence, with the property of orthogonality of permutation matrix P π , it is easy to verify the permutation invariance in statistics for per target predictive distribution.</p><formula xml:id="formula_29">Σ x * ,D Σ -1 D,D (y D -m θ (x D )) = Σ x * ,Dπ Σ -1 Dπ,Dπ (y Dπ -m θ (x Dπ )) Σ x * ,D Σ D,D Σ D,x * = Σ x * ,Dπ Σ Dπ,Dπ Σ Dπ,x * (B.5)</formula><p>To inherit such a property, NP employs a permutation invariant function in embeddings, and the predictive distribution in NP models is invariant to the order of context points. Also, when there exist multiple target samples in the predictive distribution, it is trivial that the statistics between the context and the target in a GP predictive distribution are permutation equivariant in terms of the order of target variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of DSVNP as Exchangeable Stochastic Process</head><p>In the main passage, we formulate the generation of DSVNP as:</p><formula xml:id="formula_30">ρ x 1:N +M (y 1:N +M ) = N +M i=1 p(y i |z G , z i , x i )p(z i |x i , z G )p(z G )dz 1:N +M dz G (C.1)</formula><p>which indicates the scenario of any finite collection of random variables in y-space. Our intention is to show this induces an exchangeable stochastic process. Equivalently, two conditions for Kolmogorov Extension Theorem are required to be satisfied.</p><p>• Marginalization Consistency. Generally, when the integral is finite, the swap of orders in integration is allowed. Without exception, Eq. (C.1) is assumed to be bounded with some appropriate distributions. Then, for the subset of indexes {N + 1, N + 2, . . . , N + M } in random variables y, we have:</p><formula xml:id="formula_31">ρ x 1:N +M (y 1:N +M )dy N +1:N +M = N +M i=1 p(y i |z G , z i , x i ) p(z i |x i , z G )p(z G )dz 1:N +M dz G dy N +1:N +M = N i=1 p(y i |z G , z i , x i )p(z i |x i , z G ) N +M i=N +1 p(y i |z G , z i , x i )p(z i |x i , z G ) dy N +1:N +M dz N +1:N +M p(z G )dz G dz 1:N = N i=1 p(y i |z G , z i , x i )p(z i |x i , z G )p(z G )dz 1:N dz G = ρ x 1:N (y 1:N ) (C.2)</formula><p>hence, the marginalization consistency is verified.</p><p>• Exchangeability Consistency. For any permutation π towards the index set {1, 2, . . . , N }, we have:</p><formula xml:id="formula_32">ρ x 1:N (y 1:N ) = N i=1 p(y i |z G , z i , x i )p(z i |x i , z G )p(z G )dz 1:N dz G = N i=1 p(y πi |z G , z πi , x πi )p(z πi |x πi , z G )dz πi p(z G )dz G = N i=1 p(y πi |z G , z πi , x πi )p(z πi |x πi , z G )p(z G )dz π (1:N ) dz G = ρ x π(1:N ) (y π(1:N ) ) (C.3)</formula><p>hence, the exchangeability consistency is demonstrated as well.</p><p>With properties in Eq. (C.2) and (C.3), our proposed DSVNP is an exchangeable stochastic process in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Derivation of Evidence Lower Bound with Doubly Stochastic Variational Inference</head><p>Akin to vanilla NPs, we assume the existence of a global latent variable z G , which captures summary statistics consistent between the context [x C , y C ] and the complete target [x T , y T ]. With the involvement of an approximate distribution q z G |[x C , y C , x T , y T ] , we can naturally have an initial ELBO in the following form.</p><formula xml:id="formula_33">ln p(y * |x C , y C , x * ) = ln E q(z G |x C ,y C ,x T ,y T ) p(y * |z G , x * ) p(z G |x C , y C ) q(z G |x C , y C , x T , y T ) ≥ E q(z G |x C ,y C ,x T ,y T ) ln p(y * |z G , x * ) -D KL q(z G |x C , y C , x T , y T ) p(z G |x C , y C ) (D.1)</formula><p>Note that in Eq. (D.1), the conditional prior distribution p(z G |x C , y C ) is intractable in practice and the approximation is used here and such a prior is employed to infer the global latent variable in testing processes. For the approximate posterior q(z G |x C , y C , x T , y T ), it makes use of the context and the full target information, and the sample [x * , y * ] is just an instance in the full target.</p><p>Further, by introducing a target specific local latent variable z * , we can derive another ELBO for the prediction term in the right side of Eq. (D.1) with the same trick.</p><formula xml:id="formula_34">E q(z G |x C ,y C ,x T ,y T ) ln p(y * |z G , x * ) = E q(z G |x C ,y C ,z T ,y T ) ln E q(z * |z G ,[x * ,y * ]) p(y * |z G , z * , x * ) p(z * |z G , x * ) q(z * |z G , [x * , y * ]) ≥ E q(z G |x C ,y C ,x T ,y T ) E q(z * |z G ,[x * ,y * ]) ln p(y * |z G , z * , x * ) -E q(z G |x C ,y C ,x T ,y T ) D KL [q(z * |z G , [x * , y * ]) p(z * |z G , x * )] (D.2)</formula><p>With the combination of Eq. (D.1) and (D.2), the final ELBO L as the right term in the following is formulated.</p><formula xml:id="formula_35">ln p(y * |x C , y C , x * ) implicit data likelihood ≥ E q φ 1 (z G ) E q φ 2 (z * ) ln[p(y * |z G , z * , x * ) data likelihood ] -E q φ 1 (z G ) [D KL [q(z * |z G , x * , y * ) p(z * |z G , x * ) local prior ] -D KL q(z G |x C , y C , x T , y T ) p(z G |x C , y C ) global prior (D.3)</formula><p>The real data likelihood is generally implicit, and the ELBO is an approximate objective. Note that the conditional prior distribution in Eq. (D.3), p(z * |z G , x * ) functions as a local latent variable and is approximated with a Gaussian distribution for the sake of easy implementation. With reparameterization trick, used as: z G = µ φ1 + 1 σ φ1 and z * = µ φ2 + 2 σ φ2 , we can estimate the gradient towards the sample (x * , y * ) analytically in Eq. (D.4), (D.5) and (D.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂L ∂φ</head><formula xml:id="formula_36">1 = E 1∼N (0,I) E 2 ∼N (0,I) ∂ ∂φ 1 ln p(y * |µ φ1 + 1 σ φ1 , µ φ2 + 2 σ φ2 , x * ) -E 1∼N (0,I) ∂ ∂φ 1 D KL q(µ φ2 + 2 σ φ2 |µ φ1 + 1 σ φ1 , x * , y * ) p(µ φ2 + 2 σ φ2 |µ φ1 + 1 σ φ1 , x * ) -E 1∼N (0,I) ∂ ∂φ 1 D KL q(µ φ1 + 1 σ φ1 |x C , y C , x T , y T ) p(µ φ1 + 1 σ φ1 |x C , y C ) (D.4) ∂L ∂φ 2 = E 1∼N (0,I) E 2 ∼N (0,I) ∂ ∂φ 2 ln p(y * |µ φ1 + 1 σ φ1 , µ φ2 + 2 σ φ2 , x * ) -E 1∼N (0,I) ∂ ∂φ 2 D KL q(µ φ2 + 2 σ φ2 |µ φ1 + 1 σ φ1 , x * , y * ) p(µ φ2 + 2 σ φ2 |µ φ1 + 1 σ φ1 , x * ) (D.5) ∂L ∂θ = E 1∼N (0,I) E 2∼N (0,I) ∂ ∂θ ln p θ (y * |µ φ1 + 1 σ φ1 , µ φ2 + 2 σ φ2 , x * ) (D.6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details in Experiments</head><p>Unless explicitly mentioned, otherwise we make of an one-step amortized transformation as dim lat → [µ lat, ln σ lat] to approximate parameters of the posterior in NP models. Especially for DSVNP, the approximate posterior of a local latent variable is learned with the neural network transformation in the approximate posterior [dim lat, dim latx, dim laty] → dim lat and in the piror network [dim lat, dim latx] → dim lat. (For the sake of simplicity, these are not further mentioned in tables of neural structures.) All models are trained with Adam, implemented on Pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Synthetic Experiments</head><p>For synthetic experiments, all implementations resemble that in Attentive NPs<ref type="foot" target="#foot_3">foot_3</ref> . And the neural structures for NPs are reported in Table (3), where dim lat is 128. Note that for the amortized transformations in encoders of NP, AttnNP and In training process, the maximum number of iterations for all (C)NPs is 800k,, and the learning rate is 5e-4. For testing process, in interpolation tasks, the maximum number of context points is 50, while that in extrapolation tasks is 200. Note that the coefficient for KL divergence terms in (C)NPs is set 1 as default, but for DSVNP, we assign more penalty to KL divergence term of local latent variable to avoid overfitting, where the weight is set β 2 = 1000 for simplicity. Admittedly, more penalty to such term reduces prediction accuracy and some dynamically tuning such parameter would bring some promotion in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. System Identification Experiments</head><p>In the Cart-Pole simulator, the input of the system is the vector of the coordinate/angle and their first order derivative and a random action [x c , θ, x c , θ , a], while the output is the transited state in the next time step [x c , θ, x c , θ ]. The force as the action space ranges between [-10,10] N, where the action is a randomly selected force value to impose in the system.</p><p>For training dataset from 6 configurations of environments, we sample 100 state transition pairs for each configuration as the maximum context points, and these context points work as identification of a specific configuration. The neural architectures for CNP, NP, AttnNP and DSVNP refer to Table (4), and default parameters are listed in {dim latxy = 32, dim lat = 32, dim h = 400}. All neural network models are trained with the same learning rate 1e-3. The batch size and the maximum number of epochs are 100. For AttnNP, we notice the generalization capability degrades with training process, so early stopping is used. For DSVNP, the weight of regularization is set as {β 1 = 1, β 2 = 5}, while the KL divergence term weight is fixed as 1 for NP and AttnNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Multi-output Regression Experiments</head><p>SARCOS records inverse dynamics for an anthropomorphic robot arm with seven degree freedoms, and the mission is to predict 7 joint torques with 21-dimensional state space (7 joint positions, 7 joint velocities and 7 accelerations). WQ targets at predicting the relative representation of plant and animal species in Slovenian rivers with some physical and chemical water quality parameters. SCM20D is some supply chain time series dataset for many products, while SCFP records some online click logs for several social issues.</p><p>Before data split, standardization over input and output space is operated on dataset, scaling each dimension of dataset in zero </p><formula xml:id="formula_37">[dim x, dim lat] → dim h → dim h CNP&amp;NP dim latxy → dim lat. dim h → 2 * dim y [dim x, dim y] → dim latxy → dim latxy 2 times [dim x, 2 * dim lat] → dim h → dim h ATTNNP&amp;DSVNP dim x → dim latx; [dim latx, dim laty] → dim lat. dim h → 2 * dim y</formula><p>mean and unit variance<ref type="foot" target="#foot_4">foot_4</ref> . Such pre-processing is required to ensure the stability of training. Also, we find directly treating the data likelihood term as some Gaussian and optimizing negative log likelihood of Gaussian to learn both mean and variance do harm to the prediction, hence average MSE is selected as the objective. As for the variance estimation for uncertainty, Monte Carlo estimation can be used. For all dataset, we employ the neural structure in Table ( <ref type="formula" target="#formula_5">5</ref>), and default parameters in Encoder and Decoder are in the list {dim h = 100, dim latx = 32, dim laty = 8, dim lat = 64}. The learning rate for Adam is selected as 1e-3, the batch size for all dataset is 100, the maximum number of context points is randomly selected during training, and the maximum epochs in training are up to the scale of dataset and convergence condition. Here the maximum epochs are respectively 300 for SARCOS, 3000 for SCM20D and 5000 for WQ. For the testing process, 30 data points are randomly selected as the context for prediction in each batch. Also notice that, the hyper-parameters as the weights of KL divergence term are the same in implementation as one without additional modification in this experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Image Classification and O.O.D. Detection</head><p>The implementations of NP related models and Monte-Carlo Neural Network are quite similar. On MNIST task, the feature extractor for images is taken as LeNet-like structure as [20, 'M', 50, 'F', '500']<ref type="foot" target="#foot_5">foot_5</ref> , and the decoder is one-layer transformation. On CIFAR10 task, the extractor is parameterized in VGG-style network as <ref type="bibr">[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512</ref>, 'M']<ref type="foot" target="#foot_6">foot_6</ref> , and the decoder is also one-layer transformation from latent variable to label output in softmax form. Other parameters are in the list {dim latx = 32, dim laty = 64, dim lat = 64} for MNIST and {dim latx = 32, dim laty = 64, dim lat = 128}. The labels for both are represented in one-hot encoding way and then further transform to some continuous embedding. Batch size in training is 100 as default, the number of context samples for NP related models is randomly selected no larger than 100 in each batch, while the optimizer Adam is with learning rate 1e -3 for MNIST task and 5e -5 for CIFAR10 task. The maximum epochs for both is 100 in both cases, and the size of all source and o.o.d. dataset is 10000. Dropout rates for MC-Dropout in encoder networks are respectively as 0.1 and 0.2 for LeNet-like one and VGG-like one. In the testing process, 100 samples from source dataset are randomly selected as the context points. Before prediction process (estimating predictive entropies on both domain dataset and o.o.d dataset), images on MNIST are normalized in interval [0, 1], those on CIFAR10 are standarized as well, and all o.o.d. dataset follow similar way as that on MNIST or CIFAR10. More specifically, Rademacher dataset is generated in the way: place bi-nominal distribution with probability 0.5 over in image shaped tensor and then minus 0.5 to ensure the zero-mean in statistics. Similar operation is taken in uniform cases, while Gaussian o.o.d. dataset is from standard Normal distribution. All results are reported in Table (2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Probabilistic Graphs for CNP, vanilla NP, Attentive NP and DSVNP. The blue dotted lines characterize the inference towards global latent variable zG, while the pink dotted lines are for target specific local latent variables z * . The ones in first row are training cases, while those in second row are testing cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Function Prediction in Interpolation and Extrapolation. Blue curves are ground truth with dotted units as context points, and orange ones are predicted results. Rows from up to down respectively indicate cases: single function with noise, interpolation and extrapolation towards realizations from stochastic process. The shadow regions are ±3 standard deviations from the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>GoalFigure 3 .</head><label>3</label><figDesc>Figure 3. Cart-Pole Dynamical Systems.The cart and the pole are with masses mc and mp, and the length of the pole is l. And the configuration of the simulator is up to parameters of the cart-pole mass and the ground friction coefficient here with other hyperparameters fixed in this experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Cumulative Distribution Functions of Entropies in O.O.D. Detection Tasks. Values in X-axis are example entropies ranging from 0 to 2.3, and y-axis records cumulative probabilities. The first row corresponds to the predictive result with models trained on MNIST, while the second is with models trained on CIFAR10. NN means the baseline neural network without dropout layer. Curves in CDFs closer to triangular arrows reveal better uncertainty quantification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Structure Summary over NP Related Models on Training Dataset. Here f corresponds to some functional operation. Global latent variable in CNP only governs points to predict, while that in NP works for the whole points. And local latent variables in AttnNP and DSVNP are distinguished, with the latter as a latent random variable.</figDesc><table><row><cell>NP FAMILY</cell><cell cols="4">RECOGNITION MODEL GENERATIVE MODEL PRIOR DISTRIBUTION LATENT VARIABLE</cell></row><row><cell>CNP</cell><cell>zC = f (xC , yC )</cell><cell>p(yT |zC , xT )</cell><cell>NULL</cell><cell>GLOBAL</cell></row><row><cell>NP</cell><cell>q(zG|xC , yC , xT , yT )</cell><cell>p(yT |zG, xT )</cell><cell>p(zG|xC , yC )</cell><cell>GLOBAL</cell></row><row><cell>ATTNNP</cell><cell>q(zG|xC , yC , xT , yT ),</cell><cell>p(y * |zG, z * , x * )</cell><cell>p(zG|xC , yC )</cell><cell>GLOBAL</cell></row><row><cell></cell><cell>z *  = f (xC , yC , x * )</cell><cell></cell><cell></cell><cell>+LOCAL</cell></row><row><cell cols="2">DSVNP (OURS) q(zG|xC , yC , xT , yT ),</cell><cell>p(y * |zG, z * , x * )</cell><cell>p(zG|xC , yC ),</cell><cell>GLOBAL</cell></row><row><cell></cell><cell>q(z * |zG, x * , y * )</cell><cell></cell><cell>p(z * |zG, x * )</cell><cell>+LOCAL</cell></row><row><cell cols="2">in high-dimensional cases.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Hence, we present a hierarchical way to modify NPs, and</cell><cell></cell><cell></cell></row><row><cell cols="3">the trick is to involve auxiliary latent variables for NPs and</cell><cell></cell><cell></cell></row><row><cell cols="3">derive a new evidence lower bound for different levels of ran-</cell><cell></cell><cell></cell></row><row><cell cols="3">dom variables with doubly stochastic variational inference</cell><cell></cell><cell></cell></row><row><cell cols="3">(Salimbeni &amp; Deisenroth, 2017; Titsias &amp; Lázaro-Gredilla,</cell><cell></cell><cell></cell></row><row><cell cols="3">2014). The original intention of involving auxiliary latent</cell><cell></cell><cell></cell></row><row><cell cols="3">variables is to improve the expressiveness of approximate</cell><cell></cell><cell></cell></row><row><cell cols="3">posteriors, and it is commonly used in deep generative mod-</cell><cell></cell><cell></cell></row><row><cell cols="3">els (Maaløe et al., 2016). So, as displayed in Table (1),</cell><cell></cell><cell></cell></row><row><cell cols="3">DSVNP considers a global latent variable and a local latent</cell><cell></cell><cell></cell></row><row><cell cols="3">variable to convey context information at different levels.</cell><cell></cell><cell></cell></row><row><cell cols="3">Our work is also consistent with the hierarchical implicit</cell><cell></cell><cell></cell></row><row><cell cols="3">Bayesian neural networks (Tran et al., 2016; 2017), which</cell><cell></cell><cell></cell></row><row><cell cols="3">distinguish the role of latent variables and induce more pow-</cell><cell></cell><cell></cell></row><row><cell cols="3">erful posteriors. Without exception, the local latent variable</cell><cell></cell><cell></cell></row><row><cell cols="3">z  *  refers to any data point (x  *  , y  *  ) for prediction in DSVNP</cell><cell></cell><cell></cell></row><row><cell cols="2">in the remainders of this paper.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">3.1. Neural Process with Hierarchical Latent Variables</cell><cell></cell><cell></cell></row><row><cell cols="3">To extract hierarchical context information for the predictive</cell><cell></cell><cell></cell></row><row><cell cols="3">distribution, we distinguish the global latent variable and</cell><cell></cell><cell></cell></row><row><cell cols="3">the local latent variable in the form of a Bayesian model,</cell><cell></cell><cell></cell></row><row><cell cols="3">and the induced LVM is DSVNP. This variant shares the</cell><cell></cell><cell></cell></row><row><cell cols="3">same prediction structure with AttnNP. The global latent</cell><cell></cell><cell></cell></row><row><cell cols="3">variable is shared across all observations, and the role of</cell><cell></cell><cell></cell></row><row><cell cols="3">context points resembles inducing points in sparse GP (Snel-</cell><cell></cell><cell></cell></row><row><cell cols="3">son &amp; Ghahramani, 2006), summarizing general statistics</cell><cell></cell><cell></cell></row><row><cell cols="3">in the task. As for the local latent variable in our proposed</cell><cell></cell><cell></cell></row><row><cell cols="3">DSVNP, it is an auxiliary latent variable responsible mainly</cell><cell></cell><cell></cell></row><row><cell cols="3">for variations of instance locality. From another perspec-</cell><cell></cell><cell></cell></row><row><cell cols="3">tive, DSVNP combines the global latent variable in vanilla</cell><cell></cell><cell></cell></row><row><cell cols="3">NPs with the local latent variable in conditional variational</cell><cell></cell><cell></cell></row><row><cell cols="3">autoencoder (C-VAE). This implementation in model con-</cell><cell></cell><cell></cell></row><row><cell cols="3">struction separates the global variations and sample specific</cell><cell></cell><cell></cell></row><row><cell cols="3">variations and theoretically increases the expressiveness of</cell><cell></cell><cell></cell></row><row><cell>the neural network.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>As illustrated in Figure</p>1</p>(d), the target to predict is governed by these two latent variables. Both the global latent variable z G and the local latent variable z * contribute to prediction.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>)</head><label></label><figDesc>Algorithm 1 Variational Inference for DSVNP in Training. Input: Dataset D, Maximum context points N max , Learning rate α, Batch size B and Epoch number m. Output: Model parameters φ 1 , φ 2 and θ. Initialize parameters φ 1 , φ 2 , θ of recognition model and generative model in Eq. (D.3), (D.4) and (D.5). for i = 1 to m do Draw some context number N C ∼ U [1, N max ]; Draw mini-batch instances and formulate contexttarget pairs {(x C , y C , x T , y T ) bs }</figDesc><table><row><cell>B bs=1 ∼ D;</cell></row><row><cell>Feedforward the mini-batch instances to recognition</cell></row><row><cell>model q φ1 :</cell></row><row><cell>Draw sample of latent variable z G ∼ q φ1,1 in</cell></row><row><cell>Eq. (D.3);</cell></row><row><cell>Draw sample of latent variable z  *  ∼ q φ2,1 in</cell></row><row><cell>Eq. (D.4);</cell></row><row><cell>Feedforward the latent variables to discrimination</cell></row><row><cell>model p θ :</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Average Negative Log-likelihoods over all target points on realizations from Synthetic Stochastic Process. (Figures in brackets are variances.)</figDesc><table><row><cell cols="2">PREDICTION CNP</cell><cell>NP</cell><cell cols="2">ATTNNP DSVNP</cell></row><row><cell>INTER</cell><cell cols="3">-0.802 -0.958 -1.149</cell><cell>-0.975</cell></row><row><cell></cell><cell cols="3">(1E-6) (2E-5) (8E-6)</cell><cell>(2E-5)</cell></row><row><cell>EXTRA</cell><cell>1.764</cell><cell>8.192</cell><cell>8.091</cell><cell>4.203</cell></row><row><cell></cell><cell cols="2">(1E-1) (7E1)</cell><cell>(7E2)</cell><cell>(9E0)</cell></row><row><cell cols="5">drawn from intervals U [0, 0.6] and U [0.8, 1.0], with the</cell></row><row><cell cols="5">noise drawn from ∼ N (0, 0.003 2 ). As illustrated in the</cell></row><row><cell cols="5">first row of Fig. (2), we can observe CNP and DSVNP better</cell></row><row><cell cols="5">quantify variance outside the interval [0, 1.0], while AttnNP</cell></row><row><cell cols="5">either overestimates or underestimates the uncertainty to</cell></row><row><cell cols="5">show higher or lower standard deviations in regions with</cell></row><row><cell cols="5">less observations. All models share similar properties with</cell></row><row><cell cols="5">GPs in predictive distributions, displaying lower variances</cell></row><row><cell cols="5">around observed points. As for the gap in interval [0.6, 0.8],</cell></row><row><cell cols="5">the revealed uncertainty is consistent to that in (Sun et al.,</cell></row><row><cell cols="5">2019; Hernández-Lobato &amp; Adams, 2015) with intermediate</cell></row><row><cell>variances.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Predictive Negative Log-Likelihoods and Mean Square Errors on Cart-Pole State Transition Testing Dataset. (Figures in brackets are variances.)</figDesc><table><row><cell cols="2">METRICS CNP</cell><cell>NP</cell><cell cols="2">ATTNNP DSVNP</cell></row><row><cell>NLL</cell><cell cols="3">-2.014 -1.537 -1.821</cell><cell>-2.145</cell></row><row><cell></cell><cell cols="3">(9E-4) (1E-3) (7E-3)</cell><cell>(9E-4)</cell></row><row><cell>MSE</cell><cell>0.096</cell><cell>0.074</cell><cell>0.067</cell><cell>0.036</cell></row><row><cell></cell><cell cols="3">(3E-4) (2E-4) (1E-4)</cell><cell>(2.1E-5)</cell></row><row><cell cols="5">4.3. Multi-Output Regression on Real-world Dataset</cell></row><row><cell cols="5">Further, more complicated scenarios are considered when</cell></row><row><cell cols="5">the regression task relates to multiple outputs. As investi-</cell></row><row><cell cols="5">gated in (Moreno-Muñoz et al., 2018; Bonilla et al., 2008),</cell></row><row><cell cols="5">distributions of output variables are implicit, which means</cell></row><row><cell cols="5">no explicit distributions are appropriate to be used in pa-</cell></row><row><cell cols="5">rameterizing the output. We evaluate the performance of</cell></row><row><cell cols="5">all models on dataset, including SARCOS 2 , Water Quality</cell></row><row><cell cols="5">(WQ) (Džeroski et al., 2000) and SCM20d (Spyromitros-</cell></row><row><cell cols="5">Xioufis et al., 2016). Details about these dataset and neural</cell></row><row><cell cols="5">architectures for all models are included in Appendix E.</cell></row><row><cell cols="5">Furthermore, Monte-Carlo Dropout is included for compar-</cell></row><row><cell cols="5">isons. Similar to NP (Garnelo et al., 2018b), the variance</cell></row><row><cell cols="5">parameter is not learned and the objective in optimization</cell></row><row><cell cols="5">is pointwise mean square errors (MSEs) after averaging all</cell></row><row><cell cols="5">dimensions in the output. Each dataset is randomly split into</cell></row><row><cell cols="5">2-folds as training and testing sets. The training procedure</cell></row><row><cell cols="5">in (C)NPs follows that in Algorithm (1), and some context</cell></row><row><cell cols="5">points are randomly selected in batch samples. In the testing</cell></row><row><cell cols="5">stage, we randomly select 30 instances as the context and</cell></row><row><cell cols="5">then perform predictions with (C)NPs. The weights of data</cell></row><row><cell cols="5">likelihood and KL divergence terms in models are not tuned</cell></row><row><cell>here.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">During training, ELBOs in NP related models are optimized,</cell></row><row><cell cols="5">while MSEs are used as evaluation metric in testing (Dez-</cell></row><row><cell cols="5">fouli &amp; Bonilla, 2015) 3 . The predictive results on testing</cell></row><row><cell cols="5">dataset are reported in Table (4). All MSEs are averaged</cell></row><row><cell cols="5">after 10 independent experiments. We observe DSVNP</cell></row><row><cell cols="5">outperforms other models, and deterministic context infor-</cell></row><row><cell cols="5">mation in CNP hardly increases performance. Compared</cell></row><row><cell cols="5">with NP models, MC-NN is relatively satisfying on Sarcos</cell></row><row><cell cols="5">and WQ, and AttnNP works not well in these cases. A po-</cell></row><row><cell cols="5">tential reason can be that deterministic context embedding</cell></row><row><cell cols="5">with dot product attention is less predictive for output with</cell></row><row><cell cols="5">multiple dimensions, while the role of local latent variable</cell></row><row><cell cols="5">in DSVNP not only bridges the gap between input and out-</cell></row><row><cell cols="5">put, but also extracts some correlation information among</cell></row><row><cell cols="5">variables in outputs. As a comparison to synthetic experi-</cell></row><row><cell cols="5">ments, the attention mechanism is more suitable to extract</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Predictive MSEs on Multi-Output Dataset. CNP's results are for target points. D records (input,output) dimensions, and N is the number of samples. MC-Dropout runs 50 stochastic forward propagation and average results for prediction in each data point. (Figures in brackets are variances.)</figDesc><table><row><cell cols="2">DATASET N</cell><cell>D</cell><cell cols="2">MC-DROPOUT CNP</cell><cell>NP</cell><cell>ATTNNP</cell><cell>DSVNP</cell></row><row><cell>SARCOS</cell><cell cols="2">48933 (21,7)</cell><cell>1.215(3E-3)</cell><cell cols="3">1.437(2.9E-2) 1.285(1.2E-1) 1.362(8.4E-2) 0.839(1.5E-2)</cell></row><row><cell>WQ</cell><cell>1060</cell><cell cols="2">(16,14) 0.007(9.6E-8)</cell><cell cols="3">0.015(2.4E-5) 0.007(5.2E-6) 0.01(8.5E-6)</cell><cell>0.006(1.6E-6)</cell></row><row><cell cols="2">SCM20D 8966</cell><cell cols="2">(61,16) 0.017(2.4E-7)</cell><cell cols="3">0.037(6.7E-5) 0.015(7.1E-8) 0.015(8.1E-7) 0.007(2.3E-7)</cell></row><row><cell cols="5">local information when the output dimension is lower.</cell><cell></cell></row><row><cell cols="5">4.4. Classification with Uncertainty Quantification</cell><cell></cell></row><row><cell cols="5">Here image classification is performed with NP models and</cell><cell></cell></row><row><cell cols="5">MC-Dropout, and out of distribution (o.o.d.) detection is</cell><cell></cell></row><row><cell cols="5">chosen to measure the goodness of uncertainty quantifica-</cell><cell></cell></row><row><cell cols="5">tion. We respectively train models on MNIST and CIFAR10.</cell><cell></cell></row><row><cell cols="5">The dimensions for latent variables are 64 on MNIST and</cell><cell></cell></row><row><cell cols="5">128 on CIFAR10. The training process for NP related mod-</cell><cell></cell></row><row><cell cols="5">els follows Algorithm (1), with the number of context im-</cell><cell></cell></row><row><cell cols="5">ages randomly selected in each batch update. For the testing</cell><cell></cell></row><row><cell cols="5">process, we randomly select 100 instances from the domain</cell><cell></cell></row></table><note><p>dataset as the context for (C)NP models. The commonly used measure for uncertainty in K-class o.o.d. detection is entropy (Lakshminarayanan et al., 2017), H[y * |x * ] = -Σ K c=1 p w (y * c |x * ; D tr ) ln p w (y * c |x * ; D tr ), where data point (x * , y * ) comes from either domain test dataset D te or o.o.d dataset D ood .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 .</head><label>1</label><figDesc>Pointwise Average Negative Log-likelihoods for 2000 realisations. Rows with J consider all data points including the context, while those with P exclude the context points in statistics.(Figures in brackets are variances.)    </figDesc><table><row><cell cols="2">PREDICTION CNP</cell><cell>NP</cell><cell>ATTNNP</cell><cell>DSVNP</cell></row><row><cell>INTER(J)</cell><cell>NAN</cell><cell cols="3">-0.958(2E-5) -1.149(8E-6) -0.975(2E-5)</cell></row><row><cell>INTER(P)</cell><cell cols="4">-0.802(1E-6) -0.949(2E-5) -1.141(6E-6) -0.970(2E-5)</cell></row><row><cell>EXTRA(J)</cell><cell>NAN</cell><cell>8.192(7E1)</cell><cell>8.091(7E2)</cell><cell>4.203(9E0)</cell></row><row><cell>EXTRA(P)</cell><cell>1.764(1E-1)</cell><cell>8.573(8E1)</cell><cell>8.172(7E2)</cell><cell>4.303(1E1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Tested Entropies of Logit Probability on Classification Dataset. For rows of MNIST and CIFAR10, the second figures in columns are classification accuracies. Both MC-Dropout and DSVNP are averaged with 100 Monte Carlo samples.</figDesc><table><row><cell></cell><cell>NN</cell><cell cols="2">MC-DROPOUT CNP</cell><cell>NP</cell><cell>ATTNNP</cell><cell>DSVNP</cell></row><row><cell>MNIST</cell><cell cols="2">0.011/0.990 0.009/0.993</cell><cell cols="4">0.019/0.993 0.010/0.991 0.012/0.989 0.027/0.990</cell></row><row><cell>FMNIST</cell><cell>0.385</cell><cell>0.735</cell><cell>0.711</cell><cell>0.434</cell><cell>0.337</cell><cell>0.956</cell></row><row><cell>KMNIST</cell><cell>0.282</cell><cell>0.438</cell><cell>0.497</cell><cell>0.322</cell><cell>0.294</cell><cell>0.545</cell></row><row><cell>GAUSSIAN</cell><cell>0.601</cell><cell>1.623</cell><cell>1.313</cell><cell>0.588</cell><cell>0.611</cell><cell>0.966</cell></row><row><cell>UNIFORM</cell><cell>0.330</cell><cell>1.739</cell><cell>0.862</cell><cell>0.094</cell><cell>0.220</cell><cell>0.375</cell></row><row><cell>CIFAR10</cell><cell cols="2">0.151/0.768 0.125/0.838</cell><cell cols="4">0.177/0.834 0.124/0.792 0.124/0.795 0.081/0.863</cell></row><row><cell>SVHN</cell><cell>0.402</cell><cell>0.407</cell><cell>0.459</cell><cell>0.315</cell><cell>0.269</cell><cell>0.326</cell></row><row><cell cols="2">RADEMACHER 0.021</cell><cell>0.062</cell><cell>0.079</cell><cell>0.078</cell><cell>0.010</cell><cell>0.146</cell></row><row><cell>GAUSSIAN</cell><cell>0.351</cell><cell>0.266</cell><cell>0.523</cell><cell>0.451</cell><cell>0.349</cell><cell>0.444</cell></row><row><cell>UNIFORM</cell><cell>0.334</cell><cell>0.217</cell><cell>0.499</cell><cell>0.463</cell><cell>0.261</cell><cell>0.374</cell></row></table><note><p>DSVNP, we use the network to learn the distribution parameters as: dim lat → [µ lat , ln σ lat ].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .</head><label>3</label><figDesc>Neural Network Structure of (C)NP Models for 1-D Stochastic Process. The transformations in the table are in linear form, followed with ReLU activation mostly. And Dropout rate for DNN is defined as 0.5 for all transformation layers in Encoder. As for AttnNP and DSVNP, the encoder network is doubled in the table since there exist some local variable for prediction.→ 32 → 32 → dim lat [dim x, dim lat] → 2 * dim y ATTNNP&amp;DSVNP [dim x, dim y] → 32 → 32 → dim lat [dim x, 2 * dim lat] → 2 * dim y</figDesc><table><row><cell>NP MODELS</cell><cell>ENCODER</cell><cell>DECODER</cell></row><row><cell>CNP&amp;NP</cell><cell>[dim x, dim y]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Neural Network Structure of (C)NP Models in System Identification Tasks. The transformations in the table are linear, followed with ReLU activation mostly. As for AttnNP and DSVNP, the encoder network is doubled in the table since there exist some local variable for prediction. Here only dot product attention is used in AttnNP.</figDesc><table><row><cell>NP MODELS</cell><cell>ENCODER</cell><cell>DECODER</cell></row><row><cell></cell><cell>[dim x, dim y] → dim latxy → dim latxy</cell><cell></cell></row><row><cell></cell><cell>2 times</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>Neural Network Structure of (C)NP Models in Multi-Output Regression Tasks. The transformations in the table are linear, followed with ReLU activation mostly. And Dropout rate for DNN is defined as 0.01 for all transformation layers in Encoder. As for AttnNP and DSVNP, the encoder network is doubled in the table since there exist some local variable for prediction. Here only dot product attention is used in AttnNP.CNP&amp;NPdim y → dim laty;[dim latx, dim laty] → dim lat. dim lat → dim y dim x → dim h → dim h 2 times → dim latx [dim latx, 2 * dim lat] → dim h ATTNNP&amp;DSVNP dim y → dim laty;[dim latx, dim laty] → dim lat. dim lat → dim y</figDesc><table><row><cell>NP MODELS</cell><cell>ENCODER</cell><cell>DECODER</cell></row><row><cell cols="2">DNN(MC-DROPOUT) dim x → dim h → dim h</cell><cell>dim lat → dim h</cell></row><row><cell></cell><cell>2 times</cell><cell></cell></row><row><cell></cell><cell>dim h → dim lat</cell><cell>dim h → dim y</cell></row><row><cell></cell><cell>dim x → dim h → dim h</cell><cell>→ dim latx [dim latx, dim lat] → dim h</cell></row><row><cell></cell><cell>2 times</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 .</head><label>6</label><figDesc>Neural Network Structure of (C)NP Models in Image Classification Tasks. The transformations in the table are linear, followed with ReLU activation mostly. And Dropout rate for DNN is defined as 0.5 for all transformation layers in Encoder. As for AttnNP and DSVNP, the encoder network is doubled in the table since there exist some local variable for prediction. [dim latx, 2 * dim lat] → dim y[dim latx, dim laty] → dim lat.</figDesc><table><row><cell>NP MODELS</cell><cell>ENCODER</cell><cell></cell><cell>DECODER</cell></row><row><cell cols="2">DNN(MC-DROPOUT) dim x → dim h</cell><cell></cell><cell>dim lat → dim y</cell></row><row><cell></cell><cell>embedding net</cell><cell></cell></row><row><cell></cell><cell cols="2">dim h → dim lat</cell></row><row><cell></cell><cell>dim x → dim h</cell><cell>→ dim latx</cell></row><row><cell></cell><cell>embedding net</cell><cell></cell></row><row><cell>CNP&amp;NP</cell><cell cols="2">dim y → dim laty;</cell><cell>[dim latx, dim lat] → dim y</cell></row><row><cell></cell><cell cols="2">[dim latx, dim laty] → dim lat.</cell></row><row><cell></cell><cell>dim x → dim h</cell><cell>→ dim latx</cell></row><row><cell></cell><cell>embedding net</cell><cell></cell></row><row><cell>ATTNNP&amp;DSVNP</cell><cell cols="2">dim y → dim laty;</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For brief notations, the inputs, outputs of the context and the target are respectively denoted as xC = x1:N , yC = y1:N , xT = x1:N+M , yT = y1:N+M . Only in CNP, xT = xN+1:N+M , yT = yN+1:N+M . And [x * , y * ] refers to any instance in the target.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.gaussianprocess.org/gpml/data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Directly optimizing Gaussian log-likelihoods does harm to performance based on experimental results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/deepmind/neural-processes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://scikit-learn.org/stable/modules/preprocessing.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Numbers are dimensions of Out-Channel with kernel size 5, 'F' is flattening operation, and each layer is followed with ReLU activation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Numbers are dimensions of Out-Channel with kernel size 3 and padding 1 in each layer, followed with BatchNorm and ReLU function, here M means max-polling operation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We express thanks to <rs type="person">Christos Louizos</rs> and <rs type="person">Bastiaan Veeling</rs> for helpful discussions. We would like to thank <rs type="person">Daniel Worrall</rs>, <rs type="person">Patrick Forré</rs>, <rs type="person">Xiahan Shi</rs>, <rs type="person">Sindy Löwe</rs> and anonymous reviewers for precious feedback on the initial manuscript. Q. Wang gratefully acknowledges consistent and sincere support from <rs type="person">Prof. Max Welling</rs> and was also supported by <rs type="funder">China Scholarship Council</rs> during his pursuing Ph.D at AMLAB. Also, we gratefully acknowledge the support of <rs type="institution">NVIDIA Corporation</rs> with the donation of a Titan V GPU.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Some Basic Concepts</head><p>At first, we revisit some basics, which would help us understand properties of GPs and NPs. Fundamental concepts include permutation invariant function (PIF), general stochastic function process (GSFP) etc. We stress the importance of relationship disentanglement between GPs and NPs and motivations in approximating stochastic processes (SPs) in the main passage.</p><p>Definition 1. Permutation Invariant Function. The f mapping a set of N M -dimension elements to a D-dimension vector variable is said to be a permutation invariant function if:</p><p>where x i is a M -dimensional vector,</p><p>imposes a permutation over the order of elements in the set. The PIF suggests the image of the map is regardless of the element order. Another related concept permutation equivariant function (PEF) keeps the order of elements in the output consistent with that in the input under any permutation operation π.</p><p>The Definition 1 is important, since the exchangeable stochastic function process of our interest in this domain is intrinsically a distribution over set function, and PIF plays as an inductive bias in preserving invariant statistics.</p><p>Permutation invariant functions are candidate functions for learning embeddings of a set or other order uncorrelated data structure X 1:N , and several examples can be listed in the following forms.</p><p>• Some structure with mean/summation over the output:</p><p>• Some structure with Maximum/Minimum/Top k-th operator over the output (Take maximum operator for example), such as:</p><p>• Some structure with symmetric higher order polynomials or other functions with a symmetry bi-variate function φ:</p><p>The invariant property is easy to be verified in these cases, and note that in all settings of NP family in this paper, Eq. (A.3) is used only. For the bi-variate symmetric function in Eq. (A.5) or other more complicated operators would result in more flexible functional translations, but additional computation is required as well. Some of the above mentioned transformations are instantiations in DeepSet. Further investigations in this domain can be the exploitation of these higher order permutation invariant neural network into NPs since more correlations or higher order statistics in the set can be mined for prediction. Additionally, Set Transformer is believed to be powerful in a permutation invariant representation.</p><p>Definition 2. General Stochastic Function Process. Let X denote the Cartesian product of some intervals as the index set and let dimension of observations d ∈ N. For each k ∈ N and any finite sequence of distinct indices x 1 , x 2 , .., x k ∈ X ,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic processes with applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Waymire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rates of convergence for sparse variational gaussian process regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="862" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational auto-encoded deep gaussian processes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A model-based and data-efficient approach to policy search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><surname>Pilco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Depeweg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Udluft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1184" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable inference for gaussian process models with black-box likelihoods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dezfouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1414" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting chemical parameters of river water quality from bioindicator data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demšar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grbović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="17" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural scene representation and rendering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6394</biblScope>
			<biblScope unit="page" from="1204" to="1210" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving pilco with bayesian neural network dynamics models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data-Efficient Machine Learning workshop, ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional conditional neural processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y K</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable variational gaussian process classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An approximate epistemic uncertainty analysis approach in the presence of epistemic and aleatory uncertainties</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krzykacz-Hausmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woltereck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attentive neural processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic gradient vb and the variational auto-encoder</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6965" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian compression for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ullrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3288" to="3298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The functional neural process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8743" to="8754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational implicit processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4222" to="4233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1445" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heterogeneous multi-output gaussian process prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6711" to="6720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Görür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic differential equations: an introduction with applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Oksendal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer School on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the connection between neural processes and gaussian processes with deep kernels</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Bayesian Deep Learning</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational inference for deep gaussian processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4588" to="4599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep gaussian processes with importance-weighted variational inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequential neural processes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10254" to="10264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Warped gaussian processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How to train deep variational autoencoders and probabilistic ladder networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-target regression via input space expansion: treating targets as inputs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="55" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Functional variational bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Uncertainty aware learning from demonstrations in multiple contexts using bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C G</forename><surname>Higuera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian gaussian process latent variable model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational bayes for non-conjugate inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational gaussian process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stochastic variational deep kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2586" to="2594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Variational dependent multi-output gaussian process dynamical systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4134" to="4169" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
