<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time-Evolving Dynamical System for Learning Latent Representations of Mouse Visual Neural Activity</title>
				<funder ref="#_4V6US38">
					<orgName type="full">Shenzhen KQTD</orgName>
				</funder>
				<funder ref="#_8YwjEJc #_48AAtzH #_txssX7r #_AC7HgKh #_wAKkeJV">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-10-23">23 Oct 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengyu</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Liutao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huihui</forename><surname>Zhou</surname></persName>
							<email>zhouhh@pcl.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<email>yhtian@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Electronic and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Time-Evolving Dynamical System for Learning Latent Representations of Mouse Visual Neural Activity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-23">23 Oct 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.07908v3[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Seeking high-quality representations with latent variable models (LVMs) to reveal the intrinsic correlation between neural activity and behavior or sensory stimuli has attracted much interest. In the study of the biological visual system, naturalistic visual stimuli are inherently high-dimensional and time-dependent, leading to intricate dynamics within visual neural activity. However, most work on LVMs has not explicitly considered neural temporal relationships. To cope with such conditions, we propose Time-Evolving Visual Dynamical System (TE-ViDS), a sequential LVM that decomposes neural activity into low-dimensional latent representations that evolve over time. To better align the model with the characteristics of visual neural activity, we split latent representations into two parts and apply contrastive learning to shape them. Extensive experiments on synthetic datasets and real neural datasets from the mouse visual cortex demonstrate that TE-ViDS achieves the best decoding performance on naturalistic scenes/movies, extracts interpretable latent trajectories that uncover clear underlying neural dynamics, and provides new insights into differences in visual information processing between subjects and between cortical regions. In summary, TE-ViDS is markedly competent in extracting stimulus-relevant embeddings from visual neural activity and contributes to the understanding of visual processing mechanisms. Our codes are available at <ref type="url" target="https://github.com/Grasshlw/Time-Evolving-Visual-Dynamical-System">https://github.com/Grasshlw/Time-Evolving-Visual-Dynamical-System</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of neural recording technologies, researchers are able to simultaneously record the spiking activity of large populations of neurons, opening up new avenues for exploring the brain <ref type="bibr" target="#b54">[55]</ref>. For high-dimensional neural data analysis, an important scientific problem is how to account for the intrinsic correlation between neural activity and behavioral patterns or sensory stimuli. One influential approach is latent variable models (LVMs), which construct low-dimensional latent representations that bridge to behavior or stimuli and explain neural activity well <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. To deal with the increasing scale of neural population activity, LVMs have evolved from simple mathematical models early on, such as principal component analysis and factor analysis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b43">44]</ref>, to complex artificial neural networks. More recently, advanced deep learning algorithms have enabled LVMs to extract high-quality representations from neural activity without knowledge of experimental labels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>, or to incorporate behavioral information into models to constrain the shaping of latent variables <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref>. These approaches have contributed to the analysis of neural activity in various ways, including predicting neural responses <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>, decoding related motion patterns or simple visual scenes <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>, and constructing interpretable latent structures <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Although LVMs have sparked strong interest in uncovering the underlying dynamical structure of neural activity, most studies have focused on neural data recorded from motor brain regions under specific controlled behavioral settings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>, such as pre-planned reaching movements <ref type="bibr" target="#b17">[18]</ref>. There is a paucity of studies on LVMs exploring neural data from the visual cortex <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b47">48]</ref>. Given the significant challenge of elucidating the correlation between neural activity and visual stimuli <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b16">17]</ref>, the development of robust models for extracting vision-related latent representations is critically imperative. Furthermore, due to the complex visual neural dynamics, incorporating temporal structure into models to obtain time-dependent latent representations may be a key point for the analysis of visual neural activity. However, there is also a lack of LVMs that explicitly model neural temporal relationships under visual stimuli.</p><p>In this work, we propose the Time-Evolving Visual Dynamical System (TE-ViDS), which aims to generate high-quality latent representations from visual neural activity by disentangling neural components related to visual stimuli from those influenced by internal states. First, we introduce temporal structures to explicitly establish temporal relationships in latent variables, allowing them to evolve over time and capture the temporal dependency inherent in neural activity. Second, to sufficiently utilize the characteristics of visual neural activity, we adopt the split structure approach <ref type="bibr" target="#b35">[36]</ref> and design distinct loss functions to construct two specialized parts of latent representations. The external latent representations aim to capture stimulus-relevant components within visual neural activity, while the internal latent representations reflect the dynamical internal states that influence the animal's sensory capability <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b3">4]</ref>. In doing so, TE-ViDS can generate latent representations that are more relevant to visual stimuli while also explaining the effects of internal states. We evaluate our model on synthetic and mouse visual datasets, comparing it with leading alternatives. Our model demonstrates its ability to construct meaningful latent representations, yield a greater degree of correlation between neural activity and visual stimuli, and explain the visual information processing mechanisms of the mouse visual cortex. Specifically, our main contributions are as follows.</p><p>• We introduce state factors to filter temporal information, enabling TE-ViDS to progressively compress neural activity and evolve latent variables. Regarding the distinct objectives of the two parts of latent representations, we apply self-supervised contrastive learning to shape the external and utilize a time-dependent prior distribution to guide the internal.</p><p>• Through evaluation on synthetic datasets, we show that our model more effectively recovers latent structure and handles time-sequential data well.</p><p>• Through evaluation on mouse visual datasets, we demonstrate that our model outperforms alternative models in decoding natural scenes and natural movies, as well as in extracting clear temporal trajectories of neural dynamics at different time scales.</p><p>• Further analysis reveals that our model can explain the potential variability in visual information processing between subjects and provide new evidence for the functional hierarchy of the mouse visual cortex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With the advancement of deep learning, the application of cutting-edge learning algorithms and the innovative design of model structures have greatly promoted the development of LVMs in neuroscience. Some prominent works are summarized below.</p><p>The approaches based on neural networks have become major avenues for discovering latent representations underlying neural activity, which better elucidate the mechanisms of neural representations.</p><p>A well-known model is latent factor analysis via dynamical systems (LFADS), which used RNNs in a sequential VAE framework to extract precise firing rate estimates and predict observed behavior for single-trial data on motor cortical datasets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Recurrent switching linear dynamical systems (rSLDS) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53]</ref> and low-rank RNNs <ref type="bibr" target="#b40">[41]</ref> also introduced recurrent structures, facilitating the understanding of complex nonlinear neural dynamics. Through specific latent variable designs, pi-VAE <ref type="bibr" target="#b62">[63]</ref> and Swap-VAE <ref type="bibr" target="#b35">[36]</ref> built interpretable latent structures linked to motor behavior patterns. Furthermore, numerous studies have made significant contributions to achieving the goal of dissociating behaviorally relevant and irrelevant components in neural activity. Targeted neural The decoder maps latent variables to inferred firing rates. B. The illustration of different learning objectives for the two parts of latent representations of TE-ViDS. For external latent representations, we apply contrastive loss to encourage them to distinguish the stimulus-relevant components. Given a reference sample (white dot), the red dot is a positive sample and the orange dots are negative samples. For internal latent representations, we use the KL divergence to constrain their distribution to a time-dependent prior distribution. dynamical modeling (TNDM), based on LFADS, constructed a two-pathway structure for separation <ref type="bibr" target="#b24">[25]</ref>. PSID <ref type="bibr" target="#b44">[45]</ref>, DFINE <ref type="bibr" target="#b0">[1]</ref>, and DPAD <ref type="bibr" target="#b45">[46]</ref> were dedicated to introducing supervised information into dynamical systems to capture behavior-relevant dynamics.</p><p>In the visual domain, several latent variable models have made an effort to advance the understanding of visual neural dynamics. One work used Gaussian process models <ref type="bibr" target="#b18">[19]</ref> and found that anesthesiainduced internal state fluctuations lead to correlated variability, while another proposed a networkbased linear dynamical system (fLDS) that captures neural variability under drifting grating stimuli <ref type="bibr" target="#b20">[21]</ref>. In addition, a rectified latent variable model built latent variables that show a spectrum of functional groups as neurons in the mouse visual cortex under drifting gratings <ref type="bibr" target="#b39">[40]</ref>, and a flow-based generative model recovered the distribution of visual neural activity <ref type="bibr" target="#b6">[7]</ref>. Recently, CEBRA, a selfsupervised learning model, obtained consistent latent representations and made progress in decoding movies <ref type="bibr" target="#b47">[48]</ref>. Although these studies cover various types of models, most of them are limited to simple visual stimuli and the task of reconstructing neural responses, leaving a gap for constructing high-quality latent representations from visual neural activity under naturalistic stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>To generate latent representations that can effectively explain the characteristics of visual neural activity, we introduce the Time-Evolving Visual Dynamical System (TE-ViDS), which evolves two parts of representations over time. In this section, we elaborate the concrete implementations of the model architecture and model learning (visualized in Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic notations.</head><p>Neural responses recorded from large numbers of neurons are mostly in the form of spikes, which are regarded as discrete events. In practice, it is common to discretize a period of time into small time windows and calculate the number of spikes within each window. Consequently, for the neural activity of a population of neurons over a period of time, we define a sequence input as x = (x 1 , x 2 , . . . , x T ) ∈ R T ×N , which represents spike counts of N neurons within T time windows. The corresponding low-dimensional latent variable is denoted as z = (z 1 , z 2 , . . . , z T ) ∈ R T ×M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>As aforementioned, TE-ViDS compresses visual neural activity into external latent representations and internal latent representations</p><formula xml:id="formula_0">(z t = [z (e) t , z (i) t ]</formula><p>). To evolve the two latent variables, we incorporate two dynamical systems into our model, using the state factor h t to sift and accumulate temporal information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>. In this way, the latent variables of the current time step are only conditioned on the input and state factors of the previous time steps, facilitating the capture of dynamics in causal order.</p><p>External latent variables. This part of latent representations aims to capture the stimulus-relevant components of neural activity. To maintain our focus on variability arising from internal brain states, we construct external latent variables as deterministic values:</p><formula xml:id="formula_1">z (e) t = f (e) enc f x (x t ), h<label>(e)</label></formula><p>t-1 .</p><p>(1)</p><p>Internal latent variables. This part of latent representations aims to reflect dynamical internal states that contain high variability and noise. To model such dynamics, internal latent variables are constructed as stochastic values that evolve over time. The tractable parameterized distribution (approximate posterior) is conditioned on both x t and h (i)</p><p>t-1 , while the prior distribution is conditioned solely on h (i) t-1 , endowing it with a certain degree of spontaneity:</p><formula xml:id="formula_2">z (i) t x 1:t , h (i) 1:t-1 ∼ N (µ z,t , σ 2 z,t • I), [µ z,t , σ z,t ] = f (i) enc f x (x t ), h (i) t-1 ,<label>(2)</label></formula><formula xml:id="formula_3">z(i) t h (i) 1:t-1 ∼ N (μ z,t , σ2 z,t • I), [μ z,t , σz,t ] = f (i) prior h (i) t-1 .<label>(3)</label></formula><p>Neural activity reconstruction. High-quality latent representations, as compressed forms of the original neural activity, are required to effectively reconstruct their input. In practice, to enrich the dynamic information for a more accurate reconstruction, the spike count observation is related not only to the latent variables but also to the internal state factors:</p><formula xml:id="formula_4">xt z (e) 1:t , z (i) 1:t , h (i) 1:t-1 ∼ P (r t ), r t = f dec z (e) t , z (i) t , h (i) t-1 ,<label>(4)</label></formula><p>where the parameterized distribution P is chosen to be Poisson <ref type="bibr" target="#b20">[21]</ref>, i.e., the actual inferred neural activity is spike firing rates r t .</p><p>Recurrent neural networks. The state factor is updated by GRU <ref type="bibr" target="#b10">[11]</ref>. By selectively integrating and exploiting input and latent variables, the state factor is crucial for capturing complex sequential dynamics. Besides, since the animal's dynamical internal states are inevitably affected by visual stimuli, h</p><p>t and h</p><formula xml:id="formula_6">(i)</formula><p>t corresponding to the two latent representations are updated differently:</p><formula xml:id="formula_7">h (e) t = f (e) GRU f x (x t ), h<label>(e)</label></formula><p>t-1 ,</p><formula xml:id="formula_8">h (i) t = f (i) GRU f x (x t ), z<label>(e)</label></formula><p>t , z</p><formula xml:id="formula_9">(i) t , h<label>(i)</label></formula><p>t-1 .</p><p>(</p><formula xml:id="formula_10">)<label>5</label></formula><p>All the functions f above are parameter-learnable neural networks (see Appendix A for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Learning</head><p>Given that the two parts of latent representations hold different objectives for disentanglement, we introduce the following loss function to optimize all learnable parameters in an end-to-end manner:</p><formula xml:id="formula_11">L = L recons + βL contrastive + γL regular .<label>(6)</label></formula><p>The first term which deals with the objective of inferring neural activity is formulated as</p><formula xml:id="formula_12">1 T T t=1 [L P (x t , r t )],</formula><p>where L P is Poisson negative log likelihood. The second term encourages external latent representations to distinguish stimulus-relevant components through self-supervised contrastive learning. Specifically, for a given sample x = (x 1 , . . . , x T ), we randomly select another sequence offset by several time steps as a positive sample, denoted x pos = (x 1+∆ , . . . , x T +∆ ), where ∆ can be positive or negative. The offset is smaller than the length of the sequence to ensure that the positive sample pairs overlap, i.e., the visual stimuli corresponding to the positive sample pairs are similar, which establishes an association between the external latent variables and the visual stimuli. Then, a mini-batch of negative samples is randomly selected from the entire training set. Finally, we compute the external latent variables of all selected samples and apply the NT-Xent loss <ref type="bibr" target="#b9">[10]</ref> as L contrastive to them. In doing so, external latent representations of the positive sample pairs are driven to be distinguished from the negative samples. For this term, we do not apply the time-wise operation, but flatten the temporal and spatial dimensions of the external latent variables for the loss computation. In addition, to enhance the effect of the positive sample, we adopt the practice of the swap operation <ref type="bibr" target="#b35">[36]</ref>. We exchange the external latent variables of the given sample with those of the positive sample while keeping the internal latent variables unchanged. The new latent representations are then used to compute new inferred firing rates and an additional reconstruction loss.</p><p>The third term measures the difference between the prior and the approximate posterior of the internal latent variables by the KL divergence, formulated as 1</p><formula xml:id="formula_13">T T t=1 D KL (z (i) t ∥z (i) t )</formula><p>. Besides, we compute the L2 norm of the expectation and log-variance of the prior distribution as a regularization to avoid excessive fluctuations over time and to stabilize model training.</p><p>β and γ are hyperparameters used to control the severity of the penalty for each loss term. The entire loss function actually obeys the strategy of maximizing the evidence lower bound (ELBO) of the marginal log likelihood in the VAE framework <ref type="bibr" target="#b30">[31]</ref>, while also accounting for the impact of introducing the contrastive loss <ref type="bibr" target="#b56">[57]</ref>. A detailed derivation is given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the utility of the latent representations constructed by TE-ViDS for the analysis of visual neural activity, we perform a series of experiments to compare our model with alternative LVMs from two aspects, similar to studies focusing on motor brain regions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>. First, we quantify the performance in decoding visual stimuli using latent representations, which has long served as a research hotspot for unraveling the mechanisms of visual processing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b57">58]</ref>. Second, we assess the clarity of latent temporal trajectories extracted from neural dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We carry out the experiments on two commonly used synthetic datasets and a well-known publicly available mouse visual neural dataset.</p><p>Synthetic datasets. The two synthetic datasets are crucial for evaluating the fundamental ability of LVMs to construct accurate latent variables. One is a non-temporal dataset generated from several sets of labels <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b35">36]</ref>, facilitating the assessment of class discrimination capabilities. The other is a temporal dataset constructed by the Lorenz system to test for extracting temporal relationships <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref>. A detailed description of the generating procedure is presented in Appendix C.</p><p>Mouse visual neural dataset. We use a subset of the Allen Brain Observatory Visual Coding dataset <ref type="bibr" target="#b50">[51]</ref>, which has been used in a variety of studies, including the construction of brain-like networks <ref type="bibr" target="#b49">[50]</ref>, the modeling of functional mechanisms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>, and the decoding of visual stimuli <ref type="bibr" target="#b47">[48]</ref>. This dataset was collected by Neuropixel probes from six mouse visual cortical regions simultaneously, including VISp, VISl, VISrl, VISal, VISpm, and VISam. Notably, neural activity was recorded while mice passively viewed visual stimuli without any task-driven behavior. The dataset comprises 32 sessions, each involving one mouse. In this work, we choose to analyze five mice (subjects) that have the maximum number of recorded neurons (see Appendix D), and these neurons are evenly distributed across all regions (the coefficient of variation for the number of neurons across six regions is below 0.5). We focus on neural activity in response to natural scenes and a natural movie. For natural scenes, 118 images are presented in random order, each for 250ms and 50 trials. The neural activity in the form of spike counts is binned into 10-ms windows so that each trial contains 25 time points. The natural movie is 30 seconds long with a frame rate of 30Hz, presented for 10 trials. We bin the spike counts with a sampling frequency of 120Hz and align them with the movie timestamps, resulting in four time points for each frame.</p><p>Since spike responses are quite variable across trials even under identical experimental conditions, we conduct the basic evaluation on "held-out" trials. Specifically, for each dataset, we randomly split all trials into 80% for training, 10% for validation, and 10% for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>Following previous studies <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b47">48]</ref>, we apply two quantitative metrics for evaluation.</p><p>Reconstruction score. For the two synthetic datasets, we use linear regression to fit the latent variables of models to the true latent variables, and report the R 2 as the reconstruction score. Decoding score. For the mouse visual neural dataset, we quantify the performance of models in decoding natural scenes/natural movie frames. In terms of natural scene decoding, we obtain the latent variables of the last 20 time points (50ms-250ms) in each trial, since there is a response latency in the mouse visual cortex when presented with static stimuli <ref type="bibr" target="#b50">[51]</ref>. These latent variables are then concatenated into a vector to form the latent representations of each trial, thereby retaining maximal temporal information. We use the KNN algorithm to classify the latent representations of each trial, i.e., to decode the corresponding natural scenes. In terms of natural movie frame decoding, we compute the latent variables of four time points within each frame, averaging them to create the latent representations for that frame. We also use the KNN to predict movie frames based on latent representations (900 frames in total, i.e., 900 classes). The specific KNN procedure is as follows: First, we fit the KNN on the training set and search for the optimal hyperparameter (the number of neighbors) on the validation set, using classification accuracy as the metric. The search range is set to odd numbers between 1 and 20, with odd numbers chosen to reduce decision uncertainty. Then, the accuracy on the test set is reported as the decoding score. Notably, for movie frame decoding, we follow the established approach of CEBRA <ref type="bibr" target="#b47">[48]</ref>, considering an error of less than 1s (constraint window) between the predicted frame and the true frame as a correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alternative Models</head><p>For a comprehensive analysis, we compare TE-ViDS with several leading LVMs, including three generative models (a sequential: LFADS <ref type="bibr" target="#b41">[42]</ref>, a supervised: pi-VAE <ref type="bibr" target="#b62">[63]</ref>, and a self-supervised: Swap-VAE <ref type="bibr" target="#b35">[36]</ref>) and a nonlinear encoding method with contrastive learning (CEBRA) <ref type="bibr" target="#b47">[48]</ref>. Specifically, pi-VAE<ref type="foot" target="#foot_0">foot_0</ref> incorporates additional supervised information for latent variable construction, but does not model temporal dynamics. Swap-VAE utilizes contrastive learning to construct separated latent variables, aligning with our disentanglement goal, but it also does not explicitly model temporal dynamics. For the two models that are able to model the temporal relationships, LFADS processes sequential neural activity using bidirectional RNNs, while CEBRA encodes temporal features of sequential data with fixed convolutional kernels. Furthermore, for fair comparisons, we build a small version of our model (TE-ViDS-small) with fewer trainable parameters than Swap-VAE (see Appendix E for the number of trainable parameters).</p><p>For each dataset, all models are set to latent variables of the same dimension and trained for more than 20,000 iterations until convergence. Additionally, we apply the hyperparameter grid search for all models. More details of the training setup are presented in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Synthetic Datasets</head><p>First, we visualize the reconstructed latent variables on the non-temporal dataset (Figure <ref type="figure" target="#fig_1">2A-D</ref>). TE-ViDS reliably separates the different clusters as well as recovers the structure of the true latent variables to form clear arcs. In contrast, some of the alternative models fail to construct similar structures despite separating clusters (Swap-VAE and CEBRA), and others even struggle to separate four clusters (LFADS and pi-VAE; Appendix G). Quantitatively, the reconstruction scores also suggest that our model outperforms all alternative models (Figure <ref type="figure" target="#fig_1">2E</ref>).</p><p>For the original temporal dataset, TE-ViDS performs significantly better than those models that process sequential spikes at each time point independently, and moderately better than LFADS    which also uses RNNs to process sequential data (Figure <ref type="figure" target="#fig_1">2E</ref>). Moreover, when we shuffle the time dimension for each trial data in the original dataset to obtain a dataset without temporal dependency, the performance of our model and LFADS shows a drastic degradation. However, for models utilizing time-jittered positive samples, the performance reduction observed in Swap-VAE and CEBRA was comparatively less severe. These results demonstrate the superiority of our model in dealing with temporal data and its sensitivity to temporal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on the Mouse Neural Dataset under Natural Scene Stimuli</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, TE-ViDS achieves the highest decoding scores for all mice with a noticeable improvement over other models. In particular, CEBRA, which can encode temporal relationships, instead performs poorly in this downstream task, suggesting that using time filters to extract temporal neural features is less suitable in this case. In contrast, the time-evolving latent representations constructed by our model reliably capture temporal relationships encoded in the mouse visual cortex under different static scene stimuli, which may be helpful for natural scene discrimination. We further evaluate the decoding performance of the external and internal latent variables separately (Figure <ref type="figure" target="#fig_3">3A</ref>). We find that the external latent representations significantly outperform the internal latent representations, supporting our assumption that the former are tuned to stimulus-relevant components of neural activity, while the latter point to stimulus-irrelevant internal states.</p><p>Although our model achieves the highest performance, we find that the decoding scores for different mice show large differences. We further apply Representational Similarity Analysis (RSA) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> to the original neural activity and TE-ViDS's latent representations. Specifically, we calculate the representational similarity between the representations to each pair of trials using the Pearson correlation coefficient, yielding representational similarity matrices (RSMs). We select ten scenes that elicit the strongest average responses for visualization. By comparing the RSMs of the original    neural activity and TE-ViDS (Figure <ref type="figure" target="#fig_3">3B</ref>), we show that our model can extract clearer intrinsic patterns of neural activity and reduce the impact of useless noise, facilitating the unraveling of information processing mechanisms in the visual cortex. Moreover, we observe that the RSMs of TE-ViDS between two mice exhibit significantly different patterns. For Mouse 1, there are two redder blocks, top left and bottom right, in each of the small squares, suggesting that the neural representations of closer trials in time are more similar and divided into two periods, even under different visual stimuli. This phenomenon may be due to differences in the internal states of this mouse during the two periods, and the effect of such states on neural activity is even stronger than that of visual stimuli to some extent. For Mouse 2, there is no such clear change in state. These results echo some previous studies, showing that sensory performance in mice is strongly influenced by their internal state <ref type="bibr" target="#b3">[4]</ref>, which may also be an important reason for the variability in neural activity between subjects.</p><p>In addition to tasks on "held-out" trials, we perform a more challenging task on "held-out" stimuli. All models are trained with neural responses to 95 natural scenes (approximately 80% of all scenes), and their decoding performance is evaluated on the remaining 23 scenes. The results (see Appendix H) show that TE-ViDS outperforms other models, even for these unseen stimuli.</p><p>In summary, these results demonstrate the advantages of our model in decoding natural scene stimuli and extracting fine neural dynamics (see latent trajectories over time in Appendix I). The latent representations constructed by our model help to explain potential differences in visual information processing between individuals, which deserves further exploration.  <ref type="table" target="#tab_2">2</ref>). Specifically, our model gains more advantage over LFADS than over the others. We also show that the external latent representations are more stimulus-relevant (Figure <ref type="figure" target="#fig_5">4A</ref>). The results of Figure <ref type="figure" target="#fig_5">4B</ref> show that TE-ViDS consistently outperforms alternatives across a broad range of constraint windows, especially in finer-grained frame decoding.</p><p>For visualization, we reduce the dimensions of the latent representations of each movie frame using tSNE. We set all frames within 1s as a group and show the trajectories of latent representations for the middle 10s of the movie (Figures <ref type="figure" target="#fig_5">4C-E</ref>. We focus on Mouse 2, for which most models achieve the highest scores. See Appendix I for the results of the other parts of the movie and the other models).</p><p>Compared to alternative models, the representations of TE-ViDS show clear temporal structures along movie frames with less overlap and entanglement between different time groups.</p><p>For further analysis, we also compute RSMs of the original neural activity and TE-ViDS on Mouse 2 (Figure <ref type="figure" target="#fig_5">4F</ref>). Different from the RSMs in Figure <ref type="figure" target="#fig_3">3B</ref>, each element in these matrices is the representational similarity between the representations to each pair of movie frames and each small square involves comparisons between two trials. The comparison between neural RSM and TE-ViDS RSM provides the same evidence that our model extracts clearer patterns and reduces noise. In each small square of TE-ViDS RSM, brighter regions are near the diagonal, showing that the neural representations of neighboring frames are more similar, even for different trials. This pattern suggests that under continuous and long-duration visual stimuli, this mouse has a stronger neural response to stimuli and is less affected by its internal state. This may reveal differences in the mouse's visual encoding ability under short-duration static stimuli and long-duration continuous stimuli.</p><p>To conclude, our model achieves the highest decoding scores for natural movie frames. Furthermore, we also perform experiments on "held-out" movie frame stimuli, in which our model performs best on decoding movie frames (Appendix H). Importantly, our model constructs meaningful latent representations related to the content and temporal structure of movie stimuli at large time scales, providing insights into visual information processing mechanisms for long-duration stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Experiments for Mouse Cortical Regions</head><p>To explore the ability of six mouse visual cortical regions to process visual stimuli, we randomly sampled 150 neurons from each of the six regions of five selected mice to construct visual neural activity datasets, respectively. We train TE-ViDS on these six datasets and evaluate its decoding performance for natural scenes/natural movie frames. The experiment is repeated ten times.</p><p>As shown in Figure <ref type="figure">5</ref>, the decoding performance varies considerably across regions and the performance trends across regions between decoding the two types of stimuli are essentially the same. The anatomical hierarchy of the mouse visual cortex <ref type="bibr" target="#b50">[51]</ref> shows that VISp is the primary region, while VISl, VISrl and VISal are the mid-level regions, and VISpm and VISam are the high-level regions. Additionally, VISrl is a multi-sensory region <ref type="bibr" target="#b13">[14]</ref> that receives multi-modal sensory input.</p><p>Our results show that the decoding performance is higher in the primary and mid-level regions than in the high-level regions, with the lowest performance observed in VISrl. First, the poor performance of VISrl may be due to the fact that, as a multi-sensory region, it is not well driven by visual stimuli alone. Second, while some previous studies have suggested that the mouse visual cortex is organized in a parallel structure <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b23">24]</ref>, the differences in decoding performance across regions may provide evidence that there is heterogeneous and specialized visual encoding information for each brain region. These results echo the anatomical work <ref type="bibr" target="#b50">[51]</ref> to some extent and provide new insights into the functional hierarchy of the mouse visual cortex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This work presents a novel latent variable model, TE-ViDS, by introducing temporal structures to explicitly establish temporal relationships and constructing two parts of latent representations to disentangle the stimulus-relevant components from visual neural activity. The results of synthetic and mouse neural datasets demonstrate that our model outperforms alternative models and builds latent representations that are strongly correlated with visual input, revealing intrinsic correlations between visual neural activity and visual stimuli. A series of ablation studies demonstrates the effectiveness of our model's components (Appendix J). Furthermore, TE-ViDS aids in explaining the variability in visual information processing between subjects and provides computational evidence for the functional hierarchy of the mouse visual cortex, which may contribute to the computational neuroscience community.</p><p>There is a paucity of research analyzing visual neural activity with LVMs. CEBRA has built consistent latent representations from multimodal visual neural activity <ref type="bibr" target="#b47">[48]</ref>. Our work takes a further step in the development of powerful LVMs to yield stimulus-correlated latent representations and capture neural dynamics. However, there are still some limitations. First, in the absence of recorded internal states or behavioral information, we are unable to quantitatively assess the interpretability of internal latent variables and find it difficult to interpret the functional role of inferred internal latent representations. Second, our model exhibits substantial variability in decoding performance across individual mice. Subsequent analyses suggest that the variability in neural activity between subjects and trials <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b37">38]</ref> may be caused by the animal's internal states. This could represent a potential breakthrough for LVMs to consistently construct high-quality stimulus-correlated latent representations across conditions, but further exploration is required.</p><p>Last but not least, our approach is not limited to studying visual neural spikes from mice and can be extended to neural data from other species and modalities to investigate broader principles of biological coding mechanisms, facilitating the development of computational neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Structure of TE-ViDS</head><p>The encoder and decoder of TE-ViDS are derived from Swap-VAE. The encoder consists of three blocks, the first two of which are sequentially stacked with a linear layer, a batch normalization, and a ReLU activation. The last block differs from Swap-VAE in that it additionally introduces the hidden states of the GRU as input. The output dimensions of the three blocks are N, M, and M, where N is the number of neurons and M is the number of latent variables. The decoder is a symmetric structure of the encoder, where the first two blocks are also sequentially stacked with a linear layer, a batch normalization, and a ReLU activation, and the last block is a linear layer followed by a SoftPlus activation. We set the dimensions of the two latent variables to be equal and use a one-layer GRU for each of them, where the dimensions of the hidden states are equal to the dimensions of the latent variables. The operations of each module are shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>Overall Prior Encode Decode Recurrent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation of the Loss Function of TE-ViDS</head><p>The loss function of TE-ViDS obeys the strategy to maximize the likelihood of the joint sequential distribution p(x 1:T ). Involving the latent variables z 1:T , we have the variational lower bound: log p(x 1:T ) = q(z 1:T |x 1:T ) log p(x 1:T )dz 1:T = q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) p(z 1:T |x 1:T )</p><formula xml:id="formula_14">dz 1:T = q(z 1:T |x 1:T ) log q(z 1:T |x 1:T ) p(z 1:T |x 1:T ) dz 1:T + q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T ) dz 1:T = KL(q(z 1:T |x 1:T )∥p(z 1:T |x 1:T )) + q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T ) dz 1:T ≥ q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T ) dz 1:T , ≥ q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T ) dz 1:T -L contrast ,<label>(7)</label></formula><p>where p(x 1:T , z 1:T ) is the joint distribution as well as p(z 1:T |x 1:T ) and q(z 1:T |x 1:T ) is the true posterior and the variational approximate posterior, respectively. The true posterior is intractable.</p><p>Considering Equations 1 an 5, we know that z (e)</p><p>t is deterministic values and h</p><formula xml:id="formula_15">(i)</formula><p>t is a function of x 1:t and z (i) 1:t . Therefore, we have the factorization:</p><formula xml:id="formula_16">p(x 1:T , z 1:T ) = T t=1 p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 )p(z (i) t |x 1:t-1 , z (i) 1:t-1 ),<label>(8)</label></formula><formula xml:id="formula_17">q(z 1:T |x 1:T ) = T t=1 q(z (i) t |x 1:t , z (i) 1:t-1 ),<label>(9)</label></formula><p>where q(z (i)</p><formula xml:id="formula_18">t |x 1:t , z (i) 1:t-1 ), p(z (i) t |x 1:t-1 , z (i) 1:t-1 ) and p(x t |z (i) 1:t , z<label>(e)</label></formula><p>1:t , x 1:t-1 ) are the distributions defined by Equations 2, 3 and 4, respectively. Based on the above factorization, we decompose the variational lower bound as:</p><formula xml:id="formula_19">q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T ) dz 1:T = q(z 1:T |x 1:T ) T t=1 log p(x t |z (i) 1:t , z<label>(e)</label></formula><p>1:t , x 1:t-1 )p(z</p><formula xml:id="formula_20">(i) t |x 1:t-1 , z (i) 1:t-1 ) q(z (i) t |x 1:t , z (i) 1:t-1 )</formula><formula xml:id="formula_21">dz 1:T = T t=1 q(z 1:T |x 1:T ) log p(x t |z (i) 1:t , z<label>(e)</label></formula><p>1:t , x 1:t-1 )p(z</p><formula xml:id="formula_22">(i) t |x 1:t-1 , z (i) 1:t-1 ) q(z (i) t |x 1:t , z (i) 1:t-1 ) dz 1:T . (<label>10</label></formula><formula xml:id="formula_23">)</formula><p>When we simplify the above log-likelihood to a function g(x 1:t , z 1:t ), we have:</p><formula xml:id="formula_24">q(z 1:T |x 1:T )g(x 1:t , z 1:t )dz 1:T = q(z 1:T -1 |x 1:T -1 )q(z (i) T |x 1:T , z (i) 1:T -1 )g(x 1:t , z 1:t )dz T dz 1:T -1 = q(z 1:T -1 |x 1:T -1 )g(x 1:t , z 1:t ) q(z (i) T |x 1:T , z (i) 1:T -1 )dz T dz 1:T -1 = q(z 1:T -1 |x 1:T -1 )g(x 1:t , z 1:t )dz 1:T -1 = • • • = q(z 1:t |x 1:t )g(x 1:t , z 1:t )dz 1:t .<label>(11)</label></formula><p>Therefore, we further decompose Equation 10 as:</p><p>q(z 1:T |x 1:T ) log p(x 1:T , z 1:T ) q(z 1:T |x 1:T )</p><formula xml:id="formula_25">dz 1:T = T t=1 q(z 1:t |x 1:t ) log p(x t |z (i) 1:t , z<label>(e)</label></formula><p>1:t , x 1:t-1 )p(z</p><formula xml:id="formula_26">(i) t |x 1:t-1 , z (i) 1:t-1 ) q(z (i) t |x 1:t , z (i) 1:t-1 ) dz 1:t = T t=1 q(z 1:t |x 1:t ) log p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 )dz 1:t + q(z 1:t |x 1:t ) log p(z (i) t |x 1:t-1 , z<label>(i) 1</label></formula><formula xml:id="formula_27">:t-1 ) q(z (i) t |x 1:t , z (i) 1:t-1 ) dz 1:t = T t=1 q(z 1:t |x 1:t ) log p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 )dz 1:t - q(z 1:t-1 |x 1:t-1 )KL(q(z (i) t |x 1:t , z (i) 1:t-1 )∥p(z (i) t |x 1:t-1 , z (i) 1:t-1 ))dz 1:t-1 = q(z 1:T |x 1:T ) T t=1 log p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 )- KL(q(z (i) t |x 1:t , z (i) 1:t-1 )∥p(z (i) t |x 1:t-1 , z (i) 1:t-1 )) dz 1:T = E q(z 1:T |x 1:T ) T t=1 log p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 )- KL(q(z (i) t |x 1:t , z (i) 1:t-1 )∥p(z (i) t |x 1:t-1 , z (i) 1:t-1 )) .<label>(12)</label></formula><p>Finally, for a given sequential data x, we have the loss function:</p><formula xml:id="formula_28">L ≃ T t=1   -log p(x t |z (i) 1:t , z (e) 1:t , x 1:t-1 ) reconstruction loss + KL(q(z (i) t |x 1:t , z (i) 1:t-1 )∥p(z (i) t |x 1:t-1 , z (i) 1:t-1 )) regularization loss   +Lcontrast,<label>(13)</label></formula><p>where the first and second terms correspond to L P and D KL in the main text, respectively.</p><p>Since we assume a Poisson distribution for the inferred neural activity, L P is the Poisson negative log-likelihood:</p><formula xml:id="formula_29">L P (x t , r t ) = -log r xt t x t ! e -rt = -x t log r t + r t + log x t ! ≈ -x t log r t + r t + x t log x t -x t + 1 2 log (2πx t ).<label>(14)</label></formula><p>As for D KL , under the assumption that both the prior and the approximate posterior are Gaussian, we have:</p><formula xml:id="formula_30">D KL (z<label>(i) t ∥z (i) t ) = q(z (i)</label></formula><formula xml:id="formula_31">t |x 1:t , z<label>(i) 1:t-1 ) log q(z (i)</label></formula><formula xml:id="formula_32">t |x 1:t , z<label>(i) 1:t-1 ) p(z (i)</label></formula><formula xml:id="formula_33">t |x 1:t-1 , z (i) 1:t-1 ) dz (i) t = q(z (i) t |x 1:t , z (i) 1:t-1 ) log 1 √ 2πσ 2 z,t exp - z (i) t -µ z,t 2 2σ 2 z,t 1 √ 2π σ2 z,t exp - z (i) t -μz,t 2 2σ 2 z,t dz (i) t = - E q(z (i) t ) z (i) t -µ z,t 2 2σ 2 z,t + E q(z (i) t ) z (i) t -μz,t 2 2σ 2 z,t -log σ z,t + log σz,t = - 1 2 + σ 2 z,t + µ 2 z,t -2µ z,t μz,t + μ2 z,t<label>2σ 2 z,t</label></formula><p>-log σ z,t + log σz,t</p><formula xml:id="formula_34">= 1 2 -1 + (µ z,t -μz,t ) 2 + σ 2 z,t σ2 z,t -log σ 2 z,t + log σ2 z,t .<label>(15)</label></formula><p>Finally, we apply NT-Xent loss as the contrastive loss:</p><formula xml:id="formula_35">L contrast = -log exp sim z (e) , z<label>(e)</label></formula><p>pos /τ</p><formula xml:id="formula_36">exp sim z (e) , z<label>(e)</label></formula><p>pos /τ + exp sim z (e) , z</p><p>neg /τ ,</p><p>where sim( * , * ) is the cosine similarity and τ is the temperature coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generating Procedure of Synthetic Datasets</head><p>Non-temporal dataset. First, we generate labels u i from four uniform distributions on</p><formula xml:id="formula_39">[ 2i×π 4 ,<label>(2i+1)×π 4</label></formula><p>], i ∈ {0, 1, 2, 3}, in preparation for building four clusters. Second, for each cluster, we sample 2-dimensional latent variables z from independent Gaussian distribution with (5 sin u i , 5 cos u i ) as mean and (0.6 -0.5| cos u i |, 0.5| cos u i |) as variance. Third, we feed sampled latent variables into a RealNVP network <ref type="bibr" target="#b15">[16]</ref> to form firing rates of 100-dimensional observations and generate the synthetic neural activity from the Poisson distribution. Each cluster contains 4,000 samples. This dataset, being label-dependent, is used to evaluate the models' ability to construct discriminative latent variables.</p><p>Temporal dataset. We generate three dynamic latent variables from the Lorenz system, consisting of a set of nonlinear equations. The firing rates of 30 simulated neurons are then computed by randomly weighted linear readouts from the Lorenz latent variables. Synthetic neural activity is also generated from the Poisson distribution. The hyperparameters of the Lorenz system follow a previous work <ref type="bibr" target="#b53">[54]</ref>. We run the Lorenz system for 1s (1ms for a time point) from five randomly initialized conditions. Each condition contains 20 trials. This dataset is for assessing the models' ability to represent temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Characters of Mouse Visual Neural Dataset</head><p>The full names and abbreviations of all cortical regions are listed in Table <ref type="table" target="#tab_4">3</ref> and the number of neurons for all chosen mice is also presented. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Number of Trainable Parameters of All Models</head><p>The number of model parameters is roughly proportional to the number of input neurons. We present the number of parameters for the Mouse 1 dataset in Table <ref type="table" target="#tab_5">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Training Setup</head><p>We list the information of training datasets and the hyperparameters of the model training in Table <ref type="table" target="#tab_6">5</ref>.</p><p>For each model, we perform the grid search for the learning rate (1.0 × 10 -5 to 1.0 × 10 -3 ) and the weights of each loss term (0.01 to 10) to achieve optimal performance on the validation set. Visual neural dataset. For the neural activity under natural scenes, each sample input to TE-ViDS is sequential data from 5 time points and the offset of positive samples from target samples is within ±3 time points. For the neural activity under the natural movie, each sample consists of 4 time points and the maximum absolute offset is 2. As for alternative models, pi-VAE and Swap-VAE take the neural activity of an independent time point as an input sample. LFADS processes samples with the same length as our model. For CEBRA, following the original approach <ref type="bibr" target="#b47">[48]</ref>, we take the surrounding points centered on the target point to form a sequence (5 time points for natural scenes and 4 for the natural movie).</p><p>Non-temporal dataset. Since this dataset does not involve temporal relationships, each sample can be conceptualized as a sequence comprising a single time step. In the context of contrastive learning for such data, the approach of defining positive pairs through temporal shifting is inapplicable. Instead, we adopt a strategy provided in CEBRA, selecting positive samples based on the distance of their respective labels, thereby implementing a form of supervised contrastive learning.</p><p>Temporal dataset. TE-ViDS and LFADS use 50ms of data as input, while the other models take data at one time point. The offset of positive samples is set to 5ms for our model.</p><p>All models for all datasets are trained on 1 GPU (NVIDIA A100).</p><p>G Additional Results of Alternatives on Synthetic Non-Temporal Dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Experiment on the Visual Neural Datatsets</head><p>We train models with neural activity under approximately 80% natural scene/movie stimuli and evaluate them using "held-out" stimuli, i.e., completely unseen scenes/movies. As Tables <ref type="table" target="#tab_7">6</ref> and<ref type="table" target="#tab_8">7</ref> show, TE-ViDS consistently achieves the best performance on such challenging tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Additional Results of Latent Trajectories on the Visual Neural Datatsets</head><p>For the mouse neural dataset under natural scene stimuli, we visualize the latent representations by embedding them in two dimensions using tSNE (Figure <ref type="figure">8</ref>). We focus on Mouse 1, for which most of the models achieve the highest scores. We select ten scenes that elicit the strongest average responses for visualization. Specifically, we reduce the dimensions of latent representations at a single time point and take the average across all trials, to show the latent trajectories over time for each scene.</p><p>The result of TE-ViDS exhibits a clear temporal structure for different natural scenes. For LFADS, its ability to encode temporal features of sequential neural activity results in clear temporal structures, but the latent representations of different classes are largely intermingled. For pi-VAE, Swap-VAE, and CEBRA, their latent trajectories show varying degrees of entanglement over time. These results suggest that our model effectively distinguishes between category information and captures temporal information from neural dynamics well.</p><p>For the mouse neural dataset under natural movie stimuli, in addition to Figures <ref type="figure" target="#fig_5">4C-E</ref>, we visualize the results of all models and all three parts of the movie for Mouse 2 (Figure <ref type="figure">9</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Ablation Studies</head><p>We perform ablation studies for several aspects of TE-ViDS's components and neural activity input dimensions to explore their impact on performance.</p><p>The loss function and the recurrent module of TE-ViDS. To show the effectiveness of the components of our model, we conduct some ablation studies on the loss function and the recurrent  module (Tables <ref type="table" target="#tab_9">8</ref> and<ref type="table" target="#tab_10">9</ref>). In terms of contrastive learning, we first exclude negative samples from the computation of the contrastive loss and use only the cosine distance between the external latent variables of positive sample pairs as the loss function. In other words, we only bring the positive pairs closer. This results in a decrease in performance, suggesting that negative samples are useful. Next, we remove either the contrastive loss or the swap operation and observe a similar impact for both. Finally, we remove both, meaning that there are no more objectives related to contrastive learning. The significant decrease in performance suggests that contrastive learning plays a crucial role in our models. In terms of the regular loss of the internal latent variables, we originally assumed that the prior distribution is time-dependent. When we assume that it is an independent standard normal distribution at each time step, the model's performance degrades, demonstrating that time-dependent assumptions about the prior are also important. In terms of the recurrent module, the results suggest that GRU is a better choice when considering the trade-off between performance and computational efficiency. Besides, we evaluate a non-recurrent version of our model by setting the time steps of GRU to 1, which demonstrates that the recurrent module plays a critical role.</p><p>The split design of latent variables. We build two models based on TE-ViDS, one with external variables only (External-Only) and the other with internal variables only (Internal-Only). Both models perform worse than TE-ViDS, confirming the split design's importance and effectiveness.</p><p>The dimension of latent variables and the number of input neurons. We perform ablation studies on the number of latent variables and input neurons. As shown in Figures <ref type="figure" target="#fig_10">10</ref> and<ref type="figure" target="#fig_11">11</ref>, first, the performance saturates gradually as the dimension of the latent variables increases, which suggests that it is sufficient to choose a dimension in a reasonable range (not much fewer than input neurons). Second, performance decreases as the number of sampled neurons decreases, suggesting that for each mouse, all recorded neurons contribute to the representation of visual stimuli.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Additional Experiment on the Mouse Visual Neural Dataset from CEBRA</head><p>The neural dataset used in CEBRA is also preprocessed from the Allen Brain Observatory Visual Coding dataset. The dataset is generalized by sampling different numbers of neurons from the same visual cortical region of all mice, without taking into account the variability between subjects. We evaluate our model on this dataset. The results (Figure <ref type="figure" target="#fig_1">12</ref>) show that our model performs better on the primary and mid-level regions, while CEBRA performs better on the high-level regions. Moreover, in the case of a sample with 40 time steps (CEBRA reported their highest performance in this setting), our model can achieve a performance of more than 95% with fewer trainable parameters (TE-ViDS: 0.44M; CEBRA: 1.09M). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The method overview. A. The illustration of TE-ViDS for analyzing visual neural activity in the mouse visual cortex. The encoder extracts spatial features from sequential spike data. The latent variables are evolved conditionally on features of the encoder and RNNs' state factors over time.The decoder maps latent variables to inferred firing rates. B. The illustration of different learning objectives for the two parts of latent representations of TE-ViDS. For external latent representations, we apply contrastive loss to encourage them to distinguish the stimulus-relevant components. Given a reference sample (white dot), the red dot is a positive sample and the orange dots are negative samples. For internal latent representations, we use the KL divergence to constrain their distribution to a time-dependent prior distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results on synthetic datasets. A. The true latent variables of the non-temporal dataset. B-D.The inferred latent variables of our model and some alternative models. E. The reconstruction scores of all models on the non-temporal and temporal datasets. The standard error is computed on 10 runs with different random initializations.</figDesc><graphic coords="6,335.73,77.72,168.45,53.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on the mouse neural dataset under natural scene stimuli. A. The decoding scores (%) of the full, external and internal latent representations of TE-ViDS for 118 natural scenes. B. RSMs computed on the original neural representations and TE-ViDS's latent representations, respectively (Mouse 1 and Mouse 2). Each element in a matrix is the similarity between two trials' representations. Each small square involves comparisons between two natural scenes, containing 50 trials.</figDesc><graphic coords="7,422.50,287.37,60.37,60.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results on the mouse neural dataset under natural movie stimuli. A. The decoding scores (%) of the full, external and internal latent representations of TE-ViDS. B. The decoding scores (%) for movie frames under different constraint windows of predicted frames and the true frames. C-E. Visualization results of latent trajectories (Mouse 2). Each color corresponds to all frames within 1s. Small dots denote one frame. Large dots denote the average among a group of frames. The red dashed line connects all averages. F. RSMs computed on the original neural representations and TE-ViDS's latent representations (Mouse 2). Each element is the similarity between two frames' representations. Each small square involves comparisons between two trials, containing 900 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 : 4 . 6</head><label>546</label><figDesc>Figure 5: The decoding scores (%) of TE-ViDS for natural scenes/natural movie frames on six mouse visual cortical region datasets. The box plots are based on 10 runs with different random initializations.4.6 Results on the Mouse Neural Dataset under Natural Movie StimuliTE-ViDS performs best for four mice (Table2). Specifically, our model gains more advantage over LFADS than over the others. We also show that the external latent representations are more stimulus-relevant (Figure4A). The results of Figure4Bshow that TE-ViDS consistently outperforms alternatives across a broad range of constraint windows, especially in finer-grained frame decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The operations of each module in TE-ViDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The inferred latent variables of alternatives on the non-temporal dataset.</figDesc><graphic coords="19,183.47,294.63,79.20,79.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Visualization results of latent trajectories on the mouse visual neural dataset under natural scene stimuli (Mouse 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The results of ablation studies on the dimension of latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11:  The results of ablation studies on the number of input neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The decoding scores (%) for 118 natural scenes on the mouse visual neural dataset. The standard error is computed on 10 runs with different random initializations. ViDS-small 47.08±2.86 23.95±1.<ref type="bibr" target="#b38">39</ref> 29.08±1.47 34.95±0.90 9.93±0.27 TE-ViDS 50.86±0.81 27.24±0.47 29.90±0.43 38.05±0.53 9.44±0.20</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell></cell><cell></cell><cell cols="4">Mouse 5</cell></row><row><cell>PCA</cell><cell>0.59</cell><cell>1.53</cell><cell>1.53</cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.85</cell></row><row><cell>LFADS</cell><cell cols="10">30.76±0.65 16.46±0.49 22.20±0.26 19.69±0.38 4.69±0.22</cell></row><row><cell>pi-VAE</cell><cell cols="10">7.49±0.77 19.42±0.62 22.92±0.37 13.71±1.39 2.22±0.64</cell></row><row><cell>Swap-VAE</cell><cell cols="10">32.81±1.47 24.34±0.57 14.36±1.31 14.85±1.29 3.92±0.37</cell></row><row><cell>CEBRA</cell><cell>1.53±0.15</cell><cell>3.42±0.07</cell><cell>4.86±0.19</cell><cell cols="7">2.81±0.23 1.08±0.10</cell></row><row><cell cols="5">Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class 7 Class 8 Class 9 Class 10 Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class 7 Class 8 Class 9 TE-Class 1 Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Class 7 Class 8 Class 9 Class 10 Class 10</cell><cell>Class 2</cell><cell>Class 3</cell><cell>Class 4</cell><cell>Class 5</cell><cell>Class 6</cell><cell>Class 7</cell><cell>Class 8</cell><cell>Class 9</cell><cell>Class 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The decoding scores (%, in 1s window) for natural movie frames on the mouse visual neural dataset. The standard error is computed on 10 runs with different random initializations.</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell>Mouse 5</cell></row><row><cell>PCA (baseline)</cell><cell>8.44</cell><cell>28.77</cell><cell>25.42</cell><cell>21.56</cell><cell>11.69</cell></row><row><cell>LFADS</cell><cell cols="5">8.94±0.25 26.57±2.46 26.77±2.23 24.76±1.80 12.69±1.38</cell></row><row><cell>pi-VAE</cell><cell cols="5">10.24±0.31 42.51±0.65 36.96±0.60 38.31±0.52 18.08±0.59</cell></row><row><cell>Swap-VAE</cell><cell cols="5">12.19±0.20 51.31±0.73 45.96±0.34 41.53±0.63 22.70±0.42</cell></row><row><cell>CEBRA</cell><cell cols="5">10.62±0.18 52.76±0.89 61.01±0.76 42.11±0.73 22.33±0.31</cell></row><row><cell cols="6">TE-ViDS-small 13.23±0.25 64.09±0.41 59.36±0.30 53.46±0.58 29.60±0.26</cell></row><row><cell>TE-ViDS</cell><cell cols="5">13.88±0.19 65.38±0.36 59.88±0.72 54.33±0.54 30.18±0.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neural RSM</cell><cell>TE-ViDS RSM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mouse 2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Characters of the neural dataset.</figDesc><table><row><cell>Cortical Region</cell><cell cols="6">Abbreviation Mouse 1 Mouse 2 Mouse 3 Mouse 4 Mouse 5</cell></row><row><cell>primary visual cortex</cell><cell>VISp</cell><cell>75</cell><cell>51</cell><cell>93</cell><cell>63</cell><cell>52</cell></row><row><cell>lateromedial area</cell><cell>VISl</cell><cell>39</cell><cell>30</cell><cell>56</cell><cell>38</cell><cell>20</cell></row><row><cell>rostrolateral area</cell><cell>VISrl</cell><cell>49</cell><cell>24</cell><cell>58</cell><cell>44</cell><cell>41</cell></row><row><cell>anterolateral area</cell><cell>VISal</cell><cell>42</cell><cell>51</cell><cell>43</cell><cell>71</cell><cell>46</cell></row><row><cell>posteromedial area</cell><cell>VISpm</cell><cell>62</cell><cell>90</cell><cell>17</cell><cell>19</cell><cell>64</cell></row><row><cell>anteromedial area</cell><cell>VISam</cell><cell>94</cell><cell>72</cell><cell>49</cell><cell>60</cell><cell>64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The number of model parameters for the Mouse 1 dataset.</figDesc><table><row><cell cols="6">LFADS pi-VAE Swap-VAE CEBRA TE-ViDS-small TE-ViDS</cell></row><row><cell>Number of parameters 0.45M</cell><cell>0.49M</cell><cell>0.38M</cell><cell>0.71M</cell><cell>0.29M</cell><cell>0.68M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The information of each dataset and the hyperparameters of the training.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Training Size Validation Size Test Size Latent Dimension Learning Rate Optimizer</cell></row><row><cell>non-temporal synthetic dataset</cell><cell>12800</cell><cell>-</cell><cell>3200</cell><cell>32</cell><cell>0.0005</cell><cell>Adam</cell></row><row><cell>temporal synthetic dataset</cell><cell>80000</cell><cell>-</cell><cell>20000</cell><cell>8</cell><cell>0.001</cell><cell>Adam</cell></row><row><cell>visual neural dataset under natural scenes</cell><cell>118000</cell><cell>14750</cell><cell>14750</cell><cell>128</cell><cell>0.0001</cell><cell>Adam</cell></row><row><cell>visual neural dataset under natural movie</cell><cell>28800</cell><cell>3600</cell><cell>3600</cell><cell>128</cell><cell>0.0001</cell><cell>Adam</cell></row><row><cell cols="7">Furthermore, we present the preprocessing implementation of training samples for all models.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The decoding scores (%) for "held-out" natural scenes.</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell>Mouse 5</cell></row><row><cell>LFADS</cell><cell cols="5">48.96±1.13 34.78±1.83 39.22±2.47 38.17±1.60 15.74±1.14</cell></row><row><cell>pi-VAE</cell><cell cols="5">23.57±3.17 37.04±2.42 41.91±1.90 20.70±2.05 10.35±1.10</cell></row><row><cell>Swap-VAE</cell><cell cols="5">55.48±2.34 17.65±1.94 36.61±3.21 44.09±2.39 14.17±1.81</cell></row><row><cell>CEBRA</cell><cell>5.83±0.58</cell><cell cols="3">8.87±0.80 13.04±1.40 9.39±0.77</cell><cell>4.96±0.69</cell></row><row><cell cols="6">TE-ViDS-small 62.61±2.50 41.30±1.72 47.57±3.06 50.52±2.77 23.30±1.20</cell></row><row><cell>TE-ViDS</cell><cell cols="5">65.91±1.31 38.70±1.71 46.52±2.09 53.57±1.88 24.52±1.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The decoding scores (%, in 1s window) for "held-out" natural movie frames.</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell>Mouse 5</cell></row><row><cell>LFADS</cell><cell cols="5">38.89±0.98 51.78±1.28 49.06±2.04 46.28±1.89 39.22±1.42</cell></row><row><cell>pi-VAE</cell><cell cols="5">38.89±1.34 65.39±0.95 66.56±1.31 59.11±1.34 46.17±1.13</cell></row><row><cell>Swap-VAE</cell><cell cols="5">36.22±1.83 59.72±1.35 58.33±1.62 57.72±1.77 44.50±1.19</cell></row><row><cell>CEBRA</cell><cell cols="5">37.44±1.04 54.39±1.41 52.89±0.84 48.89±0.86 37.89±1.60</cell></row><row><cell cols="6">TE-ViDS-small 39.28±2.15 68.06±1.09 72.50±0.81 65.50±0.78 47.83±1.47</cell></row><row><cell>TE-ViDS</cell><cell cols="5">42.00±0.52 66.28±1.07 69.06±0.84 66.28±1.43 47.00±1.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The decoding scores (%) of ablation studies for the loss function, the recurrent module and the split design of TE-ViDS on the mouse visual neural dataset under natural scene stimuli.</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell>Mouse 5</cell></row><row><cell>TE-ViDS</cell><cell cols="5">50.86±0.81 27.24±0.47 29.90±0.43 38.05±0.53 9.44±0.20</cell></row><row><cell>w/o negative samples</cell><cell cols="5">21.85±1.86 9.71±0.72 12.32±1.22 20.03±1.26 6.20±0.43</cell></row><row><cell>w/o contrastive loss</cell><cell cols="5">30.17±1.74 8.24±0.33 13.76±0.87 23.42±0.79 7.19±0.28</cell></row><row><cell>w/o swap operation</cell><cell cols="5">28.80±0.86 14.27±0.42 18.92±0.93 15.97±0.85 4.68±0.24</cell></row><row><cell cols="2">w/o contrastive loss and swap operation 6.88±0.64</cell><cell>3.92±0.22</cell><cell>4.17±0.72</cell><cell cols="2">6.58±0.32 2.56±0.12</cell></row><row><cell>with temporal independent prior</cell><cell cols="5">28.97±1.16 9.05±0.64 15.10±0.98 15.54±0.69 3.97±0.32</cell></row><row><cell>GRU→Vanilla RNN</cell><cell cols="5">47.64±0.78 25.61±0.68 28.58±0.66 34.15±0.43 7.17±0.30</cell></row><row><cell>GRU→LSTM</cell><cell cols="5">49.97±0.50 26.92±0.59 29.31±0.23 36.73±0.41 9.24±0.27</cell></row><row><cell>Non-recurrent</cell><cell cols="5">33.32±0.99 21.78±0.37 22.63±0.47 27.27±0.64 7.69±0.40</cell></row><row><cell>External-Only</cell><cell cols="5">45.41±1.16 21.49±0.46 23.83±0.73 27.39±0.67 6.80±0.23</cell></row><row><cell>Internal-Only</cell><cell>2.10±0.21</cell><cell>1.78±0.19</cell><cell>1.44±0.16</cell><cell cols="2">2.49±0.22 1.46±0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>The decoding scores (%, in 1s window) of ablation studies for the loss function, the recurrent module and the split design of TE-ViDS on the mouse visual neural dataset under natural movie stimuli.</figDesc><table><row><cell>Models</cell><cell>Mouse 1</cell><cell>Mouse 2</cell><cell>Mouse 3</cell><cell>Mouse 4</cell><cell>Mouse 5</cell></row><row><cell>TE-ViDS</cell><cell cols="5">13.88±0.19 65.38±0.36 59.88±0.72 54.33±0.54 30.18±0.40</cell></row><row><cell>w/o negative samples</cell><cell cols="5">11.27±0.36 49.59±1.18 45.67±0.60 44.17±0.42 19.93±0.35</cell></row><row><cell>w/o contrastive loss</cell><cell cols="5">11.24±0.23 47.98±0.90 44.09±0.67 44.02±0.47 18.24±0.41</cell></row><row><cell>w/o swap operation</cell><cell cols="5">10.22±0.31 49.39±0.57 45.30±0.41 43.12±0.57 21.83±0.39</cell></row><row><cell cols="6">w/o contrastive loss and swap operation 9.16±0.37 24.84±1.10 22.33±0.81 26.49±0.66 12.02±0.49</cell></row><row><cell>with temporal independent prior</cell><cell cols="5">12.09±0.16 57.74±0.62 53.87±0.49 46.90±0.48 22.92±0.43</cell></row><row><cell>GRU→Vanilla RNN</cell><cell cols="5">13.08±0.31 63.19±0.55 59.13±0.42 53.83±0.41 29.62±0.40</cell></row><row><cell>GRU→LSTM</cell><cell cols="5">12.77±0.25 64.69±0.53 60.00±0.60 54.37±0.41 28.50±0.61</cell></row><row><cell>Non-recurrent</cell><cell cols="5">11.14±0.30 53.26±0.48 48.31±0.47 43.81±0.40 22.98±0.44</cell></row><row><cell>External-Only</cell><cell cols="5">12.16±0.24 63.33±0.29 58.64±0.37 52.11±0.39 30.24±0.54</cell></row><row><cell>Internal-Only</cell><cell cols="5">7.57±0.24 11.73±0.43 12.86±0.35 16.10±0.33 9.89±0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>pi-VAE incorporates the label prior during training, but the inferred latent variables are built without the label prior at the evaluation stage. This way is used in all experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work is supported by grants from the <rs type="funder">Shenzhen KQTD</rs> (No. <rs type="grantNumber">20240729102051063</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62027804</rs>, No. <rs type="grantNumber">62425101</rs>, No. <rs type="grantNumber">62332002</rs>, No. <rs type="grantNumber">62088102</rs>, and No. <rs type="grantNumber">62206141</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4V6US38">
					<idno type="grant-number">20240729102051063</idno>
				</org>
				<org type="funding" xml:id="_8YwjEJc">
					<idno type="grant-number">62027804</idno>
				</org>
				<org type="funding" xml:id="_48AAtzH">
					<idno type="grant-number">62425101</idno>
				</org>
				<org type="funding" xml:id="_txssX7r">
					<idno type="grant-number">62332002</idno>
				</org>
				<org type="funding" xml:id="_AC7HgKh">
					<idno type="grant-number">62088102</idno>
				</org>
				<org type="funding" xml:id="_wAKkeJV">
					<idno type="grant-number">62206141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamical flexible inference of nonlinear latent factors and structures in neural population activity</title>
		<author>
			<persName><forename type="first">Hamidreza</forename><surname>Abbaspourazad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eray</forename><surname>Erturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><forename type="middle">M</forename><surname>Shanechi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="108" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal subspace identification for modeling discrete-continuous spiking and field potential population activity</title>
		<author>
			<persName><forename type="first">Parima</forename><surname>Ahmadipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Omid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><forename type="middle">M</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><surname>Shanechi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26001</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prefrontal cortex exhibits multidimensional dynamic encoding during decision-making</title>
		<author>
			<persName><surname>Mikio C Aoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Valerio Mante</surname></persName>
		</author>
		<author>
			<persName><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1410" to="1420" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mice alternate between discrete strategies during perceptual decision-making</title>
		<author>
			<persName><forename type="first">Zoe</forename><forename type="middle">C</forename><surname>Ashwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><forename type="middle">R</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brain</forename><surname>Laboratory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="212" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian process linking functions for mind, brain, and behavior</title>
		<author>
			<persName><forename type="first">Giwon</forename><surname>Bahg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Galdo</surname></persName>
		</author>
		<author>
			<persName><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page" from="29398" to="29406" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The functional specialization of visual cortex emerges from training parallel pathways with self-supervised predictive learning</title>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Bakhtiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mineault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25164" to="25178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A flow-based latent state generative model of neural population responses to natural images</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bashiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin-Klemens</forename><surname>Lurz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taliah</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuokun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Sinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15801" to="15815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optical imaging of neuronal populations during decision-making</title>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">Di</forename><surname>Kevin L Briggman</surname></persName>
		</author>
		<author>
			<persName><surname>Abarbanel</surname></persName>
		</author>
		<author>
			<persName><surname>Kristan</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="issue">5711</biblScope>
			<biblScope unit="page" from="896" to="901" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Mark M Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Nuyujukian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural population dynamics during reaching</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">487</biblScope>
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</title>
		<author>
			<persName><surname>Saskia Ej De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">A</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Lecoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Buice</surname></persName>
		</author>
		<author>
			<persName><surname>Groblewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><surname>Ledochowitsch</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel Millman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="138" to="151" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>James J Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoding visual neural representations by multimodal learning of brain-visual-linguistic features</title>
		<author>
			<persName><forename type="first">Changde</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiguang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="10760" to="10777" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cryptography-based approach for movement decoding</title>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Eva L Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><forename type="middle">L</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">E</forename><surname>Naufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Körding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="967" to="976" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">State dependence of noise correlations in macaque primary visual cortex</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Alexander S Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manivannan</forename><surname>James Cotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">H</forename><surname>Subramaniyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathryn</forename><forename type="middle">R</forename><surname>Denfield</surname></persName>
		</author>
		<author>
			<persName><surname>Cadwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stelios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Smirnakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Variational recurrent auto-encoders</title>
		<author>
			<persName><forename type="first">Otto</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><forename type="middle">R</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linear dynamical neural population models through nonlinear embeddings</title>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">W</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent switching dynamical systems models for multiple interacting neural populations</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Whiteway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14867" to="14878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-modal gaussian process variational autoencoders for neural and behavioral data</title>
		<author>
			<persName><forename type="first">Rabia</forename><surname>Gondur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Usama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sikandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikio</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Christian Aoi</surname></persName>
		</author>
		<author>
			<persName><surname>Keeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long-range feedback spiking network captures dynamic and static representations of the visual cortex under movie stimuli</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liutao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huihui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="11432" to="11455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Targeted neural dynamical modeling</title>
		<author>
			<persName><forename type="first">Cole</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Jude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29379" to="29392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srdjan</forename><surname>Ostojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Latent diffusion for neural spiking data</title>
		<author>
			<persName><forename type="first">Jaivardhan</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying natural images from human brain activity</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kendrick N Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="issue">7185</biblScope>
			<biblScope unit="page" from="352" to="355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enabling hyperparameter optimization in sequential autoencoders for spiking neural data</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshtkaran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A large-scale neural network training framework for generalized estimation of single-trial population dynamics</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Reza Keshtkaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Sedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Raeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diya</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">L</forename><surname>Basrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansem</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">E</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Pandarinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1572" to="1577" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representational similarity analysis-connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in systems neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matching categorical object representations in inferior temporal cortex of man and monkey</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerzy</forename><surname>Bodurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Esteky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiji</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1126" to="1141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A unifying perspective on neural manifolds and circuits for cognition</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><forename type="middle">A</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="363" to="377" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="914" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Drop, swap, and generate: A self-supervised approach for generating neural activity</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Dabagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Heng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hengen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10587" to="10599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context-dependent computation by recurrent dynamics in prefrontal cortex</title>
		<author>
			<persName><forename type="first">David</forename><surname>Valerio Mante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><surname>Newsome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">503</biblScope>
			<biblScope unit="issue">7474</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stimulus-dependent representational drift in primary visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><surname>Goard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5169</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Internal state dynamics shape brainwide activity and foraging behaviour</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>João C Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Schaak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Drew N Robson</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7789</biblScope>
			<biblScope unit="page" from="239" to="243" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latent variable models reconstruct diversity of neuronal response to drifting gratings in murine visual cortex</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jeremiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa Hm</forename><surname>Palmerston</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="75741" to="75750" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inferring stochastic low-rank recurrent neural networks from neural data</title>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Pals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Erdem Sagtekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Gloeckler</surname></persName>
		</author>
		<author>
			<persName><surname>Macke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="18225" to="18264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inferring single-trial neural population dynamics using sequential auto-encoders</title>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J O'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stavisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><forename type="middle">R</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="805" to="815" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural latents benchmark &apos;21: Evaluating latent variable models of neural population activity</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raeed</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansem</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</title>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks<address><addrLine>Jonathan Pillow, Il Memming Park, Eva Dyer, and Chethan Pandarinath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">M</forename><surname>Patrick T Sadtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Quick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">C</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Tyler-Kabara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural constraints on learning</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="issue">7515</biblScope>
			<biblScope unit="page" from="423" to="426" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Omid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamidreza</forename><surname>Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><forename type="middle">T</forename><surname>Abbaspourazad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><forename type="middle">M</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><surname>Shanechi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dissociative and prioritized modeling of behaviorally relevant neural dynamics using recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Omid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><forename type="middle">M</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><surname>Shanechi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2033" to="2045" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards the neural population doctrine</title>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learnable latent embeddings for joint behavioural and neural analysis</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Hwa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mackenzie</forename><forename type="middle">Weygandt</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">617</biblScope>
			<biblScope unit="issue">7960</biblScope>
			<biblScope unit="page" from="360" to="368" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Comparison against task driven artificial neural networks reveals functional properties in mouse visual cortex</title>
		<author>
			<persName><forename type="first">Jianghong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shea-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Buice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mousenet: A biologically constrained convolutional neural network model for the mouse visual cortex</title>
		<author>
			<persName><forename type="first">Jianghong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shea-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Mihalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Buice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1010427</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Survey of spiking in the mouse visual system reveals functional hierarchy</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">H</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Séverine</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corbett</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nile</forename><surname>Graddis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greggory</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tamina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Luviano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">592</biblScope>
			<biblScope unit="issue">7852</biblScope>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural dynamics underlying birdsong practice and performance</title>
		<author>
			<persName><forename type="first">Jonnathan</forename><surname>Singh Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Liberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">599</biblScope>
			<biblScope unit="issue">7886</biblScope>
			<biblScope unit="page" from="635" to="639" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reverse engineering recurrent neural networks with jacobian switching linear dynamical systems</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16700" to="16713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Lfads -latent factor analysis via dynamical systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Pandarinath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large-scale neural recordings call for new insights to link brain and behavior</title>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Doiron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Leifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">K</forename><surname>Churchland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Computation through neural population dynamics</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="275" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Contrastvae: Contrastive variational autoencoder for sequential recommendation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM international conference on information &amp; knowledge management</title>
		<meeting>the 31st ACM international conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2056" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural encoding and decoding with deep learning for dynamic natural vision</title>
		<author>
			<persName><forename type="first">Haiguang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun-Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4136" to="4160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gaussian process based nonlinear latent structure discovery in multivariate spike train data</title>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Keeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stable representation of a naturalistic movie emerges from episodic activity with gain variability</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Goard</surname></persName>
		</author>
		<author>
			<persName><surname>Wessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5170</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</title>
		<author>
			<persName><forename type="first">Byron M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopal</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Variational latent gaussian process for recovering single-trial dynamics from population spike trains</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il</forename><surname>Memming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1293" to="1316" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning identifiable and interpretable latent models of highdimensional neural activity using pi-vae</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue-Xin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7234" to="7247" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
