<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal-based Time Series Domain Generalization for Vehicle Intention Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
							<email>yeping_hu@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Jia</surname></persName>
							<email>xiaogangjia_hit@outlook.com</email>
						</author>
						<author>
							<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
							<email>tomizuka@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
							<email>wzhan@berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Mechanical Engineering</orgName>
								<orgName type="institution" key="instit1">University of Bristol</orgName>
								<orgName type="institution" key="instit2">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal-based Time Series Domain Generalization for Vehicle Intention Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately predicting possible behaviors of traffic participants is an essential capability for autonomous vehicles. Since autonomous vehicles need to navigate in dynamically changing environments, they are expected to make accurate predictions regardless of where they are and what driving circumstances they encountered. Therefore, generalization capability to unseen domains is crucial for prediction models when autonomous vehicles are deployed in the real world. In this paper, we aim to address the domain generalization problem for vehicle intention prediction tasks and a causal-based time series domain generalization (CTSDG) model is proposed. We construct a structural causal model for vehicle intention prediction tasks to learn an invariant representation of input driving data for domain generalization. We further integrate a recurrent latent variable model into our structural causal model to better capture temporal latent dependencies from time-series input data. The effectiveness of our approach is evaluated via real-world driving data. We demonstrate that our proposed method has consistent improvement on prediction accuracy compared to other state-of-the-art domain generalization and behavior prediction methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Researchers have developed various machine learning algorithms to enhance the accuracy of intention and motion prediction tasks for autonomous vehicles <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Although these machine learning algorithms provide excellent prediction performance, a key assumption underlying the remarkable success is that the training and test data usually follow similar statistics. Otherwise, when test domains are unseen or Out-of-Distribution (OOD) <ref type="bibr" target="#b10">[11]</ref>, the resulting train-test domain shift leads to significant degradation in prediction performance. Domain generalization (DG) aims to build models that are designed for increased robustness to domain-shift without requiring access to target domain data. Many researchers have been working on developing DG algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, but most of these models are designed for tasks such as object recognition, semantic segmentation, image classification, and disease detection. In this paper, our goal is to design a DG algorithm for vehicle intention prediction task which is different from typical DG tasks in three aspects: <ref type="bibr" target="#b0">(1)</ref> the input is numerical time-series data instead of static image; <ref type="bibr" target="#b1">(2)</ref> each input data point contains information of multiple agents (e.g. vehicles) with interactions involved, instead of a single agent (e.g. patient) with no interaction; <ref type="bibr" target="#b2">(3)</ref>   rotation angle or style for image classification task). Therefore, directly applying state-of-the-art DG approaches may not be suitable for our problem setting and could result in undesirable performance. It is thus necessary to develop a domain generalization model that considers the discrepancies in the aforementioned aspects and is applicable for vehicle intention prediction task. In this paper, a causal-based time series domain generalization (CTSDG) model for vehicle intention prediction is proposed. To the best of our knowledge, this is the first work trying to tackle time series domain generalization problem for vehicle intention prediction task. The effectiveness of our approach is evaluated on real-world driving data under different scenarios. Our method also shows consistent improvement on prediction accuracy compared to other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>Let X be the input space and Y the label space, a domain is defined as a joint distribution P XY on X × Y. In the context of domain generalization, we assume to have access to K domains, D = {D k } K k=1 , each associated with a joint distribution P k XY . In each domain, data-label pairs are sampled from a dataset D k = {(x</p><formula xml:id="formula_0">(k) i , y (k) i )} N k i=1 with (x (k) i , y<label>(k)</label></formula><p>i ) ∼ P k XY , where N k is the number of labeled data points in the k-th domain and x i = (x i t ) T t=1 = ([ξ A,i t , ξ B,i t ]) T t=1 denotes a multivariate time series with x i t ∈ R H×2 . For a given vehicle, we use ξ to represent historical trajectories. In this work, given historical trajectories of interacting vehicle pairs (i.e. car A and car B), the domain generalization task is to learn an intention predictor f (i.e. predict interaction outcomes such as pass/yield) that generalizes well to unseen target domains k / ∈ D k where P (k )</p><formula xml:id="formula_1">XY = P (k)</formula><p>XY , ∀k ∈ {1, . . . , K}. A directed black edge (-) denotes a causal relationship; dashed bidirectional edges ( ) denote correlation; hollow dashed arrows ( ) denote an inclusion relationship. Blue lines (-) denote the inference process, q(z t |x ≤t , z &lt;t ); green lines (-) represent the generation process, p(x t |z ≤t , x &lt;t ); red lines (-) show the recurrence process where h t is informed by h t-1 , which is informed by z t-1 and x t-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Invariant Representation from Causal Model</head><p>In this work, we propose a causal view of data generating process under vehicle interaction settings. More specifically, we utilize a structural causal model (SCM) based on domain knowledge to describe how interactive trajectories are generated by any vehicle pair (see Figure <ref type="figure" target="#fig_2">1</ref>). The constructed causal structure provides us with causal relations between input features and the outcome, which is used to learn the intention predictor. The detailed meaning of each node variable is as follow: Domain (D) contains different combinations of map properties such as road topology, speed limit, and traffic rules; Event (E) denotes an observable variable relate to the event of two-vehicle interaction, which includes information such as initial interaction states and the length of interaction; Driver (O) denotes an unobservable variable relate to each driver's driving preferences or style, such as the aggressiveness level and whether he/she obeys traffic rules; Causal features (X C ) represent high-level time-related causal features derived from drivers' personal behavior O and their interaction status E, which are used by humans to label the intention; Non-causal features (X N C ) represent domain dependent features which contain not only domainspecific information D, but also event E. For example, interaction event information such as when two vehicles start noticing each other or when the road negotiation begins, is related to the road geometry and traffic rule; Input Data (X) is vehicle interactive trajectories that contain sequential multivariate data, and is constructed from a mixture of causal X C and non-causal X N C features; Latent variable (Z) denotes time-related latent representations extracted from time-series vehicle interaction data; Label (Y ) is the vehicle intention label.</p><p>Invariance Condition. According to Figure <ref type="figure" target="#fig_2">1</ref>, X C is the node that causes Y , and by d-separation <ref type="bibr">[22]</ref>, the intention label is independent of domain conditioned on X C , Y ⊥ ⊥ D|X C<ref type="foot" target="#foot_0">foot_0</ref> . In other word, if such a X C can be found, then the distribution of Y conditional on X C is invariant under transferring from the source domains to the target domains. Therefore, our intention prediction task is to learn y as h(x c ) where h : C → Y. However, since X C is unobserved, we need to learn y through observed trajectories X. Specifically, we utilize a representation function q : X → Z to map the input space to a latent space, and a hypothesis function φ : Z → C to map the latent space to X C . Together, h(φ(q(x))) leads to the desired intention predictor f : X → Y and the corresponding prediction loss can be written as:</p><formula xml:id="formula_2">L y = L α (h(φ(q(X))), Y ),<label>(1)</label></formula><p>where L α is the classification loss such as a binary or categorical cross-entropy. In addition, by d-separation, X C also needs to satisfy an invariance condition: X C ⊥ ⊥ D|{E, O}, which means X C does not change with different domains when both event and driver information remain the same. However, driver information is unobservable and in many dataset there may not be an exact match based on a same interaction event across domains. Alternatively, we assume that the distance over X C between same-class inputs from different domains is bounded, which provides an alternative invariance condition that is consistent with the conditional independencies of X C . Therefore, we would like to minimize the following objective along with the prediction loss:</p><formula xml:id="formula_3">L r = Ω(xj ,xm)=1;kj =km L β (φ(q(x (kj ) j )), φ(q(x (km) m ))),<label>(2)</label></formula><p>where L β is the distance metric such as 2 , and Ω : X × X → {0, 1} is a match function such that pairs having Ω(x j , x m ) = 1 have low difference in their causal features.</p><p>Contrastive Representation Learning. To optimize L r , we first need to learn a proper match function Ω used in Eq. ( <ref type="formula" target="#formula_3">2</ref>). Specifically, we optimize a contrastive representation learning loss that minimizes distance between same-class inputs from different domains. We regard positive matches as two inputs from the same class but different domains, and negative matches as pairs with different classes. Then the loss function for every positive match pair (j, m) in a sampled mini-batch B is defined as:</p><formula xml:id="formula_4">j,m con = -log exp(s cos (x j , x m )/τ ) exp(s cos (x j , x m )/τ ) + |B| i=1,yi =yj exp(s cos (x j , x i )/τ ) ,<label>(3)</label></formula><p>where s cos (x a , x b ) = φ(q(x a )) T φ(q(x b ))/ φ(q(x a )) φ(q(x b )) is the inner product of two 2normalized vectors, τ is a temperature scaling parameter, and |B| is the batch size. Inspired from <ref type="bibr" target="#b18">[19]</ref>, for this unsupervised contrastive learning process, we initialize Ω with a random match based on classes and keep updating Ω by minimizing the contrastive loss (3) until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Capturing Temporal Latent Dependencies</head><p>From our proposed causal model in Fig. <ref type="figure" target="#fig_2">1</ref>, we also observe that Z ⊥ ⊥ D|X, meaning the latent representation is independent of domain D conditioned on input data X. Therefore, by learning the representation function q, we are able to extract a domain-invariant latent variable that represents the input space. Since the input is time-series data, the learned latent variable is expect to capture temporal latent information in the data. To explicitly model the dependencies between latent random variable across time steps, we integrate Variational Recurrent Neural Networks (VRNN) <ref type="bibr" target="#b21">[23]</ref> into our CTSDG model. The VRNN contains a VAE at every time step and these VAEs are conditioned on previous auto-encoders via the hidden state variable h t-1 of an RNN. In general, the objective function become a timestep-wise variational lower bound:</p><formula xml:id="formula_5">L v = E q(z i ≤T |x i ≤T ) [ T t=1 (-KL(q(z i t |x i ≤t , z i ≤t )||p(z i t |x i &lt;t , z i &lt;t )) + log p(x i t |z i ≤t , x i ≤t ))],<label>(4)</label></formula><p>where q(z i t |x i ≤t , z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overall Algorithm</head><p>Combining the joint optimization problem of equation ( <ref type="formula" target="#formula_2">1</ref>), ( <ref type="formula" target="#formula_3">2</ref>), and (4) leads to our CTSDG model.</p><p>The complete objective function we aim to optimize is:</p><formula xml:id="formula_6">L = L y + γL v + λL r ,<label>(5)</label></formula><p>where γ and λ are hyperparameters that control training balance among three losses for better performance. In order to prevent the classification loss from interfering learning domain-invariant representations, we learn the match function Ω before optimizing the overall objective function <ref type="bibr" target="#b4">(5)</ref>. Also, since the causal feature X C is a function of latent variable Z which is extracted by the recurrent latent variable model, the objective function (4) needs to be jointly optimized while learning the matching function using Eq.( <ref type="formula" target="#formula_4">3</ref>). The pseudocode for our proposed CTSDG method is shown in Algorithm 1. We first initialize Ω as a random match function, where given each data point, we randomly select another data point with the same class from another domain. During each training epoch, we constantly update Ω based on the currently learned hypothesis function φ. Specifically, given any data point x, we find the nearest neighbour of x among other data points with the same class from another domain by calculating their distance using L β defined in Eq.( <ref type="formula" target="#formula_3">2</ref>). After certain number of training epoch is reached, we further optimize the overall objective function (5) until convergence. Note that when calculating L r in Eq.( <ref type="formula" target="#formula_3">2</ref>), we apply the updated match function Ω to select match pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The experiment is conducted on the INTERACTION dataset <ref type="bibr" target="#b29">[31]</ref>, where two different scenarios are utilized: USA_Roundabout_FT and CHN_Merging_ZS. In this work, a domain is defined as a driving region that contains a unique combination of road topology, traffic rules, and speed limit information. Three domains are extracted from the FT scenario and one domain is extracted from the ZS scenario (see Figure <ref type="figure" target="#fig_4">2</ref>). We compare our method with ten state-of-the-art methods, including seven traditional baseline methods for domain generalization tasks and three methods for behavior prediction tasks. To evaluate the domain generalization performance, we follow the typically used leave-one-domain-out protocol <ref type="bibr" target="#b30">[32]</ref>. Details related to data selection, implementation, and more in-depth evaluations on learned latent representations and temporal latent dependencies can be found in supplementary materials.</p><p>Evaluation on USA_Roundabout_FT. We first train all models on different combinations of source domains extracted in FT and report the test results on the remaining target domain in FT. From Table 1, we observe that our method achieves an average prediction accuracy of 93.91% under all three train-test domain pairs and increases the accuracy by 2.54% compared to the best-performed baseline model. ERM and GNN method slightly outperform CTSDG when FT-2 is the target domain, but their accuracy is 10.24% and 11.26% less than our method when tested on FT-3, respectively. As seen, our method has consistent prediction accuracy regardless of testing domains. The standard deviations of CTSDG are also lower than those of all other baseline methods, which further shows our model's efficacy as it converges to more stable local optima.</p><p>Evaluation on CHN_Merging_ZS. When we compare the domain in ZS with domains in FT, we notice larger difference in road topology as well as in speed limit. In other word, the joint distribution shift is more severe between domains from two different scenarios (i.e. ZS and FT) than domains within scenario FT, which makes domain generalization more challenging. Therefore, to evaluate the generalizability of each model under large domain shift, we select different domain combinations in FT for training and the single domain in ZS for testing. From Table <ref type="table" target="#tab_0">1</ref>, we observe that CTSDG consistently outperforms all baseline models across all source-target pairs with accuracy 1% ∼ 6% higher. The average prediction accuracy of our method increases by 5% compared to the bestperformed baseline model. Also, when the model is trained using all three domains in FT, where the source domains have more diversity, learning based methods that do not specifically consider domain generalization approaches (i.e. ERM, CVAE, GAN, and GNN) have reasonable performance. However, the performance of these methods degrade drastically when training data are limited and lack of diversity. According to the results, we show the diminishing performance of traditional machine learning model to out-of-training distribution regimes and our proposed method is able to mitigate such problem while consistently outperforms other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extension to Trajectory Prediction Tasks</head><p>In this paper, although we illustrate the performance of our proposed domain generalization method for vehicle intention prediction tasks, our CTSDG approach can also be directly or indirectly utilized for trajectory prediction tasks. The most straight forward way is to frame the trajectory prediction problem as classification over a diverse set of trajectories <ref type="bibr" target="#b31">[33]</ref>, which enables us to a) ensure a desired level of coverage of the state space, b) eliminate dynamically infeasible trajectories, and c) avoid the issue of mode collapse. Moreover, the accuracy of intention classification plays an important role in regression tasks. As stated and shown in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36]</ref>, goal/intent-conditioned trajectory forecasting can improve joint-agent and per-agent predictions, compared to unconditional forecast. Conditional forecasting can be also used for planning tasks, which effectiveness is demonstrated in <ref type="bibr" target="#b7">[8]</ref>. Therefore, having an accurate and domain generalizable intention/goal predictor is the prerequisite of developing state-of-the-art trajectory predictor and motion planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present a causal-based time-series domain generalization model (CTSDG) to address domain generalization problem for vehicle intention prediction tasks. We show that CTSDG has consistent improvements in terms of prediction accuracy compared to other state-of-the-art domain generalization and behavior prediction methods using real-world driving data. In the future, we expect to extend our work to address trajectory prediction problems and consider generalization of varying number of interacting vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head><p>A Related Works Domain generalization aims to generalize models to unseen domains without knowledge about the target distribution during training. Different methods have been proposed for learning generalizable and transferable representations. A commonly-used strategy is to extract task-specific but domaininvariant features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Ghifary et al. <ref type="bibr" target="#b11">[12]</ref> learn multi-task auto-encoders to extract invariant features which are robust to domain variations. Li et al. <ref type="bibr" target="#b12">[13]</ref> extend adversarial autoencoders by imposing maximum mean discrepancy measure to align multi-domain distributions. Li et al. <ref type="bibr" target="#b16">[17]</ref> consider the conditional distribution of label space over input space, and minimize discrepancy of a joint distribution. Recently, several works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> utilize causal structures to learn invariance factors, which have shown better model generalizability on unseen data, especially on data from different distributions than the train distribution.</p><p>Dataset augmentation is another strategy that focuses on creating more out-of-domain samples by adversarially perturbing samples <ref type="bibr" target="#b35">[37]</ref>, leveraging self-supervised signals <ref type="bibr" target="#b15">[16]</ref>, or utilizing convex combinations of input and output data <ref type="bibr" target="#b20">[21]</ref>. These data augmentation methods have shown effective for model generalization, but invalid samples could be generated and degrade the model performance if input data is vehicle trajectories that need to satisfy feasibility constraints. Meta-learning can also be applied in domain generalization, by creating meta-train and meta-test sets, and training a model using the meta-train set in such a way to improve the performance on the meta-test set <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. While our proposed method can achieve promising results using standard training schemes, combining meta-learning with our approach could be an interesting future direction.</p><p>Intention prediction in autonomous driving aims to predict high-level behaviors of road entities, which results are often used in downstream trajectory prediction modules. For example, works such as <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b33">[35]</ref> and <ref type="bibr" target="#b34">[36]</ref> demonstrate that by predicting goal states and assuming that agents navigate toward those goals by following some optimal or learned trajectories, the accuracy of prediction can be improved. Therefore, accurate intention prediction algorithm is essential for autonomous vehicles to smoothly navigate in dynamic environments. Zhang et al. <ref type="bibr" target="#b37">[39]</ref> propose to model high level semantics in the form of traffic patterns through a generative model. Dong et al. <ref type="bibr" target="#b38">[40]</ref> use a probabilistic graphical model to estimate the pass or yield probability under merging scenarios. Schulz et al. <ref type="bibr" target="#b39">[41]</ref> utilize the dynamic bayesian network method for routing prediction at intersections. Zyner et al. <ref type="bibr" target="#b40">[42]</ref> obtain a probability distribution over all possible exit branches for a vehicle driving in a roundabout using recurrent neural network. Works such as <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b0">[1]</ref>, use more complicated ML models to prediction vehicle's intention. Due to the complexity of the real-world and its everchanging dynamics, the deployed autonomous vehicles inevitably face novel situations and should be able to cope with them. However, it has been repeatedly demonstrated that the reliability of ML models degrades radically when they are exposed to novel settings (i.e., under a shift away from the distribution of observations seen during their training) due to their failure to generalize, leading to catastrophic outcomes <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, Filos et al. also demonstrate that several state-of-the-art behavior prediction models for autonomous driving fail to generalize under arbitrary domain shift. Therefore, in this paper, we aim to address the domain generalization problem for vehicle intention prediction task and provide an effective solution to enhance reliability of autonomous vehicles. We hope our work can inspire further research in the field of improving generalizability for prediction algorithms in autonomous driving industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Illustrate the problem context through a toy example</head><p>Consider two vehicles interact with each other under two different domains. For the ease of understanding, we simplify the problem by temporarily making the following assumptions for the two interaction cases in Figure <ref type="figure" target="#fig_6">3</ref>: (1) The same road priorities; (2) The same two drivers interact with each other; (3) The same initial states for both vehicles. However,there are two major differences between two cases: (1) Different topological road structure of two intersected roads; (2) Two drivers have different driving styles (i.e. we let the driver in red car be more aggressive than the driver in blue car), which information is actually unavailable when making predictions.  Based on previous assumptions and settings, for a given interaction period, a plausible simulated velocity profile of two vehicles is shown in the second row of Fig. <ref type="figure" target="#fig_6">3</ref> for both cases. The goal is to predict the intention of any selected vehicle, which can be the intent of one car passes or yields the other car. According to the plots, whether the red car yields or passes the blue car doesn't rely much on the domain information but depends more on two drivers' personal information (e.g. driving style) and their initial states. Specifically, the geometry of the roads could affect several factors such as when two vehicles will start noticing each other, when the road negotiation will begin, and when drivers will agree on who should go first. All these factors could then influence the velocity profiles of two vehicles but won't have much effects on the intention of two vehicles when the aforementioned assumptions hold. In other words, if two vehicles encounter each other twice under different domains, as long as the three assumptions are true, the aggressive driver will always prefer passing than yielding the less aggressive driver. The intention is determined by features that relate to two drivers' information and their internal relations (e.g. highlighted in cyan), instead of those that relate to domain itself (e.g. highlighted in orange). Hence, we construct a model for the data collection process that assumes each input data (i.e. vehicle trajectories) is constructed from a mix of causal and non-causal features. We consider domain as a main intervention that changes the non-causal features of an input data, and propose that an ideal intention predictor should depends only on the causal features.</p><p>Although the aforementioned assumptions will not hold for real-world driving data, we are still able to assume each data is constructed from both causal and non-causal features. However, it becomes unclear what are the causal and non-causal features. Therefore, in this work, we construct a structural causal model and propose to learn an invariant representation for domain generalization based on invariant conditions identified from the causal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Model Insights</head><p>According to <ref type="bibr" target="#b43">[45]</ref>, causality, with its focus on representing structural knowledge about the data generating process that allows interventions and changes, can contribute towards understanding and resolving some limitations of current machine learning methods. In fact, machine learning models that incorporate or learn structural knowledge of an environment have been shown to be more efficient and generalize better <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b47">49]</ref>. For human drivers, the way they interact with each other under different domains should also processes some invariant structural relations, since they can quickly adapt their driving skills to novel scenarios. Therefore, we construct a structural causal model (SCM) for vehicle intention prediction task to learn an invariant representation of input driving data for domain generalization.</p><p>The domain generalization setting assumes the existence of domain-invariant patterns in the inputs (e.g. semantic features), which can be extracted to learn a predictor that performs well across seen and unseen domains. Moreover, causal predictive models are shown to have great generalizability since their output depends on stable relationships between input features and the outcome instead of associations between them. Therefore, in this work, we construct a structural causal model inspired from data generation process and propose to learn an invariant representation from temporal latent dependencies of input data for domain generalization, where the learning scheme is based on an invariant condition identified from the causal model. To be more specific, we assume each input data is constructed from a mix of inherent (causal) and domain-dependent (non-causal) features. Consequently, the captured temporal latent dependencies from sequential data also contain both causal and non-causal features.</p><p>To generate a pair of interactive trajectories from two vehicles, data-generating process first samples two drivers' driving preferences (O) and a domain (D) that could be correlated to each other shown with dashed arrow in Fig. <ref type="figure" target="#fig_2">1</ref> (e.g. driver behavior may vary from country to country ). Then, an interaction event (E) is sampled, which could correlate to domain information (e.g., the geometry of the roads may affect when two vehicles can notice/observe each other). The domain information is correspond to non-causal features (X N C ) and the driver information are correspond to causal features (X C ). Part of the event information, such as initial speed or location of two vehicles, are correspond to causal features which will influence the intention label. Another part of the event information, such as when the road negotiation begins, are correspond to non-causal features. Altogether, the causal and non-causal features construct the interactive trajectories (X) from where the time-related latent information (Z) are captured.</p><p>We further utilize D-separation to analyze the formulated structural causal model as it can: (1)connects causal relations and probabilistic independence; (2)computes all and only those independence and conditional independence relations that hold for all values of the parameter for any directed graph. At the lowest level of the causal hierarchy -association -we have discovered DAGs and d-separation as a powerful tool to reason about conditional (in)dependencies between variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Related Theory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 D-separation</head><p>Here we provide some background concepts related to d-separation used in Section 2 for analyzing our causal graph.  </p><formula xml:id="formula_7">(X ⊥ ⊥ Y )|Z. X Z Y Y Z X X Z Y X Z Y (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Worse-case Generalization Error</head><p>As mentioned in Suppl.A, utilizing causal structures to learn invariance factors has shown better model generalizability on unseen data and we also demonstrate desired performances of our causalbased model in Section 2. Theoretically (i.e. Theorem 1), the worse-case generalization error for a causal model is less than or equal to that for the associational model (i.e. model that directly map from X into Y). Detailed proof can be found in Suppl.A.2 in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Theorem 1. Consider a causal model h min c,D : X c → Y and an associational model h min a,D : X → Y trained on a dataset D ∼ P (X, Y ) with loss L. Let (x, y) ∈ D and (x , y ) / ∈ S be two input instances such that they share the same true labelling function on the causal features, y ∈ P (Y |X c = x) and y ∈ P (Y |X c = x ). Then, the worst-case generalization error for the causal model on such x is less than or equal to that for the associational model.</p><formula xml:id="formula_8">max x∈D,x L x (h min c,D , y) -L x (h min c,D , y) ≤ max x∈D,x L x (h min a,D , y) -L x (h min a,D , y)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Justification of Minimizing Contrastive Loss</head><p>According to the invariance condition derived in Section 2.2, the idea function we want to optimize is f * = arg min h E[l(y, h(x c ))] assuming x c is known. However, as X C is unobservable, we need to alternatively minimize the objective in Eq.( <ref type="formula" target="#formula_6">5</ref>). Inspired from Theorem 2 in <ref type="bibr" target="#b18">[19]</ref>, we are able to derive the following theorem under our problem setting. We can observe that whether X C will be returned by the optimization largely depends on δ c . Specifically, if a dataset has low δ c , then the learned representation will likely to be closer to the real X C . Alternatively, if a dataset has high δ c , the matching condition loses any discriminative power and thus we need to additionally learn a matching function ω to minimize δ c . Therefore, minimizing the contractive loss (Eq.( <ref type="formula" target="#formula_4">3</ref>)) is necessary in our problem. Detailed proof or the original theorem can be found in Suppl.A.6 in <ref type="bibr" target="#b18">[19]</ref>. </p><p>ac,m ) ≤ δ ac where x ac is any non-causal feature. Further, assume that the distance over X C between same-class inputs from different domains is bounded: dist</p><p>c,m ) ≤ δ c and δ c &lt; δ ac (δ c , δ ac ∈ R + ). Then for some γ and λ, a loss-minimizing classifier for the loss from Eq.( <ref type="formula" target="#formula_6">5</ref>) is the true function f * , given a P-admissible loss function and a finite number of domains k with K → ∞ in each domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Data Information</head><p>All data are collected by a drone from bird's-eye view with 10 Hz sampling frequency. Information such as reference paths and traffic regulations can be directly extracted from high definition maps that are in lanelet <ref type="bibr" target="#b48">[50]</ref> format. The detailed information of each domain can be found in Table <ref type="table" target="#tab_2">2</ref>, where we observe that any two domains are different in at least two out of five aspects. The intersect angle refers to the angle between two reference paths<ref type="foot" target="#foot_1">foot_1</ref> . The average length refers to the average length of two reference paths.</p><p>Instead of Cartesian coordinate, we utilized the Frenét Frame to represent vehicle's state. The vehicle motion in the Frenét Frame can be represented with the longitudinal position along its reference path s(t), and lateral deviation to the reference path d(t). Therefore the vehicle state at time step t can be defined as ξ t = (s(t), d(t)). Note that the reference path of a vehicle will change according to the road it is current driving on. The origin of the reference path can be defined differently according to different objectives and each reference path will have its own Frenét Frame. Since we are dealing with interaction between two vehicles, we define the origin as the intersection point (red circles in Fig. <ref type="figure" target="#fig_4">2</ref>) of their reference paths.</p><p>We manually select the interaction data from the dataset. Since we focus on two-vehicle interaction situations, we need to make sure the two vehicles are highly interactive with each other and both vehicles' behaviors should not be influenced by any other vehicles. Also, for ease of interpretation, we balance the data such that there are equal number of interactive trajectory data per class in each domain. We select a total number of 1354 time-series data from three domains in USA_Roundabout_FT and 138 time-series data from the single domain in CHN_Merging_ZS.</p><p>In terms of the labeling process, since we have the entire interaction trajectory of two vehicles, we are able to obtain the final outcome directly based on which vehicle passes the conflict point first. We categorize the intention for two interacting vehicles as either pass or yield and consider only the period that two vehicles have high interaction. In order to make sure that selected vehicle pairs are highly interactive and their behaviors are not influenced by other surrounding vehicles, we manually examine and filter the data. The are two reasons why we filter out the data that two vehicle are far from each other. Firstly, it is difficult to obtain true labels of drivers' intentions in such situation and their intentions may not simply be pass/yield. Secondly, if two vehicle are far from each other, there will be a large chance that they either haven't started interacting with each other or may interacting with other vehicles. It is worth to mention that our proposed approach can be adapted to multi-class classification with more subtle interaction patterns, but it might require human labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Implementation</head><p>For domain generalization tasks, test/target domains are not accessible during training. Therefore, we randomly sample 20% of data from source domains to construct our validation set which is used for hyperparameter tuning and use the remaining source domain data for training. We use the Adam optimizer and run all models for 500 epochs with learning rate of 1e-3. During training, we use a mini batch of 16 samples and apply early stopping criteria that the model does not experience a decrease in the validation loss for 20 epochs. The temperature scaling parameter τ is set to 0.05, and the two control parameters γ and λ are set to 1e -4 and 1, respectively. For results of each model, we report the average and standard deviation over 3 independent runs.</p><p>Input dimension of each data is 10 × 4, where we consider the past 1s of interactive trajectories with 10Hz sampling frequency and concatenate the states of two vehicles. Output classifer dimension is 2 corresponding to the possible number of intentions. Network modules in our model comprises: an encoder network ϕ enc τ , which output parameters define the inference model q(z i t |x i &lt;t , z i &lt;t ) (this is also the representation function q(x)); a prior network ϕ prior τ , which output parameters define the prior distribution p(z i t |x i &lt;t , z i &lt;t ); a decoder network ϕ dec τ , which output parameters define the generative model p(x i t |z i ≤t , x i ≤t ); feature extraction networks ϕ x τ and ϕ z τ for x and z, respectively; a recurrent process layer for hidden state update; a hypothesis function φ(z); and a function h(x c ) that maps causal features to the output intention. Specifically, all ϕ τ consist of two hidden layers of 16 neurons with ReLU activation, where the input dimension is 4 and output dimension is 16. The hidden size of the recurrent layer is set to 16, where the input dimension is 32 and output dimension is 16. We consider φ to be the last layer of the network with one hidden layer of 16 neurons and ReLU activation, where the input dimension is 16 and output dimension is 2. We take h to be identity function for simplicity. The dimension of latent representation z is set to 2. When implementing state-of-the-art methods, since most of the baseline models are not designed for time-series input data, we add a RNN layer in front of each model instead of simply flattening the time-series input. The total number of parameters of all the baselines and our method are set to about equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Ablation Study</head><p>We conduct an extensive study using both FT and ZS data to investigate contribution of three components to our method's performance: loss for extracting temporal latent dependencies, contrastive loss for learning matching function, and metrics to evaluate distance between two representations. To be more specific, CTSDG w/o L v is the CTSDG model without adding the loss term L v , where no temporal latent dependencies are modeled while learning invariant representations from the input data; CTSDG w/o con is the CTSDG model without learning the matching function Ω, where the loss L r for satisfying invariance condition of X C is optimized by randomly selected input pairs; CTSDG w/ s 1 and CTSDG w/ s 2 are CTSDG models but using 1 and 2 as distance metric between two representations, respectively. From the ablation study results in Table <ref type="table" target="#tab_3">3</ref>, we observe that by capturing the temporal latent dependencies and learning matching function for input pairs that share causal features, the model is able to have better performance especially when target domain has large domain shift compared to source domains (the fourth row). Moreover, according to the results from using 1 and 2 as distance metric, the cosine similarity used in our method seems to be the most suitable measure in the causal feature space for this prediction problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Analysis on Invariant Representation</head><p>We utilize t-SNE <ref type="bibr" target="#b49">[51]</ref> to analyze the quality of latent representation learned by our proposed model in Figure <ref type="figure" target="#fig_10">5</ref>. We train our model using two domains (FT-1,2) and evaluate on two unseen domains (FT-3 &amp; ZS). It appears from Fig. <ref type="figure" target="#fig_10">5</ref>(a) and (b) that CTSDG achieved a higher overlap between train and test domains for FT-3 than ZS, highlighting the difficulty of generalizing to the ZS domain. Such finding coincides with our prior domain knowledge as well as quantitative results in Table <ref type="table" target="#tab_0">1</ref>. Moreover, based on Fig. <ref type="figure" target="#fig_10">5(d</ref>) and (e), we find that the classes are well-separated in the training domains for both models. We also observe that when the intention label is yield, the data is more evenly dispersed. The reason could because the input time-series data with yield label are easier for the model to capture underlying domain representations that are invariant across time. Whereas the time-series data with pass label could have more temporally diversed domain-invariant representations.</p><p>In order to illustrate the effectiveness of learning match function Ω, we further utilize t-SNE to compare the latent representation learned by CTSDG with that by CTSDG without the contrastive loss in Eq. 3. Both models are trained using two domains (FT-1,2) and evaluated on an unseen domain FT-3. According to Fig. <ref type="figure" target="#fig_10">5</ref> (a) and (c), the model with learned match function has higher overlap between train and test domains than the model with random match function, especially when ground-truth target intentions are yield. We can also observe from Fig. <ref type="figure" target="#fig_10">5 (d</ref>) and (f) that without minimizing contrastive loss (Eq. 3), target data from different classes cannot be well-separated in the latent representation, which generates miss-classification cases. Therefore, we can conclude that by enforcing the difference in individual representations for same-class inputs to be low through (Eq. 3), the model can better capture domain-invariant representations.  Figure <ref type="figure" target="#fig_12">6</ref> shows the temporal latent dependencies learned by our CTSDG for a given source-target pair. We first look at the two cases from source domain (i.e. first column in Fig. <ref type="figure" target="#fig_12">6</ref>(a,b)). Looking at the neuron activation patterns along the time axis, we can see that neurons are more active when there are changes in either position or velocity profiles for two interacting vehicles. In fact, these changes are related to causal features which play important roles in determining the output intentions based on our proposed causal model. As we look at the two cases from target domain (i.e. second column in Fig. <ref type="figure" target="#fig_12">6</ref>(a,b)), we observe consistent excitatory neuron firing patterns at critical time steps, which indicate that CTSDG successfully transfers knowledge to unseen domains. Therefore, we can conclude that our CTSDG method is able to capture temporal latent dependencies that are domain invariant and related to causal features, which enables it generalize time-series driving data to unseen domains.  According to Table <ref type="table" target="#tab_0">1</ref>, IRM has the best performance among all baseline methods when the source domain is FT-1,2 and target domain is FT-3. Therefore we compare the temporal latent dependencies learned by IRM with those learned by our method (see Figure <ref type="figure" target="#fig_13">7</ref>). We observe that IRM results in sporadic activation pattern and fails to capture and transfer temporal latent dependencies. In contrast, our method can extract critical temporal dependencies that are relevant towards two vehicles' intention and such dependencies can be further transferred from source to target domain. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>factors that cause domain shift are ambiguous for our task, in contrast to clear domain shift variables for other DG tasks (e.g. image Workshop on Distribution Shifts, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:2112.02093v1 [cs.LG] 3 Dec 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the overall CTSDG framework. Observed quantities are shown as shaded nodes; nodes of latent quantities are transparent. A directed black edge (-) denotes a causal relationship; dashed bidirectional edges () denote correlation; hollow dashed arrows ( ) denote an inclusion relationship. Blue lines (-) denote the inference process, q(z t |x ≤t , z &lt;t ); green lines (-) represent the generation process, p(x t |z ≤t , x &lt;t ); red lines (-) show the recurrence process where h t is informed by h t-1 , which is informed by z t-1 and x t-1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 5 if epoch &lt; threshold then 6 Minimize</head><label>156</label><figDesc>Causal-based time-series domain generalization (CTSDG) Input :Source training domain D = {D k } K k=1 ; hyperparameters τ, γ, λ &gt; 0 Output :Intention Predictor f : X → Y 1 Randomly split source domains D into disjoint train D tr and validation D val ; 2 Initialize Ω as a random match function; 3 while not converged do 4 Sample a mini-batch from D tr ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of selected domains for driving scenarios. Three domains selected from USA_Roundabout_FT are denoted as FT-1, FT-2, and FT-3. The one domain selected from CHN_Merging_ZS is denoted as ZS. Each black arrow line represents a reference path and red circles represent intersect points of reference paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of our design motivation through a toy example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Four DAGs for three linked variables. The first two (i.e. (a) and (b)) are called chains; (c) is a fork; (d) is a collider. If these were the whole of the graph, we would have X ⊥ ⊥ Y and X ⊥ ⊥ Y |Z. For the collider, however, we would have X ⊥ ⊥ Y while X ⊥ ⊥ Y |Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 2 .</head><label>2</label><figDesc>Assume training domains such that for any two same-class inputs x (k) j and x (k ) j from domains k and k , dist</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The t-SNE plots for visualizing latent representations. The source domains come from FT-1 and FT-2. The target domain for (a)(c)(d)(f) and (b)(e) is from FT-3 and ZS, respectively. For better visualization, we color (a)-(c) based on domain information: blue dots for FT-1, green dots for FT-2, and red dots for corresponding target domain. In (d)-(f), we color data points by ground-truth vehicle intention labels, where yellow dots represent pass and magenta dots represent yield.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learned temporal latent dependencies across time for different output intentions. The source domains are FT-1,2 and the target domain is FT-3. Each column shows a single sample from either source or target domain. The top row represents the cell state of memory cell for CTSDG, where y-axis refers to the activation of each neuron and x-axis refers to activation per time-step. The second row plots two vehicles' longitudinal coordinates under the Frenét Frame with respect to time. The third row plots the velocity profiles for the two interacting vehicles.</figDesc><graphic coords="15,423.01,232.07,60.46,90.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Learned temporal latent dependencies across time for different output intentions using different methods. The source domains are FT-1,2 and the target domain is FT-3. Each column under each method shows a single sample from either source or target domain. The top row under each method represents the cell state of memory cell, where y-axis refers to the activation of each neuron and x-axis refers to activation per time-step. The second row under each method plots two vehicles' longitudinal coordinates under the Frenét Frame with respect to time. The third row under each method plots the velocity profiles for the two interacting vehicles.</figDesc><graphic coords="16,238.38,357.05,60.46,90.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Domain generalization results with prediction accuracy (%).</figDesc><table><row><cell>i ≤t ) is the inference model, p(z i t |x i &lt;t , z i &lt;t )) is the prior, p(x i t |z i ≤t , x i ≤t ) is the</cell></row><row><cell>generative model, and KL(•||•) refers to Kullback-Leibler divergence.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Detailed information of each domain.</figDesc><table><row><cell></cell><cell cols="2">Road Topology</cell><cell>Speed Limit</cell><cell>Traffic Rule</cell><cell>Location</cell></row><row><cell></cell><cell cols="2">intersect angle average length</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FT-1</cell><cell>≈ 45 •</cell><cell>medium</cell><cell>25 mph</cell><cell>yield sign</cell><cell>USA</cell></row><row><cell>FT-2</cell><cell>≈ 90 •</cell><cell>medium</cell><cell>25 mph</cell><cell>stop sign</cell><cell>USA</cell></row><row><cell>FT-3</cell><cell>&gt; 90 •</cell><cell>short</cell><cell>25 mph</cell><cell>stop sign</cell><cell>USA</cell></row><row><cell>ZS</cell><cell>≈ 10 •</cell><cell>long</cell><cell>50 mph</cell><cell>zipper merging</cell><cell>China</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our method with prediction accuracy (%).</figDesc><table><row><cell>Source</cell><cell cols="5">Target CTSDG w/o L v CTSDG w/o con CTSDG w/ s 1 CTSDG w/ s 2</cell><cell>Ours</cell></row><row><cell>FT-1,2</cell><cell>FT-3</cell><cell>75.79 (0.64)</cell><cell>83.24 (3.87)</cell><cell>81.75 (5.73)</cell><cell>79.7 (3.18)</cell><cell>86.03 (1.48)</cell></row><row><cell>FT-1,3</cell><cell>FT-2</cell><cell>95.83 (0.39)</cell><cell>95.15 (0.26)</cell><cell>95.91 (1.28)</cell><cell>96.08 (0.82)</cell><cell>96.85 (0.15)</cell></row><row><cell>FT-2,3</cell><cell>FT-1</cell><cell>97.49 (0.63)</cell><cell>97.45 (1.26)</cell><cell>96.2 (1.90)</cell><cell>95.19 (1.49)</cell><cell>98.85 (0.13)</cell></row><row><cell>FT-1,2,3</cell><cell>ZS</cell><cell>77.29 (1.82)</cell><cell>78.74 (3.02)</cell><cell>80.67 (5.54)</cell><cell>83.27 (4.74)</cell><cell>85.51 (2.66)</cell></row><row><cell cols="2">Average</cell><cell>86.60</cell><cell>88.65</cell><cell>88.69</cell><cell>88.56</cell><cell>91.81</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The notation Y ⊥ ⊥ D|XC stands for the conditional independence relationship P (Y = y, D = k|XC = xc) = P (Y = y|XC = xc)P (D = k|XC = xc).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A traffic-free reference path is normally obtained from road's centerline.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><surname>Desire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic prediction of vehicle semantic intention and motion</title>
		<author>
			<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="307" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scenario-transferable semantic graph reasoning for interaction-aware probabilistic prediction</title>
		<author>
			<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03053</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-modal probabilistic prediction of interactive behavior via an interpretable model</title>
		<author>
			<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="557" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generic prediction architecture considering both rational and irrational driving behaviors</title>
		<author>
			<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3539" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A framework for probabilistic generic traffic scene prediction</title>
		<author>
			<persName><forename type="first">Yeping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2790" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trafficpredict: Trajectory prediction for heterogeneous traffic-agents</title>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6120" to="6127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can autonomous vehicles identify, recover from, and adapt to distribution shifts?</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tigkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="998" to="1008" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="624" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning causal semantic representation for out-of-distribution prediction</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01681</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Domain generalization using causal matching</title>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07500</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Alleviating privacy attacks via causal learning</title>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9537" to="9547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02216</idno>
		<title level="m">A recurrent latent variable model for sequential data</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole d</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Koltchinskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eté de Probabilités de Saint-Flour</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational recurrent adversarial deep domain adaptation</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanachat</forename><surname>Nilanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2688" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liting</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aubrey</forename><surname>Clausse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kummerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Konigshof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03088</idno>
		<title level="m">Arnaud de La Fortelle, et al. Interaction dataset: An international, adversarial and cooperative motion dataset in interactive driving scenarios with semantic maps</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Corina Grigore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-modal trajectory prediction of surrounding vehicles with maneuver based lstms</title>
		<author>
			<persName><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Planning-based prediction for pedestrians</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garratt</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3931" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Time-tolane-change prediction with deep learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hien Q Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Biedermann</surname></persName>
		</author>
		<author>
			<persName><surname>Hoepfl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12018</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding high-level semantics by modeling traffic patterns</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3056" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Intention estimation for ramp merging control in autonomous driving</title>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bakhtiar</forename><surname>Litkouhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1584" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiple model unscented kalman filtering in dynamic bayesian networks for intention estimation and trajectory prediction</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Hubmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Löchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darius</forename><surname>Burschka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1467" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Naturalistic driver intention and path prediction using recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stewart</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Nebot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Machine learning in non-stationary environments: Introduction to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00222</idno>
		<title level="m">Interaction networks for learning about objects, relations and physics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structured agents for physical construction</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hamrick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="464" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01427</idno>
		<title level="m">A simple neural network module for relational reasoning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lanelet2: A high-definition map framework for the future of automated driving</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Poggenhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Hendrik</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Janosovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Orf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Kuhnt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1672" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
