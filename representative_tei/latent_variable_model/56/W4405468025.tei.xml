<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">G ROO T-2: Weakly Supervised Multi-Modal Instruction Following Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-07">7 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bowei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
							<email>ma@ucla.edu&gt;</email>
						</author>
						<author>
							<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><surname>Pku</surname></persName>
						</author>
						<author>
							<persName><surname>Ucla</surname></persName>
						</author>
						<author>
							<persName><surname>Bigai</surname></persName>
						</author>
						<title level="a" type="main">G ROO T-2: Weakly Supervised Multi-Modal Instruction Following Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-07">7 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.10410v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce G ROO T-2, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. GROOT-2's effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.</p><p>Figure <ref type="figure">1</ref> | By feeding a mixture of demonstrations and some multimodal labels, we learn GROOT-2, a humanaligned agent capable of understanding multimodal instructions and adaptable to various environments, ranging from video games to robot manipulation, including Atari, Minecraft, Language Table, and Simpler Env.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Developing policies that can follow multimodal instructions to solve open-ended tasks in open-world environments is a long-standing challenge in robotics and AI research. With the advancement of large-scale pretraining <ref type="bibr" target="#b8">(Baker et al., 2022;</ref><ref type="bibr" target="#b10">Brohan et al., 2022;</ref><ref type="bibr" target="#b12">Brown et al., 2020)</ref>, the research paradigm for instruction-following policies has shifted from reinforcement learning to supervised learning. In a supervised learning approach, researchers collect large amounts of demonstration data and annotate each demonstration with multimodal instructions-such as videos <ref type="bibr" target="#b20">(Duan et al., 2017;</ref><ref type="bibr" target="#b27">Jang et al., 2022)</ref>, texts <ref type="bibr" target="#b39">(Lynch et al., 2023;</ref><ref type="bibr" target="#b45">Padalkar et al., 2023)</ref>, and episode returns <ref type="bibr" target="#b15">(Chen et al., 2021)</ref>-using hindsight relabeling. In theory, the instruction-following capability of such policies , where ğ‘… = 0 corresponds to "mechanical imitation" and ğ‘… = 1 to "posterior collapse." At low ğ‘…, latent vector ğ‘§ directly outputs action sequences without considering observations ( ğµğ¶ â†’ 0 ). As ğ‘… increases, ğ‘§ represents high-level task information, such as specific object interactions. At ğ‘… = 1, ğ‘§ provides no beneficial information for decision-making.</p><p>improves as the dataset grows. However, annotating demonstrations with high-quality multimodal labels is prohibitively expensive, making it challenging to scale these methods in practice.</p><p>Another line of work <ref type="bibr" target="#b3">(Ajay et al., 2020;</ref><ref type="bibr">Cai et al., 2023b;</ref><ref type="bibr">Lynch et al., 2020c)</ref> avoids the need for additional human annotations by learning from demonstration-only data in a self-supervised manner. These approaches leverage latent variable generative models <ref type="bibr" target="#b29">(Kingma &amp; Welling, 2013)</ref> to jointly learn an encoder and a latent-conditioned policy. The resulting policy is capable of completing multiple tasks specified by a reference video <ref type="bibr">(Cai et al., 2023b)</ref>. While a reference video is generally expressive enough to represent various tasks, the inherent ambiguity in videos can lead to a learned latent space that is misaligned with human intention. For example, the encoder module may capture the dynamics between adjacent frames in a video, thereby learning a latent representation of the action sequence-a process we refer to as "mechanical imitation." While this latent space accurately reconstructs the target action sequence, the resulting latent representation is difficult for human users to leverage during policy deployment. Another potential issue is "posterior collapse," where the latent space collapses to a single point and loses its influence over the policy during inference. We attribute this mismatch between training and inference to the absence of direct supervision for aligning the latent space with human intention. As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, an ideal controllable latent-induced policy space must strike a balance between these two extremes.</p><p>We present GROOT-2 (refer to Figure <ref type="figure">1</ref>), a multimodal instructable agent developed using a latent variable model under weak supervision. To unify the training pipeline, we encode instructions from all modalities as distributions over the latent space. The training objectives consist of two key components:</p><p>(1) constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the latent-conditioned policy to learn diverse behaviors; and (2) human intention alignment, which uses relatively small sets of multimodal labels to align the latent space with human intentions. Specifically, we apply the maximum log-likelihood method in the latent space for alignment. The underlying principle is that the latent embedding encoded by multimodal labels should also be sampled from the distribution learned from the corresponding video. Our approach is both general and flexible, as demonstrated through evaluations across four diverse environments-ranging from video games to robotic manipulation-including Atari Games <ref type="bibr" target="#b9">(Bellemare et al., 2013)</ref>, Minecraft <ref type="bibr" target="#b28">(Johnson et al., 2016)</ref>, Language Table <ref type="bibr" target="#b39">Lynch et al. (2023)</ref>, and Simpler Env <ref type="bibr" target="#b31">(Li et al., 2024)</ref>. These experiments highlight GROOT-2 's robust ability to follow multimodal instructions, with extensive tests showing that scaling up either unlabeled or labeled demonstrations further enhances performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Latent Variable Models Enable Controllable Behavior Generation</head><p>In recent years, the GPT series <ref type="bibr" target="#b12">(Brown et al., 2020;</ref><ref type="bibr" target="#b50">Radford, 2018;</ref><ref type="bibr" target="#b51">Radford et al., 2019)</ref> has demonstrated impressive capabilities in controllable text generation. Its success can be attributed to self-supervised pretraining and the advantageous properties of natural language. A natural language paragraph contains rich dependencies between sentences. For instance, the title of an article sets the central theme for its body, and the response in a question-answer or dialogue is highly correlated with the preceding text. This characteristic enables large language models, trained via next-token prediction, to achieve controllable text generation through prompts during inference. Unfortunately, such strong correlations do not exist between low-level actions. A desired behavior may not have a necessary preceding trajectory segment. Thus, it isn't easy to prompt a pre-trained policy model to generate a desired behavior. Instead, the generation of actions depends on an underlying latent intention variable. A natural approach is to employ latent variable generative models to jointly model trajectory data and the latent variables that drive them, allowing for controllable behavior generation by manipulating the latent variables during inference. Next, we will elaborate on how latent variable models model trajectory data.</p><p>As a classic latent variable generative model, Variational Autoencoder (VAE, Kingma &amp; Welling ( <ref type="formula">2013</ref>)) has been widely used in fields such as image generation and text generation. With the development of the offline pretraining paradigm, recent years have seen an increasing number of works utilizing VAE to model trajectory data. Typically, its architectures consist of three components: a posterior encoder, a prior encoder, and a policy decoder. The posterior encoder, ğ‘(ğ‘§|ğœ), encodes a specific behavioral trajectory ğœ = (o 1:ğ‘ , a 1:ğ‘ ) and generates a posterior distribution over the latent space. When the action sequence can be accurately inferred from the observation sequence <ref type="bibr" target="#b8">(Baker et al., 2022;</ref><ref type="bibr" target="#b61">Zhang et al., 2022)</ref>-i.e., when the inverse dynamics model of the environment ğ‘ IDM (a 1:ğ‘ |o 1:ğ‘ ) is easily learned-the action sequence can be excluded from the posterior's input <ref type="bibr">(Cai et al., 2023b)</ref>, thus reducing the distribution condition to o 1:ğ‘ . The prior encoder, ğ‘(ğ‘§|o 1:ğ‘˜ ), generates a distribution over the latent space based on the history of observations, where ğ‘˜ denotes the length of the observation window. When ğ‘˜ = 0, the prior distribution is independent of historical observations and is typically assumed to follow a standard normal distribution N (0; 1). The decoder, ğœ‹(a ğ‘¡ |o 1:ğ‘¡ , ğ‘§), is generally a latent-conditioned policy that takes in the environment's observations along with a specific latent variable to predict the next action to be executed. According to variational inference theory, we can optimize the VAE's modeling capabilities by maximizing the Evidence Lower Bound (ELBO)</p><formula xml:id="formula_0">L ELBO = ğ”¼ ğ‘§âˆ¼ğ‘(ğ‘§ |o 1:ğ‘ ) ğ‘ âˆ‘ï¸ ğ‘¡=ğ‘˜ -log ğœ‹(a ğ‘¡ |o 1:ğ‘¡ , ğ‘§) + ğ· KL (ğ‘(ğ‘§|o 1:ğ‘ ) âˆ¥ ğ‘(ğ‘§|o â‰¤ğ‘˜ )).</formula><p>(1)</p><p>There are generally three main objectives for using VAE to model trajectory data: (1) Modeling multimodal behaviors <ref type="bibr">(Lynch et al., 2020a;</ref><ref type="bibr" target="#b41">Mees et al., 2022)</ref>: For instance, when trajectory data is collected from different individuals, the variations in action sequences across different behavior modes can be substantial. Directly applying a naive behavior cloning algorithm may result in poor modeling performance. Introducing an additional latent variable to differentiate between behavior modes can help mitigate conflicts between them during training. (2) Skill discovery <ref type="bibr" target="#b22">(Gupta et al., 2019;</ref><ref type="bibr" target="#b59">Xu et al., 2023)</ref>: Complex trajectory data is often composed of various skills. A VAE can abstract action sequences in a self-supervised manner, enabling skill reuse in downstream tasks, such as accelerating the exploration process in reinforcement learning <ref type="bibr" target="#b3">(Ajay et al., 2020;</ref><ref type="bibr" target="#b48">Pertsch et al., 2021)</ref>. (3) Following reference videos to complete open-ended tasks (also known as one-shot demonstration learning, <ref type="bibr">Cai et al. (2023b)</ref>): This approach aims to leverage the learned posterior encoder to recognize the underlying intention behind a reference video and encode it as a latent, which can then drive a policy to complete the specified task in a novel deployment. It points to a way to pre-train instruction-following policies using unlabeled trajectory data. We primarily focus on the third objective in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Modeling Behaviors with VAE Leads to Ambiguous Latent Space</head><p>Several studies on VAE <ref type="bibr" target="#b0">(Abeer et al., 2024;</ref><ref type="bibr" target="#b5">Alemi et al., 2018)</ref> have pointed out that the Pareto frontier of the ELBO contains an infinite number of solutions for the latent space, a phenomenon we refer to as latent space ambiguity. To facilitate understanding, we provide an informal illustration in Figure <ref type="figure" target="#fig_0">2</ref>, which shows several possible latent spaces when a VAE is used to model behaviors, all having similar ELBO values. We differentiate these latent spaces using the ratio ğ‘… = ğµğ¶ ğµğ¶+ğ¾ğ¿ , where ğ‘… = 0 and ğ‘… = 1 represent two extremes of the latent space. When ğ‘… approaches 0, the latent condition contains much information, nearly dictating every action of the policy's behavior. We refer to this as mechanical imitation, where the VAE effectively degenerates into an Autoencoder (AE). Conversely, when ğ‘… approaches 1, the latent loses its ability to control the policy's output, a phenomenon known as posterior collapse <ref type="bibr" target="#b21">(Fang et al., 2019;</ref><ref type="bibr" target="#b46">Pagnoni et al., 2018)</ref>, in which the VAE reduces to an Auto-regressive (AR) model. Intuitively, as ğ‘… increases, the information encoded in the latent space becomes more high-level, and the policy relies more on environmental feedback (observations) to make decisions that align with the dataset's distribution. On the other hand, when ğ‘… is smaller, the policy tends to down-weight the environment's observations. Not all latent spaces effectively support following a reference video. As shown in Figure <ref type="figure" target="#fig_1">3</ref>, the gap between the environment state in the reference video and during policy deployment requires the posterior encoder to extract intentions independent of environmental dynamics. For instance, in the Minecraft task "mining a diamond underground," a reference video may show a player walking forward and mining a diamond. If the latent encodes only the trajectory sketch, the policy might fail by colliding with obstacles in the deployment environment. This mismatch occurs because humans interpret the video as "mining the diamond" rather than copying specific actions. Aligning the latent space with human intentions is critical for improving policy steerability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Aligning Policy Learners with Weak Supervision</head><p>We explore the development of instructable agents based on latent variable models. To avoid "latent space ambiguity", we introduce human intention knowledge into the generative pretraining process of the policy model to assist in shaping the latent space. As multimodal labels associated with demonstrations carry rich human intention details, we propose a weakly supervised policy learning algorithm to leverage large amounts of unlabeled demonstration data to learn the latent space while using a small amount of multimodal labeled data to align the latent space with human intention. Ultimately, this enables instructions from all modalities to be unified within the same latent space. Next, we will elaborate on the dataset collection, training pipeline, and inference procedure.</p><p>Dataset Collection. We can collect two types of training data from the web: a large set of unlabeled demonstrations D dem = {(o 1:ğ‘ , a 1:ğ‘ )} and a relatively small set of annotated demonstrations D lab = {(o 1:ğ‘ , a 1:ğ‘ , w 1:ğ‘€ )}, where o is the image observation provided by the environment, a is the action taken by the policy, w is the word token, ğ‘ is the length of a demonstration, ğ‘€ is the length of an annotation sentence. The annotation sentence can be multimodal, such as a language sentence (with ğ‘€ â‰¥ 1) or a scaler of the episode return (with ğ‘€ = 1), which explains the behavior or outcome of the demonstration from a human's perspective. Since the annotation data is expensive to collect, we have</p><formula xml:id="formula_1">|D lab | â‰ª |D dem |.</formula><p>Training Pipeline. Our goal is to learn a shared latent space Z, per-modal instruction encoders ğ‘’(ğ‘§|ğ‘), and a latent-conditioned policy ğœ‹(a ğ‘¡ |o â‰¤ğ‘¡ , ğ‘§). Leveraging past observations is essential for a policy to make decisions in a partially observable environment such as Minecraft <ref type="bibr" target="#b28">(Johnson et al., 2016)</ref>. We call the learned policy model GROOT-2, whose training pipeline is shown in Figure <ref type="figure" target="#fig_2">4</ref>. For an unlabeled demonstration (o 1:ğ‘ , a 1:ğ‘ ), we use the encoder module to produce a prior distribution ğ‘’(ğ‘§|o 1 ) and a posterior distribution ğ‘’(ğ‘§|o 1:ğ‘ ). Using the reparameterization trick <ref type="bibr" target="#b29">(Kingma &amp; Welling, 2013)</ref>, we sample the latent ğ‘§ from the posterior distribution ğ‘’(ğ‘§|o 1:ğ‘ ) and train the policy model, conditioned on ğ‘§ and o 1:ğ‘¡ , to reconstruct the entire action sequence causally. To limit the information presented in the latent space, we introduce an auxiliary KL divergence term in the objective:</p><formula xml:id="formula_2">L dem (o, a) = ğ”¼ ğ‘§âˆ¼ğ‘’(ğ‘§ |o 1:ğ‘ ) ğ‘ âˆ‘ï¸ ğ‘¡=1 -log ğœ‹(a ğ‘¡ |o 1:ğ‘¡ , ğ‘§) + ğ›½ 1 ğ· KL (ğ‘’(ğ‘§|o 1:ğ‘ ) âˆ¥ ğ‘’(ğ‘§|o 1 )).</formula><p>(2)</p><p>This allows the model to leverage demonstration-only data to enhance the complexity of the latent space, a process we refer to as "constrained self-imitating." For a labeled demonstration  (o 1:ğ‘ , a 1:ğ‘ , w 1:ğ‘€ ), we pass the label w 1:ğ‘€ through the encoder module to obtain a latent distribution and train the policy model to reconstruct the action sequence based on the latent ğ‘§ sampled from this distribution ğ‘’(ğ‘§|w 1:ğ‘€ ). This allows human knowledge to be modeled in the latent space. Further, to make the encoder understand demonstration o 1:ğ‘ just like humans, we introduce an auxiliary MLE term: maximize the log-likelihood of ğ‘’(ğ‘§|o 1:ğ‘ ) given the latent ğ‘§ sampled from ğ‘’(ğ‘§|w 1:ğ‘€ ).</p><p>Unlike the prior behavior cloning term, the aligning term can be quickly calculated in closed form. This process is referred to as "human intention alignment":</p><formula xml:id="formula_3">L lab (o, a, w) = ğ”¼ ğ‘§âˆ¼ğ‘’(ğ‘§ |w 1:ğ‘€ ) ğ‘ âˆ‘ï¸ ğ‘¡=1 -log ğœ‹(a ğ‘¡ |o 1:ğ‘¡ , ğ‘§) -ğ›½ 2 ğ”¼ ğ‘§âˆ¼sg[ğ‘’(ğ‘§ |w 1:ğ‘€ ) ] [log ğ‘’(ğ‘§|o 1:ğ‘ )] ,<label>(3)</label></formula><p>where sg <ref type="bibr">[â€¢]</ref> denotes stop gradient operation. The MLE-based alignment objective ensures that the latent sampled from the label-conditioned distribution ğ‘’(ğ‘§|ğ‘¤ 1:ğ‘€ ) can also be sampled from its videoconditioned distribution ğ‘’(ğ‘§|ğ‘œ 1:ğ‘ ). The final loss function combines the two objectives:</p><formula xml:id="formula_4">L (D dem , D lab ) = ğ”¼ (o,a)âˆ¼D dem [L dem (o, a)] + ğ”¼ (o,a,w)âˆ¼D lab [L lab (o, a, w)] .<label>(4)</label></formula><p>Specific implementation details, such as the model design choice, can be found in the Appendix A.</p><p>Inference Procedure. GROOT-2 supports two types of instructions during inference: (1) visual-based instruction -the user can either retrieve a demonstration from the dataset as a reference video or manually record a reference video to serve as the condition for the policy; (2) label-based instruction -the user can input a text sentence or specify an expected return as the condition (depending on the label modality used during the model's training). We tested them in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Capabilities and Analysis</head><p>We aim to address the following questions: (1) How does G ROO T-2 perform in open-world video games and robotic manipulation? (2) Can GROOT-2 follow instructions beyond language and video?</p><p>(3) What insights can be gained from visualizing the learned latent space? (4) How does GROOT-2 scale with labeled and unlabeled trajectories? (5) What is the impact of backbone initialization on performance? (6) How do language and video losses influence performance?</p><p>Environment and Benchmarks. We conduct experiments across four types of representative environments: classical 2D game-playing benchmarks on Atari <ref type="bibr" target="#b9">(Bellemare et al., 2013)</ref>, 3D open-world gameplaying benchmarks on Minecraft <ref type="bibr" target="#b28">(Johnson et al., 2016;</ref><ref type="bibr" target="#b33">Lin et al., 2023)</ref>, and Robotics benchmarks on Language Table simulator <ref type="bibr" target="#b39">(Lynch et al., 2023)</ref> and Simpler Env simulator <ref type="bibr" target="#b31">(Li et al., 2024)</ref>,  illustrated in Figure <ref type="figure" target="#fig_3">5</ref>. These four simulators are used to evaluate whether G ROO T-2 can be effectively steered by returns <ref type="bibr" target="#b15">(Chen et al., 2021;</ref><ref type="bibr" target="#b42">Mnih et al., 2015)</ref>, reference videos <ref type="bibr">(Cai et al., 2023b;</ref><ref type="bibr" target="#b27">Jang et al., 2022)</ref>, and textual instructions <ref type="bibr" target="#b10">(Brohan et al., 2022</ref><ref type="bibr" target="#b11">(Brohan et al., , 2023))</ref>.</p><p>Results on the Open-World Minecraft Benchmark. To evaluate policy models in Minecraft, we used the contractor dataset from <ref type="bibr" target="#b8">Baker et al. (2022)</ref>, containing 160M frames. According to the meta information, labeled trajectories account for approximately 35% of the total data. We extended the Minecraft SkillForge Benchmark <ref type="bibr">(Cai et al., 2023b)</ref>  (2) GROOT-2 (text) performs similarly to GROOT-2 (visual) across most tasks, demonstrating that language and visual modalities share task knowledge. This enables the model to leverage both modalities for improved task completion. This highlights the advantage of combining multimodal data for better alignment with human intentions and improved policy performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the Language Table benchmark.</head><p>To assess GROOT-2's multimodal instruction following We mark the results of RT-2 in gray here, as it uses significantly more training data than ours.</p><p>(2) The performance of GROOT-2 in following video instructions dropped by approximately 6% compared to text instructions, possibly due to the ambiguity of the reference videos, where a "block to block" type video could be interpreted as a "block to relative location" type task.</p><p>(3) GROOT-1 struggled to understand the intentions conveyed by the reference videos. We observed that GROOT-1 imitated a reference video's trajectory sketch rather than their colors and shapes. This further underscores the importance of introducing language annotations for some trajectory data as a crucial method to align with human intentions.</p><p>Results on the Simpler Env Benchmark. We utilized the Simpler Env <ref type="bibr" target="#b31">(Li et al., 2024)</ref> simulation of the Google Robot environment to evaluate the policy's capability in controlling complex robotic arms. G ROO T-2 is trained on the OpenX dataset <ref type="bibr">(Collaboration et al., 2023)</ref>. We erased the text labels from half of the dataset's trajectories, achieving a 1:1 balance between labeled and unlabeled data. We evaluated three types of tasks: Pick Coke Can, Move Near, and Open/Close Drawer. Following <ref type="bibr" target="#b11">Brohan et al. (2023)</ref>; <ref type="bibr" target="#b31">Li et al. (2024)</ref>, we set up multiple variants for each task. For instance, the Pick Coke Can task involved three different poses for the Coke can; in the Move Near task, the layout and types of objects varied; and in the Open/Close Drawer task, the drawer had three layers from top to bottom. We compared G ROO T-2 with baseline methods such as RT-1 <ref type="bibr" target="#b10">(Brohan et al., 2022)</ref>, and Octo (Octo Model <ref type="bibr" target="#b44">Team et al., 2024)</ref>. Among these, RT-1-X is an efficient language-conditioned transformer-based policy trained on the entire OpenX (Collaboration et al., 2023) dataset, which can be considered the performance boundary that GROOT-2 can achieve. As shown in Table <ref type="table" target="#tab_4">3</ref>, we found that GROOT-2 (lang) and GROOT-2 (visual) achieved comparable performance to the RT-1-X model across all three tasks. This indicates that our method retains language control capabilities and imbues the policy with equivalent visual instruction control abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can GROOT-2 Follow Instructions Beyond Language and Video, Like Episode Returns?</head><p>We evaluated GROOT-2 's steerability and performance on four Atari games (Breakout, Demon  Attack, Hero, and Name This Game). Datasets from <ref type="bibr" target="#b2">Agarwal et al. (2020)</ref>, containing approximately 10M frames per game, were used. Episode returns were normalized to ğœ‡ = 0, ğœ = 1.</p><p>For training, we constructed a dataset with 30% labeled trajectories (returns) and 70% unlabeled data. Using this dataset, we trained GROOT (wsl) (weakly supervised learning). For comparison, GROOT (ssl) was trained on the same dataset without return labels in a fully self-supervised manner. Both models were jointly trained across the four games. During inference, we evaluated policy performance in following reference videos sampled from the test set with normalized returns {-1, 0, +1}, using 20 samples per category. Results (Figure <ref type="figure" target="#fig_4">6</ref>) show: (1) GROOT (ssl) can recognize behavioral quality in reference videos, constructing a rough intention space even without labeled guidance. (2) Labeled data significantly improved GROOT (wsl)'s ability to understand video instructions, with the greatest gains in Hero and Name This Game. We also evaluated GROOT (wsl) on return-style instructions with normalized rewards {-1, 0, +1}. The similarity between video and reward-conditioned performance suggests the video encoder and reward encoder share the same intention space. The Atari experiments aim to evaluate G ROO T-2 's performance on modalities beyond language and video, rather than maximizing scores, distinguishing it from traditional offline RL methods. We applied t-SNE to visualize embeddings from randomly sampled reference videos. Each point in Figure <ref type="figure" target="#fig_5">7</ref> represents a unique video, with shapes denoting game environments and colors indicating episode returns. The first row illustrates results for GROOT (ssl), where videos in Breakout, Demon Attack, and Name This Game are classified into two categories based on episode return magnitudes, suggesting that the self-supervised algorithm distinguishes only significant score differences. In contrast, GROOT (ssl) shows poor clustering and limited steerability in the Hero game. The second row shows results for GROOT (wsl), which captures continuous variations in video behavior quality across all games. As shown in the fifth column, embeddings from different environments follow a continuous pattern aligned with reward labels, indicating a shared latent space that promotes cross-environment knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What Does the Visualization of the Learned Latent Space Reveal?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Does Scaling Up Unlabeled Trajectories Impact Performance?</head><p>We trained four GROOT-2 variants with 0%, 25%, 50%, and 100% unlabeled data in Minecraft. Performance was tested on five Minecraft tasks (Chop Tree, Hunt Animals, Combat Enemies, Open Chest, Climb Mountain) and scored relative to skilled human players. For example, if a human collects 20.0 logs in 600 steps and GROOT-2 collects 15.0, the score is 0.75. Results (Figure <ref type="figure" target="#fig_6">8</ref>) show consistent improvement with more unlabeled data, with the 100% variant achieving a 5Ã— gain in the Climb Mountain task over the 0% version. It is worth noting that the Climb Mountain and Open Chest tasks do not have language instructions in the training set. To evaluate the impact of labeled trajectory proportions in the training set on the instruction-following capabilities of GROOT-2, we conducted experiments on the Language Table <ref type="table">benchmark</ref>. The total number of trajectories remained constant across different dataset configurations, with only the proportion of trajectories containing text labels varying. Figure <ref type="figure" target="#fig_7">9</ref> reports the success rate achieved by G ROO T-2 conditioned on language. At low labeled data proportions (0% -25%), the success rate rapidly increased from 10% to 65%, indicating that labeled data significantly influences model performance. However, as the labeled data proportion increased to 50% -80%, the success rate plateaued, rising slightly from 82% to 83%, demonstrating diminishing marginal gains from additional labeled data. Therefore, under resource constraints, a labeled data proportion of 50% may represent the optimal balance between performance and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Does Scaling Up Labeled Trajectories Impact Performance?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Does Backbone Initialization Affect Performance?</head><p>We evaluated different initializations for ViT (random, ImageNet, CLIP) and BERT (random, BERT, CLIP) on the Language Table Benchmark. For randomly initialized models, both backbones were unfrozen during training. According to Table <ref type="table" target="#tab_6">4</ref>, CLIP initialization yielded the best results for ViT, followed by ImageNet, with minimal difference between them, while random initialization performed worst. For BERT, CLIP and standard BERT initialization performed similarly, both surpassing random initialization. Initializing vision and language encoders with CLIP parameters improves policy performance and reduces training time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Does Language and Video Losses Impact Performance?</head><p>The L lab loss significantly enhances the model's understanding of reference videos, as observed in the Language Table <ref type="table">environment</ref>. We compared a variant without L lab loss to the full GROOT-2 model, both trained on the same scale of the Language Table <ref type="table">dataset</ref>, and tested their ability to follow reference videos using standard evaluation scripts. As shown in Table <ref type="table" target="#tab_7">5</ref>, the variant without L lab loss failed to complete any tasks. Further analysis of its output videos revealed that it mechanically mimicked the arm movement trajectories in the reference videos, completely ignoring object colors and shapes, which is inconsistent with human understanding of the reference videos.</p><p>The L dem loss is indispensable in the GROOT-2 architecture. Removing L dem causes the pipeline to degrade into an autoencoder when processing unlabeled data. Without constraints on the latent encoding, the model tends to learn the video encoder as an inverse dynamics model, encoding low-level action sequences in latent z instead of high-level task information, thereby significantly reducing the behavior cloning loss. Additionally, Table <ref type="table" target="#tab_7">5</ref> show that removing L dem causes the language encoder's latent z to collapse, leading to a dramatic drop in task success rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head><p>Learning Policies Across Diverse Domains. Developing policies for sequential control tasks in real and virtual environments poses significant challenges. Research spans domains such as robotic manipulation <ref type="bibr" target="#b39">(Lynch et al., 2023;</ref><ref type="bibr" target="#b60">Yu et al., 2019</ref><ref type="bibr">), video games (Bellemare et al., 2013;</ref><ref type="bibr" target="#b23">Guss et al., 2019)</ref>, and embodied navigation <ref type="bibr" target="#b24">(Hong et al., 2020;</ref><ref type="bibr" target="#b25">Huang et al., 2023;</ref><ref type="bibr" target="#b54">Savva et al., 2019)</ref>, with approaches categorized into reinforcement learning (RL) and imitation learning (IL) based on reward function reliance. For video games with dense rewards (e.g., ALE platform <ref type="bibr" target="#b9">(Bellemare et al., 2013)</ref>), online RL algorithms can achieve superhuman performance <ref type="bibr" target="#b7">(Badia et al., 2020;</ref><ref type="bibr" target="#b42">Mnih et al., 2015)</ref> but suffer from low efficiency, risky interactions, and limited generalization. These challenges restrict their applicability to physical <ref type="bibr" target="#b45">(Padalkar et al., 2023)</ref> or embodied environments <ref type="bibr" target="#b23">(Guss et al., 2019)</ref>, where rewards and cheap interactions are unavailable. IL, as a supervised learning paradigm, addresses these issues through batch efficiency and scalability with large datasets, leveraging Transformer architectures <ref type="bibr" target="#b27">(Jang et al., 2022;</ref><ref type="bibr" target="#b47">Pashevich et al., 2021;</ref><ref type="bibr" target="#b62">Zhang &amp; Chai, 2021)</ref>. The RT-X series <ref type="bibr" target="#b10">(Brohan et al., 2022</ref><ref type="bibr" target="#b11">(Brohan et al., , 2023;;</ref><ref type="bibr" target="#b45">Padalkar et al., 2023)</ref> advances robotic manipulation by training Transformers on large expert demonstration datasets, achieving strong zero-shot generalization. Similarly, <ref type="bibr" target="#b8">Baker et al. (2022)</ref> developed a Transformer-based policy for Minecraft using internet-scale gameplay data, solving the diamond challenge. Building on this, Schmidhuber (2019) frames RL as supervised learning, while <ref type="bibr" target="#b15">Chen et al. (2021)</ref>; <ref type="bibr" target="#b30">Lee et al. (2022)</ref> introduce "decision transformers" to model joint distributions of rewards, states, and actions from offline data, highlighting the potential for unified policy learning within Transformers.</p><p>Learning Policies to Follow Instructions. Enabling policies to follow instructions is key to building general-purpose agents. A common approach involves using language annotations from offline demonstrations to train language-conditioned policies <ref type="bibr" target="#b1">(Abramson et al., 2020;</ref><ref type="bibr" target="#b10">Brohan et al., 2022;</ref><ref type="bibr">Cai et al., 2023a;</ref><ref type="bibr" target="#b25">Huang et al., 2023;</ref><ref type="bibr" target="#b49">Raad et al., 2024;</ref><ref type="bibr" target="#b53">Reed et al., 2022;</ref><ref type="bibr">Wang et al., 2023a,b)</ref>, leveraging the compositionality of natural language for generalization. However, obtaining highquality annotations is costly. An alternative uses anticipated outcomes as instructions. <ref type="bibr" target="#b40">Majumdar et al. (2022)</ref> trained an image-goal conditioned navigation policy via hindsight relabeling (HER) <ref type="bibr" target="#b6">(Andrychowicz et al., 2017)</ref> and aligned goal spaces with text. Similarly, <ref type="bibr">Lifshitz et al. (2023) used</ref> this strategy for open-ended tasks in Minecraft. Generative latent variable models offer another solution, using label-free demonstrations to train plan-conditioned policies <ref type="bibr" target="#b3">(Ajay et al., 2020;</ref><ref type="bibr">Lynch et al., 2020b)</ref>. Extending this, <ref type="bibr">Cai et al. (2023b)</ref> applied a posterior encoder to interpret reference videos in Minecraft. Policy learning with weak supervision remains less explored. <ref type="bibr">Lynch &amp; Sermanet (2020)</ref> proposed a shared latent space conditioned on language and HER-generated goal images, while <ref type="bibr" target="#b27">Jang et al. (2022)</ref> replaced goal images with video labels under full supervision. <ref type="bibr" target="#b26">Jain et al. (2024)</ref> trained robots using human videos as task representations but required extensive paired video-trajectory data. <ref type="bibr" target="#b43">Myers et al. (2023)</ref> combined labeled and unlabeled trajectories, aligning start-goal pairs with language via contrastive learning, effective for Table Manipulation but limited in handling complex tasks or generalizing to partially observable environments like Minecraft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions, Limitations and Future Works</head><p>This paper investigates the joint learning of a latent intention space and a multimodal instructionfollowing policy under weak supervision. We identify the "latent space ambiguity" issue in latent variable generative models when handling text-free trajectory data, arising from the absence of direct human guidance in shaping the latent space. To address this, we propose a weakly supervised algorithm for training G ROO T-2. Evaluations across four diverse environments, from video games to robotic manipulation, demonstrate GROOT-2 's generality and flexibility in following multimodal instructions. However, GROOT-2 's reliance on trajectory data for training limits its applicability to video data, which lacks action labels. Considering the abundance and diversity of video data available online compared to trajectory data, extending the weak supervision framework to leverage both play and trajectory data would be a promising avenue for future work. Training Dataset. We utilize the trajectories from the Replay Buffer generated during the training of agents using the DQN algorithm on Atari, provided by Google, as our source of training data. We access this data through the interface provided in the d4rl-atari project at GitHub 1 . Specifically, the trajectory data for each game consists of three parts: mixed, medium, and expert, representing the environment interaction data from 0-1M steps, 9M-10M steps, and the final 1M steps of a training session, respectively. We construct training data of 10M steps for each Atari game, with the proportions of mixed, medium, and expert data being 2:5:3. During training, we use a single model to fit 35 selected game environments on Atari. Considering the significant differences in absolute scores across different games, we standardize the reward scores. Specifically, we calculate the cumulative reward scores for each complete trajectory and adjust them to a mean of 0 and a standard deviation of 1 using the formula ğ‘… â† (ğ‘… -ğœ‡)/ğœ, which represents the game level corresponding to that trajectory. Figure <ref type="figure" target="#fig_8">10</ref> illustrates the episode return distributions for each Atari game. Subsequently, each trajectory is segmented into 128-step fragments with the same label.</p><p>Complete Results. We selected trajectory data from 35 Atari games, totaling 350 million frames, to train GROOT-2, with 30% of the data labeled with returns and the remaining 70% containing only image observations and actions, aligning with the setup for weakly supervised training. After the model converged, we tested GROOT-2's ability to follow return-format and video-format instructions across these 35 games. When testing return-format instructions, we chose three samples within the normalized returns space: {-1, 0, 1}. For video-format instructions, we randomly sampled a segment of 128 frames from the test data with normalized rewards within the range of {-1, 0, 1}, allowing a deviation of Â±0.05. Each instruction was tested 40 times, with the results depicted in Figure <ref type="figure" target="#fig_9">11</ref>. We observed the following: (1) In the majority of games, G ROO T-2's performance showed a clear positive correlation with the game level corresponding to the instructions. (2) In certain games (such as Pong, Seaquest, Skiing, Wizard of Wor), video-format instructions yielded better control over the agent than return-format instructions. Conversely, in games like Amidar, Battle Zone, and Zaxxon, return-format instructions demonstrated significantly superior control compared to video-format instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Minecraft</head><p>Environment Description. Minecraft is a 3D sandbox game with a global monthly active user base of 100 million. It features procedurally generated worlds of unlimited size and includes dozens of biomes such as plains, forests, jungles, and oceans. The game grants players a high degree of freedom to explore the entire world. The mainstream gameplay includes gathering materials, crafting items, constructing structures, farming land, engaging in combat mobs, and treasure hunting, among others. In this game, players need to face situations that are highly similar to the real world, making judgments and decisions to deal with various environments and problems. One can easily specify a task with a natural language description or a demonstration video. Therefore, Minecraft is an ideal environment to test how an agent behaves in an open-world environment.</p><p>Observation and Action Spaces. We use the combination of <ref type="bibr">1.16.5 version MineRL (Guss et al., 2019)</ref> and MCP-Reborn<ref type="foot" target="#foot_1">foot_1</ref> as our testing platform, which is consistent with the environment used by VPT <ref type="bibr" target="#b8">(Baker et al., 2022</ref>) STEVE-1 <ref type="bibr" target="#b32">(Lifshitz et al., 2023)</ref> and GROOT-1 <ref type="bibr">(Cai et al., 2023b)</ref>. Mainly because this platform preserves observation and action space that is consistent with human players to the fullest extent. On the one hand, this design brings about high challenges, as agents can only interact with the environment using low-level mouse and keyboard actions, and can only observe visual information like human players without any in-game privileged information. The Minecraft simulator first generates an RGB image with dimensions of 640 Ã— 360 during the rendering process. Before inputting to the agent, we resize the image to 224 Ã— 224 to enable the agent to see item icons in the inventory and important details in the environment. When the agent opens the GUI, the simulator also renders the mouse cursor normally. The RGB image is the only observation that the agent can obtain from the environment during inference. It is worth noting that to help the agent see more clearly in extremely dark environments, we have added a night vision effect for the agent, which increases the brightness of the environment during nighttime. Our action space is almost identical to that of humans, except for actions that involve inputting strings. It consists of two parts: the mouse and the keyboard. The mouse movement is responsible for changing the player's camera perspective and moving the cursor when the GUI is opened. The left and right buttons are responsible for attacking and using items. The keyboard is mainly responsible for controlling the agent's movement. To avoid predicting null action, we used the same joint hierarchical action space as <ref type="bibr" target="#b8">Baker et al. (2022)</ref>, which consists of button space and camera space. Button space encodes all combinations of keyboard operations and a flag indicating whether the mouse is used, resulting in a total of 8461 candidate actions. The camera space discretizes the range of one mouse movement into 121 actions. Therefore, the action head of the agent is a multi-classification network with 8461 dimensions and a multi-classification network with 121 dimensions.</p><p>Training Dataset. The contractor data is a Minecraft offline trajectory dataset provided by <ref type="bibr" target="#b8">Baker et al. (2022)</ref>, which is recorded by professional human players. In this dataset, human players play the game while the system records the image sequence ğ‘œ 1:ğ‘ , action sequence ğ‘ 1:ğ‘ , and metadata ğ‘’ 1:ğ‘ generated by the players. Excluding frames containing empty actions, the dataset contains 1.6 billion frames with a duration of approximately 2000 hours. The metadata records the 7 kinds of events triggered by the agent in the game at each timestep, i.e. craft item, pickup, mine block, drop item, kill entity, use item, and custom. We augment each event with a text description using the OpenAI chatGPT service. To construct trajectory data with textual labels, we enumerate all timesteps within the trajectory where an event occurs. From this point, we count 112 frames backward and 16 frames forward to form a segment of 128 frames. The textual label for this segment is derived from the text associated with the event. It is important to note that many events occur frequently; for example, when the player is mining a tunnel, the event "mine block: cobblestone" is triggered on average twice per second. To address this issue, if a segment generated by an event overlaps with a previously generated segment, it is skipped. Each event collects a maximum of 2000 segments, and across all 1518 events, 414,387 segments are included. It is noteworthy that a significant amount of duplication persists within these segments, as a single segment may encompass multiple events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Language Table</head><p>Environment Description. Language Table <ref type="bibr" target="#b39">(Lynch et al., 2023)</ref> is a comprehensive evaluation suite proposed by the Google for assessing a robot's ability to follow natural language instructions to solve Table Manipulation tasks. It includes a dataset, environment, benchmarks, and a baseline policy. The evaluation benchmark encompasses over 87, 000 diverse behaviors and more than 600, 000 trajectories annotated with text instruction. In addition to data from real environments, the suite also provides a simulator akin to a real environment along with corresponding simulated data.</p><p>Observation and Action Spaces. Language Table's simulated environment resembles the real-world tabletop manipulation scenario, which consists of an xArm6 robot, constrained to move in a 2D plane with a cylindrical end-effector, in front of a smooth wooden board with a fixed set of 8 plastic blocks, comprising 4 colors and 6 shapes. In both simulation and real collection, they use high-rate human teleoperation with a 3rd person view (line-of-sight in real). Actions are 2D delta Cartesian setpoints, from the previous setpoint to the new one. They batch collected training and inference data to 5hz observations and actions.</p><p>Training Dataset. We use the training trajectories from the official Language Table repository. An oracle script generates the trajectories and covers all 5 task families, each containing 20M trajectories, in a total of 100M trajectories. The dataset names are: language-table-blocktoblockoracle-sim, language-table-blocktoblockrelative-oracle-sim, language-table-blocktoabsolute-oracle-sim, language-table-blocktorelative-oracle-sim, language-table-separate-oracle-sim.</p><p>Task Definition. The evaluation benchmark consists of 5 task families (block2block, block2abs,  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 |</head><label>2</label><figDesc>Figure2| The ELBO Objective of the VAE and Latent Space Spectrum. We define a spectrum based on ğ‘… = ğµğ¶ ğµğ¶+ğ¾ğ¿ , where ğ‘… = 0 corresponds to "mechanical imitation" and ğ‘… = 1 to "posterior collapse." At low ğ‘…, latent vector ğ‘§ directly outputs action sequences without considering observations ( ğµğ¶ â†’ 0 ). As ğ‘… increases, ğ‘§ represents high-level task information, such as specific object interactions. At ğ‘… = 1, ğ‘§ provides no beneficial information for decision-making.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 |</head><label>3</label><figDesc>Figure 3 | Comparison of Policies with Different Latent Spaces. The reference video depicts digging for diamonds. A policy that mechanically imitates the trajectory falls into lava, while one aligned with human intention avoids lava and successfully reaches the diamonds.</figDesc><graphic coords="4,64.71,85.04,465.84,144.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 |</head><label>4</label><figDesc>Figure 4 | Pipeline for Constructing a Training Batch for G ROO T-2. Each batch includes two sample types: (1) demonstration-only samples for learning a latent-conditioned policy (Constrained Self-Imitating); and (2) labeled samples (text or expected returns) for aligning the latent space with human intentions (Human Intention Alignment). The sample ratio varies by dataset distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | Diverse visual environments used in the experiments. We test our GROOT-2 on both video games (simple Atari games and the complex Minecraft game) and robotic manipulation environments (LanguageTable and Simpler Env). Minecraft is a partially observable open-ended environment, while others are fully observable.</figDesc><graphic coords="6,74.13,85.04,447.03,122.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 |</head><label>6</label><figDesc>Figure 6 | Comparison of Weakly Supervised (WSL) and Self-Supervised (SSL) Learning on 4 Atari Games.Policies are evaluated under return and reference video conditions. For return conditioning, normalized returns are input into the encoder, while for video conditioning, videos with similar returns (error &lt; 0.05) are used.</figDesc><graphic coords="9,74.13,85.04,447.01,107.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7 | t-SNE Visualization of Learned Latent Spaces on Four Atari Games. The first row shows results under self-supervised learning, while the second row displays GROOT-2 's performance under weakly supervised learning. Points represent reference videos, with shapes indicating games and colors denoting episode returns. The first four columns compare individual games, and the last column shows a mixed-game comparison.</figDesc><graphic coords="9,74.13,254.88,447.03,166.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 |</head><label>8</label><figDesc>Figure 8 | Performance Comparison on Unlabeled Demonstrations. Human-normalized task scores are averaged over 20 rollouts across 5 Minecraft tasks to evaluate the agent's reference-video following ability.</figDesc><graphic coords="10,74.13,85.04,447.03,100.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 |</head><label>9</label><figDesc>Figure 9 | Ablation Study on Labeled Trajectories in the Language Table.</figDesc><graphic coords="10,363.52,527.66,169.39,96.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 |</head><label>10</label><figDesc>Figure 10 | Distribution of episode returns for each Atari game.</figDesc><graphic coords="21,99.64,85.04,396.00,378.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 |</head><label>11</label><figDesc>Figure 11 | IQM scores (with 95% confidence interval) of GROOT-2 which is jointly trained on 35 Atari games. G ROO T-2 can understand both the returns-format instructions and video-format instructions on most of the games. The performance of the G ROO T-2 exhibits a positive correlation with the game level corresponding to the provided instructions.</figDesc><graphic coords="25,99.64,146.56,396.00,486.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 |</head><label>12</label><figDesc>Figure 12 | GROOT-2 can infer the intention behind the reference video and follow it to complete tasks. The left visualizes three reference videos along with their textual descriptions. The right figure displays the policy's rollout trajectories when conditioned on the reference videos. The white dashed line represents the arm's movement trajectory, and the red dashed circle highlights the arm's final position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,74.13,376.48,447.03,185.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 |</head><label>1</label><figDesc>Results on the Open-World Minecraft Benchmark. This benchmark includes 8 task families and 100 tasks. Each task is evaluated 30 times across three seeds, and the average success rate is calculated per task family. For example, Combat (16) indicates 16 tasks in the Combat family.</figDesc><table><row><cell>Methods</cell><cell>Prompt</cell><cell>Combat (16)</cell><cell>Hunt (10)</cell><cell>Ride (4)</cell><cell>Breed (8)</cell><cell>Craft (20)</cell><cell>Mine (20)</cell><cell>Interact (10)</cell><cell>Plant (12)</cell></row><row><cell>VPT</cell><cell>N/A</cell><cell>11 Â±3</cell><cell>20 Â±4</cell><cell>7 Â±2</cell><cell>2 Â±0</cell><cell>4 Â±1</cell><cell>7 Â±2</cell><cell>21 Â±6</cell><cell>22 Â±7</cell></row><row><cell>STEVE-1</cell><cell>lang</cell><cell>12 Â±3</cell><cell>9 Â±2</cell><cell>54 Â±8</cell><cell>4 Â±2</cell><cell>5 Â±2</cell><cell>6 Â±3</cell><cell>53 Â±9</cell><cell>33 Â±8</cell></row><row><cell>STEVE-1</cell><cell>visual</cell><cell>15 Â±4</cell><cell>10 Â±3</cell><cell>38 Â±9</cell><cell>6 Â±2</cell><cell>6 Â±2</cell><cell>10 Â±4</cell><cell>40 Â±8</cell><cell>43 Â±7</cell></row><row><cell>GROOT-1</cell><cell>visual</cell><cell>18 Â±5</cell><cell>28 Â±8</cell><cell>26 Â±6</cell><cell>12 Â±3</cell><cell>15 Â±4</cell><cell>22 Â±7</cell><cell>57 Â±8</cell><cell>75 Â±6</cell></row><row><cell>GROOT-2</cell><cell>lang</cell><cell>40 Â±7</cell><cell>43 Â±5</cell><cell>46 Â±6</cell><cell>22 Â±6</cell><cell>18 Â±3</cell><cell>37 Â±5</cell><cell>55 Â±4</cell><cell>75 Â±9</cell></row><row><cell>GROOT-2</cell><cell>visual</cell><cell>37 Â±4</cell><cell>48 Â±7</cell><cell>51 Â±4</cell><cell>20 Â±4</cell><cell>27 Â±3</cell><cell>36 Â±7</cell><cell>63 Â±6</cell><cell>77 Â±7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 |</head><label>2</label><figDesc>Results on the Language TableBenchmark. We reported success rates (in %) within 200 steps for each instruction modality, averaging over 250 rollouts. Results are averaged over 3 seeds with mean and stderr. "-" indicates missing data. The percentages in parentheses indicate the proportion of labels used.</figDesc><table><row><cell>Task Family</cell><cell cols="8">BC-Zero LAVA RT-1 GROOT-1 GROOT-2 (50%) GROOT-2 (100%)</cell></row><row><cell></cell><cell>lang</cell><cell cols="2">lang lang</cell><cell>visual</cell><cell>lang</cell><cell>visual</cell><cell>lang</cell><cell>visual</cell></row><row><cell>block to block</cell><cell>-</cell><cell>90 Â±2</cell><cell>-</cell><cell>8 Â±2</cell><cell>84 Â±9</cell><cell>78 Â±9</cell><cell>86 Â±8</cell><cell>82 Â±7</cell></row><row><cell>block to absolute loc</cell><cell>-</cell><cell>72 Â±4</cell><cell>-</cell><cell>10 Â±3</cell><cell>70 Â±8</cell><cell>68 Â±8</cell><cell>76 Â±6</cell><cell>70 Â±8</cell></row><row><cell>block to block relative loc</cell><cell>-</cell><cell>72 Â±3</cell><cell>-</cell><cell>4 Â±1</cell><cell>74 Â±9</cell><cell>64 Â±7</cell><cell>76 Â±8</cell><cell>62 Â±6</cell></row><row><cell>block to relative loc</cell><cell>-</cell><cell>64 Â±4</cell><cell>-</cell><cell>8 Â±2</cell><cell>82 Â±5</cell><cell>78 Â±6</cell><cell>84 Â±6</cell><cell>80 Â±4</cell></row><row><cell>separate two blocks</cell><cell>-</cell><cell>94 Â±2</cell><cell>-</cell><cell>12 Â±2</cell><cell>98 Â±1</cell><cell>96 Â±2</cell><cell>98 Â±0</cell><cell>98 Â±0</cell></row><row><cell>Overall</cell><cell>72 Â±3</cell><cell cols="2">78 Â±4 74 Â±13</cell><cell>8 Â±2</cell><cell>82 Â±8</cell><cell>76 Â±7</cell><cell>84 Â±6</cell><cell>78 Â±8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>from 30 to 100 tasks, grouped into eight families: Combat, Hunt, Ride, Breed, Craft, Mine, Interact, and Plant. Details are in the Appendix C. We compared G ROO T-2 with three baselines: (1) VPT<ref type="bibr" target="#b8">(Baker et al., 2022)</ref>, a foundational model trained on YouTube data via imitation learning, lacking instruction-following; (2) STEVE-1<ref type="bibr" target="#b32">(Lifshitz et al., 2023)</ref>, which supports text and future image-conditioned instructions; and (3) GROOT-1(Cai  et al., 2023b), a self-supervised model using reference videos as instructions. Key findings from Table1are as follows: (1) G ROO T-2 (visual) consistently outperforms GROOT-1 across all task categories, with particularly notable gains in mob interaction tasks like Combat and Hunt. Comparing trajectories on Hunt, GROOT-1 mechanically repeats "attack" actions, while G ROO T-2 actively tracks objects, showing that text data enhances object-centric understanding and better aligns with human intentions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 |</head><label>3</label><figDesc>Results on the Simpler Env Benchmark. We report the success rate (in %) of the video-instruction and language-instruction following for each model on 3 task families. The percentages in parentheses indicate the proportion of labels used. capabilities in the context of Robotic TableManipulation, we utilized the Google Language Table as our testing platform and compared it with methods such as LAVA<ref type="bibr" target="#b39">(Lynch et al., 2023)</ref>, RT-1<ref type="bibr" target="#b10">(Brohan et al., 2022)</ref>, GROOT-1(Cai et al., 2023b). We utilize a dataset provided by<ref type="bibr" target="#b39">Lynch et al. (2023)</ref> comprising 100M trajectories. We removed the text labels from half of the trajectories in the dataset, creating a 1 : 1 ratio of labeled to unlabeled trajectories. Given that the Language Table environment comes with five task families, all of which are instructed solely through language, we curated five reference videos for each task with relatively clear intentions to evaluate the model's ability to comprehend video instructions. Detailed specifics are provided in the appendix D. The experimental results are shown in the Table2. We observed that: (1) G ROO T-2 leads by an absolute success rate of 4% following text-based instructions compared, likely due to GROOT-2's more refined model architecture design.</figDesc><table><row><cell>Methods</cell><cell>Prompt</cell><cell></cell><cell cols="2">Pick Coke Can</cell><cell></cell><cell cols="4">Move Near Open/Close Drawer</cell></row><row><cell></cell><cell></cell><cell cols="4">H-Pose V-Pose S-Pose Avg</cell><cell>Avg</cell><cell cols="3">Open Close Avg</cell></row><row><cell>RT-1-X</cell><cell>lang</cell><cell>57</cell><cell>20</cell><cell>70</cell><cell>49</cell><cell>32</cell><cell>7</cell><cell>52</cell><cell>29</cell></row><row><cell>Octo-base</cell><cell>lang</cell><cell>5</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>2</cell><cell>1</cell></row><row><cell>GROOT-2 (50%)</cell><cell>visual lang</cell><cell>42 52</cell><cell>18 20</cell><cell>52 50</cell><cell>37 41</cell><cell>35 42</cell><cell>29 27</cell><cell>30 33</cell><cell>30 30</cell></row><row><cell>GROOT-2 (100%)</cell><cell>visual lang</cell><cell>40 53</cell><cell>22 23</cell><cell>47 52</cell><cell>36 42</cell><cell>35 45</cell><cell>27 27</cell><cell>33 35</cell><cell>30 31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 |</head><label>4</label><figDesc>Ablation study on the backbone initialization.</figDesc><table><row><cell cols="5">Variants -L lab baseline -L dem baseline</cell></row><row><cell>Prompt</cell><cell cols="2">vision vision</cell><cell>lang</cell><cell>lang</cell></row><row><cell cols="2">SR (in %) 10 Â±2</cell><cell>76 Â±7</cell><cell>12 Â±3</cell><cell>82 Â±8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 |</head><label>5</label><figDesc>Ablation on L lab and L dem objectives.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/takuseno/d4rl-atari</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/Hexeption/MCP-Reborn</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Model Architecture</head><p>This section outlines the architectural design choices employed in our approach. GROOT-2 utilizes a Transformer encoder-decoder architecture, augmented with a probabilistic latent space. We detail the components of the model in a structured sequence: extract representations, encode instructions, and decode actions.</p><p>Extract Representations. This paragraph elaborates on the backbone networks used to extract representations from various data modalities. We denote the modalities of image observation, language instruction, and expected returns as ğ‘œ 1:ğ‘ , ğ‘¤ 1:ğ‘€ , and ğ‘Ÿ, respectively. For vision inputs, we utilize a pre-trained Vision Transformer (ViT) <ref type="bibr" target="#b19">(Dosovitskiy et al., 2020)</ref> initialized with CLIP <ref type="bibr" target="#b52">(Radford et al., 2021)</ref> weights. Specifically, the ğ‘¡-step image observation ğ‘œ ğ‘¡ is resized to 224 Ã— 224 and processed to extract 7 Ã— 7 patch embeddings <ref type="bibr">[49]</ref> . The video representation ğ‘¥ ğ‘£ is then composed of the averages of these embeddings across the video frames, denoted as</p><p>, where avg(â€¢) refers to spatial average pooling to minimize computational overhead and ğ‘ represents the video length. Textual inputs are processed using the BERT encoder <ref type="bibr" target="#b18">(Devlin et al., 2019)</ref> of the CLIP model. Rather than utilizing the [CLS] token as the final representation, we retain all word embeddings generated by BERT as</p><p>The BERT model parameters are kept frozen during training. For the scalar-form modality of expected returns, we employ a simple Multi-Layer Perceptron (MLP) to process these values, represented as ğ‘¥ ğ‘Ÿ â† MLP(ğ‘Ÿ). These embeddings are then forwarded to subsequent modules.</p><p>Encode Multimodal Instructions with Non-Causal Transformer. Recent works <ref type="bibr" target="#b34">(Lu et al., 2023;</ref><ref type="bibr" target="#b53">Reed et al., 2022;</ref><ref type="bibr" target="#b56">Team et al., 2023)</ref> have demonstrated the Transformer's effectiveness in capturing both intra-modal and inter-modal relationships, which inspires us to adopt a unified Transformer encoder for encoding multimodal instructions. This approach offers two significant advantages: (1) It eliminates the need for designing separate architectures and tuning hyperparameters for each modality. (2) It promotes the sharing of underlying representations across different modalities. Instructions are represented as a sequence of embeddings. Before encoding, each embedding is augmented with a modality-specific marker. For instance, video instructions are represented as</p><p>, where [VID] is a learnable embedding. Decode Action with Causal Transformer. Given a latent ğ‘§ and a temporal sequence of perceptual observations ğ‘œ 1:ğ‘¡ , the policy aims to predict the next action ğ‘ ğ‘¡ . Following prior works <ref type="bibr" target="#b8">(Baker et al., 2022;</ref><ref type="bibr">Cai et al., 2023b;</ref><ref type="bibr" target="#b49">Raad et al., 2024)</ref>, we employ the Transformer-XL model <ref type="bibr" target="#b17">(Dai et al., 2019)</ref> in our policy network, which enables causal attention to past memory states and facilitates smooth predictions. Additionally, we utilize the shared vision backbone to extract vision representations, thereby representing perceptual inputs as ğ‘¥ ğ‘œ 1:ğ‘¡ . A significant challenge with this approach is low efficiency: each new observation ğ‘¥ ğ‘œ ğ‘¡ adds up to 49 tokens to the input sequence, substantially increasing memory and computational demands. To address this issue, we introduce a pre-fusion mechanism inspired by <ref type="bibr" target="#b1">Abramson et al. (2020)</ref>; <ref type="bibr" target="#b4">Alayrac et al. (2022)</ref>; <ref type="bibr" target="#b39">Lynch et al. (2023)</ref>. Specifically, we deploy a lightweight cross-attention module XATTN(q = â€¢; kv = â€¢) to perform spatial pooling on ğ‘¥ ğ‘œ ğ‘¡ , using ğ‘§ as the query and ğ‘¥ ğ‘œ ğ‘¡,[1] , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘œ  ğ‘¡,[49] as the keys and values:</p><p>This pre-fusion mechanism not only reduces the sequence length but also enhances the integration of perceptual and latent representations. Utilizing the latent-fused representations ğ‘¥ ğ‘§ 1:ğ‘¡ as the input sequence, we articulate the action decoding process in an autoregressive manner:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyper-parameters</head><p>Hyper-parameters for training G ROO T-2 are shown in Table <ref type="table">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Atari</head><p>Environment Description. Atari 2600 games contain a lot of diverse video games, which is a widespread benchmark to evaluate the decision-making capability of an agent. The Atari games do not inherently support multitasking concepts; agents are typically tasked with optimizing for the highest possible rewards. However, an advanced human player can deliberately control their gameplay level and achieve any potential score. The ability to "control scores" is generally considered a higher intelligence level compared with merely "winning the game". Therefore, this paper does not emphasize the highest absolute score an agent can achieve in the Atari environment. Instead, it focuses on evaluating the agent's ability to follow instructions in the form of videos and "desired cumulative rewards" and to perform at the appropriate level. Especially when videos serve as conditions, the agent needs to infer the player's level demonstrated in the reference gameplay, which poses a significant challenge for the current agents. To our knowledge, this setting has not been explored by previous works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Family Video Description</head><p>block to block put the red moon to the blue moon block to block put the blue moon towards the yellow star block to block slide the red pentagon close to the green cube block to block slide the green star to the red moon block to block put the green cube next to the red pentagon block to absolute location slide the blue cube to the upper left corner block to absolute location push the blue moon to the top left of the board block to absolute location move the red moon to the bottom left block to absolute location slide the yellow star to the right side of the board block to absolute location push the yellow pentagon to the left side block to block relative location move the green star to the left side of the yellow pentagon block to block relative location push the green star diagonally up and to the right of the green cube block to block relative location put the red moon to the bottom left side of the yellow star block to block relative location slide the yellow pentagon to the bottom left side of the red pentagon block to block relative location slide the blue cube to the top of the blue moon block to relative location push the green cube right block to relative location slide the yellow pentagon downwards and to the right block to relative location push the blue cube somewhat to the left block to relative location move the blue moon to the right block to relative location slide the red pentagon up separate pull the yellow pentagon apart from the blue moon separate pull the green star apart from the yellow star separate pull the blue cube apart from the blue moon and red pentagon separate move the blue cube away from the yellow star separate move the green star away from the yellow pentagon block2rel, block2blockrel, separate), totaling 696 distinct task variants. We report the success rate of the agent within 200 steps on each task as the final metric. Considering that the Language Table <ref type="table">inherently</ref> includes instructions in the language modality for its 5 task families, we have curated a set of reference videos for each task family, each with relatively clear intentions, to serve as a video instruction set. This is done to evaluate the model's ability to comprehend video instructions. The details are in Table <ref type="table">7</ref>. We visualize some examples when conditioning GROOT-2 on reference videos in Figure <ref type="figure">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Simpler Env</head><p>Environment Description. Simpler Env is a physical simulator proposed by <ref type="bibr" target="#b31">Li et al. (2024)</ref>, efficient, scalable, and informative complements to real-world evaluations. It can be used to evaluate diverse sets of rigid-body tasks (non-articulated / articulated objects, tabletop / non-tabletop tasks), with many intra-task variations (e.g., different object combinations; different object/robot positions and orientations), for each of two robot embodiments (Google Robot and WidowX).</p><p>Observation and Action Spaces. The observation and action spaces of Simpler Env are the same as the Language Table . The action sequence is expected to be a 6D end-effector pose trajectory with a gripper flag indicating the open/close status. Before feeding the image observation into the policy, we resize the image to a 224 Ã— 224 resolution.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiobjective latent space optimization of generative molecular design models</title>
		<author>
			<persName><surname>Anm Nafiz Abeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Nathan M Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">J</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Jun</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Patterns</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Brussee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Carnevale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Cassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachita</forename><surname>Chhaparia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05672</idno>
		<title level="m">Imitating interactive intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An optimistic perspective on offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Opal: Offline primitive discovery for accelerating offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13611</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.14198</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">248476411</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fixing a broken elbo</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwight</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>ArXiv, abs/1707.01495</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:3532908" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Agent57: Outperforming the atari human benchmark</title>
		<author>
			<persName><forename type="first">AdriÃ </forename><surname>PuigdomÃ¨nech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Video pretraining (vpt): Learning to act by watching unlabeled online videos</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Zhokhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Sampedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno>ArXiv, abs/2206.11795</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">249953673</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rt-1: Robotics transformer for real-world control at scale</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justice</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><surname>Jesmonth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deeksha</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodilyn</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jornell</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Quiambao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grecia</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pannag</forename><forename type="middle">R</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Sanketi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspiar</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Sumedh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Sontakke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ho Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sichun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brianna</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zitkovich</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.06817</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:254591260" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rt-2: Vision-language-action models transfer web knowledge to robotic control</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justice</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15818</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.14165</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">218971783</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:256194112" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="13734" to="13744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Groot: Learning to follow instructions by watching gameplay videos</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">235294299</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O'</forename><surname>Abby</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Neill</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Abhiram</forename><surname>Rehman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Maddukuri</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Padalkar</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Acorn</forename><surname>Lee</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Pooley</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Gupta</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Mandlekar</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Jain</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tung</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bewley</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Herzog</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Irpan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Khazatsky</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Rai</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gupta</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Anikait</forename><surname>Kolobov</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Singh</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Garg</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Kembhavi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Xie</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Antonin</forename><surname>Brohan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Raffin</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Arefeh</forename><surname>Sharma</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Arhan</forename><surname>Yavary</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Jain</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Balakrishna</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wahid</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Beomjoon</forename><surname>Burgess-Limerick</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Kim</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>SchÃ¶lkopf</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Wulfe</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Cewu</forename><surname>Ichter</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Lu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Le</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Finn</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chenfeng</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Chi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Huang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chuer</forename><surname>Agia</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Pan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Coline</forename><surname>Fu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Devin</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Morton</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Driess</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Pathak</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Shah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>BÃ¼chler</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Jayaraman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Kalashnikov</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Sadigh</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Johns</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Fangchen</forename><surname>Foster</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ceola</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xia</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Zhao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Freek</forename><surname>Vieira Frujeri</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Gaoyue</forename><surname>Stulp</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Sukhatme</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Salhotra</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Yan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Feng</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Schiavi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Berseth</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Guangwen</forename><surname>Kahn</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Hao-Shu</forename><surname>Su</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Fang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Shi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Heni</forename><surname>Bao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Ben Amor</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Christensen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Homer</forename><surname>Furuta</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Hongjie</forename><surname>Walke</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Huy</forename><surname>Fang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Ha</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Mordatch</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Radosavovic</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Leal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jad</forename><surname>Liang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jaehyung</forename><surname>Abou-Chakra</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jaimyn</forename><surname>Kim</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Drake</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Schneider</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Hsu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Bohg</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Bingham</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jensen</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Gao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Hu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jiankai</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jianlan</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Luo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Tan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Oh</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jingpei</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Lu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">JoÃ£o</forename><surname>Malik</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>SilvÃ©rio</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hejna</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Booher</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Salvador</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Junhyek</forename><surname>Lim</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Han</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Rao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Pertsch</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Keegan</forename><surname>Hausman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Keerthana</forename><surname>Go</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Gopalakrishnan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kendra</forename><surname>Goldberg</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Byrne</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kento</forename><surname>Oslund</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kawaharazuka</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Black</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Ehsani</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kirsty</forename><surname>Lekkala</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Krishan</forename><surname>Ellis</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Rana</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Srinivasan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Fang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Kunal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kuo-Hao</forename><surname>Singh</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Zeng</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hatch</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Hsu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Itti</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Lerrel</forename><surname>Yunliang Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Pinto</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fei-Fei</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Linxi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><surname>Jim" Fan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ott</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Lee</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Magnum</forename><surname>Weihs</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Lepert</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Masayoshi</forename><surname>Memmel</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Masha</forename><surname>Tomizuka</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><forename type="middle">Guaman</forename><surname>Itkina</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Castro</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Spero</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Du</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Ahn</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Mingtong</forename><surname>Yip</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Minho</forename><surname>Ding</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Heo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Kumar Srirama</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Sharma</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Moo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Kim</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nicklas</forename><surname>Kanazawa</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hansen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Heess</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nikhil</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Joshi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Suenderhauf</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nur</forename><surname>Di Palo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Muhammad Mahi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Shafiullah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Mees</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Osbert</forename><surname>Kroemer</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Bastani</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pannag</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">"</forename><surname>Sanketi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Tree</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Miller</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Yin</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wohlhart</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>David Fagan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Mitrano</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sermanet</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Abbeel</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Qiuyu</forename><surname>Sundaresan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Vuong</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Rafailov</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ria</forename><surname>Tian</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Doshi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Mart</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Xiao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Kollar</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Tianli</forename><surname>Yu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Ding</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Davchev</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Zhao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Armstrong</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Trinity</forename><surname>Darrell</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Vidhi</forename><surname>Chung</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Jain</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Vanhoucke</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Zhou</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Burgard</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Geng</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xuanlin</forename><surname>Liangwei</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Pang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yecheng</forename><surname>Lu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ma</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Kim</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhou</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Wu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Dou</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yoonyoung</forename><surname>Cho</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Youngwoon</forename><surname>Lee</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Cui</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open X-Embodiment Collaboration</orgName>
			</affiliation>
		</author>
		<ptr target="https://arxiv.org/abs/2310.08864" />
		<editor>Mart&apos;in, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1285</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/p19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019-01">Jan 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.04805</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">52967399</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.11929</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">225039882</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradly</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Implicit deep latent variable models for text generation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11527</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11956</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minerl: A large-scale dataset of minecraft demonstrations</title>
		<author>
			<persName><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholay</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Topin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cayden</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><forename type="middle">M</forename><surname>Codel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019. 199000710</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vln-bert: A recurrent vision-and-language bert for navigation</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="page">227228335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jiangyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silong</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiongkun</forename><surname>Linghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12871</idno>
		<title level="m">An embodied generalist agent in 3d world</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vid2robot: End-to-end videoconditioned policy learning with cross-attention transformers</title>
		<author>
			<persName><forename type="first">Vidhi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Attarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pannag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sanketi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12943</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bc-z: Zero-shot task generalization with robotic imitation learning</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohi</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.02005</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">237257594</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bignell</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">9953039</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:216078090" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-game decision transformers</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Mengjiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><surname>Michalewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27921" to="27936" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Homer</forename><surname>Rich Walke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyuan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishikaa</forename><surname>Lunawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Sieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.05941</idno>
		<title level="m">Evaluating real-world robot manipulation policies in simulation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Steve-1: A generative model for text-to-behavior in minecraft</title>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
		<idno>ArXiv, abs/2306.00937</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">258999563</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mcu: A task-centric framework for open-ended agent evaluation in minecraft</title>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08367</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Marten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.17172</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language conditioned imitation learning over unstructured data</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07648</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning latent plans from play</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohi</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1113" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning latent plans from play</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohi</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1113" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning latent plans from play</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohi</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1113" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive language: Talking to robots in real time</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayzaan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Zson: Zeroshot object-goal navigation using multimodal goal embeddings</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavika</forename><surname>Devnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno>ArXiv, abs/2206.12403</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:250048645" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">What matters in language conditioned robotic imitation learning over unstructured data</title>
		<author>
			<persName><forename type="first">Oier</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="11205" to="11212" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Kirkeby</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:205242740" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Goal representations for instruction following: A semi-supervised language interface to control</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Wang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Homer</forename><surname>Rich Walke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Hansen-Estruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-An</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Jalobeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kolobov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3894" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Octo: An open-source generalist robot policy</title>
		<author>
			<persName><forename type="first">Octo</forename><surname>Model Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dibya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Homer</forename><surname>Walke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeep</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Hejna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Liang Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Yunliang Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pannag</forename><surname>Sanketi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorsa</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>Delft, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Open x-embodiment: Robotic learning datasets and rt-x models</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Padalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acorn</forename><surname>Pooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Khazatsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anikait</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08864</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Conditional variational autoencoder for neural machine translation</title>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04405</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Episodic transformer for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pashevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15942" to="15952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accelerating reinforcement learning with learned skill priors</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngwoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Lim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Scaling instructable agents across many simulated worlds</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Abi Raad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catarina</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethanie</forename><surname>Brownfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Buttimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Cant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Chakera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10179</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:231591445" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">Jost Tobias Springenberg, et al. A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Habitat: A platform for embodied ai research</title>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9339" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Reinforcement learning upside down: Don&apos;t predict rewards -just map them to actions</title>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>ArXiv, abs/1912.02875</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">208857600</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.05997</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Xskill: Cross embodiment skill discovery</title>
		<author>
			<persName><forename type="first">Mengda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3536" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10897</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">204852201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:250626771" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Hierarchical task learning from language instructions with unified transformers and self-monitoring</title>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03427</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
