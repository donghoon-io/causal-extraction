<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Variational Inference in Discrete VAEs using Error Correcting Codes</title>
				<funder ref="#_qzrsQXC">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_cSezb7D">
					<orgName type="full">BBVA Foundation</orgName>
				</funder>
				<funder ref="#_gJWWhC7">
					<orgName type="full">Generación de Conocimiento</orgName>
				</funder>
				<funder ref="#_AA9ZuHE">
					<orgName type="full">ELLIS Unit Madrid (European Laboratory for Learning and Intelligent Systems)</orgName>
				</funder>
				<funder ref="#_vF6BDmc">
					<orgName type="full">Universidad Rey Juan Carlos</orgName>
				</funder>
				<funder ref="#_hgMmDWn">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_DskzkGr">
					<orgName type="full">Comunidad de Madrid</orgName>
				</funder>
				<funder ref="#_nwf9xem">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-06-10">10 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">María</forename><surname>Martínez-García</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grace</forename><surname>Villacrés</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Fuenlabrada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Fuenlabrada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Fuenlabrada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Fuenlabrada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Klipsch School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">New Mexico State University</orgName>
								<address>
									<settlement>Las Cruces</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Klipsch School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">New Mexico State University</orgName>
								<address>
									<settlement>Las Cruces</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Klipsch School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">New Mexico State University</orgName>
								<address>
									<settlement>Las Cruces</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Klipsch School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">New Mexico State University</orgName>
								<address>
									<settlement>Las Cruces</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><forename type="middle">M</forename><surname>Olmos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Instituto de Investigación Sanitaria Gregorio Marañón</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="institution">Instituto de Investigación Sanitaria Gregorio Marañón</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Instituto de Investigación Sanitaria Gregorio Marañón</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Dept. of Signal Theory and Communications</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<settlement>Leganés</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="institution">Instituto de Investigación Sanitaria Gregorio Marañón</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Variational Inference in Discrete VAEs using Error Correcting Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-10">10 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.07840v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite advances in deep probabilistic models, learning discrete latent representations remains challenging. This work introduces a novel method to improve inference in discrete Variational Autoencoders by reframing the inference problem through a generative perspective. We conceptualize the model as a communication system, and propose to leverage Error Correcting Codes (ECCs) to introduce redundancy in latent representations, allowing the variational posterior to produce more accurate estimates and reduce the variational gap. We present a proof-of-concept using a Discrete Variational Autoencoder with binary latent variables and low-complexity repetition codes, extending it to a hierarchical structure for disentangling global and local data features. Our approach significantly improves generation quality, data reconstruction, and uncertainty calibration, outperforming the uncoded models even when trained with tighter bounds such as the Importance Weighted Autoencoder objective. We also outline the properties that ECCs should possess to be effectively utilized for improved discrete variational inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Discrete latent space models represent data using a finite set of features, making them particularly well-suited for representing inherently discrete data modalities. Additionally, they provide benefits in terms of interpretability and computational efficiency compared to continuous representations <ref type="bibr" target="#b32">[Van Den Oord et al., 2017</ref><ref type="bibr" target="#b10">, Jang et al., 2017</ref><ref type="bibr">, Vahdat et al., 2018a]</ref>. Consequently, recent advancements in deep generative models have increasingly embraced discrete latent representations <ref type="bibr" target="#b20">[Razavi et al., 2019</ref><ref type="bibr" target="#b19">, Ramesh et al., 2021</ref><ref type="bibr" target="#b22">, Rombach et al., 2022]</ref>.</p><p>However, model training and inference of low-dimensional discrete latent representations remains technically challenging due to non-differentiability, which often requires the use of approximations that can lead to unstable optimization <ref type="bibr" target="#b10">[Jang et al., 2017</ref><ref type="bibr" target="#b12">, Kool et al., 2019]</ref>. Currently, Vector Quantized-Variational Autoencoders (VQ-VAEs) <ref type="bibr" target="#b32">[Van Den Oord et al., 2017</ref><ref type="bibr" target="#b20">, Razavi et al., 2019]</ref> stand out as state-of-the-art discrete latent variable models. They rely on a non-probabilistic encoder trained with a straight-through estimator, which lacks uncertainty quantification in the latent space. In contrast, fully probabilistic discrete Variational Autoencoders (VAEs) <ref type="bibr" target="#b11">[Kingma and Welling, 2014]</ref> employ continuous relaxations such as Gumbel-Softmax <ref type="bibr" target="#b10">[Jang et al., 2017]</ref> or Concrete <ref type="bibr" target="#b17">[Maddison et al., 2017]</ref> for differentiable sampling <ref type="bibr" target="#b19">[Ramesh et al., 2021</ref><ref type="bibr" target="#b16">, Lievin et al., 2020]</ref>. However, these methods can be unstable, as gradient variance is highly sensitive to the temperature parameter. The Discrete Variational Autoencoder (DVAE) framework <ref type="bibr" target="#b21">[Rolfe, 2016</ref><ref type="bibr">, Vahdat et al., 2018a,b]</ref> addresses this issue by augmenting binary latent representations with continuous variables, allowing for stable reparameterization after marginalizing the latent bits. In this setting, Boltzmann machines serve as discrete priors to enhance model flexibility, at the cost of degraded interpretability. This work introduces a novel approach to improve variational inference in discrete latent variable models, particularly discrete VAEs, by offering an alternative perspective on the inference problem. Rather than viewing VAEs as lossy compression methods employed to minimize data reconstruction error, we adopt a generative perspective, where inference seeks to recover the latent vector that generated a given observation. This allows us to conceptualize the generative model as a communication system, where we reinterpret latent vectors as messages transmitted through a nonlinear channel given by the VAE decoder, and then recovered by the VAE encoder. Based on this analogy, we propose to incorporate Error Correcting Codes (ECCs) to introduce redundancy into latent representations before they are processed by the decoder to generate data. Our approach builds on well-established techniques from digital communications and data storage, where information is protected using ECCs before transmission or storage to reduce the overall error rate during recovery <ref type="bibr" target="#b18">[Moon, 2005]</ref>. Notably, Shannon's landmark work demonstrated that properly designed error correction schemes can achieve arbitrarily low error rates <ref type="bibr" target="#b26">[Shannon, 1948]</ref>. We use ECCs to deterministically introduce controlled redundancy to the latent vectors, increasing their Hamming distance and creating a higher-dimensional but sparser space where only a subset of vectors are valid. As illustrated in Fig. <ref type="figure" target="#fig_0">1a</ref>, this increased separation between latent codes allows for some errors in inference while still enabling recovery of the correct latent code by identifying the closest valid vector. Hence, this structure facilitates error correction during inference.</p><p>The integration of ECCs into discrete latent variable models is a novel approach that remains compatible with existing training methods for discrete generative models. Note that, while deep learning has been widely explored in digital communications <ref type="bibr" target="#b8">[Guo et al., 2022</ref><ref type="bibr" target="#b34">, Wu et al., 2023</ref><ref type="bibr" target="#b27">, Shen et al., 2023</ref><ref type="bibr" target="#b37">, Ye et al., 2024</ref><ref type="bibr" target="#b2">, Chen et al., 2024]</ref>, the use of ECCs as a design tool in machine learning remains largely unexplored. This work introduces a new research direction, leveraging communications theory to enhance deep generative models.</p><p>In our experiments, we use a simplified version of DVAE++ <ref type="bibr" target="#b31">[Vahdat et al., 2018b]</ref> (referred to as uncoded DVAE) and show that the added redundancy enables a more accurate variational approximation to the true posterior, using simple independent priors. We highlight that error rates and variational gaps are linked through bounds derived from mismatch hypothesis testing, showing that minimizing the variational gap tightens the upper bound on the error rate <ref type="bibr" target="#b24">[Schlüter et al., 2013</ref><ref type="bibr" target="#b36">, Yang et al., 2024]</ref>. Across different datasets, our results indicate that the DVAE with ECCs (Coded-DVAE) leads to reduced error rates in inference, resulting in smaller variational gaps. This improvement translates into superior generation quality, improved data reconstruction, and critically calibrated uncertainty in the latent space. Notably, Fig. <ref type="figure" target="#fig_0">1b</ref> shows that Coded-DVAE achieves a significantly tighter training bounds even when the baseline is trained using the Importance Weighted Autoencoder (IWAE) objective <ref type="bibr" target="#b1">[Burda et al., 2016]</ref>.</p><p>Contributions. The primary contribution of this work is presenting a new perspective on the inference problem by framing the generative model as a communication system. We present proof-of-concept results that show how training discrete latent variable models can be improved by incorporating ECC techniques, an approach that, to the best of our knowledge, is novel in the existing literature. We then introduce a coded version of a discrete VAE, utilizing block repetition codes, and show that encoding and decoding can be done with linear complexity. Our results show that the Coded-DVAE model significantly reduces error rates during inference, leading to better data reconstruction, generation quality, and improved uncertainty calibration in the latent space. We further generalize this approach to other coding schemes and propose a hierarchical structure inspired by polar codes <ref type="bibr" target="#b0">[Arikan, 2009]</ref>, which effectively separates high-level information from finer details.<ref type="foot" target="#foot_0">foot_0</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OUR BASELINE: THE UNCODED DVAE</head><p>This section introduces a simplified version of the DVAE++ <ref type="bibr" target="#b31">[Vahdat et al., 2018b</ref>] (uncoded DVAE), which serves as the foundational model upon which the subsequent aspects of our work are constructed. A key advantage of this framework is its stable training process, facilitated by reparame-terization through smoothing transformations, in contrast to more unstable methods like Gumbel-Softmax <ref type="bibr" target="#b10">[Jang et al., 2017]</ref>. To ensure that any performance improvements in the Coded-VAE stem solely from the coding scheme rather than a complex prior, we adopt a simple independent prior. Let X = {x 0 , . . . , x N } denote a collection of unlabeled data, where x i ∈ R K represents a K-dimensional feature vector. While Rolfe <ref type="bibr">[2016]</ref>, <ref type="bibr" target="#b31">Vahdat et al. [2018b]</ref> and <ref type="bibr">Vahdat et al. [2018a]</ref> use Boltzmann machine priors, we consider a generative probabilistic model characterized by a simple low-dimensional binary latent variable m ∈ {0, 1} M comprising independent and identically distributed (i.i.d.) Bernoulli components p(m) = M j=1 p(m j ) = M j=1 Ber(ν), where ν = p(m j = 1). Since backpropagation through discrete samples is generally not possible, a smoothing transformation p(z|m) = M j=1 p(z j |m j ) of these binary variables is introduced. Following <ref type="bibr" target="#b31">Vahdat et al. [2018b]</ref>, we introduce truncated exponential distributions given by</p><formula xml:id="formula_0">p(z|m = 1) = e β(z-1) Z β , p(z|m = 0) = e -βz Z β ,<label>(1)</label></formula><p>for m ∈ {0, 1}, z ∈ [0, 1], and Z β = (1 -e -β )/β. The parameter β serves as an inverse temperature term, similar to the one in the Gumbel-Softmax relaxation <ref type="bibr" target="#b10">[Jang et al., 2017]</ref>. Given the simplicity of the defined binary prior, the complexity of the model is primarily determined by the likelihood function p θ (x|z) = p(f θ (z)), the decoder function, where the likelihood is a Neural Network (NN) (referred to as the decoder) with parameter set θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">VARIATIONAL FAMILY AND INFERENCE</head><p>Following Rolfe <ref type="bibr">[2016]</ref>, we assume an amortized variational family of the form q η (m, z|x) = q η (m|x)p(z|m), where</p><formula xml:id="formula_1">q η (m|x) = M j=1 Ber(m j ; q j ) = M j=1 Ber(m j ; g j,η (x)) (2)</formula><p>where Ber(m j ; q j ) is a Bernoulli with parameter q j = q η (m j = 1|x) and g η (x) is a NN (referred to as the data encoder) with parameter set η. Inference is achieved by maximizing the Evidence Lower Bound (ELBO), which can be expressed as</p><formula xml:id="formula_2">log p(x) ≥ q η (m, z|x) log p θ (x, z, m) q η (m, z) dmdz = E qη(m,z|x) log p θ (x|z) -D KL q η (m|x)||p(m) ,<label>(3)</label></formula><p>where the first term corresponds to the reconstruction of the observed data and the second is the Kullback-Leibler (KL) Divergence between the variational family and the prior distribution, which acts as a regularization term. This can be computed in closed form as D KL q η (m|x)||p(m) = M j=1 q j log qj ν +(1-q j ) log 1-qj 1-ν , with ν = p(m j = 1). Since p θ (x|z) does not depend on the binary latent variable m, we can marginalize the posterior distribution as</p><formula xml:id="formula_3">q η (z|x) = M j=1 q η (z j |x), q η (z j |x) = 1 k=0 q η (m j = k|x)p(z j |m j = k). (4)</formula><p>As shown in <ref type="bibr" target="#b31">Vahdat et al. [2018b]</ref>, the corresponding inverse Cumulative Density Function (CDF) is given by</p><formula xml:id="formula_4">F -1 qη(zj |x) (ρ) = - 1 β log -b + √ b 2 -4c 2 ,<label>(5)</label></formula><p>where b = ρ + e -β (q j -ρ) /(1 -q j ) -1 and c = -[q j e -β ]/(1 -q j ). Equation ( <ref type="formula" target="#formula_4">5</ref>) defines a differentiable function that converts a sample ρ from an independent uniform distribution U(0, 1) into a sample from q η (z|x). Thus, we can apply the reparameterization trick to sample from the latent variable z and optimize the ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ON THE VARIATIONAL GAP AND THE INFERENCE ERROR</head><p>While VAEs are typically viewed as lossy compression methods, this work introduces an alternative perspective on the inference problem from a generative viewpoint: we first sample a latent vector m, generate an observation x, and aim to minimize the error rate when recovering m from x. This requires approximating the true, unknown posterior distribution p θ (m|x) with a sufficiently accurate proposed distribution q η (m|x). The gap between q η (m|x) and p θ (m|x) is precisely the variational gap, which can be related to the error rate in inference, i.e. comparing the true m with samples drawn from q η (m|x).</p><p>In statistical classification, particularly in the context of multiple hypothesis testing, classification error is the primary performance metric. Bayes' decision rule (also known as Bayes' test) minimizes classification error, assuming the true probability distributions governing the classification problem are known. In the case of model mismatch, e.g., when a posterior distribution is learned using variational inference, statistical bounds can be used to relate the classification error and the model estimation error <ref type="bibr" target="#b24">[Schlüter et al., 2013</ref><ref type="bibr" target="#b36">, Yang et al., 2024]</ref>. In particular, for a generative model x ∼ p θ (x|m) with discrete latent vector m, let 1 -A θ be the error rate when estimating m from x using the true posterior p θ (m|x) and 1 -A η be the error rate using the variational approximate posterior q η (m|x). Then, ∆ . = A θ -A η can be bounded as</p><formula xml:id="formula_5">0 ≤ ∆ 2 ≤ 1 -e -2D KL (qη(m,x)||p θ (m,x)) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">D KL (q η (m, x)||p θ (m, x)) = E p(x) [D KL (q η (m|x)||p θ (m|x))]</formula><p>is the variational gap. For fixed θ (and thus for fixed A θ ), any improvement in η that reduces the expected variational gap also tightens an upper bound on ∆</p><formula xml:id="formula_7">. = A θ -A η .</formula><p>In fields where reliable data transmission or storage is crucial, introducing ECCs is a well-established approach to reduce the error rate when estimating a discrete source m transmitted through a noisy channel with output x. Building on this, we propose using ECCs to protect m with controlled redundancy that the variational posterior q η (m|x) can leverage. This way, it is possible to reduce error rates in inference and refine the variational parameters η on the enhanced model, thus narrowing the variational gap. Notably, according to (6), a smaller variational gap results in a tighter upper bound on the error rates. In the following sections, we show that simple coding schemes enhance inference, improving generation, reconstruction, log-likelihoods (LLs), and uncertainty calibration in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CODED-DVAE</head><formula xml:id="formula_8">x i z i c i m i N Figure 3: Graph- ical model of the Coded-DVAE.</formula><p>This section extends the previously described DVAE, introducing an ECC over m. We refer to this model as Coded-DVAE. The corresponding graphical model is shown in Fig. <ref type="figure">3</ref>, where the diamond node represents a deterministic transformation.</p><p>In ECCs, we augment the dimensionality of the binary latent space from M to D, introducing redundancy in a controlled and deterministic manner, where R = M/D is the coding rate. An ECC is designed to maximize the separation between the 2 M possible codewords within the D-bit binary space, enabling efficient error correction by searching for the nearest valid codeword (see Fig. <ref type="figure" target="#fig_0">1a</ref>). A random selection of codewords results in random block codes <ref type="bibr" target="#b26">[Shannon, 1948]</ref>, which are very robust and amenable to theoretical analysis. However, their lack of structure makes them impractical, as encoding and decoding require exhaustive codeword enumeration. Appendix N provides a detailed formulation of the encoding and decoding process for a random code within the Coded-DVAE.</p><p>We adopt a much simpler linear coding scheme, namely repetition codes, where each bit of the original message m (i.e., each information bit) is duplicated multiple times to form the encoded message c. Intuitively, the more times an information bit is repeated, the better it is protected.</p><p>Our experiments consider uniform (M, D) repetition codes where all bits are repeated L times, resulting in codewords of dimension D = M L and a coding rate of R = 1/L. Note that repetition codes represent a special case of linear ECCs since each codeword can be computed by multiplying a binary vector m by an M ×D generator matrix G, such that c = m T G, where k-th row, with k = 1, . . . , M , has entries equal to one at columns L(k -1) + 1, L(k -1) + 2, . . . , Lk, and zero elsewhere. For example, for M = 3 and L = 2, the generator matrix of the (3, 6) repetition code is</p><formula xml:id="formula_9">G =   1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1   .<label>(7)</label></formula><p>The generative process of the Coded-DVAE is similar to the uncoded version and is illustrated in Fig. <ref type="figure">3</ref>. We assume the same prior distribution p(m), but in this case the samples m are deterministically encoded using G. Consequently, the smoothing transformations are now defined over c</p><formula xml:id="formula_10">p(z|c) = D j=1 p(z j |c j ),<label>(8)</label></formula><p>with p(z j |c j ) following Eq. ( <ref type="formula" target="#formula_0">1</ref>). The likelihood p(x|z) is again of the form p θ (x|z) = p(f θ (z)). Compared to the uncoded case, the input dimensionality to the decoder f θ (z) is L times larger due to the introduced redundancy. Therefore, the overall architecture of the decoder (detailed in Appendix C) remains unchanged, except for the first Multilayer Perceptron (MLP) layer that processes the input z. When using a repetition code with rate R = 1/L, the additional parameters of f θ (z) amount to (L -1) × h, where h is the dimension of the first hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VARIATIONAL FAMILY AND INFERENCE</head><p>Since c is a deterministic transformation of m, its randomness is entirely determined by m. Therefore, following the recognition model described in Fig. <ref type="figure">3</ref>, we assume a variational family factorizing as</p><formula xml:id="formula_11">q η (m, z|x) = q η (m|x)p(z|c),<label>(9)</label></formula><p>where q η (m|x) = M j=1 q η (m j |x) is computed in two steps. First, we construct an encoder g η (x) similar to the one in (2), which estimates c from x</p><formula xml:id="formula_12">q u η (c|x) = D j=1 q u η (c j |x) = D j=1</formula><p>Ber(c j ; g j,η (x)), (10)  where the superscript u indicates that this posterior does not utilize the ECC (uncoded). Now, we leverage the known redundancy introduced by the ECC to constrain the solution of q u η (c|x), given that each bit from m has been repeated L times to create c. To do so, we follow a soft decoding approach, where the marginal posteriors of the information bits are derived from the marginal posteriors of the encoded bits, exploiting the code's known structure. In the case of repetition codes, we compute the all-are-zero and the allare-ones products of probabilities of the bits in c that are copies of the same message bit and re-normalize as</p><formula xml:id="formula_13">q(m k = 1|x) = 1 Z Lk j=L(k-1)+1 q u η (c j = 1|x), q(m k = 0|x) = 1 Z Lk j=L(k-1)+1 q u η (c j = 0|x),<label>(11)</label></formula><p>for k = 1, . . . , M and Z is the normalization constant. For the M = 3, L = 2 toy example in (7)</p><formula xml:id="formula_14">q η (m 1 = 1|x) ∝ q u η (c 1 = 1|x)q u η (c 2 = 1|x), q η (m 2 = 1|x) ∝ q u η (c 3 = 1|x)q u η (c 4 = 1|x), q η (m 3 = 1|x) ∝ q u η (c 5 = 1|x)q u η (c 6 = 1|x).</formula><p>This approach can be seen as a soft majority voting strategy. All operations in (11) are differentiable and implemented in the log domain. We consider the same architecture for the encoder g η (x) in (10) as in the uncoded case, with the only difference being the final MLP layer. The additional overhead in the coded case requires (L -1) × h ′ parameters in the last layer, where h ′ is the output dimension. In Appendix K.2, we conduct an ablation study on the number of trainable parameters to show that performance gains do not stem from the increased parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EFFICIENT REPARAMETERIZATION</head><p>Given the variational family in (9), the ELBO remains unchanged from (3). However, the reparameterization trick in (5) assumes independent bits, which does not hold for c. To address this issue efficiently during training, we adopt a soft encoding approach. We compute a marginal probability for each bit in c, leveraging the ECC structure and the marginal posterior probabilities of the information bits in m. For a repetition code, this involves simply replicating the posterior probabilities q η (m k |x), k = 1, . . . , M , for each copy of the same information bit<ref type="foot" target="#foot_1">foot_1</ref> .Hence, we treat the bits in c as independent but distributed according to q η (m|x). The training algorithm can be found in Appendix B.</p><p>When marginalizing c using the soft encoding marginals, we disregard potential correlations between the coded bits. Consequently, sampling from the marginals may produce an invalid codeword. However, since we do not sample the coded bits during training but instead propagate their marginal probabilities, we consider this method to have minimal negative impact. In fact, it can be seen as a form of probabilistic dropout, which enhances robustness during training. Since encoded words may contain inconsistent bits, the decoder learns to utilize the correlated inputs from repeated bits rather than disregarding them. Remark that when sampling from the generative model in test time, we use hard bits encoded into valid codewords, yielding visually appealing samples, indicating effective training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>This section empirically evaluates the Coded-DVAE with repetition codes and its uncoded counterpart on reconstruc-  Observe that more details are visualized as we increase the number of bits in the latent space and decrease the coding rate. The table at the right includes reconstruction metrics. Acc is the semantic accuracy, and Conf. Acc the confident semantic accuracy. Entropy is the average entropy of q η (m|x) in the test set.</p><p>tion and generation tasks. In particular, we present results for MNIST <ref type="bibr">[Deng, 2012]</ref>, FMNIST <ref type="bibr" target="#b35">[Xiao et al., 2017]</ref>, CI-FAR10 <ref type="bibr" target="#b13">[Krizhevsky et al., 2009]</ref>, and Tiny ImageNet <ref type="bibr" target="#b15">[Le and Yang, 2015]</ref>. Additionally, we compare the coded model to the uncoded DVAE trained with the IWAE objective (see Fig. <ref type="figure" target="#fig_0">1</ref> and Appendix H). All experiments use a fixed architecture (see Appendix C), modifying the encoder's output and decoder's input to accommodate the repetition code. We identify the models based on the number of information bits for uncoded models and the code rate for coded models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GENERATION</head><p>We first evaluate the models using the image generation task. Fig. <ref type="figure" target="#fig_2">4</ref> presents examples of randomly generated images using various model configurations on FMNIST. Additional results for other datasets are provided in the Supplementary Material. All models produce more detailed and diverse images as the number of information bits increases. However, when the number of latent vectors becomes too high for the dataset's complexity, some codebook entries remain unspecialized during training. This results in generation artifacts, where images contain overlapping features from different object classes. A visual inspection of Fig. <ref type="figure" target="#fig_2">4</ref> suggests that such artifacts occur more frequently in the uncoded model.</p><p>Error metrics. The effectiveness of the ECC-based inference can be assessed by generating images and measuring errors in recovering m by either drawing one sample from the variational posterior q η (m|x) or using the Maximum a Posteriori (MAP) estimate. As shown in the table included in Fig. <ref type="figure" target="#fig_2">4</ref>, the coded models significantly reduce both Bit Error Rate (BER) and Word Error Rate (WER) compared to the uncoded case for the same number of latent bits. Note also that the error rates grow with the number of latent bits, which is expected due to the increased complexity of the inference process.</p><p>Log-likelihood (LL). Aditionally, we estimated the data LL using importance sampling with 300 samples per observation. As shown in the table included in Fig. <ref type="figure" target="#fig_2">4</ref>, coded models consistently outperform their uncoded counterparts on both train and test sets, aligning with the rest of the results. LL values generally improve with more information bits, reflecting increased model flexibility. However, reducing the code rate does not enhance LL, suggesting possible decoder overfitting, where reconstruction improves, but LL declines. This may be due to the use of feed-forward networks at the decoder's input, which might not fully capture the correlations in coded words (see Appendix L for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RECONSTRUCTION</head><p>We also assess the model's performance in image reconstruction. In the table in Fig. <ref type="figure" target="#fig_3">5</ref>, we report the Peak Signal-To-Noise Ratio (PSNR) of reconstructions on the FMNIST test set, calculated as</p><formula xml:id="formula_15">PSNR = 20 log max(x i,j ) RMSE(x, x ′ ) ,<label>(12)</label></formula><p>where max(x i,j ) is the highest pixel value, RMSE represents the Root Mean Square Error, and x ′ the reconstructed image. Results for the other datasets can be found in the Supplementary Material. In all cases, coded models outperform their uncoded counterparts in reconstruction quality, as indicated by higher PSNR values, a trend also visible in Fig. <ref type="figure" target="#fig_3">5</ref>. As the number of information bits increases, PSNR improves, reflecting greater model flexibility to capture data structure. We also observe improved PSNR with reduced code rates, as more redundancy is added. However, this increased redundancy does not enhance the model's flexibility, which is determined by the number of information bits.</p><p>Semantic Accuracy. Since the PSNR operates at the pixel level, it does not account for the semantic errors committed by the model. To address this, we evaluate reconstruction accuracy in Fig. <ref type="figure" target="#fig_3">5</ref>, checking whether the model reconstructs images within the correct semantic class (type of clothing in FMNIST). For this, we trained an image classifier for each dataset and compared the predicted labels of the reconstructed images with the ground truth labels. We also report a confident reconstruction accuracy, which is calculated by considering only the images projected into a latent vector with posterior probability above 0.4 3 . Results indicate that incorporating an ECC enables the model to produce latent spaces that better capture the semantics of the data.</p><p>Posterior Uncertainty Calibration. Finally, we report the average entropy of the variational posterior q η (m|x) over the test set in the table included in Fig. <ref type="figure" target="#fig_3">5</ref>. The low entropy in the uncoded models indicates low uncertainty when the model maps data points into the latent space, which could be beneficial if the model consistently assigns high probability to the correct latent vectors. However, the semantic accuracy results show that this is not the case for the uncoded model, as it often projects images into vectors corresponding to the wrong semantic class with high confidence.</p><p>Coded models, in contrast, improve semantic accuracy and exhibit higher entropy. This indicates that (i  demonstrates significant uncertainty (high entropy) for images where the class is not well identified. This is illustrated in Fig. <ref type="figure">6</ref>, where we show images with class reconstruction errors caused by the MAP latent word from q η (m|x). The reconstructions of the three most probable latent vectors, along with their respective probabilities, are displayed. Notably, the uncoded model shows high confidence regardless of the reconstruction result, while the coded posterior shows greater uncertainty. Note that this does not imply an uninformative distribution, as information bits remain recoverable. Additionally, increasing the number of latent bits does not lead to an excessive increase in entropy, despite the exponential growth in the number of vectors.</p><formula xml:id="formula_16">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS ON CIFAR10 AND IMAGENET</head><p>Since MNIST-like datasets are relatively simple, it is challenging to fully assess the benefits of introducing ECCs in our model. To address this, we present additional results on more complex datasets, CIFAR10 and Tiny ImageNet, which feature colored images with more intricate shapes, patterns, and greater diversity. We trained both uncoded and coded models with different configurations to better understand the impact of the ECC. For context, the DVAE++ model <ref type="bibr" target="#b31">[Vahdat et al., 2018b]</ref> required 128 binary latent variables to achieve state-of-the-art performance on these datasets, using a more complex model with Boltzmann Machine priors. In Fig. <ref type="figure">7</ref>, we show reconstruction examples, and in Fig. <ref type="figure" target="#fig_6">8</ref>, we present randomly generated images. Additional results are available in Appendices F and G. The results align with those from previous sections, but the performance difference is even more pronounced in this case. We find that the uncoded DVAE struggles to decouple spatial information in the images, while the Coded-DVAE shows particular promise for learning low-dimensional discrete latent representations, even for complex datasets. It is important to note that we used a simple architecture to isolate the gains achieved purely through the introduction of ECCs in the latent space, keeping a simple independent prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">BEYOND REPETITION CODES</head><p>We have presented compelling proof-of-concept results that incorporating ECCs, like repetition codes, into DVAEs can improve performance. We believe this opens a new path for designing latent probabilistic models with discrete latent variables. Although a detailed analysis of the joint design of ECC and encoder-decoder networks is beyond the scope of this work, in Appendix I we outline key properties that any ECCs must satisfy to be integrated within this framework.</p><p>Drawing inspiration from polar codes <ref type="bibr" target="#b0">[Arikan, 2009]</ref>, we introduce a hierarchical Coded-DVAE with two layers. The graphical model is shown in Fig. <ref type="figure" target="#fig_7">9</ref>. In this model, the latent bits m 1 are encoded using a repetition code in the first layer, producing c 1 and z 1 . Simultaneously, the vector m 2 is linearly combined with m 1 using modulo 2 operations m 1 ⊕m 2 and then encoded using a repetition code, yielding c 2 and z 2 . Both soft vectors are concatenated and fed to the decoder to generate x. Since m 1 appears in both generative branches, it receives stronger protection. Inference follows a similar approach to the Coded-DVAE, incorporating the linear combination of m 1 and m 2 used in the second branch. This hierarchical structure allows the model to separate high-level information from finer details, as we show in the results presented in Appendix J. The bits in m 1 offer stronger protection, allowing the model to encode the semantic information (type of clothing in FMNIST). As a result, during inference, it is more likely to project the image into a latent vector m 1 that represents the correct semantic class. In contrast, m 2 provides weaker protection and encodes image details that are less crucial for reconstruction. Fig. <ref type="figure" target="#fig_7">9</ref> demonstrates that by keeping m 1 fixed (semantic information) and sampling m 2 randomly (fine details) we can generate diverse images within the same semantic class. Furthermore, our work reveals numerous avenues for future research. Firstly, investigating architectures capable of efficiently utilizing the correlations and structure introduced by the ECCs, in contrast to the feed-forward networks employed in this study. We also contemplate exploring more complex and robust coding schemes, conducting theoretical analyses aligned with Shannon's channel capacity and mutual information to determine the fundamental parameters of the ECC needed to achieve reliable inference, exploring different modulations, and integrating these concepts into state-of-the-art models based on discrete representations.</p><formula xml:id="formula_17">x i z 1 z 2 c 1 c 2 m 1 m 2 m 1 = [1 0 1 1 0] m 1 = [1 1 1 0 0] m 1 = [0 1 1 0 1] m 1 = [0 0 1 1 1] m 1 = [1 1 1 1 1] m 1 = [0 0 1 0 0] m 1 = [1 0 1 1 0] m 1 = [1 1 1 0 0] m 1 = [0 1 1 0 1] m 1 = [0 0 1 1 1] m 1 = [1 1 1 1 1] m 1 = [0 0 1 0 0] m 1 = [1 0 1 1 0] m 1 = [1 1 1 0 0] m 1 = [0 1 1 0 1] m 1 = [0 0 1 1 1] m 1 = [1 1 1 1 1] m 1 = [0 0 1 0 0]</formula><p>The following Appendices offer further details on the model architecture, implementation, and experimental setup. They also include additional results on the FMNIST <ref type="bibr" target="#b35">[Xiao et al., 2017]</ref>, MNIST <ref type="bibr">[Deng, 2012]</ref>, CIFAR10 <ref type="bibr" target="#b13">[Krizhevsky et al., 2009]</ref>, and Tiny ImageNet <ref type="bibr" target="#b15">[Le and Yang, 2015]</ref> datasets, along with comparisons to uncoded models trained with the IWAE objective <ref type="bibr" target="#b1">[Burda et al., 2016]</ref>, and a description of the hierarchical Coded-DVAE. Given the length of the material, we have included a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A UNCODED TRAINING ALGORITHM</head><p>The following pseudo-code describes the training process for the uncoded DVAE. It's important to note that the main difference from the training of the Coded-DVAE lies in the fact that the encoder directly outputs q u η (m|x i ), which is used to sample z. Therefore, we skip the soft decoding and coding steps.</p><p>Algorithm 1 Training the model with uncoded inference.</p><p>1: Input: training data x i . 2: repeat 3:</p><formula xml:id="formula_18">q u η (m|x i ) ← forward encoder g η (x i ) 4: z ← sample from (5) 5: p θ (x|z) ← forward decoder f θ (z) 6:</formula><p>Compute ELBO according to (3) 7:</p><p>θ, η ← U pdate(ELBO) 8: until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CODED TRAINING ALGORITHM</head><p>The following pseudo-code describes the training process for the Coded-DVAE. Here, we utilize soft decoding to leverage the added redundancy and retrieve the marginal posteriors of the information bits m, correcting potential errors in q u η (c|x i ). We then apply the soft encoding technique to incorporate the structure of the code and sample z using the reparameterization trick as described in (5). </p><formula xml:id="formula_19">q u η (c|x i ) ← forward encoder g η (x i ) 4:</formula><p>q η (m|x i ) ← soft decoding by aggregating q u η (c|x i ) according to (11) 5:</p><p>q η (c|x i ) ← repeat posterior bit probabilities q η (m|x i ) according to G 6:</p><p>z ← sample from (5) 7:</p><formula xml:id="formula_20">p θ (x|z) ← forward decoder f θ (z) 8:</formula><p>Compute ELBO according to (3) 9:</p><p>θ, η ← U pdate(ELBO) 10: until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ARCHITECTURE</head><p>In this section, we detail the architecture used to obtain the experimental results with FMNIST and MNIST (28x28 gray-scale images). Note that, across the experiments, we only modify the output layer of the encoder and the input layer of the decoder to adapt to the different configurations of the model. This modification leads to a minimal alteration in the total number of parameters. In Section K.2, we conduct an ablation study on the number of trainable parameters to show that the enhancement in performance is not attributed to the increased dimensionality introduced by redundancy.</p><p>For the additional CIFAR10 experiments, we change the input of the encoder and the output of the decoder to process the 32x32 color images. For the Tiny ImageNet experiments, we do the same to process 64x64 color images. The rest of the architecture remains unchanged. These architectures are comprehensively described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ENCODER</head><p>The encoder NN consists of 3 convolutional layers followed by two fully connected layers. We employed Leaky ReLU as the intermediate activation function and a Sigmoid as the output activation, since the encoder outputs bit probabilities. The full architecture is detailed in Fig. <ref type="figure" target="#fig_9">10</ref>.  </p><formula xml:id="formula_21">in = 1 out = 64 k = (3,3) p = 1 s = 1 conv 1 in = 64 out = 128 k = (2,2) p = 0 s = 2 conv 2 in = 128 out = 256 k = (2,2) p = 0 s = 2 conv 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 DECODER</head><p>The decoder architecture is inspired by the one proposed in <ref type="bibr" target="#b25">Schuster and Krogh [2023]</ref>. It is composed of two fully connected layers, followed by transposed convolutional layers with residual connections and Squeeze-and-Excitation (SE) layers <ref type="bibr" target="#b9">[Hu et al., 2018]</ref>. We employed Leaky ReLU as the intermediate activation function and a Sigmoid as output activation, given that we consider datasets with gray-scale images. The complete architecture is detailed in Fig. <ref type="figure" target="#fig_11">11</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D FMNIST RESULTS</head><p>In this section, we present supplementary results obtained with the FMNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 TRAINING</head><p>We present the evolution of the ELBO and its terms throughout the training process. The models were trained for 200 epochs using an Adam optimizer with a learning rate of 10 -4 , and a batch size of 128. Fig. <ref type="figure" target="#fig_12">12</ref> displays the results for configurations with 5 information bits, Fig. <ref type="figure" target="#fig_13">13</ref> for 8 information bits, and Fig. <ref type="figure" target="#fig_14">14</ref> for 10 information bits. The colors in all plots represent the various code rates. Across all cases, coded models achieve superior bounds. The main differences in the ELBO come from the different performances in reconstruction. As we have observed in the different experiments, coded models are capable of generating more detailed images and accurate reconstructions. Introducing the repetition code also leads to smaller KL values, indicating that the posterior latent features are potentially disentangled and less correlated.</p><p>We observe that, as we decrease the code rate, we obtain better bounds in general. Adding redundancy does not increase the model's flexibility, since the information bits determine the number of latent vectors. However, the introduction of ECCs in the model allows for latent spaces that better capture the structure of the images while employing the same number of latent vectors.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 RECONSTRUCTION AND GENERATION</head><p>In this section, we augment the results presented in the main text, including outcomes obtained with the training dataset in Table <ref type="table" target="#tab_4">1</ref>. We include again the results obtained in the test to facilitate comparison. Additionally, we assess reconstruction quality using the Structural Similarity Index Measure (SSIM) to provide a more comprehensive evaluation. The results remain consistent across the two data partitions, and the analysis conducted for the test set also applies to training data.</p><p>In all the cases, the coded models yield higher PSNR and SSIM values than their uncoded counterparts, indicating a superior performance in reconstruction. We observe a general improvement in PSNR and SSIM as we increase the number of information bits (i.e., as we augment the latent dimensionality of the model) and decrease the code rate (i.e., as we introduce more redundancy).</p><p>As we discussed in the main text, the reconstruction metrics such as the PSNR and SSIM do not account for the semantic errors committed by the model. Therefore, we additionally report the semantic accuracy and the confident semantic accuracy. While the reconstruction accuracy is computed across the entire dataset partitions, for the confident accuracy, we only consider those images projected into a latent vector with a probability exceeding 0.4. We observe that coded models better capture the semantics of the images while employing the same number of latent vectors, significantly outperforming the uncoded models in terms of accuracy in all the cases.  In Fig. <ref type="figure" target="#fig_15">15</ref>, we include additional examples of randomly generated images using different model configurations. We observe that coded models can generate more detailed and diverse images than their uncoded counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MNIST RESULTS</head><p>In this section, we report the results obtained with the MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 TRAINING</head><p>We present the evolution of the ELBO and its terms throughout the training process. The models were trained for 100 epochs using an Adam optimizer with a learning rate of 10 -4 , and a batch size of 128. Fig. <ref type="figure" target="#fig_16">16</ref> displays the results for configurations with 5 information bits, Fig. <ref type="figure" target="#fig_17">17</ref> for 8 information bits, and Fig. <ref type="figure" target="#fig_18">18</ref> for 10 information bits. The colors in all plots represent the various code rates.</p><p>The results are consistent with the ones obtained for FMNIST. Across all the configurations, coded models achieve superior bounds. The main differences in the ELBO come from the different performances in reconstruction. As we have observed   We observe that, as we decrease the code rate, we obtain better bounds in general. Adding redundancy does not increase the model's flexibility, since the information bits determine the number of latent vectors. However, the introduction of ECCs in the model allows for latent spaces that better capture the structure of the images while employing the same number of latent vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 RECONSTRUCTION</head><p>We first evaluate the model's performance in reconstructing data by examining its uncoded and coded versions across different configurations, varying the number of information bits and code rates. All the results obtained with MNIST are consistent with those presented in the main text for FMNIST.</p><p>In Table <ref type="table" target="#tab_5">2</ref> we quantify the quality of the reconstructions measuring the PSNR and the SSIM in both training and test sets. In all the cases, coded models yield higher PSNR and SSIM values, indicating a superior performance in reconstruction. This improvement is also evident through visual inspection of Fig. <ref type="figure" target="#fig_7">19</ref>, where the coded models better capture the details in the images. Similar to the results observed on FMNIST, we find that both PSNR and SSIM generally improve as the number of  As we discussed in the main text, reconstruction metrics such as PSNR and SSIM do not account for the semantic errors committed by the model. Therefore, we additionally evaluate the reconstruction accuracy, ensuring that the model successfully reconstructs images within the same class as the original ones. We also provide a confident reconstruction accuracy, for which we do not count errors when the MAP value of q(m|x) is below 0.4. In light of the results, we argue that introducing ECCs in the model allows for latent spaces that better capture the semantics of the images while employing the same number of latent vectors, outperforming the uncoded models in all the cases.</p><p>We also report the average entropy of the variational posterior over the test set in Table <ref type="table" target="#tab_5">2</ref>. If we consider the entropy together with the semantic accuracy, we can argue that coded VAE is aware that multiple vectors might be related to the same image class, and that the posterior shows larger uncertainties for images for which the model has not properly identified the class. We illustrate this argument in Fig. <ref type="figure" target="#fig_8">20</ref>, where we show some images selected so that the MAP latent word of q(m|x) induces class reconstruction errors. We show the reconstruction of the 4 most probable latent vectors and their corresponding probabilities. Observe that the uncoded model is confident no matter the reconstruction outcome, while in the coded posterior, the uncertainty is much larger.</p><p>Table <ref type="table" target="#tab_7">4</ref> shows the log-likelihood values obtained for the MNIST dataset with various model configurations. Coded models consistently outperform their uncoded counterparts for both the training and test sets, consistent with the findings observed using the FMNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 GENERATION</head><p>In this section, we evaluate the model in the image generation task. In Fig. <ref type="figure" target="#fig_20">21</ref>, we show examples of randomly generated images using different model configurations in MNIST. These results are consistent with the ones obtained in reconstruction, and with the ones obtained for FMNIST, where we observe that the coded models can generate more detailed and diverse images.</p><p>The improved inference provided by the repetition code can also be tested by generating images using the generative model and counting errors using the MAP solution of the variational posterior distribution, as well as sampling from the approximate posterior distribution. Table <ref type="table" target="#tab_6">3</ref> reports the BER and WER for MNIST. Remarkably, for the same number of latent bits, coded models reduce both the BER and WER w.r.t. the uncoded case. Note also that the error rates grow with the number of latent bits, but this is expected due to the increased complexity of the inference process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F CIFAR10 RESULTS</head><p>In this section, we provide additional results using the CIFAR10 dataset with different model configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 TRAINING</head><p>We present the evolution of the ELBO and its terms throughout the training process. The models were trained for 300 epochs using Adam optimizer with a learning rate of 10 -4 , and a batch size of 128. Fig. <ref type="figure" target="#fig_21">22</ref> displays the results for configurations with 70 information bits, Fig. <ref type="figure" target="#fig_22">23</ref> for 100 information bits, and Fig. <ref type="figure" target="#fig_23">24</ref> for 130 information bits. The colors in all plots represent the various code rates.</p><p>Across all the configurations, coded models achieve superior bounds. The main differences in the ELBO come from the different performances in reconstruction. As we have observed across the different experiments, coded models are capable of better capturing the structure of the data, generating more detailed images and accurate reconstructions.</p><p>In the coded case, we do not observe significant differences in the obtained bounds as we increase the number of information bits and reduce the code rate. However, the difference is notable if we compare the coded and uncoded models. Adding redundancy does not increase the model's flexibility, since the information bits determine the number of latent vectors.   However, the introduction of ECCs in the model allows for latent spaces that better capture the structure of the images while employing the same number of latent vectors.</p><p>It's important to note that we are currently using feed-forward networks at the decoder's input. However, this approach may not be suitable for the correlations present in our coded words. Utilizing an architecture capable of effectively leveraging these correlations among the coded bits could potentially enable us to better exploit the introduced redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 RECONSTRUCTION AND GENERATION</head><p>We first evaluate the model's performance in reconstructing data by examining its uncoded and coded versions across different configurations, varying the number of information bits and code rates.</p><p>In Table <ref type="table" target="#tab_8">5</ref> we quantify the quality of the reconstructions measuring the PSNR and the SSIM in both training and test sets. Coded models yield higher PSNR and SSIM values in train, and similar values in test, although the coded models with lower rates outperform the rest of the configurations. However, the improvement in reconstruction is evident through visual inspection of Fig. <ref type="figure" target="#fig_8">25</ref>, where the coded models better capture the details in the images. We observe that the coded model produces images that better resemble the structure of the dataset, while the uncoded DVAE cannot decouple spatial information from the images and project it in the latent space. We hypothesize that, to adequately model complex images, transitioning to a hierarchical structure may be necessary. This would allow for explicit modeling of both global and local information. However, despite employing this rather simple model, we observe that coded configurations outperform their uncoded counterparts in capturing colors and textures.</p><p>We also evaluate the model in the image generation task. In Fig. <ref type="figure" target="#fig_8">26</ref>, we show examples of randomly generated images using different model configurations in CIFAR10. These results are consistent with the ones obtained in reconstruction, as we observe that the coded models can generate more detailed and diverse images. Additionally, we obtained the Fréchet Inception Distance (FID) score using the test set and 10k generated samples. For this, we used the implementation available at <ref type="url" target="https://github.com/mseitzer/pytorch-fid">https://github.com/mseitzer/pytorch-fid</ref>. We can observe that the coded models significantly reduced the FID score in all the cases compared to their uncoded counterparts. For the coded models, we do not observe a clear influence of the code rate on the quality of the generations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G TINY IMAGENET RESULTS</head><p>In this section, we provide additional results using the Tiny ImageNet dataset with different model configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 TRAINING</head><p>We present the evolution of the ELBO and its terms throughout the training process. The models were trained for 300 epochs using an Adam optimizer with a learning rate of 10 -4 , and a batch size of 128. Fig. <ref type="figure" target="#fig_25">27</ref> displays the results for configurations with 70 information bits, Fig. <ref type="figure" target="#fig_26">28</ref> for 100 information bits, and Fig. <ref type="figure" target="#fig_27">29</ref> for 130 information bits. The colors in all plots represent the various code rates.</p><p>As in the rest of the datasets, coded models achieve superior bounds across all the configurations. The main differences in the ELBO come from the different performances in reconstruction. As we have observed across the different experiments, coded models are capable of better capturing the structure of the data, generating more detailed images and accurate reconstructions.   In the coded case, we do not observe significant differences in the obtained bounds as we increase the number of information bits and reduce the code rate. However, the difference is notable if we compare the coded and uncoded models. Adding redundancy does not increase the model's flexibility, since the information bits determine the number of latent vectors. However, the introduction of ECCs in the model allows for latent spaces that better capture the structure of the images while employing the same number of latent vectors.</p><p>It's important to note that we are currently using feed-forward networks at the decoder's input. However, this approach may not be suitable for the correlations present in our coded words. Utilizing an architecture capable of effectively leveraging these correlations among the coded bits could potentially enable us to better exploit the introduced redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 RECONSTRUCTION AND GENERATION</head><p>We first evaluate the model's performance in reconstructing data by examining its uncoded and coded versions across different configurations, varying the number of information bits and code rates.</p><p>In Table <ref type="table" target="#tab_10">7</ref>, we quantify the quality of the reconstructions measuring the PSNR and the SSIM in both training and test sets. Coded models yield higher PSNR and SSIM values in train, and similar values in test, although coded models with lower rates outperform the rest of the configurations. However, the improvement in reconstruction is evident through  visual inspection of Fig. <ref type="figure" target="#fig_28">30</ref>, where the coded models better capture the details in the images. We observe that the coded model produces images that better resemble the structure of the dataset, while the uncoded DVAE cannot decouple spatial information from the images and project it in the latent space.</p><p>We hypothesize that, to adequately model complex images, transitioning to a hierarchical structure may be necessary. This would allow for the explicit modeling of both global and local information. However, despite employing this rather simple model, we observe that coded configurations outperform their uncoded counterparts in capturing colors and textures.</p><p>We also evaluate the model in the image generation task. In Fig. <ref type="figure" target="#fig_29">31</ref>, we show examples of randomly generated images using different model configurations in Tiny ImageNet. These results are consistent with the ones obtained in reconstruction, where we observe that the coded models can generate more detailed and diverse images. Additionally, we obtained the FID score using the test set and 10k generated samples. For this, we used the implementation available at <ref type="url" target="https://github.com/mseitzer/pytorch-fid">https: //github.com/mseitzer/pytorch-fid</ref>. We can observe that the coded models significantly reduced the FID score in all the cases compared to their uncoded counterparts.</p><p>For the coded models, we do not observe a clear influence of the code rate on the quality of the generations in CIFAR-10. However, in Tiny ImageNet, smaller code rates produce worse FID scores. We hypothesize that this may be due to the presence of artifacts in the generated images. Our experiments indicate that coded models with lower rates attempt to model fine details in images, which can lead to artifacts in generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H IWAE RESULTS</head><p>One could draw a parallel between the Coded-DVAE with repetition codes and the well-known IWAE <ref type="bibr" target="#b1">[Burda et al., 2016]</ref>, but the two approaches are fundamentally different. In the IWAE, independent samples are drawn from the variational posterior and propagated independently through the generative model to obtain a tighter variational bound on the marginal log-likelihood. In our method, we jointly propagate the output of the ECC encoder through the generative model, obtaining a single prediction and exploiting the introduced known correlations in the variational approximation of the posterior. In the case of repetition codes, the ECC encoder outputs are repeated bits, or repeated probabilities in the case of soft encoding. However, our approach extends beyond repetition codes, opening a new field for improved inference in discrete latent variable models.</p><p>In our approach, we specifically utilize the redundancy introduced by the repetition code to correct potential errors made by the encoder through a soft decoding approach. This results in a more accurate approximation of the posterior p(m|x) and an improved proposal for sampling, resulting in improved performance over uncoded models, even those trained with the tighter IWAE bound. To compute the IWAE objective in the uncoded case, we draw samples from the posterior using the reparameterization trick described in Eq. ( <ref type="formula" target="#formula_4">5</ref>), and compute the importance weights as w = p θ (x|z)p(z) qη(z|x) . Here, the prior and posterior over z are obtained by marginalizing out m in p(z, m) = p(m)p(z|m) and q η (m, z|x) = q η (m|x)p(z|m), respectively.</p><p>Additionally, we implemented two extensions of the IWAE bound to confirm that the observed improvements are not due to a known shortcoming of the reparametrized gradient estimator of the IWAE bound. Following <ref type="bibr" target="#b4">Daudel et al. [2023]</ref>, we implemented the VR-IWAE generalization, which introduces a hyperparameter α ∈ [0, 1) into the objective function. This formulation interpolates between the IWAE bound (recovered when α = 0) and the ELBO (recovered when α = 1). For our experiments, we selected a midpoint value of α = 0.5. Additionally, we implemented the doubly-reparametrized (DR) gradient estimator associated with the VR-IWAE objective, as described in <ref type="bibr" target="#b4">Daudel et al. [2023]</ref>. We trained the models on FMNIST using 10, 20, and 30 posterior samples, matching the number of repetitions used in our coded models. We included an additional experiment training an uncoded model with the IWAE bound considering 100 samples.</p><p>From the results, outlined in Table <ref type="table" target="#tab_12">9</ref>, we observe that training uncoded models with tighter bounds (even if we use bounds that are meant to become tighter as the number of samples increases) does not lead to substantial improvements in performance. These models consistently underperform relative to their coded counterparts, which are trained using the ELBO as the objective function. For a fair comparison, we use the same neural network architecture for the uncoded and the coded posterior (differing only in the final dense layer to match the respective output dimensionalities), the same network architecture for the decoder (differing only in the first dense layer to match the respective input dimensionalities), and the same simple independent prior. Fig. <ref type="figure" target="#fig_8">32</ref> shows the evolution of the different objectives, with coded models obtaining better bounds than the uncoded IWAE Figure <ref type="figure" target="#fig_8">32</ref>: Evolution of the ELBO and the IWAE objectives for various configurations. Observe that the IWAE provides a tighter bound than the ELBO in the uncoded setting. However, coded models obtain even better bounds using the same number of samples/repetitions. models, even when considering 100 samples. In Table <ref type="table" target="#tab_12">9</ref>, we present test metrics, with coded models again overperforming their uncoded counterparts trained with the IWAE objective. We observe that some metrics slightly decline augmenting the number of samples, likely due to overfitting, since we applied a common early stopping point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I BEYOND REPETITION CODES</head><p>We have presented compelling proof-of-concept results that incorporating ECCs, like repetition codes, into DVAEs can improve performance. We believe this opens a new path for designing latent probabilistic models with discrete latent variables. Although a detailed analysis of the joint design of ECC and encoder-decoder networks is beyond the scope of this work, we will outline key properties that any ECCs must satisfy to be integrated within this framework.</p><p>• Scalable hard encoding m → c . Our model requires hard encoding for generation once the model is trained. This process should have linear complexity in M .</p><p>• Scalable soft encoding p(m) → p(c) . Soft encoding is required during training for reparameterization. This process should also have linear complexity in M .</p><p>• Scalable soft decoding p(c) → p(m) . Our model employs soft-in soft-out (SISO) decoding during inference. This process should again be linearly complex in M .</p><p>• Differentiability. Both encoding and decoding processes must be differentiable w.r.t. the inputs to enable gradient computation and backpropagation.</p><p>Since Shannon's landmark work <ref type="bibr" target="#b26">[Shannon, 1948]</ref>, researchers have been developing feasible ECC schemes that meet (or approach) the aforementioned properties for ever-increasing values of M and R. This is driven by the high throughput demands in digital communications and limited power constraints in the case of wireless communications. In this regard, state-of-the-art ECC schemes such as Low Density Parity Check (LDPC) codes <ref type="bibr" target="#b7">[Gallager, 1962]</ref> can be designed to meet the above criteria, leveraging their linear algebraic structure. Similarly, encoding and decoding algorithms for polar codes <ref type="bibr" target="#b0">[Arikan, 2009]</ref> have complexity of O(D log D), approaching this target. Additionally, in digital communications, soft decision decoding is known to provide savings of up to 2-3 dB of signal energy over hard decision decoding (binary input).</p><p>For this reason, significant effort has been also made in the communications field to develop efficient and powerful SISO decoders, such as the sum-product algorithm (SPA) <ref type="bibr" target="#b14">[Kschischang et al., 2001]</ref> for LDPC codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J HIERARCHICAL CODED-DVAE RESULTS</head><p>Inspired by polar codes <ref type="bibr" target="#b0">[Arikan, 2009]</ref>, we present a hierarchical Coded-DVAE with two layers of latent bits, as illustrated in Fig. <ref type="figure">33</ref>. In this model, the latent bits m 1 are encoded using a repetition code in the first layer, producing c 1 and z 1 . Concurrently, the bits in the second layer, m 2 , are linearly combined with m 1 following m 1,2 = m 1 ⊕ m 2 , considering a binary field or Galois field. The resulting vector is then encoded with another repetition code to produce c 2 , which is subsequently modulated into z 2 . Finally, both z 1 and z 2 are concatenated and passed through the decoder network to generate x. The model provides stronger protection for m 1 , as it appears in both branches of the generative model. Inference follows a similar approach to the one employed in the Coded-DVAE, incorporating the linear combination of m 1 and m 2 used in the second branch.</p><p>We adopt the same variational family as used in the standard Coded-VAE; however, in this case, we incorporate both hierarchical levels, leading to</p><formula xml:id="formula_22">q η (m, z|x) = q η (m 1 |x)q η (m 2 |x)p(z 1 |c 1 )p(z 2 |c 2 ),<label>(13)</label></formula><p>where q η (m 1 |x) is calculated following the same approach as in the Coded-DVAE with repetition codes, computing the all-are-zero and all-are-ones products of probabilities of the bits in c 1 that are copies of the same message bit. The posterior</p><formula xml:id="formula_23">x i z 1 z 2 c 1 c 2 m 1 m 2</formula><p>Figure 33: Graphical model of the hierarchical Coded VAE with two layers.</p><p>q η (m 2 |x) considering both the encoder's output and the inferred posterior distribution q η (m 1 |x). Note that in this case, the decoder outputs the probabilities for both c 1 and c 2 , with c 2 being the encoded version of the linear combination m 1,2 = m 1 ⊕ m 2 . Consequently, we first obtain q η (m 1,2 |x) following the same approach as in the Coded-DVAE with repetition codes, and determine q η (m 2 |x) as</p><formula xml:id="formula_24">q η (m 2 |x) = M j=1 Ber(p j ),<label>(14)</label></formula><formula xml:id="formula_25">p j = q η (m 1,2,j = 1|x)q η (m 1,j = 0|x) + q η (m 1,2,j = 0|x)q η (m 1,j = 1|x).<label>(15)</label></formula><p>After obtaining q η (m 1 |x) and q η (m 2 |x), we recalculate the posterior bit probabilities for the linear combination q ′ η (m 1,2 |x) as</p><formula xml:id="formula_26">q ′ η (m 1,2 |x) = M j=1</formula><p>Ber(q j ),</p><formula xml:id="formula_27">q j = q η (m 1,j = 1|x)q η (m 2,j = 0|x) + q η (m 1,j = 0|x)q η (m 2,j = 1|x).<label>(16)</label></formula><p>Next, we apply the soft encoding approach to incorporate the repetition code structure at both levels of the hierarchy. The posterior probabilities q η (m 1 |x) are repeated to obtain q η (c 1 |x), and the posterior probabilities q η (m 1,2 |x) are repeated to produce q η (c 2 |x). Utilizing the reparameterization trick from Eq. ( <ref type="formula" target="#formula_4">5</ref>), we sample z 1 and z 2 , concatenate them to form z, and pass this through the decoder to generate p θ (x|z). The model is trained by maximizing the ELBO, given by</p><formula xml:id="formula_29">ELBO = E qη(m,z|x) log p θ (x|z) -D KL q η (m 1 |x)||p(m 1 ) -D KL q η (m 2 |x)||p(m 2 ) ,<label>(18)</label></formula><p>where both p(m 1 ) and p(m 2 ) are assumed to be independent Bernoulli distributions with bit probabilities of 0.5, consistent with the other scenarios.</p><p>We obtained results on the FMNIST dataset using a model with 5 information bits per branch and repetition rates of R = 1/10 and R = 1/20. In this case, we applied the same code rate to both branches, although varying code rates could be used to control the level of protection at each hierarchy level. Tables <ref type="table" target="#tab_13">10</ref> and<ref type="table" target="#tab_14">11</ref> present the metrics obtained for the different configurations. Specifically, Table <ref type="table" target="#tab_13">10</ref> shows the overall metrics obtained with this structure, and Table <ref type="table" target="#tab_14">11</ref> compares the error metrics across the two hierarchy levels. As expected, m 2 shows poorer error metrics compared to m 1 , since the model provides more redundancy to m 1 incorporating it in both branches. Although the overall metrics and generation quality are similar to those of the Coded-DVAE with 10 information bits (see tables in Figs. <ref type="figure" target="#fig_3">5</ref> and<ref type="figure" target="#fig_2">4</ref>), the introduced hierarchy results in a more interpretable latent space. In this setup, m 1 captures global features (such as clothing types in the FMNIST dataset), while m 2 controls individual features, as we can observe in Fig. <ref type="figure" target="#fig_30">34</ref>, where we show examples of the model's generative outputs for fixed m 1 and random samples of m 2 .  </p><formula xml:id="formula_30">m 1 = [0 1 0 0 1] m 1 = [0 1 0 1 1] m 1 = [1 1 1 0 1] m 1 = [1 1 0 0 0] m 1 = [1 1 1 1 1] m 1 = [1 0 0 0 1] (a) m 1 = [1 0 1 1 0] m 1 = [1 1 1 0 0] m 1 = [0 1 1 0 1] m 1 = [0 0 1 1 1] m 1 = [1 1 1 1 1] m 1 = [0 0 1 0 0] (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K ABLATION STUDY</head><p>In this section, we conduct ablation studies on the hyperparameter β of the model, responsible for regulating the decay of exponentials in the smoothing transformation, as well as on the number of trainable parameters in the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1 ABLATION STUDY ON THE HYPERPARAMETER β</head><p>Across all experiments, we have consistently configured the hyperparameter β, which controls the decay of exponentials in the smoothing transformation, to a value of 15. To illustrate its impact on the overall performance of the model, we conducted an ablation study on the value of this hyperparameter for both uncoded and coded cases.</p><p>The smoothing distribution employed for the reparameterization trick consists of two overlapping exponentials. The hyperparameter β functions as a temperature term, regulating the decay of the distributions and, consequently, influencing the degree of overlapping. A lower β value results in more overlapped tails, while a higher value leads to less overlapped distributions. A priori, we would like these distributions to be separated, allowing us to retrieve the true value of the bit and effectively use the latent structure of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1.1 Coded model</head><p>We first evaluate the influence of the parameter β in coded models. We take as a reference the coded model with 8 information bits and a rate R = 1/30, and train it using β = 5, 10, 15, 20. We assess the performance of the model in reconstruction and    In Fig. <ref type="figure" target="#fig_31">35</ref> we show examples of reconstructed images using the different configurations to assess reconstruction through visual examination, and Table <ref type="table" target="#tab_15">12</ref> contains the associated reconstruction metrics. All the configurations achieve similar performances, although the models trained with β = 10 and β = 15 seem to be the best configurations for this scenario. Larger values may result in unstable training and inferior performance.</p><p>Next, we evaluate the model in the image generation task. Fig. <ref type="figure" target="#fig_32">36</ref> contains examples of randomly generated images using the different configurations. Table <ref type="table" target="#tab_16">13</ref> reports the obtained BER and WER, and Table <ref type="table" target="#tab_17">14</ref> the estimated log-likelihood of the different values of β. The model trained with β = 15 stands out in terms of error metrics, although achieves similar log-likelihood values as the model trained with β = 10. Again, these two configurations appear to be the most suitable in this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.1.2 Uncoded model</head><p>We first evaluate the influence of the parameter β in uncoded models. We take as a reference the coded model with 8 information bits and train it using β = 5, 10, 15, 20. We assess the performance of the model in reconstruction and generation tasks. We observe that the uncoded model is also robust, achieving similar performance across configurations.</p><p>In Fig. <ref type="figure" target="#fig_33">37</ref> we show examples of reconstructed images using the different configurations to assess reconstruction through visual examination, and Table <ref type="table" target="#tab_18">15</ref> contains the associated reconstruction metrics. All the configurations achieve similar performances, although the models trained with β = 10 and β = 15 seem to be the best configurations for this scenario.</p><p>Larger values may result in unstable training and inferior performance, as we can clearly observe in this case.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2 ABLATION STUDY ON THE NUMBER OF TRAINABLE PARAMETERS</head><p>A consistent architecture was employed across all experiments, which is detailed in Section C. However, since the introduction of the code alters the dimensionality of the latent space, it is necessary to adjust the encoder's output and the decoder's input. This results in an augmentation of the trainable parameters in the coded cases compared to their uncoded counterparts.</p><p>Given that a higher number of parameters usually results in better performance, we conducted an ablation study on the model's trainable parameters to confirm that the improved performance introduced by the coded models is not due to this factor. We adjusted the hidden dimensions of the encoder and decoder architectures to ensure both configurations (coded and uncoded) have roughly the same number of trainable parameters. We have conducted the ablation study using the uncoded model with 8 bits and the coded 8/240 model trained on FMNIST.</p><p>We adjusted the encoder's last hidden dimension and the decoder's first hidden dimension to equalize the parameter count between the uncoded and coded models. This adjustment was straightforward since the last layers of the encoder and the first layers of the decoder are feed-forward layers. We kept the latent dimension of the model unchanged, ensuring that the modification solely pertained to the neural network architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2.1 Evaluation</head><p>This section provides an empirical evaluation of the models trained with the adjusted parameter count, demonstrating that the enhanced performance observed in coded models does not result from an augmented number of trainable parameters. We found that the performance of the original and adjusted models is very similar, meaning that the conclusions drawn in the main text hold even in this scenario.</p><p>We first evaluate the reconstruction performance, measuring the PSNR and reconstruction accuracy in both train and test sets, which are included in Table <ref type="table" target="#tab_22">19</ref>. Then, we assess generation measuring the BER and WER, reported in Table <ref type="table" target="#tab_23">20</ref>. Finally, we compute the log-likelihood for train and test sets, shown in Table <ref type="table" target="#tab_24">21</ref>.</p><p>In terms of reconstruction, both the original and adjusted models exhibit very similar performance, observed in both reconstruction quality and accuracy. However, the adjusted models show slightly inferior results than the original ones.</p><p>For the coded model, this might be attributed to reduced flexibility when decreasing the number of parameters. As for the uncoded model, the increased complexity while maintaining the same low-dimensional latent vector may not provide enough expressiveness to leverage the added flexibility in the architecture, potentially causing the observed decrease in performance. We observe the same behavior when we evaluate the BER and WER.</p><p>Analyzing the results, especially the log-likelihood values shown in Table <ref type="table" target="#tab_24">21</ref>, we can argue that increasing the flexibility in the architecture does not necessarily lead to improved performance in this scenario. The coded models exhibit similar performance with both the original and adjusted parameter counts, consistently outperforming the uncoded models. These results indicate that the performance enhancement is attributed to the introduction of ECCs in the latent space, rather than differences in the architecture required to handle the introduced redundancy.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L EVALUATING LOG-LIKELIHOOD USING THE SOFT-ENCODING MODEL</head><p>The reparameterization trick introduced in (5) requires that bits are independent, a condition not met in c once the code's structure is introduced. To address this issue, during training, we employ a soft encoding strategy. We assume the bits in c are independent and equally distributed according to q η (m|x). Therefore, instead of directly repeating sampled bits in m to obtain c following c = m T G, we repeat the posterior probabilities for the copies of the same bit and sample z using the reparameterization trick in (5). Remark that, despite the soft encoding assumption during training, the generative results in <ref type="bibr">Fig. 4,</ref><ref type="bibr">8,</ref><ref type="bibr">15,</ref><ref type="bibr">21,</ref><ref type="bibr">26,</ref><ref type="bibr">26,</ref><ref type="bibr">and 31</ref> are obtained through hard encoding. Namely, we sampled an information word m and obtained c by repeating its bits.</p><p>While the hard-encoding images are visually appealing, to evaluate the Coded-DVAE LL for a given image x, we have to leverage the soft-encoding model since it is unrealistic for a sample from the soft-encoding model to have equally repeated bits. In the soft-coded model, we sample bit probabilities from a prior distribution that we model through the product of M independent Beta distributions, and we use a proposal distribution model similar to the Vamp-prior in <ref type="bibr" target="#b28">Tomczak and Welling [2018]</ref>. Namely, a mixture model with components given by q(m|x) for different training points.</p><p>In the main text and Section K, we report the LL values for different model configurations trained on the FMNIST dataset. Table <ref type="table" target="#tab_7">4</ref> presents the results obtained for the MNIST dataset. Due to their simplicity, these datasets do not require high-dimensional latent spaces, and competitive results can be achieved with just 8 or 10 information bits. However, for more complex datasets like CIFAR10 or Tiny ImageNet, high-dimensional latent spaces are necessary to capture spatial information, colors, and textures. For these high-dimensional datasets, we were unable to obtain valid log-likelihood estimates because the number of samples needed for importance sampling to converge was too large. However, given the difference in the level of detail in the reconstructed and generated images between coded and uncoded models (see Sections F and G), coded models are expected to better approximate the true marginal likelihood of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M CONNECTION TO PREVIOUS WORKS ON VAES AS SOURCE CODING METHODS</head><p>Effective learning of low-dimensional discrete latent representations from complex data is a technically challenging task.</p><p>In this work, we propose a novel method to improve inference in discrete VAEs within a fully probabilistic framework, introducing a new perspective on the inference problem. While previous studies have analyzed VAEs using rate-distortion (RD) theory <ref type="bibr" target="#b3">[Chen et al., 2022</ref><ref type="bibr" target="#b29">, Townsend et al., 2019</ref><ref type="bibr" target="#b32">, Van Den Oord et al., 2017]</ref>, our approach stems from a different yet complementary perspective that remains compatible with these works.</p><p>In the literature, VAEs are often interpreted as lossy compression models (e.g., VAEs as source coding methods), but our contribution is best understood from a generative perspective. We conceptualize inference via q(m|x) as a decoding process, where the goal is to recover the discrete latent variable from the observed data. We sample a latent vector m, generate a data point x (e.g., an image), and aim to minimize the error rate in recovering m from x. Achieving this requires the variational approximation q(m|x) to closely match the true posterior p(m|x).We demonstrate that incorporating redundancy into the generative process reduces errors in estimating m, leading to a more accurate latent posterior approximation. This improvement directly enhances our model's overall performance, as confirmed by our experimental results. Additionally, error rates and variational gaps are related through bounds derived from mismatch hypothesis testing. These bounds demonstrate that minimizing the variational gap by maximizing the ELBO effectively tightens an upper bound on the error rate <ref type="bibr" target="#b24">[Schlüter et al., 2013</ref><ref type="bibr" target="#b36">, Yang et al., 2024]</ref>.</p><p>In RD theory, the primary focus is on compression within the latent space, typically analyzed from an encoder/decoder and reconstruction (distortion) perspective. RD theory establishes theoretical limits on achievable compression rates and describes how practical models may diverge from these limits. Using RD practical compression methods, Townsend et al.</p><p>[2019] demonstrates how asymmetric numeral systems (ANS) can be integrated with a VAE to improve its compression rate by jointly encoding sequences of data points, bringing performance closer to RD theoretical limits. Similarly, <ref type="bibr" target="#b3">Chen et al. [2022]</ref> shows that a complex prior distribution in a VAE using an autoregressive invertible flow narrows the gap between the approximate and the true posterior distribution, thereby enhancing the overall performance of the VAE. We note that even using an independent prior, the hierarchical code structure outlined in Section J naturally decouples information across different latent spaces at various conceptual levels. Specifically, the most relevant information (class label) is captured by the most protected latent space, while the other space captures fine-grained features. This effect is not straightforward to enforce through direct design of a more complex prior.</p><p>Our method complements all these efforts by introducing redundancy in the generative pathway to enhance variational inference, leading to more accurate latent posterior approximations. Notably, even when using a complex prior distribution, ECCs can still be leveraged to enhance inference and overall model performance. In this work, we demonstrate that our approach yields more robust models even with a simple, fixed independent prior, as evidenced by improved LL, generation quality, and reconstruction metrics. Moreover, in Section J, we show that integrating a hierarchical ECC with the same independent prior leads to even greater performance gains.</p><p>It is also important to discuss the VQ-VAE <ref type="bibr" target="#b32">[Van Den Oord et al., 2017]</ref>, one of the most relevant works related to our approach, which is well-known for effectively learning compressed discrete representations of complex data. We believe that highlighting key differences between VQ-VAEs and our method can provide useful insights. A key distinction lies in the structure and dimensionality of the latent space. In image modeling, VQ-VAEs typically employ a latent matrix where indices correspond to codewords in a codebook. This design allows different codewords to capture specific patches of the original image, improving reconstruction and generation (the latter through an autoregressive prior on the latent representations). However, representing each data point as a grid of embeddings complicates interpretability. In contrast, our method encodes the entire image into the latent space, rather than partitioning it into patches. Another major difference is the nature of the encoder. VQ-VAEs rely on a non-probabilistic encoder that maps inputs to the nearest latent code using a distance-based metric. This deterministic mapping limits the model's ability to capture and quantify uncertainty in the latent space. Our approach, on the other hand, operates within a fully probabilistic framework, allowing for uncertainty quantification in the learned latent representations. Additionally, our method is fully differentiable, enabling seamless gradient computation and backpropagation for end-to-end training. Our approach also allows us to leverage the reparameterization trick introduced in DVAE++ <ref type="bibr" target="#b31">[Vahdat et al., 2018b</ref>], which we found to be more stable than continuous relaxations like the Gumbel-Softmax used in VQ-VAEs with stochastic quantization <ref type="bibr" target="#b33">[Williams et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N VARIATIONAL INFERENCE AT CODEWORD LEVEL</head><p>Here, we present an alternative variational family that is valid for any ECC, including random codes. We assume that we have a deterministic mapping of the form c = C(m). We assume a variational family of the form q η (c, z|x) = q η (c|x)p(z|c),</p><p>q η (c|x) ∝ p(c)q u η (c|x), (20)</p><formula xml:id="formula_32">q u η (c|x) = M/R j=1 Ber(g j,η (x)),<label>(21)</label></formula><p>where g η (x) represents the output of the encoder with a parameter set denoted as η. Note that q u η (c|x) corresponds to the uncoded posterior, which we subsequently constrain using the prior distribution p(c) over the code words to obtain the coded posterior q η (c|x). Then, the coded posterior distribution can be defined as a categorical distribution over the set of codewords C(m), which is given by q η (c|x) = Cat 1 W q u η (c 1 |x), . . . ,</p><formula xml:id="formula_33">1 W q u η (c 2 M |x) = 1 W ci∈C(m)</formula><p>M/R j=1 g j,η (x) ci,j (1 -g j,η (x)) (1-ci,j ) , <ref type="bibr">(22)</ref> where W = ci∈C(m) q u η (c i |x) is a constant for normalization. Inference is done by maximizing the ELBO, which can be expressed as ELBO = q η (c, z|x) log p θ (x, z, c) q η (c, z|x) dcdz = E qη(c,z|x) log p θ (x|z)p(z|c)p(c) q η (c|x)p(z|c)</p><p>= E qη(c,z|x) log p θ (x|z) -D KL q η (c|x)||p(c) .</p><p>(23)</p><p>In this case, due to the inability to compute the KL Divergence in closed form, we approximate it via Monte Carlo, sampling from the categorical distribution q η (c|x). The reconstruction term also needs to be approximated via Monte Carlo. Since the use of channel coding introduces structural dependencies among the components of the vectors c, we can no longer assume their independence. Consequently, the formulation of the smoothing transformation as independent mixtures introduced in the DVAE is no longer applicable. Hence, this approach involves sampling c ′ from the categorical distribution q η (c|x) and subsequently applying the transformation over the sampled word. Thus, we obtain a smooth transformation for each sample c ′ using the inverse CDFs of p(z j |c j = 0) and p(z j |c j = 1), which are given by</p><formula xml:id="formula_34">F -1 p(zj |c ′ j =0) (ρ) = - 1 β log 1 -ρ(1 -e -β ) ,<label>(24)</label></formula><formula xml:id="formula_35">F -1 p(zj |c ′ j =1) (ρ) = 1 β log ρ(1 -e -β ) + e -β + 1. (<label>25</label></formula><formula xml:id="formula_36">)</formula><p>These are differentiable functions that convert samples ρ from a uniform distribution U(0, 1) into a sample from q η (z, c = c ′ |x) following</p><formula xml:id="formula_37">q η (z, c = c ′ |x) = = M/R j=1 F -1 p(zj |c ′ j =1) (ρ) c ′ j + F -1 p(zj |c ′ j =0) (ρ) (1-c ′ j ) .<label>(26)</label></formula><p>Thus, we can apply the reparameterization trick to obtain samples from the latent variable z and optimize the reconstruction term of the ELBO with respect to the parameters θ of the decoder.</p><p>The KL Divergence term is approximated via Monte Carlo, drawing samples from q η (c|x). Since it is not possible to backpropagate through discrete variables, we approximate the gradients with respect to the parameters of the encoder using the REINFORCE leave-one-out estimator <ref type="bibr">[Salimans and</ref><ref type="bibr">Knowles, 2014, Kool et al., 2019]</ref>, given by</p><formula xml:id="formula_38">g LOO = = 1 S -1 S s=1 f η z (s) , c (s) ▽ η log q η c (s) |x -f η S s=1 ▽ η log q η c (s) |x ,<label>(27)</label></formula><p>where f η (z (s) , c (s) ) = log q η (c (s) |x) p θ (x|z (s) )p(c (s) ) , (28)</p><formula xml:id="formula_39">f η = 1 S S s=1</formula><p>f η (z (s) , c (s) ).</p><p>(29)</p><p>Defining a distribution over the codebook can seem intuitive, but scalability becomes challenging as the size of the codebook increases. The reason for this is that the posterior distribution must be evaluated for all codewords during both inference and test time. However, it can still provide a bound, enabling the utilization of more complex codes with theoretical guarantees that can outperform the previously proposed repetition codes.</p><p>Algorithm 3 Training the model with inference at codeword level.</p><p>1: Input: training data x i , codebook. 2: repeat 3: q u η (c|x i ) ← forward encoder g η (x i )</p><p>4:</p><p>q η (c|x i ) ← evaluate q u η (c|x i ) over the codebook and normalize 5:</p><p>c ← sample from q η (c|x i )</p><p>6:</p><p>z ← modulate c </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ECC within a DVAE. Fig. (a) illustrates the method and how adding redundancy transforms a dense Mdimensional latent space into a sparse D-dimensional one with 2 M valid vectors, enabling error correction in the variational posterior via minimum distance. Fig. (b) compares the ELBO and IWAE objectives for coded and uncoded models, where coding schemes with M = 8 and D = 80, 160, 240 introduce D/8 redundancy bits per latent bit.</figDesc><graphic coords="2,309.34,121.52,362.54,120.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Graphical model of the uncoded DVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evaluation of generation in FMNIST. Uncurated randomly generated samples (left) and evaluation metrics (right), including BER and WER sampled from the variational posterior, BER and WER using the MAP value, and test LL.</figDesc><graphic coords="5,70.99,151.73,146.97,146.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reconstruction performance in FMNIST. The figure at the left shows an example of reconstructed test images obtained with different model configurations.Observe that more details are visualized as we increase the number of bits in the latent space and decrease the coding rate. The table at the right includes reconstruction metrics. Acc is the semantic accuracy, and Conf. Acc the confident semantic accuracy. Entropy is the average entropy of q η (m|x) in the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the Coded-DVAE recognizes multiple latent vectors that might be related to a given semantic class, and (ii) the model's posterior</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Example of erroneous reconstructions in FM-NIST. The first column in each image shows the original images, while the second column displays the reconstructions. The q η (m|x) probability is indicated in each row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Generation results for CIFAR10 (left) and Tiny ImageNet (right).</figDesc><graphic coords="8,41.59,129.08,183.02,183.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Hierarchical Coded-DVAE. Graphical model (right) and generated images with a 5/100 code per branch (left). Here, m 1 is fixed, and m 2 is randomly sampled.</figDesc><graphic coords="8,330.63,330.07,204.52,199.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2</head><label>2</label><figDesc>Training the Coded-DVAE with repetition codes. 1: Input: training data x i , matrix G. 2: repeat 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Block diagram of the encoder architecture for FMNIST and MNIST.</figDesc><graphic coords="14,-27.28,157.18,301.45,301.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Block diagram of the decoder architecture for FMNIST and MNIST.</figDesc><graphic coords="14,394.31,506.38,266.47,266.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Evolution of the ELBO during training with 5 information bits on FMNIST.</figDesc><graphic coords="15,60.16,283.54,474.97,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Evolution of the ELBO during training with 8 information bits on FMNIST.</figDesc><graphic coords="15,58.57,526.86,478.14,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Evolution of the ELBO during training with 10 information bits on FMNIST.</figDesc><graphic coords="16,58.01,85.20,476.32,145.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Example of randomly generated, uncurated images using different model configurations.</figDesc><graphic coords="16,244.81,603.67,161.16,161.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Evolution of the ELBO during training with 5 information bits on MNIST.</figDesc><graphic coords="17,60.89,539.59,473.52,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Evolution of the ELBO during training with 8 information bits on MNIST.</figDesc><graphic coords="18,59.62,85.15,476.05,145.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Evolution of the ELBO during training with 10 information bits on MNIST.</figDesc><graphic coords="18,58.23,309.31,478.84,146.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Figure 19: Example of reconstructed test images obtained with different model configurations. Observe that more details are visualized as we increase bits in the latent space and decrease the coding rate.</figDesc><graphic coords="19,196.74,275.02,354.32,118.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Example of randomly generated, uncurated images using different model configurations.</figDesc><graphic coords="20,140.16,505.13,164.49,164.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Evolution of the ELBO during training with 70 information bits on CIFAR10.</figDesc><graphic coords="21,58.78,539.71,477.74,144.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Evolution of the ELBO during training with 100 information bits on CIFAR10.</figDesc><graphic coords="22,58.68,85.47,477.92,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Evolution of the ELBO during training with 130 information bits on CIFAR10.</figDesc><graphic coords="22,57.03,310.13,481.22,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 25 :Figure 26 :</head><label>2526</label><figDesc>Figure 25: Example of reconstructed test images obtained with different model configurations. Observe that more details are visualized as we increase bits in the latent space and introduce redundancy.ORIGINAL UNCODED 70</figDesc><graphic coords="23,55.72,263.06,167.30,167.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Evolution of the ELBO during training for the configurations with 70 information bits.</figDesc><graphic coords="24,60.16,539.59,474.97,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Evolution of the ELBO during training for the configurations with 100 information bits.</figDesc><graphic coords="25,56.85,85.47,481.58,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Evolution of the ELBO during training for the configurations with 130 information bits.</figDesc><graphic coords="25,56.85,310.13,481.58,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Example of reconstructed test images obtained with different model configurations. Observe that more details are visualized as we increase the bits in the latent space and introduce redundancy.</figDesc><graphic coords="26,110.67,204.18,162.96,162.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 31 :</head><label>31</label><figDesc>Figure 31: Example of randomly generated, uncurated images using different model configurations.</figDesc><graphic coords="26,376.38,369.99,161.71,161.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 34 :</head><label>34</label><figDesc>Figure 34: Examples of generated images using the hierarchical Coded-DVAE with (a) a 5/50 repetition code in each branch, and (b) a 5/100 repetition code in each branch. In all the examples provided, m 1 was fixed while m 2 was randomly sampled.</figDesc><graphic coords="31,-10.09,147.83,174.17,175.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 35 :</head><label>35</label><figDesc>Figure 35: Example of reconstructed images obtained with different values of β using the coded model with an 8/240 code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 36 :</head><label>36</label><figDesc>Figure 36: Example of randomly generated, uncurated images with different values of β using the coded model with an 8/240 code.</figDesc><graphic coords="32,163.05,302.27,180.14,180.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 37 :</head><label>37</label><figDesc>Figure 37: Example of reconstructed images obtained with different values of β using an uncoded model 8 information bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 38 :</head><label>38</label><figDesc>Figure 38: Example of randomly generated, uncurated images with different values of β using an uncoded model 8 information bits.</figDesc><graphic coords="33,163.15,583.16,181.29,181.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 39 :</head><label>39</label><figDesc>Figure 39: Example of reconstructed images obtained with different configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 40 :</head><label>40</label><figDesc>Figure 40: Example of randomly generated, uncurated images using different model configurations.</figDesc><graphic coords="35,164.53,543.63,180.14,180.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 41 :Figure 42 :</head><label>4142</label><figDesc>Figure 41: Graphic representation of the Coded-DVAE, illustrating both the generative and inference paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="28,69.74,73.07,455.80,191.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.960•10 -4 0.046 9.800•10 -4 -241.882 coded 5/80 0.008 4.000•10 -6 0.039 2.000•10 -5 -232.992 coded 5/100 0.010 2.120•10 -4 0.049 9.600•10 -4 -241.404</figDesc><table><row><cell>UNCODED 5</cell><cell>UNCODED 8</cell><cell>UNCODED 10</cell><cell>Model</cell><cell cols="5">BER BER MAP WER WER MAP LL test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>uncoded 5</cell><cell>0.051</cell><cell>0.051</cell><cell>0.195</cell><cell>0.190</cell><cell>-267.703</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">coded 5/50 0.011 1uncoded 8 0.089</cell><cell>0.088</cell><cell>0.384</cell><cell>0.376</cell><cell>-249.880</cell></row><row><cell></cell><cell></cell><cell></cell><cell>coded 8/80</cell><cell>0.021</cell><cell>0.003</cell><cell>0.144</cell><cell>0.021</cell><cell>-232.992</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">coded 8/160 0.027</cell><cell>0.001</cell><cell>0.189</cell><cell>0.009</cell><cell>-235.819</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">coded 8/240 0.037</cell><cell>0.004</cell><cell>0.231</cell><cell>0.023</cell><cell>-238.459</cell></row><row><cell></cell><cell></cell><cell></cell><cell>uncoded 10</cell><cell>0.142</cell><cell>0.144</cell><cell>0.622</cell><cell>0.644</cell><cell>-244.997</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">coded 10/100 0.040</cell><cell>0.005</cell><cell>0.321</cell><cell>0.036</cell><cell>-230.772</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">coded 10/200 0.044</cell><cell>0.006</cell><cell>0.341</cell><cell>0.039</cell><cell>-234.849</cell></row><row><cell>CODED 5/100</cell><cell>CODED 8/240</cell><cell>CODED 10/300</cell><cell cols="2">coded 10/300 0.045</cell><cell>0.002</cell><cell>0.349</cell><cell>0.014</cell><cell>-238.647</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table of Contents for easier navigation. Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 F.2 Reconstruction and generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Ablation study on the hyperparameter β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K.2 Ablation study on the number of trainable parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table><row><cell>A Uncoded training algorithm B Coded training algorithm C Architecture F CIFAR10 results I Beyond Repetition Codes J Hierarchical Coded-DVAE results K Ablation study K.1 L Evaluating log-likelihood using the soft-encoding model M Connection to previous works on VAEs as source coding methods N Variational inference at codeword level O Computational resources P Coded-DVAE scheme F.1 H IWAE results Q Hierarchical Coded-DVAE scheme</cell><cell>.</cell><cell>13 13 13 20 21</cell></row></table><note><p>C.1 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 C.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 D FMNIST results 14 D.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 D.2 Reconstruction and generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 E MNIST results 17 E.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 E.2 Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 E.3 Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G Tiny ImageNet results 24 G.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 G.2 Reconstruction and generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of reconstruction performance in FMNIST.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>SSIM (train)</cell><cell>Acc (train)</cell><cell>Conf. Acc. (train)</cell><cell>PSNR (test)</cell><cell>SSIM (test)</cell><cell>Acc (test)</cell><cell>Conf. Acc. (test)</cell></row><row><cell>uncoded 5</cell><cell>14.490</cell><cell>0.438</cell><cell>0.541</cell><cell>0.541</cell><cell>14.477</cell><cell>0.437</cell><cell>0.536</cell><cell>0.536</cell></row><row><cell>coded 5/50</cell><cell>16.375</cell><cell>0.559</cell><cell>0.656</cell><cell>0.702</cell><cell>16.241</cell><cell>0.551</cell><cell>0.647</cell><cell>0.700</cell></row><row><cell>coded 5/80</cell><cell>16.824</cell><cell>0.585</cell><cell>0.694</cell><cell>0.751</cell><cell>16.624</cell><cell>0.574</cell><cell>0.688</cell><cell>0.748</cell></row><row><cell>coded 5/100</cell><cell>17.001</cell><cell>0.596</cell><cell>0.708</cell><cell>0.760</cell><cell>16.702</cell><cell>0.580</cell><cell>0.700</cell><cell>0.757</cell></row><row><cell>uncoded 8</cell><cell>15.644</cell><cell>0.513</cell><cell>0.601</cell><cell>0.602</cell><cell>15.598</cell><cell>0.509</cell><cell>0.594</cell><cell>0.595</cell></row><row><cell>coded 8/80</cell><cell>17.877</cell><cell>0.647</cell><cell>0.769</cell><cell>0.842</cell><cell>17.318</cell><cell>0.619</cell><cell>0.750</cell><cell>0.816</cell></row><row><cell>coded 8/160</cell><cell>18.828</cell><cell>0.690</cell><cell>0.807</cell><cell>0.878</cell><cell>17.713</cell><cell>0.641</cell><cell>0.783</cell><cell>0.831</cell></row><row><cell>coded 8/240</cell><cell>19.345</cell><cell>0.713</cell><cell>0.831</cell><cell>0.921</cell><cell>17.861</cell><cell>0.653</cell><cell>0.799</cell><cell>0.893</cell></row><row><cell>uncoded 10</cell><cell>16.053</cell><cell>0.542</cell><cell>0.650</cell><cell>0.652</cell><cell>16.000</cell><cell>0.536</cell><cell>0.644</cell><cell>0.648</cell></row><row><cell cols="2">coded 10/100 18.827</cell><cell>0.690</cell><cell>0.813</cell><cell>0.885</cell><cell>17.694</cell><cell>0.639</cell><cell>0.790</cell><cell>0.850</cell></row><row><cell cols="2">coded 10/200 19.937</cell><cell>0.735</cell><cell>0.846</cell><cell>0.897</cell><cell>18.009</cell><cell>0.659</cell><cell>0.814</cell><cell>0.871</cell></row><row><cell cols="4">coded 10/300 20.529 0.754 0.855</cell><cell>0.907</cell><cell cols="3">18.111 0.662 0.817</cell><cell>0.870</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of reconstruction performance in MNIST.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>SSIM (train)</cell><cell>Acc (train)</cell><cell>Conf. Acc. (train)</cell><cell>PSNR (test)</cell><cell>SSIM (test)</cell><cell>Acc (test)</cell><cell>Conf. Acc. (test)</cell><cell>Entropy</cell></row><row><cell>uncoded 5</cell><cell>13.491</cell><cell>0.434</cell><cell>0.702</cell><cell>0.703</cell><cell>13.483</cell><cell>0.430</cell><cell>0.701</cell><cell>0.702</cell><cell>0.277</cell></row><row><cell>coded 5/50</cell><cell>14.983</cell><cell>0.601</cell><cell>0.887</cell><cell>0.923</cell><cell>14.888</cell><cell>0.598</cell><cell>0.887</cell><cell>0.920</cell><cell>2.073</cell></row><row><cell>coded 5/80</cell><cell>15.436</cell><cell>0.641</cell><cell>0.899</cell><cell>0.936</cell><cell>15.263</cell><cell>0.634</cell><cell>0.895</cell><cell>0.929</cell><cell>2.237</cell></row><row><cell>coded 5/100</cell><cell>15.590</cell><cell>0.652</cell><cell>0.905</cell><cell>0.931</cell><cell>15.352</cell><cell>0.641</cell><cell>0.898</cell><cell>0.924</cell><cell>2.382</cell></row><row><cell>uncoded 8</cell><cell>14.530</cell><cell>0.558</cell><cell>0.860</cell><cell>0.864</cell><cell>14.490</cell><cell>0.558</cell><cell>0.860</cell><cell>0.868</cell><cell>0.513</cell></row><row><cell>coded 8/80</cell><cell>16.878</cell><cell>0.739</cell><cell>0.937</cell><cell>0.964</cell><cell>16.042</cell><cell>0.699</cell><cell>0.912</cell><cell>0.947</cell><cell>3.105</cell></row><row><cell>coded 8/160</cell><cell>18.108</cell><cell>0.802</cell><cell>0.957</cell><cell>0.974</cell><cell>16.497</cell><cell>0.736</cell><cell>0.927</cell><cell>0.951</cell><cell>3.645</cell></row><row><cell>coded 8/240</cell><cell>19.984</cell><cell>0.838</cell><cell>0.967</cell><cell>0.978</cell><cell>16.688</cell><cell>0.752</cell><cell>0.936</cell><cell>0.957</cell><cell>3.881</cell></row><row><cell>uncoded 10</cell><cell>14.879</cell><cell>0.591</cell><cell>0.888</cell><cell>0.891</cell><cell>14.816</cell><cell>0.589</cell><cell>0.887</cell><cell>0.890</cell><cell>0.636</cell></row><row><cell cols="2">coded 10/100 17.584</cell><cell>0.777</cell><cell>0.945</cell><cell>0.972</cell><cell>16.795</cell><cell>0.744</cell><cell>0.928</cell><cell>0.968</cell><cell>4.080</cell></row><row><cell cols="2">coded 10/200 20.060</cell><cell>0.875</cell><cell>0.973</cell><cell>0.977</cell><cell>16.863</cell><cell>0.765</cell><cell>0.932</cell><cell>0.944</cell><cell>4.411</cell></row><row><cell cols="4">coded 10/300 21.083 0.902 0.979</cell><cell>0.984</cell><cell cols="3">17.114 0.781 0.941</cell><cell>0.945</cell><cell>4.810</cell></row><row><cell cols="5">information bits increases and the code rate decreases.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of the BER, WER in MNIST.</figDesc><table><row><cell>Model</cell><cell cols="4">BER BER MAP WER WER MAP</cell></row><row><cell>uncoded 5</cell><cell>0.002</cell><cell>0.001</cell><cell>0.008</cell><cell>0.004</cell></row><row><cell>coded 5/50</cell><cell>0.007</cell><cell>0.000</cell><cell>0.034</cell><cell>0.000</cell></row><row><cell>coded 5/80</cell><cell>0.004</cell><cell>0.000</cell><cell>0.021</cell><cell>0.000</cell></row><row><cell cols="2">coded 5/100 0.009</cell><cell>0.000</cell><cell>0.045</cell><cell>0.000</cell></row><row><cell>uncoded 8</cell><cell>0.015</cell><cell>0.013</cell><cell>0.071</cell><cell>0.057</cell></row><row><cell>coded 8/80</cell><cell cols="4">0.020 9.750•10 -5 0.147 7.800•10 -4</cell></row><row><cell cols="5">coded 8/160 0.021 2.500•10 -5 0.160 2.000•10 -4</cell></row><row><cell cols="4">coded 8/240 0.023 7.800•10 -4 0.167</cell><cell>0.006</cell></row><row><cell>uncoded 10</cell><cell>0.057</cell><cell>0.052</cell><cell>0.373</cell><cell>0.353</cell></row><row><cell cols="4">coded 10/100 0.030 2.040•10 -4 0.258</cell><cell>0.002</cell></row><row><cell cols="4">coded 10/200 0.034 3.260•10 -4 0.282</cell><cell>0.003</cell></row><row><cell cols="4">coded 10/300 0.041 9.600•10 -4 0.331</cell><cell>0.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the LL in MNIST.</figDesc><table><row><cell>Model</cell><cell cols="2">LL (train) LL (test)</cell></row><row><cell>uncoded 5</cell><cell>-149.049</cell><cell>-148.997</cell></row><row><cell>coded 5/50</cell><cell>-117.979</cell><cell>-119.094</cell></row><row><cell>coded 5/80</cell><cell>-114.911</cell><cell>-116.639</cell></row><row><cell>coded 5/100</cell><cell>-115.189</cell><cell>-117.200</cell></row><row><cell>uncoded 8</cell><cell>-127.079</cell><cell>-127.555</cell></row><row><cell>coded 8/80</cell><cell>-96.554</cell><cell>-104.692</cell></row><row><cell>coded 8/160</cell><cell>-96.014</cell><cell>-107.436</cell></row><row><cell>coded 8/240</cell><cell>-97.316</cell><cell>-111.312</cell></row><row><cell>uncoded 10</cell><cell>-120.594</cell><cell>-121.332</cell></row><row><cell>coded 10/100</cell><cell>-92.545</cell><cell>-99.373</cell></row><row><cell>coded 10/200</cell><cell>-86.072</cell><cell>-106.249</cell></row><row><cell>coded 10/300</cell><cell>-88.904</cell><cell>-110.799</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Reconstruction metrics in CIFAR10 with different model configurations.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>SSIM (train)</cell><cell>PSNR (test)</cell><cell>SSIM (test)</cell></row><row><cell>uncoded 70</cell><cell>17.985</cell><cell>0.340</cell><cell>17.596</cell><cell>0.324</cell></row><row><cell>coded 70/700</cell><cell>23.790</cell><cell>0.709</cell><cell>17.731</cell><cell>0.323</cell></row><row><cell>coded 70/1400</cell><cell>24.555</cell><cell>0.748</cell><cell>18.008</cell><cell>0.357</cell></row><row><cell>coded 70/2100</cell><cell>25.551</cell><cell>0.748</cell><cell>18.401</cell><cell>0.385</cell></row><row><cell>uncoded 100</cell><cell>18.509</cell><cell>0.381</cell><cell>18.334</cell><cell>0.370</cell></row><row><cell cols="2">coded 100/1000 24.754</cell><cell>0.754</cell><cell>18.229</cell><cell>0.375</cell></row><row><cell cols="2">coded 100/2000 24.866</cell><cell>0.761</cell><cell>18.927</cell><cell>0.432</cell></row><row><cell cols="2">coded 100/3000 25.646</cell><cell>0.793</cell><cell>18.920</cell><cell>0.426</cell></row><row><cell>uncoded 130</cell><cell>18.951</cell><cell>0.419</cell><cell>18.758</cell><cell>.408</cell></row><row><cell cols="2">coded 130/1300 25.007</cell><cell>0.767</cell><cell>18.887</cell><cell>0.426</cell></row><row><cell cols="2">coded 130/2600 25.460</cell><cell cols="3">0.784 19.416 0.464</cell></row><row><cell cols="3">coded 130/3900 25.515 0.785</cell><cell>19.292</cell><cell>0.456</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of the BER, WER, and FID in CIFAR10 with different model configurations.</figDesc><table><row><cell>Model</cell><cell>BER</cell><cell>BER MAP</cell><cell>WER</cell><cell>WER MAP</cell><cell>FID</cell></row><row><cell>uncoded 70</cell><cell cols="5">0.162 0.162 1.000 0.999 177.524</cell></row><row><cell>coded 70/700</cell><cell cols="5">0.101 0.081 1.000 0.988 104.977</cell></row><row><cell cols="6">coded 70/1400 0.088 0.044 0.999 0.917 104.078</cell></row><row><cell cols="6">coded 70/2100 0.090 0.029 0.999 0.811 102.795</cell></row><row><cell>uncoded 100</cell><cell cols="5">0.182 0.179 1.000 1.000 172.063</cell></row><row><cell cols="6">coded 100/1000 0.123 0.104 1.000 0.999 107.887</cell></row><row><cell cols="6">coded 100/2000 0.114 0.060 1.000 0.990 101.182</cell></row><row><cell cols="6">coded 100/3000 0.138 0.077 1.000 0.997 107.287</cell></row><row><cell>uncoded 130</cell><cell cols="5">0.197 0.194 1.000 1.000 164.138</cell></row><row><cell cols="6">coded 130/1300 0.144 0.115 1.000 0.999 109.905</cell></row><row><cell cols="6">coded 130/2600 0.164 0.102 1.000 1.000 110.250</cell></row><row><cell cols="6">coded 130/3900 0.185 0.132 1.000 1.000 108.561</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Reconstruction metrics in Tiny ImageNet with different model configurations.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>SSIM (train)</cell><cell>PSNR (test)</cell><cell>SSIM (test)</cell></row><row><cell>uncoded 70</cell><cell>15.598</cell><cell>0.207</cell><cell>15.402</cell><cell>0.196</cell></row><row><cell>coded 70/700</cell><cell>18.156</cell><cell>0.367</cell><cell>15.158</cell><cell>0.191</cell></row><row><cell>coded 70/1400</cell><cell>18.396</cell><cell>0.383</cell><cell>15.419</cell><cell>0.203</cell></row><row><cell>coded 70/2100</cell><cell>18.228</cell><cell>0.369</cell><cell>15.789</cell><cell>0.223</cell></row><row><cell>uncoded 100</cell><cell>16.012</cell><cell>0.229</cell><cell>15.774</cell><cell>0.215</cell></row><row><cell cols="2">coded 100/1000 18.298</cell><cell>0.378</cell><cell>15.677</cell><cell>0.218</cell></row><row><cell cols="2">coded 100/2000 18.647</cell><cell>0.404</cell><cell>15.892</cell><cell>0.232</cell></row><row><cell cols="2">coded 100/3000 18.729</cell><cell>0.408</cell><cell>16.167</cell><cell>0.248</cell></row><row><cell>uncoded 130</cell><cell>16.278</cell><cell>0.243</cell><cell>16.009</cell><cell>0.228</cell></row><row><cell cols="2">coded 130/1300 18.719</cell><cell>0.409</cell><cell>15.900</cell><cell>0.233</cell></row><row><cell cols="2">coded 130/2600 18.818</cell><cell cols="3">0.433 16.329 0.259</cell></row><row><cell cols="3">coded 130/3900 19.020 0.415</cell><cell cols="2">16.288 0.259</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Evaluation of the BER, WER, and FID in Tiny Ima-geNet with different model configurations.</figDesc><table><row><cell>Model</cell><cell>BER</cell><cell>BER MAP</cell><cell>WER</cell><cell>WER MAP</cell><cell>FID</cell></row><row><cell>uncoded 70</cell><cell cols="5">0.143 0.140 1.000 0.999 265.474</cell></row><row><cell>coded 70/700</cell><cell cols="5">0.096 0.072 0.998 0.978 171.993</cell></row><row><cell cols="6">coded 70/1400 0.104 0.066 1.000 0.972 170.496</cell></row><row><cell cols="6">coded 70/2100 0.096 0.034 0.998 0.832 176.245</cell></row><row><cell>uncoded 100</cell><cell cols="5">0.164 0.162 1.000 1.000 234.358</cell></row><row><cell cols="6">coded 100/1000 0.099 0.074 1.000 0.996 153.743</cell></row><row><cell cols="6">coded 100/2000 0.097 0.053 1.000 0.981 162.889</cell></row><row><cell cols="6">coded 100/3000 0.098 0.035 1.000 0.925 163.049</cell></row><row><cell>uncoded 130</cell><cell cols="5">0.200 0.198 1.000 1.000 219.003</cell></row><row><cell cols="6">coded 130/1300 0.129 0.107 1.000 0.999 165.064</cell></row><row><cell cols="6">coded 130/2600 0.114 0.060 1.000 0.996 164.759</cell></row><row><cell cols="6">coded 130/3900 0.128 0.070 1.000 0.999 170.603</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison of test metrics between our method and the uncoded DVAE trained with the IWAE objective.</figDesc><table><row><cell>Model</cell><cell cols="5">BER WER Entropy Acc. Conf. Acc. PSNR SSIM</cell></row><row><cell>uncoded 8</cell><cell>0.089 0.384</cell><cell>0.467</cell><cell>0.594</cell><cell>0.595</cell><cell>15.598 0.509</cell></row><row><cell>uncoded 8 IWAE 10 samples</cell><cell>0.063 0.372</cell><cell>1.309</cell><cell>0.617</cell><cell>0.640</cell><cell>14.282 0.470</cell></row><row><cell>uncoded 8 IWAE 20 samples</cell><cell>0.075 0.447</cell><cell>1.391</cell><cell>0.634</cell><cell>0.651</cell><cell>14.237 0.460</cell></row><row><cell>uncoded 8 IWAE 30 samples</cell><cell>0.074 0.438</cell><cell>1.564</cell><cell>0.619</cell><cell>0.641</cell><cell>13.757 0.457</cell></row><row><cell>uncoded 8 IWAE 100 samples</cell><cell>0.107 0.583</cell><cell>2.274</cell><cell>0.596</cell><cell>0.663</cell><cell>12.996 0.433</cell></row><row><cell>uncoded 8 IWAE (α = 0.5, 10 samples)</cell><cell>0.059 0.353</cell><cell>0.929</cell><cell>0.630</cell><cell>0.638</cell><cell>14.582 0.488</cell></row><row><cell>uncoded 8 IWAE (α = 0.5, 20 samples)</cell><cell>0.067 0.388</cell><cell>1.066</cell><cell>0.638</cell><cell>0.644</cell><cell>14.432 0.479</cell></row><row><cell>uncoded 8 IWAE (α = 0.5, 30 samples)</cell><cell>0.053 0.349</cell><cell>1.165</cell><cell>0.630</cell><cell>0.644</cell><cell>14.178 0.472</cell></row><row><cell cols="2">uncoded 8 IWAE (DR, α = 0.5, 10 samples) 0.052 0.312</cell><cell>1.004</cell><cell>0.623</cell><cell>0.629</cell><cell>14.542 0.483</cell></row><row><cell cols="2">uncoded 8 IWAE (DR, α = 0.5, 20 samples) 0.056 0.353</cell><cell>1.041</cell><cell>0.600</cell><cell>0.614</cell><cell>14.231 0.463</cell></row><row><cell cols="2">uncoded 8 IWAE (DR, α = 0.5, 30 samples) 0.057 0.352</cell><cell>1.068</cell><cell>0.602</cell><cell>0.607</cell><cell>14.016 0.449</cell></row><row><cell>coded 8/80</cell><cell>0.021 0.144</cell><cell>2.905</cell><cell>0.750</cell><cell>0.816</cell><cell>17.318 0.619</cell></row><row><cell>coded 8/160</cell><cell>0.027 0.189</cell><cell>3.637</cell><cell>0.783</cell><cell>0.831</cell><cell>17.713 0.641</cell></row><row><cell>coded 8/240</cell><cell>0.037 0.231</cell><cell>4.000</cell><cell>0.799</cell><cell>0.893</cell><cell>17.861 0.653</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparison of the obtained metrics for the Coded-DVAE with polar codes with different configurations, which we refer to as 'hierarchical Coded-DVAE'.</figDesc><table><row><cell>Model</cell><cell>BER WER</cell><cell>Acc</cell><cell cols="2">Conf. Acc PSNR</cell></row><row><cell cols="3">hier. 5/50 0.099 0.400 0.753</cell><cell>0.800</cell><cell>17.130</cell></row><row><cell cols="3">hier. 5/100 0.050 0.330 0.784</cell><cell>0.870</cell><cell>17.513</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Comparison of the obtained error metrics in the different hierarchy levels.</figDesc><table><row><cell>Model</cell><cell cols="4">BER m 1 WER m 1 BER m 2 WER m 2</cell></row><row><cell>hier. 5/50</cell><cell>0.079</cell><cell>0.259</cell><cell>0.119</cell><cell>0.362</cell></row><row><cell>hier. 5/100</cell><cell>0.026</cell><cell>0.110</cell><cell>0.075</cell><cell>0.287</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Evaluation of reconstruction performance in FMNIST with different values of β using the coded model with an 8/240 code.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>Acc (train)</cell><cell>Conf. Acc. (train)</cell><cell>PSNR (test)</cell><cell>Acc (test)</cell><cell>Conf. Acc. (test)</cell><cell>Entropy</cell></row><row><cell>5</cell><cell cols="2">18.344 0.791</cell><cell>0.895</cell><cell cols="2">17.614 0.766</cell><cell>0.849</cell><cell>4.025</cell></row><row><cell>10</cell><cell cols="2">19.106 0.822</cell><cell>0.904</cell><cell cols="2">17.737 0.793</cell><cell>0.872</cell><cell>4.023</cell></row><row><cell>15</cell><cell cols="2">19.345 0.831</cell><cell>0.921</cell><cell cols="2">17.861 0.799</cell><cell>0.893</cell><cell>4.000</cell></row><row><cell>20</cell><cell cols="2">18.797 0.809</cell><cell>0.887</cell><cell cols="2">17.837 0.787</cell><cell>0.877</cell><cell>3.810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Evaluation of the BER and WER in FMNIST with different values of β using the coded model with an 8/240 code.</figDesc><table><row><cell cols="2">Beta BER WER</cell></row><row><cell>5</cell><cell>0.150 0.726</cell></row><row><cell>10</cell><cell>0.080 0.480</cell></row><row><cell>15</cell><cell>0.037 0.231</cell></row><row><cell>20</cell><cell>0.065 0.399</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Evaluation of the log-likelihood (LL) in FM-NIST with different values of β using the coded model with an 8/240 code.</figDesc><table><row><cell cols="3">Beta LL (train) LL (test)</cell></row><row><cell>5</cell><cell>-228.448</cell><cell>-234.629</cell></row><row><cell>10</cell><cell>-229.379</cell><cell>-237.495</cell></row><row><cell>15</cell><cell>-231.679</cell><cell>-238.459</cell></row><row><cell>20</cell><cell>-229.627</cell><cell>-235.927</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>Evaluation of reconstruction performance in FMNIST with different values of β using an uncoded model 8 information bits.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>Acc (train)</cell><cell>Conf. Acc. (train)</cell><cell>PSNR (test)</cell><cell>Acc (test)</cell><cell>Conf. Acc. (test)</cell><cell>Entropy</cell></row><row><cell>5</cell><cell cols="2">14.239 0.503</cell><cell>0.501</cell><cell cols="2">14.237 0.503</cell><cell>0.491</cell><cell>0.231</cell></row><row><cell>10</cell><cell cols="2">15.624 0.606</cell><cell>0.603</cell><cell cols="2">15.571 0.598</cell><cell>0.598</cell><cell>0.357</cell></row><row><cell>15</cell><cell cols="2">15.644 0.601</cell><cell>0.602</cell><cell cols="2">15.598 0.594</cell><cell>0.595</cell><cell>0.467</cell></row><row><cell>20</cell><cell cols="2">13.717 0.464</cell><cell>0.466</cell><cell cols="2">13.743 0.460</cell><cell>0.462</cell><cell>0.383</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 :</head><label>16</label><figDesc>Evaluation of the BER and WER in FMNIST with different values of β using an uncoded model 8 information bits.</figDesc><table><row><cell cols="2">Beta BER WER</cell></row><row><cell>5</cell><cell>0.203 0.852</cell></row><row><cell>10</cell><cell>0.086 0.384</cell></row><row><cell>15</cell><cell>0.089 0.384</cell></row><row><cell>20</cell><cell>0.278 0.939</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 :</head><label>17</label><figDesc>Evaluation of the log-likelihood (LL) in FM-NIST with different values of β using an uncoded model 8 information bits.Next, we evaluate the model in the image generation task. Fig.38contains examples of randomly generated images using the different configurations. Table16reports the obtained BER and WER, and Table17the estimated log-likelihood of the different values of β. The models trained with β = 15 and β = 10 clearly outperform the other two in this task, generating more diverse and detailed images, and obtaining better error metrics and log-likelihood values.</figDesc><table><row><cell cols="3">Model LL (train) LL (test)</cell></row><row><cell>5</cell><cell>-256.431</cell><cell>-257.983</cell></row><row><cell>10</cell><cell>-247.507</cell><cell>-249.460</cell></row><row><cell>15</cell><cell>-247.964</cell><cell>-249.880</cell></row><row><cell>20</cell><cell>-272.460</cell><cell>-273.554</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18 :</head><label>18</label><figDesc>Parameter count.</figDesc><table><row><cell>Model</cell><cell cols="2"># encoder parameters # decoder parameters</cell></row><row><cell>uncoded 8</cell><cell>6,592,008</cell><cell>19,341,185</cell></row><row><cell>uncoded 8 adjusted</cell><cell>6,717,538</cell><cell>19,581,035</cell></row><row><cell>coded 8/240</cell><cell>6,711,024</cell><cell>19,578,753</cell></row><row><cell>coded 8/240 adjusted</cell><cell>6,583,174</cell><cell>19,332,871</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19 :</head><label>19</label><figDesc>Evaluation of reconstruction performance in FMNIST with the adjusted parameter count.</figDesc><table><row><cell>Model</cell><cell>PSNR (train)</cell><cell>Acc (train)</cell><cell>Conf. Acc. (train)</cell><cell>PSNR (test)</cell><cell>Acc (test)</cell><cell>Conf. Acc. (test)</cell><cell>Entropy</cell></row><row><cell>uncoded 8</cell><cell cols="2">15.644 0.601</cell><cell>0.602</cell><cell cols="2">15.598 0.594</cell><cell>0.595</cell><cell>0.659</cell></row><row><cell>uncoded 8 adj.</cell><cell cols="2">15.530 0.586</cell><cell>0.586</cell><cell cols="2">15.491 0.581</cell><cell>0.580</cell><cell>0.449</cell></row><row><cell>coded 8/240</cell><cell cols="2">19.345 0.831</cell><cell>0.921</cell><cell cols="2">17.861 0.799</cell><cell>0.893</cell><cell>4.609</cell></row><row><cell cols="3">coded 8/240 adj. 19.383 0.828</cell><cell>0.883</cell><cell cols="2">17.771 0.792</cell><cell>0.828</cell><cell>3.952</cell></row><row><cell>ORIGINAL</cell><cell></cell><cell cols="2">UNCODED 8</cell><cell></cell><cell></cell><cell>UNCODED 8 adj.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CODED 8/240</cell><cell></cell><cell></cell><cell>CODED 8/240 adj.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20 :</head><label>20</label><figDesc>Evaluation of the BER and WER in FMNIST with the adjusted parameter count.</figDesc><table><row><cell>Model</cell><cell>BER WER</cell></row><row><cell>uncoded 8</cell><cell>0.089 0.384</cell></row><row><cell>uncoded 8 adj.</cell><cell>0.125 0.561</cell></row><row><cell>coded 8/240</cell><cell>0.037 0.231</cell></row><row><cell cols="2">coded 8/240 adj. 0.064 0.399</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 21 :</head><label>21</label><figDesc>Evaluation of the log-likelihood (LL) in FMNIST with the adjusted parameter count.</figDesc><table><row><cell>Model</cell><cell cols="2">LL (train) LL (test)</cell></row><row><cell>uncoded 8</cell><cell>-247.964</cell><cell>-249.880</cell></row><row><cell>uncoded 8 adj.</cell><cell>-250.543</cell><cell>-252.408</cell></row><row><cell>coded 8/240</cell><cell>-231.679</cell><cell>-238.459</cell></row><row><cell>coded 8/240 adj.</cell><cell>-229.283</cell><cell>-238.302</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code: https://github.com/mariamartinezgarcia/codedVAE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For the example in (7), this results in p θ (c|x) ≈ qη(c|x) = Ber(c1; q1)Ber(c2; q1)Ber(c3; q2)Ber(c4; q2)Ber(c5; q3)Ber(c6; q3).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">María Martínez-García</rs> was supported by the <rs type="funder">Generación de Conocimiento</rs> grant <rs type="grantNumber">PID2022-142506NA-I00</rs>. <rs type="person">Grace Villacrés</rs> was supported by the <rs type="funder">Comunidad de Madrid</rs> within the 2023-2026 agreement with <rs type="funder">Universidad Rey Juan Carlos</rs> for the granting of direct subsidies for the promotion, encouragement of research and technology transfer, line of Action A Emerging Doctors, under <rs type="person">Project OrdeNGN</rs> (<rs type="grantNumber">Ref. F1177</rs>). <rs type="person">David Mitchell</rs> was supported by the <rs type="funder">National Science Foundation</rs> under Grant No. <rs type="grantNumber">CCF-2145917</rs> and he acknowledges travel support from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> under Grant Agreement No. <rs type="grantNumber">951847</rs>. <rs type="person">Pablo M. Olmos</rs> was supported by the <rs type="funder">Comunidad de Madrid</rs> <rs type="grantNumber">IND2022/TIC-23550</rs>, <rs type="projectName">IDEA-CM</rs> project (<rs type="grantNumber">TEC-2024/COM-89</rs>), the <rs type="funder">ELLIS Unit Madrid (European Laboratory for Learning and Intelligent Systems)</rs>, the <rs type="grantName">2024 Leonardo Grant for Scientific Research and Cultural Creation</rs> from the <rs type="funder">BBVA Foundation</rs>, and by projects <rs type="grantNumber">MICIU/AEI/10.13039/501100011033/FEDER</rs> and <rs type="institution">UE</rs> (<rs type="grantNumber">PID2021-123182OB-I00</rs>; EPiCENTER).</p><p>We would like to thank <rs type="person">Isabel Valera</rs> for her valuable feedback on earlier drafts of the manuscript, as well as for the discussions that helped us improve the presentation of our method to the machine learning community.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gJWWhC7">
					<idno type="grant-number">PID2022-142506NA-I00</idno>
				</org>
				<org type="funding" xml:id="_vF6BDmc">
					<idno type="grant-number">Ref. F1177</idno>
				</org>
				<org type="funding" xml:id="_hgMmDWn">
					<idno type="grant-number">CCF-2145917</idno>
				</org>
				<org type="funding" xml:id="_qzrsQXC">
					<idno type="grant-number">951847</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funded-project" xml:id="_DskzkGr">
					<idno type="grant-number">IND2022/TIC-23550</idno>
					<orgName type="project" subtype="full">IDEA-CM</orgName>
				</org>
				<org type="funding" xml:id="_AA9ZuHE">
					<idno type="grant-number">TEC-2024/COM-89</idno>
					<orgName type="grant-name">2024 Leonardo Grant for Scientific Research and Cultural Creation</orgName>
				</org>
				<org type="funding" xml:id="_cSezb7D">
					<idno type="grant-number">MICIU/AEI/10.13039/501100011033/FEDER</idno>
				</org>
				<org type="funding" xml:id="_nwf9xem">
					<idno type="grant-number">PID2021-123182OB-I00</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Channel Polarization: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels</title>
		<author>
			<persName><forename type="first">Erdal</forename><surname>Arikan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3051" to="3073" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Importance Weighted Autoencoders</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Fault and Severity Diagnosis for Self-Organizing Networks Using Deep Supervised Learning and Unsupervised Transfer Learning</title>
		<author>
			<persName><forename type="first">Kuan-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Chi</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ta-Sung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="157" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Alpha-divergence variational inference meets importance weighted auto-encoders: Methodology and asymptotics</title>
		<author>
			<persName><forename type="first">Kamélia</forename><surname>Daudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">243</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">The MNIST Database of Handwritten Digit Images for Machine Learning Research</title>
		<imprint/>
	</monogr>
	<note>Best of the Web</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-density parity-check codes</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of Deep Learning-Based CSI Feedback in Massive MIMO Systems</title>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Kai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">Ye</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8017" to="8045" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Buy 4 REINFORCE Samples, Get a Baseline for Free</title>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 Deep Reinforcement Learning meets Structured Prediction Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frank R Kschischang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-A</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tiny ImageNet Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Hierarchical Discrete Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Julien Lievin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Dittadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Error Correction Coding: Mathematical Methods and Algorithms</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-Shot Text-to-Image Generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aaron Van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discrete Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolfe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On Using Control Variates with Stochastic Approximation for Variational Bayes and its Connection to Stochastic Linear Regression</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.1022</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Novel tight classification error bounds under mismatch conditions based on f-divergence</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Nussbaum-Thom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Deep Generative Decoder: MAP estimation of representations improves modelling of single-cell RNA data</title>
		<author>
			<persName><forename type="first">Viktoria</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">497</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication. The Bell System Technical</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Learning for Super-Resolution Channel Estimation in Reconfigurable Intelligent Surface Aided Systems</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arumugam</forename><surname>Nallanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1491" to="1503" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vae with a vampprior</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="9" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical Lossless Compression with Latent Variables using Bits Back Coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DVAE++: Discrete Variational Autoencoders with Overlapping Transformations</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="5035" to="5044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Discrete Representation Learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical Quantized Autoencoders</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4524" to="4535" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sensing Integrated DFT-Spread OFDM Waveform and Deep Learning-Powered Receiver Design for Terahertz Integrated Sensing and Communication Systems</title>
		<author>
			<persName><forename type="first">Yongzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Lemic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="595" to="610" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: a Novel Image Dataset For Benchmarking Machine Learning Algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Refined statistical bounds for classification error mismatches with constrained bayes error</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahe</forename><surname>Eminyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="283" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning-Based Scheduling for NR-U/WiGig Coexistence in Unlicensed mmWave Bands</title>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="73" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
