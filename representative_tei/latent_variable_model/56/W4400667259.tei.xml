<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Diffusion for Neural Spiking Data</title>
				<funder ref="#_fdGBUvX">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder ref="#_XVUPH5q">
					<orgName type="full">Human Frontier Science Program</orgName>
					<orgName type="abbreviated">HFSP</orgName>
				</funder>
				<funder ref="#_QWcsXg2">
					<orgName type="full">Germany&apos;s Excellence Strategy</orgName>
				</funder>
				<funder ref="#_MuKsarf">
					<orgName type="full">ERC, DeepCoMechTome</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_fptZ2UX">
					<orgName type="full">German Federal Ministry of Education and Research (Tübingen AI Center</orgName>
				</funder>
				<funder ref="#_gv5wacu #_jkKvZJr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jaivardhan</forename><surname>Kapoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Auguste</forename><surname>Schulz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julius</forename><surname>Vetter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning in Science</orgName>
								<orgName type="institution">University of Tübingen &amp; Tübingen AI Center</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department Empirical Inference</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Diffusion for Neural Spiking Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern datasets in neuroscience are becoming increasingly high-dimensional with fast-paced innovations in measurement technology <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b22">23]</ref>, granting access to hundreds to thousands of simultaneously recorded neurons. At the same time, the types of animal behaviors and sensory stimuli under investigation have become more naturalistic and complex, resulting in experimental setups with heterogeneous trials of varying length, or lacking trial structure altogether <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57]</ref>. Therefore, a key target in systems neuroscience has shifted towards understanding the relationship between high-dimensional neural activity and complex behaviors.</p><p>For high-dimensional neural recordings, analyses that infer low-dimensional structures have been very useful for making sense of such data <ref type="bibr" target="#b10">[11]</ref>. For example, latent variable models (LVMs) are often used to identify neural population dynamics not apparent at the level of single neurons <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. More recently, deep learning-based approaches based on variational autoencoders (VAEs) <ref type="bibr">[26, 44,</ref> 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2407.08751v2 [q-bio.NC] 2 Dec 2024 <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> have become particularly popular for inferring latent neural representations due to their expressiveness and ability to scale to large, heterogeneous neural recordings with behavioral covariates <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>However, in addition to learning latent representations, another important consideration is the ability to act as faithful generative models of the data. In other words, models should be able to produce diverse, realistic samples of the neural activity they were trained on, ideally in a behavior-or stimulus-dependent manner. Models with such capabilities not only afford better interpretability analyses and diagnoses for whether structures underlying the data are accurately learned, but have a variety of downstream applications surrounding the design of closed-loop in silico experiments. For example, with faithful generative models, one can simulate population responses to hypothetical sensory, electrical, or optogenetic stimuli, as well as possible neural activity underlying hypothetical movement patterns. Most VAE-based approaches focus on the interpretability of the inferred latents, but not the ability to generate realistic and diverse samples when conditioning on external covariates, while sample-realistic models (e.g., based on generative adversarial networks (GANs) <ref type="bibr" target="#b16">[17]</ref>) do not provide access to underlying low-dimensional representations. As such, there is a need for models of neural population spiking activity that both provide low-dimensional latent representations and can (conditionally) generate realistic neural activity.</p><p>Here, we propose Latent Diffusion for Neural Spiking data (LDNS), which combines the ability of autoencoders to extract low-dimensional representations of discrete neural population activity, with the ability of (conditional) denoising diffusion probabilistic models (or, diffusion models) to generate realistic neural spiking data by modeling the inferred low-dimensional continuous representations. LDNS allows for (un)conditional generation of neural spiking data through combining a regularized autoencoder with diffusion models that act on the low-dimensional latent time series underlying neural population activity.</p><p>Diffusion models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref> have been highly successful for conditional and unconditional data generation in several domains, including images <ref type="bibr" target="#b19">[20]</ref>, molecules <ref type="bibr" target="#b58">[59]</ref>, and audio spectrograms <ref type="bibr" target="#b26">[27]</ref> and have demonstrated samplingfidelity that outperforms that of VAEs and GANs <ref type="bibr" target="#b19">[20]</ref>. A key strength of diffusion models that makes them particularly attractive in the context of modeling neural datasets is the ability to flexibly condition the generation on various (potentially complex) covariates, such as to simulate neural activity given certain behaviors. Recently, diffusion models have been extended to continuous neural time series such as local field potentials (LFPs) and electroencephalography (EEG) recordings <ref type="bibr" target="#b52">[53]</ref>. However, due to the discrete nature of spiking data, standard diffusion models cannot be easily applied, thus excluding their use on many datasets in systems neuroscience.</p><p>To bypass these limitations, LDNS employs a regularized autoencoder using structured state-space (S4) layers <ref type="bibr" target="#b17">[18]</ref> to project the high-dimensional discrete spiking data into smooth, low-dimensional latents without making assumptions about the trial structure. We then train a diffusion model with S4 layers as a generative model of the inferred latents-akin to latent diffusion for images <ref type="bibr" target="#b44">[45]</ref>, where generation can be flexibly conditioned on behavioral covariates or task conditions.</p><p>A fundamental assumption of most low-dimensional latent variable models is that all statistical dependencies between observations are mediated by the latent space. However, in neural spiking data, there are prominent statistical dependencies that ought to persist conditional on the latent state, e.g., single-neuron dynamics such as refractory periods, burstiness, firing rate adaptation, or potential direct synaptic interactions. We show how such additional structure can be accounted for in LDNS, by equipping it with an expressive observation model <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref>: We use a Poisson model for spike generation with autoregressive couplings which are optimized post hoc to capture the temporal structure of single-neuron activity. This allows LDNS to capture a wide range of biological neural dynamics <ref type="bibr" target="#b54">[55]</ref>, with only a small additional computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main contributions</head><p>In summary, LDNS is a flexible method that allows for both high-fidelity diffusion-based sampling of neural population activity and access to time-aligned low-dimensional representations, which we validate on a synthetic dataset. Next, we show the utility and flexibility of this approach on complex real datasets: First, LDNS can handle variable-length spiking recordings from the human cortex. Second, LDNS can unconditionally generate faithful neural spiking activity recorded from monkeys performing a reach task. We demonstrate how LDNS can be equipped with an expressive autoregressive observation model that accounts for additional dependencies between data points (e.g., single neuron dynamics), increasing the realism of generated samples. Third, LDNS can generate realistic neural activity while conditioning on either reach direction or full reach trajectories (time series), including unseen behaviors that are then accurately decoded from the simulated neural data. Overall, LDNS enables simultaneous inference of low-dimensional latent representations for single-trial data interpretation and high-fidelity diffusion-based (conditional) generation of diverse neural spiking datasets, which will allow for closed-loop in silico experiments and hypothesis testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Diffusion for Neural Spiking Data (LDNS)</head><p>We consider a dataset recorded from a population of n neurons, consisting of trials with spiking data s ∈ N n×T 0 (sorted into bins of fixed length resulting in spike counts over time), and optional simultaneously recorded behavioral covariates y ∈ R n (that can also be time-varying y ∈ R n×T ). A dataset of M such trials D can be written as D = {s (i) , y (i) }, possibly with varying trial lengths T 1 . . . T M . We make the assumption that a large fraction of the variability in this dataset can be captured with a few underlying latent variables z ∈ R d×T , where d &lt; n.</p><p>Our goal is to generate realistic spiking data s * that faithfully capture both population-level and singleneuron dynamics of s 1...T with the ability to optionally condition the generation on behavior y cond . To this end, we propose a new method, LDNS, that combines the strength of neural dimensionality reduction approaches with that of diffusion-based generation.</p><p>LDNS uses a two-stage training framework, adopted from the highly successful family of latent diffusion models (LDMs) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b58">59]</ref>. To train LDNS, we first train a regularized autoencoder <ref type="bibr" target="#b13">[14]</ref> to compress the spiking data into a low-dimensional continuous latent space (Fig. <ref type="figure" target="#fig_0">1</ref>). Concretely, we focus on two objects of interest for the LDNS autoencoder: (1) inferring a time-aligned, lowdimensional smooth representation z ∈ R d×T that preserves the shared variability of the spiking data, and (2) predicting smooth firing rates λ that are most likely to give rise to the observed spiking data.</p><p>In the second stage, we train a diffusion model in latent space, possibly employing conditioning to make generation contingent on external (e.g., behavioral) covariates (Fig. <ref type="figure" target="#fig_0">1</ref>). For the diffusion model, our main objective is the generation of z * ∈ R d×T that captures the distribution of inferred autoencoder latents. We also want the ability to sample latent trajectories of varying length.</p><p>In both stages, we use structured state-space (S4) <ref type="bibr" target="#b17">[18]</ref> layers for modeling temporal dependencies. S4 layers consist of state-space transition matrices that can be unrolled into arbitrary-length convolution kernels, allowing sequence modeling of varying lengths. For details on network architectures and S4 layers, see appendix A1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regularized autoencoder for neural spiking data</head><p>For the spiking data, we choose a Poisson observation model, and train autoencoders by minimizing the Poisson negative log-likelihood of the input spikes s given the predicted rates λ = decoder(z). To enforce smoothness in the latent space, where z = encoder(s), we add an L 2 regularization along with a temporal smoothness regularizer over z, resulting in the combined loss</p><formula xml:id="formula_0">L AE = E s∼D      n,T i=1 t=1 (λ i (t) -s i (t) ln λ i (t)) Poisson NLL +β 1 ∥z∥ 2 L2 reg. +β 2 K,T k=1 t=k+1 ∥z(t) -z(t -k)∥ 2 (1 + k) temporal smoothness      .<label>(1)</label></formula><p>Code available at <ref type="url" target="https://github.com/mackelab/LDNS">https://github.com/mackelab/LDNS</ref>.</p><p>To prevent the autoencoder from predicting highly localized Poisson rates, which have sharp peaks at input spike locations, we further regularize training using coordinated dropout <ref type="bibr" target="#b23">[24]</ref>, i.e., we randomly mask input spikes and compute the loss on the predicted rates at the masked locations (details in appendix A1.2).</p><p>Accounting for single-neuron dynamics with an expressive observation model So far, LDNS (like most latent variable models for neural data) uses a Poisson observation model, which assumes that all statistical dependencies are mediated by the latent state. To address this limitation and to capture dynamics and variability, which are "private" to individual neurons (such as refractory periods or burstiness), we propose to learn an autoregressive observation model. We make the predicted Poisson rates for each neuron i dependent also on recent spiking history, by including additional spike history couplings h i <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32]</ref>, resulting in the observation model</p><formula xml:id="formula_1">s i (t) ∼ exp   log λ i (t) + h i,0 + T ′ τ =1 h i,τ s i (t -τ )   ,<label>(2)</label></formula><p>where T ′ corresponds to the time-lagged window length. This modification is learned post hoc, and the parameters h i are fit with a maximum-likelihood objective (details in appendix A1.3). This approach does not alter the latent dynamics, while augmenting the model with single-neuron autoregressive dynamics. We observe that including spike history increases the realism of generated data and enables us to accurately capture single-neuron autocorrelation structures (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Denoising Diffusion Probabilistic Models</head><p>In the second stage of training, we train diffusion models <ref type="bibr" target="#b19">[20]</ref> to generate (conditional) samples from the distribution of inferred latents. The training dataset therefore contains autoencoder-derived latents of each trial, and optionally, additional conditioning information such as the corresponding behavior, i.e., D z = {z (i) = encoder(s (i) ), y (i) }.</p><p>Diffusion models aim to approximate the data distribution q(z) through an iterative denoising process starting from standard Gaussian noise. For latent z (denoted as z 0 for diffusion timestep 0), we first produce a noised version at step t by adding Gaussian noise of the form q(z t |z 0 ) = N ( √ ᾱt z 0 , (1 -ᾱt )I). Here, ᾱt = t k=1 α k , where the noise scaling factors α 1 . . . α T follow a fixed linear schedule. We then train a neural network to approximate the reverse process p θ (z t-1 |z t ) for each diffusion timestep. The true (denoising) reverse transition q(z t-1 |z t ) is intractable-however, we can apply variational inference to learn the conditional reverse transition q(z t-1 |z t , z 0 ), which has a closed form written as</p><formula xml:id="formula_2">q(z t-1 |z t , z 0 ) = N √ α t (1 -ᾱt-1 ) 1 -ᾱt z t + √ ᾱt-1 (1 -α t ) 1 -ᾱt z 0 , (<label>1</label></formula><formula xml:id="formula_3">-α t )(1 -ᾱt-1 ) 1 -ᾱt I .<label>(3)</label></formula><p>We train the neural network µ θ (z t , t) to approximate the mean of this distribution by optimizing the loss E z0∼Dz,ϵ0,t ∥ϵ θ (z t , t) -ϵ 0 ∥ 2 , where ϵ 0 is the noise used to generate z t from z 0 , and ϵ θ (z t , t) is the equivalent reparameterization for µ θ (z t , t). At test time, we sequentially sample z t-1 given z t using the learned transition p θ (z t-1 |z t ), starting from standard Gaussian noise. Using S4 layers in the denoising network allows us to generate latents with varying lengths. This is achieved by unrolling the state transition matrix in the S4 layers to the desired length for each denoising step.</p><p>Diffusion models may be conditioned on fixed-length and time-varying covariates y, in which case we learn the approximate reverse transition p θ (z t-1 |z t , y). Details on the conditioning mechanisms in appendix A1.4.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and tasks</head><p>We first evaluate the performance of LDNS on a synthetic spiking dataset where we have access to the ground-truth firing rates and latents. We choose the Lorenz attractor <ref type="bibr" target="#b28">[29]</ref> as a low-dimensional, non-linear dynamical system commonly used in neuroscience <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. We simulate rates as an affine mapping from the 3-dimensional system to a 128-dimensional neural space, and sample from a Poisson distribution to generate spiking data. Next, we showcase the applicability of LDNS on two neural datasets: We apply our method on a highly complex dataset of human neural activity (128 units) recorded during attempted speech <ref type="bibr" target="#b55">[56]</ref>. This dataset poses a challenge to many modeling approaches due to the different imagined sentences, resulting in variable lengths of the neural time series (between 2-10 seconds with a sampling rate of 50 Hz). Finally, we apply LDNS to model premotor cortical activity (182 units) recorded from monkeys performing a delayed center-out reach task with barriers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>. The multi-modal nature of the dataset allows us to assess both unconditional as well as conditional generation of neural spiking activity given monkey reach directions and entire velocity profiles of the performed reaches. See appendix A2,A3 for data and training details.</p><p>For the unconditional generation of monkey reach recordings (Sec. 3.4), we train both a Poisson observation model as well as a spike history-dependent autoregressive observation model. For all other experiments, we only train a Poisson observation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare LDNS to the most commonly known VAE-based latent variable model: Latent Factor Analysis via Dynamical Systems (LFADS <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref>), which has been shown to outperform various classical latent variable models on a variety of tasks ( <ref type="bibr" target="#b37">[38]</ref>, details in appendix A4).</p><p>To ensure that we use optimal hyperparameters for LFADS, we follow the auto-ML pipeline proposed by Keshtkaran et al. <ref type="bibr" target="#b24">[25]</ref>. This approach, termed AutoLFADS, has been shown to perform better than the original LFADS on benchmark tasks <ref type="bibr" target="#b37">[38]</ref>. For the unconditional generation of monkey reach recordings, we further compared to additional VAE baselines <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref> (appendix A5).</p><p>Metrics For all experiments, we assess how well LDNS-generated samples match the spiking data in the training distribution. Concretely, we compare population-level statistics by computing 1) the distribution over the population spike count, which sums up all spikes co-occurring in the population in a single time bin (i.e., spike count histogram), and 2) pairwise correlations of LDNS samples and the spiking data for each pair of neurons. For single-neuron statistics, we compare 3) the mean and 4) standard deviation of the inter-spike-interval distribution for each neuron (mean isi and std isi). When multiple spikes occur in a single time bin, the spike times are distributed equally in this bin <ref type="bibr" target="#b11">[12]</ref>. To further evaluate population dynamics, we compare the principal components of smoothed spikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LDNS captures the true spiking data distribution with an underlying Lorenz system</head><p>We simulate trials of length 256 timesteps from the three-dimensional (chaotic) Lorenz system (Fig. <ref type="figure" target="#fig_1">2a</ref>). The regularized autoencoder extracts smooth latent time series (eight latent dimensions) from the 128-dimensional spiking data, resulting in smooth firing rate predictions that closely match the ground-truth rates (Fig. <ref type="figure" target="#fig_1">2b</ref>, Supp. Fig. <ref type="figure" target="#fig_6">A2,</ref><ref type="figure" target="#fig_7">A3</ref>). We then train a diffusion model on the extracted autoencoder latents. Latents sampled from the diffusion model (red) preserve the attractor geometry of the Lorenz system (Fig. <ref type="figure" target="#fig_1">2c</ref>, left, three of the eight latent dimensions), indicating that LDNS preserves a meaningful latent space. The architectural choice of S4 layers allows for length generalization: although we train on time segments of 256-time steps, we can sample and successfully generate latent trajectories that are much longer, but still accurately reflect the Lorenz dynamics (Fig. <ref type="figure" target="#fig_1">2c</ref>, middle, 16× longer generation). In comparison, LFADS exhibits instabilities when generating such longer sequences (appendix A6.1). Overall, the latent time series distribution is captured well by the diffusion model, with matching power spectral densities (PSD) per latent dimension (Fig. <ref type="figure" target="#fig_1">2c</ref>, right, other dimensions in Supp. Fig. <ref type="figure" target="#fig_7">A3</ref>).</p><p>To assess the sampling fidelity of the generated synthetic neural activity, we compute a variety of spike statistics frequently used in neuroscience. LDNS captures both population-level statistics, such as the population spike count histogram and pairwise correlations between neurons (Fig. <ref type="figure" target="#fig_1">2d</ref>), as well as single-neuron statistics, quantified by the mean and standard deviation of inter-spike-intervals (Fig. <ref type="figure" target="#fig_1">2e</ref>). LDNS also captures the temporal correlation structure of the data (Supp. Fig. <ref type="figure" target="#fig_3">A4</ref>). These results demonstrate that LDNS can both perform inference of low-dimensional latents and provide high-fidelity diffusion-based generation that perfectly captures the statistics of the ground-truth synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling variable-length trials of neural activity recorded in human cortex</head><p>Next, we assess whether LDNS is capable of capturing real electrophysiological data, applying it to neural recordings from human cortex during attempted speech (Fig. <ref type="figure" target="#fig_2">3a</ref>, top, Willett et al. <ref type="bibr" target="#b55">[56]</ref>).</p><p>A participant with a degenerative disease who is unable to produce intelligible speech attempts to vocalize sentences prompted on a screen, while neural population activity is recorded from the ventral premotor cortex. Since there is a large variation in the length of prompted sentences (Fig. <ref type="figure" target="#fig_2">3a</ref>, bottom), this dataset allows us to evaluate the performance of LDNS on real data in naturalistic settings with variable-length and highly heterogeneous dynamics.</p><p>To account for varying trial length during autoencoder training, we pad all trials to a maximum length of 512 bins and compute the reconstruction loss only on the observed time bins. For the diffusion model, we indicate the target trial length with a binary mask as a conditioning variable.</p><p>This approach allows us to infer time-aligned latents underlying the cortical activity of the participants, compressing the population activity by a factor of four before training an unconditional diffusion model on these latents. Resulting samples of LDNS, mimicking human cortical activity, are visually indistinguishable from the real data (Fig. <ref type="figure" target="#fig_2">3b,</ref><ref type="figure">c</ref>, additional samples in Supp. Fig. <ref type="figure">A7</ref>). This is reflected in closely matched population spike count histograms (Fig. <ref type="figure" target="#fig_2">3d</ref>, left), and single neuron statistics such as mean and standard deviation of the inter-spike interval (Fig. <ref type="figure" target="#fig_2">3d</ref>, right). Additionally, real and LDNS-sampled spikes display similar population dynamics, as reflected in the top principal components (Supp. Fig. <ref type="figure">A8</ref>). While LDNS tends to overestimate some pairwise correlations, it captures prominent features of the correlation structure in the data (Fig. <ref type="figure" target="#fig_2">3e</ref>, Pearson correlation coefficient r = 0.47), and our analysis indicates that this slight mismatch already arises at the autoencoder stage (Supp. Fig. <ref type="figure" target="#fig_12">A9</ref>).</p><p>LDNS allows for both inferring latent representations and generating variable-length trial data, making it applicable to complex real neural datasets without a fixed trial structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Realistic generation of spiking data from a monkey performing reach tasks</head><p>We further evaluate LDNS in a different setting by applying it to model sparse spiking data recorded from a monkey performing a reaching task constrained by barriers that form a maze (Fig. <ref type="figure" target="#fig_3">4a</ref>, left). The variety of different maze architectures leads to diverse reach movements of both curved and straight reaches (Fig. <ref type="figure" target="#fig_3">4a</ref>, right). We again infer low-dimensional latent trajectories that capture the shared variability of the neural population and then train an unconditional diffusion model on these latents. Sampled spikes from LDNS closely resemble the true, sparse population data (Fig. <ref type="figure" target="#fig_3">4b</ref>, additional samples in Supp. Fig. <ref type="figure" target="#fig_0">A11</ref>), and closely match population-level spike statistics (Fig. <ref type="figure" target="#fig_3">4c</ref>). Single neuron statistics in this low spike count regime (a maximum of three spikes per neuron in 5 ms bins) are also captured well (Fig. <ref type="figure" target="#fig_3">4d</ref>), and are on par with or better than LFADS <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref> (see Table <ref type="table" target="#tab_0">1</ref> for summary of main comparisons, and appendix A5 for additional baselines <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref>). Beyond spiking statistics, we observe that LDNS also preserves the temporal structure of population dynamics, as reflected in the top principal components of smoothed spikes (Supp. Fig. <ref type="figure" target="#fig_4">A15</ref>). Thus, LDNS can generate spiking data that is faithful at the level of both single-neuron and population dynamics.  Both LFADS and the LDNS autoencoder are optimized by maximizing the Poisson log-likelihood, and thus cannot capture single-neuron dynamics such as refractoriness <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b54">55]</ref>, which can have a strong influence on the observed autocorrelation structure. Given the overall sparsity of the spiking data and resulting low correlations (Supp. Fig. <ref type="figure" target="#fig_0">A10</ref>), we focus on the temporal structure of autocorrelations averaged within groups of neurons (≈ 45 neurons per group) split by their instantaneous correlation strength <ref type="bibr" target="#b31">[32]</ref>: darker colors correspond to the highest correlated group of four, lighter colors correspond to the group with the second highest correlations (Fig. <ref type="figure" target="#fig_3">4e</ref>). We then compare these auto-correlations to those of grouped LDNS samples with a Poisson observation model (red). As expected, LDNS with Poisson observations is unable to capture the dip in the data auto-correlation at 5 ms lags (one time bin) (Fig. <ref type="figure" target="#fig_3">4e</ref>, left).</p><p>To overcome this mismatch, we train an additional spike history-dependent autoregressive observation model on top of the inferred rates (LDNSsh, for spike history). In contrast to the Poisson samples, autoregressive samples can capture this aspect of neural spiking data very accurately while also improving the overall fit to the empirical auto-correlation (Fig. <ref type="figure" target="#fig_3">4e</ref>, right). Moreover, the post hoc optimization of these filters also improves modeling of other single-neuron, as well as population-level statistics, such as the population spike count histogram or the mean of the isi (Table <ref type="table" target="#tab_0">1</ref>, Supp. Fig. <ref type="figure" target="#fig_14">A13</ref>).</p><p>We view this post-hoc augmentation as a key modular contribution, which can be flexibly applied to other generative models. To this end, we extend AutoLFADS with spike history dependence (LFADSsh), improving its performance across metrics. The augmented LFADSsh also captures the dip in autocorrelation at 5 ms lags (Supp. Fig. <ref type="figure" target="#fig_1">A12</ref>). Still, in both observation model variants, LDNS maintains superior or comparable performance (Table <ref type="table" target="#tab_0">1</ref>).</p><p>Thus, Poisson LDNS allows for the generation of spiking data that is on par or better in terms of sampling fidelity than previous approaches. Incorporating spike-history dependence and sampling spikes autoregressively allows us to further increase the realism of generated spike trains, leading to a large improvement on several of the considered metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conditional generation of neural activity given reach directions or velocity profiles</head><p>Lastly, we assess the ability of conditional LDNS to generate realistic neural activity conditioned on behavioral covariates of varying complexity: the reach angle or entire velocity time series (Fig. <ref type="figure" target="#fig_4">5a</ref>). We first validate that the autoencoder predicts firing rates that allow us to linearly decode the behavior following the ridge-regression approach proposed in <ref type="bibr" target="#b37">[38]</ref>. Decoded behavior from autoencoder reconstructed rates matches the true trajectories of unseen test trials (Fig. <ref type="figure" target="#fig_4">5b</ref>).</p><p>Given that the autoencoder performs adequately, we then test the ability to generate neural time series conditioned on the initial reach angle performed by the monkey θ reach . Indeed, from the generated samples of neural activity, we can decode-using the same linear decoder-realistic reach kinematics that are consistent with the conditioning angle θ reach and overall reach kinematics (Fig. <ref type="figure" target="#fig_4">5c</ref>). This indicates that LDNS can generate realistic neural activity consistent with a queried reach direction.</p><p>An even more challenging task that is intriguing for hypothesis generation is the ability to mimic an entire experiment and ask what the neural activity would have looked like if the monkey had performed a particular hypothetical movement. To this end, we train a diffusion model on the same autoencoder-inferred latents but now condition on entire velocity traces (Fig. <ref type="figure" target="#fig_4">5d</ref>). Velocityconditioned LDNS is able to produce different samples of neural activity that are consistent with, but not exact copies of, the reach trajectories of the held-out trials given as the conditioning covariate. Such closed-loop conditioning experiments open the possibility of making predictions about neural activity during desired unseen behaviors, and thus make experimentally testable predictions.</p><p>Finally, to understand how LDNS incorporates behavioral information, we analyzed latent trajectories that were conditionally sampled based on straight reach movements in different directions (Fig. <ref type="figure" target="#fig_4">5e</ref>). Individual samples of latent trajectories vary smoothly within a trial (Fig. <ref type="figure" target="#fig_4">5f</ref>), while reach direction varies smoothly across samples in the first principal component (PC1) of the latents (Fig. <ref type="figure" target="#fig_4">5g,</ref><ref type="figure">left</ref>). Projection onto the first two PCs of latent trajectories shows clear clustering by reach direction (Fig. <ref type="figure" target="#fig_4">5g</ref>, right), and we show that such clustering arises already at the autoencoder stage (Supp. Fig. <ref type="figure" target="#fig_17">A17</ref>).</p><p>In summary, LDNS not only produces faithful spiking samples but also allows for flexible conditioning. Furthermore, LDNS learns an interpretable latent space with behaviorally-relevant structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Latent variable models of neural population dynamics LDNS builds on previous LVMs in neuroscience, which have been extensively applied to infer low-dimensional latent representations of neural spiking data <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b62">63]</ref> (see <ref type="bibr" target="#b37">[38]</ref> for a comprehensive list.) In addition to capturing shared population-level dynamics and dependence on external stimuli <ref type="bibr" target="#b3">[4]</ref>, LVMs have been extended to allow autoregressive neuron-level (non-Poisson) dynamics <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b61">62]</ref> or even direct neural interactions <ref type="bibr" target="#b53">[54]</ref>. While these methods often have useful inductive biases (e.g., linear dynamical systems <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28]</ref> or Gaussian process priors <ref type="bibr" target="#b60">[61]</ref>), these models are typically not expressive enough to yield realistic neural samples across a range of conditions.</p><p>Deep LVMs and other deep learning-based approaches Variational autoencoders (VAEs) <ref type="bibr" target="#b25">[26]</ref> are particularly popular in neuroscience as they allow us to infer low-dimensional dynamics underlying high-dimensional discrete data <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b45">46]</ref>, especially when combined with nonlinear recurrent neural networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21]</ref>. VAEs have been used to infer identifiable low-dimensional latent representations conditioned on behavior <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b20">21]</ref> and have incorporated smoothness priors using Gaussian Processes to regularize the latent space <ref type="bibr" target="#b15">[16]</ref>. However, the generation performance of VAEs is rarely explored in neuroscience. Besides VAEs, generative adversarial networks (GANs <ref type="bibr" target="#b16">[17]</ref>) have been proposed to synthesize spiking neural population activity <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref>. While GANs produce high-fidelity samples, they are challenging to train reliably and lack a low-dimensional latent space. More recently, transformer-based architectures have also been adapted to model neural activity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b59">60]</ref>, though often with the focus of accurate decoding of behavior instead of generation of realistic spiking samples, while also lacking an explicit latent space <ref type="bibr" target="#b2">[3]</ref>. Lastly, deterministic approaches utilizing RNNs for dynamical systems reconstruction also target low-dimensional latent dynamics underlying neural data <ref type="bibr" target="#b18">[19]</ref>, but they do not act as probabilistic generative models.</p><p>Diffusion models LDNS leverages recent advances in diffusion models, which have become state-of-the-art for high-fidelity generation in several domains <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, including continuous-valued neural signals such as EEG <ref type="bibr" target="#b52">[53]</ref>, as well as in time series forecasting and imputation tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b42">43]</ref>. Similar to the LDNS architecture, Alcaraz and Strodthoff <ref type="bibr" target="#b1">[2]</ref> also use an S4-based denoiser for imputation. More specifically, LDNS is inspired by latent diffusion models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b14">15]</ref>, which benefit from operating on the latent space of an autoencoder and flexible conditioning mechanisms to generate samples based on a given covariate, as is done with text-to-image <ref type="bibr" target="#b44">[45]</ref> and other cross-modality scenarios. Conveniently, this allows LDNS to bypass the challenges of directly modeling discrete-valued spiking data, by instead transforming spikes into the continuous latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and discussion</head><p>We here proposed LDNS, a flexible generative model of neural spiking recordings that simultaneously infers low-dimensional latent representations and generates realistic neural activity conditioned on behavioral covariates. We apply LDNS to model three different datasets: synthetic data simulated from chaotic Lorenz dynamics, human cortical recordings with heterogeneous and variable-length trials, and finally, neural recordings in monkeys performing reach actions in a maze. Through our experiments, we demonstrate how several features of LDNS are beneficial for modeling complex datasets in neuroscience:</p><p>First, following other LDMs in the literature, LDNS decouples latent inference and probabilistic modeling of the data, offering flexibility in reusing the trained autoencoder and diffusion model. For the monkey recordings, all diffusion models (unconditional, conditioned on reach angle, and conditioned on hand velocities) operate in the latent space of the same autoencoder, in contrast to existing approaches that require end-to-end retraining for each type of conditioning variable. LDNS is also faster to train than AutoLFADS, which requires population-based training to optimize hyperparameters (appendix A3.1). Second, we show that LDNS autoencoders can be augmented with per-neuron autoregressive dynamics to capture single-neuron temporal dynamics (e.g., refractoriness), which otherwise cannot be captured with population-level shared dynamics. Third, as a result of the length-generalizable autoencoders and diffusion models using S4 layers, LDNS can generate variable-length trials in both the Lorenz example and human cortical recordings-a feature that will be particularly useful in modeling datasets recorded during naturalistic stimuli or behavior.</p><p>Altogether, these features enable LDNS to generate realistic neural activity, especially when conditioned on behavioral covariates. In our experiments, we demonstrate that unseen movement trajectories can be used to conditionally generate samples of neural activity, from which we can decode these hypothetical behaviors. These generated latent trajectories reflect behavioral information in an interpretable way. Our methodology is general and can be applied to recordings from any brain region, beyond the motor and speech cortex examples shown here. Thus, LDNS opens up further possibilities for hypothesis generation and testing in silico, potentially enabling stronger links between experimental and computational works.</p><p>Limitations In real neural data, the latent dimensionality of the system is not known, and as with all LVMs (which often assume that population dynamics are intrinsically low-dimensional), choosing an appropriate latent dimension can be challenging. Furthermore, any modeling errors at the encoding and decoding stage of the autoencoder will affect the overall performance of the latent diffusion approach. Nevertheless, in our experiments, we found that autoencoder training is fast, stable, and reasonably robust to hyperparameter configurations. While LDNS was still able to model the data well under relatively severe compression (e.g., 182-to-16 for the monkey recordings), optimizing latent dimensionality to balance expressiveness and interpretability remains a goal for future research.</p><p>Broader impact Realistic spike generation capabilities increase the risk of research manipulation by generating synthetic data that may be difficult to detect. On the other hand, LDNS could be useful for the dissemination of privatized clinical data, though we acknowledge the critical importance of protecting data privacy when working with sensitive human participant data. Finally, synthetically generated data (conditioned on unseen behavioral conditions) could be useful for augmenting the training of brain-computer interface decoding models.</p><p>by the AI4Med-BW graduate program. We thank Chethan Pandarinath for providing access to their compute cluster to train AutoLFADS. We thank Christian F. Baumgartner and all Mackelab members for feedback and discussions. We would like to also thank our reviewers for their insightful comments which improved our paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.3 Spike-history-augmented Poisson model</head><p>The parameters of the autoregressive observation model in Eq. ( <ref type="formula" target="#formula_1">2</ref>) are learned by maximizing the Poisson log-likelihood. Training is performed jointly for all neurons with a history length of T ′ = 20, corresponding to 100 ms, using the AdamW optimizer <ref type="bibr" target="#b29">[30]</ref> (learning rate 0.1, weight decay 0.01).</p><p>In our implementation, we use the Softplus function given by Softplus(x) = log(1 + exp(x)) as an approximation to the exponential function in Eq. ( <ref type="formula" target="#formula_1">2</ref>), which is accurate for the low-count regime while increasing numerical stability. During autoregressive sampling, we limit the maximum possible spike count to 5 spikes, which corresponds to the biological maximum, limited by the refractory period for 5 ms time bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.4 Diffusion model</head><p>We consider four variants of our proposed diffusion model with time-mixing and channel-mixing layers for four different tasks. In all cases, except for the conditioning mechanism, the internal architecture remains the same (Supp. Fig. <ref type="figure" target="#fig_0">A1</ref>). For diffusion timestep and fixed-length conditioning vector, we shift and scale the inputs and outputs to the time mixing and channel mixing blocks using adaptive instance normalization, as done in Peebles and Xie <ref type="bibr" target="#b36">[37]</ref>.</p><p>1. Unconditional generation for synthetic spiking data with Lorenz Dynamics and cortical spiking data in monkeys -we use only time conditioning.</p><p>2. Angle-conditioned generation for cortical monkey spiking data -we add an embedding MLP([cos θ, sin θ]) of the reach angle θ to the timestep embedding output.</p><p>3. Trajectory conditioned generation for cortical monkey spiking data -we concatenate the hand velocities v x , v y of the monkey with the input as two additional channels.</p><p>4. Unconditional variable-length generation for cortical human spiking data -we concatenate the desired length (with a maximum sequence length of 512) as a centered binary mask channel in the input. We only backpropagate through the central section of the output corresponding to the binary mask.</p><p>We use a DDPM scheduler with 1000 timesteps and ϵ-parameterization. To stabilize and speed up training, we train all diffusion models using a smooth L 1 loss, written as</p><formula xml:id="formula_4">L(x; δ) = x 2 /(2δ) if |x| &lt; δ |x| -δ otherwise,<label>(7)</label></formula><p>with δ = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Dataset access information</head><p>All real-world datasets used in this work are publicly available under open-access licenses. Our work does not involve the collection of new experimental data.</p><p>The human BCI dataset is available at <ref type="url" target="https://datadryad.org/stash/downloads/file_stream/2547369">https://datadryad.org/stash/downloads/file_ stream/2547369</ref> under a CC0 1.0 Universal Public Domain Dedication license. This dataset was originally published in Willett et al. <ref type="bibr" target="#b56">[57]</ref>. The data was collected under appropriate ethical oversight, with approval from the Institutional Review Board at Stanford University (protocol #20804).</p><p>The monkey reaching dataset (MC_Maze) is available through the DANDI Archive (<ref type="url" target="https://dandiarchive.org/dandiset/000128">https:// dandiarchive.org/dandiset/000128</ref>, ID: 000128) under a CC-BY-4.0 license. This dataset contains sorted unit spiking times and behavioral data from primary motor and dorsal premotor cortex during a delayed reaching task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 Hyperparameters and compute resources</head><p>Table <ref type="table">2</ref>: Training details for autoencoder models on Lorenz, Monkey reach, and Human BCI datasets. We used the AdamW <ref type="bibr" target="#b30">[31]</ref> optimizer, whose learning rate was linearly increased over in the initial period and then decayed to 10% of the max value with a cosine schedule. Mean firing rate for Lorenz was 0.3. In all cases, we used K = 5 for the temporal smoothness loss in Eq. 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.1 Computational Resources</head><p>We performed all training and evaluation of LDNS on the Lorenz and Monkey reach datasets on an NVIDIA RTX 3090 GPU with 24GB RAM. For the Human BCI data, we used an NVIDIA A100 40GB GPU.</p><p>The autoencoder for the Lorenz dataset is trained in ≈ 6 minutes, and the diffusion model in ≈ 20 minutes. For the evaluation, all sampling is performed on the GPU in 5 minutes. The effective GPU wallclock time (time when the GPU is utilized) for the entire training and evaluation run is within 30 minutes. Optimizing the autoregressive observation model took less than 1 minute.</p><p>AutoLFADS, the baseline used for unconditional sampling for the Monkey reach dataset, was trained on a cluster of 8 NVIDIA RTX 2080TI GPUs for one day. As it requires automated hyperparameter tuning to achieve the best accuracy using population-based training (PBT, <ref type="bibr" target="#b23">[24]</ref>), AutoLFADS is significantly more compute-expensive to train than LDNS.</p><p>For the Human BCI dataset, due to larger trial lengths, more data points, and more heterogeneous temporal dynamics, we trained a slightly larger autoencoder and diffusion model than in Monkey reach. The autoencoder took 50 minutes to train, and the diffusion model took 10 hours to train. Sampling from the trained model took 9 minutes, resulting in a total of under 12 hours of effective GPU wallclock time.</p><p>We ran several preliminary experiments for LDNS to optimize the architecture and hyperparameters, as well as for designing appropriate evaluations. We estimate the total effective GPU wallclock time to be ≈ 10× that of the final model runs. The AutoLFADS baseline was only trained once with PBT, as this framework automatically optimizes the model hyperparameters.</p><p>We implemented all training and evaluation code using the Pytorch framework<ref type="foot" target="#foot_1">foot_1</ref> , and used Weights &amp; Biases<ref type="foot" target="#foot_2">foot_2</ref> to log metrics during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Baseline comparison: Latent Factor Analysis via Dynamical Systems -LFADS</head><p>Latent Factor Analysis via Dynamical Systems (LFADS) is a sequential variational autoencoder used to infer latent dynamical systems from neural population spiking activity <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b35">36]</ref>. LFADS consists of an encoder, a generator, and optionally, a controller, all of which are RNNs. The generator RNN implements the learned latent dynamical system, given an initial condition and time-varying inputs.</p><p>The internal states of the generator are mapped through affine transformations to lower-dimensional latent factors and single-neuron Poisson firing rates. The encoder RNN maps the neural population activity into an approximate posterior over the generator's initial condition.</p><p>At each timestep, the controller RNN receives both encoded neural activity and the latent factors from the previous timestep and outputs an approximate posterior over the input to the generator. The entire model is trained end-to-end to maximize the ELBO, as is done in VAEs. To address the difficulty of hyperparameter optimization for LFADS, Population-Based Training (PBT) has been proposed to automate hyperparameter selection, termed AutoLFADS <ref type="bibr" target="#b24">[25]</ref>.</p><p>In our experiments with the monkey reach dataset, we use the PyTorch implementation of AutoLFADS <ref type="bibr" target="#b46">[47]</ref>. We use the hyperparameters and search ranges from Pei et al. <ref type="bibr" target="#b37">[38]</ref>, but omit the controller RNN to simplify generation from prior samples. Although this might limit the model's expressiveness, prior research indicates that the monkey reach data can be well-modeled as autonomous, without external inputs from the controller <ref type="bibr" target="#b9">[10]</ref>. LFADS has previously performed well on this data without the controller <ref type="bibr" target="#b35">[36]</ref>.</p><p>We generate samples from LFADS by sampling initial conditions from the Gaussian prior, running the generator RNN forward, and Poisson-sampling spikes from the resulting firing rates. For inclusion of spike history in the observation model of LFADS, we used the same training method and hyperparameter settings as in LDNSsh (appendix A1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 Supplementary baseline comparisons</head><p>For an extended baseline comparison, we implemented two additional methods for the task of unconditional generation on the Monkey dataset (Sec. 3.4) -Targeted Neural Dynamical Modeling (TNDM, <ref type="bibr" target="#b20">[21]</ref>) and Poisson-identifiable VAE (pi-VAE, <ref type="bibr" target="#b62">[63]</ref>). It is important to note that while both TNDM and pi-VAE have demonstrated success in analyzing neural and behavioral data, neither was specifically designed for realistic spike train generation. The architectural choices in our implementation of these methods reflect their original intended applications in neural data analysis rather than generation of neural spiking data. Nevertheless, our comparisons show that LDNS, especially with spike-history, is superior or on par with all other methods (Table <ref type="table" target="#tab_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.1 Targeted Neural Dynamical Modeling (TNDM)</head><p>TNDM <ref type="bibr" target="#b20">[21]</ref> is a VAE-based model designed to jointly model neural activity and behavior. TNDM extends LFADS by using an RNN to generate latent dynamics that are mapped to both neural activity and behavioral variables. TNDM separates the latent space into behavior-specific and behaviorindependent subspaces to disentangle task-relevant and intrinsic neural dynamics.</p><p>For our comparison on the unconditional monkey reach task, we trained TNDM using the architecture and hyperparameters proposed in the original implementation of the paper <ref type="foot" target="#foot_3">4</ref> . We used 64-dimensional latent dynamics for each of the two sets. These project to a total of 10 latent factors z (5 behaviorspecific z r and 5 behavior-independent z i ), which is the maximum number demonstrated in the original work.</p><p>To generate unconditional samples, we sampled initial generator states from a standard normal prior N (0, I), then generated the latent dynamics and projected into neuron rates the same way as in LFADS. TNDM, performs well in matching real data in spike statistics (Supp. Fig. <ref type="figure" target="#fig_14">A13</ref> cyan, Supp. Fig. <ref type="figure" target="#fig_16">A16e</ref>) and temporal dynamics (Supp. Fig. <ref type="figure" target="#fig_4">A15</ref>). Overall, we observe that LDNS captures spike statistics better than TNDM, except for the the population spike history count. In all metrics, LDNS augmented with spike history outperforms TNDM on spike statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.2 Poisson-identifiable VAE (pi-VAE)</head><p>Poisson-identifiable VAE (pi-VAE) <ref type="bibr" target="#b62">[63]</ref> is a VAE-based model for count data that ensures identifiability in the latent space. pi-VAE does not model temporal dependencies, instead treating each time point as an independent sample.</p><p>We trained pi-VAE on the monkey dataset using the original architecture and hyperparameters <ref type="foot" target="#foot_4">5</ref> . We used a General Incompressible-flow Network as a decoder, with 2 behaviorally relevant dimensions and 2 independent dimensions in the latent space. However, our evaluation context differs significantly from the original paper's demonstrations: while pi-VAE was initially evaluated on 50ms time bins and straight reaches only, our comparison uses 5ms bins and conditions on angles across all reaches at both middle and end trajectory points. The "label", or behavior, is presented as a 4-dimensional vector containing the cosine and sine of initial and final reach angles. Since this has a conditional latent space, sampling is performed by sampling angles randomly.</p><p>Importantly, sampling from pi-VAE does not introduce any temporal dependence between spike bins within a trial -pi-VAE was not intended to be a generative model of neural spiking data. The lack of temporal modeling in pi-VAE's is a fundamental limitation for generating realistic spike trains, as evident in our empirical comparisons (Supp. Fig. <ref type="figure" target="#fig_14">A13</ref> in yellow, Supp. Fig. <ref type="figure" target="#fig_4">A15,</ref><ref type="figure" target="#fig_16">A16f</ref>). Note that this failure cannot be diagnosed simply from looking at the sampled spiking data (Supp. Fig. <ref type="figure" target="#fig_3">A14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.3 Contributions of LDNS in context to LFADS, TNDM and pi-VAE</head><p>• LDNS is designed specifically for the purpose of accurately generating neural spiking data (unconditionally or conditionally)-a task often ignored by other LVMs designed for neural data analysis such as LFADS, pi-VAE, and TNDM. • The S4 autoencoder and diffusion model in LDNS are trained in separate stages, offering modularity, while both components naturally account for temporal dependencies (unlike pi-VAE). • S4 is autoregressive, similar to other RNN-based models, but empirically we found it to perform better when extending past the training trial length (compared to LFADS, see Sec. A6.1). • One feature provided by some neural-behavioral analysis models (such as pi-VAE and TNDM) is an explicit disentangling of neural vs. behavior-relevant latents. While we found LDNS latents contain relevant behavioural information (Fig. <ref type="figure" target="#fig_4">5e</ref>-g, Supp. Fig. <ref type="figure" target="#fig_17">A17</ref>), we did not explicitly supervise the latent space to induce this property. • Finally, the spike history-dependent observation model in LDNS is modular and can be optimized post-hoc using rate predictions of any model to improve spike generation quality.</p><p>We observed this with LDNS as well as LFADS (Table <ref type="table" target="#tab_0">1</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6.1 Length generalization of LFADS on Lorenz</head><p>To analyze whether LFADS <ref type="bibr" target="#b35">[36]</ref> exhibits similar length generalization properties as LDNS, we trained an AutoLFADS model on the Lorenz dataset (256 bins). We used the same architecture as the Monkey dataset, with 40-dimensional latent dynamics. We sampled initial conditions from the LFADS prior, then generated dynamics for both the original 256 steps and for an extended duration of 16× the training length. Qualitatively, we observed that while LFADS produced trajectories resembling the attractor dynamics of the ground truth Lorenz system (Supp. Fig. <ref type="figure" target="#fig_8">A5a</ref>, across various dimension combinations), these trajectories often diverged when run for longer intervals (Supp. Fig. <ref type="figure" target="#fig_8">A5b</ref>). However, the system eventually returned to typical dynamics. Furthermore, when generating for extended durations, we observed that the mean population firing rates sometimes reached extreme values in some samples (Supp. Fig. <ref type="figure" target="#fig_11">A6</ref>), though they eventually returned to typical ranges. This behavior was not observed in LDNS samples, suggesting that bidirectional generation in the diffusion model provides more stability in variable-length generation.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Latent Diffusion for Neural Spiking data.LDNS allows for (un)conditional generation of neural spiking data through combining a regularized autoencoder with diffusion models that act on the low-dimensional latent time series underlying neural population activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Realistic generation of spiking data with underlying chaotic dynamics. a) Synthetic spiking data from an underlying Lorenz system with a Poisson observation model. b) Accurate, smooth rate predictions of the autoencoder for held-out spiking data. c) Plotted trace of sampled latents (256 bins training length, left) and 16× the original training length (middle). The sampled latent distribution matches the PSD of the autoencoder latents (right; median, 10%, and 90% percentiles). d) LDNS population spike count histogram (kde: kernel density estimate) and pairwise cross-correlations match the training distribution. e) LDNS single neuron statistics, i.e., mean inter-spike interval (isi) and std isi, match the training distribution.</figDesc><graphic coords="5,108.00,72.00,396.00,172.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Unconditional generation of variable-length trials of human spiking data during attempted speech. a) Multi-unit activity is recorded from speech production-related regions of the brain (top) during attempted vocalization of variable-length sentences (bottom). b) Neural activity during sentences of different lengths. c) LDNS unconditionally sampled trials with different lengths, using the Poisson observation model. d) LDNS population spike count histogram, and mean and std of the isi match those of the data. e) Correlation matrices of the data (left) and LDNS samples (middle), and scatterplot of the pairwise correlations of data vs. LDNS samples (right).</figDesc><graphic coords="6,108.00,72.00,396.01,115.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Realistic generation of spiking data in a monkey performing reach tasks. a) A monkey performs diverse reach movements in different mazes. b) Neural activity during a reach trial and a sampled trial from LDNS with a Poisson observation model. c) The LDNS population spike count histogram, and pairwise correlations match those of the data. d) LDNS mean-and std isi match the monkey data distribution. e) Auto-correlation of data, LDNS samples with Poisson observations (left), and LDNS samples with spike history, grouped according to correlation strength.</figDesc><graphic coords="7,108.00,72.00,395.99,148.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Generation conditioned on monkey reach directions and velocity traces. a) Closed loop assessment: do conditionally generated latents translate to neural activity consistent with the desired direction or reach movement? b) Unseen reach movements (data) and corresponding movements decoded from the rates predicted by the autoencoder (ae). c) Decoded reach directions of LDNS samples conditioned on initial reach angles θ. d) Decoded reach directions of LDNS samples conditioned on 3 unseen reach movements (velocities v x , v y ). e) Straight reaches from the test set used for velocity conditioning. f) LDNS sampled latents conditioned on trajectories shown in e) vary smoothly over time and reflect information about reach angles. g) PCs of sampled LDNS latents shown in f) reveal meaningful and separable information about behavior.</figDesc><graphic coords="8,108.00,72.00,396.01,175.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure A1: (Top left) The S4 autoencoder architecture. (Top right) Architecture for the autoencoder blocks used in the encoder. (Bottom left) The S4 diffusion model architecture. (Bottom right) Architecture for the diffusion blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A2 :</head><label>A2</label><figDesc>Figure A2: The autoencoder captures the gt Lorenz synthetic firing rate perfectly a) Autoencoder predictions (pred) and true rates from the test set, together with their difference (error, in red). b) Reconstructions sampled from the Poisson observation model (right) closely resemble the test sample (left). Both spiking activity is binarized for the visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A3 :</head><label>A3</label><figDesc>Figure A3: The S4 autoencoder infers smooth latents from discrete spikes and samples from the diffusion model capture the latent distribution. a) Inferred autoencoder latents for a test sample. b) Power spectral density for all eight latent dimensions for the inferred autoencoder training set and samples from the diffusion model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A5 :</head><label>A5</label><figDesc>Figure A5: Length generalization of LFADS on Lorenz Different projections of a 40-dimensional latent space from LFADS trained on the Lorenz system. Trajectories are compared between a) the original length and b) 16 times the original length using sampled initial conditions. For comparison with LDNS length generalization, see Fig.2c.</figDesc><graphic coords="23,147.60,263.71,316.82,177.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure A5: Length generalization of LFADS on Lorenz Different projections of a 40-dimensional latent space from LFADS trained on the Lorenz system. Trajectories are compared between a) the original length and b) 16 times the original length using sampled initial conditions. For comparison with LDNS length generalization, see Fig.2c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure A5: Length generalization of LFADS on Lorenz Different projections of a 40-dimensional latent space from LFADS trained on the Lorenz system. Trajectories are compared between a) the original length and b) 16 times the original length using sampled initial conditions. For comparison with LDNS length generalization, see Fig.2c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A6 :</head><label>A6</label><figDesc>Figure A6: Mean population firing rates for eight different samples, shown for both the original length (pink) and 16× the original length (purple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A9 :</head><label>A9</label><figDesc>Figure A9: Correlation matrices for real spiking data from human and LDNS, comparing the autoencoder-inferred (ae) correlation (sampled from reconstructed rates) and correlation of sampled spikes (diff). The deviations from the data (gt) already arise at the autoencoder stage.</figDesc><graphic coords="25,108.00,318.80,396.00,112.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure A13 :</head><label>A13</label><figDesc>FigureA13: Performance comparison with additional baselines pi-VAE<ref type="bibr" target="#b62">[63]</ref>, TNDM<ref type="bibr" target="#b20">[21]</ref>, LFADS<ref type="bibr" target="#b35">[36]</ref>, LFADS with spike history (LFADSsh), LDNS and LDNS with spike history (LDNSsh). Mean and standard deviation across 5 folds sampled with replacement.</figDesc><graphic coords="27,108.00,235.39,396.01,64.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A14 :Figure A15 :</head><label>A14A15</label><figDesc>Figure A14: Visual comparison of sampled spiking data from LDNS and all baselines with real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure A16 :</head><label>A16</label><figDesc>Figure A16: Population-level and single neuron-level statistics of a) LDNS, b) LDNSsh (with spike history), c) LFADS, d) LFADSsh (with spike history), e) TNDM, and f) pi-VAE.</figDesc><graphic coords="28,108.00,72.00,396.02,450.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure A17 :</head><label>A17</label><figDesc>Figure A17: Comparison of LDNS latent space trajectories of inferred and conditionally latents a) Straight reaches from the Monkey reach test set. b) LDNS sampled latents (velocityconditioned on reaches shown in a). c) PCs of LDNS sampled latents. d) Autoencoder-inferred latents of corresponding neural activity for the reaches shown in a). e) PCs of autoencoder-inferred latents.</figDesc><graphic coords="29,108.00,301.57,396.00,127.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="22,108.00,242.28,396.03,232.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="24,187.20,103.07,237.60,253.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model metrics comparison. D KL for the population spike count histogram and RMSE comparisons. Mean and standard deviation across 5 folds sampled with replacement. sh represents observation models with spike history. Bolded entries represent best-performing values for Poisson and spike-history observation models.</figDesc><table><row><cell>Method D KL psch</cell><cell cols="3">RMSE pairwise corr RMSE mean isi RMSE std isi</cell></row><row><cell>AutoLFADS 0.0040 ± 2.2e-4</cell><cell>0.0026 ± 1.25e-5</cell><cell>0.039 ± 0.003</cell><cell>0.029 ± 0.001</cell></row><row><cell cols="2">LDNS 0.0039 ± 3.9e-4 0.0025 ± 1.1e-5</cell><cell cols="2">0.037 ± 0.001 0.023 ± 0.001</cell></row><row><cell>AutoLFADSsh 0.0036 ± 2.1e-4</cell><cell>0.0026 ± 1.8e-5</cell><cell>0.034 ± 0.002</cell><cell>0.023 ± 0.0001</cell></row><row><cell cols="2">LDNSsh 0.0016 ± 6.2e-4 0.0025 ± 1.07e-5</cell><cell cols="2">0.024 ± 0.002 0.023 ± 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Training details for diffusion models on Lorenz, Monkey reach, and Human BCI datasets. We used the same learning rate scheduler as for the autoencoder.For the Monkey reach dataset, the autoencoder with the given hyperparameters is trained in ≈ 8 minutes, and the unconditional and conditional diffusion models in 40 minutes to 1 hour. With similar sampling times as in Lorenz, the effective GPU wallclock time is approximately within one hour.</figDesc><table><row><cell>Parameter</cell><cell cols="3">Lorenz Monkey Reach Human BCI</cell></row><row><cell>Model Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latent channels</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Hidden layer channels</cell><cell>64</cell><cell>256</cell><cell>384</cell></row><row><cell>Num diffusion blocks</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell>Num denoising steps</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Training Details</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>AdamW weight decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Num epochs</cell><cell>1000</cell><cell>2000</cell><cell>2000</cell></row><row><cell>Num warmup epochs</cell><cell>50</cell><cell>50</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Added Baselines metrics comparison. D KL for the population spike count histogram and RMSE comparisons. Mean and standard deviation across 5 folds sampled with replacement. Bolded entries represent best-performing values for Poisson observation and spike-history observation model, respectively.</figDesc><table><row><cell>Method D KL psch</cell><cell cols="3">RMSE pairwise corr RMSE mean isi RMSE std isi</cell></row><row><cell>pi-VAE 0.0063 ± 2.9e-4</cell><cell>0.0031 ± 1.08e-5</cell><cell>0.064 ± 0.002</cell><cell>0.034 ± 0.001</cell></row><row><cell cols="2">TNDM 0.0028 ± 6.6e-5 0.0027 ± 1.17e-5</cell><cell>0.057 ± 0.004</cell><cell>0.029 ± 0.001</cell></row><row><cell>AutoLFADS 0.0040 ± 2.2e-4</cell><cell>0.0026 ± 1.25e-5</cell><cell>0.039 ± 0.003</cell><cell>0.029 ± 0.001</cell></row><row><cell>LDNS 0.0039 ± 3.9e-4</cell><cell>0.0025 ± 1.1e-5</cell><cell cols="2">0.037 ± 0.001 0.023 ± 0.001</cell></row><row><cell>AutoLFADSsh 0.0036 ± 2.1e-4</cell><cell>0.0026 ± 1.8e-5</cell><cell>0.034 ± 0.002</cell><cell>0.023 ± 0.0001</cell></row><row><cell cols="2">LDNSsh 0.0016 ± 6.2e-4 0.0025 ± 1.07e-5</cell><cell cols="2">0.024 ± 0.002 0.023 ± 0.001</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Github link: https://github.com/state-spaces/s4/blob/main/models/s4/s4.py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Paszke et. al. PyTorch: An Imperative Style, High-Performance Deep Learning Library (2019)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Lukas Beiwald. Experiment Tracking with Weights and Biases (2022)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Code adapted from Github respository: https://github.com/HennigLab/tndm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Code adapted from article by Lyndon Duong (2021) -https://www.lyndonduong.com/pivae.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">German Research Foundation (DFG)</rs> through <rs type="funder">Germany's Excellence Strategy</rs> (EXC-Number <rs type="grantNumber">2064/1</rs>, PN 390727645) and <rs type="grantNumber">SFB1233 (PN 276693517</rs>), <rs type="grantNumber">SFB 1089 (PN 227953431</rs>), <rs type="grantNumber">SPP2041 (PN 34721065</rs>), the <rs type="funder">German Federal Ministry of Education and Research (Tübingen AI Center</rs>, FKZ: <rs type="grantNumber">01IS18039</rs>), the <rs type="funder">Human Frontier Science Program (HFSP)</rs>, and the <rs type="funder">European Union</rs> (<rs type="funder">ERC, DeepCoMechTome</rs>, <rs type="grantNumber">101089288</rs>). We utilized the <rs type="institution">Tübingen Machine Learning Cloud</rs>, supported by <rs type="funder">DFG</rs> <rs type="grantNumber">FKZ INST 37/1057-1 FUGG. JK</rs>, AS, and JV are members of the <rs type="projectName">International Max Planck Research School for Intelligent Systems (IMPRS-IS</rs>) and JV is supported</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QWcsXg2">
					<idno type="grant-number">2064/1</idno>
				</org>
				<org type="funding" xml:id="_gv5wacu">
					<idno type="grant-number">SFB1233 (PN 276693517</idno>
				</org>
				<org type="funding" xml:id="_jkKvZJr">
					<idno type="grant-number">SFB 1089 (PN 227953431</idno>
				</org>
				<org type="funding" xml:id="_fptZ2UX">
					<idno type="grant-number">SPP2041 (PN 34721065</idno>
				</org>
				<org type="funding" xml:id="_XVUPH5q">
					<idno type="grant-number">01IS18039</idno>
				</org>
				<org type="funding" xml:id="_MuKsarf">
					<idno type="grant-number">101089288</idno>
				</org>
				<org type="funded-project" xml:id="_fdGBUvX">
					<idno type="grant-number">FKZ INST 37/1057-1 FUGG. JK</idno>
					<orgName type="project" subtype="full">International Max Planck Research School for Intelligent Systems (IMPRS-IS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A1 LDNS architecture</head><p>Here we describe the exact network components and architecture for the autoencoder and diffusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1 Structured State-Space Layers (S4)</head><p>A central component of our autoencoder architecture is the recently introduced structured state space models (S4) <ref type="bibr" target="#b17">[18]</ref>. With an input sequence x = [x 1 . . . x T ] ∈ R T and corresponding output y = [y 1 . . . y T ] ∈ R T , an S4 layer applies the following operation for each timestep -</p><p>where the discretized state and input matrices A, B given continuous analogues A, B and step size ∆ are computed as</p><p>When the state s t is not required, this recurrent computation of the output y given input sequence y can be unrolled into a parallelizable convolution operation y = K * x, with unrolled kernel (6)</p><p>We used the S4 implementation 1 provided by Gu et al. <ref type="bibr" target="#b17">[18]</ref> that stably initializes the state transition matrix A using a diagonal plus low-rank approximation. For a multivariate input-output pair x, y ∈ R D×T , we apply D separate univariate S4 layers for each dimension and then mix them in the channel-mixing layer using an MLP (see next section). Each univariate input-output mapping consists of H separate S4 "head" that are expanded and contracted from and into a single dimension.</p><p>Due to its recurrent nature, S4 is a causal layer, enabling variable-length training and inference. To enable bidirectionality, we flip the input signal x in time, apply H/2 S4 heads each for the flipped and unflipped signal, and then combine these at the end into univariate signal y. This allows bidirectional flow of information from front-to-back and back-to-front of the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2 Autoencoder</head><p>We include temporal information only in the encoder and model the decoder as a lightweight pointwise MLP for the autoencoder (Supp. Fig. <ref type="figure">A1</ref>). This allows us to temporally align the latents with the signal, and ensure that no further temporal dynamics are introduced when mapping the latents back into Poisson rates.</p><p>We use causal S4 layers, allowing length generalization and handling of variable-length signals.</p><p>During training, we pad the input spiking data with zeros into a fixed length and only backpropagate through the unpadded output rates.</p><p>Furthermore, to infer smooth rates and avoid spiking behavior, we use coordinated dropout <ref type="bibr" target="#b23">[24]</ref>. For each time bin independently, we mask the input spikes to zero with random probability p and scale up the remaining spikes by 1 1-p (this preserves the firing statistics of the spiking data). We then backpropagate through the Poisson NLL loss only over the masked positions, effectively preventing the network from collapsing to a spiking prediction of the Poisson rates.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7 Supplementary Figures Human BCI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8 Supplementary Figures Monkey Reach task</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Whole-brain functional imaging at cellular resolution using light-sheet microscopy</title>
		<author>
			<persName><forename type="first">Misha</forename><forename type="middle">B</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Orger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">N</forename><surname>Robson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><forename type="middle">J</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-based time series imputation and forecasting with structured state space models</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alcaraz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</title>
		<author>
			<persName><forename type="first">Antonis</forename><surname>Antoniades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><forename type="middle">S</forename><surname>Canzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024-10">October 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-dimensional models of neural population activity in sensory cortical circuits</title>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">W</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified, Scalable Framework for Neural Population Decoding</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinam</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkataramana</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Nachimuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A flow-based latent state generative model of neural population responses to natural images</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bashiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin-Klemens</forename><surname>Lurz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taliah</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuokun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Sinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Integrating multimodal data for joint generative modeling of complex dynamics</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Koppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Durstewitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding</title>
		<author>
			<persName><forename type="first">Zijiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Lin Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Helen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mcmaze: macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kaufman</surname></persName>
		</author>
		<idno>2022. Version 0.220113.0400</idno>
		<imprint/>
	</monogr>
	<note>version 0.220113.0400) [data set</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Nuyujukian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural population dynamics during reaching</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for large-scale neural recordings</title>
		<author>
			<persName><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distance function for spike prediction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carola</forename><forename type="middle">A M</forename><surname>Yovanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Baden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal alignment and latent gaussian process factor inference in population spike trains</title>
		<author>
			<persName><forename type="first">Lea</forename><surname>Duncker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">It&apos;s raw! audio generation with state-space models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher R'e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal Gaussian Process Variational Autoencoders for Neural and Behavioral Data</title>
		<author>
			<persName><forename type="first">Rabia</forename><surname>Gondur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Usama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sikandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikio</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Christian Aoi</surname></persName>
		</author>
		<author>
			<persName><surname>Keeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized teacher forcing for learning chaotic dynamics</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Monfared</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Durstewitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Targeted neural dynamical modeling</title>
		<author>
			<persName><forename type="first">Cole</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Jude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable Bayesian GPFA with automatic relevance determination and discrete noise models</title>
		<author>
			<persName><forename type="first">Kristopher</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ta-Chu</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Hennequin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully integrated silicon probes for high-density recording of neural activity</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Steinmetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">H</forename><surname>Siegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enabling hyperparameter optimization in sequential autoencoders for spiking neural data</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshtkaran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A large-scale neural network training framework for generalized estimation of single-trial population dynamics</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Reza Keshtkaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Sedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Raeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diya</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">L</forename><surname>Basrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansem</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">E</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Pandarinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deterministic Nonperiodic Flow</title>
		<author>
			<persName><forename type="first">N</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Atmospheric Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="1963-03">March 1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Empirical models of spiking in neural populations</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Jakob H Macke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Behavioral decomposition reveals rich encoding structure employed across neocortex in rats</title>
		<author>
			<persName><forename type="first">Bartul</forename><surname>Mimica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuçe</forename><surname>Tombaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Battistin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Jingyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">A</forename><surname>Fuglstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><surname>Whitlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synthesizing realistic neural population activity patterns using generative adversarial networks</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Molano-Mazon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arno</forename><surname>Onken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Piasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Panzeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">M B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">G</forename><surname>Makin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
		<title level="m">Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inferring single-trial neural population dynamics using sequential auto-encoders</title>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stavisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><forename type="middle">R</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaimie</forename><forename type="middle">M</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Sussillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural latents benchmark &apos;21: Evaluating latent variable models of neural population activity</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Felix C Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raeed</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansem</forename><surname>Hasan Chowdhury</surname></persName>
		</author>
		<author>
			<persName><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E O'</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">M</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">E</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Memming</forename><surname>Il</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><forename type="middle">L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><surname>Pandarinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamical segmentation of single trials from population neural data</title>
		<author>
			<persName><forename type="first">Biljana</forename><surname>Petreska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopal</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust learning of low-dimensional dynamics from large neural ensembles</title>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eftychios</forename><forename type="middle">A</forename><surname>Pnevmatikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatio-temporal correlations and visual signalling in a complete neuronal population</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Litke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Chichilnisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="995" to="999" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adversarial training of neural encoding models on population spike trains. Real Neurons &amp; Hidden Units: Future directions at the intersection of neuroscience and artificial intelligence @ NeurIPS</title>
		<author>
			<persName><forename type="first">Poornima</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Atayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting</title>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling conditional distributions of neural and behavioral data with masked variational autoencoders</title>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lobato-Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Ramdya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Sedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01230</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>James Sofroniew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Svoboda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<title level="m">Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Lfads -latent factor analysis via dynamical systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Pandarinath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CSDI: Conditional score-based diffusion models for probabilistic time series imputation</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generating realistic neurophysiological time series with denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling the impact of common noise inputs on the network activity of retinal ganglion cells</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vidne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayant</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Litke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Chichilnisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Capturing the Dynamical Repertoire of Single Neurons with Generalized Linear Models</title>
		<author>
			<persName><forename type="first">Alison</forename><forename type="middle">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Data for: A high-performance speech neuroprosthesis</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Willett</surname></persName>
		</author>
		<idno type="DOI">10.5061/dryad.x69p8czpq</idno>
		<ptr target="https://doi.org/10.5061/dryad.x69p8czpq" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A high-performance speech neuroprosthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">M</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofei</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">H</forename><surname>Avansino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Young</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foram</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">F</forename><surname>Kamdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><forename type="middle">R</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaul</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Druckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaimie</forename><forename type="middle">M</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gaussian process based nonlinear latent structure discovery in multivariate spike train data</title>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Keeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geometric latent diffusion models for 3d molecule generation</title>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Collinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity</title>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopal</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Variational latent gaussian process for recovering single-trial dynamics from population spike trains</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il</forename><surname>Memming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue-Xin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
