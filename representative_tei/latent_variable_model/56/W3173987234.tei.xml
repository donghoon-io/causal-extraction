<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_ZgQ8JA5">
					<orgName type="full">French National research Agency (ANR</orgName>
				</funder>
				<funder ref="#_BJwCnnj #_PFdDyaw">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_jwGcUWr">
					<orgName type="full">French National research Agency (ANR)</orgName>
				</funder>
				<funder ref="#_Jd4E7TT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><surname>Kolaczyk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Pierre Barbillon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Julien</forename><surname>Chiquet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julie</forename><surname>Mme</surname></persName>
						</author>
						<author>
							<persName><surname>Josse</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Eric</forename><surname>Kolaczyk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Pierre</forename><surname>Latouche</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">THÈSE DE DOCTORAT</orgName>
								<orgName type="laboratory">Établissement d&apos;inscription</orgName>
								<orgName type="institution">Université Paris-Saclay École doctorale de mathématiques Hadamard</orgName>
								<address>
									<postCode>2019SACLS289</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Pierre Latouche</orgName>
								<orgName type="institution" key="instit1">Université Paris-Sud</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<orgName type="institution" key="instit3">Université Paris Descartes)</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<address>
									<country>Rapporteur</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université Paris Descartes) Rapporteur Mme Tabea REBAFKA (Sorbonne Université</orgName>
								<address>
									<country>Examinatrice</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Établissement d'accueil : AgroParisTech Laboratoire d'accueil : Mathématiques et informatique appliquées, UMR 518 INRA Spécialité de doctorat : Mathématiques appliquées Timothée TABOUY Impact de l'échantillonnage sur l'inférence de structures dans les réseaux : application aux réseaux d'échanges de graines et à l'écologie Date de soutenance : 30 Septembre 2019 Après avis des rapporteurs :</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Merci à Mahendra d'avoir accepté de collaborer avec moi et de m'avoir accueilli à Jouy régulièrement. Merci de ta disponibilité et de ta grande gentillesse, c'était une vrai chance de travailler avec toi.</p><p>Un grand merci au groupe MIRES de m'avoir accueilli au cours de ces 3 dernières années. J'ai été grâce à vous le témoin privilégié de beaux moments de partage et de collaboration scientifique. Un merci particulier à Vanesse, Christian, Mathieu et Matthieu pour vos échanges passionnants et sincères.</p><p>Je voudrais ici remercier chaleureusement tout le laboratoire de statistique de l'Agro dans lequel j'ai passé ces 3 dernières années. L'ambiance de travail, vos qualités de chercheurs et vos amitiés ont réellement contribué à me faire grandir à bien des niveaux. Soyez tous assurés de ma gratitude et de mon amitié. Merci Émilie, Maud, Liliane, Christophe, Christophe, Christophe, Marie-Laure, Julie, Éric, Céline, Tristan, Michel, Laure, Jessica, Isabelle, Séverine. Une mention spéciale pour le grand chef Gabriel sans qui les conseils de labo seraient moroses, là où tu passes la folie et les rires suivent ! Un merci particulier à mes co-bureaux successifs et amis. Je pense en premier à Pierre, Marie et Anna qui m'ont accueilli en début de thèse, votre gentillesse et vos bons conseils sans oublier ce tour du monde en bateau virtuel resteront de super souvenirs. Merci Marie, Mathieu, James et Raphaëlle pour avoir partagé avec enthousiasme le BDDSS à tour de rôle. Marie, ta joie permanente et ta gentillesse me resteront à jamais, merci de ton amitié. Mathieu, pour ton amitié et toutes nos discussions je te remercie chaleureusement, j'espère que nos routes se recroiseront à l'avenir :) Et le dernier et non le moindre, merci mon cher Félix pour ta gentillesse et ton amitié sans faille, je te dois beaucoup dans mon admission en médecine et te suis à jamais reconnaissant. Merci Paul, pour ta discrétion et ton exemple, c'est toujours une joie de te croiser. Merci enfin à Saint-Clair mon petit frère de thèse, je te souhaite le meilleur pour la suite, à Yann pour ton amitié et ta gentillesse, à Jade, Rhana, Annarosa, Martina, et Joe pour votre grande gentillesse.</p><p>Merci à tous ceux que j'ai rencontrés au cours de séminaires et conférences, qui par leurs idées et questions, m'ont aidés à mieux comprendre et à me poser plus de questions sur mes travaux. Je pense en particulier à Vincent, toujours souriant et gentil, dont les travaux m'ont beaucoup apporté, tu es sûrement la personne dont je connais le mieux la thèse :) Sylvain, grand frère de tous à l'Agro, généreux et souriant, tu inspires beaucoup de monde. J'aurais beaucoup aimé continuer et apprendre avec toi. Jean-Benoist l'irréductible, ta passion et vision profonde des statistiques et de l'informatique forcent le respect ! Merci de ton invitation à Compiègne dont je garde un très bon souvenir :) Merci enfin à Valérie pour ton sourire et ta gentillesse.</p><p>Je voudrais remercier maintenant avec beaucoup de gratitude et d'émotion tout ceux à qui je dois d'être là aujourd'hui. Je pense à tous mes professeurs de lycée qui ont cru en moi, et à mes professeurs à l'université. Je pense à Mme Carré qui m'a encouragé et a cru en moi alors que je redoublais et surtout à Mme Berton grâce à qui je suis allé étudier à Orsay. Un grand merci à Dominique Hulin et Jean-Christophe Léger, responsable respectivement des L3 et des L1, à l'époque, à Orsay. La gentillesse et l'énergie que vous avez déployées à croire en chacun de vos étudiants sont pour beaucoup dans ma réussite. J'ai une pensée pour tous les professeurs passionnés qui m'ont enseigné durant mon cursus, soyez tous sincèrement remerciés. Je pense en particulier à Aurélien Galateau qui a été pour moi un exemple et une motivation, merci pour votre gentillesse et votre attention. Un grand merci aussi à toute l'équipe des matheux de l'IUT d'Orsay qui m'ont accueilli avec beaucoup de gentillesse pour faire mon monitorat de thèse.</p><p>Je voudrais ici remercier tous mes amis et co-étudiants de mathématiques avec qui j'ai buché et souffert pendant mes années d'études et de préparation de l'agrégation. Je pense à Arnaud, Justine, Victor, Nadège, Yvann, Vincent, Ridwann, Antoine, Antoine, Amandine, Dimitri, Éric, Laurent et Ghislain. Pour vos amitiés qui durent et tous les bons moments partagés je vous dis un grand MERCI. Je n'oublie pas la petite Margaux qui ira loin c'est sûr :) Merci de ton amitié et de ton sourire ;) Enfin, merci à toute la promo du M2 MSV, 4 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q c'était un réel plaisir de travailler avec vous et de vous connaître, je pense en particulier à Christophe dont j'espère que les aventures Montpelliéraine se passent bien :) Je n'oublie pas Christophe Giraud, merci de votre attention et de ne jamais compter votre temps pour répondre à nos questions et nous guider.</p><p>Merci à Matéo pour ton soutient et ton amitié ;) Merci à ma belle famille au complet, pour votre soutient sans faille. Pour me suivre dans tout ce que je fais de fou et pour ne jamais cesser de chercher à comprendre. Merci pour votre amour et confiance, c'est très important pour moi.</p><p>Merci à mes frères et soeurs, à Laure, Emmanuelle, Emmanuel, Tiphaine, Elise et Briac. Pour tout ce qui nous rassemble et tout ce que vous êtes, pour votre amour et parce que je vous aime, MERCI.</p><p>Merci à mes parents, sans qui je ne serai rien. Pour votre exemple en tout, votre soutient et votre amour inconditionnel. Parce que je vous doit tout et bien plus encore, MERCI du fond du coeur.</p><p>Merci à celui qui guide nos pas sans jamais nous abandonner et donne sens à toutes choses.</p><p>Merci à mon petit Octave, si adorable. T'avoir parmi nous est un vrai bonheur.</p><p>Enfin merci à celle qui m'accepte comme je suis, qui met de la lumière chaque jour que Dieu fait et m'aime peu importe le temps qu'il fait en moi. MERCI mon amour, cette thèse t'es dédiée évidemment. 5 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q CONTENTS CONTENTS 5 Bibliography 119 8 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q List of Symbols q n is the number of nodes in the network, q N = 1, n the set of nodes, q D = N × N the set of dyads, q Y ∈ M n (R) the adjacency matrix of the network, q Q ∈ N the number of blocks (or groups), q Q = 1, Q the set of blocks, q z ∈ Q n the vector of block memberships, q Z i ∈ {0, 1} Q the vector such that (Z i ) q = Z iq = 1 {zi=q} , q X i ∈ R N some covariates of node i such that (X i ) q = X iq , q X ij ∈ R m some covariates of dyad (i, j), q X the set of covariates ({X i } i on nodes and {X ij } ij on dyads),</p><p>q R ∈ M n (R) the sampling matrix : R ij = 1 si Y ij sampled, 0 otherwise, q V ∈ {0, 1} n the sampling vector indicating which node is sampled (V i = 1) or not (V i = 0), q Y o = {Y ij : R ij = 1} the observed part of the adjacency matrix, q Y m = {Y ij : R ij = 0} the non-observed part of the adjacency matrix, q D o = {(i, j) : R ij = 1} the observed dyads, q D m = {(i, j) : R ij = 0} the non-observed dyads.</p><p>CONTENTS CONTENTS 10 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction Introduction Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ÉTAT DE L'ART INTRODUCTION</head><p>réduit, nous ne sommes pas en mesure de dire quels ont été les échanges ; seuls leurs liens avec les interrogés sont connus.</p><p>Le second exemple est celui d'un réseau de co-régulation de gènes. Le réseau extrait de la plateforme string <ref type="bibr" target="#b114">(Szklarczyk et al., 2015)</ref>, accessible à l'adresse <ref type="url" target="http://www.string-db.org">http://www.string-db. org</ref>, est le réseau de voisinage de la protéine ER (pour Estrogen Receptor) codé par le gène ESR1 (pour EStrogen Receptor 1). Ce réseau est composé de 741 protéines (ou gènes codant ces protéines) et chaque dyade (i.e. une paire de noeud) est pondérée par un score appartenant à [0, 1] reflétant le niveau de confiance calculé sur la base d'expériences ou de travaux de recherche. Plus la valeur est proche de 1 plus la probabilité que cette arête existe est grande. Notant ω ij le poids associé à la dyade (i, j), nous définissons la matrice dépendant du seuil γ :</p><formula xml:id="formula_0">A γ = (A γ ) i,j =    1 if ω ij &gt; 1 -γ, NA if γ ≤ ω ij ≤ 1 -γ, 0 if ω ij &lt; γ.</formula><p>(1.1)</p><p>Cette façon d'échantillonner la matrice des poids associés aux arêtes est cette fois-ci centrée sur les dyades.</p><p>Remarque 1. Dans la suite on notera les entrées manquantes dans les dyades par NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">État de l'art</head><p>Dans cette section, nous allons présenter des modèles de graphes aléatoires, leurs spécificités et quelques résultats théoriques justifiant l'emploi de certains plutôt que d'autres. Ensuite, nous présenterons quelques méthodes d'inférence de ces modèles et leurs limites. Enfin nous introduirons la problématique des données manquantes et l'échantillonnage des réseaux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modèles probabilistes de graphes aléatoires</head><p>Nous présentons ici une revue de modèles probabilistes de graphes aléatoires. Les modèles génératifs de graphes comme les modèles petit monde (E J <ref type="bibr">Newman et al., 2002)</ref> et d'attachement préférentiel <ref type="bibr" target="#b8">(Barabási and Albert, 1999)</ref> ne seront pas présentés ici. Nous commençons par le premier modèle (historiquement parlant) défini par Erdős et Rényi. Puis nous continuons avec les modèles de graphes aléatoires géométriques et exponentiels. Enfin nous aborderons la grande classe des modèles de graphes aléatoires à espaces d'états latents, utilisés pour modéliser l'hétérogénéité des connexions observée dans un réseau. Rappelons qu'en mathématiques, un graphe G est la donnée du couple ({Noeuds}, {Arêtes}). L'ensemble des arêtes est constitué des dyades dont la valeur associée est non nulle. On représente G par une matrice d'adjacence Y = (Y ij ) 1≤i,j≤n ∈ M n (R) où n est le nombre de noeuds et Y ij = 1 s'il y a une arête entre les noeuds i et j, Y ij = 0 sinon. Cette matrice peut contenir d'autres valeurs que 0 ou 1. C'est le cas des graphes pondérés qui ne sont pas le sujet principal de cette thèse. D'éventuelles covariables associées aux noeuds seront désignées par la variable X = (X i ) 1≤i≤n . Si en revanche elles sont associées aux dyades, on les notera X = (X ij ) 1≤i,j≤n . Elles peuvent être catégorielles, quantitatives ou bien un mélange des deux. Rappelons que N := 1, n désigne l'ensemble des noeuds du graphe et D := N × N l'ensemble des dyades. Finalement nous noterons θ et Θ respectivement le paramètre associé à un modèle et l'espace des paramètres. Par la suite nous nous intéresserons plus particulièrement au cas de graphes binaires (sauf mention du contraire) non-orientés (i.e. dont la matrice d'adjacence Y est symétrique) et sans boucles (i.e. la diagonale de Y est nulle). Ce choix est fait pour plus de simplicité. Cependant tout ce qui suit s'étend naturellement aux cas de graphes pondérés et/ou nonsymétriques et/ou avec des boucles. 12 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèle d'Erdős-Rényi</head><p>Soit p ∈ [0, 1] une probabilité. Ce modèle couramment noté G(n, p) est défini comme suit :</p><formula xml:id="formula_1">(Y ij ) i,j ∼ iid B(p)</formula><p>où B est la loi de Bernoulli. On note que la loi des degrés est binomiale : D i := j Y ij ∼ Bin(n -1, p). Citons l'article <ref type="bibr">Gilbert (1959)</ref> et le livre plus récent <ref type="bibr" target="#b15">Bollobas (2001)</ref> dans lequel ce modèle a été largement étudié.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèles de graphes aléatoires géométriques</head><p>Les modèles de graphes aléatoires géométriques sont généralement utilisés pour modéliser des réseaux de télécommunications sans fil ou bien des propagations de phénomènes naturels tels que des incendies de forêt ou encore des épidémies. Nous citons le livre de <ref type="bibr" target="#b99">Penrose (2003)</ref> qui est une référence incontournable concernant cette classe de modèles. Dans ces modèles, les noeuds sont identifiés à des positions spatiales aléatoires (N i ) 1≤i≤n dans un espace métrique (E, d) suivant une certaine distribution (uniforme ou gaussienne par exemple). De plus, conditionnellement à la position des noeuds et en se fixant un seuil r &gt; 0, la matrice d'adjacence Y est déterministe. Plus précisément on a (Y ij ) i,j = 1 d(Ni,Nj )≤r .</p><p>À noter le lien entre ces modèles et le précédent : on peut montrer que localement, les graphes géométriques contiennent un graphe d <ref type="bibr">'Erdős-Rényi (voir Channarond, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèle Expected Degree (EDD)</head><p>Adapté des modèles définis dans <ref type="bibr" target="#b32">Chung and Lu (2002)</ref> et <ref type="bibr" target="#b94">Newman (2003)</ref>, le modèle EDD permet de contrôler en espérance les degrés de chaque noeud. Dans la suite on notera (K i ) i les « degrés attendus » (non nécessairement entiers) associés à chaque noeud et G une densité de probabilité. Alors,</p><formula xml:id="formula_2">(K i ) i ∼ iid G (Y ij ) i,j | (K i ), (K j ) ∼ ind B(K i K j /κ), κ ∈ R. De cette façon, E[D i |K i ] ∝ K i , ∀i ∈ 1, n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèle de graphes aléatoires exponentiels (ERGM)</head><p>La famille exponentielle permet à elle seule de définir un grand nombre de lois classiques telles que les lois de Bernoulli, binomiale, Poisson et gaussienne par exemple. L'idée des ERGM (Exponential Random Graph Models) est d'utiliser la caractère générique de l'écriture de la famille exponentielle pour modéliser un graphe. Ce modèle exprime la loi d'un graphe en fonction de statistiques exhaustives, telles que le nombre d'arêtes dans le graphe, le nombre de triangles pour décrire tout type de dépendance entre les arêtes. Leur loi est de la forme</p><formula xml:id="formula_3">P(Y = y) = 1 Λ θ exp C∈D θ C g C (y)</formula><p>où y est une réalisation de Y et (i) chaque C est appelé une configuration, i.e. un sous-ensemble d'arêtes ou de dyades parmi un sous-ensemble de noeuds du graphe ;</p><p>(ii) g C (y) = (i,j)∈C y ij indique si la configuration C apparaît dans Y ; 13 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ÉTAT DE L'ART INTRODUCTION</head><p>(iii) les paramètres θ C sont associés à la configuration C. Si θ C = 0 alors les arêtes de la configuration sont indépendantes conditionnellement aux arêtes restantes du graphe. Une valeur non nulle de θ C induit de la dépendance entre les arêtes de C conditionnellement aux restes des arêtes. Enfin, quand une valeur positive de θ C encourage la configuration, une valeur négative la pénalise ;</p><p>(iv) Λ θ est une constante de normalisation dépendant de θ,</p><formula xml:id="formula_4">Λ θ = y exp C∈D θ C g C (y) . (1.2)</formula><p>On remarquera que toute collection de paramètres {θ C , C ∈ D} ne conduit pas automatiquement à un modèle où la distribution jointe de Y est bien définie. Les conditions de validité sont décrites dans le théorème de <ref type="bibr">Hammersley-Clifford (voir Besag, 1974)</ref>. On notera certaines limites théoriques et pratiques importantes évoquées dans <ref type="bibr">Chatterjee and Diaconis (2013)</ref> à propos des ERGM (ce travail concerne exclusivement le cas des graphes denses et s'inscrit dans le cadre de la théorie limite des graphes développée dans Lovász, 2012) comme la dégénérescence de modèles (beaucoup de réalisations sont en fait soit des graphes vides d'arêtes soit complets), ou l'inutilité des statistiques exhaustives prises en comptes dans le modèle <ref type="bibr">(Bhamidi et al., 2011)</ref>, ou encore le fait que beaucoup de réalisations des ERGM ne se distinguent pas d'un graphe simulé suivant le modèle d'Erdős-Rényi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèles de graphes aléatoires à espaces d'états latents</head><p>Dans cette partie nous allons présenter en détail la classe des modèles à espaces d'états latents. Ces modèles ont pour principal objectif la modélisation de l'hétérogénéité des profils des individus dans un graphe. Ils permettent aussi de faire du clustering en regroupant les noeuds dans des groupes homogènes d'un point de vue de leurs connectivité. Nous noterons Z := {Z i , i ∈ N } les variables latentes associées aux noeuds. Dans la suite deux cas se dissocieront : quand l'espace latent est fini et quand il est continu. Quand il est fini, nous noterons Q le nombre d'états latents (ou groupes) et Q := 1, Q . Les variables latentes prennent alors la forme de Q-uplets composés de 1 et de 0. La valeur 1 à la coordonnée k signifie que le noeud est dans l'état latent k. Un noeud peut appartenir à 0, 1 ou plusieurs états latents suivant les modèles. Quand l'espace latent est continu, nous considérerons que les variables latentes appartiennent à R d . D'après la Figure <ref type="figure" target="#fig_1">1</ref>.1, on a P(Y |Z) = ⊗ (i,j)∈D P(Y ij |Z i , Z j ). Ainsi on définit le Stochastic 14 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART Block Model dans le cas général comme suit :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Block Model (SBM)</head><note type="other">.</note><formula xml:id="formula_5">(Z i ) i ∼ iid M(1, α) (Y ij ) i,j | (Z iq = 1), (Z j = 1) ∼ ind f (•; δ q )</formula><p>où α = (α 1 , ..., α Q ) est le paramètre de mélange et δ = (δ q ) (q, )∈Q 2 la matrice des paramètres de connexion intra et inter-groupes. Cette matrice est aussi appelée matrice de connectivité dans le cas de données binaires. Le cas du SBM binaire (ou Bernoulli) a été étudié dans <ref type="bibr" target="#b111">Snijders and Nowicki (1997)</ref> et <ref type="bibr" target="#b36">Daudin et al. (2008)</ref>. Il correspond au cas où la loi de Y ij sachant Z i , Z j est la loi de Bernoulli (voir Table <ref type="table">1</ref>.1) et</p><formula xml:id="formula_6">∀y ∈ {0, 1} et ∀π ∈ [0, 1], f (y; π) = π y (1 -π) 1-y .</formula><p>C'est ce cas qui concentrera l'essentiel de notre attention dans la thèse.</p><p>Le cas des SBM pondérés a été largement développé dans <ref type="bibr" target="#b88">Mariadassou et al. (2010)</ref>. On y trouve par exemple les cas détaillés (modèle et estimation) des lois d'émissions Poisson, régression poissonnienne avec effets homogène (PRMH) ou inhomogène (PRMI), multinomiale, gaussienne (univariée, bivariée ou encore la régression linaire). Nous rappelons quelques lois classiques et les paramètres associés dans la Table <ref type="table">1</ref>.1. Nous citerons aussi les modèles Multiplex SBM <ref type="bibr" target="#b9">(Barbillon et al., 2015)</ref> et Multipartite SBM <ref type="bibr" target="#b7">(Bar-Hen et al., 2018)</ref> dans lesquels des liens de différentes natures entre des individus sont modélisés ; le Stochastic Topic Block Model <ref type="bibr" target="#b17">(Bouveyron et al., 2016</ref><ref type="bibr" target="#b34">et Corneli et al., 2018)</ref> dans lequel les arêtes représentent un échange de texte et enfin les modèles de SBM dynamiques <ref type="bibr" target="#b90">(Matias and Miele, 2017</ref><ref type="bibr" target="#b91">, Matias et al., 2018</ref><ref type="bibr" target="#b84">et Ludkin et al., 2018)</ref> tous directement inspirés du SBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modèle</head><p>Loi de probabilité Paramètre δ q Bernoulli B(π q ) π q Bernoulli avec covariables B(g(γ q + X T ij β))</p><p>(γ q , β)</p><formula xml:id="formula_7">Poisson P(λ q )</formula><p>λ q PRMH P(λ q e β T X ij ) (λ q , β)</p><formula xml:id="formula_8">PRMI P(λ q e β T q X ij ) (λ q , β q ) Multinomial M(1; p q ) p q</formula><p>Gaussien univarié N (µ q , σ 2 q )</p><p>(µ q , σ 2 q )</p><p>Gaussien bivarié N (µ q , Σ q ) (µ q , Σ q )</p><p>Régression linéaire simple</p><formula xml:id="formula_9">N (a q + bX ij , σ 2 ) (a q , b, σ 2 ) Régression linéaire N (β T q X ij , σ 2 q )</formula><p>(β q , σ 2 q )</p><p>Table <ref type="table">1</ref>.1 -Modèles de SBM et lois de probabilités de : Y ij |Z iq = 1, Z j = 1, sauf pour le modèle gaussien bivarié dont la loi est celle de :</p><formula xml:id="formula_10">(Y ij , Y ji )|Z iq = 1, Z j = 1.</formula><p>Le SBM est identifiable (voir <ref type="bibr" target="#b24">Celisse et al., 2012)</ref> sauf sur un ensemble de mesure de Lebesgue nulle et à permutation des labels près.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Degree Corrected Stochastic Block Model (DCSBM).</head><p>Une limite du SBM tel que défini précédemment concerne le fait que tous les noeuds d'un même groupe sont stochastiquement équivalents. En particulier E[Y ij |Z iq = 1, Z j = 1] = π q ne dépend que des classes des noeuds i et j et en espérance 2 noeuds d'un même groupe ont les mêmes degrés. Ainsi, dans le but de prendre en compte la propension de chaque noeud à se connecter aux autres et introduire plus d'hétérogénéité dans les degrés des noeuds du graphe, les auteurs de <ref type="bibr" target="#b68">Karrer and Newman (2011)</ref> ont proposé le DCSBM dans lequel chaque noeud i a un paramètre γ i qui lui est propre et influe sur son degré de sorte que E[Y ij |Z iq = 1, Z j = 1] = γ i γ j π q . Ces paramètres de degré sont soumis à la contrainte i γ i 1 {Ziq=1} = 1, ∀q ∈ Q pour que le modèle soit identifiable. 15 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ÉTAT DE L'ART INTRODUCTION</head><p>Ce modèle a été étudié dans le cas d'une loi d'émission de Bernoulli ou Poisson. La loi de Poisson permet une mise à jour explicite des paramètres du modèle a contrario de la loi de Bernoulli. La loi de Poisson est donc utilisée pour approcher la loi de Bernoulli dans le cas d'une faible espérance correspondant à des données parcimonieuses. Une étude théorique de ces modèles est menée dans <ref type="bibr">Zhao et al. (2012b)</ref>.</p><p>Popularity-Adjusted Block Model (PABM). La « popularité » des noeuds est fortement liée à la notion de structure en communauté (ou affiliation). La popularité observée d'un noeud i au sein de la communauté (ou groupe) q est donnée par P iq = j, Zj =q Y ij et son espérance µ iq = E[P iq ]. On remarque que, dans le cas du SBM, µ iq = #{j, Z jq = 1}π ciq et que pour le DCSBM, µ iq = γ i π ciq où c i = Q q=1 q1 {Ziq=1} . Donc, dans le SBM, les noeuds d'une même communauté ont tous la même popularité et dans le DCSBM, la popularité des noeuds est modulée par leurs degrés. Ainsi, deux noeuds de forts degrés ont plus de chance de se connecter entre eux que deux noeuds de faibles degrés. Ce point de modélisation n'est pas très réaliste si par exemple les communautés représentent des clans opposés. En effet, dans ce cas les noeuds de forts degrés représentent les « chefs » opposés et il est peu probable qu'ils se connectent entre eux. Au contraire, les noeuds de faible degré représentant des gens dont la position sociale est moins extrême se connecteront plus volontiers. Le PABM <ref type="bibr" target="#b107">(Sengupta and</ref><ref type="bibr">Chen, 2018 et Noroozi et al., 2019)</ref> prend en compte cet aspect de modélisation dans sa définition. Soit (λ iq ) i∈N ,q∈Q les paramètres de popularité associés à chaque noeud et vis-à-vis de chaque classe. Alors,</p><formula xml:id="formula_11">(Z i ) i ∼ iid M(1, α) (Y ij ) i,j | (Z iq = 1), (Z j = 1) ∼ ind B(λ i λ jq ).</formula><p>Le modèle est identifiable sous la contrainte que Λ q = Λ q où Λ q := j, Zj =q λ j . De plus, sous le PABM, µ iq = λ iq Λ qci .</p><p>Overlapping Stochastic Block Model (OSBM). Une hypothèse du SBM est que chaque noeud appartient à un et un seul groupe. L'overlapping SBM <ref type="bibr" target="#b75">(Latouche et al., 2011)</ref> est une adaptation du SBM qui permet à chaque noeud d'appartenir à plusieurs groupes. Plus précisément, la loi de Z est donnée par</p><formula xml:id="formula_12">(Z i ) i ∼ iid ⊗ Q q=1 B(α q ).</formula><p>On remarque que cette définition autorise toutes les composantes de Z i à être à zéro, ce qui permet de ne pas avoir de classe composée de noeuds de faible degré comme on peut souvent observer dans les problèmes de classification. En effet, ces « outliers » ne sont pas classés par OSBM, i.e. Z i = 0. On peut maintenant donner la loi conditionnelle de</p><formula xml:id="formula_13">Y ij sachant Z i et Z j : (Y ij ) i,j | (Z iq = 1), (Z j = 1) ∼ ind B(g(a q )) (1.3) où g(x) = (1 + e x ) -1 . De plus a ZiZj = Z T i W Z j + Z T i U + Z T j V + W * avec W ∈ M Q (R) et U, V ∈ M Q1 (R).</formula><p>Le premier terme de (1.3) décrit les interactions entre les noeuds i et j alors que les second et troisième termes modélisent respectivement les capacités des noeuds i et j à se connecter aux autres noeuds. Le dernier terme est scalaire et représente un biais pour modéliser la parcimonie. Enfin, nous noterons que l'OSBM est identifiable <ref type="bibr" target="#b75">(Latouche et al., 2011)</ref>, à échange des numéros des groupes près et sauf sur un ensemble de mesure de Lebesgue nulle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Membership Stochastic Block Model (MMSBM).</head><p>Ce modèle est défini pour la première fois dans <ref type="bibr" target="#b1">Airoldi et al. (2008)</ref> avec pour but de prendre en compte dans chaque relation possible la variabilité propre à chaque noeud de se connecter aux autres. Dans le 16 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART modèle OSBM, chaque noeud appartient à plusieurs classes et la probabilité que deux noeuds se connectent dépend de toutes les classes auxquels appartiennent les deux noeuds. Dans MMSBM, pour chaque dyade, la probabilité que les deux noeuds en question se connectent ne dépend que d'un seul des groupes auxquels appartient chaque noeud. Plus précisément,</p><formula xml:id="formula_14">(γ i ) i ∼ iid Dirichlet(α) (Z i→j ) i,j ∼ ind M(1, γ i ) (Z j→i ) i,j ∼ ind M(1, γ j ) (Y ij ) i,j | (Z i ), (Z j ) ∼ ind B(Z T i→j πZ j→i ).</formula><p>Le paramètre de mélange est donc α ∈ Q, et la matrice Q × Q de connectivité est π, comme dans le SBM. Notons que l'identifiabilité de ce modèle n'a pas encore été démontrée.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Block Model (LBM).</head><p>Le LBM est un modèle fréquemment utilisé pour faire du clustering de ligne et colonne de matrice a priori non carrée (ou bi-clustering), pas nécessairement une matrice d'adjacence au sens classique. Il a été introduit par <ref type="bibr" target="#b52">Govaert and Nadif (2003)</ref>. Nous noterons M ∈ M nd (R) une telle matrice. Ce modèle est très lié au SBM qui correspond au cas où les groupes en colonnes sont identiques aux groupes en lignes. Il permet en particulier de modéliser un graphe bipartite. On définit Z et W les variables latentes respectivement associées aux lignes et aux colonnes, leur loi étant donnée par</p><formula xml:id="formula_15">(Z i ) i ∼ iid M(1, α) (W j ) j ∼ iid M(1, ρ). On a donc (M ij ) i,j | (Z i ), (W j ) ∼ ind f (•; Z T i πW j ).</formula><p>Les paramètres α = (α 1 , ..., α Q1 ) et ρ = (ρ 1 , ..., ρ Q2 ) sont les paramètres de mélange associés respectivement aux lignes et aux colonnes et π ∈ M Q1Q2 (R) la matrice des paramètres de connexions inter et intra-groupes. Comme dans le cas du SBM la densité f peut par exemple appartenir à la famille exponentielle ou être une densité quelconque.</p><p>Le LBM est identifiable sous certaines conditions décrites dans <ref type="bibr" target="#b69">Keribin et al. (2015)</ref> et à permutation près des labels en lignes et en colonnes. <ref type="bibr" target="#b56">Handcock et al. (2007)</ref> et inspiré du SBM, le LPCM a pour but de modéliser un réseau basé sur les distances sociales entre individus. Ceci implique que deux personnes ayant un profil social proche auront une plus grande probabilité de se connecter. Cette tendance à s'affilier à ses semblables s'appelle l'homophilie. On associe (comme dans le SBM) à chaque noeud du graphe une variable latente continue à valeurs dans R d correspondant aux caractéristiques sociales des noeuds :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Position Cluster (LPCM). Introduit par</head><formula xml:id="formula_16">(Z i ) i ∼ iid q∈Q α q N (µ q , σ 2 q I d ).</formula><p>(1.4)</p><p>Les noeuds sont donc d'abord placés dans des groupes à la façon du SBM avec probabilités α, puis leurs caractéristiques sociales -qui dépendent du groupe auxquels ils appartiennent -suivent une certaine loi normale multi-dimensionnelle. Les variables latentes sont ainsi indépendantes conditionnellement aux groupes auxquels chaque noeud appartient (correspondant à la première étape du processus). Finalement,</p><formula xml:id="formula_17">(Y ij ) i,j | (Z i ), (Z j ) ∼ ind B(g(β 0 c ij -β 1 Z i -Z j )) (1.5)</formula><p>avec g = (1 + e x ) -1 et β 1 ≥ 0 pour que la probabilité soit faible quand la distance sociale est grande entre i et j et élevée quand ils sont proches socialement. Les (c ij ) (i,j)∈D jouent le rôle de covariables observées sur les arêtes.</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction 2. <ref type="bibr">ÉTAT</ref>   </p><formula xml:id="formula_18">(Z i ) i ∼ iid f (Y ij ) i,j | (Z i ), (Z j ) ∼ ind B k Z i -Z j h n avec h n &gt; 0 et k : R → [0,</formula><formula xml:id="formula_19">(Z i ) i ∼ iid U(I) (Y ij ) i,j | (Z i ), (Z j ) ∼ ind f (Z i , Z j ).</formula><p>(1.6)</p><p>Le modèle défini ainsi n'est pas identifiable. En effet, pour toute mesure σ préservant les fonctions définies et à image dans [0, 1], composer la fonction f avec σ donne exactement le même modèle qu'avec la fonction f . Pour pallier ce problème, on peut par exemple supposer que x → f (x, y)dy est croissante <ref type="bibr" target="#b77">(Latouche et al., 2018)</ref>. Enfin, pour relier le SBM au graphon, on remarquera que l'équation (1.6) permet de définir le SBM comme un graphon. En effet, si la fonction f est constante sur chaque bloc rectangulaire de taille α q ×α et vaut π q , alors ceci correspond au SBM de paramètres α et π tel que défini précédemment. La variable latente Z i associée au noeud i est alors simplement le numéro du sous-intervalle dans lequel tombe la variable aléatoire Z i . (ii) La loi du sous-graphe induit pas l'ensemble de noeuds allant de 1 à n -1 est la même que la loi de G n-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Garanties théoriques : théorème de représentation et loi limite</head><p>(iii) Pour tout 1 &lt; k &lt; n, les sous-graphes de G n induits par les ensembles de noeuds 1, i et i + 1, n sont indépendants. 18 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limite de graphes aléatoires</head><p>Le chapitre 11 du livre <ref type="bibr" target="#b82">Lovász (2012)</ref> est consacré à l'étude de la convergence des suites de graphes aléatoires denses et à l'identification de la loi (ou graphe) limite. Dans le cadre théorique qui y est développé il est montré que le graphon est le bon modèle limite pour des suites de graphes aléatoires tels que ceux satisfaisant les conditions du Théorème 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inférence des modèles de graphe</head><p>Il existe globalement 2 grandes familles de méthode pour estimer les modèles décrits à la section 2.1 : (i) les méthodes particulaires de type MCMC (Markov Chain Monte Carlo) et (ii) les méthodes dérivées de l'algorithme EM (Expectation -Maximization, <ref type="bibr" target="#b39">Dempster et al., 1977)</ref>. Concernant les méthodes MCMC, les algorithmes de Metropolis-Hastings ou l'échantillonneur de Gibbs (voir <ref type="bibr">Gilks et al., 1995)</ref> sont les plus couramment utilisés. Leur but est d'estimer une loi a posteriori. Concernant les dérivées de l'EM, il existe de nombreux algorithmes tels que le CEM <ref type="bibr" target="#b23">(Celeux and Govaert, 1991)</ref>, le VEM <ref type="bibr" target="#b67">(Jordan et al., 1998</ref><ref type="bibr" target="#b66">et Jaakkola, 2000)</ref>, ou encore le VBEM <ref type="bibr" target="#b76">(Latouche et al., 2012</ref><ref type="bibr" target="#b18">et Brault, 2014)</ref>, dont l'objectif est d'estimer le maximum de vraisemblance. Ces deux familles de méthodes sont parfois couplées, comme dans les algorithmes SEM <ref type="bibr" target="#b22">(Celeux and Diebolt, 1985)</ref>, MCEM <ref type="bibr" target="#b127">(Wei and Tanner, 1990)</ref>, SAEM <ref type="bibr" target="#b38">(Delyon et al., 1999)</ref> ou encore SEM-Gibbs et SEM+VEM (voir <ref type="bibr" target="#b18">Brault, 2014)</ref>. Certains algorithmes de clustering comme les k-means <ref type="bibr" target="#b112">(Steinhaus (1956)</ref> et MacQueen ( <ref type="formula">1967</ref>)) et le Spectral Clustering <ref type="bibr" target="#b40">(Donath and Hoffman (1973)</ref> et <ref type="bibr" target="#b104">Rohe et al. (2010)</ref> appliqué au SBM) sont fréquemment utilisés pour initialiser les algorithmes MCMC ou les dérivées de l'EM. L'algorithme des k-means divise les données en k groupes en minimisant la distance d'un point à la moyenne des points de son groupe. Le spectral clustering quant à lui applique une méthode de clustering comme les k-means aux vecteurs propres de la matrice Laplacienne associée à un graphe.</p><p>Concernant les modèles de graphes aléatoires à variables latentes, il est impossible de factoriser la loi de Z conditionnellement à Y (voir <ref type="bibr" target="#b78">Lauritzen, 1996)</ref>. Ces modèles ne peuvent donc pas être estimés avec l'algorithme EM. Les algorithmes MCMC ont d'abord été utilisés pour estimer ces modèles on citera <ref type="bibr" target="#b23">Celeux and Govaert (1991)</ref>, <ref type="bibr" target="#b96">Nowicki and Snijders (2001)</ref> pour le SBM et <ref type="bibr" target="#b52">Govaert and Nadif (2003)</ref> pour le LBM. Ils souffrent cependant d'un manque de rapidité computationnelle et ne s'appliquent qu'à des réseaux composés de plusieurs centaines de noeuds. Les algorithmes avec approximation variationnelle <ref type="bibr" target="#b67">(Jordan et al. (1998)</ref>, <ref type="bibr" target="#b66">Jaakkola (2000)</ref>), basés sur l'EM et dans lesquels la loi de Z|Y est approchée ont ensuite été utilisés : voir <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> pour le SBM, <ref type="bibr" target="#b69">Keribin et al. (2015)</ref> pour le LBM, <ref type="bibr" target="#b76">Latouche et al. (2012)</ref> pour le OSBM, <ref type="bibr" target="#b1">Airoldi et al. (2008)</ref> pour le MMSBM, <ref type="bibr" target="#b74">Latouche and Robin (2016)</ref> pour le graphon. Ceci est vrai aussi pour toutes les versions du SBM (dynamiques ou pas).</p><p>Pour l'estimation de très grands graphes, on citera le Largest Gaps Algorithm (LGA) <ref type="bibr" target="#b26">(Channarond (2013)</ref> pour le SBM et <ref type="bibr" target="#b18">Brault (2014)</ref> pour le LBM) qui estime les paramètres du modèle ainsi que la classification des noeuds à partir de la densité empirique des degrés des noeuds en évaluant les plus grands « gaps » dans la densité. Cependant cet algorithme s'applique principalement à de gros graphes composés d'au moins plusieurs milliers de noeuds.</p><p>L'estimation des modèles ERGM quant à eux se fait essentiellement sur la base d'algorithmes MCMC, voir <ref type="bibr" target="#b109">Snijders (2002)</ref>, <ref type="bibr">Hunter and Handcock (2006a)</ref> et <ref type="bibr" target="#b70">Kolaczyk (2009)</ref> pour plus de détails. On remarquera que la constante de normalisation Λ θ dépend de θ, c'est un problème pour utiliser des algorithmes MCMC comme le Metropolis-Hastings.</p><p>Dans la suite nous présentons en détail l'algorithme VEM avec comme fil rouge son application au SBM.</p><p>Variational EM. Cet algorithme a pour but de maximiser la vraisemblance des données observées. On décompose pour cela la log-vraisemblance des données log p θ (Y) de la façon 19 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction 2. ÉTAT DE L'ART INTRODUCTION suivante : log p θ (Y) = log p θ (Y, Z) -log p θ (Z|Y).</p><p>(1.7)</p><p>Comme dans le raisonnement amenant à l'algorithme EM décrit dans <ref type="bibr" target="#b39">Dempster et al. (1977)</ref>, on intègre (1.7) de chaque coté par rapport à une loi Q portant seulement sur Z. On obtient :</p><formula xml:id="formula_20">log p θ (Y) = E Q [log p θ (Y, Z)] + H(Q) + KL[Q p θ (Z|Y)] (1.8)</formula><p>où H est l'entropie de Shannon (positive, maximale pour la loi uniforme) et KL(Q, P) la divergence de Kullback-Leibler (positive et nulle si et seulement si P loi = Q). Si Q était égale à p θ (Z|Y), alors maximiser (1.8) en θ reviendrait à utiliser exactement l'algorithme EM. Cependant, à cause de la structure de dépendance existante dans les modèles de graphes aléatoires à variables latentes entre les variables Y et Z (Figure <ref type="figure" target="#fig_1">1</ref>.1), le calcul de l'étape E de l'algorithme EM, autrement dit le calcul de E Y|Z [log p θ (Y, Z)], se trouve compromis car la distribution de P(Z|Y) ne se factorise d'aucune façon. Finalement, en utilisant la décomposition (1.8) et la positivité de la divergence de Kullback-Leibler, on obtient :  <ref type="bibr">(voir Jaakkola, 2000</ref><ref type="bibr" target="#b36">et Daudin et al., 2008)</ref> de prendre la classe des distributions factorisables, i.e. qui s'écrivent sous la forme</p><formula xml:id="formula_21">log p θ (Y) ≥ J θ,Q (Y) := E Q [log p θ (Y, Z)] + H(Q) (1.9) = log p θ (Y) -KL[Q p θ (Z|Y)]. (1.10) Améliorer la borne inférieure (1.10) équivaut à minimiser KL[Q p θ (Z|Y)] en Q dans l'en- semble des distributions, c'est-à-dire trouver Q = p θ (Z|Y)</formula><formula xml:id="formula_22">Q(Z, τ ) = i∈N Q(Z i , τ i ).</formula><p>(1.11)</p><p>Cette approximation est aussi appelée « champs moyens ». Dans le cadre précis du SBM on prendra  <ref type="bibr">(voir Lauritzen, 1996)</ref>. Finalement en utilisant les équations (1.9) et (1.10), en remplaçant l'étape E de l'algorithme EM par une étape dite variationnelle E (ou VE) on obtient l'algorithme VEM qui consiste à itérer les 2 étapes suivantes jusqu'à convergence vers un maximum local :</p><formula xml:id="formula_23">Q(Z, τ ) = ⊗ i∈N M(Z i ; τ i ) avec τ i = (τ i1 , ..., τ iQ ) ∈ [0, 1] Q . On</formula><p>• VE-step : avec l'estimation courante θ h of θ, calculer</p><formula xml:id="formula_24">τ h+1 = arg min τ ∈[0,1] Q KL[Q(Z; τ )||p θ h (Z|Y)].</formula><p>• M-step : mettre à jour l'estimation courante θ h de θ</p><formula xml:id="formula_25">θ h+1 = arg max θ E Q(Z;τ h+1 ) [log(p θ (Y, Z))].</formula><p>Cet algorithme produit une séquence {τ h , θ h , h ≥ 0} croissante pour la fonction J θ,Q . 20 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART Sélection de modèle. L'estimation du nombre de groupes dans les modèles de mélanges avec variables latentes se fait généralement en utilisant le critère ICL (Integrated Completed Likelihood) proposé dans <ref type="bibr" target="#b13">Biernacki et al. (2000)</ref>. Ce critère est un dérivé du critère BIC (Bayesian Information Criterion) de <ref type="bibr">Schwarz (1978)</ref> et pénalise la vraisemblance complète classifiante des données.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Théorie des données manquantes</head><p>La problématique des données manquantes est centrale en statistique. Nous rappelons dans cette partie les rudiments de la théorie introduite dans <ref type="bibr" target="#b105">Rubin (1976)</ref> adaptée aux données « réseaux ». Rappelons les notations suivantes : (ii) Missing At Random (MAR), si l'échantillonnage ne dépend que des valeurs des données observées mais pas des valeurs des données non-observées. En termes mathématiques,</p><formula xml:id="formula_26">Définition 1 (Matrice et vecteur d'échantillonnage). Soit R = (R ij ) (i,j)∈D , alors pour tout (i, j) ∈ D, R ij = 1 si la dyade (i,</formula><formula xml:id="formula_27">D o := {(i, j) : R ij = 1}, D m := {(i, j) : R ij = 0}, Y o := {Y ij : (i, j) ∈ D o } et Y m := {Y ij : (i, j) ∈ D m }.</formula><formula xml:id="formula_28">R |= Y m | Y o .</formula><p>(iii) Not Missing At Random (NMAR), si l'échantillonnage dépend des valeurs de toutes les données. C'est le cas par exemple de la censure.</p><p>On remarque que (i) est un cas particulier de (ii). L'intérêt de la dichotomie entre le cas MAR et NMAR réside dans la proposition suivante.</p><formula xml:id="formula_29">Proposition 1. Si la loi de R satisfait (i) ou (ii), alors pour tout ψ ∈ Ψ tel que p θ,ψ (Y o , R) = 0 : arg max θ p θ,ψ (Y o , R) = arg max θ p θ (Y o ),</formula><p>où p désigne la vraisemblance des données. Ceci revient à dire que la loi du processus d'échantillonnage ne perturbe pas l'inférence des paramètres du modèle génératif des données.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Échantillonner un réseau</head><p>Dans le domaine de l'échantillonnage de réseau, nous allons aborder deux problématiques partant de postulats différents. D'un coté la théorie des données manquantes développée par D. Rubin rappelée dans la section 2.4, qui s'inscrit dans un cadre « model-based » au sens où Y est une variable aléatoire. On s'intéresse dans ce cas explicitement à ce qui caractérise sa loi. D'un autre coté une approche dite « design-based » (voir <ref type="bibr" target="#b70">Kolaczyk, 2009)</ref> dans laquelle 21 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction C'est un estimateur sans biais de s. En effet :</p><formula xml:id="formula_30">E[ŝ µ ] = E i∈E t i µ i = E i∈N t i µ i 1 i∈E = i∈N t i µ i E [1 i∈E ] = s.</formula><p>Pour la suite, nous noterons µ ij = P(i ∈ E, j ∈ E).</p><p>Nous allons maintenant donner deux exemples de stratégies d'échantillonnage ainsi que les probabilités associées aux estimateurs d'Horvitz-Thompson.</p><p>Sous-graphe induit. On sélectionne aléatoirement et uniformément un sous-ensemble de noeuds d'un graphe G = (N, A) et on observe tout le sous graphe induit par ces noeuds (i.e. le sous-graphe G = (N , A ) de G composé des noeuds échantillonnés (N de cardinal n ) ainsi que les arêtes de G entre eux (A )). On a donc</p><formula xml:id="formula_31">µ i = n n et µ ij = n (n -1) n(n -1) .</formula><p>Sous-graphe incident. On sélectionne par un tirage sans remise un sous-ensemble d'arêtes A (de cardinal N A ) d'un graphe G et on observe tous les noeuds incident à ces arêtes : N . On a donc</p><formula xml:id="formula_32">µ i =      1 - N A -Di n N A n , si n ≤ N A -D i 1 , sinon et µ ij = n N A ou D i est le degré du noeud i.</formula><p>On trouvera dans <ref type="bibr" target="#b70">Kolaczyk (2009)</ref> d'autres exemples d'échantillonnages centrés sur les noeuds ainsi que les probabilités d'être échantillonné µ i et µ ij respectivement associées aux noeuds et aux arêtes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 2. ÉTAT DE L'ART </p><formula xml:id="formula_33">P(R ij = 1|Y, ψ) = 1 -(1 -ψ) 2 , ∀i &lt; j.</formula><p>De plus, en notant 1 n le vecteur colonne de dimension n composé de 1, le vecteur V définie dans la définition 1 est donnée pas la relation logique : </p><formula xml:id="formula_34">V = [R1 n = (n -1)1 n ]. Réciproque- ment on peut retrouver R à partir de V avec la relation R = 1 n • V + V • 1 n -V • V,</formula><formula xml:id="formula_35">P(R = r|Y, ψ) = P(V = v|Y, ψ) = ψ v T 1n (1 -ψ) n-v T 1n .</formula><p>Cet échantillonnage est MCAR car la probabilité d'observer une dyade est complètement indépendante de sa valeur et de la valeur des autres dyades du graphe. Ce qui n'est pas le cas des deux stratégies d'échantillonnage que nous allons définir dans la suite (qui sont MAR mais pas MCAR). Dans ces exemples, les dyades seront observées par vagues successives et la probabilité d'observer une dyade dépend des dyades vues aux vagues précédentes.</p><p>One-wave link-tracing design. Pour cet échantillonnage, on sélectionne indépendamment des noeuds du graphe avec probabilité ψ. Ceci constitue un premier groupe de noeuds V 0 . Puis on observe toutes les dyades dont au moins l'une des extrémités inclue un des noeuds sélectionnés auparavant. Les voisins des noeuds du premier groupe (i.e. les noeuds partageant une arête avec les noeuds du premier groupe) constituent alors un deuxième groupe de noeuds que l'on notera V 1 . Finalement, on observe toutes les dyades incluant les noeuds du deuxième groupe.</p><p>On a donc la relation</p><formula xml:id="formula_36">V = V 0 + V 1 et V 1 est relié à V 0 et Y par la relation logique V 1 = [YV 0 (1 -V 0 ) &gt; 0] où</formula><p>est le produit de Hadamard de deux matrices. Par ailleurs les variables R et V sont reliées de la même façon que pour l'ego-centric design. En conséquence, la loi du one-wave link-tracing design est donnée par</p><formula xml:id="formula_37">P(R = r|Y, ψ) = v0, v0+[Yv0 (1-v0)&gt;0]=v ψ v T 0 1n (1 -ψ) n-v T 0 1n .</formula><p>Multi-wave link-tracing design. Cet échantillonnage consiste en l'application successive du one-wave link-tracing design. Nous noterons k le nombre de « vagues » successives d'échantillonnages. En utilisant les notations précédemment définies, pour m ∈ 1, k on définit l'ensemble des noeuds échantillonnés à la k-ième vague sachant toutes les vagues précédentes, par la relation logique</p><formula xml:id="formula_38">V m = [YV m-1 (1 - m-1 t=0 V t ) &gt; 0]</formula><p>. Finalement, la loi du k-waves link tracing design est donnée par</p><formula xml:id="formula_39">P(R = r|Y, ψ) = v0, v0+v1+•••+v k =k ψ v T 0 1n (1 -ψ) n-v T 0 1n .</formula><p>23 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction 3. CONTRIBUTIONS DE LA THÈSE INTRODUCTION</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contributions de la thèse</head><p>Nous décrivons dans cette section ce que nous avons apporté d'original dans l'étude du SBM avec données manquantes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Données manquantes dans le SBM</head><p>Le SBM est un modèle à variables latentes correspondant aux groupes auxquels appartiennent les noeuds. Cela signifie que sans la connaissance des groupes, la vraisemblance de Y n'a pas de forme explicite. La notion de « Missing At Random » telle que définie dans <ref type="bibr" target="#b105">Rubin (1976)</ref> n'est donc pas applicable directement et nécessite une adaptation. Considérons la vraisemblance des données observées :</p><formula xml:id="formula_40">p θ,ψ (Y o , R) = p θ (Y o , Y m , Z)p ψ (R|Y o , Y m , Z)dZdY m .</formula><p>(1.12)  </p><formula xml:id="formula_41">Z Y R Z Y R (a) (b) Z Y R Z Y R (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stratégies d'échantillonnages</head><p>Nous avons vu dans la partie 1 des exemples de réseaux échantillonnés par des stratégies respectivement noeuds-centrées et dyades-centrées. Nous donnons maintenant des exemples de stratégies d'échantillonnages d'un réseau binaire inspirées des données décrites dans la section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stratégies dyades-centrées :</head><p>Random-dyad sampling : chaque dyade (i, j) ∈ D a la même probabilité P(R ij = 1) = ρ := ψ d'être observée, indépendamment les unes des autres.</p><p>• Double standard sampling : soit ψ := (ρ 1 , ρ 0 ) ∈ [0, 1] 2 . L'échantillonnage double standard (ou deux poids deux mesures) est une stratégie pour laquelle la probabilité 24 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 3. CONTRIBUTIONS DE LA THÈSE d'observer une dyade dépend de sa valeur :</p><formula xml:id="formula_42">(R ij ) i,j | (Y ij ) ∼ ind B(ρ 1 1 {Yij =1} + ρ 0 1 {Yij =0} ).</formula><p>Le cas ρ 1 = ρ 0 correspond au random-dyad sampling.</p><p>• Block-dyad sampling : soit ψ := (ρ q ) (q, )∈Q 2 ∈ M Q ([0, 1]). L'échantillonnage blockdyad (ou bloc-dyade) est une stratégie où la probabilité d'observer une dyade (i, j) dépend des classes des noeuds i et j :</p><formula xml:id="formula_43">(R ij ) i,j | (Z i ), (Z j ) ∼ ind B(ρ ZiZj ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stratégies noeuds-centrées :</head><p>Star and snowball samplings : l'échantillonnage star (ou en étoile) consiste à sélectionner aléatoirement un ensemble de noeuds, puis à observer les lignes correspondantes de la matrice Y. L'échantillonnage snowball (ou boule de neige) quant à lui se déroule en plusieurs « vagues » dont la première correspond à un échantillonnage en étoile. Les vagues successives consistent à observer les voisins des noeuds sélectionnés à la vague précédente. Ces échantillonnages correspondent au multi-waves link tracing design défini dans la sous-sous section 2.5 pour respectivement k = 0 et k ≥ 1.</p><p>• Star degree sampling : l'échantillonnage star degree (ou en étoile basé sur les degrés) est une stratégie en étoile dans laquelle les probabilités d'échantillonner les noeuds {ρ 1 , . . . , ρ n } dépendent des degrés des noeuds de la façon suivante :</p><formula xml:id="formula_44">(V i ) i | (D i ) ∼ ind B(logistic(a + bD i )) où ψ := (a, b) ∈ R 2 , D i = j Y ij .</formula><p>• Block-node sampling : block-node sampling (en étoile basé sur les groupes) est aussi un échantillonnage en étoile dans lequel la probabilité d'observer un noeud dépend du groupe auquel il appartient. On a donc</p><formula xml:id="formula_45">(V i ) i | (Z i ) ∼ ind ρ Zi où ψ := (ρ 1 , ..., ρ Q ) ∈ [0, 1] Q .</formula><p>Les échantillonnages marqués par une étoile sont MAR (et même MCAR) si les probabilités de sélectionner les noeuds ou les dyades ne dépendent pas de la valeur des dyades ou des groupes auxquels appartiennent les noeuds. Ceux marqués par un rond plein • sont quant à eux NMAR car dépendant de la valeur des dyades : double standard sampling, star degree sampling ; ou des groupes des noeuds : class-dyad sampling, class sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inférence MAR et NMAR dans le SBM</head><p>Inférence dans le cas MAR.</p><p>L'inférence du SBM dans le cas ou les données manquantes sont MAR (a fortiori MCAR) est identique à celle du SBM sans données manquantes, à ceci près que l'on ne considère plus toutes les entrées de la matrice Y mais seulement l'ensemble des dyades observées, noté dans la thèse D o . De plus, le nombre de noeuds observés est composé de l'ensemble des noeuds pour lesquels au moins une dyade est observée : N o := {i : ∃ j ∈ N s.t. R ij = 1}. 25 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONTRIBUTIONS DE LA THÈSE INTRODUCTION</head><p>Inférence dans les cas NMAR.</p><p>Nous ne traiterons que du cas des réseaux binaires. La principale difficulté est de trouver une approximation de loi type « champs moyens » de la loi conditionnelle (Y o , R)|(Y m , Z). <ref type="bibr">Dans Tabouy et al. (2019a)</ref>, nous avons proposé l'approximation suivante : </p><formula xml:id="formula_46">Q(Z, Y o ) = i∈N M(Z i , τ i ) (i,j)∈D m B(Y ij ; ν ij ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Garanties théoriques</head><p>Identifiabilité.</p><p>Nous avons montré que les paramètres du SBM ainsi que ceux des échantillonnages MCAR random dyad sampling et star sampling ainsi que de l'échantillonnage NMAR class sampling sont identifiables. Dans le cas des échantillonnages MCAR, comme il y a indépendance entre R et Y, l'identifiabilité se montre en deux temps. Dans un premier temps on montre que les paramètres ψ sont identifiables puis que θ l'est aussi.</p><p>Proposition 2. Le paramètre d'échantillonnage ρ &gt; 0 du random-dyad (resp. star) sampling est identifiable par rapport à la distribution de l'échantillonnage.</p><p>Théorème 2. Soit n ≥ 2Q. Supposons que pour tout 1 ≤ q ≤ Q, ρ &gt; 0 les coordonnées de πα sont deux à deux différentes. Alors, sous le random-dyad (resp. star) sampling, les paramètres du SBM sont identifiables par rapport à la distribution de la partie observée du SBM, à la permutation des labels près.</p><p>Le théorème suivant établit l'identifiabilité du SBM échantillonné suivant le class sampling. À la différence du cas MCAR, on ne peut pas prouver l'identifiabilité de ψ et de θ séparément, cela doit être fait conjointement à cause de la dépendance entre R et Y.</p><formula xml:id="formula_47">Théorème 3. Soit n ≥ 2Q et supposons que pour tout 1 ≤ q ≤ Q, ρ q &gt; 0, α q &gt; 0 les coordonnées de o = πα et de t = ( Q k=1 π 1k ρ k α k , . . . , Q k=1 π Qk ρ k α k ) sont deux à deux différentes.</formula><p>Alors, sous le class sampling, les paramètres du SBM sont identifiables par rapport à la distribution du SBM, à la permutation des labels près. 26 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 3. CONTRIBUTIONS DE LA THÈSE Les résultats d'identifiabilité associés aux échantillonnages double standard sampling et star degree sampling ne sont pas encore prouvés.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistance et normalité asymptotique.</head><p>Suivant les travaux <ref type="bibr">de Celisse et al. (2012)</ref>, <ref type="bibr" target="#b12">Bickel et al. (2013)</ref> et <ref type="bibr">de Brault et al. (2017)</ref>, nous avons montré que dans le cadre du SBM avec une loi d'émission appartenant à la famille exponentielle à un paramètre et sous les conditions d'échantillonnage du random dyad sampling, les estimateurs du maximum de vraisemblance et les estimateurs issus de l'approximation variationnelle du SBM sont consistants et asymptotiquement normaux. De plus, les variances asymptotiques de ces estimateurs sont identiques et explicites. Finalement, nous avons aussi montré que le SBM est identifiable pour toute loi d'émission des dyades appartenant à la famille exponentielle à un paramètre en présence de données manquantes générées par un échantillonnage random-dyad sampling. Ces résultats sont développés dans le chapitre 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prise en compte de covariables</head><p>Dans la section 2.1, nous avons vu qu'il était possible d'intégrer la connaissance de covariables sur les noeuds ou sur les dyades dans la modélisation du SBM. Cette inclusion des covariables dans la loi d'émission des dyades conditionnellement aux variables latentes est souvent source d'ambiguïté. Il s'agit non pas de prendre en compte l'effet des covariables sur la structure du graphe, mais bien de l'enlever.</p><p>Le but de notre travail est d'inclure dans le SBM la connaissance de covariables, ainsi que de définir des stratégies d'échantillonnage de graphes dépendants des covariables. Nous montrons qu'un SBM binaire avec données manquantes NMAR peut être équivalent à un SBM prenant en compte les covariables avec données manquantes MCAR. Ce travail est motivé par le fait que l'on n'a pas toujours accès aux covariables pendant l'échantillonnage ou dans les données.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Données manquantes, SBM et covariables.</head><p>Nous avons étudié trois modèles de SBM avec covariables. Dans le premier, les variables latentes dépendent directement des covariables, dans le second la loi des dyades dépend directement des covariables, finalement le troisième est la combinaison du modèle 1 et 2. L'échantillonnage quant à lui dépend directement des covariables. Les liens de dépendance conditionnelle entre les variables aléatoires Y, Z et R associées à ces modèles sont décrits dans la figure <ref type="figure" target="#fig_1">1</ref></p><formula xml:id="formula_48">.3. X Z Y R X Z Y R X Z Y R</formula><p>(modèle 1) (modèle 2) (modèle 3) Les modèles 1 et 2 sont définis ainsi : 27 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction</p><formula xml:id="formula_49">3. CONTRIBUTIONS DE LA THÈSE INTRODUCTION Modèle 1. Soit α • = (α •1 , ..., α •Q ) où α •q ∈ [0, 1] Q pour tout q ∈ Q, on définit α iq = e β T q Xi1 {q =Q} 1 + Q-1 k=1 β T k X i , ∀(i, q) ∈ N × Q, avec β q ∈ R N pour tout q ∈ 1, Q -1 and β Q = 0. De plus nous avons (Z i ) i | (X i ) ∼ iid M(1, α i ), (Y ij ) i,j | (Z i ), (Z j ) ∼ ind B(π ZiZj ), avec π ZiZj ∈ [0, 1] pour tout (i, j) ∈ N 2 .</formula><p>Modèle 2.</p><formula xml:id="formula_50">(Z i ) i ∼ iid M(1, α), (Y ij ) i,j | (Z i ), (Z j ), (X i ), (X j ) ∼ ind B(g(γ zizj + β T φ(X i , X j ))), où γ q ∈ R, β ∈ R m , α ∈ [0, 1] Q , g(x) = (1 + e -x ) -1 et φ(•, •) : R N × R N → R m est une</formula><p>fonction symétrique (i.e. φ(x, y) = φ(y, x)) mesurant la proximité entre deux vecteurs.</p><p>Le modèle suivant est la combinaison des deux modèles précédents.</p><formula xml:id="formula_51">Modèle 3. Soit α • = (α •1 , ..., α •Q ) ∈ [0, 1] Q alors α iq = e β t q Xi1 {q =Q} 1 + Q-1 k=1 β t k X i , ∀(i, q) ∈ N × Q, avec β q ∈ R N pour tout q ∈ 1, Q -1 and β Q = 0. Alors Z i | X i ∼ iid M(1, α i ), ∀i ∈ N , Y ij | Z i , Z j , X ij ∼ ind B(g(γ zizj + β t X ij )), ∀(i, j) ∈ N 2 ,</formula><p>les paramètres et les notations sont identiques au modèle 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stratégies d'échantillonnage.</head><p>Comme dans la section 3.2 pour le SBM, nous définissons deux stratégies d'échantillonnage (une dyade-centrée et une autre noeud-centrée), pour lesquelles la probabilité d'échantillonner un noeud ou une dyade dépend de la valeur des covariables.</p><p>• Dyad covariates sampling :</p><formula xml:id="formula_52">soient α ∈ R, κ ∈ R p et ψ(•, •) : R N × R N → R p . On a donc (R ij ) i,j | (X i ), (X j ) ∼ iid B(g(α + κ T ψ(X i , X j ))).</formula><p>• Node covariates sampling : soient ν ∈ R et η ∈ R N . La probabilité d'observer toutes les dyades associées à un noeud est donnée par</p><formula xml:id="formula_53">(V i ) i | (X i ) ∼ iid B(g(ν + η T X i )).</formula><p>Considérant les modèles 1 et 2 décrit ci-dessus et conditionnellement à X, les stratégies dyad covariates sampling et node covariates sampling sont MCAR. En particulier, d'après la proposition 1, l'inférence des paramètres des modèles 1 et 2 avec des données manquantes produites suivant ces stratégies d'échantillonnage se fait sur la partie observée des données. 28 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q INTRODUCTION 3. CONTRIBUTIONS DE LA THÈSE Cas d'équivalence de modèles.</p><p>La proposition suivante illustre l'article <ref type="bibr" target="#b93">Molenberghs et al. (2008)</ref>. En effet, nous montrons qu'un modèle de SBM avec covariables et données manquantes MCAR peut dans certains cas être équivalent à un SBM sans covariables et données manquantes NMAR.</p><p>Proposition 3. Soit X ∈ M 1×n (Q), on modélise des covariables et des variables latentes Z de la façon suivante :</p><formula xml:id="formula_54">X i ∼ iid M(1, ν), ∀i ∈ N , P(Z ia = 1|X ia = 1) = δ et P(Z ib = 1|X ia = 1) = 1 -δ Q -1 , b = a, ∀i ∈ N .</formula><p>De plus, on définit la probabilité d'observer une dyade comme suit :</p><formula xml:id="formula_55">p q = P(R ij = 1|X iq = 1, X j = 1).</formula><p>On utilise enfin le modèle 1 pour la loi des dyades conditionnellement à X et Z.</p><p>Alors ce modèle est équivalent à un SBM binaire échantillonné avec une stratégie block-dyad sampling dont les paramètres d'échantillonnage seraient les suivants :</p><formula xml:id="formula_56">ρ q = P(R ij = 1|Z iq = 1, Z j = 1), = δ 2 p q ν q ν + 1-δ Q-1 2 a =q b = p ab ν a ν b + 2δ 1-δ Q-1 a =q b= p ab ν a ν b (δν q + 1-δ Q-1 c =q ν c )(δν + 1-δ Q-1 c = ν c )</formula><p>.</p><p>Ce résultat théorique est illustré par des simulations dans le chapitre 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Package R : missSBM</head><p>Une partie des développements méthodologiques de la thèse ont été implémentés dans le package R missSBM <ref type="bibr">(Tabouy et al., 2019c)</ref> disponible sur le CRAN (<ref type="url" target="https://CRAN.R-project.org/package=missSBM">https://CRAN. R-project.org/package=missSBM</ref>). Il contient plusieurs fonctions permettant (i) de simuler un SBM binaire avec ou sans covariables (simulate), (ii) d'échantillonner un réseau selon les stratégies décrites dans la section 3.2 (sampling) et (iii) d'inférer les paramètres d'un SBM binaire avec ou sans covariables, de prédire les groupes des noeuds avec des données manquantes ou pas en prenant en compte la stratégie d'échantillonnage employée (estimate). L'inférence est conduite avec l'algorithme VEM décrit dans la section 2.3. Les détails du package, de sa structure et de son utilisation sont développés au chapitre 5. 29 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Introduction 3. CONTRIBUTIONS DE LA THÈSE INTRODUCTION 30 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational inference of Stochastic Block Model from sampled data</head><p>This chapter has been published as an article in the Journal of the American Statistical Association <ref type="bibr">(Tabouy et al., 2019a)</ref>. This paper deals with non-observed dyads during the sampling of a network and consecutive issues in the inference of the Stochastic Block Model (SBM). We review sampling designs and recover Missing At Random (MAR) and Not Missing At Random (NMAR) conditions for the SBM. We introduce variants of the variational EM algorithm for inferring the SBM under various sampling designs (MAR and NMAR) all available as an R package on the CRAN (see chapter 5 for more details). Model selection criteria based on Integrated Classification Likelihood are derived for selecting both the number of blocks and the sampling design. We investigate the accuracy and the range of applicability of these algorithms with simulations. We explore two real-world networks from ethnology (seed circulation network) and biology (protein-protein interaction network), where the interpretations considerably depends on the sampling designs considered.</p><p>Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION 1 Introduction</head><p>Networks arise in many fields of application for providing an intuitive way to represent interactions between entities. In this paper, a network is composed by a fixed set of nodes, and an interaction between a pair of nodes (dyad) is called an edge. We consider undirected binary networks with no loop, which can be represented by symmetric adjacency matrices filled with zeros and ones.</p><p>Various statistical models exist for depicting the probability distribution of the adjacency matrix (see, e.g. <ref type="bibr" target="#b51">Goldenberg et al., 2010 and</ref><ref type="bibr" target="#b110">Snijders, 2011</ref>, for a survey). A highly desirable feature is their capability to describe the heterogeneity of real-world networks. In this perspective, the family of models endowed with a latent structure (reviewed in <ref type="bibr">Matias and Robin, 2014)</ref> offers a natural way to introduce heterogeneity. Within this family the Stochastic Block Model (in short SBM, see <ref type="bibr">Frank and</ref><ref type="bibr" target="#b44">Harary, 1982 and</ref><ref type="bibr" target="#b60">Holland et al., 1983)</ref> describes a broad variety of network topologies by positing a latent structure (or a clustering) on the nodes, then making the probability distribution of the adjacency matrix dependent on this latent structure. In order to estimate SBMs, Bayesian approaches were first developed <ref type="bibr">(Snijders and</ref><ref type="bibr" target="#b111">Nowicki, 1997 and</ref><ref type="bibr">Nowicki and</ref><ref type="bibr" target="#b96">Snijders, 2001)</ref> prior to variational approaches <ref type="bibr" target="#b36">(Daudin et al., 2008 and</ref><ref type="bibr" target="#b76">Latouche et al., 2012)</ref>. On the theoretical side, <ref type="bibr" target="#b24">Celisse et al. (2012)</ref> study the conditions for identifiability and the consistency of the variational estimators; <ref type="bibr" target="#b12">Bickel et al. (2013)</ref> prove their asymptotic normality. Several generalizations are possible such as weighted or directed variants <ref type="bibr" target="#b88">(Mariadassou et al., 2010)</ref>, mixed-membership and overlapping SBM <ref type="bibr" target="#b1">(Airoldi et al., 2008 and</ref><ref type="bibr" target="#b75">Latouche et al., 2011)</ref>, degree-corrected SBM <ref type="bibr" target="#b68">(Karrer and Newman, 2011)</ref>, dynamic SBM <ref type="bibr" target="#b89">(Matias and Miele, 2016)</ref>, or multiplex SBM <ref type="bibr" target="#b9">(Barbillon et al., 2015)</ref>.</p><p>This paper deals with inference in the SBM when the network is not fully observed. We consider cases where all the nodes are observed but information regarding the presence/absence of an edge is missing for some dyads. In other words the adjacency matrix contains missing values, a situation often met with real-world networks. For instance in social sciences, network data consists in interactions between individuals: the set of individuals is fixed, possibly known from a census. Information about the presence/absence of an edge is only available when at least one of the two individuals is available for an interview, otherwise it is missing. See <ref type="bibr" target="#b119">Thompson and Frank (2000)</ref>, <ref type="bibr" target="#b120">Thompson and Seber (1996)</ref>, <ref type="bibr" target="#b70">Kolaczyk (2009)</ref> and <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref> for a review of network sampling techniques. Even though some papers deal with SBM inference under missing data condition <ref type="bibr" target="#b0">(Aicher et al., 2014 and</ref><ref type="bibr" target="#b122">Vinayak et al., 2014)</ref>, the sampling mechanism responsible for the missing values is overlooked in the inference, contrary to the approach developed in our paper.</p><p>Our contributions. A typology of sampling designs is introduced in Section 2.2. We adapt the theory developed in <ref type="bibr" target="#b105">Rubin (1976)</ref> and <ref type="bibr" target="#b81">Little and Rubin (2014)</ref> to the SBM by splitting the sampling designs into the three usual classes of missing data: i) Missing Completely At Random (MCAR), where the sampling does not depend on the data, neither on the observed nor on the unobserved part of the network.</p><p>ii) Missing At Random (MAR), where the probability of being sampled is independent on the value of the missing data. For network data, the sampling does not depend on the presence/absence of an edge of an unobserved (or missing) dyad. MCAR is a particular case of MAR.</p><p>iii) Not Missing At Random (NMAR), where the sampling scheme is guided by unobserved dyads in some way.</p><p>Section 2.3 introduces several examples of sampling designs (MAR and NMAR) for which we derive conditions for identifiability of the SBM parameters. Estimation of the SBM in the MAR cases can be handled with the Variational EM (VEM) of <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> by conducting the inference only on the observed part of the 32 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 2. STATISTICAL FRAMEWORK network (Section 3.1). NMAR is more difficult to deal with as the sampling design must be taken into account in the inference. We introduce in Section 3.2 a general variational algorithm <ref type="bibr" target="#b67">(Jordan et al., 1998)</ref> to deal with NMAR cases when the sampling design relies on a probability distribution which is explicitly known 1 . Our variational approach is based on a double mean-field approximation applied to the latent distribution of the clustering and to the distribution of the missing dyads. We implement VEM algorithms that produce unbiased estimators for three natural NMAR sampling designs: a dyad-centered strategy, a node-centered strategy, and a block-centered strategy. We also derive an Integrated Classification Likelihood criterion (ICL, <ref type="bibr" target="#b13">Biernacki et al., 2000)</ref> for selecting the number of blocks.</p><p>Although it is not possible to distinguish whether the sampling is MAR or NMAR <ref type="bibr" target="#b93">(Molenberghs et al., 2008)</ref>, the ICL can also be used to select which sampling design is the best fit for the data.</p><p>In Section 4.2 we show the good performance of our VEM algorithms on simulations for both MAR (Section 4.1) and NMAR conditions. Finally we investigate two very different real-world networks with missing values, namely a Kenyan seed exchange network (Section 5.1), and a protein-protein interaction (PPI) network (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related works.</head><p>In the few papers dealing with missing data for networks, the sampling design is rarely discussed. Even if not explicitly stated they all assume MAR conditions. <ref type="bibr" target="#b0">Aicher et al. (2014)</ref> propose a weighted SBM modeling simultaneously the presence/absence of an edge and its weight. Missing data are handled by dropping the corresponding terms in the likelihood and the inference is conducted by a variational algorithm. In <ref type="bibr" target="#b123">Vincent and Thompson (2015)</ref> a Bayesian augmentation procedure is introduced to estimate simultaneously the size of the population and the clustering when the sampling design is a one-wave snowball. Apart from the SBM, the exponential random graph model has been studied in the MAR setting in <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref>.</p><p>The matrix completion literature brings additional insights since SBM inference can be seen as a low-rank matrix estimation. <ref type="bibr" target="#b122">Vinayak et al. (2014)</ref> introduce a convex program for the matrix completion problem where the underlying matrix has a simple affiliation structure defined via an SBM. The entries are sampled independently with the same probability, corresponding to a MAR case. In <ref type="bibr" target="#b37">Davenport et al. (2014)</ref> the case of noisy 1-bit observations is studied and a likelihood-based strategy is developed with theoretical justifications ensuring good matrix completion. <ref type="bibr" target="#b27">Chatterjee (2015)</ref> proves strong results for large matrices with noisy entries estimation, by means of a universal singular value thresholding.</p><p>Another related question is when the status of some dyads (absence/presence) is not clear in errorfully observed graph. Such uncertainties can be taken into account <ref type="bibr" target="#b100">(Priebe et al., 2015 and</ref><ref type="bibr" target="#b6">Balachandran et al., 2017)</ref>. The latter reference studies the error propagation made by using estimators computed on observed sub-graphs, in order to estimate the number of existing edges in the real underlying graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stochastic Block Model</head><p>In an SBM, nodes from a set N {1, . . . , n} are distributed among a set Q {1, . . . , Q} of hidden blocks that model the latent structure of the graph. The blocks are described by the latent random vectors</p><formula xml:id="formula_57">Z i q = (Z i1 , . . . , Z iQ ) i∈N with multinomial distribution M(1, α = (α 1 , . . . , α Q ))</formula><p>. The probability of an edge between any dyad in D N × N only depends on the blocks the two nodes belong to. Hence, the presence of an edge between i and j, indicated by the binary variable Y ij , is independent on the other edges conditionally on the 1 More complex sampling schemes -for instance adversarial strategies -are thus not handled 33 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 2. STATISTICAL FRAMEWORK latent blocks:</p><formula xml:id="formula_58">Y ij | Z iq = 1, Z j = 1 ∼ ind B(π q ), ∀(i, j) ∈ D, ∀(q, ) ∈ Q × Q,</formula><p>where B stands for the Bernoulli distribution. In the following, π = (π q ) (q, )∈Q×Q is the</p><formula xml:id="formula_59">Q × Q matrix of connectivity probabilities, Y = (Y ij ) (i,j</formula><p>)∈D is the n × n adjacency matrix of the random graph, Z = (Z iq ) i∈N ,q∈Q is the n × Q matrix of the latent blocks and θ = (α, π) are the unknown parameters. In the undirected binary case, Y ij = Y ji for all (i, j) ∈ D and Y ii = 0 for all i ∈ N . Similarly, π q = π q for all (q, ) ∈ Q × Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sampled data in the SBM framework</head><p>The sampled data is an n×n matrix with entries in {0, 1, NA}. It corresponds to the adjacency matrix Y where unobserved dyads have been replaced by NA's. More formally, let R be the n × n sampling matrix recording the data sampled during this process, such that</p><formula xml:id="formula_60">R ij = 1 if Y ij is observed and 0 otherwise; also define D o = {(i, j) : R ij = 1}, D m = {(i, j) : R ij = 0}, Y o = {Y ij : (i, j) ∈ D o } and Y m = {Y ij : (i, j) ∈ D m }</formula><p>to denote the sets of variables respectively associated with the observed and missing data. The number of nodes n is assumed to be known. The sampling design is the description of the stochastic process that generates R. It is assumed that the network exists before the sampling design acts upon it. Moreover, the sampling design is fully characterized by the conditional distribution p ψ (R|Y), the parameters of which are such that ψ and θ live in a product space Θ × Ψ.</p><p>Hence the joint probability density function of the observed data satisfies</p><formula xml:id="formula_61">p θ,ψ (Y, R) = p θ (Y o , Y m , Z)p ψ (R|Y o , Y m , Z)dY m dZ. (2.1)</formula><p>Simplifications may occur in (2.1) depending on the sampling design, leading to the three usual types of missingness (MCAR, MAR and NMAR). This typology depends on the relations between the adjacency matrix Y, the latent structure Z and the sampling R, so that the missingness is characterized by four directed acyclic graphs displayed in Figure <ref type="figure" target="#fig_8">2</ref>.1. On the basis of these DAGs, the sampling design is Proof.</p><formula xml:id="formula_62">Z Y R Z Y R (a) (b) Z Y R Z Y R (c) (d)</formula><formula xml:id="formula_63">MCAR if R |= (Y m , Z, Y o ), MAR if R |= (Y m , Z) | Y o ,</formula><formula xml:id="formula_64">To prove i), if R satisfies MAR conditions, then p ψ (R|Y o , Y m , Z) = p ψ (R|Y o ).</formula><p>Moreover, θ and ψ lie in a product space so that (2.1)</p><formula xml:id="formula_65">factorizes into p θ,ψ (Y o , R) = p θ (Y o )p ψ (R|Y o</formula><p>). This corresponds to the ignorability condition of <ref type="bibr" target="#b105">Rubin (1976)</ref> and <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref>. The proof of ii) is postponed to the supplementary materials. 34 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 2. STATISTICAL FRAMEWORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sampling design examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAR examples</head><p>Definition 1 (Random-dyad sampling). Each dyad (i, j) ∈ D has the same probability P(R ij = 1) = ρ to be observed independently of the others.</p><p>This design is trivially MCAR because each dyad is sampled with the same probability ρ which does not depend on Y.</p><p>Definition 2 (Star and snowball sampling). The star sampling consists in selecting uniformly a set of nodes, then observing corresponding rows of matrix Y. Snowball sampling is initialized by a star sampling which gives a first "wave" of nodes. The second wave is composed by the neighbors of the first. Successive waves can then be obtained. The final set of observed dyads corresponds to all dyads involving at least one of these nodes.</p><p>These two designs are node-centered and MAR. Indeed, selecting nodes independently in star sampling or in the first wave of snowball sampling corresponds to MCAR sampling. Successive waves are then MAR since they are built on the basis of the previously observed part of Y. Expressions of the corresponding distributions p ψ (R|Y o ) are given in <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref>.</p><p>Identifiability of random-dyad and star sampling designs. Since random-dyad and star samplings are MCAR, the identifiability is assessed in two steps by proving the identifiability of, first, the sampling parameter ψ = ρ and second, the SBM parameters θ = (α, π) given ρ. Our proofs, postponed to the supplementary materials, follow <ref type="bibr" target="#b24">Celisse et al. (2012)</ref> who established the identifiability of the SBM without missing data.</p><p>Proposition 5. The sampling parameter ρ &gt; 0 of random-dyad (resp. star) sampling is identifiable w.r.t. the sampling distribution.</p><p>Theorem 1. Let n ≥ 2Q and assume that for any 1 ≤ q ≤ Q, ρ &gt; 0, α q &gt; 0 and that the coordinates of πα are pairwise distinct. Then, under random-dyad (resp. star) sampling, SBM parameters are identifiable w.r.t. the distribution of the observed part of the SBM up to label switching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMAR examples</head><p>Definition 3 (Double standard sampling). Let ρ 1 , ρ 0 ∈ [0, 1]. Double standard sampling consists in observing dyads with probabilities</p><formula xml:id="formula_66">P(R ij = 1|Y ij = 1) = ρ 1 , P(R ij = 1|Y ij = 0) = ρ 0 . (2.2) Denote S o = (i,j)∈D o Y ij , So = (i,j)∈D o (1 -Y ij )</formula><p>and similarly for S m , Sm . In this dyad-centered sampling design satisfying DAG (b), the log-likelihood is</p><formula xml:id="formula_67">log p ψ (R|Y ) = S o log ρ 1 + So log ρ 0 + S m log(1 -ρ 1 ) + Sm log(1 -ρ 0 ), with ψ = (ρ 0 , ρ 1 ).</formula><p>(2.3) Definition 4 (Star sampling based on degrees -Star degree sampling). Star degree sampling consists in observing all dyads corresponding to nodes selected with probabilities {ρ 1 , . . . , ρ n } such that</p><formula xml:id="formula_68">ρ i = logistic(a+bD i ) for all i ∈ N where (a, b) ∈ R 2 , D i = j Y ij and logistic(x) = (1 + e -x ) -1 .</formula><p>In this node-centered sampling design satisfying DAG (b), the log-likelihood is</p><formula xml:id="formula_69">log p ψ (R|Y ) = i∈N o log ρ i + i∈N m log(1 -ρ i ), with ψ = (a, b).</formula><p>(2.4) 35 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VARIATIONAL INFERENCE</head><p>Definition 5 (Class sampling). Class sampling consists in observing all dyads corresponding to nodes selected with probabilities {ρ</p><formula xml:id="formula_70">1 , . . . , ρ Q } such that ρ q = P(i ∈ N o | Z iq = 1) for all (i, q) ∈ N × Q.</formula><p>In this node-centered sampling design satisfying DAG (d), the log-likelihood is</p><formula xml:id="formula_71">log p ψ (R|Z) = i∈N o q∈Q Z iq log ρ q + i∈N m q∈Q Z iq log(1 -ρ q ), with ψ = (ρ 1 , . . . , ρ Q ).</formula><p>(2.5)</p><p>Identifiability of class sampling. Theorem 2 establishes the identifiability of the SBM sampled under NMAR class sampling design (see the supplementary materials for the proof).</p><p>Note that the identifiability of the sampling parameters ψ = (ρ 1 , . . . , ρ Q ) and of the SBM parameters must be proved jointly because of the dependence between the network and the sampling. It is worth mentioning that both α q and ρ q are identifiable and not only their product. Although somewhat counter-intuitive, this fact is supported by the inference algorithm for class sampling in Section 3.3, which weights the recovery of the latent clusters by taking the unbalanced sampling into account.</p><p>Theorem 2. Let n ≥ 2Q and assume that for any 1 ≤ q ≤ Q, ρ q &gt; 0, α q &gt; 0, and that the coordinates of o = πα and t = (</p><formula xml:id="formula_72">Q k=1 π 1k ρ k α k , . . . , Q k=1 π Qk ρ k α k ) are pairwise distinct.</formula><p>Then, under class sampling, SBM and class sampling parameters are identifiable w.r.t. the distributions of the SBM and the sampling up to label switching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Inference</head><p>Derivations of the practical variational algorithms considerably change depending on the missing data condition at play. We start by MAR to gently introduce the variational principle for SBM, then develop algorithms in a series of NMAR conditions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MAR inference</head><p>By Proposition 4 part (i), inference in the MAR case is conducted on Y o . The EM algorithm is unfeasible since it requires the evaluation of the conditional mean of the complete log-likelihood</p><formula xml:id="formula_73">E Z|Y o [log p θ (Y o , Z)]</formula><p>which is intractable when Y comes from an SBM. The variational approach circumvents this limitation by maximizing a lower bound of the loglikelihood based on an approximation pτ of the true conditional distribution p θ (Z|Y o ),</p><formula xml:id="formula_74">log p θ (Y o ) ≥ J τ,θ (Y o ) log(p θ (Y o )) -KL[p τ (Z)||p θ (Z|Y o )], = E pτ [log(p θ (Y o , Z))] -E pτ [log pτ (Z)],</formula><p>(2.6)</p><p>where τ are some variational parameters and KL is the Kullback-Leibler divergence. The approximated distribution is chosen so that the integration over the latent variables simplifies by factorization. Recall from Section 2.1 that the latent vectors Z i q = (Z i1 , . . . , Z iQ ) i∈N are independent with a multinomial prior distribution. Thus, in order to factorize the likelihood in a convenient way, a natural variational counterpart to</p><formula xml:id="formula_75">p θ (Z|Y o ) is pτ (Z) = i∈N m(Z i q; τ i ),</formula><p>where τ i = (τ i1 , . . . , τ iQ ), and m(•; τ i ) is the multinomial probability density function with parameters τ i . The VEM sketched in Algorithm 1 consists in alternatively maximizing J w.r.t. τ = {τ 1 , . . . , τ n } (the variational E-step) and w.r.t. θ (the M-step). The two maximization problems are solved straightforwardly following <ref type="bibr" target="#b36">Daudin et al. (2008)</ref>:</p><formula xml:id="formula_76">1. The parameters θ = (α, π) maximizing J θ(Y o ) when τ is held fixed are αq = i∈N o τiq card (N o ) , πq = (i,j)∈D o τiq τj Y ij (i,j)∈D o τiq τj</formula><p>. 36 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VARIATIONAL INFERENCE</head><p>2. The variational parameters τ maximizing J τ (Y o ) when θ is held fixed are obtained with the following fixed point relation:</p><formula xml:id="formula_77">τiq ∝ α q   (i,j)∈D o ∈Q b(Y ij ; π q ) τj   ,</formula><p>where b(x, π) = π x (1 -π) 1-x the Bernoulli probability density function.</p><p>Algorithm 1: Variational EM for MAR inference in SBM Initialization: Set up τ (0) with some clustering algorithm repeat</p><formula xml:id="formula_78">θ (h+1) = arg max θ J Y o ; τ (h) , θ M-step τ (h+1) = arg max τ J Y o ; τ , θ (h+1) variational E-step until θ (h+1) -θ (h) &lt; ε</formula><p>Algorithm 1 generates a sequence {τ (h) , θ (h) ; h 0} with increasing J(Y o ; τ (h) , θ (h) ). Since there is no guarantee for convergence to the global maximum, we run the algorithm from several different initializations to finally retain the best solution.</p><p>Model selection of the number of blocks. The Integrated Classification Likelihood (ICL) criterion of <ref type="bibr" target="#b13">Biernacki et al. (2000)</ref> is relevant for latent variable models where the likelihood -and thus BIC -is intractable. <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> derive a variational ICL for the SBM which we adapt to missing data conditions:</p><formula xml:id="formula_79">if θ = arg max log p θ (Y o , Z) then ICL(Q) = -2E pτ log p θ (Y o , Z; Q) + Q(Q + 1) 2 log card (D o ) + (Q -1) log card (N o ) .</formula><p>Note that each dyad is only counted once since we work with symmetric networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NMAR inference: the general case</head><p>In contrast to the MAR case, conducting inference on the observed dyads only may bias the estimates in the NMAR case. In fact, all observed data (including the sampling matrix R in addition to Y o ) must be taken into account. The likelihood of the observed data is thus log p θ,ψ (Y o , R) and the corresponding completed likelihood has the following decomposition:</p><formula xml:id="formula_80">log p θ,ψ (Y o , R, Y m , Z) = log p ψ (R|Y o , Y m , Z) + log p θ (Y o , Y m , Z), (2.7)</formula><p>where an explicit form of p ψ (R|Y o , Y m , Z) requires further specification of the sampling. The joint distribution p θ (Y o , Y m , Z) has a form similar to the MAR case. Now, the approximation is required both on latent blocks Z and missing dyads Y m to approximate</p><formula xml:id="formula_81">p θ (Z, Y m |Y o ).</formula><p>We suggest a variational distribution where complete independence is forced on Z and Y m , using a multinomial (resp. Bernoulli ) distribution for Z (resp. for Y m ):</p><formula xml:id="formula_82">pτ,ν (Z, Y m ) = pτ (Z) pν (Y m ) = i∈N m(Z i• ; τ i ) (i,j)∈D m b(Y ij ; ν ij ), (2.8)</formula><p>where τ and ν = {ν ij , (i, j) ∈ D m } are two sets of variational parameters respectively associated with Z and Y m . This leads to the following lower bound for log p θ,ψ (Y o , R):</p><formula xml:id="formula_83">J τ ,ν,θ,ψ (Y o , R) = E pτ,ν [log p θ,ψ (Y o , R, Y m , Z)] -E pτ,ν [log pτ,ν (Z, Y m )] .</formula><p>37 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VARIATIONAL INFERENCE</head><p>By means of Decomposition (2.7) of the completed log-likelihood, variational approximation (2.8) and entropies of multinomial and Bernoulli distributions, one has</p><formula xml:id="formula_84">J τ ,ν,θ,ψ (Y o , R) = E pτ,ν [log p ψ (R|Y o , Y m , Z)] + (i,j)∈D o (q, )∈Q 2 τ iq τ j log b(Y ij , π q ) + (i,j)∈D m (q, )∈Q 2 τ iq τ j log b(ν ij , π q ) + i∈N q∈Q τ iq log(α q /τ iq ) - (i,j)∈D m ν ij log(ν ij ) + (1 -ν ij ) log(1 -ν ij ). (2.9) In (2.9), E pτ,ν [log p ψ (R|Y o , Y m , Z)</formula><p>] can be integrated over the variational distribution pτ,ν (Z, Y m ), as expected. The practical computations depend on the sampling design.</p><p>The general VEM algorithm used to maximize (2.9) is sketched in its main lines in Algorithm 2. Both the E-step and the M-step split into two parts: the maximization must be performed on the SBM parameters θ and the sampling design parameters ψ respectively. The variational E-step is performed on the parameters τ of the latent block Z and on the parameters ν of the missing data Y m .</p><p>Algorithm 2: Variational EM for NMAR inference in SBM Initialisation: set up τ (0) , ν (0) and ψ (0)  repeat (h) , ν (h) , ψ (h) , θ M-step a)</p><formula xml:id="formula_85">θ (h+1) = arg max θ J Y o , R; τ</formula><formula xml:id="formula_86">ψ (h+1) = arg max ψ J Y o , R; τ (h) , ν (h) , ψ, θ (h+1) M-step b) τ (h+1) = arg max τ J Y o , R; τ , ν (h) , ψ (h+1) , θ (h+1) VE-step a) ν (h+1) = arg max ν J Y o , R; τ (h+1) , ν, ψ (h+1) , θ (h+1) VE-step b) until θ (h+1) -θ (h) &lt; ε</formula><p>Interestingly, resolution of the two steps concerned with the optimization of the parameters related with the SBM -that is to say, θ and τ -can be stated almost independently of any further specification of the sampling design. Proposition 6. Consider the lower bound J τ ,ν,θ,ψ (Y o , R) given by (2.9).</p><p>1. The parameters θ = (α, π) maximizing (2.9) when all others are held fixed are</p><formula xml:id="formula_87">αq = 1 n i∈N τiq , πq = (i,j)∈D o τiq τj Y ij + (i,j)∈D m τiq τj νij (i,j)∈D τiq τj .</formula><p>2. The optimal τ in (2.9) when all other parameters are held fixed verifies</p><formula xml:id="formula_88">τiq ∝ λ iq α q   (i,j)∈D o ∈Q b(Y ij ; π q ) τj     (i,j)∈D m ∈Q b(ν ij ; π q ) τj  </formula><p>with λ iq a simple constant depending on the sampling design.</p><p>Proof. These results are simply obtained by differentiation of (2.9).</p><p>The two steps concerned with ψ and ν are specific to the sampling designs used to describe R. Further details are provided below for the designs presented in Section 2.3. 38 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VARIATIONAL INFERENCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NMAR: specificities related to the choice of the sampling</head><p>In light of Figure <ref type="figure" target="#fig_8">2</ref>.1, NMAR conditions specified by DAGs (b), (c) or (d) induce different simplifications for the conditional distribution of the sampling design R:</p><formula xml:id="formula_89">DAG (b) p ψ (R|Y o , Y m , Z) = p ψ (R|Y o , Y m ), DAG (c) p ψ (R|Y o , Y m , Z) = p ψ (R|Y o , Y m , Z), DAG (d) p ψ (R|Y o , Y m , Z) = p ψ (R|Z). This induces different evaluations of E pτ,ν [log p θ,ψ (Y o , R, Y m , Z)]</formula><p>in the lower bound (2.9) for double standard sampling, star degree sampling and class sampling. We obtain below explicit formulas of ψ and ν by differentiation of the corresponding variational lower bounds. The computations are tedious but straightforward and thus eluded in the following.</p><formula xml:id="formula_90">Double-standard sampling. Let s m = (i,j)∈D m ν ij , sm = (i,j)∈D m (1 -ν ij )</formula><p>be the variational counterparts of S m and Sm . From (2.3) we have</p><formula xml:id="formula_91">E p log p ψ (R|Y) = S o log ρ 1 + So log ρ 0 + s m log(1 -ρ 1 ) + sm log(1 -ρ 0 ).</formula><p>Proposition 7 (double standard sampling).</p><p>1. The parameters ψ = (ρ 0 , ρ 1 ) maximizing (2.9) when all others are held fixed are</p><formula xml:id="formula_92">ρ0 = So So + sm , ρ1 = S o S o + s m .</formula><p>(2.10)</p><p>2. The optimal ν in (2.9) when all other parameters are held fixed are</p><formula xml:id="formula_93">νij = logistic   log 1 -ρ 1 1 -ρ 0 + (q, )∈Q 2 τ iq τ j log π q 1 -π q   . Moreover, λ iq = 1 ∀(i, q) ∈ N × Q for optimization of τ in Proposition 6.b).</formula><p>Class sampling. According to (2.5) we have</p><formula xml:id="formula_94">E p log p ψ (R|Y) = i∈N o q∈Q τ iq log(ρ q ) + i∈N m q∈Q τ iq log(1 -ρ q ).</formula><p>Proposition 8 (class sampling).</p><p>1. The parameters ψ = (ρ 1 ...ρ Q ) maximizing (2.9) when all others are held fixed are</p><formula xml:id="formula_95">ρq = i∈N o τ iq i∈N τ iq . (2.11)</formula><p>2. The optimal ν in (2.9) when all other parameters are held fixed verify</p><formula xml:id="formula_96">νij = logistic   (q, )∈Q 2 τ iq τ j log π q 1 -π q   . Moreover λ iq = ρ 1 {i∈N o } q (1 -ρ q )</formula><p>1 {i∈N m } for optimization of τ in Proposition 6.b). 39 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VARIATIONAL INFERENCE</head><p>Star degree sampling. From Expression (2.4) of the likelihood, one has a+bDi) ) has no explicit form, an additional variational approximation is needed <ref type="bibr" target="#b67">(Jordan et al., 1998)</ref>. This technique was recently used in random graph framework <ref type="bibr" target="#b77">(Latouche et al., 2018)</ref>. It relies on the following approximation of the logistic function:</p><formula xml:id="formula_97">E p log p ψ (R|Y) = - i∈N m a + b Di + i∈N E p -log(1 + e -(a+bDi) ) , where Di = E p [D i ] = i∈N m ν ij + i∈N o Y ij is the approximation of the degrees. Because E p -log(1 + e -(</formula><formula xml:id="formula_98">g(x) ≥ g(ζ) + x -ζ 2 + h(ζ)(x 2 -ζ 2 ), h(ζ) = -1 2ζ logistic(ζ) - 1 2 (2.12) for all (x, ζ) ∈ R × R + .</formula><p>This leads to a lower bound of the initial lower bound:</p><formula xml:id="formula_99">log p θ,ψ (Y o , R) ≥ J τ ,ν,θ,ψ (Y o , R) ≥ J τ ,ν,ζ,θ,ψ (Y o , R), (2.13) with ζ = (ζ i , i ∈ N ) such that ζ i &gt; 0 is an additional set of variational parameters used to approximate -log(1 + e -x</formula><p>). The second lower bound J τ ,ν,ζ,θ,ψ is derived from Equation (2.12) and given in the supplementary materials for completeness . At the end of the day, we have an additional set of variational parameters to optimize, and a corresponding additional step in Algorithm 2. Expression of all the parameters specific to star degree sampling by differentiating J τ ,ν,ζ,θ,ψ .</p><p>Proposition 9 (star degree sampling).</p><formula xml:id="formula_100">Let D 2 i = E p D i 2 and D- k = Dk -ν k . 1. The parameters ψ = (a, b) maximizing J τ ,ν,ζ,θ,ψ (Y o , R) when others are held fixed are b = 2 n 2 -card (N m ) i∈N (h(ζ i ) Di ) -1 2 i∈N Di -i∈N m Di × i∈N h(ζ i ) 2 i∈N (h(ζ i ) D 2 i ) × i∈N h(ζ i ) -2 i∈N h(ζ i ) Di 2 , â = - b i∈N h(ζ i ) Di + n 2 -card (N m ) i∈N h(ζ i ) . 2. The parameters ζ maximizing J τ ,ν,ζ,θ,ψ (Y o , R) when others are held fixed are ζi = a 2 + b 2 D 2 i + 2ab Di , ∀i ∈ N . 3. The optimal ν in J τ ,ν,ζ,θ,ψ (Y o , R</formula><p>) when all other parameters are held fixed verify</p><formula xml:id="formula_101">νij = logistic (q, )∈Q 2 τ iq τ j log π q 1 -π q -b + 2h(ζ i ) ab + b 2 (1 + D-j i ) + 2h(ζ j ) ab + b 2 (1 + D-i j ) . (2.14) Moreover, λ iq = 1 ∀(i, q) ∈ N × Q for optimization of τ in Proposition 6.b).</formula><p>Model selection. In NMAR cases, ICL can be useful not only to select the appropriate number of blocks but also for selecting the most appropriate sampling design when it is unknown. Contrary to the MAR case, ICL is no longer a straightforward generalization of <ref type="bibr" target="#b36">Daudin et al. (2008)</ref>. Indeed, the complete likelihood and thus the penalization needs to take into account the sampling design. Let us consider a model with Q blocks and a sampling design with K parameters (i.e. the dimension of ψ). The ICL criterion is a Laplace approximation of the complete likelihood p(Y o , Y m , R, Z|Q, K) with p(θ, ψ|Q, K) the prior distributions on the parameters such that</p><formula xml:id="formula_102">p(Y o , Y m , R, Z|Q, K) = Θ×Ψ p θ,ψ (Y o , Y m , R, Z|Q, K)p(θ, ψ|Q, K)dθdψ.</formula><p>40 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 4. SIMULATION STUDY Proposition 10. For a model with Q blocks, a sampling design with a vector of parameters</p><formula xml:id="formula_103">ψ ∈ R K and ( θ, ψ) = arg max (θ,ψ) log p θ,ψ (Y o , Y m , R, Z), then ICL(Q) = -2E pτ,ν ; θ, ψ log p θ, ψ (Y o , Y m , R, Z|Q, K) + pen ICL (Q), pen ICL =    K + Q(Q+1) 2 log n(n-1) 2 + (Q -1) log(n) for dyad-centered sampling Q(Q+1) 2 log n(n-1) 2 + (K + Q -1) log(n) for node-centered sampling</formula><p>Note that an ICL criterion for MAR sampling designs can be constructed in the same fashion for the purpose of comparison with NMAR sampling designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Simulation study</head><p>In this section, we illustrate the relevance of our approaches on network data simulated under the SBM and sampled under MAR and NMAR conditions. The quality of the inference is assessed by computing the distance between the estimated and the true connectivity matrices π in terms of Frobenius norm. The quality of the clustering recovery is measured with the adjusted Rand index (ARI, <ref type="bibr" target="#b101">Rand, 1971)</ref> between the true classification and the clustering obtained by maximum posterior probabilities for each τ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAR condition</head><p>Algorithm 1 for MAR samplings is tested on affiliation networks with 3 blocks. The number of blocks is assumed to be known. For this topology the probability of connection within a block is η and is ten times stronger than the probability of connections between nodes from different blocks. We generate networks with n = 200 nodes and marginal probabilities of belonging to blocks α = (1/3, 1/3, 1/3). The sampling design is chosen as a random-dyad sampling with a varying ρ. The difficulty is controlled by two parameters: the sampling effort ρ and the overall connectivity in matrix π, defined by c = q α q α π q , which is directly related to the choice of η: the lower the η, the sparser the network and the harder the inference. The simulation is repeated 500 times for each configuration (c, ρ). Figure <ref type="figure" target="#fig_8">2</ref>.2 displays the results in terms of estimation of π and of classification recovery, for varying connectivity c and sampling effort ρ. Our method achieves good performances even with a low sampling effort provided that the connectivity is not too low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NMAR condition</head><p>Under NMAR conditions we conduct an extensive simulation study by considering various network topologies (namely affiliation, star and bipartite), the connectivity matrix of which are given in Figure <ref type="figure" target="#fig_8">2</ref>.3. We use a common tuning parameter to control the connectivity of the networks in each topology: the lower the , the more contrasted the topology.</p><p>Among the three schemes developed in Section 2.3, we investigate thoroughly the double standard sampling, for which we exhibit a large panel of situations where the gap is large between the performances of the algorithms designed for MAR or NMAR cases. Other sampling designs are explored in the supplementary materials.</p><p>Simulated networks have n = 100 nodes, with varying in {0.05, 0.15, 0.25}. Prior probabilities α are chosen specifically for affiliation, star and bipartite topologies, respectively (1/3, 1/3, 1/3), (1/6, 1/3, 1/6, 1/3) and (1/4, 1/4, 1/4, 1/4). The exploration of the sampling parameters ψ = (ρ 0 , ρ 1 ) is done on a grid [0.1, 0.9] × [0.1, 0.9] discretized by steps of 0.1. Algorithm 2 is initialized with several random initializations and spectral clustering.</p><p>In Figure <ref type="figure" target="#fig_8">2</ref>.4, the estimation error is represented as a function of the difference between the sampling design parameters (ρ 0 , ρ 1 ): the closer this difference to zero, the closer to the MAR case. As expected, Algorithm 1 designed for MAR only performs well when 41 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 4. SIMULATION STUDY The adjacency matrix Y is generated under random-dyad sampling strategy for various connectivity c = q α q α π q .</p><formula xml:id="formula_104">π -π F / π F Adjusted Rand Index</formula><p>1 -</p><formula xml:id="formula_105">1 - 1 - (a) affiliation    1 - 1 - 0 0 1 - 0 0 0 1 - 1 - 0 0 1 - 0    (b) star    1 - 1 - 1 - 1 -    (c) bipartite Figure 2.3 -Matrix π in different</formula><p>topologies with inter/intra block probabilities. 42 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 4. SIMULATION STUDY ρ 1 -ρ 0 ≈ 0. Algorithm 2 designed for NMAR double-standard sampling shows relatively flat curves which means that its performances are roughly constant no matter the sampling condition. Model selection. Simulations are also conducted to study the performances of ICL. We compare results for the different topologies described in Figure <ref type="figure" target="#fig_8">2</ref>.3 for = 0.05. Rates of correct answers for selecting the number of blocks Q under a double standard sampling with different sampling rates are displayed in Table <ref type="table" target="#tab_16">2</ref>.1. The ARI is also provided. The ICL shows a satisfactory ability to select the true Q even if the selection task obviously needs a larger sampling effort than the estimation task. It is worth mentioning that a whole block may not be sampled, which leads the ICL to select a lower number of blocks. In such a case the ARI is a meaningful additional information to demonstrate that the clustering remains coherent with the true one. In Table <ref type="table" target="#tab_16">2</ref>.2, results concern the rates of correct selections of the sampling design when the two designs in competition are the random-dyad and the double standard samplings. 43 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 4. SIMULATION STUDY Each configuration is simulated 500 times. 44 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p><formula xml:id="formula_106">|ρ 0 -ρ 0 |/|ρ 0 | |ρ 1 -ρ 1 |/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IMPORTANCE OF ACCOUTING FOR MISSING VALUES IN REAL NETWORKS</head><p>As expected, the rate of correct answers increases with the sampling rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Importance of accouting for missing values in real networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Seed exchange network in the region of Mount Kenya</head><p>In a context of subsistence farming, studies which investigate the relationships between crop genetic diversity and human cultural diversity patterns have shown that seed exchanges are embedded in farmers' social organization. Data on seed exchanges of sorghum in the region of Mount Kenya were collected and analyzed in <ref type="bibr" target="#b73">Labeyrie et al. (2016</ref><ref type="bibr" target="#b72">Labeyrie et al. ( , 2014))</ref>. The sampling is node-centered since the exchanges are documented by interviewing farmers who are asked to declare to whom they gave seeds and from whom they receive seeds. Since an interview is time consuming, the sampling is not exhaustive. A limited space area was defined where all the farmers were interviewed. The network is thus collected with missing dyads since information on the potential links between two farmers who were cited but not interviewed is missing. With the courtesy of Vanesse Labeyrie, we analyzed the Mount Kenya seed exchange network involving 568 farmers among which 155 were interviewed.</p><p>Although other farmers in this region might be connected to non-interviewed farmers, we focus on this closed network of 568 nodes. Since we only know that the sampling is node-centered, we fit SBM under the three node-centered sampling designs presented in Section 2.2 (star (MAR), class and star degree sampling). The ICL criterion is minimal for 10 blocks under the star degree sampling and for 11 blocks under the class degree sampling. The clusterings between the SBMs obtained with either class or star degree sampling remain close from each other (ARI: 0.6) and both unravel a strong community structure. The model selected by ICL for MAR sampling is composed by 11 blocks. The ARIs between MAR clustering and the two other clusterings are lower (around 0.4). Finally, note that interviewed and non-interviewed farmers are mixed up in the blocks of the three selected models. The ICL criteria computed for the three sampling designs are a slightly in favor of the MAR sampling.</p><p>On top of network data, categorical variables are available for discriminating the farmers such as the ntora 2 they belong to (10 main ntoras plus 1 grouping all the others) and the dialect they speak (4 dialects). In Figure <ref type="figure" target="#fig_8">2</ref>.6, we compute ARIs between the ntoras (left panel), the dialects (right panel) and the clusterings obtained with the SBM under the three node-centered sampling designs for a varying number of blocks. Even though the ARIs remain low, the clusterings from class or star degree sampling seem to catch a non negligible fraction of the social organization, larger than the one caught by the clustering 2 The ntora is a small village or a group of neighborhoods 45 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IMPORTANCE OF ACCOUTING FOR MISSING VALUES IN REAL NETWORKS</head><p>from the MAR sampling. These two categorical variables, reflecting some aspects of the social organization, could partially explain the structure of the exchange network.</p><p>ARI q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 0.05 0.10 0.15 4 8 12 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 0.05 0.10 4 8 12 number of blocks sampling q q q Class Degree MAR </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ER (ESR1) Protein-Protein Interaction network in breast cancer</head><p>Estrogen receptor 1 (ESR1) is a gene that encodes an estrogen receptor protein (ER), a central actor in breast cancer. Uncovering its relations with other proteins is essential for a better understanding of the disease. To this end, various bioinformatics tools are available to centralize knowledge about possible relations between proteins into networks known as Protein-Protein Interaction (PPI) networks. The platform string <ref type="bibr" target="#b114">(Szklarczyk et al., 2015)</ref> accessible via <ref type="url" target="http://www.string-db.org">http://www.string-db.org</ref> is one of the most popular tools for this task.</p><p>Given a set of one (or several) initial protein(s) provided by the user, it is possible to recover a valued network between all proteins connected to the initial set. The value of an edge in this network corresponds to a score obtained by aggregating different types of knowledge (wet-lab experiments, textmining, co-expression data, etc. . . ), reflecting a level of confidence. Thus, it is possible for a given protein -we choose ER here -to recover the PPI network between all proteins involved. Our ambition is to rely on a SBM with missing data to finely analyze such networks: we rather describe a dyad as missing (thus not choosing between 0 or 1) if its level of confidence is too low. The PPI network in the neighborhood of ER is composed by 741 proteins connected by edges with values in (0, 1]. We remove ER from this set of proteins, as well as the zinc finger protein 44. Indeed, they were both connected to most of the other proteins and would thus only blur the underlying clustering structure. We denote ω ij the weight associated with dyad (i, j). By means of a tuning parameter γ reflecting the level of confidence, the adjacency matrix is defined as follows:</p><formula xml:id="formula_107">A γ = (A γ ) ij =    1 if ω ij &gt; 1 -γ, NA if γ ≤ ω ij ≤ 1 -γ, 0 if ω ij &lt; γ.</formula><p>(2.15)</p><p>In order to analyze the ER-centered network, Algorithm 1 (random-dyad MAR sampling) and Algorithm 2 (double-standard NMAR sampling) were applied on A γ for γ varying in {.15, .25, .35}, hence taking the uncertainties on the missing dyads into account with various thresholds. The ICL criterion in Figure <ref type="figure" target="#fig_8">2</ref>.7 systematically chooses the NMAR modeling against the MAR modeling, whatever the value of γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>46</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 6. CONCLUSION We study the best MAR and NMAR models associated with γ = 0.35, which value exhibits a clearer choice of the ICL than for γ = {.15, .25} for both MAR and NMAR modelings. The two corresponding SBMs have 11 clusters for MAR sampling and 13 clusters for NMAR sampling. The ARI between the two clusterings is around 0.39: this is mainly due to a large block in the random-dyad MAR clustering which contains much more nodes than any of the blocks in the NMAR clustering. The latter dispatches many of these nodes in four blocks (see the supplementary materials for a more detailed exposition of results). To prove that this finest clustering of the nodes is more relevant from the biological point of view, we propose a validation based on external biological knowledge. To this end, we rely on the Gene Ontology (GO) annotation <ref type="bibr" target="#b5">(Ashburner et al., 2000)</ref> which provides a DAG of ontologies to which genes are annotated if the proteins encoded by these genes are involved in a known biological process. Here, we use GO to perform enrichment analysis (that is to say identifying classes of genes that are over-represented in a large set of genes, via a simple hypergeometric test) on genes corresponding to the proteins present in the large block for MAR, and the corresponding four blocks for NMAR. Interestingly, at a significance level of 1%, we find a single significant biological process for MAR modeling while 13 are found significant in the NMAR case. We check that it is not due to a simple threshold effect by looking at the ranks of the p-values of the 13 NMAR significant processes in the 100 first most significant terms found in the MAR model: only 5 of the NMAR processes are found, with high ranks (24, 33, 39, 56 and 77) far from the smallest MAR p-values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper shows how to deal with missing data on dyads in the SBM. We study MAR and NMAR sampling designs motivated by network data and propose variational approaches to perform inference under this series of designs, accompanied with model selection criteria. Relevance of the method is illustrated on numerical experiments both on simulated and real-world networks. An R-package called missSBM is available on the CRAN (see chapter 5 for more details).</p><p>This work focuses on undirected binary networks. However, it can be adapted to other SBMs, in particular those developed in <ref type="bibr" target="#b88">Mariadassou et al. (2010)</ref> for (un)directed valued networks with a distribution of weights belonging to the exponential family. It could also be adapted to the degree-corrected SBM <ref type="bibr" target="#b68">(Karrer and Newman, 2011)</ref>, where the sampling design would depend on the degree correction parameters. This should lead to a design close to the star degree sampling. In future works, we plan to investigate the consistency of 47 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 6. CONCLUSION the variational estimators of SBM under missing data conditions, looking for similar results as the ones obtained in <ref type="bibr" target="#b12">Bickel et al. (2013)</ref> for fully observed networks. Another path of research is to consider missing data where we cannot distinguish between a missing dyad and the absence of an edge like in <ref type="bibr" target="#b100">Priebe et al. (2015)</ref> and <ref type="bibr" target="#b6">Balachandran et al. (2017)</ref>. 48 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 7. SUPPLEMENTARY 7 Supplementary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Proof of Proposition 4.ii)</head><p>Recall the following result from graphical model theory (see <ref type="bibr">Giraud, 2014, Formula 7</ref>.1).</p><p>Lemma. Let X, Y, W and Z be random variables, then for all h measurable,</p><formula xml:id="formula_108">X |= (Y, W ) | Z ⇒ X |= Y | (h(W ), Z).</formula><p>Applying this lemma with h the identity, function,</p><formula xml:id="formula_109">we get R |= (Y m , Z) | Y o ⇒ R |= Z | (Y o , Y m ) , and then R |= Z | (Y o , Y m ) implies R |= Z | Y.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Identifiability</head><p>When the sampling is node-centered, we denote</p><formula xml:id="formula_110">V i = 1 if node i is observed and V i = 0 otherwise. Proof of Proposition 5. Let ρ, ρ &gt; 0 be such that p ρ (R) = p ρ (R) (resp. p ρ (V) = p ρ (V)). Since R (resp.V) does not depend on Y, then P ρ (R ij = 1) = ρ = ρ = P ρ (R ij = 1) (resp. P ρ (V i = 1) = ρ = ρ = P ρ (V i = 1)).</formula><p>Proof of Theorem 1. Let P o</p><p>[n] denote the probability distribution function of Y o . We show that there exists a unique (α, π)</p><formula xml:id="formula_111">corresponding to P o [n] . Define s q = P(Y ij R ij = 1|Z iq = 1) = ρ(π α) q (resp. s q = P(Y ij = 1|Z iq = 1) = (π α) q</formula><p>for star sampling). Up to reordering, s 1 &lt; s 2 &lt; . . . &lt; s Q are the coordinates of the vector s in the increasing order. Let S denote the Vandermonde matrix defined by S i,q = s i q , for 0 ≤ i &lt; Q and 1 ≤ q ≤ Q. S is invertible since the coordinates of s are all different. For i ≥ 1, S i,q = P(Y 12 R 12 = 1, . . . , Y 1i+1 R 1i+1 = 1|Z 1q = 1) for random-dyad sampling (resp. S i,q = P(Y 12 = 1, . . . , Y 1i+1 = 1|Z 1q = 1) for star degree sampling). Let us also define</p><formula xml:id="formula_112">u i = 1≤k≤Q α k s i k (resp. u i = 1≤k≤Q ρα k s i k ), i = 0, . . . , 2Q -1 . For i ≥ 1, u i = P(Y 12 R 12 = 1, . . . , Y 1i+1 R 1i+1 = 1) (resp. u i = P(Y 12 = 1, . . . , Y 1i+1 = 1, V 1 = 1)</formula><p>). Note that n ≥ 2Q is a necessary requirement on n since Y i,i = 0 by assumption. Hence, given P o</p><p>[n] and ρ, u 0 = 1 and u 1 , . . . , u 2Q-1 are known. Furthermore, set M the (Q + 1) × Q matrix given by M i,j = u i+j for every 0 ≤ i ≤ Q and 0 ≤ j &lt; Q, and let M i denote the square matrix obtained by removing the row i from M . The coefficients of M Q , for 0 ≤ i, j &lt; Q, are</p><formula xml:id="formula_113">M i,j = 1≤k≤Q s i k α k s j k (resp. M i,j = 1≤k≤Q ρs i k α k s j k ) , with 0 ≤ i, j &lt; Q . Defining the diagonal matrix A = Diag(α), it comes that M Q = SAS t (resp. M Q = ρSAS t )</formula><p>, where S and A are invertible, but unknown at this stage, and</p><formula xml:id="formula_114">ρ &gt; 0. With D k = det(M k ) and the polynomial B(x) = Q k=0 (-1) k+Q D k x k , it yields D Q = det(M Q ) = 0 and the degree of B is equal to Q. Set C i = (1, s i , . . . , s Q i ) t</formula><p>and let us notice that B(s i ) is the determinant of the square matrix produced when appending C i as the last column to M . The Q + 1 columns of this matrix are linearly dependent, since they are all linear combinations of the</p><formula xml:id="formula_115">Q vectors C 1 , C 2 , . . ., C Q . Hence B(s i ) = 0 and s i is a root of B for every 1 ≤ i ≤ Q. This proves that B = D Q Q i=1 (x -s i ).</formula><p>Then, one knows that s = (s 1 , . . . , s Q ) (as the roots of B defined from M ) and S. It results that</p><formula xml:id="formula_116">A = S -1 M Q (S t ) -1 , which yields a unique (α 1 , . . . , α Q ) (resp. A = ρ -1 S -1 M Q (S t ) -1</formula><p>). 49 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2</p><p>It only remains to determine π. For 0 ≤ i, j &lt; Q, let us introduce U i,j the probability that the first row of Y o begins with i+1 occurrences of 1, and the second row of Y o ends up with j occurrences of 1</p><formula xml:id="formula_117">(i + 1 + j ≤ n -1 implies n ≥ 2Q). Then, U i,j = k,l S i,k α k π k,l α l S j,l (resp. U i,j = k,l ρ 2 S i,k α k π k,l α l S j,l ), for 0 ≤ i, j &lt; Q, and the Q × Q matrix U = SAπAS t . The conclusion results from π = A -1 S -1 U (S t ) -1 A -1 (resp. π = ρ -2 A -1 S -1 U (S t ) -1 A -1 ).</formula><p>Proof of Theorem 2. Let P [n] denote the probability distribution function of (Y o , R). We show that there exists a unique (α, π, ρ) corresponding to P <ref type="bibr">[n]</ref> .</p><p>Identifiability of α. Up to reordering, let t 1 &lt; t 2 &lt; . . . &lt; t Q denote the coordinates of the vector t in the increasing order, we have :</p><formula xml:id="formula_118">t q = P(Y ij = 1, V j = 1|Z iq = 1).</formula><p>Let T denote the Vandermonde matrix defined by T i,q = t i q , for 0</p><formula xml:id="formula_119">≤ i &lt; Q and 1 ≤ q ≤ Q. T is invertible since the coordinates of t are all different. For i ≥ 1, T i,q = P(Y 12 = 1, . . . , Y 1i+1 = 1, V 2 = 1, . . . , V i+1 = 1|Z 1q = 1). Let us also define v i = 1≤k≤Q α k t i k , i = 0, . . . , 2Q -1 . For i ≥ 1, v i = P(Y 12 = 1, . . . , Y 1i+1 = 1, V 2 = 1, . . . , V i+1 = 1</formula><p>). Hence given P [n] , v 0 = 1 and v 1 , . . . , v 2Q-1 are known. Furthermore, set N the (Q + 1) × Q matrix given by N i,j = v i+j for 0 ≤ i ≤ j ≤ Q, and let N i denote the square matrix obtained by removing the row i from N . The coefficients of N Q are</p><formula xml:id="formula_120">N i,j = v i+j = 1≤k≤Q t i k α k t j k , with 0 ≤ i, j &lt; Q . Defining the diagonal matrix A = Diag(α), it comes that N Q = T AT t , where T and A are invertible. With D k = det(N k ) and the polynomial B(x) = Q k=0 (-1) k+Q D k x k , it yields D Q = det(N Q ) = 0 and the degree of B is equal to Q.</formula><p>Set C i = (1, t i , . . . , t Q i ) t and let us notice that B(t i ) is the determinant of the square matrix produced when appending C i as last column to N . The Q + 1 columns of this matrix are linearly dependent, since they are all linear combinations of the</p><formula xml:id="formula_121">Q vectors C 1 , C 2 , . . ., C Q . Hence B(t i ) = 0 and t i is a root of B for every 1 ≤ i ≤ Q. This proves that B = D Q Q i=1 (x -t i ).</formula><p>Then, one knows that t = (t 1 , . . . , t Q ) (as the roots of B defined from N ) and T . It results that</p><formula xml:id="formula_122">A = T -1 N Q (T t ) -1 , which yields a unique (α 1 , . . . , α Q ).</formula><p>Identifiability of ρ. Up to reordering, let o 1 &lt; o 2 &lt; . . . &lt; o Q denote the coordinates of the vector o in the increasing order, then s q = P(Y ij = 1, V i = 1|Z iq = 1) = ρ q o q . Let O denote the Vandermonde matrix defined by O i,q = o i q , for 0 ≤ i &lt; Q and 1 ≤ q ≤ Q. O is invertible since the coordinates of o are all different. For i ≥ 1, O i,q = P(Y 12 = 1, . . . , Y 1i+1 = 1|Z 1q = 1). Let us also define</p><formula xml:id="formula_123">u i = 1≤k≤Q ρ k α k o i k , i = 0, . . . , 2Q -1 . For i ≥ 1, u i = P(Y 12 = 1, . . . , Y 1i+1 = 1, V 1 = 1). Hence given P [n]</formula><p>, u 0 = 1 and u 1 , . . . , u 2Q-1 are known. Furthermore, set M the (Q + 1) × Q matrix given by M i,j = u i+j for every 0 ≤ i ≤ Q and 0 ≤ j &lt; Q, and let M i denote the square matrix obtained by removing the row i from M . The coefficients of M Q are</p><formula xml:id="formula_124">M i,j = u i+j = 1≤k≤Q o i k α k ρ k o j k , with 0 ≤ i, j &lt; Q .</formula><p>Defining the diagonal matrix B = Diag(ρ), it comes that M Q = OABO t , where O, B and A are invertible. Using the same algebraic argument than for the identifiability of α, it results that</p><formula xml:id="formula_125">B = A -1 O -1 M Q (O t ) -1</formula><p>, which yields, because of the identifiability of α, a unique (ρ 1 , . . . , ρ Q ). 50 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 Identifiability of π. For 0 ≤ i, j &lt; Q, let us introduce U i,j the probability that the first row of Y o begins with i + 1 occurrences of 1, and the second row of Y o ends up with j occurrences of 1.</p><formula xml:id="formula_126">U i,j = P {Y 12 = 1, . . . , Y 1i+2 = 1, V 2 = 1, . . . , V i+2 = 1} {Y 2n-j = 1, . . . , Y 2n = 1, V n-j = 1, . . . , V n = 1} , Then, U i,j = k,l T i,k α k π k,l ρ k α l T j,l , for 0 ≤ i, j &lt; Q, and the Q × Q matrix U = T AπABT t . The conclusion results from π = A -1 T -1 U (T t ) -1 B -1 A -1 .</formula><p>7.3 Derivation of second lower bound in star degree sampling</p><formula xml:id="formula_127">J τ ,ν,ζ,θ,ψ = C miss + n i=1 E p logistic(ζ i ) + (a + bD i ) -ζ i 2 + h(ζ i )((a + bD i ) 2 -ζ 2 i ) , = C miss + n i=1 logistic(ζ i ) + (a + b Di ) -ζ i 2 + h(ζ i )(a 2 + 2ab Di + b 2 Di -ζ 2 i ) ,</formula><p>where</p><formula xml:id="formula_128">C miss = i∈N m a + b Di and Di = E p D i 2 = V p (D i ) + E p [D i ] 2 = j∈N m ν ij (1 - ν ij ) + j∈N m ν ij + j∈N o Y ij 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Proof of Proposition 10</head><p>If the sampling design is node-centered (respectively dyad-centered), the first term in (2.7) consists in n Bernoulli random variables (respectively n(n-1)/2 Bernoulli random variables). The term log p ψ (R|Y o , Y m , Z, Q) can be derived using a BIC approximation:</p><formula xml:id="formula_129">log p ψ (R|Y, Z, Q) arg max ψ log p(R|Y, Z, ψ, Q) - 1 2 pen BIC ,</formula><p>where pen BIC = K × log(n) if the sampling design is node-centered,</p><formula xml:id="formula_130">K × log n(n-1)</formula><p>2 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Simulations for star degree and class samplings</head><p>With node-centered samplings such as star-degree and class samplings, it is more difficult to find configurations different from MAR sampling designs: with dyad-centred doublestandard design, the sampling parameters were directly related to the probability of an edge conditionally on the observation of the corresponding dyad, which is no longer the case for the node-centered designs. The sampling designs being hardly different from a MAR design, the NMAR inference does not show much improvement compared to the MAR inference. Still, we exhibit some interesting situations where star degree and class samplings deserve an appropriate treatment, presented herein. We simulate networks with n = 100 nodes under an affiliation topology with intra-community probability (resp. inter community probability) equal to 0.5 (resp. 0.05) and α = (0.25, 0.5, 0.25). Sampling parameters are chosen such that ψ = (a, b) = (-3.6, 0.1) for star degree sampling, which makes nodes with highest degrees preferably selected. In class sampling, these parameters are set to ψ = (ρ 1 , ρ 2 , ρ 3 ) = (0.75, 0.5, 0.05), which makes nodes from the largest block and from a small block preferably selected while the other small block is under-sampled. In Figure <ref type="figure" target="#fig_8">2</ref>.8, the estimation errors and the ARI are pictured for both cases. The sampling rates (i.e. rates of observed dyads over total number of dyads) lie in the intervals [0.558, 0.844] for 51 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 class sampling and [0.162, 0.622] for star degree sampling. These two intervals arise from the values of the parameter ψ explored for these two designs. We compare the performances of Algorithm 2 to an oracle (when inference is conducted via a classical VEM algorithm on a fully observed network) and with Algorithm 1. When facing NMAR condition, Algorithm 2 shows a slight improvement over Algorithm 1 even if it remains far from the oracle.</p><p>ππ F / π F ARI(Z, Ẑ) class q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6</p><p>[0.558,0.844] q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.80 star degree q q q q q q q q q q q q q q q q q q q 0.0 0.5 1.0 1.5 (0.162,0.622] q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Additional results for the ER Protein-Protein Interaction network</head><p>In this appendix, we provide more details on the NMAR clustering of the ER network discussed in Section 5.2. We represent the estimated connectivity matrix π in Figure <ref type="figure" target="#fig_8">2</ref>.9b for NMAR, which exhibits a network-structure with 13 blocks (or sets of proteins) the sizes of which can be sketched from Figure <ref type="figure" target="#fig_8">2</ref>.9a. In Figure <ref type="figure" target="#fig_8">2</ref>.9c, we represent on the same matrix the network with the original missing data and the imputed missing dyads with the variational parameters ν. Interestingly, many of the imputed values are close to 1, which might help validating some relationships which were still uncertain in the biological literature. Figure <ref type="figure" target="#fig_8">2</ref>.9a shows that the MAR clustering leads to a large cluster (block 3) which is mainly split into 4 blocks in the NMAR clustering (blocks 3, 4, 6 and 13). 52 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 MAR estimated clusters  Figure <ref type="figure" target="#fig_8">2</ref>.9 -ER PPI network analysis with SBM under missing data conditions 53 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 2 54 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency and Asymptotic Normality of Stochastic Block Models Estimators from Sampled Data</head><p>This chapter is a joint work with Mahendra Mariadassou and is available as a preprint on arXiv <ref type="bibr" target="#b87">(Mariadassou and Tabouy, 2019)</ref>. Statistical analysis of network is an active research area and the literature counts a lot of papers concerned with network models and statistical analysis of networks. However, very few papers deal with missing data in network analysis and we reckon that, in practice, networks are often observed with missing values. In this paper we focus on the Stochastic Block Model with valued edges and consider a MCAR setting by assuming that every dyad (pair of nodes) is sampled identically and independently of the others with probability ρ &gt; 0. We prove that maximum likelihood estimators and its variational approximations are consistent and asymptotically normal in the presence of missing data as soon as the sampling probability ρ satisfies ρ log(n)/n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the last decade, statistical network analyses has a been a very active research topic and the statistical modeling of networks has found many applications in social sciences and biology for example <ref type="bibr" target="#b0">Aicher et al. (2014)</ref>, <ref type="bibr" target="#b9">Barbillon et al. (2015)</ref>, <ref type="bibr" target="#b88">Mariadassou et al. (2010)</ref>, <ref type="bibr" target="#b126">Wasserman and Faust (1994)</ref> and <ref type="bibr" target="#b128">Zachary (1977)</ref>.</p><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many random graphs models have been widely studied, either from a theoretical or an empirical point of view. The first model studied was Erdős-Rényi model <ref type="bibr" target="#b43">(Erdős and Renyi, 1959)</ref> which assumes that each pair of nodes (dyad) is connected independently to the others with the same probability. This model assumes homogeneity of all nodes across the network. In order to alleviate this constraint, many families of models have been introduced. Most are endowed with a latent structure (reviewed in <ref type="bibr">Matias and Robin, 2014)</ref> to capture heterogeneity across nodes. Among those, the Stochastic Block Model (in short SBM, see <ref type="bibr">Frank and</ref><ref type="bibr" target="#b44">Harary, 1982 and</ref><ref type="bibr" target="#b60">Holland et al., 1983</ref>) is one of the oldest and most studied as it is highly flexible and can capture a large variety of structures (affiliation, hub, bipartite and many other). In order to estimate this model, Bayesian approaches were first proposed <ref type="bibr">(Snijders and</ref><ref type="bibr" target="#b111">Nowicki, 1997 and</ref><ref type="bibr">Nowicki and</ref><ref type="bibr" target="#b96">Snijders, 2001)</ref> but have been superseded by variational methods <ref type="bibr" target="#b36">(Daudin et al., 2008 and</ref><ref type="bibr" target="#b76">Latouche et al., 2012)</ref>. The former class of approaches are exact but lack the computational efficiency and scalability that the latter offers.</p><p>Theoretical guarantees concerning maximum likelihood estimators (in short MLE) and variational methods for the binary SBM estimation is not an easy task and have been widely studied. In <ref type="bibr" target="#b24">Celisse et al. (2012)</ref>, consistency of MLE and variational estimates is proven but asymptotic normality requires that the estimators converges at rate at least n -1 , which is not proven in the paper, although some results were available for some particular cases (affiliation for example). <ref type="bibr" target="#b3">Ambroise and Matias (2012)</ref> tackles the specific case of affiliation model with equal group proportion and proves the consistency and asymptotic normality of parameter estimates. <ref type="bibr" target="#b12">Bickel et al. (2013)</ref> extends those results to arbitrary binary SBM graphs and improves <ref type="bibr" target="#b24">Celisse et al. (2012)</ref> by removing the condition on the convergence rate. Following along the path of <ref type="bibr" target="#b12">Bickel et al. (2013)</ref>, <ref type="bibr" target="#b19">Brault et al. (2017)</ref> proved consistency and asymptotic normality of estimators (MLE and variational) to weighted Latent Block Models where the weights distribution belongs to a regular one-dimensional exponential family. In particular, considering non-bounded edge values invalidates several parts of the proofs for binary graphs and requires substantial adaptations and additional results, notably concentration inequalities for sums of unbounded, non-gaussian random variables.</p><p>Some results are also available for the related semi-parametric problem of assignment reconstruction. <ref type="bibr" target="#b86">Mariadassou and Matias (2015)</ref> show that the conditional distribution of the (latent) assignments converge to a degenerate distribution and <ref type="bibr" target="#b104">Rohe et al. (2010)</ref> prove that, when the data are generated according to a SBM model, spectral methods are consistent. <ref type="bibr">Choi et al. (2012a)</ref> extend those results to settings where the density of the graph goes to 0 as Ω(log α (n)/n) (for α large enough) and/or the number of groups goes to +∞ as √ n. Finally, <ref type="bibr">Wang and Bickel (2017)</ref> and <ref type="bibr" target="#b62">Hu et al. (2017)</ref> also show that model selection for the number of groups is consistent for dense graphs, they suggest using a penalized likelihood criteria with penalty of the form k(k+1) 2 log(n) + λn log(k) where λ is a tuning parameter. In this paper we consider a simple setting with fixed number of groups and fixed density but weighted edges and missing values. In most network studies, there is a strong asymmetry between the presence of an edge and its absence: the lack of proof that an edge exists is taken as proof that the edge does not exist and edges with uncertain status are considered as non existent in the graph. This is the strategy adopted in most sparse asymptotic settings where the density of edges goes to 0 asymptotically <ref type="bibr" target="#b12">Bickel et al. (2013)</ref>. We adopt a different point of view where edges with uncertain status are considered as missing, rather than absent and explicitly accounted for their missing nature. We use the framework of <ref type="bibr" target="#b105">Rubin (1976)</ref> and its application to network data, see <ref type="bibr" target="#b70">Kolaczyk (2009)</ref> and <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref>, for parameter inference in presence of missing values and more specifically its applications to <ref type="bibr">SBM Tabouy et al. (2019a)</ref>. We prove that, in the MCAR setting where each dyad is missing independently and with the same probability, the MLE and variational estimates are still consistent and asymptotically normal.</p><p>The article is organized as follows. We first present the model and missing data theory applied to our context with some examples of sampling designs. We then posit some defi-56 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 2. STATISTICAL FRAMEWORK nitions and discuss the assumptions required for our results in Section 2. In Section 3 we establish asymptotic normality for the complete-observed model (i.e. observed SBM where latent variables are known). Section 4 is the main result of this paper and states that the observed-likelihood behaves like the complete-observed likelihood (i.e. joint likelihood of the observed data and latent variables) close to its maximum. The proof is sketched in Section 5. Consequences for the MLE and variational estimator, as well as comparison to existing results, are in discussed in Section 6. Technical lemmas and details of the proofs are available in the appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical framework 2.1 Stochastic Block Model</head><p>In SBM, nodes from a set N {1, . . . , n} are distributed among a set Q {1, . . . , Q} of hidden blocks that model the latent structure of the graph. The block-memberships are encoded by (z i , i ∈ N ) where the z i are independent random variables with prior probabilities α = (α 1 , . . . , α Q ), such that P(z i = q) = α q , for all q ∈ Q. The value y ij of any dyad (i, j) in D = N × N , with i = j, only depends on the blocks i and j belong to. The variables (y ij )s are thus independent conditionally on the (z i )s:</p><formula xml:id="formula_131">y ij | z i = q, z j = ∼ ind ϕ(., π q ), ∀(i, j) ∈ D, i = j, ∀(q, ) ∈ Q × Q.</formula><p>In the following, y = (y ij ) i,j∈D is the n × n adjacency matrix of the random graph, z = (z 1 , . . . , z n ) the n-vector of the latent blocks. With a slight abuse of notation, we associate to z i a binary vector (z i1 , . . . , z iQ ) such that z i = q ⇔ z iq = 1, z i = 0, for all = q. In this case z is a n × Q matrix.</p><p>We note the complete parameter set as θ = (α, π) ∈ Θ where Θ stands for the parameter space. When performing inference from data, we note θ = (α , π ) the true parameter set, i.e. the parameter values used to generate the data, and z the true (and usually unobserved) memberships of nodes. For any z, we also note:</p><p>• z +q = i z iq the size of block q for membership z</p><p>• z +q its counterpart for z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Missing data for SBM</head><p>Regarding SBM inference, a missing value corresponds to a missing entry in the adjacency matrix y, typically denoted by NA's. We rely on the n × n sampling matrix r to record the missing state of each entry:</p><formula xml:id="formula_132">(r ij ) = 1 if y ij is observed, 0 otherwise. (2.1)</formula><p>As a shortcut, we use y o = {y ij : r ij = 1} and y m = {y ij : r ij = 0} to respectively denote the observed and missing dyads. The sampling design is the description of the stochastic process that generates r. It is assumed that the network exists before the sampling design acts upon it, which is fully characterized by the conditional distribution p ψ (r|y), the parameters of which are such that ψ and θ live in a product space Θ × Ψ. In this paper we are going to focus on a specific type of missingness, called missing completely at random (MCAR) for which p ψ (r|y) = p ψ (r) and leave aside more complex forms of dependencies such as Missing at random (MAR) and Not missing at random (NMAR). 57 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STATISTICAL FRAMEWORK</head><p>We then follow the framework of <ref type="bibr" target="#b105">(Rubin, 1976)</ref> and <ref type="bibr">Tabouy et al. (2019a)</ref> for missing data and define the joint probability density function as</p><formula xml:id="formula_133">p θ,ψ (y o , z, r) = p θ (y o , y m , z)p ψ (r|y o , y m , z)dy m .</formula><p>(2.2)</p><p>Property 1. According to Equation (2.2), if the sampling design is MCAR, then maximizing p θ,ψ (y o , z, r) or p θ,ψ (y o , r) in θ is equivalent to maximizing p θ (y o ) in θ, this corresponds to the ignorability notion defined in <ref type="bibr" target="#b105">Rubin (1976)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sampling design examples</head><p>We present here some examples of sampling designs to illustrate differences between notions of MCAR, MAR and NMAR.</p><p>Definition 3 (Random dyad sampling). Each dyad (i, j) ∈ D has the same probability P(r ij = 1) = ρ of being observed, independently of the others. This design is MCAR.</p><p>Definition 4 (Random node sampling). The random node sampling consists in selecting independently with probability ρ a set of nodes and then observing the corresponding rows and columns of matrix y.</p><p>The major point in both examples is that the probability (ρ in random dyad sampling and 1 -(1 -ρ) 2 in the random node sampling) of observing a dyad does not depend on its value. In contrast, the following dyad-centered sampling design adapted to binary networks is NMAR since the probability to observe a dyad depends on its value: Definition 5 (Double standard sampling). Each dyad (i, j) ∈ D is observed, independently of other dyads, with a probability depending on its value: P(r ij = 1|y ij = 0) = ρ 0 and P(r ij = 1|y ij = 1) = ρ 1 .</p><p>For non-binary networks, specifying the sampling design is more involved and requires defining the sampling density for every possible value of y ij , e.g. (P(r ij = 1|y ij = k)) k∈N for Poisson-valued edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Observed-likelihoods</head><p>When the labels are known, the complete-observed log-likelihood is given by:</p><formula xml:id="formula_134">L co (z; θ) = log p(y o , z; θ) = i,q z iq log α q + i,j,q, i =j z iq z j r ij log ϕ(y ij ; π q ) (2.3)</formula><p>But the labels are usually unobserved, and the observed log-likelihood is obtained by integration over all memberships:</p><formula xml:id="formula_135">L o (θ) = log p(y o ; θ) = log z∈Z p(y o , z; θ) .</formula><p>(2.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Models and Assumptions</head><p>We focus here on parametric models where ϕ belongs to a regular one-dimension exponential family in canonical form:</p><formula xml:id="formula_136">ϕ(y, π) = b(y) exp(πy -ψ(π)), (2.5)</formula><p>where π belongs to the space A, so that ϕ(•, π) is well defined for all π ∈ A. Classical properties of exponential families ensure that ψ is convex, infinitely differentiable on Å, that (ψ ) -1 is well defined on ψ ( Å). Furthermore, when y π ∼ ϕ(., π), E[y π ] = ψ (π) and 58 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 2. STATISTICAL FRAMEWORK</p><formula xml:id="formula_137">V[y π ] = ψ (π).</formula><p>In the following, we assume that missing data are produced according to a random dyad sampling with parameter ρ &gt; 0.</p><p>Moreover, we make the following assumptions on the parameter space :</p><p>A 1 : There exists a positive constant c, and a compact interval</p><formula xml:id="formula_138">C π such that ρ ∈ [c, 1 -c], Θ ⊂ [c, 1 -c] Q × C Q×Q π with C π ⊂ Å.</formula><p>A 2 : The true parameter θ = (α , π ) lies in the interior of Θ.</p><formula xml:id="formula_139">A 3 : The map π → ϕ(•, π) is injective.</formula><p>A 4 : The coordinates of π ψ (α ), where ψ is applied component-wise, are pairwise distinct.</p><p>The previous assumptions are standard. Assumption A 1 ensure that the group proportions and the sampling parameter are bounded away from 0 and 1 so that no group disappears when n goes to infinity. It also ensures that π is bounded away from the boundaries of the A. This is essential for the sub-exponential properties of Propositions 13 and 14. A 2 and A 3 are necessary for identifiability purposes: the model is trivially not identifiable if the map π → ϕ(., π) is not injective. A 4 states the identifiability of SBM parameters under random dyad sampling. Note that, combined with A 3 , it implies that all columns and all rows of π are distinct and therefore there are no two groups with identical connectivity profiles. In the following, we consider that Q, the number of classes (or groups) is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Identifiability</head><p>Since r is independent on y, the identifiability of SBM with emission law in the one-dimension exponential family under random dyad sampling can be stated in two steps. First the sampling parameter ρ and secondly the SBM parameters θ = (α , π ) given ρ.</p><p>Proposition 11. The sampling parameter ρ &gt; 0 of random dyad sampling is identifiable w.r.t. the sampling distribution.</p><p>Proof. See <ref type="bibr">Tabouy et al. (2019a)</ref>. The proof does not depend on y being binary but also holds for y distributed as in Eq. (2.5).</p><p>Proposition 12. Let n ≥ 2Q and assume that for any 1 ≤ q ≤ Q, ρ &gt; 0, π q &gt; 0 and that the coordinates of α ψ (π ), where ψ is applied component-wise, are pairwise distinct. Then, under random dyad sampling, SBM parameters are identifiable w.r.t. the distribution of the observed part of the SBM up to label switching.</p><p>Proof. The proof is nearly identical to the one written in <ref type="bibr">Tabouy et al. (2019a)</ref> and inspired by <ref type="bibr" target="#b24">Celisse et al. (2012)</ref> for the binary SBM under random dyad sampling. However, substituting E[y ij |z i = q] to s q in the proof ensures that α is identifiable. Finally, the fact that (ψ ) -1 is a one-to-one map ensures that π is identifiable.</p><p>Note that asymptotically, the assumption n ≥ 2Q is always satisfied since Q is fixed and n grows to infinity. 59 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 2. STATISTICAL FRAMEWORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Sub-exponential variables</head><p>Remarque 2. Since we restricted π in a bounded subset of Å, the variance of y π is bounded away from 0 and +∞. We note</p><formula xml:id="formula_140">σ2 = sup π∈Cπ V(y π ) &lt; +∞ and σ 2 = inf π∈Cπ V(y π ) &gt; 0.</formula><p>(2.6)</p><p>Similarly, since π belongs to a bounded subset of a open interval, there exists a constant κ &gt; 0, such that [π -κ, π + κ] ⊂ Å uniformly over all π ∈ C π Proposition 13. With the previous notations, if π ∈ C π and y π ∼ ϕ(., π), then y π is sub-exponential with parameters (σ 2 , κ -1 ).</p><p>Proposition 14. Considering x = y π r ij + λr ij (we recall that r ij ∼ B(ρ)), with r ij independent of y π and λ ∈ R bounded. There are non-negative numbers ν and b such that x is sub-exponential with parameters (ν 2 , b -1 ).</p><p>Proof. These results derive directly from theorem 20 (statement 2.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Symmetry</head><p>We now introduce the concepts of assignments and parameter symmetries, that must be accounted for when studying the asymptotic properties of the MLE. Complications stemming from symmetries are related to but no equivalent to the problem of label-switching in mixture models.</p><p>Definition 6 (permutation). Let s be a permutation on {1, . . . , Q}. If A is a matrix with Q columns and n rows, we define A s as the matrix obtained by permuting the columns of A according to s, i.e. for any row i and column q of A, A s iq = A is(q) . If C is a matrix with Q rows and Q columns, C s is defined similarly:</p><formula xml:id="formula_141">A s = A is(q) i,q C s = C s(q)s( ) q,</formula><p>Definition 7 (equivalence). We define the following equivalence relationships:</p><p>• Two assignments z and z are equivalent, noted ∼, if they are equal up to label permutation, i.e. there exists a permutation s such that z = z s .</p><p>• Two parameters θ and θ are equivalent, noted ∼, if they are equal up to label permutation, i.e. there exists a permutation s such that (α s , π s ) = (α , π ).</p><p>• (θ, z) and (θ , z ) are equivalent, noted ∼, if they are equal up to label permutation on π and z, i.e. there exists a permutation s such that (π s , z s ) = (π , z ). This is label-switching.</p><p>Definition 8 (symmetry). We say that the parameter θ exhibits symmetry for the permutation s if (α s , π s ) = (α, π).</p><p>θ exhibits symmetry if it exhibits symmetry for any non trivial permutations s. Finally the set of permutations for which θ exhibits symmetry is noted Sym(θ).</p><p>Remarque 3. The set of parameters that exhibit symmetry is a manifold of null Lebesgue measure in Θ. The notion of symmetry allows us to deal with a notion of non-identifiability of the class labels that is subtler than and different from label switching. More precisely Label switching is when :</p><formula xml:id="formula_142">p(y o , z, θ) = p(y o , z s , θ s ), θ = θ s ∀s Symmetry is when : p(y o , z, θ) = p(y o , z s , θ), ∀s ∈ Sym(θ)</formula><p>In particular, in label-switching, z and z s have the same likelihood but under equivalent yet different parameters θs. In contrast, in the presence of symmetry, multiple assignments can have exactly the same likelihood under θ. 60 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COMPLETE-OBSERVED MODEL</head><p>The issue of symmetry forces us to use a notion of distance between assignment that is invariant to label permutation. Definition 9 (distance). We define the following distance, up to equivalence, between configurations z and z :</p><formula xml:id="formula_143">z -z 0,∼ = inf z ∼z z -z 0</formula><p>where, for all matrix z, we use the Hamming norm • 0 defined by</p><formula xml:id="formula_144">z 0 = 1 2 i,q</formula><p>1{z iq = 0}.</p><p>Definition 10 (Set of local assignments). We note S(z , r) the set of configurations that have a representative (for ∼) within relative radius r of z :</p><formula xml:id="formula_145">S(z , r) = {z : z -z 0,∼ ≤ rn}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Other definitions</head><p>We finally introduce a few useful notions that will be instrumental in the proofs. The first is "regular" assignments, for which each group has "enough" nodes:</p><p>Definition 11 (c-regular assignments). Let z ∈ Z. For any c &gt; 0, we say that z is c-regular if min q z +q ≥ cn.</p><p>(2.7)</p><p>Class distinctness δ(π) captures the differences between groups: lower values of δ(π) means that at least two classes are very similar. δ(π) is intrinsically linked to the convergence rate of several estimates. Definition 12 (class distinctness). For θ = (α, π) ∈ Θ. We define:</p><formula xml:id="formula_146">δ(π) = min q,q max KL(π q , π q ) with KL(π, π ) = E π [log(ϕ(Y, π)/ϕ(Y, π ))] = ψ (π)(π -π ) + ψ(π ) -ψ(π)</formula><p>the Kullback-Leibler divergence between ϕ(., π) and ϕ(., π ), when ϕ comes from an exponential family.</p><p>Remarque 4. Since all π have distinct rows and columns, δ(π) &gt; 0.</p><p>Finally, the confusion matrix allows to compare groups between assignments: Definition 13 (confusion matrix). For given assignments z and z , we define the confusion matrix between z and z , noted IR(z), as follows:</p><formula xml:id="formula_147">IR(z) qq = 1 n i z iq z iq (2.8)</formula><p>Definition 14. For more conciseness, we define S = (S q ) q = ψ (π q ) q (2.9)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Complete-observed Model</head><p>In the following we study the asymptotic properties of the complete-observed data model, i.e. when the true assignment z is known. 61 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 3. COMPLETE-OBSERVED MODEL Proposition 15. Under random dyad sampling, defining N i = i,j R ij and Ω 0,n = {∀i ∈ {1, ..., n}, N i 1} the set of nodes with at least one dyaddy observed. Then</p><formula xml:id="formula_148">P lim n→+∞ Ω 0,n = 1.</formula><p>Proof. This proposition is a direct consequence of Borel-Cantelli's theorem. Details are available in appendix 10.</p><p>Remarque 5. This result shows that, with high probability, the network has no unobserved node. In the remainder, we work conditionally on Ω 0,n .</p><p>Let θ c = ( α, π) be the MLE of θ in the complete-observed data model. Simple manipulations of Equation (2.3) yield:</p><formula xml:id="formula_149">αq = α q (z) = z +q n y q (z) = i =j y ij r ij z iq z j i =j r ij z iq z j π q = π q (z) = (ψ ) -1 ( y q (z)) (3.1)</formula><p>Since there are missing values in the adjacency matrix, we need the following technical lemma to prove asymptotic normality of π q 's in the complete data model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemme 1.</head><p>U</p><formula xml:id="formula_150">n = 1 n(n -1) i =j r ij z iq z j P -----→ n→+∞ ρα q α l</formula><p>Proof. The proof of this lemma is based on Hoeffding's decomposition for U-statistics and on the proof of Hoeffding's concentration inequality. Details are postponed to appendix 10.</p><p>Proposition 16. Let Σ α = Diag(α ) -α (α ) T . Σ α is semi-definite positive, of rank Q -1, and α is asymptotically normal:</p><formula xml:id="formula_151">√ n ( α (z ) -α ) D ----→ n→∞ N (0, Σ α ) (3.2)</formula><p>Similarly, let V (π ) be the matrix defined by [V (π )] q = 1/ψ (π q ) and Σ π = ρ -1 Diag -1 (α )V (π ) Diag -1 (α ). Then the estimates πq (z ) are independent and asymptotically Gaussian with limit distribution:</p><formula xml:id="formula_152">n(n -1) ( π q (z ) -π q ) D ----→ n→∞ N (0, Σ π ,q ) for all q, (3.3)</formula><p>Proof. The proof is postponed to appendix 10. The first part is a direct application of central limit theorem for i.i.d. variables and the second part relies on a variant of the central limit theorem for random sums of random variables.</p><p>Proposition 17 (Local asymptotic normality). Let L co be the complete likelihood function defined on Θ by L co (α, π) = log p (y o , z ; θ). For any s, t and u in a compact set, we have:</p><formula xml:id="formula_153">L co α + s √ n , π + u n(n -1) = L co (θ ) + s T Y α + Tr(u T Y π ) - 1 2 s T Σ α s + 1 2 Tr (u u) T Σ π + o P (1)</formula><p>62 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MAIN RESULT</head><p>where denote the Hadamard product of two matrices (element-wise product) and Σ α and Σ π are defined in Proposition 16. Y α is asymptotically Gaussian with zero mean and variance matrix Σ α . Y π is a random matrix with independent entries that are asymptotically Gaussian zero mean and variance Σ π .</p><p>Proof. This result is based on a Taylor expansion of L co in a neighborhood of (α , π ). Details are available in appendix 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Result</head><p>Our main result compares the observed likelihood ratio p(y o ; θ)/p(y o ; θ ) with the complete likelihood p(y o , z ; θ )/p(y o , z ; θ ) to show that they have the same argmax. To ease the comparison, we work only on the high probability set Ω 1 of c/2-regular configurations, i.e. that have Ω(n) nodes in each group as defined in Section 2, Proposition 18. Define Z 1 as the subset of Z made of c/2-regular assignments, with c defined in assumption H 1 . Note Ω 1 the event {z ∈ Z 1 }, then:</p><formula xml:id="formula_154">P θ Ω1 ≤ Q exp - nc 2 2 .</formula><p>Proof. This proposition is a consequence of Hoeffding's inequality. See appendix 10 for more details.</p><p>We can now state our main result:</p><p>Theorem 15 (complete-observed). Assume that A 1 to A 4 with random-dyad sampling hold for the Stochastic Block Model of known order with n × n observations coming from an univariate exponential family and define # Sym(θ) as the set of permutation s for which θ = (α, π) exhibits symmetry. Then, for n tending to infinity, the observed likelihood ratio behaves like the complete likelihood ratio, up to a bounded multiplicative factor:</p><formula xml:id="formula_155">p(y o ; θ) p(y o ; θ ) = # Sym(θ) # Sym(θ ) max θ ∼θ p(y o , z ; θ ) p(y o , z ; θ ) (1 + o P (1)) + o P (1)</formula><p>where the o P is uniform over all θ ∈ Θ.</p><p>The maximum over all θ that are equivalent to θ stems from the fact that because of label-switching, θ is only identifiable up to its ∼-equivalence class from the observed likelihood, whereas it is completely identifiable from the complete likelihood. The multiplicative factor arises from the fact that equivalent assignments have exactly the same complete likelihood and contribute equally to the observed likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 16. If Θ contains only parameters with no symmetry:</head><formula xml:id="formula_156">p(y o ; θ) p (y o ; θ ) = max θ ∼θ p(y o , z ; θ ) p(y o , z ; θ ) (1 + o P (1)) + o P (1)</formula><p>where the o P is uniform over all Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proof Sketch</head><p>The proof of theorem relies on controlling deviations of the log-likelihood ratios from their expectations. We introduce a few notations for those quantities. 63 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 5. PROOF SKETCH</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">log-likelihood ratios</head><p>Definition 17. We define the conditional log-likelihood ratio LR and its expectation ELR as:</p><formula xml:id="formula_157">LR(θ, z) = log p(y o |z; θ) p(y o |z ; θ ) and ELR(θ, z) = E θ [ LR(θ, z)| z ] (5.1)</formula><p>We also define the profile ratio Λ and its counterpart Λ as:</p><formula xml:id="formula_158">Λ(z) = max θ LR(θ, z) and Λ(z) = max θ ELR(θ, z).</formula><p>(5.2)</p><p>Proposition 19. Conditionally on z , we have</p><formula xml:id="formula_159">ȳq (z) := E θ [ y q (z)|z ] = IR(z) T S IR(z) q α q (z) α (z) (5.3)</formula><p>with ȳq (z) = 0 for z such that α q (z) = 0 or α (z) = 0.</p><p>Remarque 6. Note the absence of the random variable r in ȳq (z).</p><p>The following decomposition of p(y o ; θ) highlights the importance of F n (θ, z):</p><formula xml:id="formula_160">p(y o ; θ) = (z) p(y o , z; θ) = p(y o |z ; θ ) (z)</formula><p>p(z; θ) exp(LR(θ, z)).</p><p>Since LR(θ, z) ≤ Λ(z), the profile ratio is useful to remove the dependency on θ and reduce the study to a series of problems depending only on z. The following propositions show when those quantities reach their maximum values and what the corresponding values are.</p><p>Proposition 20 (maximum of ELR and Λ in θ). The functions LR(θ, z) and ELR(θ, z) are maximum respectively in π for π(z) and π(z) defined by: π(z) q = (ψ ) -1 ( y q (z)) and π(z) q = (ψ ) -1 (ȳ q (z)) so that Λ(z) = LR( π(z), z) and Λ(z) = ELR( π(z), z).</p><p>Proposition 21 (Local upper bound for Λ). Conditionally upon Ω 1 , there exists a positive constant C such that for all z ∈ S(z , C):</p><formula xml:id="formula_161">Λ(z) ≤ -cρn 3δ(π ) 4 z -z 0,∼ (5.4)</formula><p>Proposition 22 (maximum of ELR and Λ in (θ, z)). ELR can be written:</p><formula xml:id="formula_162">ELR(θ, z) = -ρn 2 q,q ,</formula><p>IR(z) q,q IR(z) , KL(π q , π q ) ≤ 0.</p><p>(5.5)</p><p>Conditionally on the set Ω 1 of regular assignments and for n &gt; 2/c, (i) ELR is maximized at (π , z ) and its equivalence class and ELR(π , z ) = 0.</p><p>(ii) Λ is maximized at z and its equivalence class and Λ(z ) = 0.</p><p>(iii) The maximum of Λ (and hence the maximum of ELR) is well separated.</p><p>Proofs of Propositions 19, 20, 22 and 21 are postponed to Appendix 11. 64 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 5. PROOF SKETCH</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">High level view of the proof</head><p>The proof proceeds with an examination of the asymptotic behavior of LR on three types of configurations that partition Z:</p><p>1. global control: for z such that Λ(z) = Ω(-n 2 ), Proposition 23 proves a large deviation behavior and shows that LR = -Ω P (n 2 ). In turn, those assignments contribute a o P of p(y o , z ; θ )) to the sum (Proposition 24).</p><p>2. local control: a small deviation result (Proposition 25) is needed to show that the combined contribution of assignments close to but not equivalent to z is also a o P of p(y o , z ; θ ) (Proposition 26).</p><p>3. equivalent assignments: Proposition 27 examines which of the remaining assignments, all equivalent to z , contribute to the sum.</p><p>These results are presented in next section 5.3 and their proofs postponed to Appendix 11. They are then put together in section 5.4 to prove our main result. The remainder of the section is devoted to the asymptotic of the ML and variational estimators as a consequence of the main result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Different asymptotic behaviors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Control</head><p>Proposition 23 (large deviations of LR). Let Diam(Θ) = sup θ,θ θθ ∞ . For all ε n &lt; νb and n large enough that 2 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p><formula xml:id="formula_163">√ 2n 2 n ≥ Q 2 sup θ,z LR(θ, z) -Λ(z) = O p (n 2 n ) (5.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">VARIATIONAL AND MAXIMUM LIKELIHOOD ESTIMATES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivalent assignments</head><p>It remains to study the contribution of equivalent assignments.</p><p>Proposition 27 (contribution of equivalent assignments). For all θ ∈ Θ, we have</p><formula xml:id="formula_164">z∼z p(y o , z; θ) p(y o , z ; θ ) = # Sym(θ) max θ ∼θ p(y o , z ; θ ) p(y o , z ; θ ) (1 + o P (1))</formula><p>where the o P is uniform in θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Proof of the main result</head><p>Proof. We work conditionally on Ω </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Variational and Maximum Likelihood Estimates</head><p>This section is devoted to the asymptotic of the ML and variational estimators in the incomplete data model as a consequence of the main result 15. Note that, with high probability, ML and variational estimators have no symmetry since the set {θ : # Sym(θ) &gt; 1} is a manifold of null Lebesque's measure in Θ. 66 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 7. DISCUSSION</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ML estimator</head><p>The asymptotic behavior of the maximum likelihood estimator in the incomplete data model is a direct consequence of Theorem 15 and Proposition 17.</p><p>Corollary 18 (Asymptotic behavior of θ M LE ). Denote θ M LE the maximum likelihood estimator and use the notations of Proposition 16. There exist permutations s of {1, . . . , Q} such that</p><formula xml:id="formula_165">α (z ) -α s M LE = o P n -1/2 , π (z ) -π s M LE = o P n -1 .</formula><p>Hence, the maximum likelihood estimator for the SBM under random-dyad sampling condition is consistent and asymptotically normal, with the same behavior as the maximum likelihood estimator in the complete data model. The proof is postponed to appendix 11.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Variational estimator</head><p>Due to the complex dependency structure of the observations, the maximum likelihood estimator of the SBM is not numerically tractable, even with the Expectation Maximization algorithm. In practice, a variational approximation is often used (see <ref type="bibr" target="#b36">Daudin et al., 2008)</ref>: for any joint distribution Q ∈ Q on Z a lower bound of L(θ) is given by</p><formula xml:id="formula_166">J (Q, θ) = L(θ) -KL (Q, p (.; θ, y o )) = E Q [L co (z; θ)] + H (Q) .</formula><p>where</p><formula xml:id="formula_167">H (Q) = -E Q [log(Q)]. Choosing Q to be the set of product distributions, such that for all z Q (z) = i,q Q (z iq = 1)</formula><p>ziq allows us to obtain tractable expressions of J (Q, θ). The variational estimate θ var of θ is defined as</p><formula xml:id="formula_168">θ var ∈ argmax θ∈Θ max Q∈Q J (Q, θ) .</formula><p>The following corollary states that θ var has the same asymptotic properties as θ M LE and θ M C , in particular is consistent and asymptotically normal.</p><p>Theorem 19 (Variational estimate). Under the assumptions of Theorem 15, there exist permutations s of {1, . . . , Q} such that</p><formula xml:id="formula_169">α (z ) -α s var = o P n -1/2 , π (z ) -π s var = o P n -1 .</formula><p>The proof is very similar to the proof of Theorem 18 and postponed to appendix 11.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Close examination of the different proofs, especially of Prop. 26, reveals that the quantities driving convergence of the estimates are ρnδ(π ), which must go to +∞ with n to ensure validity of Prop. 24, and ρnt n δ(π ), which must be larger than log(n) while t n → 0, to ensure validity of Prop. 26. Both conditions are met as soon as ρ log(n)/n, allowing for a 67 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 8. ACKNOWLEDGMENT large fraction of missing edges. Note that this limiting rate for missingness is the same as the one found for graph density in sparse settings to achieve consistency and local asymptotic normality of θ <ref type="bibr" target="#b12">(Bickel et al., 2013)</ref>.</p><p>In this paper, we focused on data sampled according to random dyad sampling. However, as described in section 2.3, there are many other ways to sample a network. In the case of node-centered sampling design, like random node sampling, the main difficulty to prove consistency and asymptotic normality is the dependency between the r ij variables. Indeed, in random node sampling, the variable r i0j0 depends on all r ij0 and r i0j (for all i, j ∈ N ). As a consequence, many results proved in this paper are not valid under random node sampling. NMAR sampling designs raises problem of their own: each design requires its own estimation procedure <ref type="bibr">(Tabouy et al., 2019a)</ref> and therefore its own analysis. For example, even parameter estimation under the double standard sampling for binary networks mentioned in section 2.3 is still an unsolved problem: numerical experiments suggest that θ = (π, α) and ψ = (ρ 0 , ρ 1 ) are jointly identifiable but there is no formal proof. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 9. SUPPLEMENTARY 9 Supplementary 10 Technical results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Proof of proposition 15</head><p>Proof. Noticing that</p><formula xml:id="formula_170">N i ∼ Bin(n -1, ρ), then P(N i 1) = 1 -(1 -ρ) n-1 . As a con- sequence P(Ω 0,n ) i P(N i = 0) = n(1 -ρ) n-1 -→ n→+∞ 0</formula><p>, and P(Ω 0,n ) -→ n→+∞ 1. Then P(lim sup(Ω 0,n )) = 0 by Borel-Cantelli's theorem (because n P(Ω 0,n ) converge), and as lim sup Ω 0,n = n 0 q n Ω 0,n = n 0 q n Ω 0,n = lim inf Ω 0,n , the result follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Proof of lemma 1</head><p>Proof. Noticing that E[r ij z iq z j ] = ρα q α l and defining q q, i,j = r ij z iq z j -ρα q α l . By Hoeffding decomposition for U-statistics (see <ref type="bibr" target="#b57">Hoeffding (1948)</ref>)</p><formula xml:id="formula_171">U n = 1 n(n -1) i =j (r ij z iq z j -ρα q α l ) = 1 n! σ∈Sn 1 n 2 n 2 i=1 q q, σ(i),σ(i+ n 2 ) , (10.1)</formula><p>where for each permutation σ ∈ S,</p><formula xml:id="formula_172">n 2 i=1 q q, σ(i),σ(i+ n</formula><p>2 ) is a sum of independent r.v. Then, for γ &gt; 0 by Jensen's inequality and Hoeffding's lemma about bounded r.v.</p><formula xml:id="formula_173">E [exp(γU n )] ≤ 1 n! σ∈Sn E exp   γ n 2 n 2 i=1 q q, σ(i),σ(i+ n 2 )   ≤ exp γ 2 8 n 2 .</formula><p>Finally, using the same proof than Hoeffding's inequality allows us to conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Proof of proposition 16</head><p>Proof. Since α (z ) = (α 1 (z ) , . . . , αg (z )) is the sample mean of n i.i.d. multinomial random variables with parameters 1 and α , a simple application of the central limit theorem (CLT) gives:</p><formula xml:id="formula_174">Σ α ,qq = α q (1 -α q ) if q = q -α q α q if q = q which proves Equation (3.2) where Σ α is semi-definite positive of rank Q -1.</formula><p>Similarly, ψ ( π q (z )) is the average of i =j r ij z iq z j i.i.d. random variables with mean ψ π q and variance ψ π q . i =j r ij z iq z j is itself random but thanks to lemma 1 :</p><formula xml:id="formula_175">1 n(n-1) i =j r ij z iq z j P -----→ n→+∞ ρα q α l</formula><p>. Therefore, by Slutsky's lemma and the CLT for random sums of random variables <ref type="bibr" target="#b108">Shanthikumar and Sumita (1984)</ref>, we have:</p><formula xml:id="formula_176">n(n -1)ρα q α (ψ ( π q (z )) -ψ (π q )) = n(n -1)ρα q α i =j y ij r ij z iq z j i =j r ij z iq z j -ψ (π q ) D -----→ n→+∞ N (0, ψ (π q ))</formula><p>69 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MAIN RESULTS</head><p>The differentiability of (ψ ) -1 and the delta method then gives:</p><formula xml:id="formula_177">n(n -1) ( π q (z ) -π q ) D -----→ n→+∞ N 0, 1 ρα q α ψ (π q )</formula><p>and the independence results from the independence of π q (z ) and π q (z ) as soon as q = q or = , as they involve different sets of i.i.d. variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Proof of proposition 17</head><p>Proof. By Taylor expansion,</p><formula xml:id="formula_178">L co α + s √ n , π + u n(n -1) = L co (θ ) + 1 √ n s T ∇L coα (θ ) + 1 n(n -1) Tr u T ∇L coπ (θ ) + 1 n s T H α (θ ) s + 1 n(n -1) Tr (u u) T H π (θ ) + o P (1)</formula><p>where ∇L coα (θ ) and ∇L coπ (θ ) denote the respective components of the gradient of L co evaluated at θ and H α and H π denote the conditional hessian of L co evaluated at θ . By inspection, H α /n and H π /(n(n -1)) converge in probability to constant matrices Σ α , Σ π and the random vectors ∇L coα (θ ) / √ n and ∇L coπ (θ ) / n(n -1) converge in distribution by central limit theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">Proof of proposition 18</head><p>Proof. In regular configurations, each group has Ω(n) members, where u n = Ω(n) if there exists two constant a, b &gt; 0 such that for n enough large an ≤ u n ≤ bn. c/2-regular assignments, with c defined in Assumption H 1 , have high P θ -probability in the space of all assignments, uniformly over all θ ∈ Θ.</p><p>Each z +q is a sum of n i.i.d Bernoulli r.v. with parameter α q ≥ α min ≥ c. A simple Hoeffding bound shows that</p><formula xml:id="formula_179">P θ z +q ≤ n c 2 ≤ P θ z +q ≤ n α q 2 ≤ exp -2n α q 2 2 ≤ exp - nc 2 2</formula><p>taking a union bound over Q values of q leads to Proposition 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Proof of proposition 19)</head><p>Proof. First of all we will prove equation 5.3,</p><formula xml:id="formula_180">ȳq (z) = E θ i =j z iq z j r ij y ij i =j z iq z j r ij z = E θ E θ i =j z iq z j r ij y ij i =j z iq z j r ij R, z z = E θ i =j z iq z j r ij S ZiZj i =j z iq z j r ij z ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>70</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MAIN RESULTS</head><p>where Z i = q ⇔ z iq = 1. Noticing that the (i, j) for which z iq z j = 0 does not contributes in any of the two terms of the ratio. The calculus of this expectation is then equivalent to calculate an expectation of the general form</p><formula xml:id="formula_181">E θ n i=1 aiRi n i=1</formula><p>Ri , (a i ) i∈{1,..,n} ∈ R n and</p><formula xml:id="formula_182">T i iid ∼ B(ρ). Lemme 2. E θ n i=1 a i T i n i=1 T i = n i=1 a i n . Proof. Define N = n i=1 T i and noticing that E[T i |N = k] = k n . Conditionally to N ≥ 1 E n i=1 a i T i n i=1 T i = E E n i=1 a i T i N N = n i=1 a i n .</formula><p>Now, applying lemma 2 with N o q (z) = i =j z iq z j r ij leads to</p><formula xml:id="formula_183">E θ i =j z iq z j r ij S ZiZj i =j z iq z j r ij z , N o q (z) ≥ 1 = IR(z) T S IR(z) q α q (z) α (z) 1 N o q (z)≥1 .</formula><p>Finally, E θ [ y q (z)|z , N o q (z) = 0] can be arbitrarily defined at the same value than E θ [ y q (z)|z , N o q (z) ≥ 1] which conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Proof of proposition 20</head><p>Proof. Defining ν(y, π) = yπ -ψ(π). For y fixed, ν(y, π) is maximized at π = (ψ ) -1 (y). Manipulations yield</p><formula xml:id="formula_184">LR(θ, z) = log p(y o ; z, θ) -log p(y o ; z , θ ) = q N o q (z)ν( y q (z), π q ) - q N o q (z )ν( y q (z ), π q )</formula><p>which is maximized at π q = (ψ ) -1 ( y q (z)). Similarly with</p><formula xml:id="formula_185">N q (z) = i =j z iq z j , ELR(θ, z) = E θ [log p(y o ; z, θ) -log p(y o ; z , θ )|z ] = ρ q N q (z)ν(ȳ q (z), π q ) - q N q (z )ν(ψ (π q ), π q )</formula><p>is maximized at π q = (ψ ) -1 (ȳ q (z)). 71 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 11. MAIN RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Proof of Proposition 22 (maximum of ELR and Λ)</head><p>Proof. We condition on z and prove Equation (5.5):</p><formula xml:id="formula_186">ELR(θ, z) = E θ log p(y o ; z, θ) p(y o ; z , θ ) z = i j q,q , E θ y ij (π q -π q ) -(ψ(π q ) -ψ(π q )) ρz iq z iq z j z j = n 2 ρ q,q ,</formula><p>IR(z) q,q IR(z) , ψ (π q )(π q -π q ) + ψ(π q ) -ψ(π q ) = -n 2 ρ q,q , IR(z) q,q IR(z) , KL(π q , π q )</p><p>If z is regular, and for n &gt; 2/c, all the rows of IR(z) have at least one positive element and we can apply Lemma 3.2 of <ref type="bibr" target="#b12">Bickel et al. (2013)</ref> to characterize the maximum for ELR.</p><p>The maximality of Λ(z ) results from the fact that Λ(z) = ELR( π(z), z) where π(z) is a particular value of π, Λ is immediately maximum at z ∼ z , and for those, we have π(z) ∼ π .</p><p>The separation and local behavior of G around z is a direct consequence of the proposition 21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Proof of Proposition 21 (Local upper bound for Λ)</head><p>Proof. We work conditionally on z . The principle of the proof relies on the extension of Λ to a continuous subspace of M Q ([0, 1]), in which the confusion matrix is naturally embedded. The regularity assumption allows us to work on a subspace that is bounded away from the borders of M Q ([0, 1]). The proof then proceeds by computing the gradient of Λ at and around its argmax and using those gradients to control the local behavior of Λ around its argmax. The local behavior allows us in turn to show that Λ is well-separated.</p><p>Note that Λ only depends on z through IR(z). We can therefore extend it to matrix U ∈ U c where U is the subset of matrices M Q ([0, 1]) with each row sum higher than c/2.</p><formula xml:id="formula_187">Λ(U ) = -ρn 2 q,q , U qq U KL π q , πq where πq = πq (U ) = (ψ ) -1 U T S U q [U T 1U ] q</formula><p>and 1 is the Q × Q matrix filled with 1. Confusion matrix IR(z) satisfy IR(z)1I = α(z ), with 1I = (1, . . . , 1) T a vector only containing 1 values, and are obviously in U c as soon as z is c/2 regular. The maps f q,q , , : (U ) → KL(π q , πq (U )) are twice differentiable with second derivatives bounded over U c and therefore so is Λ(U ). Tedious but straightforward computations show that the derivative of Λ at D α := Diag(α(z )) is:</p><formula xml:id="formula_188">A qq (z ) := -1 n 2 ∂ Λ ∂U qq (D α ) = 2ρ α (z )KL π q , π q A(z ) is the matrix-derivative of -Λ/n 2 at D α .</formula><p>Since z is c/2-regular and by definition of δ(π ), A(z ) qq ≥ cρδ(π ) if q = q and A(z ) qq = 0 for all q. By boundedness of the second derivative, there exists C &gt; 0 such that for all D α and all H ∈ B(D α , C), we have:</p><formula xml:id="formula_189">-1 n 2 ∂ Λ ∂U qq (H) ≥ ρ 7cδ(π ) 8 if q = q ≤ ρ cδ(π ) 8</formula><p>if q = q 72 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MAIN RESULTS</head><p>Choose U in U c ∩ B(D α , C) satisfying U 1I = α(z ). U -D α have non-negative off diagonal coefficients and negative diagonal coefficients. Furthermore, the coefficients of U, D α sum up to 1 and Tr(D α ) = 1. By Taylor expansion, there exists H also in</p><formula xml:id="formula_190">U c ∩ B(D α , C) such that -1 n 2 Λ (U ) = -1 n 2 Λ (D α ) + Tr (U -D α ) -1 n 2 ∂ Λ ∂U (H) ≥ ρ cδ(π ) 8 [7 q =q (U -D α ) qq - q (U -D α ) qq = cρ 3δ(π ) 4 (1 -Tr(U ))</formula><p>To conclude the proof, assume without loss of generality that z ∈ S(z , C) achieves the . 0,∼ norm (i.e. it is the closest to z in its representative class). Then U = IR(z) is in (U c ∩B(D α , C) and satisfy U 1I = α(z ). We just need to note n(1-Tr(IR(z))) = z-z 0,∼ to end the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5">Proof of Proposition 23 (global convergence LR)</head><p>Proof. Conditionally upon z ,</p><formula xml:id="formula_191">LR(θ, z) -Λ(z) ≤ LR(θ, z) -ELR(θ, z) = i j (π zizj -π z i z j ) y ij r ij -ψ (π z i z j )ρ + i j (ψ(π zizj ) -ψ(π z i z j )) (r ij -ρ) = qq (π q -π q ) W qq ≤ sup Γ∈IR Q 2 ×Q 2 Γ ∞ ≤Diam(Θ) qq Γ qq W qq := Z</formula><p>uniformly in θ, where the W qq are independent and by Taylor expansion defined by:</p><formula xml:id="formula_192">W qq = i j z iq z j z i,q z j (y ij r ij -ψ (π q )ρ -(r ij -ρ)C qq ) , C qq ∈ ψ (Θ)</formula><p>is the sum of n 2 IR(z) qq IR(z) sub-exponential variables with parameters (ν 2 , 1/b) and is therefore itself sub-exponential with parameters (n 2 IR(z</p><formula xml:id="formula_193">) qq IR(z) ν 2 , 1/b). According to Proposition B.3 of Brault et al. (2017) , E θ [Z|z ] ≤ Q 2 Diam(Θ) √ n 2 ν 2 and Z is sub- exponential with parameters (n 2 Diam(Θ) 2 (2 √ 2) 2 ν 2 , 2 √ 2 Diam(Θ)/b).</formula><p>In particular, for all ε n &lt; νb</p><formula xml:id="formula_194">P θ Z ≥ νQ 2 Diam(Θ)n 1 + √ 8n 2 ε n Q 2 z ≤ P θ Z ≥ E θ [Z|z ] + ν Diam(Θ)n 2 2 √ 2ε n z ≤ exp - n 2 ε 2 n 2</formula><p>We can then remove the conditioning and take a union bound. 73 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 11. MAIN RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.6">Proof of Proposition 24 (contribution of far away assignments)</head><p>Proof. Conditionally on z , we know from proposition 22 that Λ is maximal in z and its equivalence class. Choose 0 &lt; t n decreasing to 0 but satisfying nρtn √</p><formula xml:id="formula_195">log(n) → +∞. According to 22 (iii), for all z / ∈ S(z , t n ) Λ(z) ≤ -cρn 3δ(π ) 4 z -z 0,∼ ≤ -cρ 3δ(π ) 4 n 2 t n (11.1) since z -z 0,∼ ≥ nt n . Set ε n = inf(5cρδ(π )t n /( √ 2ν Diam(Θ)), νb) and n large enough that n ≥ Q 2 n √ 8</formula><p>. By proposition 23, and with our choice of ε n , with probability higher than 1</p><formula xml:id="formula_196">-∆ 1 n (ε n ), z / ∈S(z ,tn) p(y o , z; θ) = p(y o |z , θ ) z / ∈S(z ,tn) p(z; θ)e LR(θ,z)-Λ(z)+ Λ(z) ≤ p(y o |z , θ ) z p(z; θ)e LR(θ,z)-Λ(z)-3n 2 tncρδ(π )/4 ≤ p(y o |z , θ ) z p(z; θ)e -n 2 tncρδ(π )/8 = p(y o , z ; θ ) p(z ; θ ) e -n 2 tncρδ(π )/8 ≤ p(y o , z ; θ ) exp -n 2 t n cρδ(π ) 8 + n log 1 c = p(y o , z ; θ )o(1)</formula><p>where the second line comes from inequality (11.1), the third from the global control studied in Proposition 23 and the definition of ε n , the fourth from the definition of p(y o , z ; θ ), the fifth from the bounds on α and the last from nρtn √ log(n)</p><p>→ +∞.</p><p>In addition, with our choice of t n , we have ε n log(n)/n so that the series n ∆ 1 n (ε n ) converges and:</p><formula xml:id="formula_197">z / ∈S(z ,t nd ) p(y o , z; θ) = p(y o ; z , θ )o P (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.7">Proof of Proposition 25 (local convergence LR)</head><p>Proof. We work conditionally on z ∈ Z 1 . Choose ε ≤ κσ 2 small. Assignments z at . 0,∼ -distance less than c/4 of z are c/4-regular. According to Proposition B.1 of <ref type="bibr" target="#b19">Brault et al. (2017)</ref> , y q and ȳq are at distance at most ε with probability higher than 1 -</p><formula xml:id="formula_198">2 exp -n 2 c 2 ε 2 32(ν 2 +b -1 ε) . Defining Λ(z) = q N o q (z)ν(ȳ q (z), π q ) - q</formula><p>N o q (z )ν(ψ (π q ), π q ), 74 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MAIN RESULTS</head><p>where Λ(z) = E Λ(z)|z . Manipulation of Λ, Λ and Λ yield</p><formula xml:id="formula_199">Λ(z) -Λ(z) n 2 ≤ Λ(z) -Λ(z) n 2 + Λ(z) -Λ(z) n 2 = 1 n 2 q N o q (z) [f ( y q ) -f (ȳ q )] -N o q (z )π q ( y q -ȳ q ) + 1 n 2 q f (ȳ q ) N o q (z) -N o q (z ) -ρ(N q (z) -N q (z )) =A q + 1 n 2 q [N o q (z) -ρN q (z)](f (ȳ q ) -f (ȳ q ))</formula><p>where</p><formula xml:id="formula_200">f (x) = x(ψ ) -1 (x) -ψ • (ψ ) -1 (x), y q = y q (z ) and ȳ q = ψ (π q ).</formula><p>Concerning the first term. The function f is twice differentiable on Å with f (x) = (ψ ) -1 (x) and f (x) = 1/ψ • (ψ ) -1 (x). f (resp. f ) are bounded over I = ψ (C π ) by C π (resp. 1/σ 2 ) so that:</p><formula xml:id="formula_201">f ( y q ) -f (ȳ q ) = f (ȳ q ) ( y q -ȳq ) + Ω ( y q -ȳq ) 2</formula><p>By Proposition B.1 (adapted for SBM) of <ref type="bibr" target="#b19">Brault et al. (2017)</ref> , ( y q -ȳq )</p><formula xml:id="formula_202">2 = O P (1/n 2 )</formula><p>where the O P is uniform in z and does not depend on z . Similarly,</p><formula xml:id="formula_203">f (ȳ q ) = f (ȳ q ) + Ω(ȳ q -ȳ q ) = π q + Ω(ȳ q -ȳ q )</formula><p>ȳq is a convex combination of the S q = ψ (π q ) therefore,</p><formula xml:id="formula_204">|ȳ q -ȳ q | = IR(z) T S IR(z) q α q (z) α (z) -ȳ q ≤ 1 - IR(z) qq IR(z) α q (z) α (z) (S max -S min )</formula><p>Note that:</p><formula xml:id="formula_205">q, N o q (z) 1 - IR(z) qq IR(z) α q (z) α (z) = n 2 ρ(1 + o P (1)) q, [1 -IR(z) qq IR(z) ] = n 2 ρ(1 + o P (1))[1 -Tr(IR(z)) 2 ] ≤ nρ(1 + o P (1))2 z -z 0,∼</formula><p>and y q -ȳq = o P (1). Therefore</p><formula xml:id="formula_206">1 n 2 q, N o q (z)Ω(ȳ q -ȳ q ) × ( y q -ȳq ) = o P z -z 0,∼<label>n</label></formula><p>The remaining term writes</p><formula xml:id="formula_207">1 n 2 q, π q N o q (z)( y q -ȳq ) -N o q (z )( y q -ȳ q )</formula><p>and is also o P (( zz 0,∼ /n) uniformly in z and z ∈ Ω 1 by Proposition 28. 75 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MAIN RESULTS</head><p>Concerning the second term. For all q, , defining</p><formula xml:id="formula_208">N + q (z, z ) = n 2 q IR(z) qq (z) IR(z) (z) -n 2 IR(z) qq IR(z) N - q (z, z ) = n 2 q IR(z) qq (z) IR(z) (z) -n 2 IR(z) qq IR(z)</formula><p>and noticing that N + q (z, z ) = #{(i, j) : z iq = 1, z j = 1, (z q , z j ) = (z q , z j )} and N - q (z, z ) = #{(i, j) : z iq = 1, z j = 1, (z q , z j ) = (z q , z j )}. Using the following notations</p><formula xml:id="formula_209">ρ+ q = 1 N + q (z, z ) (i,j)∈N + q (z,z ) R ij , ρ- q = 1 N - q (z, z ) (i,j)∈N - q (z,z ) R ij</formula><p>we are able to write</p><formula xml:id="formula_210">A q = i&lt;j ziq=1,z j =1 (R ij -ρ) - i&lt;j z iq =1,z j =1 (R ij -ρ) =N + q (z, z )(ρ + q -ρ) -N - q (z, z )(ρ - q -ρ).</formula><p>Where the second equality is the sum of independent random variables. Note that :</p><formula xml:id="formula_211">q N + q (z, z ) = q N - q (z, z ) = n 2 q, [1 -IR(z) qq IR(z) ] = n 2 [1 -Tr(IR(z)) 2 ]</formula><p>≤ n2 zz 0,∼ also that ρ+ q -ρ = o P (1) and ρq -ρ = o P (1). Therefore</p><formula xml:id="formula_212">1 n 2 q f (ȳ q )A q = o P z -z 0,∼ n .</formula><p>Concerning the third term. Using arguments developed previously leads to the same conclusion than before :</p><formula xml:id="formula_213">1 n 2 q [N o q (z) -ρN q (z)](f (ȳ q ) -f (ȳ q )) = o P z -z 0,∼ n .</formula><p>As a conclusion, writing</p><formula xml:id="formula_214">sup z z Λ(z) -Λ(z ) n z -z 0,∼ = sup z z Λ(z) -Λ(z) n z -z 0,∼ + Λ(z) -Λ(z ) n z -z 0,∼</formula><p>and noticing that Λ(z)-Λ(z ) n z-z 0,∼ ≤ 0 since Λ is maximized in z (see 22). We have</p><formula xml:id="formula_215">sup z z Λ(z) -Λ(z ) n z -z 0,∼ = o P (1).</formula><p>76 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 11. MAIN RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.8">Proof of Proposition 26 (contribution of local assignments)</head><p>Proof. By Proposition 18, it is enough to prove that the sum is small compared to p(z , y o ; θ ) on Ω 1 . We work conditionally on z ∈ Z 1 . Choose z in S(z , C) with C defined in proposition 24. For C small enough, we can assume without loss of generality that z is the representative closest to z and note r = zz 0 . Then:</p><formula xml:id="formula_216">LR(θ, z) ≤ Λ(z) -Λ(z) + Λ(z) ≤ Λ(z) -Λ(z) -cρ 3δ(π ) 4 nr ≤ cρ 3δ(π ) 4 nr(1 + o P (1))</formula><p>where the first line comes from the definition of Λ, the second line from Proposition 22 and the third from Proposition 25. Thanks to proposition 29, we also know that:</p><formula xml:id="formula_217">log p(z; θ) p(z ; θ ) ≤ O P (1) exp M c/4 r</formula><p>There are at most n r Q r assignments z at distance r of z and each of them has at most</p><formula xml:id="formula_218">Q Q equivalent configurations. Therefore, z∈S(z ,c) z z p(z, y o ; θ) p(z , y o ; θ ) ≤ O P (1) r≥1 n r Q Q+r exp rM c/4 -cρ 3δ(π ) 4 nr(1 + o P (1)) = O P (1) 1 + e (Q+1) log Q+M c/4 -cρn 3δ(π )(1+o P (1)) 4 n -1 ≤ O P (1)a n exp(a n )</formula><p>where a n = ne (Q+1) log Q+M c/4 -cρn 3δ(π )(1+o P (1))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>= o P (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.9">Proof of Proposition 27 (contribution of equivalent assignments)</head><p>Proof. Choose s permutations of {1, . . . , Q} and assume that z = z ,s . Then p(y o , z; θ) = p(y o , z ,s ; θ) = p(y o , z ; θ s ). If furthermore s ∈ Sym(θ), θ s = θ and immediately p(y o , z; θ) = p(y o , z ; θ). We can therefore partition the sum as 77 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 11. MAIN RESULTS </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.10">Proof of Corollary 18: Behavior of θ M LE</head><p>We may prove the corollary by contradiction. Note first that unless Θ is constrained and with high probability, θ M LE and θ(z ) exhibit no symmetries. Indeed, equalities like y q = y q , have vanishingly small probabilities of being simultaneously true when y ij is discrete, and even null when y ij is continuous. Assume then min s ( </p><formula xml:id="formula_219">α s M LE -α (z )) = o P (1/ √ n) or min s ( π s M LE -π (z )) = o P (1/n)</formula><formula xml:id="formula_220">p(y o , z ; θ ) -max s p y o , z ; θ s M LE p(y o , z ; θ ) = o P (1)</formula><p>which contradicts Eq (11.2) and concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.11">Proof of Corollary 19: Behavior of J (Q, θ)</head><p>Remark first that for every θ and for every z,</p><formula xml:id="formula_221">p (y o , z; θ) ≤ exp [J (δ z , θ)] ≤ max Q∈Q exp [J (Q, θ)] ≤ p (y o ; θ)</formula><p>where δ z denotes the dirac mass on z. By dividing by p (y o ; θ ), we obtain</p><formula xml:id="formula_222">p (y o , z; θ) p (y o ; θ ) ≤ max Q∈Q exp [J (Q, θ)] p (y o ; θ ) ≤ p (y o ; θ) p (y o ; θ ) .</formula><p>78 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">SUB-EXPONENTIAL RANDOM VARIABLES</head><p>As this inequality is true for every couple z, we have in particular:</p><formula xml:id="formula_223">max z∼z p (y o , z; θ) p (y o ; θ ) = max θ ∼θ p y o , z ; θ p (y o ; θ ) ≤ max Q∈Q exp [J (Q, θ)] p (y o ; θ ) .</formula><p>Noticing that p (y o ; θ ) = # Sym(θ )p (y o , z ; θ ) (1 + o p (1)), Theorem 15 therefore leads to the following bounds:</p><formula xml:id="formula_224">max θ ∼θ p y o , z ; θ p (y o , z ; θ ) (1 + o P (1)) ≤ max Q∈Q exp [J (Q, θ)] p (y o , z ; θ ) ≤ # Sym(θ)max θ ∼θ p y o , z ; θ p (y o , z ; θ ) (1 + o P (1)) + o P (1).</formula><p>Again unless Θ is constrained, θ V AR exhibits no symmetries with high probability and the same proof by contradiction as in appendix 11.10 gives the result.</p><p>12 Sub-exponential random variables</p><p>We now prove two propositions regarding sub-exponential variables. Recall first that a random variable X is sub-exponential with parameters (τ</p><formula xml:id="formula_225">2 , b) if for all λ such that |λ| ≤ 1/b, E[e λ(X-E(X)) ] ≤ exp λ 2 τ 2 2 .</formula><p>In particular, all distributions coming from a natural exponential family are sub-exponential. Sub-exponential variables satisfy a large deviation Bernstein-type inequality:</p><formula xml:id="formula_226">P(X -E[X] ≥ t) ≤ exp -t 2 2τ 2 if 0 ≤ t ≤ τ 2 b exp -t 2b if t ≥ τ 2 b (12.1) So that P(X -E[X] ≥ t) ≤ exp - t 2 2(τ 2 + bt)</formula><p>The sub-exponential property is preserved by summation and multiplication.</p><p>• If X is sub-exponential with parameters (τ 2 , b) and α ∈ R, then so is αX with parameters (α 2 τ 2 , αb)</p><formula xml:id="formula_227">• If the X i , i = 1, . . . , n are sub-exponential with parameters (τ 2 i , b i ) and independent, then so is X = X 1 + • • • + X n with parameters ( i τ 2 i , max i b i ) Theorem 20 (Equivalent</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>characterizations of sub-exponential variables). For a zero-mean random variable X, the following statements are equivalent:</head><p>1. There are non-negative numbers (ν, b -1 ) such that</p><formula xml:id="formula_228">E[e λX ] ≤ exp λ 2 ν 2 2 for all |λ| &lt; b.</formula><p>2. There is a positive number c 0 &gt; 0 such that E[e λX ] &lt; ∞ for all |λ| &lt; c 0 .</p><p>3. There are constants c 1 , c 2 &gt; 0 such that P(|X| ≥ t) ≤ c 1 e -c2t for all t &gt; 0. 79 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 13. LIKELIHOOD RATIO OF ASSIGNMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The quantity γ</head><formula xml:id="formula_229">:= sup k≥2 E[X k ] k! 1/k is finite.</formula><p>Proof. A proof of this theorem can be found in <ref type="bibr" target="#b124">Wainwright (2015)</ref>.</p><p>Proposition 28 (Maximum in z). Let (z be any configuration and z the ∼-equivalent configuration that achieves zz 0 = zz 0,∼ let y q = ŷq, (z) (resp. ȳq (z)) and y q = ŷq, (z ) (resp. ȳ q = ȳq (z ) = ψ (π q )) be as defined in Equations (3.1) and (5.3).</p><p>Under the assumptions of the section 2.5, for all ε ≤ κσ 2 , P max</p><formula xml:id="formula_230">z z max k,l N o q (z)(ŷ q, -ȳq ) -N o q (z )( y q -ȳ q ) n z -z 0 &gt; ε = o(1)</formula><p>Proof. Note r = zz 0 . The numerator within the max in the fraction can be expanded to</p><formula xml:id="formula_231">Z q (z) = i,j (z iq z j -z iq z j )(y ij r ij -π z iq z j ρ)</formula><p>and is thus a sum of at most N = nr non-null centered sub-exponential random variables with parameters (a 2 , 1/w). It is therefore a centered sub-exponential with parameters (N a 2 , 1/w). By Bernstein inequality, for all ε ≤ κa 2 we have</p><formula xml:id="formula_232">P(Z ≥ εnr) ≤ exp - nrε 2 2a 2 .</formula><p>There are at most n r Q r Q Q z at . 0,∼ distance r of z . An union bound shows that:</p><formula xml:id="formula_233">P max z z max q, Z q (z) n z -z 0 ≥ ε ≤ r≥1 r= z-z 0,∼ Q 2 P(Z q (z) ≥ εnr) ≤ r≥1 Q Q exp -nrε 2 /2a 2 + r log(nQ) + 2 log(Q) = o(1)</formula><p>where the last equality is true as soon as nε n log n.</p><p>13 Likelihood ratio of assignments Proof. Note then that:</p><formula xml:id="formula_234">p(z; θ) p(z ; θ ) = p(z; α) p(z ; α ) = p(z; α) p(z ; α(z )) p(z ; α(z )) p(z ; α ) ≤ p(z; α(z)) p(z ; α(z )) p(z ; α(z )) p(z ; α ) ≤ exp M c/4 z -z 0 × p(z ; α(z )) p(z ; α ) ≤ O P (1) exp M c/4 z -z 0</formula><p>where the first inequality comes from the definition of α(z) and the second from Lemma B.6 of <ref type="bibr" target="#b19">Brault et al. (2017)</ref> and the fact that z and z are c/4-regular. Finally, local asymptotic normality of the MLE for multinomial proportions ensures that p(z ; α(z )) p(z ;α ) = O P (1). 80 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 14. GENERAL TECHNICAL RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">General technical results</head><p>Proposition 30 (Maximum in z). Let z be a configuration and ŷq, (z) resp. ȳq (z) be as defined in Equations (3.1) and (5.3). Under the assumptions of the section 2.5, for all ε &gt; 0</p><formula xml:id="formula_235">P max z max k,l α q (z) α (z)|ŷ q, -ȳq | &gt; ε ≤ Q n+2 exp - n 2 ε 2 2(σ 2 + κ -1 ε) . (14.1)</formula><p>Additionally, the suprema over all c/2-regular assignments satisfies:</p><formula xml:id="formula_236">P max z∈Z1 max k,l |ŷ q, -ȳq | &gt; ε ≤ Q n+2 exp - n 2 c 2 ε 2 8(σ 2 + κ -1 ε) . (14.2)</formula><p>Note that equations 14.1 and 14.2 remain valid when replacing c/2 by any c &lt; c/2.</p><p>Proof. The random variables Y ij R ij are sub-exponential with parameters (ν 2 , 1/b). Conditionally to z , z +q z + (ŷ q, -ȳq ) is a sum of z +q z + centered sub-exponential random variables. By Bernstein's inequality <ref type="bibr" target="#b16">Boucheron et al. (2013)</ref>, we therefore have for all t &gt; 0</p><formula xml:id="formula_237">P(z +q z + |ŷ q, -ȳq | ≥ t) ≤ 2 exp - t 2 2(z +q z + ν 2 + b -1 t) In particular, if t = n 2 y, P ( α q (z) α (z)|ŷ q, -ȳq | ≥ x) ≤ 2 exp - n 2 y 2 2( α q (z) α (z)ν 2 + b -1 y) ≤ 2 exp - n 2 y 2 2(ν 2 + b -1 y)</formula><p>uniformly over z. Equation (14.1) then results from a union bound. Similarly,</p><formula xml:id="formula_238">P (|ŷ q, -ȳq | ≥ x) = P ( α q (z) α (z)|ŷ q, -ȳq | ≥ α q (z) α (z)y) ≤ 2 exp - n 2 y 2 α q (z) 2 α (z) 2 2( α q (z) α (z)ν 2 + b -1 y α q (z) α (z)) ≤ 2 exp - n 2 c 2 y 2 8(ν 2 + b -1 y)</formula><p>Where the last inequality comes from the fact that c/2-regular assignments satisfy α q (z) α (z) ≥ c 2 /4. Equation (14.2) then results from a union bound over Z 1 ⊂ Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemme 3. If X is a zero mean random variable, sub-exponential with parameters</head><formula xml:id="formula_239">(σ 2 , b), then |X| is sub-exponential with parameters (8σ 2 , 2 √ 2b). Proof. Note µ = E|X| and consider Y = |X| -µ. Choose λ such that |λ| &lt; (2 √ 2b) -1 . We need to bound E[e λY ]. Note first that E[e λY ] ≤ E[e λX ] + E[e -λX</formula><p>] &lt; +∞ is properly defined by sub-exponential property of X and we have</p><formula xml:id="formula_240">E[e λY ] ≤ 1 + k=2 |λ| k E[|Y | k ] k!</formula><p>where we used the fact that E[Y ] = 0. We know bound odd moments of |λY |.</p><formula xml:id="formula_241">E[|λY | 2k+1 ] ≤ (E[|λY | 2k ]E[|λY | 2k+2 ]) 1/2 ≤ 1 2 (λ 2k E[Y 2k ] + λ 2k+2 E[Y 2k+2 ])</formula><p>81 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">GENERAL TECHNICAL RESULTS</head><p>where we used first Cauchy-Schwarz and then the arithmetic-geometric mean inequality. The Taylor series expansion can thus be reduced to</p><formula xml:id="formula_242">E[e λY ] ≤ 1 + 1 2 + 1 2.3! E[Y 2 ]λ 2 + +∞ k=2 1 (2k)! + 1 2 1 (2k -1)! + 1 (2k + 1)! λ 2k E[Y 2k ] ≤ +∞ k=0 2 k λ 2k E[Y 2k ] (2k)! ≤ +∞ k=0 2 3k λ 2k E[X 2k ] (2k)! = E cosh 2 √ 2λX = E e 2 √ 2λX + e -2 √ 2λX 2 ≤ e 8λ 2 σ 2 2</formula><p>where we used the well-known inequality</p><formula xml:id="formula_243">E[|X -E[X]| k ] ≤ 2 k E[|X| k ] to substitute 2 2k E[X 2k ] to E[Y 2k ].</formula><p>Proposition 31 (concentration for sub-exponential). Let X 1 , . . . , X n be independent zero mean random variables, sub-exponential with parameters</p><formula xml:id="formula_244">(σ 2 i , b i ). Note V 2 0 = i σ 2 i and b = max i b i .</formula><p>Then the random variable Z defined by:</p><formula xml:id="formula_245">Z = sup Γ∈IR n Γ ∞≤M i Γ i X i is also sub-exponential with parameters (8M 2 V 2 0 , 2 √ 2M b). Moreover E[Z] ≤ M V 0 √ n so that for all t &gt; 0, P(Z -M V 0 √ n ≥ t) ≤ exp - t 2 2(8M 2 V 2 0 + 2 √ 2M bt) (14.3) Proof. Note first that Z can be simplified to Z = M i |X i |. We just need to bound E[Z].</formula><p>The rest of the proposition results from the fact that the |X i | are sub-exponential (8σ 2 i , 2 √ 2b i ) by Lemma 3 and standard properties of sums of independent re-scaled subexponential variables.</p><formula xml:id="formula_246">E[Z] = E    sup Γ∈IR n Γ ∞≤M i Γ i X i    = E i M |X i | ≤ M i E[X 2 i ] = M i σ i ≤ M i 1 1/2 i σ 2 i 1/2 = M V 0 √ n using Cauchy-Schwarz.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemme 4.</head><p>Let Z 1 be the subset of Z of c-regular configurations, as defined in Definition 11. Let</p><formula xml:id="formula_247">S Q = {α = (α 1 , α 2 , . . . , α Q ) ∈ [0, 1] Q : Q k=1 α k = 1} be the Q-dimensional simplex and note S Q c = S Q ∩ [c, 1 -c] Q .</formula><p>Then there exists two positive constants M c and M c such that for all z, z in Z 1 and all α ∈ S Q c |log p(z; α(z)) -log p(z ; α(z ))| ≤ M c zz 0 82 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 14. GENERAL TECHNICAL RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Consider the entropy map H</head><formula xml:id="formula_248">: S Q → R defined as H(α) = - Q k=1 α k log(α k ). The gradient ∇H is uniformly bounded by Mc 2 = log 1-c c in . ∞ -norm over S Q ∩ [c, 1 -c] Q . Therefore, for all α, α ∈ S Q ∩ [c, 1 -c] Q , we have |H(α) -H(α )| ≤ M c 2 α -α 1</formula><p>To prove the inequality, we remark that z ∈ Z 1 translates to α(z)</p><formula xml:id="formula_249">∈ S Q ∩ [c, 1 -c] Q , that log p(z; α(z))-log p(z ; α(z )) = n[H( α(z))-H( α(z ))] and finally that α(z)-α(z ) 1 ≤ 2 n z -z 0 .</formula><p>83 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 3 14. GENERAL TECHNICAL RESULTS 84 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This chapter deals with missing data in the Stochastic Block Model (in short SBM) with covariates. The case with no covariates and missing data in the SBM has already been treated in Chapter 2. We follow this work and introduce three different models of SBM with covariates on nodes or dyads. In order to explore the link between the sampling design and the covariates we define one sampling design centered on dyads and another one centered on nodes, both depending on covariates. We exhibit a specific case where a sampling design which is MAR conditionally on covariates becomes NMAR when the covariates are not observed. We show explicitly the equivalence between the two models and run simulations to compare the inference with a MAR model conditionally on covariates, NMAR model and a MAR model. We also propose another case where there is no explicit equivalence but where the NMAR model leads to better inference than a MAR model when the covariates are missing.</p><p>Related works. Stochastic Block Models with the adding of covariates are popular to model heterogeneity in networks. The main purpose of accounting for covariates in the model is to remove covariates effects on clustering. In <ref type="bibr" target="#b118">Tallberg (2004)</ref>, the authors proposed a Bayesian approach to model an SBM with covariates with a multinomial probit model where distribution of the block given by the latent variable Z i depends on the vector of covariates X i . As a consequence, in this model covariates act on dyads through nodes memberships. When covariates are associated to dyads, <ref type="bibr" target="#b88">Mariadassou et al. (2010)</ref> combined these covariates with the hidden structure using the framework of the generalized linear model, for example in the binary SBM with covariates</p><formula xml:id="formula_250">Y ij |z i = q, z j = , X ij ∼ ind B(logistic(γ q + β T X ij )). (1.1)</formula><p>Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STOCHASTIC BLOCK MODELS WITH COVARIATES AND SAMPLING MODELS</head><p>A very close model is proposed in <ref type="bibr" target="#b130">Zanghi et al. (2010)</ref> with Gaussian distribution on dyads. In these frameworks, heterogeneity is modeled by parameters (γ q ) q corresponding when X = 0 to inter and intra block probabilities (or connectivity matrix) in the binary SBM. It accounts for heterogeneity that is not explained by the regression term β T X ij . A similar interpretation is given in <ref type="bibr">Choi et al. (2012b)</ref> who first apply a regression step and then perform SBM inference on the residuals. SBM with covariates has also been applied in <ref type="bibr" target="#b92">Miele et al. (2014)</ref> in a spatial data context where nodes correspond to entities that have explicit geographic locations. If no hidden structure is supposed, <ref type="bibr" target="#b103">Robins et al. (2007)</ref> show how to incorporate covariates into a graph model. Alternative models are used to model covariates in SBM, like in <ref type="bibr" target="#b113">Sweet (2015)</ref> where authors use a hierarchical Bayesian model. When the principal purpose is community detection, the previous models enable to detect structure in the network beyond the effect of covariates while the following references focus on detecting structure by using both covariates and the network. We cite <ref type="bibr" target="#b131">Zhang et al. (2016)</ref> which is based on a joint community detection criterion and <ref type="bibr" target="#b14">Binkiewicz et al. (2017)</ref> using spectral properties of the adjacency matrix of a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stochastic Block Models with covariates and sampling models</head><p>We will consider three different structures in which covariates and missing data impact the SBM. Conditional dependencies of these models are represented with a DAG in Figure <ref type="figure">4</ref>.1.</p><p>We choose to study only cases where covariates represented by X impact either the sampling design or the network model directly. Furthermore, note that the systematic edge between Z and Y is part of the SBM. Finally, we do not consider any edge between nodes Y, Z and node R since we require that conditionally on X the missing data are MAR. In the first model, covariates influence directly latent variables and the sampling. In the second model covariates influence also the sampling and the distribution of edges. Finally, in the last model, covariates influence latent variables, the sampling and the distribution of edges.</p><formula xml:id="formula_251">X Z Y R X Z Y R X Z Y R (model 1) (model 2) (model 3) Figure 4</formula><p>.1 -DAGs of relationships between Y, Z, R and X considered in the framework of missing data for SBM with covariates.</p><formula xml:id="formula_252">Model 1 Writing α • = (α •1 , ..., α •Q ) ∈ [0, 1] Q we define α iq = e β t q Xi1 {q =Q} 1 + Q-1 k=1 β t k X i , ∀(i, q) ∈ N × Q, with β q ∈ R N for all q ∈ 1, Q -1 and β Q = 0. Then we have Z i | X i ∼ iid M(1, α i ), ∀i ∈ N , Y ij | Z i , Z j ∼ ind B(π zizj ), ∀(i, j) ∈ N 2 ,</formula><p>with π q ∈ [0, 1], (q, ) ∈ Q 2 . 86 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STOCHASTIC BLOCK MODELS WITH COVARIATES AND SAMPLING MODELS</head><p>Model 2</p><formula xml:id="formula_253">Z i ∼ iid M(1, α), ∀i ∈ N , Y ij | Z i , Z j , X ij ∼ ind B(g(γ zizj + β t X ij )), ∀(i, j) ∈ N 2 , where γ ∈ M Q ([0, 1]), β ∈ R m , α ∈ [0, 1] Q , g(x) = (1 + e -x</formula><p>) -1 . Note that if the covariates are linked with nodes, the covariates are transferred to dyads by using a "similarity" symmetric function φ(•,</p><formula xml:id="formula_254">•) : R N × R N → R m such that X ij = φ(X i , X j ) for all (i, j) in D. Model 3 Writing α • = (α •1 , ..., α •Q ) ∈ [0, 1] Q we define α iq = e β t q Xi1 {q =Q} 1 + Q-1 k=1 β t k X i , ∀(i, q) ∈ N × Q, with β q ∈ R N for all q ∈ 1, Q -1 and β Q = 0. Then we have Z i | X i ∼ iid M(1, α i ), ∀i ∈ N , Y ij | Z i , Z j , X ij ∼ ind B(g(γ zizj + β t X ij )), ∀(i, j) ∈ N 2 ,</formula><p>with parameters and notations defined as in Model 2.</p><p>In the three previous models, we need to specify the distribution R|X. This distribution can be naturally one of the two following distributions depending whether X is a set of covariates on nodes or on dyads. We then define two sampling designs, one dyad-centered and the other node-centered. In both sampling, the probability to observe a dyad (resp. node) depends on the value of the covariate. If the covariates have no effect, then the probability to observe a dyad (resp. node) is independent of the dyad (resp. node).</p><p>Definition 6 (Dyad sampling). Let δ ∈ R, κ ∈ R m . The probability to observe a dyad is</p><formula xml:id="formula_255">P(R ij = 1|X ij ) = g(δ + κ t X ij ), with ψ = {δ, κ}.</formula><p>For all i ∈ N , we denote by</p><formula xml:id="formula_256">V i = 1 if the node i is observed, V i = 0 otherwise. Then, if V i = 1 we have R ij = 1 for all j ∈ N .</formula><p>Definition 7 (Node sampling). Let ν ∈ R and η ∈ R N . The probability to observe all dyads corresponding to a node is</p><formula xml:id="formula_257">P(V i = 1|X i ) = g(ν + η t X i ),</formula><p>with ψ = {ν, η}.</p><p>In the next paragraph, we exhibit a particular case where we can provide an equivalence between a MAR model conditionally on covariates and a NMAR model when covariates are not observed. Indeed, when sampling a network, some covariates may not be available. In this case, a NMAR model may compensate for the absence of covariates.</p><p>A case of equivalence. In this paragraph, we demonstrate on a specific case that an SBM with covariates where the missing data are MAR conditionally on covariates can be equivalent to an SBM without covariates with missing data NMAR. We first recall the definition of the block-node sampling design where nodes are sampled depending on the group they belong to. 87 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STATISTICAL INFERENCE</head><p>Definition 8 (Block-node sampling). Let ρ = (ρ q ) 1≤q≤Q ∈ [0, 1] Q . In block-node sampling, the conditional distribution of V|Z is given by</p><formula xml:id="formula_258">V i |Z i ∼ ind B(ρ zi ).</formula><p>(2.1) Proposition 32. Let's define the covariates matrix</p><formula xml:id="formula_259">X := [X 1 , ..., X n ] ∈ M Q×n ({0, 1}) for Q &gt; 1.</formula><p>The model is generated as follow</p><formula xml:id="formula_260">X i ∼ iid M(1, α), ∀i ∈ N , P(Z ia = 1|X ia = 1) = δ and P(Z ib = 1|X ia = 1) = 1 -δ Q -1 , b = a, ∀i ∈ N .</formula><p>The probabilities to observe a dyad are given by</p><formula xml:id="formula_261">p q = P(V i = 1|X iq = 1).</formula><p>Then, this model corresponds to a conventional SBM under block-node sampling, with parameters</p><formula xml:id="formula_262">ρ q = P(V i = 1|Z iq = 1), = δp q α q + 1-δ Q-1 =q p α δα q + 1-δ Q-1 (1 -α q ) . Remark 1. δ = 1 ⇒ ρ q = p q .</formula><p>This result illustrate the article <ref type="bibr" target="#b93">Molenberghs et al. (2008)</ref> which states that any model generating missing data that are MAR has a dual model generating missing data NMAR and fitting equally the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Statistical inference</head><p>On the basis of Figure <ref type="figure">4</ref>.1, the type of missingness for SBM is defined as follows:</p><p>Sampling design for SBM is</p><formula xml:id="formula_263">     MCAR if R |= (Y m , Z, Y o ) | X, MAR if R |= (Y m , Z) | (Y o , X), NMAR otherwise. (3.1) Proposition 33. From (3.1), if the sampling is MAR or MCAR then maximizing p θ,ψ (Y o , R) or p θ (Y o ) in θ is equivalent.</formula><p>In the light of (3.1), sampling designs defined in definitions 6 and 7 are missing completely at random (MCAR) conditionally on covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference of Model 1</head><p>In the MAR case, inference can be conducted directly and without bias on the observed part of the adjacency matrix. We start by recalling the complete likelihood p θ (Y o , Z|X) which has an explicit form contrary to the likelihood of the observed data p θ (Y o |X).</p><p>The complete log-likelihood restricted to the observed variables is</p><formula xml:id="formula_264">log p θ (Y o , Z|X) = (i,j)∈D o q, Z iq Z j log b(Y ij , π q ) + i∈N o q Z iq log (α iq ) , (3.2) with b(x, π) = π x (1 -π) 1-</formula><p>x the Bernoulli probability density function. 88 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STATISTICAL INFERENCE</head><p>By Equation (3.2) and "mean-field" approximation, the close form of the lower bound is</p><formula xml:id="formula_265">J τ ,θ (Y o |X) = (i,j)∈D o q, τ iq τ j log b(Y ij , π q ) + i∈N o q τ iq log(α iq /τ iq ).</formula><p>(3.3)</p><p>We refer to the notations defined page 5 for D o and N o . The two maximization problems are solved as stated in the following proposition, straightforwardly derived from <ref type="bibr" target="#b36">Daudin et al. (2008)</ref>.</p><p>Proposition 34. Consider the lower bound J τ ,θ (Y o |X) given by (3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The parameter</head><formula xml:id="formula_266">π maximizing J π (Y o |X) when τ is held fixed is πq = (i,j)∈D o τiq τj Y ij (i,j)∈D o τiq τj , α = arg max α J τ ,θ (Y o |X)</formula><p>has no explicit form and must be estimated with an optimization algorithm. Corresponding gradient is</p><formula xml:id="formula_267">∂J τ ,θ (Y o |X) ∂α iq = τ iq α iq . (3.4)</formula><p>2. The variational parameters τ maximizing J τ (Y o |X) when θ is held fixed are obtained thanks to the following fixed point relation:</p><formula xml:id="formula_268">τiq ∝ α iq   (i,j)∈D o ∈Q b(Y ij ; π q ) τj   .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference of Model 2</head><p>The complete log-likelihood for Model 2 is</p><formula xml:id="formula_269">log p θ (Y o , Z|X) = (i,j)∈D o Y ij g Z i t γZ j + β t X ij + (1 -Y ij ) log(1 -g Z i t γZ j + β t X ij ) + i∈N o Z t i log (α) , (3.5) with h(x) = log(g(x)) = -log(1 + e -x</formula><p>). As a consequences the variational lower bound is given by 1. The parameters γ and β maximizing J τ ,θ (Y o |X) when all other parameters are held fixed have no explicit forms and must be estimated with an optimization algorithm.</p><formula xml:id="formula_270">J τ ,θ (Y o |X) = (i,j)∈D o q, τ iq τ j (Y ij -1)(γ q + β t X ij ) + h(γ q + β t X ij ) (3.6) + i,q τ iq log α q τ iq . (<label>3</label></formula><p>Corresponding gradients are</p><formula xml:id="formula_271">∂J τ ,θ (Y o |X) ∂γ q = (i,j)∈D o τ iq τ j Y ij -1 + e -(γ q +β t Xij ) 1 + e -(γ q +β t Xij ) , (3.8) ∂J τ ,θ (Y o |X) ∂β k = (i,j)∈D o q, τ iq τ j (X ij ) k Y ij -1 + e -(γ q +β t Xij )</formula><p>1 + e -(γ q +β t Xij ) . (3.9) 89 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4</p><formula xml:id="formula_272">3. STATISTICAL INFERENCE Concerning α, αq = i∈N o τiq card (N o ) .</formula><p>2. The optimal τ in J τ ,θ (Y o |X) when all other parameters are held fixed verify</p><formula xml:id="formula_273">τiq ∝ α q (i,j)∈D o exp (Y ij -1)(γ q + β t X ij ) + h(γ q + β t X ij ) τj .</formula><p>(3.10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference of Model 3</head><p>The inference of Model 3 can be done directly by combination of Model 1 and Model 2 estimation procedures. The complete log-likelihood for Model 3 is</p><formula xml:id="formula_274">log p θ (Y o , Z|X) = (i,j)∈D o Y ij g Z i t γZ j + β t X ij + (1 -Y ij ) log(1 -g Z i t γZ j + β t X ij ) + i∈N o Z t i log (α i ) , (3.11) with h(x) = log(g(x)) = -log(1 + e -x</formula><p>). As a consequences the variational lower bound is given by 1. The parameters α, γ and β maximizing J τ ,θ (Y o |X) when all other parameters are held fixed have no explicit forms and must be estimated with an optimization algorithm.</p><formula xml:id="formula_275">J τ ,θ (Y o |X) = (i,j)∈D o q, τ iq τ j (Y ij -1)(γ q + β t X ij ) + h(γ q + β t X ij ) (3.12) + i,q τ iq log α iq τ iq . (<label>3</label></formula><p>Corresponding gradients are the same as in Equations (3.4), (3.8) and (3.9).</p><p>2. The variational parameters τ maximizing J τ ,θ (Y o |X) when θ is held fixed are obtained thanks to the following fixed point relation:</p><formula xml:id="formula_276">τiq ∝ α iq (i,j)∈D o exp (Y ij -1)(γ q + β t X ij ) + h(γ q + β t X ij ) τj .</formula><p>(3.14)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model selection</head><p>The Integrated Classification Likelihood criterion was introduced by <ref type="bibr" target="#b13">Biernacki et al. (2000)</ref> for mixture models, where the likelihood -and thus BIC -is usually intractable. <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> adapted a variational ICL in the context of SBM. Under MAR condition, this criterion requires a slight adaptation stated in the following proposition.</p><p>Proposition 37 (Model selection). For an SBM with Q blocks and for θ = arg max log p θ (Y o , Z), the ICL criterion is given by</p><formula xml:id="formula_277">ICL(Q) = -2E pτ log p θ (Y o , Z; Q, X) + pen Q , and pen Q =          Q(Q+1) 2 log card (D o ) + N (Q -1) log card (N o ) (in Model 1), Q(Q+1) 2 + m log card (D o ) + (Q -1) log card (N o ) (in Model 2), Q(Q+1) 2 + m log card (D o ) + N (Q -1) log card (N o ) (in Model 3), (3.15)</formula><p>Note that a dyad is only counted once since we work with symmetric networks. The number of blocks chosen is the one associated to the lowest ICL. 90 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4 4. ILLUSTRATIONS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Illustrations</head><p>We consider in this section two cases where the NMAR modeling appears like a good alternative when some covariates are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Example 1</head><p>In this Section we illustrate Proposition 32 on a simple example available in Figure <ref type="figure">4</ref>.2. In this example, we consider an SBM with covariates with n = 100 nodes following Model 1. Furthermore, the network is sampled according to a block-node sampling with sampling parameters p = (.8, .2, .5) corresponding to probabilities to observe a node conditionally to his covariate. As a consequence, with these parameters, the sampling rate is varying in the interval <ref type="bibr">(.46, .828</ref>]. Finally the parameter δ is varying and takes three different values : 1, 2/3 and 1/3. The greater δ the less X depends on Z.</p><p>In Figure <ref type="figure">4</ref>.2, boxplots correspond to estimations of the connectivity matrix on the left and to the adjusted Rand index (in short <ref type="bibr">ARI Rand, 1971)</ref> between the true clustering and the predicted one on the right. We run the VEM algorithms for SBM with block-node sampling (NMAR) and with node sampling (MAR) described in <ref type="bibr">(Tabouy et al., 2019a)</ref> and finally VEM for SBM with covariates corresponding to Model 1 (see Section 3.1).</p><p>Figure <ref type="figure">4</ref>.2 shows that in this example, making the MAR assumption is not the best way to deal with missing data when covariates are missing. Furthermore, we see that the estimation of the clustering and of π is made with the same accuracy for VEM_NMAR and VEMCov1, moreover results do not depend on the value of the parameter δ. Finally, note that the estimations of parameter ρ (results not reported here), linked to parameter p as described in Proposition 32, appears to be good in terms of Frobenius norm when δ is equal to 1 and 2/3 but is degrading when δ gets low as when it is equal to 1/3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Example 2</head><p>Considering an SBM with covariates on dyads with missing entries, we explore various network topologies (namely affiliation, star and bipartite). The connectivity matrix represented in SBM with covariates context by the matrix g -1 (γ) of which are given in Figure <ref type="figure">4</ref>.3. This parameter is classically considered (see <ref type="bibr" target="#b88">Mariadassou et al., 2010)</ref> as monitoring heterogeneity in the SBM and then the topology of the network.</p><p>In the following, we will consider that each dyad has one covariate and that they are generated conditionally to latent variables Z as following :</p><formula xml:id="formula_278">X ij = 1 {zi=zj } (1 -B ij ) + (1 -1 {zi=zj } )B ij , (4.2)</formula><p>where B ij are independent Bernoulli variables with the tuning parameter p. According to these covariates, the SBM is as in model 3 with the specificity that we give here the conditional distribution of X|Z and not the inverse as suggested in Figure <ref type="figure">4</ref>.1. A simple application of Bayes formula allow us to find the conditional distribution of Z|X if wanted.</p><p>Coming back to the definition of X ij , note that the smaller the parameter p, the more X depend on Z. The value p = 0 corresponds to the case where X encode exactly Z. This particular case when X is observed is called "Oracle" in order to show the response of the VEM described in Section 3.2 when covariates are available. Cases corresponding to p = .2 and p = .5 are explored in Figure <ref type="figure">4</ref>.5. In parallel of the Oracle we run VEM for SBM without 91 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4 4. ILLUSTRATIONS δ = 2/3 q q q q q q q q q q q q 0.00 δ = 1/3 q q q q q q q q q 0.00  </p><formula xml:id="formula_279">π -π F / π F ARI(Z, Ẑ) δ = 1 q q q 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>92</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4 4. ILLUSTRATIONS covariates and missing data from a double-standard sampling. This design states that every edge is sampled with probability ρ 1 and every non-edge is sampled with probability ρ 0 . It implies that missing data are NMAR. We also run random-dyad sampling where every dyad is sampled with probability ρ. It implies that missing data are MAR. More details are available in <ref type="bibr">Tabouy et al. (2019a)</ref> about these sampling designs and estimation methods. They are respectively called "NMAR" and "MAR". Simulated networks have n = 100 nodes. Marginal probabilities α are chosen specifically for affiliation, star and bipartite topologies, respectively (1/3, 1/3, 1/3), <ref type="bibr">(.15, .35, .15, .35)</ref>  <ref type="table"></ref>and<ref type="table">(1/4, 1/4, 1/4, 1/4</ref>). The sampling parameters δ is fixed at 1 and κ takes iteratively values <ref type="bibr">(-1.4, -.8, -.25, .1)</ref> in order to have a panel of sampling rates. Algorithms are initialized with spectral clustering.</p><p>Ŷm -Y m F /nm ARI(Z, Ẑ) affiliation q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.2 0.4 0.6 0.8 [0.169,0.294] (0.294,0.432] (0.432,0.587] (0.587,0.745] q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 0.25 0.50 0.75 1.00</p><p>[0.169,0.294] (0.294,0.432] (0.432,0.587] (0.587,0.745] bipartite q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 method Oracle VEM_mar VEM_nmar star q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0.1 0.2 0.3 0.4 0.5 [0.137,0.254] (0.254,0.488] (0.488,0.695] (0.695,0.732] q q q q q q q q q q q q q q q q q q q q q q q 0.00 Simulations on Figure <ref type="figure">4</ref>.4 show that "NMAR" algorithm is a good alternative when covariates are unknown whereas "MAR" algorithm does not estimate well parameters and does not predict correctly the clustering. However, the more the sampling rate increases, the more similar are the performance of algorithms "NMAR" and "MAR". On Figure <ref type="figure">4</ref>.5, where parameter p denoting how much X encode Z, the difference between "NMAR" and "MAR" decrease when p increase. Indeed, "NMAR" algorithm accuracy is linked to parameter p in the way that it is greater as much as p is close to 0, which corresponds to the case where X 93 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 4 5. CONCLUSION depends the most on Z. We have not yet fully understood the reason for this phenomenon, it would be worth studying.</p><p>Ŷm -Y m F /nm ARI(Z, Ẑ) p = 0.2 q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.2 0.4 0.6 0.8 [0.203,0.341] (0.341,0.486] (0.486,0.631] (0.631,0.764] q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 p = 0.5 q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.3 0.4 0.5 0.6 [0.249,0.419] (0.419,0.559] (0.559,0.686] (0.686,0.809] q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we described several combining sampling of a network generated under an SBM with covariates. Algorithms have been proposed to estimate parameters of the models and recover the clustering. Furthermore, we investigate cases where some covariates are not available and show through two examples where NMAR modeling can be a better alternative than MAR modeling. During our exploration, we encounter many simulation settings leading to similar performances for MAR and NMAR models (simulations not reported here). These considerations would deserve further investigations to understand the relatively good robustness of the MAR modeling. Note that however, we never found MAR approximation to be better that NMAR approximation. 94 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 Chapter 5 missSBM: An R Package for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handling Missing Values in the Stochastic Block Model</head><p>This chapter is available as a preprint on arXiv <ref type="bibr">(Tabouy et al., 2019b)</ref>. The Stochastic Block Model (SBM) is a popular probabilistic model for random graph.</p><p>It is commonly used to perform clustering on network data by aggregating nodes that share similar connectivity patterns into blocks. When fitting an SBM to a network which is partially observed, it is important to account for the underlying process that originates the missing values, otherwise the inference may be biased. This paper introduces missSBM, an R-package fitting the SBM when the network is partially observed, i.e. the adjacency matrix contains not only 1 or 0 encoding presence or absence of edges but also NA encoding missing information between pairs of nodes. It implements a series of algorithms for the binary SBM, with the possibility of accounting for covariates if needed, by performing variational inference for several sampling mechanisms, the methodology of which is detailed in <ref type="bibr">Tabouy et al. (2019a)</ref>. Our implementation automatically explores different block numbers to select the most relevant according to the Integrated Classification Likelihood (ICL) criterion. The ICL criterion can also help to determine which sampling mechanism fits the best the data. Finally, missSBM can be used to perform imputation of missing entries in the adjacency matrix. We illustrate the package on a network data set consisting in interactions between blogs sampled during the French presidential election in 2007.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION 1 Introduction</head><p>In many fields of science, networks are a natural way to represent interaction data. To cite a few examples, a network may represent social interactions such as friendship or collaboration between people in a social network, regulation between genes and their products in a gene regulatory network, or predation between animals in a food web. Notice that we only consider here networks which can be represented by graphs composed by simple edges connecting pairs of nodes (also referred to as dyads in the following). At this day, there exist many softwares performing network-related analyzes. Unsurprisingly, the R community is extremely active in this area. Indeed, the R programming language is especially well-designed for performing data manipulation and visualization, and thus appropriate for handling network data. Among the many available R packages related to networks, we suggest a classification into three groups: i) Packages performing representation, manipulations, visualization tasks, and/or packages computing descriptive statistics on networks. This group clearly occupies the first place in terms of number of packages. We may cite non exhaustively the following top representatives: igraph <ref type="bibr" target="#b35">(Csardi and Nepusz, 2006)</ref>, network <ref type="bibr">(Butts, 2008a)</ref>, statnet <ref type="bibr" target="#b54">(Handcock et al., 2008)</ref>, or sna <ref type="bibr">(Butts, 2008b)</ref>.</p><p>ii) Packages fitting (probabilistic) models on network data. Most of the existing packages in this group are dedictated to the estimation of a specific network model: important examples include ergm <ref type="bibr" target="#b64">(Hunter et al., 2008)</ref>, fitting the family of exponential random graph models introduced in Hunter and <ref type="bibr">Handcock (2006b)</ref>; mixer <ref type="bibr" target="#b4">(Ambroise et al., 2015)</ref> and blockmodels <ref type="bibr" target="#b80">(Leger, 2016)</ref>, fitting the family of Stochastic Block Models <ref type="bibr" target="#b60">(Holland et al., 1983 and</ref><ref type="bibr">Nowicki and</ref><ref type="bibr" target="#b96">Snijders, 2001)</ref>; or latentnet <ref type="bibr" target="#b71">(Krivitsky and Handcock, 2008)</ref>, implementing the latent space approach of <ref type="bibr" target="#b58">Hoff et al. (2002)</ref>.</p><p>iii) Packages learning the structure of a network from an external source of data, such as huge <ref type="bibr">(Zhao et al., 2012a)</ref>, or bnstruct <ref type="bibr" target="#b45">Franzin et al. (2017)</ref>. These packages generally rely on an specific graphical modeling of the data (e.g., Gaussian graphical models <ref type="bibr" target="#b78">(Lauritzen, 1996)</ref> in huge, or Bayesian networks <ref type="bibr" target="#b98">(Pearl, 2011)</ref> in bnstruct).</p><p>In addition to this brief typology, the interested reader may consult the CRAN task view on the related topic of graphical modeling <ref type="bibr" target="#b59">(Hojsgaard, 2019)</ref>.</p><p>The package missSBM that we introduce here belongs to the second category, that is, softwares that fit a probabilistic model to network data. missSBM is dedicated to the estimation of the stochastic block model (SBM). The SBM is a mixture of Erdős-Rényi random graphs <ref type="bibr" target="#b43">(Erdős and Renyi, 1959)</ref> that allows a high degree of heterogeneity in connectivity profiles. As a consequences it generally fits well real-world network data, while retaining the advantage of being a generative model (contrary to mechanistic approaches such as the <ref type="bibr">Barabási-Albert model (Albert and Barabási, 2002)</ref>, defined by a preferential attachment algorithm). The main outcome of the SBM inference on a network is a clustering of its nodes -or "blocks" sharing the same connectivity properties.</p><p>Even though there already exist efficient R packages for SBM inference such as Blockmodels <ref type="bibr" target="#b79">(Leger, 2015)</ref> and mixer <ref type="bibr" target="#b4">(Ambroise et al., 2015)</ref>, they can only be applied to complete data sets. The main feature introduced in missSBM is to deal with cases where the network data is only partially observed. More precisely, we consider situations where the adjacency matrix encoding the network data contains contains not only 1 or 0 encoding presence or absence of edges but also NA encoding missing information between dyads. In the presence of missing data, it is important to account for the underlying process that originates the missing values in the estimation of a probabilistic model, otherwise estimation of the model parameters may be biased. In particular, one has to take the type of missing data mechanisms into account (Missing at Random or Not, see <ref type="bibr" target="#b105">Rubin, 1976)</ref>. This issue has been 96 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 2. STATISTICAL FRAMEWORK studied in the context of network data in <ref type="bibr" target="#b55">Handcock and Gile (2010)</ref> for exponential random graph models and in our previous paper <ref type="bibr">Tabouy et al. (2019a)</ref> for stochastic block models. The package missSBM is an implementation of the methodology developed therein.</p><p>Specifically, missSBM implements variational algorithms in the vein of <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> for estimating the SBM for binary network data, with or without covariates, under various missing data mechanisms. This includes cases of incomplete data where the inference can be made only on the observed part of the data (Missing at Random), or cases where it is necessary to take the sampling design in the inference into account (Not Missing at Random). Although version 0.2.0 of missSBM only deals with binary networks (either directed or not), we deploy a structure that let the possibility to easily include other variants in the future by adopting an oriented-object programming spirit thanks to R6-classes and the R6 package of <ref type="bibr" target="#b25">Chang (2017)</ref>. In particular, extending missSBM to weighted SBM with exponential distributions of the edges <ref type="bibr" target="#b88">(Mariadassou et al., 2010</ref>) should be straightforward.</p><p>The paper is organized as follows: Section 2 briefly introduces the statistical framework of the binary SBM, with or without covariates, and summarizes the key points for estimating the SBM under missing data condition, further detailed in <ref type="bibr">Tabouy et al. (2019a)</ref>. Section 3 presents the package structure; Section 4 provides basic user guidelines. We finally detail in Section 5 a case study which analyzes a network data set describing the French blogosphere during French presidential election of 2007, illustrating the most striking features of the package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Binary Stochastic Block Model (SBM)</head><p>In an SBM, nodes from a set N 1, n are distributed among a set Q 1, Q of hidden blocks that model the latent structure of the graph. The group memberships are described by categorical variables (Z i , i ∈ N ) with multinomial distribution M(1, α = (α 1 , ..., α Q )). The probability of an edge between any pair of nodes (or dyad) in D N × N only depends on the blocks the two nodes belong to. Hence, the presence of an edge between i and j, indicated by the binary variable Y ij , is independent on the other edges conditionally on the latent blocks:</p><formula xml:id="formula_280">Y ij |Z i , Z j ∼ ind B(π ZiZj ), ∀(i, j) ∈ N 2 , (2.1)</formula><p>where B stands for the Bernoulli distribution. In the following, we denote by π = (π q ) (q, )∈Q 2 the connectivity matrix, α the mixture parameters, Z = (Z 1 , ..., Z n ) T the n×Q membership matrix and Y = (Y ij ) (i,j)∈D the n × n adjacency matrix. The vector encompassing all the unknown model parameters is θ = (α, π). A schematic representation of the binary SBM in the undirected case is given in Figure <ref type="figure" target="#fig_25">5</ref>.1, where we highlight the latent clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Accounting for External Covariates</head><p>On top of information about connections between nodes, it is common for network data to be accompanied with additional information on nodes or dyads, that we call covariates: for instance in social network, nodes may belong to different categories (gender, occupation, nationality). Covariates on dyads usually represent distances between nodes covariates: for example in a context of spatial data where nodes corresponds to entities that have explicit geographic locations, dyad covariates may be the distances between the nodes. Depending on the analysis, we may want to detect a connectivity pattern beyond the covariate effect. To do so, we present here a variant of the SBM implemented in missSBM that allows the user to include covariates in the model. Let X ij denote the vector of length m of covariates for dyad (i, j). If the covariates correspond to the nodes, i.e. X i ∈ R N is associated with node i for all i ∈ N , they are transferred on the dyad level through a symmetric "similarity" function 97 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 2. STATISTICAL FRAMEWORK </p><formula xml:id="formula_281">A1 A2 A3 π •• B1 B2 B3 B4 B5 π •• C1 C2 π •• π •• π •• π •• • Q = {•, •, •} blocks • α • = P(i ∈ •), • ∈ Q, i = 1, . . . , n • π •• = P(i ↔ j|i ∈ •, j ∈ •)</formula><formula xml:id="formula_282">φ(•, •) : R N × R N → R m : X ij φ(X i , X j ). In the following, X [X ij ] i,j∈N ∈ (R m</formula><p>) n×n denotes the covariates. An SBM accounting for covariates is</p><formula xml:id="formula_283">Z i ∼ iid M(1, α), ∀i ∈ N , Y ij | Z i , Z j , X ∼ ind B(g(γ zizj + β X ij ), ∀(i, j) ∈ N 2 , (2.2)</formula><p>where</p><formula xml:id="formula_284">γ q ∈ R, β ∈ R m , α = (α 1 , ..., α Q ), g(x) = (1 + e -x</formula><p>) -1 . In this case the vector of unknown parameters is defined by θ = (γ, β, α). Note the connection between this model and logistic regression. In this framework, heterogeneity is modeled by parameters (γ q ) q corresponding to inter and intra block probabilities (or connectivity matrix) in the binary SBM. It accounts for heterogeneity that is not explained by the regression term β T X ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Missing Data and SBM</head><p>The main feature of missSBM is to deal with missing values to perform unbiased estimation of the parameters of the model underlying an incompletely observed network. The sampled data can be encoded in an adjacency matrix Y where missing information -here dyads not sampled -is encoded by NA's. We also define the n × n sampling matrix R such as R ij = 1 if the dyad Y ij is sampled and R ij = 0 otherwise. For convenience we define Y o = {Y ij : R ij = 1} and Y m = {Y ij : R ij = 0} the respective sets of observed and non-observed dyads. At this stage it is important to notice that the number of nodes n is assumed to be known.</p><p>In our framework, a sampling design is a stochastic process that generates R. We then rely on the standard missing data theory of <ref type="bibr" target="#b105">Rubin (1976)</ref> to classify those designs either into Missing Completely At Random (MCAR), Missing At Random (MAR) or Not Missing At Random (NMAR) cases. We summarize the analyzes conducted in <ref type="bibr">Tabouy et al. (2019a)</ref> on missing data for network as follows:</p><p>Sampling design for SBM is</p><formula xml:id="formula_285">     MCAR if R |= (Y, Z), MAR if R |= (Y m , Z) | Y o , NMAR otherwise.</formula><p>(2.3) Note that MCAR missingness is a particular case of MAR missingness. Denoting by ψ the set of parameters associated with the sampling design distribution that generates R, we assume that ψ and θ are living in a product space, so that we can derive the following proposition: 98 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 2. STATISTICAL FRAMEWORK Proposition 38. From (2.3), if the sampling is MAR or MCAR then maximizing p θ,ψ (Y o , R) or p θ (Y o ) in θ is equivalent.</p><p>In words, the inference can be conducted on the observed part of the network data when the sampling is MAR or NMAR without incurring any biased. In these cases, adaptation of existing algorithm for SBM inference is straightforward. NMAR sampling designs require, however, more refined inference strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Examples of Sampling Designs for Networks</head><p>This section reviews a series of stochastic processes -or sampling designs -available in missSBM either for sampling an existing network or to be accounted for in the inference of an SBM model. Note that the sampling design may depend either on i) the values of the dyads in the network; ii) the latent clustering of the nodes; or iii) some covariates. We specify for each case when the sampling is MAR or NMAR. The sampling design examples detailed in the following assume the independence between dyad observation for dyad-centered sampling designs and between node observation for node-centered sampling design conditionally on Y, Z and X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dyad-Centered Sampling Designs</head><p>• Dyad sampling (MAR): each dyad (i, j) ∈ D has the same probability P(R ij = 1) ψ to be observed.</p><p>• Double standard sampling (NMAR): let ψ (ρ 1 , ρ 0 ) ∈ [0, 1] 2 . Double standard sampling consists in observing dyads with probabilities</p><formula xml:id="formula_286">P(R ij = 1|Y ij = 1) = ρ 1 , P(R ij = 1|Y ij = 0) = ρ 0 .</formula><p>The probabiliy for sampling a dyad thus intrinsically depend on the presence/absence of the corresponding edge.</p><p>• Block-dyad sampling (NMAR): this sampling consists in observing all dyads with probabilities ψ (ψ q ) (q, )∈Q 2 such that</p><formula xml:id="formula_287">ψ q = P(R ij = 1 | Z iq = 1, Z j = 1).</formula><p>Thus sampling thus depends on the underlying clustering of the network.</p><p>• Covar-dyad sampling (MAR):let us define ψ (α, κ) ∈ R × R m . In this sampling, the probability to observe a dyad is driven by the effect of a given covariate:</p><formula xml:id="formula_288">P(R ij = 1|X ij ) = g(α + κ t X ij ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-Centered Sampling Designs</head><p>A node-centered sampling consists in drawing some nodes to observe with probabilities given by the sampling design. Observing a node means observing all the dyads implying this node. For all i ∈ N , we denote by V i = 1 if the node i is observed, V i = 0 otherwise. Then, if V i = 1 we have R ij = 1 for all j ∈ N .</p><p>• Node sampling (MAR): the probabilities for observing nodes are uniform: P(V i = 1) = ψ for all i ∈ N .</p><p>• Degree sampling (NMAR): for all node i ∈ N , P(V i = 1) = ρ i where (ρ 1 , . . . , ρ n ) ∈ [0, 1] n are such that ρ i = g(a + bD i ) for all i ∈ N where ψ (a, b) ∈ R 2 and D i = j Y ij . 99 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STATISTICAL FRAMEWORK</head><p>• Block-node sampling (NMAR): this sampling consists in observing all dyads corresponding to nodes selected with probabilities ψ (ψ 1 , . . . , ψ Q ) ∈ [0, 1] Q such that ψ q = P(V i = 1 | Z iq = 1) for all (i, q) ∈ N × Q.</p><p>• Covar-node sampling (MAR): let ψ (ν, η) ∈ R × R N . The probability to observe a node is</p><formula xml:id="formula_289">P(V i = 1|X i ) = g(ν + η t X i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Estimation Procedure</head><p>Optimization: a Variational EM</p><p>The SBM is a latent state space model which can be seen as a mixture model for random graphs. As such the EM algorithm <ref type="bibr" target="#b39">(Dempster et al., 1977)</ref> is the natural choice for the inference. It is based on the evaluation of the expectation of the complete loglikelihood of the model, with respect to the conditional distribution of the latent variables given the data. However, this expectation is intractable in the SBM due to the structure of dependency between the latent variables Z and the network Y. In fact, it would require to sum over all possible clustering for all pairs of nodes, which is out of reach even for a moderate number of nodes or clusters. To address this shortcoming in the complete data situation, <ref type="bibr" target="#b36">Daudin et al. (2008)</ref> introduced a variational EM, based on the variational principles of <ref type="bibr" target="#b67">Jordan et al. (1998)</ref>. The idea is to maximize a lower bound of the loglikelihood based on an approximation of the true conditional distribution of the latent variable Z.</p><p>In the case of an SBM with missing data, the level of difficulty is higher since the set of latent variables encompasses both Z (the latent clustering) and Y m (the missing dyads). We propose here a variational distribution of the conditional distribution p θ (Z, Y m |Y o ) where complete independence is forced on Z and Y m , using a multinomial m(•), respectively a Bernoulli b(•) distribution for Z and Y m :</p><formula xml:id="formula_290">pτ,ν (Z, Y m ) = pτ (Z) pν (Y m ) = i∈N m(Z i ; τ i ) (i,j)∈D m b(Y ij ; ν ij ),</formula><p>where τ = {τ i , i ∈ N } and ν = {ν ij , (i, j) ∈ D m } are two sets of variational parameters respectively associated with Z and Y m . Interestingly, τ 's are proxies for the posterior probabilities of memberships for all nodes, and ν's correspond to the imputed values of the missing dyads in the network data. This approximation leads to the following lower bound J of the loglikelihood, where KL is the Kullback-Leibler divergence between the true and approximated conditional distribution:</p><formula xml:id="formula_291">log p θ,ψ (Y o , R) ≥ J τ ,ν,θ,ψ (Y o , R) log p θ,ψ (Y o , R) -KL(p τ ,ψ (Z, Y m ) p θ (Z, Y m Y o )), = E pτ,ν [log p θ,ψ (Y o , R, Y m , Z)] -E pτ,ν [log pτ,ν (R, Y m )] .</formula><p>Note that when we chose p to be the true conditional distribution of the latent variables Z, Y m , we meet again the quantity maximized in the standard EM algorithm.</p><p>Based on this approximation, the variational EM algorithm consists in alternatively updates of the variational parameters {τ , ν} (the VE-step) and updates of θ, ψ maximizing J τ ,θ (the M-step). Steps VE and M are iterated until convergence like in a standard EM. The algorithm converges to a local maximum of the lower bound of the loglikelihood. This variational principle is translated into a series of algorithms for handling missing data with all sampling designs introduced in Section 2.4. We underline that some time consuming parts of the VEM algorithms are coded in C++ using the Rcpp package <ref type="bibr" target="#b42">(Eddelbuettel and François, 2011)</ref> to interface C++ with R. 100 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STRUCTURE OF THE PACKAGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>It is well known that algorithms based on EM algorithm have a great sensitivity to initialization. This step therefore requires a special attention. In missSBM we implemented several methods for initializing the variational EM: the Absolute Eigenvalues Spectral Clustering (detailed in <ref type="bibr" target="#b104">Rohe et al., 2010)</ref>; a Hierarchical Ascending Classification (HAC) based on Manhattan norm between nodes; and the K-means algorithm. Experiences showed us that the HAC algorithm is sometimes more robust in cases where the network contains a large proportion of missing dyads. However, in general, we recommend the use of the Absolute Eigenvalues Spectral Clustering, which is used by default in the package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of the Number of Blocks</head><p>A main difficulty met when conducting SBM inference lies in the estimation of the number of blocks, generally unknown to the user. To remedy this problem we use the Integrated Classification Likelihood (ICL) criterion defined in <ref type="bibr" target="#b13">Biernacki et al. (2000)</ref> and routinely used in the framework of mixture models. More precisely, we implement in missSBM an exploration procedure designed to produce a smooth and robust ICL curve, by avoiding to get stuck in local minima. This "smoother" is divided into two steps, forward and backward:</p><p>The forward step creates new initializations for each number Q of block considered, by splitting blocks obtained from estimations with Q -1 blocks. On the other hand, the backward step tries new initializations for each Q by merging groups of the model with Q + 1 groups. The best model in terms of ICL is always retain. The procedure can be iterated until an satisfying smoothing is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structure of the Package</head><p>The R package missSBM is coded using object-oriented programming with R6 package <ref type="bibr" target="#b25">(Chang, 2017)</ref>. The hierarchical structure and inheritance relations are represented in Figure <ref type="figure" target="#fig_25">5</ref>.2.  101 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GUIDELINES FOR USERS</head><p>The package contains two general R6 classes: SBM and networkSampling. These two classes instantiate objects that collect all parameters needed to respectively define an SBM and a sampling design, as depicted in Sections 2.1 and 2.2 for SBMs and in Section 2.4 for the sampling design. These classes both give birth to two child classes, a sampler class and a fit class, which inheritate of all methods and fields of their respective mother classes. The sampler class allows the user to obtain a realization of network data sampled under an SBM (i.e. an adjacency matrix Y), and a realization of the sampling of a network (i.e. a sampling matrix R). The fit classes contains methods for estimating the SBM and the sampling design parameters. Finally, SBM_fit gives birth to two child classes: SBM_fit_nocovariates and SBM_fit_covariates, respectively dedicated to the estimation of binary SBM with or without covariates. Note that the two classes SBM and network_Sampling can be used totally independently. In particular, it is possible to fit SBM without missing data. In order to perform SBM inference when the network is partially observed, two others classes of object are introduced: sampledNetwork and missSBM_fit. The former instantiates an object which collects any information about presence or absence of dyads and nodes in a sampled network. The latter makes the connection between objects of classes SBM and network_sampling, in order to make it possible to infer SBM from partially-observed network data, under several sampling designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Guidelines for Users</head><p>The package missSBM allows the user to do three differents actions: simulate a network under the SBM, sample a network according to a panel of sampling designs, and estimate an SBM from partially observed network data. To do so, three standard R functions are exported to the user, which use internally the classes of object detailed in Section 3. We describe in the following the usage of these functions plus some additional auxiliary functions, as well as their connections with models defined in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulate a network</head><p>The function simulate draws a realization of an SBM, with the following usage: The arguments of simulate and their mathematical counterpart from (2.1) and (2.2) are described in Table <ref type="table" target="#tab_34">5</ref> Note that the network simulated are by default undirected and without covariates. In the case of an SBM with covariate(s), covarParam must not include intercepts since this role is played by parameters (γ q ) q, in model (2.2). 102 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GUIDELINES FOR USERS</head><p>The output of simulate is an R6 object belonging to the class SBM_sampler (see Figure <ref type="figure" target="#fig_25">5</ref>.2). An non-exhaustive list of the most useful fields, accessible via $ are presented in Table <ref type="table" target="#tab_34">5</ref>.2. A plot method is also available which allows the user to either draw the network by reordering rows and columns of the adjacency matrix Y according to the nodes memberships, or draw the predicted connectivity matrix ZYZ , i.e. the probability of connection for each nodes once reordered block-wise. This drawing is a simple yet powerful way to visualize the structure of large networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling a network</head><p>The function sample draws a realization of a sampling matrix R from a distribution chosen from the series of sampling designs defined in Section 2.4. The usage is the following:  </p><formula xml:id="formula_292">φ R N × R N → R m Table 5</formula><p>.3 -Arguments of sample and their counterparts in the SBM. ( ) possible values for sampling are characters string in "dyad", "double-standard", "block-dyad", "node", "blocknode", "degree", "covar-node" and "covar-dyad" for SBM without covariate, and "dyad", "node", "covar-node" and "covar-dyad" SBM with covariates.</p><p>Note that the dimension (or length) of parameters depends on the sampling design selected, as described in Section 2.4. Argument clusters only needs to be specified for "block-dyad" and "block-node" sampling designs. Arguments covarMatrix is by-default NULL, covarSimilarity is set to the l1_similarity function defined as follows: (</p><formula xml:id="formula_293">x, y) ∈ R d × R d → -|x -y| 1 ∈ R d</formula><p>and finally intercept is set to 0. These last three arguments only needs to be specified in a context of SBM with covariate(s). Note that the intercept is not included in parameters and must be specified independently.</p><p>The output of sample is an R6 object belonging to the class sampledNetwork (see Figure <ref type="figure" target="#fig_25">5</ref>.2). An non-exhaustive list of the most useful fields is given in Table <ref type="table" target="#tab_34">5</ref>.4, where stands for the Hadamard (or element-wise) product. A plot method is defined for sampledNetwork objects, drawing the adjacency matrix Y of the network with missing entries induced by the sampling matrix R. 103 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GUIDELINES FOR USERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameters estimation, prediction and clustering</head><p>The purpose of the function estimate is to perform variational inference of a SBM from sampled adjacency matrix, with the following usage: missSBM::estimate(sampledNet, vBlocks, sampling, clusterInit = "spectral", useCovariates = TRUE, control = list()) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preparation.</head><p>When the network data is not obtained from the function sample but from real-world network, the user has to format this data to an object with class sampledNetwork. We implemented a simple function for this task, taking as an input an adjacency matrix filled with {0, 1, NA}:</p><p>missSBM::prepare_data(adjacencyMatrix, covariates = NULL, similarity = missSBM:::l1_similarity)</p><p>Tuning the optimization. The argument control is a list to finely tune the variational EM algorithm, with the following entries:</p><p>(i) threshold: optimization stops when a V-EM step changes the objective function by less than threshold, (default = 1.10 -3 );</p><p>(ii) maxIter: optimization stops when the number of iteration exceeds maxIter, default = 50 if useCovariates is TRUE, 100 otherwise;</p><p>(iii) fixPointIter: number of iterations in the fix point algorithm used to solve the Variational E step, = 2 if useCovariates is TRUE, 5 otherwise; 104 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 4. GUIDELINES FOR USERS (iv) cores: number of thread be ran in parallel, = 1 by default;</p><p>(v) trace: integer for verbosity (0, 1, 2), useless when cores &gt; 1.</p><p>Estimation ouputs: classes missSBM_collection and missSBM_fit. The output of estimate is an R6 object with class missSBM_collection, fields of which are described in Table <ref type="table" target="#tab_34">5</ref>.6.  Among the fields of missSBM_collection, the field models is a list with length(vblocks) elements which are R6 objects with class missSBM_fit (see Figure <ref type="figure" target="#fig_25">5</ref>.2). These missSBM_fit objects, fields of which are detailed in 5.7, contain all the results of the inference for a fixed number of block Q. Smoothing and model exploration. At the end of the estimation process, it is common that the algorithm get stuck in some local minima for some values of Q, the number of blocks. The consequence is an "non-smooth" ICL curve, which is theoretically supposed to be convex. We thus provide to the user a post-processing function to "smooth" the ICL by exploring additional models with new initialization. Smoothing of the ICL curve is performed with function smooth, detailed in the following : 105 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 5. ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE missSBM::smooth(Robject, type = c("forward", "backward", "both"), control = list())</p><p>The basic idea is to apply a split and/or merge strategy to the path of models in a collection of SBM, in order to find better initialization. This should result in a "smoothing" of the ICL, that should be close to convex. Arguments of smooth are described in Table <ref type="table" target="#tab_34">5</ref>.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Argument Description</head><p>R6 object / class Robject an output from estimate missSBM_collection type kind of ICL smoothing : "forward", "backward" or "both" character control controlling the variational EM algorithm list Table <ref type="table" target="#tab_34">5</ref>.9 -Arguments of smooth function with short descriptions and their types.</p><p>This function acts directly on Robject (i.e. by reference). The argument type is bydefault set to "forward". Finally, control is a list composed of 3 elements : (i) cores the number of R processes allowed set by-default to 1, (ii) iterates the number of iteration by-default set to 1 and (iii) trace which is by default TRUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Illustration: the 2007 French political blogosphere</head><p>This section illustrates the features of missSBM by conducting an analysis of a real-world network data. It is a sub-network of the French political blogosphere, extracted from a snapshot of over 1100 blogs collected during a period preceding the 2007 French presidential election, and manually classified by the "Observatoire Présidentielle project" (see <ref type="bibr" target="#b129">Zanghi et al., 2008)</ref>, . The network is composed of 196 blogs representing nodes in the network and 1432 edges indicating that at least one of the two blogs references the other. On top of missSBM, our analysis relies on magrittr and igraph for standard (network) data manipulation. We also fix the seed for reproducibility:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>set.seed(1234) library(missSBM) library(igraph) library(magrittr)</head><p>The frenchblog2007 data set is shipped with missSBM 1 . We provide the data as an igraph object, which is the most convenient way of manipulating network data in R, in our opinion. We extract the adjacency matrix corresponding to the blog network and the vertex attribute party, providing the political party of each blog, also giving a natural classification of the nodes that could be used as a reference in our analyzes. data("frenchblog2007", package = "missSBM") class(frenchblog2007)</p><p>[1] "igraph" adjacencyMatrix &lt;-frenchblog2007 %&gt;% as_adj(sparse = FALSE) party &lt;-vertex.attributes(frenchblog2007)$party Once reordered row-wise and column-wise according to party, the adjacency matrix shows us that a part of the underlying structure is indeed supported by the political party (see Figure <ref type="figure" target="#fig_25">5</ref>.3).</p><p>1 earlier versions of this data set were available in packages mixer and sand 106 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 The global sampling rate obtained for this of the network data is</p><formula xml:id="formula_294">sampledNet$samplingRate ## [1] 0.7950811</formula><p>The sampleNet object owns a plot method representing the network data in the missing data context (see right-panel in Figure <ref type="figure" target="#fig_25">5</ref>.3): plot(sampledNet, clustering = factor(party), main = "") Estimation of a partially observed network. We are now in position of performing inference of SBM under missing data condition. We fit two types of model: first the SBM under the NMAR block-node sampling design, i.e. under the design that truly generated the missing entries; second the SBM under the star sampling, a.k.a MCAR node sampling in the package. The estimation is ran on both models with the same setting as for the fully observed data (i.e., by varying the number of blocks in 1, 15 and using the same tuning parameters for the initialization step and the optimization procedure). The ICL curve is smoothed with a forward-backward smoothing.</p><p>We first run the estimation for the block-node sampling and report ICL values before and after smoothing: sbm_block &lt;-estimate(sampledNet, vBlocks, "block-node", control = control) ICL_block_nsm &lt;-sbm_block$ICL missSBM::smooth(sbm_block, smoothing_type, control) Figure <ref type="figure" target="#fig_25">5</ref>.4 illustrates the dramatic change in the model selection process due to the smoothing. 108 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 Sampling design comparison. Now, we consider the simple MCAR node sampling which basically performs inference only on the observed part of the network, neglecting the process that originates the missing values:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE</head><p>sbm_node &lt;-estimate(sampledNet, vBlocks, "node", control = control) missSBM::smooth(sbm_node, smoothing_type, control)</p><p>Figure <ref type="figure" target="#fig_25">5</ref>.5 shows how ICL can be used to select which sampling design fit at best the data by comparing the smoothed ICL curves for "block-node" and "node" sampling designs. Note that the curve associated with the block-node sampling uniformly dominates the curve associated with the star sampling, showing that this sampling design is more adapted to the network data at play. We also represent the ICL curve of the SBM estimated on the fully observed network: although the values of the ICLs cannot be compared with the ones obtained for the partially observed network (indeed, data are not the same in this case), the number of block selected in the different cases remains comparable. The ICL criterion selects 11 blocks for an SBM adjusted on the fully observed network, while the SBM with missing entries accounted for by block-node sampling only selects 9 blocks, with a more flatter ICL curve. Indeed, due to the partial sampling, some blocks are less well represented than others; and it seems more likely to gather some blocks together considering the information available. 109 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE</head><p>Regarding the clusterings obtained by the three variants (fully observed, missing entries with MCAR modeling and missing entries with NMAR block-node sampling), we compare them with the Adjusted Rand Index (ARI, <ref type="bibr" target="#b101">Rand, 1971)</ref>, computed with the aricode package <ref type="bibr" target="#b29">(Chiquet and Rigaill, 2018)</ref>. We use the classification of the SBM fitted on the fully observed network as a reference, since its clustering was used to sample the network with the blocknode sampling design. We typically expect that an SBM relying on a better modeling of the missing values shall lead to a clustering closer to the reference. This is indeed the case when looking at the next piece of code, where it is shown that the ARI with the reference clustering is 50% higher for the SBM with block-sampling than for SBM with MCAR node sampling: aricode::ARI(sbm_block$bestModel$fittedSBM$memberships, cl0)</p><p>[1] 0.6031785 aricode::ARI(sbm_node$bestModel$fittedSBM$memberships , cl0)</p><p>[1] 0.4412234 Extraction of the SBM with block-sampling design. The model that we finally retain it thus block-sampling with 9 blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>myModel &lt;-sbm_block$bestModel</head><p>As seen from Figure <ref type="figure" target="#fig_25">5</ref>.2, myModel is an object with class missSBM_fit with two important fields used for storing the results of the estimation of both the SBM (field fittedSBM) and the sampling design (fittedSampling). The important fields and methods are recalled to the user thanks to the print methods: $type, $parameters, $prob, $df $penalty, $vExpec 110 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 Representation and validation With myModel, we now have at hand a tool for analyzing the clustering of the French political blogosphere. The first output is the connectivity matrix of the network, which puts into light the community structure of the blogosphere. Indeed, it is revealed by a diagonal filled with high probabilities and off-diagonal with low probabilities. Thus, nodes (blogs) into blocks connects with high probability with other nodes of the same block and with low probability with nodes of other blocks. Such a network concentrates most of its edges between nodes of the same blocks. This can be seen by displaying the probability of connection predicted by the SBM at the whole network scale:</p><p>plot(myModel$fittedSBM, type = "connectivity")</p><p>For validation, we suggest to compare the clustering of the model with the node attribute corresponding to the political parties to which blogs belong to. First, we remark that the SBM fitted on missing entries carry the same amount of information regarding the political party than the SBM adjusted on the fully observed network: aricode::ARI(party, myModel$fittedSBM$memberships)</p><p>[1] 0.4279665 aricode::ARI(party, sbm_full$bestModel$fittedSBM$memberships)</p><p>[1] 0.4244866 A more detailed comparison between blocks inferred by the SBM and political parties is reported in Figure <ref type="figure" target="#fig_25">5</ref>.7 with an alluvial diagram.</p><p>Also remember that missSBM performs imputation of the missing dyads in the adjacency matrix. Thus, we can compare the imputed values with the value sof the dyad in the fully observed network to validate the performance of our approach. Using the R package pROC <ref type="bibr" target="#b102">(Robin et al., 2011)</ref>, we check the quality of the imputation. The following piece of code generates a singe ROC curve for the current sampling and imputation. Results in Figure <ref type="figure" target="#fig_25">5</ref>.8 display ROC curves for 100 samplings of missing entries (always with block-sampling design). The sampling rate varies between ≈ 0.4 and ≈ 0.9. It shows the robustness and the good performance of the imputation method. 111 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 Finally, the Area Under the Curve (AUC) when the sampling rate is varying between 0.43 and 0.9 is plotted in Figure <ref type="figure" target="#fig_25">5</ref>.9. We can see that the more the sampling rate increases the more the AUC increases. 112 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 5. ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE AUC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.6 0.7 0.8 0.9 0.4 0.6 0.8 sampling rate Figure <ref type="figure" target="#fig_25">5</ref>.9 -Area Under the Curve (AUC) in function of the sampling rate.</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Chapter 5 5. ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE 114 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion et perspectives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Conclusion</head><p>Dans le chapitre 2 nous avons étudié les différents types de données manquantes selon la théorie développée dans <ref type="bibr" target="#b105">Rubin (1976)</ref> et nous l'avons adaptée au cas du SBM. Fort de la dichotomie MAR/NMAR nous avons donné des exemples de stratégies d'échantillonnage aussi bien centrées sur les noeuds que sur les dyades illustrant les cas de données manquantes MAR et NMAR pour le SBM. Un algorithme EM avec approximation variationnelle a été proposé pour s'adapter à tous les cas de données manquantes dont la loi du processus d'échantillonnage suit un modèle connu. Une étude de simulation poussée a permis de montrer la pertinence de la prise en compte de la nature des données manquantes dans l'inférence. De plus elle a permis de montrer qu'il était possible de choisir parmi plusieurs stratégies laquelle expliquait le mieux les données. Enfin l'identifiabilité du SBM sous trois des modèles de données manquantes définis dans ce chapitre a été établie. Dans le chapitre 3 nous nous sommes intéressés à l'étude asymptotique des estimateurs du maximum de vraisemblance et de l'approximation variationnelle du SBM en présence de données manquantes. Le cadre considéré est celui d'un SBM dont la loi d'émission d'une dyade appartient à la famille exponentielle à un paramètre avec des données manquantes telles que chaque dyade est observée indépendamment et avec même probabilité. Ainsi, les estimateurs du maximum de vraisemblance et de l'approximation variationnelle sont consistants et asymptotiquement normaux. De plus, la variance asymptotique est explicite. Enfin, nous avons montré l'identifiabilité du SBM pour toute la famille exponentielle à un paramètre en présence de données manquantes.</p><p>Le chapitre 4 introduit différents modèles de SBM avec covariables et données manquantes. Les liens entre données manquantes et covariables sont définis ainsi que des exemples de stratégies d'échantillonnage. Nous avons montré que sous certaines conditions, un SBM sans covariable avec données manquantes NMAR est équivalent à un SBM avec covariables et données manquantes MAR. De plus, à travers des simulations nous montrons que sans la connaissance de covariables, les algorithmes NMAR développés dans le chapitre 2 sont dans certains cas une alternative intéressante pour l'estimation des paramètres du modèle et la prédiction de la classification par rapport au fait de considérer les données manquantes MAR pour un SBM sans covariables.</p><p>Finalement, le chapitre 5 décrit le package missSBM, développé dans la language R et actuellement sur le CRAN. Chaque fonction est précisément présentée ainsi que son utilisation. Plusieurs exemples reproductibles fondés sur des jeux de données disponibles directement dans le package sont proposés pour permettre une meilleure introduction et prise en main de missSBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PERSPECTIVES Conclusion et perspectives</head><p>Une extension naturelle serait d'étudier d'autres lois pour les dyades du SBM comme par exemple la loi de Poisson pour des données de comptage et la loi normale pour des données continues. En effet, définir des stratégies d'échantillonnage et une méthode d'inférence pour les cas NMAR n'est pas évident et ne découle pas directement de notre travail. Il apparaît que l'approximation variationnelle que nous avons proposée (i.e. supposer les dyades manquantes indépendantes, de même lois avec leurs propres paramètres) pour le cas binaire avec des données manquantes NMAR ne produise pas des estimateurs explicites des paramètres variationnels et du SBM. Ceci résulte du fait que dans le cas binaire, la vraisemblance est linéaire en les dyades et s'intègre directement, comme pour la loi binomiale et la loi multinomiale. Cependant ce n'est le cas par exemple pour la loi de Poisson où la vraisemblance d'une dyade donnée par log p θ (Y ij |Z iq = 1, Z j = 1) = -π q + Y ij log(π q ) -log(Y ij !) dont l'espérance quand Y ij est manquant n'est pas directement calculable avec une approximation de loi de type "champs moyen". De plus, il n'est pas évident que des algorithmes d'optimisation soient efficaces compte tenu de la non convexité en général du problème. Il pourrait être intéressant aussi de se demander si des méthodes comme des algorithmes consistant à simuler conditionnellement au reste les Y ij manquants permettraient de résoudre ce problème.</p><p>De nombreux problèmes théoriques persistent encore à propos de l'étude asymptotique des estimateurs du maximum de vraisemblance, ainsi que de celle des estimateurs du maximum de vraisemblance issus de l'approximation variationnelle pour des stratégies d'échantillonnage noeuds centrés MAR et toutes les stratégies NMAR. Ce problème est difficile et nécessite probablement pour chaque stratégie une preuve spécifique. De plus, il serait intéressant de montrer l'identifiabilité des modèles définis lorsqu'elle n'est pas connue. Il semble que les preuves classiques d'identifiabilité du SBM ne s'appliquent pas pour toutes les stratégies que nous avons définies. Notre intuition est que ces modèles sont identifiables, les simulations allant dans ce sens. Finalement, il nous semblerait important de mieux comprendre pourquoi dans les cas où des noeuds sont échantillonnés, faire l'hypothèse que des données manquantes sont MAR quand elles sont NMAR constitue une hypothèse robuste dans de nombreux cas que nous avons pu observer sur des simulations. Il serait important de pouvoir identifier ces cas de manière à pouvoir prévoir et prévenir quand l'hypothèse MAR est crédible. D'autres stratégies d'échantillonnage pour le cas de réseaux binaires qui paraîtraient naturelles pourraient aussi être définies. Le package missSBM a d'ailleurs été pensé pour pouvoir accueillir facilement de nouvelles stratégies.</p><p>Dans toute cette thèse, la taille des réseaux étudiés est supposé connue. Cette hypothèse pose naturellement une question récurrente que nous n'avons pas explorée qui est de ne plus supposer la taille du réseau connue. C'est ce qu'il est proposé de faire dans l'article de <ref type="bibr" target="#b123">Vincent and Thompson (2015)</ref> dans lequel les auteurs estiment conjointement la taille du réseau et les paramètres du SBM qu'ils explorent avec un échantillonnage one-wave snowball sampling. On pourrait se demander si leurs méthodes s'appliquent à d'autres cas comme ceux que nous étudions. Notre intuition est que cela ne devrait pas modifier la nature des données manquantes, ce qui devrait rendre simple l'inférence des cas MAR. Les cas NMAR nécessite d'estimer la taille du réseau, on pourrait imaginer par exemple que n soit aléatoire avec une loi puissance et l'estimer alternativement avec les paramètres du modèle.</p><p>Il serait intéressant aussi de se poser la question de l'impact des "liens fallacieux" (i.e. observés avec du bruit) décrits dans <ref type="bibr">Clauset et al. (2008) et Guimerà and</ref><ref type="bibr" target="#b53">Sales-Pardo (2009)</ref> sur la modélisation et la nature des données manquantes ainsi que leur prise en compte dans l'inférence.</p><p>Enfin, faisant le lien avec des problèmes venant de l'écologie, on pourrait se demander comment prendre en compte des stratégies NMAR pour le SBM qui échantillonnent uniquement des liens existant (i.e. des arêtes) et ceci en tenant compte éventuellement de covariables. Ceci consisterait a priori à appliquer une stratégie d'échantillonnage comme le 116 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Conclusion et perspectives 2. PERSPECTIVES deux poids deux mesures en prenant en compte à la fois la valeur des covariables associées à une dyade et la valeur de la dyade dans la probabilité de l'échantillonner. 117 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 2. PERSPECTIVES Conclusion et perspectives 118 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q BIBLIOGRAPHY BIBLIOGRAPHY C. Matias and S. Robin. Modeling heterogeneity in random graphs through latent space models: a selective review. ESAIM Proc. Sur., 47:55-74, 2014. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Titre : Impact de l'échantillonnage sur l'inférence de structures dans les réseaux : application aux réseaux d'échanges de graines et à l'écologie Title : Impact of sampling on structure inference in networks: application to seed exchange networks and to ecology Keywords : Networks, Stochastic Block Model, missing data Abstract : In this thesis we are interested in studying the stochastic block model (SBM) in the presence of missing data. We propose a classication of missing data into two categories Missing At Random and Not Missing At Random for latent variable models according to the model described by D. Rubin. In addition, we have focused on describing several network sampling strategies and their distributions. The inference of SBMs with missing data is made through an adaptation of the EM algorithm: the EM with variational approximation. The identiability of several of the SBM models with missing data has been demonstrated as well as the consistency and asymptotic normality of the maximum likelihood estimators and variational approximation estimators in the case where each dyad (pair of nodes) is sampled independently and with equal probability. We also looked at SBMs with covariates, their inference in the presence of missing data and how to proceed when covariates are not available to conduct the inference. Finally, all our methods were implemented in an R package available on the CRAN. A complete documentation on the use of this package has been written in addition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Illustration: the 2007 French political blogosphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 2 Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1.1 -Modèle graphique réduit associé au SBM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Dans la suite nous noterons ψ ∈ Ψ respectivement le paramètre d'échantillonnage et l'espace des paramètres associé. Nous supposerons tout le temps que les paramètres θ et ψ vivent dans un espace produit. On considérera aussi que les données précèdent toujours l'échantillonnage, c'est pourquoi nous nous intéressons à la loi de R sachant Y. Ainsi, en reprenant les travaux de D.B. Rubin nous dirons que les données manquantes sont (i) Missing Completely At Random (MCAR), si l'échantillonnage ne dépend ni des valeurs des données observées ni des valeurs des données non-observées. Mathématiquement cela revient à dire que R |= Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 . 2 -</head><label>12</label><figDesc>Figure 1.2 -Graphes acycliques dirigés de dépendances conditionnelles entre Y, Z et R dans un cadre de données manquantes dans le SBM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 . 3 -</head><label>13</label><figDesc>Figure 1.3 -Graphes acycliques dirigés de dépendances conditionnelles entre Y, Z et R dans un cadre de données manquantes dans le SBM avec covariables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 1 -</head><label>21</label><figDesc>Figure 2.1 -DAGs of relationships between Y, Z and R in the framework of missing data for SBM. DAG where R is a parent node are not reviewed since the network exists before the sampling design acts upon it. The systematic edge between Z and Y is due to the definition of the SBM. Note that the DAG (b) may correspond to MAR or NMAR samplings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and NMAR otherwise. We derive Proposition 4 from these definitions. Proposition 4. If the sampling is MCAR or MAR then i) arg max θ p θ,ψ (Y o , R) = arg max θ p θ (Y o ) for any ψ such that p θ,ψ (Y o , R) = 0 and ii) the sampling design necessary satisfies DAG (a) or (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 2.2 -Estimation error of π and Adjusted Rand Index averaged over 500 simulations in the MAR setting.The adjacency matrix Y is generated under random-dyad sampling strategy for various connectivity c = q α q α π q .</figDesc><graphic coords="42,99.21,186.20,178.58,178.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2</head><label>2</label><figDesc>Figure 2.4 -Double standard setting: estimation error of π and adjusted Rand index averaged over 500 simulations for affiliation, bipartite and star topologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2</head><label>2</label><figDesc>Figure 2.6 -ARIs computed between the clusterings given by an SBM under class, star degree and MAR samplings with a varying number of blocks Q and ntora of farmers (lefthand-side) or dialect spoken by farmers (right-hand-side)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 2.7 -ICL criteria for SBMs with random-dyad MAR sampling and double-standard NMAR sampling in the thresholded ER network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 2.8 -Estimation error of π and ARI averaged over 500 simulations in star degree and class settings. The topology is affiliation with = 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(b) Matrix of connectivity π for NMAR inference (double standard) ; intensity of the color is proportional to the probability of connection between blocks. (c) ER PPI network reordered by blocks inferred with SBM with NMAR modeling. Left panel: original data with NA entries colored in gray (upper triangle) and data imputed with νij (lower triangle); right panel: zoom of blocks (1,2,3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>6 )</head><label>6</label><figDesc>Proposition 24 (contribution of global assignments). Choose t n decreasing to 0 such that ρntn √ log(n) → +∞. Then conditionally on Ω 1 and for n large enough that 2 √ 2n 2 n ≥ Q 2 , we have: sup θ∈Θ z / ∈S(z ,tn) p(z, y o ; θ) = o P (p(z , y o ; θ )) uses Propositions 25 and 22 to show that the combined contribution to the observed likelihood of assignments close to z is also a o P of p(z , y o ; θ ): Proposition 26 (contribution of local assignments). With the previous notations and C the positive constant defined in Proposition 21: sup θ∈Θ z∈S(z ,C) z z p(z, y o ; θ) = o P (p(z , y o ; θ )) 65</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>o , z ; θ ) p(y o , z ; θ) uni-modal in θ, with a mode in θ M C . By consistency of θ M C , either p(y o , z ; θ) = o P (p(y o , z ; θ )) or p(y o , z ; θ) = O P (p(y o , z ; θ )) and θ → θ . In the latter case, any θ ∼ θ other than θ is bounded away from θ and thus p(y o , z ; θ ) = o P (p(y o , z ; θ )). In summary, θ ∼θ p(y o , z ; θ ) p(y o , z ; θ ) = max θ ∼θ p(y o , z ; θ ) p(y o , z ; θ ) (1 + o P (1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>. 7 )</head><label>7</label><figDesc>Proposition 35. Consider the maximization of the lower bound (3.7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>. 13 )</head><label>13</label><figDesc>Proposition 36. Consider the lower bound J τ ,θ (Y o |X) given by (3.13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 4 . 3 -</head><label>43</label><figDesc>Figure 4.3 -Matrix g -1 (γ) in different topologies with inter/intra block probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Figure 4.4 -On the left side we estimate the prediction error of missing dyads where n m is the number of missing dyads and on the right side the ARI between the predicted clustering and the true one. In this case p = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 4 . 5 -</head><label>45</label><figDesc>Figure4.5 -On the left side we estimate the prediction error of missing dyads where n m is the number of missing dyads and on the right side the ARI between the predicted clustering and the true one. In these cases p = .2 and p = .5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 . 1 -</head><label>51</label><figDesc>Figure 5.1 -Schematic representation of an undirected network following the stochastic block model with 3 blocks. Colors are blocks in which nodes are dispatched with probabilities α and dyads distribution between nodes depends on colors of nodes with probabilities π.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 5</head><label>5</label><figDesc>Figure 5.2 -Diagram of relation between classes of objects in missSBM, with three levels of inheritance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>missSBM::simulate(nNodes, mixtureParam, connectParam, directed = FALSE, covariates = NULL, covarParam = NULL)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>missSBM::sample(adjacencyMatrix, sampling, parameters, clusters = NULL, covariates = NULL, similarity = l1_similarity, intercept = 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>a</head><label></label><figDesc>list of models with class missSBM_fit ICL the vector of ICL associated to the models in the collection bestModel best model according to the ICL (a missSBM_fit object) optimizationStatus a data.frame summarizing the optimization process for all models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>Figure 5.4 -ICL criterion for block-sampling design with/without smoothing of the ICL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>Figure 5.5 -ICL criterion curves for th block-node and node sampling designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 5</head><label>5</label><figDesc>Figure 5.6 -Probability predicted by the SBM with block-node sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 5</head><label>5</label><figDesc>Figure 5.7 -Alluvial plot between block-node sampling clustering and political parties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>Mots Clefs : Réseaux, modèle à blocs stochastiques, données manquantes Résumé :Dans cette thèse nous nous intéressons à l'étude du modèle à bloc stochastique (SBM) en présence de données manquantes. Nous proposons une classication des données manquantes en deux catégories Missing At Random et Not Missing At Random pour les modèles à variables latentes suivant le modèle décrit par D. Rubin. De plus, nous nous sommes attachés à décrire plusieurs stratégies d'échantillonnages de réseau et leurs lois. L'inférence des modèles de SBM avec données manquantes est faite par l'intermédiaire d'une adaptation de l'algorithme EM : l'EM avec approximation variationnelle. L'identiabilité de plusieurs des SBM avec données manquantes a pu être démontrée ainsi que la consistance et la normalité asymptotique des estimateurs du maximum de vraisemblance et des estimateurs avec approximation variationnelle dans le cas où chaque dyade (paire de n÷uds) est échantillonnée indépendamment et avec même probabilité. Nous nous sommes aussi intéressés aux modèles de SBM avec covariables, à leurs inférence en présence de données manquantes et comment procéder quand les covariables ne sont pas disponibles pour conduire l'inférence. Finalement, toutes nos méthodes ont été implémentées dans un package R disponible sur le CRAN. Une documentation complète sur l'utilisation de ce package a été écrite en complément.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Très proche dans sa définition du LPCM, le LPM (voir Channarond, 2013, pour une définition et une étude théorique du modèle) modélise aussi la notion d'homophilie. Dans ce modèle, les variables latentes sont continues et appartiennent à l'espace R d . Soit f une densité par rapport à la mesure de Lebesgue sur R d , on considère le modèle suivant :</figDesc><table /><note><p><p>DE L'ART INTRODUCTION</p>Latent Positions Model (LPM).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1] une fonction isotrope (i.e. ne dépendant pas de la direction) et décroissante par rapport à la norme euclidienne. Ce modèle de graphe introduit par<ref type="bibr" target="#b83">Lovász and Szegedy (2006)</ref> est très populaire car très général dans sa formulation. De plus il peut être vu comme une limite pour les graphes denses. Il est très activement étudié. On cite par exemple le livre de<ref type="bibr" target="#b82">Lovász (2012)</ref> qui est une référence incontournable concernant ce modèle.</figDesc><table /><note><p><p>Graphon (ou</p>W -graph). Un graphon est une fonction symétrique et mesurable f : (I 2 , B(I 2 )) → (I, B(I)) avec I = [0, 1]. Comme dans les modèles LPCM et LPM, les variables latentes sont continues :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Théorème de représentation par un modèle de Graphon Le</head><label></label><figDesc></figDesc><table /><note><p><p><p><p><p>résultat suivant justifie l'étude des modèles à variables latentes et particulièrement du SBM. Il établit le modèle de graphon -qui est une version générale et continue d'un modèle à variable latente -comme une représentation possible de tous ces modèles.</p>Théorème 1.</p><ref type="bibr" target="#b83">(Lovász and Szegedy, 2006</ref></p>, Théorème 2.7) Pour toute suite de modèles (G n ) de graphes aléatoires de taille n vérifiant les conditions suivantes, il existe une fonction symétrique κ : [0, 1] 2 → [0, 1] telle que le modèle a la même loi que H n,κ :</p>(i) La loi de G n est invariante par permutation des numéros des noeuds (on dit que c'est un modèle de graphe échangeable).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>j) est échantillonnée au cours du processus d'échantillonnage et R ij = 0 dans le cas contraire. Pour les échantillonnages centrés sur les noeuds, nous utiliserons plus spécifiquement la variable V = (V i ) i∈N = (1 {i est échantillonné} ) i∈N indiquant quel noeud a été échantillonné.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>2. ÉTAT DE L'ART INTRODUCTIONY n'est pas une variable aléatoire, i.e. l'aléa est dû à l'échantillonnage seul. Dans ce second cas, les quantités d'intérêts sont des caractéristiques structurelles du réseau telles que le nombre d'arêtes, la moyenne des degrés des noeuds, la loi des scores de centralité associés aux noeuds ou encore des covariables associés aux noeuds. Dans Handcock and Gile (2010) les auteurs traitent du cas « model-based » pour des échantillonnages MAR dans le cadre des ERGM et du cas « design-based » pour les mêmes échantillonnages. Imaginons que l'on veuille estimer la somme de quantités d'intérêts associées aux noeuds d'un réseau (âge, taille, sexe, etc.) notée s = i∈N t i . Soit E = {i 1 , ...i n E } un échantillon de taille n E de l'ensemble des noeuds tel que pour chaque i dans E la quantité t i est observée.Supposons de plus que l'échantillonnage utilisé consiste en un tirage uniforme avec remise. Si désormais l'échantillonnage n'est pas un simple tirage uniforme avec remise, on se demande si l'estimateur ŝ de s est encore sans biais, si oui comment le débiaiser. La réponse est apportée dans<ref type="bibr" target="#b61">Horvitz and Thompson (1952)</ref>. Supposons que pour tout noeud i dans N la probabilité qu'il soit dans E est donnée par µ i . Alors, l'estimateur d'Horvitz-Thompson de s est</figDesc><table><row><cell cols="4">Cadre « design-based » : estimateurs d'Horvitz-Thompson</cell></row><row><cell>Alors ŝ = n n E</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ŝµ =</cell><cell>i∈E</cell><cell>t i µ i</cell><cell>.</cell></row></table><note><p>i∈E t i est un estimateur sans biais de s (i.e. E[ŝ] = s).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Dans cette section, les données Y suivent un modèle probabiliste. Le but est d'inférer les paramètres de la loi de Y à partir d'un échantillon incomplet de Y. Suivant le travail de<ref type="bibr" target="#b55">Handcock and Gile (2010)</ref>, nous allons définir quelques stratégies d'échantillonnage Missing at Random de graphe et nous donnerons leurs lois. Rappelons que dans le cas de données manquantes MAR, l'inférence des paramètres de la loi de Y se fait sur la partie observée des données uniquement et sans biais induit par la stratégie d'échantillonnage. Nous supposerons par la suite les graphes symétriques et sans boucles, cependant tout ce qui suit se transpose directement au cas des graphes non-symétriques avec ou sans boucles.</figDesc><table><row><cell>Cadre « model-based »</cell></row><row><cell>Ego-centric design. Cet échantillonnage consiste à sélectionner indépendamment des</cell></row><row><cell>noeuds du graphe avec probabilité ψ et d'observer toutes les valeurs des dyades entre les</cell></row><row><cell>noeuds sélectionnés. Ainsi :</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2 Statistical framework . . . . . . . . . . . . . . . . . . . . . . . . . 33</head><label></label><figDesc></figDesc><table><row><cell>2.1</cell><cell>Stochastic Block Model . . . . . . . . . . . . . . . . . . . . . . . 33</cell></row><row><cell>2.2</cell><cell>Sampled data in the SBM framework . . . . . . . . . . . . . . . . 34</cell></row><row><cell>2.3</cell><cell>Sampling design examples . . . . . . . . . . . . . . . . . . . . . . 35</cell></row><row><cell>3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . 36</head><label></label><figDesc></figDesc><table><row><cell>3.1</cell><cell>MAR inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36</cell></row><row><cell>3.2</cell><cell>NMAR inference: the general case . . . . . . . . . . . . . . . . . 37</cell></row><row><cell>3.3</cell><cell>NMAR: specificities related to the choice of the sampling . . . . 39</cell></row><row><cell>4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Simulation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41</head><label></label><figDesc></figDesc><table><row><cell>4.1</cell><cell>MAR condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41</cell></row><row><cell>4.2</cell><cell>NMAR condition . . . . . . . . . . . . . . . . . . . . . . . . . . . 41</cell></row><row><cell>5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Importance of accouting for missing values in real networks . 45</head><label></label><figDesc></figDesc><table><row><cell>5.1</cell><cell cols="2">Seed exchange network in the region of Mount Kenya . . . . . . 45</cell></row><row><cell>5.2</cell><cell>ER (ESR1) Protein-Protein Interaction network in breast cancer</cell><cell>46</cell></row><row><cell>6</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 7 Supplementary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49</head><label></label><figDesc></figDesc><table><row><cell>7.1</cell><cell>Proof of Proposition 4.ii) . . . . . . . . . . . . . . . . . . . . . . 49</cell></row><row><cell>7.2</cell><cell>Identifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49</cell></row><row><cell>7.3</cell><cell>Derivation of second lower bound in star degree sampling . . . . 51</cell></row><row><cell>7.4</cell><cell>Proof of Proposition 10 . . . . . . . . . . . . . . . . . . . . . . . 51</cell></row><row><cell>7.5</cell><cell>Simulations for star degree and class samplings . . . . . . . . . . 51</cell></row><row><cell>7.6</cell><cell>Additional results for the ER Protein-Protein Interaction network 52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 2 .</head><label>2</label><figDesc>|ρ 1 | 1 -Performance of the ICL criterion: rates of correct answers when choosing the number of blocks and Adjusted Rand Indexes. Tested configurations are different sampling rates and three topologies (affiliation, bipartite and star) under a double standard sampling.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-0.75 -0.5 -0.25</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell><cell>-0.75 -0.5 -0.25</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ρ 1 -ρ 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">topology affiliation bipartite star</cell><cell cols="2">ε 0.05 0.15 0.25</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>Figure</p>2</p>.5 -Double standard setting: estimation error of ρ 0 and ρ 1 averaged over 500 simulations for affiliation, bipartite and star topologies.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 2 .</head><label>2</label><figDesc>2 -Rates of correct answers of the ICL criterion when choosing between Randomdyad sampling (MAR) and double standard sampling (NMAR) in each of the 18 configurations. A configuration is the combination of a topology (affiliation, bipartite and star), a sampling rate and a sampling design. Each configuration is simulated 500 times.</figDesc><table><row><cell cols="5">sampling rate sampling affiliation bipartite star</cell></row><row><cell>(0.096, 0.367]</cell><cell>MAR</cell><cell>0.73</cell><cell>0.67</cell><cell>0.63</cell></row><row><cell></cell><cell>NMAR</cell><cell>0.72</cell><cell>0.75</cell><cell>0.75</cell></row><row><cell>(0.367, 0.638]</cell><cell>MAR</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>NMAR</cell><cell>0.91</cell><cell>0.78</cell><cell>0.82</cell></row><row><cell>(0.638, 0.909]</cell><cell>MAR</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell>NMAR</cell><cell>0.91</cell><cell>0.8</cell><cell>0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>1 . Choose z ∈ Z 1 and a sequence t n decreasing to 0 but satisfying ρnt n / log(n) → +∞. According to Proposition 24,</figDesc><table><row><cell></cell><cell cols="2">sup</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">θ∈Θ z / ∈S(z ,tn)</cell><cell></cell><cell></cell></row><row><cell cols="4">And Proposition 27 allows us to conclude</cell><cell></cell></row><row><cell>p(y o ; θ) p(y o ; θ )</cell><cell>=</cell><cell># Sym(θ) # Sym(θ )</cell><cell>θ ∼θ max</cell><cell>p(y</cell></row></table><note><p>p(z, y o ; θ) = o P (p(z , y o ; θ )) Since t n decreases to 0, it gets smaller than C (used in proposition 26) for n large enough. As this point, Proposition 26 ensures that: sup θ∈Θ z∈S(z ,tn) z z p(z, y o ; θ) = o P (p(z , y o ; θ )) And therefore the observed likelihood ratio reduces as: p(y o ; θ) p(y o ; θ ) = z∼z p(y o , z; θ) + z z p(y o , z; θ) z∼z p(y o , z; θ ) + z z p(y o , z; θ ) = z∼z p(y o , z; θ) + p(y o ; z , θ )o P (1) z∼z p(y o , z; θ ) + p(y o ; z , θ )o P (1) o , z ; θ ) p(y o , z ; θ ) (1 + o P (1)) + o P (1).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>with covariates and sampling models 86 3 Statistical inference</head><label></label><figDesc>Inference of Model 1 . . . . . . . . . . . . . . . . . . . . . . . . . 88 3.2 Inference of Model 2 . . . . . . . . . . . . . . . . . . . . . . . . . 89 3.3 Inference of Model 3 . . . . . . . . . . . . . . . . . . . . . . . . . 90 3.4 Model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90</figDesc><table><row><cell></cell><cell>Chapter 4 SBM with covariates and</cell></row><row><cell></cell><cell>missing values</cell></row><row><cell>Contents</cell><cell></cell></row><row><cell>1</cell><cell>Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85</cell></row><row><cell>2</cell><cell>Stochastic Block Models</cell></row></table><note><p>. . . . . . . . . . . . . . . . . . . . . . . . . . 88 3.1 4 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>The network topology is affiliation, which means that diagonal parameters of the connectivity matrix π are greater than extra-diagonal parameters. Indeed we choose</figDesc><table><row><cell></cell><cell></cell><cell>.2 .05 .05</cell><cell></cell><cell></cell></row><row><cell>π =</cell><cell></cell><cell>.05 .2 .05</cell><cell> and α = (1/4, 1/2, 1/4).</cell><cell>(4.1)</cell></row><row><cell></cell><cell></cell><cell>.05 .05 .2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head></head><label></label><figDesc>.1.</figDesc><table><row><cell cols="2">Argument Description</cell><cell>Correspondence</cell><cell>Lives in</cell></row><row><cell>n</cell><cell>number of nodes</cell><cell>n</cell><cell>N</cell></row><row><cell>alpha</cell><cell>mixture parameters</cell><cell>α</cell><cell>[0, 1] Q</cell></row><row><cell>pi</cell><cell>matrix of inter and intra clusters probabilities</cell><cell>π</cell><cell>MQ([0, 1])</cell></row><row><cell>directed</cell><cell>whether the network is directed or not</cell><cell></cell><cell>{T, F}</cell></row><row><cell cols="2">covariates list of covariates</cell><cell>(Xi)i∈N , (Xij)ij</cell><cell>( )</cell></row><row><cell cols="2">covarParam regression parameters of the covariates</cell><cell>β</cell><cell>R m</cell></row><row><cell cols="4">Table 5.1 -Arguments of simulate and their counterparts in the SBM. ( ) is a list with</cell></row><row><cell cols="4">N entries (the N covariates corresponding to nodes) or a list of m n × nmatrices (covariates</cell></row><row><cell cols="2">corresponding to dyads).</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head></head><label></label><figDesc>draw Y re-ordered by blocks if type="network" and ZYZ if type="connectivity"</figDesc><table><row><cell>Field</cell><cell>Description</cell><cell>Correspondence</cell></row><row><cell>adjacencyMatrix</cell><cell>adjacency matrix of the simulated network</cell><cell>Y</cell></row><row><cell>blocks</cell><cell>matrix of blocks memberships</cell><cell>Z</cell></row><row><cell>memberships</cell><cell>vector of blocks memberships</cell><cell>(which.max(τ i )) i∈N</cell></row><row><cell>Method</cell><cell>Description</cell><cell></cell></row><row><cell>plot(object, type)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 5 .</head><label>5</label><figDesc>2 -Selection of important fields and methods in class SBM_sampler with their mathematical counterparts</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 5 .</head><label>5</label><figDesc>3 describes the arguments of sample and their mathematical counterparts.</figDesc><table><row><cell>Argument</cell><cell>Description</cell><cell>Correspondence</cell><cell>Lives in</cell></row><row><cell cols="2">adjacencyMatrix adjacency matrix the network</cell><cell>Y</cell><cell>Mn({0, 1})</cell></row><row><cell>sampling</cell><cell>name of the sampling design</cell><cell></cell><cell>( )</cell></row><row><cell>parameters</cell><cell>sampling parameter(s)</cell><cell>ψ</cell><cell>Ψ</cell></row><row><cell>clusters</cell><cell>vector of blocks memberships</cell><cell>(which.max(τ i))i∈N</cell><cell>Q n</cell></row><row><cell>covariates</cell><cell>list of covariates</cell><cell>(Xi)i∈N , (Xij)ij</cell><cell>( )</cell></row><row><cell>similarity</cell><cell>similarity function for covariates on dyads</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head></head><label></label><figDesc>Table 5.4 -Selection of important fields and methods in class sampledNetwork.</figDesc><table><row><cell>Field</cell><cell>Description</cell><cell>Correspondence</cell></row><row><cell cols="2">adjacencyMatrix sampled adjacency matrix with NA at missing entries</cell><cell>Y R</cell></row><row><cell>covarMatrix</cell><cell>the matrix of covariates (if applicable)</cell><cell>X</cell></row><row><cell>covarArray</cell><cell>the array of covariates (if applicable)</cell><cell>(X ij ) ij</cell></row><row><cell>missingDyads</cell><cell>the set of dyades not sampled in the network</cell><cell>Y m</cell></row><row><cell>observedDyads</cell><cell>the set of dyades sampled in the network</cell><cell>Y o</cell></row><row><cell>observedNodes</cell><cell>vector of sampled nodes</cell><cell>V</cell></row><row><cell>samplingRate</cell><cell>rate of observed dyades</cell><cell>mean(R)</cell></row><row><cell>samplingMatrix</cell><cell>sampling matrix</cell><cell>R</cell></row><row><cell>Method</cell><cell>Description</cell><cell></cell></row><row><cell>plot(object)</cell><cell cols="2">draw the adjacency matrix Y and the sampling matrix R</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 5 .</head><label>5</label><figDesc>5 provides a basic description of the arguments of estimate.</figDesc><table><row><cell>Argument</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note><p>5 -Arguments of sample with short descriptions and types.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 5 .</head><label>5</label><figDesc>6 -Structure of missSBM_collection (output of estimate).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head></head><label></label><figDesc>Table 5.8 -Selection of fields in fittedSBM and fittedSampling with their mathematical counterparts.</figDesc><table><row><cell>Field</cell><cell>Description</cell><cell></cell><cell cols="2">R6 object / class</cell></row><row><cell>fittedSBM</cell><cell cols="2">the adjusted Stochastic Block Model</cell><cell cols="2">SBM_fit_(no)covariates</cell></row><row><cell cols="3">fittedSampling the estimated sampling process</cell><cell cols="2">networkSampling_fit</cell></row><row><cell>sampledNet</cell><cell cols="2">the sampled network data</cell><cell>sampledNetwork</cell></row><row><cell cols="3">imputednetwork the adjacency matrix with imputed values</cell><cell>matrix</cell></row><row><cell>monitoring</cell><cell cols="2">status of the optimization process</cell><cell>data.frame</cell></row><row><cell>vICL</cell><cell cols="2">ICL criterion associated to Q</cell><cell>double</cell></row><row><cell>R6 object</cell><cell>Field</cell><cell>Description</cell><cell cols="2">Correspondence</cell></row><row><cell></cell><cell>blocks</cell><cell cols="2">estimated probability of block belonging</cell><cell>{τ i } i∈N</cell></row><row><cell></cell><cell cols="2">connectParam connectivity matrix</cell><cell></cell><cell>π</cell></row><row><cell>fittedSBM</cell><cell>connectProb covarParam</cell><cell>imputed adjacency matrix regression parameter</cell><cell cols="2">Y o ∪ {ν ij } (i,j)∈D m β</cell></row><row><cell></cell><cell>memberships</cell><cell>vector of blocks memberships</cell><cell cols="2">(which.max(τ i )) i∈N</cell></row><row><cell></cell><cell cols="2">mixtureParam mixture parameters</cell><cell></cell><cell>α</cell></row><row><cell>fittedSampling</cell><cell>parameters</cell><cell>sampling parameter</cell><cell></cell><cell>ψ</cell></row></table><note><p><p><p><p><p><p><p>vBound value of the variational bound J τ ,θ at each step double vExpec value of E pτ [log(p θ (Y, Z))] at each step double penalty penalty of the model with Q blocks double</p>Table</p>5</p>.7 -Selection of fields in object missSBM_fit with descriptions and types.</p>We finally give additional details on fields fittedSBM and fittedSampling in Table</p>5</p>.8.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head></head><label></label><figDesc>For illustrative purpose, we sample the blog network to mimic missing data and create a new adjacency matrix with missing entries. Since the data are interactions between blogs (who references who), it is natural to sample the network with a node-centered sampling design: the following code generates a realization of a 196 × 196 sampling matrix according to the block-node sampling, where blocks corresponds to the clustering estimated by SBM on the full data set and where sampling rate is either low (0.2) or high (0.8) in each block.</figDesc><table><row><cell cols="3">5. ILLUSTRATION: THE 2007 FRENCH POLITICAL BLOGOSPHERE</cell></row><row><cell cols="3">## * Useful fields (most are special object themselves)</cell></row><row><cell>##</cell><cell cols="2">$fittedSBM (the adjusted stochastic block model)</cell></row><row><cell>##</cell><cell cols="2">$fittedSampling (the estimated sampling process)</cell></row><row><cell>##</cell><cell cols="2">$sampledNetwork (the sampled network data)</cell></row><row><cell>##</cell><cell cols="2">$imputedNetwork (the adjacency matrix with imputed values)</cell></row><row><cell>##</cell><cell cols="2">$monitoring, $vICL, $vBound, $vExpec, $penalty</cell></row><row><cell cols="3">Sampling the network data. cl0 &lt;-sbm_full$bestModel$fittedSBM$memberships</cell></row><row><cell cols="3">samplingParameters &lt;-base::sample(</cell></row><row><cell>x</cell><cell></cell><cell>= c(0.2, 0.8),</cell></row><row><cell cols="2">size</cell><cell>= sbm_full$bestModel$fittedSBM$nBlocks,</cell></row><row><cell cols="3">replace = TRUE)</cell></row><row><cell cols="3">sampledNet &lt;-sample(adjacencyMatrix, "block-node", samplingParameters, cl0)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Motivations : des données manquantes dans les réseauxLes réseaux sont un moyen naturel de représenter les interactions entre des individus ou plus généralement des entités (protéines, espèces, sites web, etc.). On les rencontre dans de nombreux domaines comme en biologie, en sociologie, en écologie, en industrie, ou Internet. Il existe en statistique de nombreux modèles de réseaux -dont certains seront présentés plus loin -et techniques permettant d'analyser des réseaux(Kolaczyk,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2009" xml:id="foot_1"><p>et Matias and  Robin, 2014). L'estimation de ces modèles n'est pas en général un problème facile.Une difficulté supplémentaire souvent rencontrée par le statisticien est de n'avoir accès qu'à un échantillon incomplet du graphe à étudier. En effet, pour des raisons simples telles que le manque de temps, le coût ou la malchance, un observateur peut être conduit à collecter des données incomplètes. On est donc amené à étudier un réseau partiellement observé tout en voulant connaître les propriétés du réseau complet sous-jacent. Dans la suite nous allons motiver notre travail avec deux exemples, le premier concernant des données d'ethnobiologie avec un réseau d'échanges de semences et le second des données génomiques avec un réseau de co-régulation de gènes (ou réseau protéine-protéine).Le premier exemple que nous étudions est celui d'un réseau d'échanges de semences entre agriculteurs de la région du Mont Kenya. Nous remercions chaleureusement Vanesse Labeyrie d'avoir partagé ces données (voir<ref type="bibr" target="#b72">Labeyrie et al., 2014</ref><ref type="bibr" target="#b73">Labeyrie et al., , 2016) )</ref> et pris du temps pour en discuter. L'étude de ce réseau a pour but de mieux comprendre l'impact des échanges sur la diversité génétique cultivée. Des études cherchant à lier la diversité génétique cultivée et la diversité culturelle humaine ont montré que les échanges de semences sont intimement liés à l'organisation sociale entre les agriculteurs. C'est un exemple de résultat auquel nous souhaiterions apporter une confirmation (ou non) d'ordre statistique. Ces données sont échantillonnées suivant un processus noeud-centré au sens où certains agriculteurs ont été interrogés pour renseigner à qui ils donnaient et de qui ils recevaient des semences. Ainsi, 155 agriculteurs ont été interrogés, renseignant leurs liens avec un total de 777 agriculteurs (dont les 155 interrogés). Puis, en accord avec les ethnologues, pour simplifier l'étude, nous avons réduit le réseau à 568 des 777 agriculteurs. Pour les 413 non interrogés du réseau</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>PerspectivesAu cours de cette thèse, nous avons abordé la question des données manquantes dans le SBM. Cependant de nombreuses questions que nous n'avons pas eu le temps d'aborder restent en suspens et mériteraient de l'attention.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements 8 Acknowledgment</head><p>The authors thank <rs type="person">Pierre Barbillon</rs> (<rs type="affiliation">INRA-MIA, AgroParisTech</rs>), Julien chiquet (<rs type="affiliation">INRA-MIA, AgroParisTech</rs>), <rs type="person">Stéphane Robin</rs> (<rs type="affiliation">INRA-MIA, AgroParisTech</rs>) and <rs type="person">James Ridgway</rs> (<rs type="affiliation">CFM</rs>) for their helpful remarks and suggestions. This work is supported by two public grants overseen by the <rs type="funder">French National research Agency (ANR</rs>): first as part of the « <rs type="programName">Investissement d'Avenir » program</rs>, through the « IDI 2017 » project funded by the <rs type="funder">IDEX Paris-Saclay</rs>, <rs type="grantNumber">ANR-11-IDEX-0003-02</rs>, and second by the « EcoNet » project.</p></div>
<div><head>Acknowledgments</head><p>The authors thank all members of MIRES group for fruitful discussions on network sampling designs. This work is supported by public grants overseen by the <rs type="funder">French National research Agency (ANR)</rs> as part of the "<rs type="programName">Investissement d'Avenir" program</rs>, through the "<rs type="programName">IDI 2017</rs>" project funded by the <rs type="funder">IDEX Paris-Saclay</rs>, <rs type="grantNumber">ANR-11-IDEX-0003-02</rs>, and second by the "<rs type="projectName">EcoNet"</rs> project, <rs type="grantNumber">ANR-18-CE02-0010</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZgQ8JA5">
					<orgName type="program" subtype="full">Investissement d&apos;Avenir » program</orgName>
				</org>
				<org type="funding" xml:id="_BJwCnnj">
					<idno type="grant-number">ANR-11-IDEX-0003-02</idno>
				</org>
				<org type="funding" xml:id="_jwGcUWr">
					<orgName type="program" subtype="full">Investissement d&apos;Avenir&quot; program</orgName>
				</org>
				<org type="funded-project" xml:id="_PFdDyaw">
					<idno type="grant-number">ANR-11-IDEX-0003-02</idno>
					<orgName type="project" subtype="full">EcoNet&quot;</orgName>
					<orgName type="program" subtype="full">IDI 2017</orgName>
				</org>
				<org type="funding" xml:id="_Jd4E7TT">
					<idno type="grant-number">ANR-18-CE02-0010</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ?</p><p>? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ?</p><p>? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</p><p>? ? ? ? ? ? ? ? ?</p><p>? ?</p><p>Figure <ref type="figure">5</ref>.3 -Adjacency matrix of the French blogosphere reordered according to political party; left: fully observed; right: sampled according to a block-node sampling design. Blue color range corresponds to edges between nodes from the same cluster; red color range between node from different clusters; gray color correspond to missing dyads.</p><p>Standard SBM estimation. At this stage, the data set has no missing entries: every dyads and nodes are observed. The adjacency matrix Y of the fully-observed network is adjacencyMatrix. We can first perform a standard SBM estimation on the fully observed network. To do so we set first the algorithm parameters that will be common to all analyzes:</p><p>## Algorithm parameters : ## vBlocks &lt;-1:15 control &lt;-list(cores = 10, trace = 0, iterates = 2) smoothing_type &lt;-"both"</p><p>We then proceed to the estimation of the fully-observed network, including smoothing of the ICL. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning latent block structure in weighted networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Compl. Net</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New consistent and asymptotically normal parameter estimates for random-graph mixture models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B-Met</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="35" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MixeR: Random Graph Clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grasseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoebeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E A</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>R package version 1.8.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Eppig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the propagation of low-rate measurement error to subgraph counts in large networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Viles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Block model for multipartite networks. Applications in ecology and ethnobiology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Donnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.286.5439.509</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic block models for multiplex networks: an application to networks of researchers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barbillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Donnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lazega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. C-Appl</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial interaction and the statistical analysis of lattice systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<idno type="ISSN">00359246</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="236" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mixing time of exponential random graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sly</surname></persName>
		</author>
		<idno type="DOI">10.1214/10-AAP740</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asymptotic normality of maximum likelihood and its variational approximation for stochastic blockmodels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1922" to="1943" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing a mixture model for clustering with the integrated completed likelihood</title>
		<author>
			<persName><forename type="first">C</forename><surname>Biernacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.865189</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="719" to="725" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Covariate-assisted spectral clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asx008</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="377" />
			<date type="published" when="2017">03 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Random Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Concentration Inequalities: A Nonasymptotic Theory of Independence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>OUP</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stochastic topic block model for the clustering of vertices in networks with textual edges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zreik</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-016-9713-7</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Estimation and model selection for the latent block model. Theses</title>
		<author>
			<persName><forename type="first">V</forename><surname>Brault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
		<respStmt>
			<orgName>Université Paris Sud -Paris XI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Consistency and Asymptotic Normality of Latent Blocks Model Estimators</title>
		<author>
			<persName><forename type="first">V</forename><surname>Brault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Keribin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A package for managing relational data in r</title>
		<author>
			<persName><forename type="first">C</forename><surname>Butts</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v024.i02</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Social network analysis with sna</title>
		<author>
			<persName><forename type="first">C</forename><surname>Butts</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v024.i06</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The sem algorithm : a probabilistic teacher algorithm derived from the em algorithm for the mixture problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diebolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rapport de recherche RR-1364</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>Inria, Institut National de Recherche en Informatique et en Automatique</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A classification em algorithm for clustering and two stochastic versions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consistency of maximum-likelihood and variational estimators in the stochastic block model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Celisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pierre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Stat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1847" to="1899" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">R6: Classes with Reference Semantics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>R package version 2.2.2</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Clustering in a random graph : models with latent space. Theses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Channarond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
		<respStmt>
			<orgName>Université Paris Sud -Paris XI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matrix estimation by universal singular value thresholding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="214" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating and understanding exponential random graph models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<idno type="DOI">10.1214/13-AOS1155</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">aricode: Efficient Computations of Standard Clustering Comparison Measures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chiquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigaill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>R package version 0.1.1</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels with growing number of classes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels with a growing number of classes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asr053</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Connected components in random graphs with given expected degree sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/PL00012580</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Combinatorics</title>
		<idno type="ISSN">0219-3094</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="145" />
			<date type="published" when="2002-11">Nov 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical structure and the prediction of missing links in networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The dynamic stochastic topic block model for dynamic networks with textual edges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-018-9832-4</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The igraph software package for complex network research</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nepusz</surname></persName>
		</author>
		<ptr target="http://igraph.org" />
	</analytic>
	<monogr>
		<title level="m">Complex Systems:1695</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A mixture model for random graphs</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. comp</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">1-bit matrix completion. Information and Inference: A</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wootters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="223" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convergence of a stochastic approximation version of the em algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Delyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lavielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1018031103</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="128" />
			<date type="published" when="1999">03 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B-Met</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lower bounds for the partitioning of graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Donath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.175.0420</idno>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<idno type="ISSN">0018-8646</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="420" to="425" />
			<date type="published" when="1973-09">Sep. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random graph models of social networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.012582999</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rcpp: Seamless R and C++ integration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eddelbuettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>François</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v040.i08</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Renyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cluster inference by using transitivity indices in empirical graphs</title>
		<author>
			<persName><forename type="first">O</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Harary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">380</biblScope>
			<biblScope unit="page" from="835" to="840" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">bnstruct: an r package for bayesian network structure learning in the presence of missing data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Franzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sambo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Di Camillo</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btw807</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1250" to="1252" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Evaluating overfit and underfit in models of network community structure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghasemian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hosseinmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
		<idno>CoRR, abs/1802.10582</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Random graphs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Gilbert</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177706098</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1959</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Markov Chain Monte Carlo in Practice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hall/Crc Interdisciplinary</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><surname>Statistics</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Francis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Introduction to high-dimensional statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A survey of statistical network models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="233" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Clustering with block mixture models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<idno type="ISSN">0031-3203. Biometrics</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="473" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Missing and spurious interactions and the reconstruction of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guimerà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sales-Pardo</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0908366106</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="22073" to="22078" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">statnet: Software tools for the representation, visualization, analysis and simulation of network data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Butts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v024.i01</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling social networks from sampled data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Gile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Model-based clustering for social networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tantrum</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-985X.2007.00471.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="354" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A class of statistics with asymptotically normal distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="325" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Cran task view: graphical models in R</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hojsgaard</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/views/gR.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1952.10483446</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On consistency of model selection for stochastic block models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01238" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inference in curved exponential family models for networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handcock</surname></persName>
		</author>
		<idno type="DOI">10.1198/106186006X133069</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<idno type="ISSN">1061- 8600</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="583" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ergm: A package to fit, simulate and diagnose exponential-family models for networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Butts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v024.i03</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Inference in curved exponential family models for networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="583" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Advanced Mean Field Methods: Theory and Practice, chapter</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial on variational approximation methods</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="105" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels and community structure in networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">16107</biblScope>
			<date type="published" when="2011-01">Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Estimation and selection for the latent block model on categorical data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Keribin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-014-9472-2</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1201" to="1216" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Statistical analysis of network data, methods and models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fitting latent cluster models for networks with latentnet</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krivitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handcock</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v024.i05</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Influence of ethnolinguistic diversity on the sorghum genetic patterns in subsistence farming systems in eastern kenya</title>
		<author>
			<persName><forename type="first">V</forename><surname>Labeyrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Calatayud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buiron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wambugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Glaszmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">92178</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Seed exchange networks, ethnicity, and sorghum diversity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Labeyrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Muthamia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="103" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Variational Bayes model averaging for graphon functions and motif frequencies inference in W-graph models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Robin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-015-9607-0</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Overlapping stochastic block models with application to the french political blogosphere</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Birmelé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="page" from="309" to="336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Variational bayesian inference and complexity control for stochastic block models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Birmelé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Modelling</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="115" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Goodness of fit of logistic regression models for random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ouadah</surname></persName>
		</author>
		<idno type="DOI">10.1080/10618600.2017.1349663</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Graphical models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">blockmodels: Latent and Stochastic Block Model Estimation by a &apos;V-EM&apos; Algorithm</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Leger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>R package version 1.1.1</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Blockmodels: A r-package for estimating in latent block model and stochastic block model, with various probability functions, with or without covariates</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Leger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.07587" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical analysis with missing data</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Large Networks and Graph Limits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<idno type="ISSN">0095-8956</idno>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Dynamic stochastic block models: parameter estimation and detection of changes in community structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ludkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1201" to="1213" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Convergence of the groups posterior distribution in latent or stochastic block models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="537" to="573" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Consistency and asymptotic normality of stochastic block models estimators from sampled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tabouy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1903.12488" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Uncovering latent structure in valued graphs: A variational approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="715" to="742" />
			<date type="published" when="2010-06">06 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Statistical clustering of temporal networks through a dynamic stochastic block model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B-Met</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Statistical clustering of temporal networks through a dynamic stochastic block model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12200</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1119" to="1141" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A semiparametric extension of the stochastic block model for longitudinal networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rebafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villers</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asy016</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="680" />
			<date type="published" when="2018-09">Sept. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Spatially constrained clustering of ecological networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dray</surname></persName>
		</author>
		<idno type="DOI">10.1111/2041-210X.12208</idno>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="771" to="779" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Every missing not at random model has got a missing at random counterpart with equal fit</title>
		<author>
			<persName><forename type="first">G</forename><surname>Molenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beunckens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Kenward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B-Met</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<idno type="DOI">10.1137/S003614450342480</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Estimation and clustering in popularity adjusted stochastic block model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1902.00431" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockstructures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A B</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">455</biblScope>
			<biblScope unit="page" from="1077" to="1087" />
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Reverend bayes on inference engines: A distributed hierarchical approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second AAAI Conference on Artificial Intelligence, AAAI&apos;82</title>
		<meeting>the Second AAAI Conference on Artificial Intelligence, AAAI&apos;82</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Random Geometric Graphs. Oxford studies in probability 5</title>
		<author>
			<persName><forename type="first">M</forename><surname>Penrose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Statistical inference on errorfully observed graphs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="930" to="953" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">proc: an open-source package for r and s+ to analyze and compare roc curves</title>
		<author>
			<persName><forename type="first">X</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tiberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lisacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">An introduction to exponential random graph (p*) models for social networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pattison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lusher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.socnet.2006.08.002</idno>
		<ptr target="https://doi.org/10.1016/j.socnet.2006.08.002" />
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<idno type="ISSN">0378-8733</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Special Section: Advances in Exponential Random Graph (p*) Models</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Spectral clustering and the high-dimensional stochastic block model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1978</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A block model for node popularity in networks with community structure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12245</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="386" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A central limit theorem for random sums of random variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shanthikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sumita</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-6377</idno>
		<ptr target="http://dx.doi.org/10.1016/0167-6377" />
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90008" to="90015" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo estimation of exponential random graph models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Social Structure</title>
		<idno type="ISSN">1529-1227</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Statistical models for social networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="131" to="153" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. class</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="100" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Sur la division des corps matériels en parties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Acad. Polon. Sci. Cl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="801" to="804" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Incorporating covariates into stochastic blockmodels</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Sweet</surname></persName>
		</author>
		<idno type="DOI">10.3102/1076998615606110</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="635" to="664" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">String v10: protein-protein interaction networks, integrated over the tree of life</title>
		<author>
			<persName><forename type="first">D</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franceschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Forslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huerta-Cepas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Tsafou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic. Acids Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Variational inference for stochastic block models from sampled data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tabouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiquet</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2018.1562934</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">ja</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">misssbm: An r package for handling missing values in the stochastic block model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tabouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiquet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.12201" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">An R package for adjusting Stochastic Block Models from networks data sampled under various missing data conditions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tabouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>R package version 0.2.0</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A bayesian approach to modeling stochastic blockstructures with covariates</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tallberg</surname></persName>
		</author>
		<idno type="DOI">10.1080/00222500590889703</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Mathematical Sociology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Model-based estimation with link-tracing sampling designs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Survey Methodology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Adaptive Sampling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Wiley</publisher>
			<pubPlace>New-York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Consistencies and inconsistencies between model selection and link prediction in networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Valles-Catala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sales-Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guimerà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review. E</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Graph clustering with missing data: Convex algorithms and analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neu. In</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Estimating the size and distribution of networked populations with snowball sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1402.4372v2" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Basic tail and concentration bounds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<ptr target="https://www.stat.berkeley.edu/mjwain/stat210" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Likelihood-based model selection for stochastic block models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<idno type="DOI">10.1214/16-AOS1457</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faust</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511815478</idno>
		<title level="m">Social Network Analysis: Methods and Applications. Structural Analysis in the Social Sciences</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A monte carlo implementation of the em algorithm and the poor man&apos;s data augmentation algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanner</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1990.10474930</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<idno type="ISSN">0162-1459</idno>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">411</biblScope>
			<biblScope unit="page" from="699" to="704" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
		<idno type="DOI">10.1086/jar.33.4.3629752</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Anthropological Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Fast online graph clustering via erdős-rényi mixture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zanghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2008.06.019</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2008.06.019" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3592" to="3599" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Clustering based on random graph model embedding vertex features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zanghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2010.01.026</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2010.01.026" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<idno type="ISSN">0167-8655</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Community detection in networks with node features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1214/16-EJS1206</idno>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Statist</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3153" to="3178" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">The huge package for highdimensional undirected graph estimation in r</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1059" to="1062" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Consistency of community detection in networks under degree-corrected stochastic block models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1214/12-AOS1036</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2266" to="2292" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
