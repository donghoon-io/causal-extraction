<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On some limitations of probabilistic models for dimension-reduction: Illustration in the case of probabilistic formulations of partial least squares</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-22">22 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lola</forename><surname>Étiévant</surname></persName>
							<email>lola.etievant@gmail.com.https</email>
						</author>
						<author>
							<persName><forename type="first">Vivian</forename><surname>Viallon</surname></persName>
							<email>viallonv@iarc.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Nutritional Methodology and Biostatistics</orgName>
								<orgName type="institution">Institut Camille Jordan</orgName>
								<address>
									<postCode>69622</postCode>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">International Agency for Research on Cancer</orgName>
								<address>
									<postCode>69372</postCode>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On some limitations of probabilistic models for dimension-reduction: Illustration in the case of probabilistic formulations of partial least squares</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-22">22 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.09498v2[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Partial Least Squares (PLS) refer to a class of dimension-reduction techniques aiming at the identification of two sets of components with maximal covariance, to model the relationship between two sets of observed variables x ∈ R p and y ∈ R q , with p ≥ 1, q ≥ 1. Probabilistic formulations have recently been proposed for several versions of the PLS. Focusing first on the probabilistic formulation of the PLS-SVD proposed by el Bouhaddani et al. ( <ref type="formula">2018</ref>), we establish that the constraints on their model parameters are too restrictive and define particular distributions for (x, y), under which components with maximal covariance (solutions of PLS-SVD) are also necessarily of respective maximal variances (solutions of principal components analyses of x and y, respectively). We propose an alternative probabilistic formulation of PLS-SVD, no longer restricted to these particular distributions. We then present numerical illustrations of the limitation of the original model of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>. We also briefly discuss similar limitations in another latent variable model for dimension-reduction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>keywords: PLS; probabilistic formulation; identifiability 1 Introduction Principal Component Analysis (PCA), Canonical Correlation Analysis (CCA) and Partial Least Squares (PLS) are arguably among the most popular multivariate methods for dimension-reduction. They have been described and applied for many years <ref type="bibr" target="#b10">(Hotelling, 1933</ref><ref type="bibr" target="#b11">, 1936</ref><ref type="bibr" target="#b15">, Jöreskog and Wold, 1982</ref><ref type="bibr" target="#b20">, Sampson et al., 1989</ref><ref type="bibr" target="#b26">, Wold, 1985)</ref>, and are still the subject of active research and discussion <ref type="bibr" target="#b0">(Abdi et al., 2013</ref><ref type="bibr" target="#b13">, Jolliffe, 2002</ref><ref type="bibr" target="#b14">, Jolliffe and Cadima, 2016</ref><ref type="bibr" target="#b16">, Krishnan et al., 2011)</ref>. Overall, these methods aim at the identification of weight vectors, from which components are defined as linear transformations of the observed variables. Under each particular method, the weights are chosen so that the corresponding components meet a particular criterion. For example, given a data matrix X containing n observations of a p-variate variable x (with n ≥ 1, p ≥ 1), the goal of PCA is to identify r ≤ p unit weight vectors that define r mutually orthogonal principal components with maximal variances; the matrix of principal components X = XA then consists of linear combinations of the p columns of X, with the matrix of weights A given by the eigenvectors associated with the r largest eigenvalues of the sample variance matrix X X. On the other hand, given two data matrices X and Y, that gather the n ≥ 1 observations for a pair of variables (x, y), with x ∈ R p and y ∈ R q , p, q ≥ 1, the goal of CCA and PLS is to model the relationship between x and y by identifying weight vectors that define components with maximal association.</p><p>Although CCA, which targets components with maximal correlation, is sometimes considered as a PLS technique, the PLS qualifier usually specifically refers to the class of methods that target components with maximal covariance <ref type="bibr" target="#b24">(Wegelin, 2000)</ref>. The family of PLS methods consists of a number of techniques, such as PLS Regression, PLS-W2A or PLS-SVD <ref type="bibr" target="#b19">(Rosipal and</ref><ref type="bibr">Krämer, 2006, Wegelin, 2000)</ref>. PLS Regression treats the two sets of variables asymmetrically: it focuses on the construction of components from one set of variables, which are then considered as predictors of the second set of variables (the response). On the other hand, PLS-W2A, PLS-SVD and CCA adopt a more symmetrical perspective, and aim at the identification of two sets of weight vectors defining two sets of components. In particular, PLS-SVD, sometimes also referred to as PLS-SB or PLS-C <ref type="bibr" target="#b16">(Krishnan et al., 2011</ref><ref type="bibr" target="#b20">, Sampson et al., 1989</ref><ref type="bibr" target="#b24">, Wegelin, 2000)</ref>, is based on the Singular Value Decomposition (SVD) of the sample covariance matrix X Y, and defines the two sets of weight vectors as left and right singular vectors of X Y, respectively. More precisely, the optimization problem is given by argmax a 1 ∈R p ,b 1 ∈R q a 1 X Yb 1 s.t. a 1 a 1 = b 1 b 1 = 1, and argmax a j ∈R p ,b j ∈R q a j X Yb j s.t.</p><p>a j a j = b j b j = 1, a j X Yb k = 0, for k, j ∈ {1, . . . , min(p, q)}, k = j.</p><p>In contrast, CCA, which also treats the two sets of observed variables symmetrically but aims at the identification of two sets of components with maximal correlation, corresponds to the following optimization problem argmax</p><formula xml:id="formula_0">a 1 ∈R p ,b 1 ∈R q a 1 X Yb 1 s.t. a 1 X Xa 1 = b 1 Y Yb 1 = 1</formula><p>and argmax a j ∈R p ,b j ∈R q a j X Yb j s.t. a j X Xa j = b j Y Yb j = 1,</p><formula xml:id="formula_1">a j X Xa k = b j Y Yb k = 0,</formula><p>for k, j ∈ {1, . . . , min(p, q)}, k = j.</p><p>For the sake of completeness, we shall further recall that both PLS Regression and PLS-W2A are iterative methods, based on a principle called deflation, which is applied iteratively to guarantee some particular orthogonality properties <ref type="bibr" target="#b12">(Höskuldsson, 1988</ref><ref type="bibr" target="#b19">, Rosipal and Krämer, 2006</ref><ref type="bibr" target="#b24">, Wegelin, 2000</ref><ref type="bibr" target="#b26">, Wold, 1985)</ref>.</p><p>Over the last two decades, several probabilistic formulations of these various dimension-reduction techniques have been introduced, first under a Gaussian setting. They include the Probabilistic PCA (PPCA) <ref type="bibr" target="#b23">(Tipping and Bishop, 1999)</ref>, the Probabilistic CCA (PCCA) <ref type="bibr" target="#b2">(Bach and Jordan, 2005)</ref>, as well as several versions of Probabilistic PLS (PPLS) <ref type="bibr" target="#b4">(el Bouhaddani et al., 2018</ref><ref type="bibr" target="#b17">, Li et al., 2015</ref><ref type="bibr" target="#b28">, Zheng et al., 2016)</ref>. Regarding these three probabilistic formulations of the PLS, both <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> and <ref type="bibr" target="#b17">Li et al. (2015)</ref> focus on PLS Regression, while el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref> consider a symmetrical PLS approach. Overall, all these probabilistic formulations rely on structural equations that define the observed variables as linear combinations of some latent variables plus some Gaussian noise. Parameter estimation under these latent variable models is then usually performed via an Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b6">(Dempster et al., 1977)</ref>, under appropriate constraints imposed on the model parameters to mimic their non-probabilistic counterpart. Giving access to all the likelihood-based inference machinery, these probabilistic formulations have a number of advantages compared to their standard formulation counterpart <ref type="bibr" target="#b19">(Rosipal and</ref><ref type="bibr">Krämer, 2006, Smilde et al., 2004)</ref>. The estimation can deal with missing data, while still being computationally efficient <ref type="bibr" target="#b23">(Tipping and</ref><ref type="bibr">Bishop, 1999, Zheng et al., 2016)</ref>. Moreover, covariates can be included in the model <ref type="bibr" target="#b5">(Chiquet et al., 2017)</ref>, and penalized versions of the likelihood can be used to encourage sparsity or structured sparsity, e.g., in a high-dimensional framework <ref type="bibr" target="#b9">(Guan and Dy, 2009</ref><ref type="bibr" target="#b18">, Park et al., 2017</ref><ref type="bibr" target="#b27">, Zeng et al., 2017)</ref>. Finally, the probabilistic formulation is very versatile, and turns several complex settings into natural extensions of the simple Gaussian ones mentioned above. For example, probabilistic PCA models have been proposed for binary data and count data <ref type="bibr" target="#b5">(Chiquet et al., 2017</ref><ref type="bibr" target="#b8">, Durif et al., 2019)</ref>. Extensions to even more complex settings, including mediation analysis where three sets of observed variables are involved, have also been proposed <ref type="bibr" target="#b7">(Derkach et al., 2019)</ref>.</p><p>To recap, probabilistic formulations of dimension-reduction techniques enjoy a number of appealing properties. However, appearances can be deceptive, and we will show in this article that some caution is needed when developing and applying them. Indeed, despite their apparent ability to fully capture the relationships among the variables under study, some of these models manage to do so under very particular distributions only, which greatly limits their applicability and interest. In particular, they are most often misspecified in practice and, when they are not, their parameters could be estimated under much simpler models. For illustration, we will mainly focus on the probabilistic PLS model proposed by el Bouhaddani et al. ( <ref type="formula">2018</ref>), which we will simply refer to as the PPLS model from now on. In Section 2.1, we recall the principle of the PPLS model as proposed by el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>, and emphasize that it can be regarded as a probabilistic formulation of PLS-SVD. In Section 2.2 we show that this PPLS model suffers from the aforementioned defect, and actually defines a set of very particular distributions for (x, y), under which components of maximal covariance are also of respective maximal variances. We propose an alternative probabilistic formulation of PLS-SVD in Section 2.3. In Section 2.4, we briefly discuss the connection of the proposed model with the probabilistic formulation of the CCA proposed by <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>. We turn our attention to the probabilistic formulation of PLS Regression proposed by <ref type="bibr" target="#b17">Li et al. (2015)</ref> in Section 2.5, which we will refer to as the PPLSR model, and show that it suffers from similar limitations. In Section 3, we present numerical examples to illustrate the limitations of the original PPLS model of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>. Concluding remarks are presented in Section 4.  <ref type="formula" target="#formula_8">3</ref>). Note that the later has one set of latent variables t only. Moreover, the structure of the noise parts e and f differs between the two models (see Equations ( <ref type="formula" target="#formula_2">1</ref>) and (3) below). In both models, x and y are the observed variables whereas circled variables are unobserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PPLS models</head><p>The PPLS model proposed by el Bouhaddani et al. ( <ref type="formula">2018</ref>) can be graphically represented as depicted in Figure <ref type="figure">1</ref> A. More specifically, it is defined by the following structural equations, which relate the two observed sets of variables x ∈ R p and y ∈ R q to two sets of latent variables t ∈ R r and u ∈ R r , with r &lt; min(p, q),</p><formula xml:id="formula_2">x = tW + e, y = uC + f , u = tB + h,<label>(1)</label></formula><p>together with the following constraints (a)-(j) on the model parameters (a) t ∼ N(0 r , Σ t ).</p><p>(b) Σ t is a r × r diagonal matrix, with strictly positive diagonal elements.</p><p>(c) e ∼ N(0 p , σ 2 e I p ).</p><formula xml:id="formula_3">(d) f ∼ N(0 q , σ 2 f I q ). (e) h ∼ N(0 r , σ 2 h I r ).</formula><p>(f) e, f and h are independent.</p><p>(g) W and C are respectively p × r and q × r semi-orthogonal matrices.</p><p>(h) B is a diagonal matrix, with strictly positive diagonal elements.</p><p>(i) the diagonal elements of Σ t B are strictly decreasingly ordered.</p><p>(j) r &lt; min(p, q).</p><p>Here I p denote the identity matrix of size p × p, and 0 p the vector (0, . . . , 0) of size p. The parameters of the model are given by θ</p><formula xml:id="formula_4">= (W,C, B, Σ t , σ 2 e , σ 2 f , σ 2 h ). In particular, matrices W = (W 1 , • • • ,W r ) and C = (C 1 , • • • ,C r )</formula><p>contain the two sets of weight vectors; note that they are the "true weights", defined from the theoretical distribution of (x, y). Given estimates W and C of these quantities, two sets of empirical components can be defined as linear combination of the two sets observed variables. In this work, we will mostly focus on components defined as X = X W and Y = Y C; we recall that, when working with latent variable models, an alternative strategy consists in using appropriate conditional expectations of the latent variables; see <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref> and Section 2.3 below for more details. We shall further stress that in either case, the components do not directly correspond to the latent variables t and u. In particular, x = xW = t + eW and y = yC = u + fC typically differ from t and u, respectively.</p><p>Under the constraints (a)-(j), el Bouhaddani et al. ( <ref type="formula">2018</ref>) establish the identifiability of their model (up to sign for the columns of parameters W and C).</p><p>In particular, the identifiability of parameters W and C is given by the following Proposition.</p><p>Proposition 1. Under the PPLS model given in Equation (1) along with the constraints (a) -(j), the columns of W and C are the uniquely defined (up to sign) left and right singular vectors corresponding to the r largest singular values of Cov(x, y), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This result has already been established by el Bouhaddani et al. (2018) in their</head><p>Lemma 1, so we here only recall the sketch of the proof. Under the PPLS model, we have Cov(x, y) = W Σ t BC , where W and C are semi-orthogonal matrices, and Σ t B is diagonal with strictly positive decreasingly ordered diagonal elements. It follows that the first r non-null singular values of Cov(x, y) are all distinct, and are the diagonal elements of Σ t B. As a result, the columns of W and C are uniquely defined (up to sign) as the first r left and right singular vectors of Cov(x, y), respectively.</p><p>Although they do not mention it, their model can therefore be regarded as a probabilistic formulation of PLS-SVD. In particular, this means that the two sets of components x = xW and y = yC coincide with the two sets of components with maximal covariance, targeted by the PLS-SVD.</p><p>However, we establish in Section 2.2 that the two sets of weights W and C, which are the theoretical solutions of the PPLS model, are also necessarily theoretical solutions of two PPCA models for x and y, respectively. In other words, the PPLS model defines a set of very particular distributions for (x, y) under which the two sets of components with maximal covariance, x = xW and y = yC, are also necessarily of respective maximal variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limitation of the PPLS model proposed by el Bouhaddani et al. (2018)</head><p>In Supporting Information A, we establish the following result.</p><p>Proposition 2. Under the PPLS model given in Equation (1) along with the constraints (a)-(j), the columns of W and C are eigenvectors corresponding to the r largest eigenvalues of Var(x) and Var(y), respectively.</p><p>Proposition 2 notably implies that, under the PPLS model, the two sets of components x = xW and y = yC are not only of maximal covariance (as implied by Proposition 1), but they are also necessarily of respective maximal variances. This result, whose proof is given in Supporting Information A, can equivalently be deduced from the fact that solutions W and C of the PPLS model are also necessarily solutions of two PPCA models, for x and y respectively. More precisely, the PPLS model implies that both x and y fulfill the following PPCA model, presented here for a generic observed variable</p><formula xml:id="formula_5">z ∈ R d z = vV + g,<label>(2)</label></formula><p>under the constraints</p><formula xml:id="formula_6">(α) v ∼ N(0 r , Σ v ). (β ) Σ v is a r × r diagonal matrix, with strictly positive diagonal elements. (γ) g ∼ N(0 d , σ 2 g I d ). (δ ) V is a d × r semi-orthogonal matrix. (ε) r &lt; d.</formula><p>This PPCA model is a variation of the one introduced by <ref type="bibr" target="#b23">Tipping and Bishop (1999)</ref>; see Supporting Information B for more details.</p><p>First consider this PPCA model for the observed variable x ∈ R p . By comparing, on the one hand, constraints (a), (b), (c), (g) and (j) with constraints (α)-(ε), and, on the other hand, Equation ( <ref type="formula" target="#formula_5">2</ref>) and the first equation in Equation ( <ref type="formula" target="#formula_2">1</ref>), it appears that the unique solution W of the PPLS model necessarily corresponds to one of the possibly many solutions V of this PPCA model for x. More precisely, when the solution of the PPCA model for x is unique (up to sign), that is when the diagonal elements of Σ t are all distinct, then the r largest eigenvalues of Var(x) are all of algebraic multiplicity equal to one, the associated eigenvectors are uniquely defined (up to sign), and they correspond to the columns of V . They are also the columns W 1 , . . . ,W r of W , although not necessarily in the same order; columns of W and V are in the same order if, and only if, the diagonal elements of Σ t are in decreasing order too. Now, if the diagonal elements of Σ t are not all distinct, then the solution V of the PPCA model for x is not unique, but the columns of W still necessarily constitute one of these solutions, that is one particular set of eigenvectors corresponding to the r largest eigenvalues of Var(x).</p><p>Similarly, the PPLS model implies that the PPCA model above holds for the observed variable y ∈ R q too, and that the unique solution C of the PPLS model necessarily corresponds to one of the possible solutions of this PPCA model for y. More precisely, if the diagonal elements of Σ t B 2 are all distinct, then the columns of C correspond to the uniquely defined r eigenvectors associated with the r largest eigenvalues of Var(y). On the other hand, if the diagonal elements of Σ t B 2 are not all distinct, then the columns of C still constitute one of the solutions of the PPCA model for y; in particular, they are one of the possible sets of eigenvectors for the r largest eigenvalues of Var(y).</p><p>Putting all this together, the PPLS model of el Bouhaddani et al. ( <ref type="formula">2018</ref>) corresponds to a model where two PPCA models, one for x and one for y, are related to each other via the third equation in Equation (1). Therefore, the weight matrices W and C, solutions of their PPLS model, are also necessarily solutions of two PPCA models for x and y, so that their model defines a subset of very particular distributions for (x, y), under which components x = xW and y = yC are not only of maximal covariance, but also of respective maximal variances. In particular, if the diagonal elements of Σ t are all distinct, and if the same holds true for Σ t B 2 , the "solutions" of the two distinct PPCA models are uniquely defined, and then each of the two marginal distributions of x and y are sufficient to respectively identify each of the two sets of weights that define components with maximal covariance. As will be confirmed in Section 3, this greatly limits the applicability of the PPLS model: it is most often misspecified in practice, and when it is not, two PCAs (or PPCAs) are often sufficient to estimate the weight matrices W and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Alternative probabilistic formulation of the PLS-SVD</head><p>We now present an alternative probabilistic formulation of the PLS-SVD, named PPLS-SVD, which corrects the main defect of the model proposed by el Bouhaddani et al. ( <ref type="formula">2018</ref>) and defines a broader set of distributions for (x, y). Our overall objective was to keep the same general form as that of el Bouhaddani et al. ( <ref type="formula">2018</ref>), but with weaker constraints, in such a way that the weights W and C cannot generally be identified from the marginal distributions of x and y only.</p><p>In the PPLS model, assumptions (a)-(j) are related to various aspects of the model: the distributions of the errors terms, the distributions of the latent variables, as well as "direct" constraints on the model parameter θ</p><formula xml:id="formula_7">= (W,C, B, Σ t , σ 2 e , σ 2 f , σ 2 h ).</formula><p>In order to keep the link with the PLS-SVD, we still assume that the weights matrices W and C are semi-orthogonal, and that the variance matrices of the latent variables are diagonal. As a start, we thus only relax the constraints (c) and (d) on the isotropy of the variance matrices for the error terms e and f . To be as general as possible, we simply assume that these variance matrices are positive semi-definite, so that the error terms e and f are simply two non-degenerate Gaussian vectors. We therefore replace constraints (c) and (d) by constraints (c*) and (d*) presented below. But then, to preserve the identifiability of the model (see below), we have to consider a model with only one set of latent variables, in the same vein as the PCCA model of <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>. The PPLS-SVD model, depicted in Figure <ref type="figure">1</ref> B, is then defined by the following two structural equations</p><formula xml:id="formula_8">x = tW + e, y = tC + f ,<label>(3)</label></formula><p>under the constraints (a), (b), (g), (j) and (c*) e ∼ N(0 p , Ψ e ), with Ψ e a p × p semi-positive definite matrix.</p><p>(d*) f ∼ N(0 q , Ψ f ), with Ψ f a q × q semi-positive definite matrix.</p><p>(f*) e and f are independent.</p><p>(i*) the diagonal elements of Σ t are strictly decreasingly ordered.</p><p>Conditions (f*) and (i*) are the analogues of conditions (f) and (i), respectively, in the case where only one set of latent variables is considered. Further observe that Cov(x, y) = W Σ t C , Var(x) = W Σ t W + Ψ e , and Var(y) = CΣ t C + Ψ f , where θ = (W,C, Σ t , Ψ e , Ψ f ) are the parameters of our model. Again, the graphical model presented in Figure <ref type="figure">1</ref> B does not completely specify our model, which is defined through the structural equations in (3), together with the constraints (a), (b), (c*), (d*), (f*), (g), (i*), and (j).</p><p>We now present the sketch of the proof of the identifiability of the PPLS-SVD model, which is an adaptation of the one developed by el Bouhaddani et al. ( <ref type="formula">2018</ref>); we refer to Supporting Information C for a more detailed proof. Consider two pairs of random variables, (x, y) and ( x, y), drawn from two PPLS-SVD models, with respective parameters θ = (W,C, Σ t , Ψ e , Ψ f ) and θ = ( W , C, Σ t , Ψ e , Ψ f ), and respective variance-covariance matrices Σ and Σ. Now, assume that Σ = Σ. This is equivalent to</p><formula xml:id="formula_9">W Σ t W + Ψ e = W Σ t W + Ψ e , (<label>4</label></formula><formula xml:id="formula_10">) CΣ t C + Ψ f = C Σ t C + Ψ f ,<label>(5)</label></formula><formula xml:id="formula_11">W Σ t C = W Σ t C .<label>(6)</label></formula><p>Matrices W , C, W , and C are all semi-orthogonal, and both Σ t and Σ t are diagonal with strictly decreasing diagonal elements. As detailed in Supporting Information C, Equation (6) implies that Σ t = Σ t , W = W J and C = CJ, with J a diagonal matrix with ±1 elements on the diagonal. Then, Equation (4) implies that Ψ e = Ψ e , while Equation ( <ref type="formula" target="#formula_10">5</ref>) implies that Ψ f = Ψ f . As a result, the parameters of the PPLS-SVD model given in Equation (3) are identifiable (up to sign for the columns of W and C). In particular, because Cov(x, y) = W Σ t C , parameters W and C are identified (up to sign) as the first r left and right singular vectors of Cov(x, y), respectively. Moreover, because Var(x) = W Σ t W + Ψ e , and Var(y) = CΣ t C + Ψ f , with Ψ e and Ψ f two positive semi-definite matrices, we shall stress that W and C can generally not be identified from the eigendecomposition of Var(x) and Var(y), respectively. In other words, the two sets of weight matrices W and C define components with maximal covariance, which are not necessarily of respective maximal variances, and W and C cannot generally be identified separately from the marginal distributions of x and y. Our PPLS-SVD model can therefore be regarded as a more general probabilistic formulation of the PLS-SVD, which defines a much broader and interesting set of distributions than the original PPLS model of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>.</p><p>We will now conclude this Section with a few remarks on our model. First, we shall stress that the residuals, e and f of our model, may be more than simple noise terms. Indeed, they consist of everything that is not in the shared part between x and y. In particular, e may contain some signal from additional latent variables specific to x, as in the probabilistic PLS Regression model proposed by <ref type="bibr" target="#b28">Zheng et al. (2016)</ref>; see Equation ( <ref type="formula" target="#formula_15">7</ref>) below. Similarly, f may contain some signal from additional latent variables specific to y.</p><p>Second, two sets of components can be defined as linear transformations of x and y, respectively. As above, just as under the standard PLS-SVD <ref type="bibr" target="#b24">(Wegelin, 2000)</ref>, a first strategy consists in defining x = xW and y = yC. Following <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>, alternative components are defined as x * = E(t|x; θ ) and y * = E(t|y; θ ). As E(t|x;</p><formula xml:id="formula_12">θ ) = x W Σ t W + Ψ e -1 W Σ t and E(t|y; θ ) = y CΣ t C + Ψ f -1</formula><p>CΣ t , x * and y * are linear transformations of x and y too, but the linear sub-spaces corresponding to x * and y * , and x and y, respectively, differ, unless Ψ e and Ψ f are zero matrices <ref type="bibr" target="#b2">(Bach and Jordan, 2005)</ref>.</p><p>Finally, a last remark concerns the estimation of the parameters under our model. Parameters W and C could be estimated by performing a simple SVD of the covariance matrix Cov(X, Y). Alternatively, an EM algorithm would yield estimates for all the parameters θ , while taking into account all the constraints of the model. It would further allow various extensions, such as the inclusion of covariates, etc. However, the derivation of the EM algorithm is less straightforward under our extended model than under the original PPLS model. In particular, the updates in each of the M-steps of the EM for the parameters W and C require an optimization problem over the Stiefel Manifold to be solved <ref type="bibr">(Siegel, 2019, Wen and</ref><ref type="bibr" target="#b25">Yin, 2013)</ref>, while these updates have closed-form expressions under the original PPLS model of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>. Although we have not fully devised it, additional details on a possible EM algorithm are presented in Supporting Information D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Link between the alternative probabilistic formulation of</head><p>the PLS-SVD presented in Section 2.3 and the probabilistic formulation of the CCA proposed by <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref> As mentioned in the Introduction, <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref> proposed a probabilistic formulation of the CCA. Their PCCA model is defined by the following structural equations, which relate the two sets of observed variables x ∈ R p and y ∈ R q to one set of latent variables t ∈ R r ,</p><formula xml:id="formula_13">x = t W + e, y = t C + f ,</formula><p>together with the constraints ( α) t ∼ N(0 r , Σ t ).</p><p>( β ) Σ t is the r × r identity matrix.</p><p>( γ) e ∼ N(0 p , Ψ e ), with Ψ e a p × p semi-positive definite matrix.</p><p>( δ ) f ∼ N(0 q , Ψ f ), with Ψ f a q × q semi-positive definite matrix.</p><p>(ε) e and f are independent.</p><p>( ζ ) r &lt; min(p, q).</p><p>Both the PPCA model and our PPLS-SVD model proposed in Section 2.3 can be graphically represented as in Figure <ref type="figure">1</ref> B, and they share the same structural equations (see Equation ( <ref type="formula" target="#formula_8">3</ref>)). However, as they are probabilistic formulations of two different dimension-reduction techniques, they differ in the constraints imposed on their parameters. More precisely, constraint (b) in Section 2.3 differs from constraint ( β ) above, and the PPLS-SVD model further imposes constraint (g) on parameters W and C. In other words, in our probabilistic formulation of the PLS-SVD, if we no longer require W and C to be semi-orthogonal matrices, and if we further require Σ t to be the identify matrix, the model will coincide with the PCCA of <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>.</p><p>Note, the number of degrees of freedom for the variance matrices of the noise parts is the same in the two models. However, the number of independent parameters "of interest" under the PCCA of <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref> is (p + q)r, as parameters W and C have pr and qr degrees of freedom, respectively, while it equals (p + qr)r under our PPLS-SVD, as parameter Σ t has r degrees of freedom and parameters W and C have pr -r(r + 1) 2 and qr -r(r + 1) 2 degrees of freedom, respectively. In other words, our PPLS-SVD model is more constrained than the PCCA model proposed by <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>. <ref type="bibr" target="#b17">Li et al. (2015)</ref> and alternative probabilistic formulation of PLS Regression proposed by <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> As mentioned in the Introduction, probabilistic formulations of other PLS methods have been proposed in the literature. For example, <ref type="bibr" target="#b17">Li et al. (2015)</ref> propose a probabilistic formulation of PLS Regression. Their PPLSR model is defined by the following structural equations, which relate the two sets of observed variables x ∈ R p and y ∈ R q to one set of latent variables t ∈ R r ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Limitation of the PPLSR model proposed by</head><formula xml:id="formula_14">x = tW + e, y = tC + f , under the constraints (a) t ∼ N(0 r , I r ). (b) e ∼ N(0 p , σ 2 e I p ). (c) f ∼ N(0 q , σ 2 f I q ).</formula><p>(d) e and f are independent.</p><p>Although they do not clearly state it, <ref type="bibr" target="#b17">Li et al. (2015)</ref> implicitly assume that r &lt; p, as t is a "low-dimensional representation" of x. Contrary to our PPLS-SVD model given in Equation ( <ref type="formula" target="#formula_8">3</ref>) in Section 2.3, the weight matrices W and C are not supposed to be semi-orthogonal, and the components of the latent variable t are not only independent of each other, but also of unit variance. In that sense, the PPLSR model is inspired by the factor analysis model <ref type="bibr" target="#b3">(Basilevsky, 1994)</ref>, just as the PPCA model proposed by <ref type="bibr" target="#b23">Tipping and Bishop (1999)</ref> and recalled in Supporting Information B. More importantly, the error terms e and f are assumed to be of isotropic variances, just as in the original PPLS model proposed by el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>. <ref type="bibr" target="#b17">Li et al. (2015)</ref> do not study the identifiability of their model, but, if r &lt; p, it is easy to show that parameters σ 2 e and σ 2 f are identifiable, and parameters W and C are identifiable up to an orthogonal transformation.</p><p>As a matter of fact, if r &lt; min(p, q), the PPLSR model corresponds to the PCCA model proposed by <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>, but with the additional constraint of isotropic variances for the error terms. Just as in the original PPLS model of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>, this constraint is too strong: if r &lt; min(p, q), the PPLSR model of <ref type="bibr" target="#b17">Li et al. (2015)</ref> defines a very particular set of distributions, under which the theoretical solutions of the PPLSR (and hence, of the PCCA) model, W and C, are also necessarily solutions of two PPCA models (in the sense of <ref type="bibr" target="#b23">Tipping and Bishop (1999)</ref>) for x and y, respectively.</p><p>In the context of PLS Regression, it is maybe more sensible to assume that q ≤ r &lt; p. But then, the PPLSR model proposed by <ref type="bibr" target="#b17">Li et al. (2015)</ref> still defines a set of particular distributions for (x, y), under which the solution W of the PPLSR is also necessarily solution of a PPCA model for x. This observation confirms what <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> already suggested: the PPLSR model proposed by <ref type="bibr" target="#b17">Li et al. (2015)</ref> is not an appropriate probabilistic formulation of PLS Regression. <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> then proposed an extension of the PPLSR model, defined by the following structural equations, which relate the two sets of observed variables x ∈ R p and y ∈ R q to two sets of latent variables t ∈ R r and u ∈ R s ,</p><formula xml:id="formula_15">x = tW + uQ + e, y = tC + f ,<label>(7)</label></formula><p>under the constraints</p><formula xml:id="formula_16">(a*) t ∼ N(0 r , I r ). (b*) u ∼ N(0 s , I s ). (c*) e ∼ N(0 p , Σ e ). (d*) f ∼ N(0 q , Σ f ).</formula><p>(e*) Σ e is a p × p diagonal matrix, with strictly positive diagonal elements.</p><p>(f*) Σ f is a q × q diagonal matrix, with strictly positive diagonal elements.</p><p>(g*) e and f are independent.</p><p>Again, <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> implicitly assume that p &gt; max(r, s).</p><p>The inclusion of the additional set of latent variables in the model, u, which is related to x only, has the same impact in terms of identifiability of the parameter W as the relaxation of the assumption of isotropic variance for the error terms e that we considered in our PPLS-SVD model (see Equation ( <ref type="formula" target="#formula_8">3</ref>)). In particular, the model proposed by <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> defines a more general set of distributions than the original model of <ref type="bibr" target="#b17">Li et al. (2015)</ref>, and the weight parameter W can generally not be identified from the marginal distribution of x only.</p><p>It is noteworthy that <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> do not consider the inclusion of an additional set of latent variables related to y only. Consequently, observing that the variances of the error terms e and f are now simply assumed to be diagonal with positive diagonal elements, the marginal distribution of y is still sufficient to identify C (up to an orthogonal transformation) if this matrix satisfies the condition given in the Theorem 5.1 of <ref type="bibr" target="#b1">Anderson and Rubin (1956)</ref>, and if r &lt; q. In the context of PLS regression, this is not necessarily a limitation. Moreover, the inclusion of a third set of latent variables in the model, related to y only, would define a model equivalent to the PCCA model of <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>.  <ref type="formula">2018</ref>) under the original PPLS model, depending on whether this model is correctly specified or not. For comparison, we further considered estimates returned by the standard (non-probabilistic) PLS-SVD, and the standard PCA (successively applied on the "x and y parts" of the data). The PLS-W2A, which is another symmetrical PLS method that we briefly described in the Introduction (see <ref type="bibr" target="#b19">Rosipal and Krämer (2006)</ref>, <ref type="bibr" target="#b24">Wegelin (2000)</ref>, <ref type="bibr" target="#b26">Wold (1985)</ref> for more details), was originally considered too. As expected, estimates returned by the PLS-W2A and PLS-SVD methods were very similar under the original PPLS model (because Var(xW ) and Var(yC) are diagonal under the original PPLS model). But, as they were very similar in the second simulation study too, we finally decided to omit the presentation of the results from PLS-W2A.</p><p>We set the dimensions of the observed sets of variables x and y to p = q = 20, the dimension of the sets of latent variables to r = 3, and make the sample size vary in n ∈ {50, 250, 500, 1000, 5000}. In the first simulation study, we work under the same setting as that considered by el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref> in their simulation study. More precisely, data (X, Y) are generated under the original PPLS model, in the particular case where the diagonal elements of both Σ t and Σ t B 2 are all distinct. Weight matrices W and C are randomly drawn from the sets of semi-orthogonal matrices of size p × r and size q × r, respectively, and the diagonal elements of Σ t and B are respectively set to σ 2 t i = exp(-(i -1)/5) and b i = 1.5exp(3(i -1)/10), for i ∈ {1, 2, 3}, just as in el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>. As for the variances of e, f and h, they are chosen so that the signal-to-noise ratios are equal to 0.25: σ 2 e = 0.4, σ 2 f = 4 and σ 2 h = 5.33. The main objective of this first study is to empirically confirm that, when the original PPLS model of el Bouhaddani et al. ( <ref type="formula">2018</ref>) is correctly specified, the weights returned by the corresponding EM algorithm are similar to those returned by two PCAs applied to the x and y parts of the data. In the second simulation study, data are generated under a model similar to the original PPLS model, except that e and f are not of isotropic variance anymore; instead e and f are drawn from multivariate Gaussian variables with arbitrary positive semi-definite variance matrices. More precisely, to make sure we work under really misspecified models where solutions of the PLS-SVD differ from solutions of two PCAs, we chose positive-definite matrices ensuring that eigenvectors of matrices Var(x) and Var(y) were not too close to the left and right singular vectors of Cov(x, y) (using a simple acceptance rejection method). The main objective of this second study is to describe how the solutions of the EM algorithm of el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref> behaves when components of maximal covariance are not of respective maximal variances too, that is when the original PPLS model is misspecified. In both studies, the results are computed over 1000 replicates. For the comparisons of weight vectors, we use the cosine similarity, which simply reduces to the dot product in our case since both the true and estimated weight vectors are of unit length. Results from our simulation studies can be replicated using our R scripts available on GitHub.</p><p>Figure <ref type="figure">2</ref> presents the median of the cosine similarity (in absolute values) between the true weights W and C and their estimates, when the original PPLS model is correctly specified (top panels), and when it is not (bottom panels). Each of the three columns of Figure <ref type="figure">1</ref> presents the results for one particular pair (W i ,C i ) i∈{1,2,3} . We shall stress that the columns of the estimated weight matrices returned by each of the three compared methods were first re-arranged to make sure they matched the ordering of the true weight matrices, just as in the simulation study conducted by el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>.</p><p>When the PPLS model is correctly specified (top panels of Figure <ref type="figure">2</ref>), estimates returned by the EM algorithm under the original PPLS models perform similarly to estimates returned by the other PLS techniques (PLS-SVD and PLS-W2A), and they are all reasonably close to the true weight vectors. In particular, their cosine similarity with the true weight vectors tend to 1 as sample size increases. But, as expected, this is also the case for the estimates returned by two PCAs successively applied to X and Y. This empirically confirms that when the diagonal elements of both Σ t and Σ t B 2 are all distinct under the original PPLS model, solutions of the PLS-SVD coincide with those of the PCAs, limiting the interest of the PLS-SVD in such cases. Moreover, we shall recall that when the diagonal elements of Σ t and/or Σ t B 2 are not all distinct, solutions of the PLS-SVD still constitute one of the solutions of the PCAs, indicating that whenever the PPLS model is correctly specified, it is of limited practical interest as its solutions are particular solutions of the PCAs.</p><p>On the other hand, when the original PPLS model is misspecified (bottom panels of Figure <ref type="figure">2</ref>), our results show that estimates returned by the two PCAs are quite far from the true weight vectors (as expected, by design), while those returned by the PLS-SVD still perform well. As for the EM algorithm devised under the original PPLS model, it performs much worse than the PLS-SVD, and not much better than the two PCAs. To better describe the estimates returned by the EM algorithm devised under the original PPLS model, Figure <ref type="figure">3</ref> presents the median of the cosine similarities (in absolute values) between these estimates and those returned by (i) the two distinct PCAs and (ii) the standard PLS-SVD. Interestingly, these results show that, on average, estimates returned by the EM algorithm under the original PPLS model are closer to those returned by the PCAs, especially when the original PPLS model is misspecified. Figure <ref type="figure">1</ref> in Supporting Information E further presents the box-plots of the absolute value of the cosine similarities between the estimates returned by the EM algorithm devised under the original PPLS model and those returned by (i) two distinct PCAs, and (ii) the standard PLS-SVD, in our second simulation study (when the original PPLS model is misspecified). These box-plots suggest that, when solutions of the PLS-SVD differ from solutions of two PCAs, estimates returned by the EM algorithm proposed by el Bouhaddani et al. ( <ref type="formula">2018</ref>) are generally closer to those returned by the two PCAs.</p><p>All these empirical results confirm that the EM algorithm devised by el Bouhaddani et al. ( <ref type="formula">2018</ref>) under their PPLS model suffers from a severe limitation: in real-life examples, there is no guarantee that the estimated weight vectors it returns really capture the relationship between x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our results stress that some caution is needed when developing and applying latent variable models for dimension-reduction: when imposing too strong constraints on the model parameters, a model whose structural equations seem to correctly describe the relationships between the observed variables, may turn out to be too simplistic. It can define very particular distributions, under which parameters of interest could be obtained under simpler models, and this greatly limits its applicability. In particular, we showed that the constraints considered in the probabilistic formulation of PLS-SVD of el Bouhaddani et al. ( <ref type="formula">2018</ref>) are too strong: they imply that the weight matrices W and C of their PPLS model are also necessarily solutions of two distinct PPCA models for x and y, respectively. As a result, the original PPLS model defines a very particular subset of distributions for the pair (x, y), under which the two sets of components of maximal covariance are necessarily of respective maximal variances too. Although not striking, this defect severely limits the practical interest of this model. In the same way, the con- straints used in the probabilistic formulation of PLS Regression of <ref type="bibr" target="#b17">Li et al. (2015)</ref> are too strong: they imply that the weight matrix W of their PPLSR model is also necessarily solution of a PPCA model for x (in the sense of <ref type="bibr" target="#b23">Tipping and Bishop (1999)</ref>).</p><p>As shown in the present article, it is sometimes possible to correct for these defects. <ref type="bibr" target="#b28">Zheng et al. (2016)</ref> proposed an alternative probabilistic formulation of PLS Regression than the one of <ref type="bibr" target="#b17">Li et al. (2015)</ref>, under which the joint distribution of (x, y) is in general necessary for the identification of the weight matrix allowing the construction of the predictors. On the other hand, in the case of the PPLS model originally proposed by el <ref type="bibr" target="#b4">Bouhaddani et al. (2018)</ref>, we were able to relax some of the constraints, and develop an alternative probabilistic formulation of the PLS-SVD, under which the joint distribution of (x, y) is generally necessary for the identification of the weight matrices. However, the implementation of an EM algorithm for the estimation of the parameters is less straightforward under the PPLS-SVD model. In particular, each M-step of the algorithm requires a numerical optimization step to update the estimates of parameters W and C, whereas such updates have closed-form expressions under the original PPLS model. Alternatively, we could propose another version of the model, where parameters W and C would not have to be semi-orthogonal matrices; this would then simplify the EM algorithm. But for the model to be identifiable, we would have to impose Σ t = I r ; identifiability would then hold up to an orthogonal transformation for parameters W and C. However, in that case, the corresponding model would coincide precisely with the PCCA model proposed by <ref type="bibr" target="#b2">Bach and Jordan (2005)</ref>; see Section 2.4. In particular, it would no longer be a probabilistic formulation of the PLS-SVD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Graphical representations for: A -The PPLS model proposed by el Bouhaddani et al. (2018) and recalled in Equation (1). B -Our PPLS-SVD model given in Equation (3). Note that the later has one set of latent variables t only. Moreover, the structure of the noise parts e and f differs between the two models (see Equations (1) and (3) below). In both models, x and y are the observed variables whereas circled variables are unobserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label></label><figDesc>Illustration of the limitation of the PPLS model proposed by el Bouhaddani et al. (2018) Now, we present results from two simulation studies aimed to illustrate the limitations of the PPLS model of el Bouhaddani et al. (2018) (see Equation (1)). More precisely, our objective is to illustrate the behavior of the estimates for W and C returned by the EM algorithm devised by el Bouhaddani et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure2: Medians of the cosine similarities (in absolute values) between the true weight vectors and estimates returned by (i) the PPLS EM algorithm, (ii) two distinct PCAs on X and Y, and (iii) PLS-SVD on (X, Y). The results are computed over 1000 replicates, for p = q = 20, r = 3 and different sample sizes n ∈ {50, 250, 500, 1000, 5000}. The top panels correspond to the first simulation study where the original PPLS model is correctly specified, while the bottom panels correspond to the second simulation study where the original PPLS model is misspecified.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors are grateful to <rs type="person">Anne-Laure Fougères</rs>, <rs type="person">Thibault Espinasse</rs>, <rs type="person">Edouard Ollier</rs> and <rs type="person">Franck Picard</rs> for fruitful discussion and comments on earlier versions of this manuscript, and to the referees of <rs type="person">Statistica Neerlandica</rs> for their valuable suggestions.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The final published article and its Supporting Information are available at the Statistica Neerlandica website.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement</head><p>The R code created to generate and analyze the data that support the findings of this study is openly available in repository PPLS-SVD at <ref type="url" target="https://github.com/Etievant/PPLS-">https://github.com/Etievant/PPLS-</ref>SVD.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclaimer</head><p>Where authors are identified as personnel of the International Agency for Research on Cancer/World Health Organization, the authors alone are responsible for the views expressed in this article and they do not necessarily represent the decisions, policy or views of the International Agency for Research on Cancer/World Health Organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting information</head><p>Supporting Information may be found online in the Supporting Information Section at the end of this article.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Esposito Vinzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Russolillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trinchera</surname></persName>
		</author>
		<title level="m">New Perspectives in Partial Least Squares and Related Methods</title>
		<imprint>
			<publisher>Springer Proceedings in Mathematics &amp; Statistics</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical inference in factor analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, Calif</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956">1956</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="111" to="150" />
		</imprint>
	</monogr>
	<note>Contributions to Econometrics, Industrial Research, and Psychometry</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley, Department of Statistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical Factor Analysis and Related Methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basilevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic partial least squares model: identifiability, estimation and application</title>
		<author>
			<persName><forename type="first">S</forename><surname>El Bouhaddani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jongbloed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Houwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="331" to="346" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational inference for probabilistic poisson pca</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chiquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2674" to="2698" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High dimensional mediation analysis with latent variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Derkach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Sampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="745" to="756" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic count matrix factorization for single cell expression data analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Durif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lambert-Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4011" to="4019" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="417" to="441" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relations between two sets of variables</title>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pls regression methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Höskuldsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemometrics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="211" to="228" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal component analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal component analysis: a review and recent developments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cadima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<date type="published" when="2016">2016. 20150202</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Soft modeling: The basic design and some extensions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jöreskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems under indirect observation. Causality, structure, prediction. (Conference held October 18-20</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Joreskog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1979">1982. 1979</date>
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partial least squares (pls) methods for neuroimaging: A tutorial and review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="455" to="475" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic partial least squares regression for quantitative analysis of raman spectra</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nyagilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Mining and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic penalized principal component analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications for Statistical Applications and Methods</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krämer</surname></persName>
		</author>
		<editor>C. Saunders, M. Grobelnik, S. Gunn and J. Shawe-Taylor</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="34" to="51" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neurobehavioral effects of prenatal alcohol: Part ii. partial least squares analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Streissguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurotoxicology and Teratology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="477" to="491" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Accelerated optimization with orthogonality constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Siegel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05204</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Smilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
		<title level="m">Multi Way Analysis -Applications in Chemical Sciences</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey of partial least squares (pls) methods, with emphasis on the two-block case</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wegelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington, Department of Statistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A feasible method for optimization with orthogonality constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="397" to="434" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Partial least squares</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Encyclopedia of Statistical Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="581" to="591" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse probabilistic principal component analysis model for plant-wide process monitoring</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Korean Journal of Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic learning of partial least squares regression model: Theory and industrial applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="80" to="90" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
