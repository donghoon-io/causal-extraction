<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion</title>
				<funder ref="#_du75auA">
					<orgName type="full">XEROX University Affairs Committee</orgName>
					<orgName type="abbreviated">UAC</orgName>
				</funder>
				<funder ref="#_gN8J2K4">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Israel</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
							<email>israel--dejene.gebru@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sileye</forename><surname>Ba</surname></persName>
							<email>sileye.ba@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaofei</forename><surname>Li</surname></persName>
							<email>xiaofei.li@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
							<email>radu.horaud@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silèye</forename><surname>Ba</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA Grenoble Rhône-Alpes</orgName>
								<orgName type="institution" key="instit2">Montbonnot Saint-Martin</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2017.2648793</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speaker diarization</term>
					<term>audio-visual tracking</term>
					<term>dynamic Bayesian network</term>
					<term>sound source localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speaker diarization consists of assigning speech signals to people engaged in a dialogue. An audio-visual spatiotemporal diarization model is proposed. The model is well suited for challenging scenarios that consist of several participants engaged in multi-party interaction while they move around and turn their heads towards the other participants rather than facing the cameras and the microphones. Multiple-person visual tracking is combined with multiple speech-source localization in order to tackle the speech-to-person association problem. The latter is solved within a novel audio-visual fusion method on the following grounds: binaural spectral features are first extracted from a microphone pair, then a supervised audio-visual alignment technique maps these features onto an image, and finally a semisupervised clustering method assigns binaural spectral features to visible persons. The main advantage of this method over previous work is that it processes in a principled way speech signals uttered simultaneously by multiple persons. The diarization itself is cast into a latent-variable temporal graphical model that infers speaker identities and speech turns, based on the output of an audio-visual association process, executed at each time slice, and on the dynamics of the diarization variable itself. The proposed formulation yields an efficient exact inference procedure. A novel dataset, that contains audio-visual training data as well as a number of scenarios involving several participants engaged in formal and informal dialogue, is introduced. The proposed method is thoroughly tested and benchmarked with respect to several state-of-the art diarization algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In human-computer interaction (HCI) and human-robot interaction (HRI) it is often necessary to solve multi-party dialogue problems. For example, if two or more persons are engaged in a conversation, one important task to be solved, prior to automatic speech recognition (ASR) and natural language processing (NLP), is to correctly assign temporal segments of speech to corresponding speakers. In the speech and language processing literature this problem is referred to as speaker diarization, or "who speaks when?" A number of diarization methods were recently proposed, e.g. <ref type="bibr" target="#b0">[1]</ref>. If only unimodal data are available, the task is extremely difficult. Acoustic data are inherently ambiguous because they contain mixed speech signals emitted by several persons, corrupted by reverberations, by other sound sources and by background noise. Likewise, the detection of speakers from visual data is very challenging and it is limited to lip and facial motion detection from frontal close-range images of people: in more general settings, such as informal gatherings, people are not always facing the cameras, hence lip reading cannot be readily achieved.</p><p>Therefore, an interesting and promising alternative consists of combining the merits of audio and visual data. The two modalities provide complementary information and hence audio-visual approaches to speaker diarization are likely to be more robust than audio-only or vision-only approaches. Several audio-visual diarization methods have been investigated for the last decade, e.g. <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Diarization is based on audio-visual association, on the premise that a speech signal coincides with the visible face of a speaker. This coincidence must occur both in space and time.</p><p>In formal scenarios, e.g. meetings, diarization is facilitated by the fact that participants take speech turns, which results in (i) a clear-cut distinction between speech and non-speech and (ii) the presence of short silent intervals between speech segments. Moreover, participants are seated, or are static, and there are often dedicated close-field microphones and cameras for each participant e.g. <ref type="bibr" target="#b7">[8]</ref>. In these cases, the task consists of associating audio signals that contain clean speech with frontal images of faces: audio-visual association methods based on temporal coincidence between the audio and visual streams seem to provide satisfactory results, e.g. canonical correlation analysis (CCA) <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> or mutual information (MI) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Nevertheless, temporal association between the two modalities is only effective on the premises that (i) speech segments are uttered by a single person at a time, that (ii) single-speaker segments are relatively long, and that (iii) speakers continuously face the cameras.</p><p>In informal scenarios, e.g. ad-hoc social events, the audio signals are provided by distant microphones, hence the signals are corrupted by environmental noise and by reverberations. Speakers interrupt each other, hence short speech signals may occasionally be uttered simultaneously by different speakers. Moreover, people often wander around, turn their head away from the cameras, may be occluded by other people, suddenly appear or disappear from the cameras' fields of view, etc. Some of these problems were addressed in the framework of audio-visual speaker tracking, e.g. <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, audio-visual tracking is mainly concerned with finding speaker locations and speaker trajectories, rather than solving the speaker diarization problem.</p><p>In this paper it is proposed a novel spatiotemporal diarization model that is well suited for challenging scenarios that consist of several participants engaged in multi-party dialogue. The participants are allowed to move around and to turn their heads towards the other participants rather than facing the cameras. We propose to combine multiple-person visual tracking with multiple speech source localization in order to tackle the speech to person association problem. The latter is solved within a novel audio-visual fusion method on the following grounds: acoustic spectral features are extracted from a microphone pair, a novel supervised audio-visual alignment technique maps these features onto the image plane such that the audio and visual modalities are represented in the same mathematical space, a semi-supervised clustering method assigns the acoustic features to visible persons. The main advantage of this method over previous work is twofold: it processes in a principled way speech signals uttered simultaneously by multiple persons, and it enforces spatial coincidence between audio and visual features.</p><p>Moreover, we cast the diarization process into a latentvariable temporal graphical model that infers over time both speaker identities and speech turns. This inference is based on combining the output of the proposed audio-visual fusion, that occurs at each time-step, with a dynamic model of the diarization variable (from the previous time-step to the current time-step), i.e. a state transition model. We describe in detail the proposed formulation which is efficiently solved via an exact inference procedure. We introduce a novel dataset that contains audio-visual training data as well as a number of scenarios involving several participants engaged in formal and informal dialogue. We thoroughly test and benchmark the proposed method with respect to several state-of-the art diarization algorithms.</p><p>The remainder of this paper is organized as follows. Section II describes the related work. Section III describes in detail the temporal graphical model. Section IV describes visual feature detection and Section V describes the proposed audio features and their detection. Section VI describes the proposed semi-supervised audio-visual association method. The novel audio-visual dataset is presented in detail in Section VII while numerous experiments, tests, and benchmarks are presented in Section VIII. Finally, Section IX draws some conclusions. Videos, Matlab code and additional examples are available online. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The task of speaker diarization is to detect speech segments and to group segments that correspond to the same speaker without any prior knowledge about the speakers involved nor their number. This can be done using auditory features alone, or a combination of auditory and visual features. Mel frequency cepstral coefficients (MFCC) is often the representation of choice whenever audio signal segments correspond to a single speaker. Then the diarization pipeline consists of 1 <ref type="url" target="https://team.inria.fr/perception/avdiarization/">https://team.inria.fr/perception/avdiarization/</ref> splitting the audio frames into speech and non-speech frames, of extracting an MFCC feature vector from each speech frame and of performing agglomerative clustering such that each cluster found at the end corresponds to a different speaker <ref type="bibr" target="#b16">[17]</ref>. Consecutive speech frames are assigned either to the same speaker and grouped into segments, or to different speakers, by using a state transition model, e.g. HMM.</p><p>The use of visual features for diarization has been motivated by the importance of audio-visual synchrony. Indeed, it was shown that facial and lip movements are strongly correlated with speech production <ref type="bibr" target="#b17">[18]</ref> and hence visual features, extracted from frontal views of speaker faces, can be used to increase the discriminative power of audio features in numerous tasks, e.g. speech recognition <ref type="bibr" target="#b18">[19]</ref>, source separation, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and diarization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. In the latter case, the most common approaches involve the analysis of temporal correlation between the two modalities such that the face/lip movements that best correlate with speech correspond to an active speaker.</p><p>Garau et al. <ref type="bibr" target="#b1">[2]</ref> compare two audio-visual synchronization methods, based on mutual information (MI) and on canonical correlation analysis (CCA), and using MFCC auditory features combined with motion amplitude computed from facial feature tracks. They conclude that MI performs slightly better than CCA and that vertical facial displacements (lip and chin movements) are the visual features the most correlated with speech production. MI that combines gray-scale pixel-value variations extracted from a face region with acoustic energy is also used by Noulas et al. <ref type="bibr" target="#b2">[3]</ref>. The audio-visual features thus extracted are plugged into a dynamic Bayesian network (DBN) that perform speaker diarization. The method was tested on video meetings involving up to four participants which are recorded with several cameras, such that each camera faces a participant. More recently, both El Khoury et al. <ref type="bibr" target="#b3">[4]</ref> and Kapsouras et al. <ref type="bibr" target="#b6">[7]</ref> propose to cluster audio features and face features independently and then to correlated these features based on temporal alignments between speech and face segments.</p><p>The methods mentioned so far yield good results whenever clean speech signals and frontal views of faces are available. A speech signal is said to be clean if it is noise free and if it corresponds to a single speaker; hence audio clustering based on MFCC (mel-frequency cepstral coefficients) features performs well. Moreover, time series of MFCC features seem to correlate well with facial-feature trajectories. If several faces are present, it is possible to select the facial feature trajectory that correlate the most with the speech signal, e.g. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, in realistic settings, participants are not always facing the camera, consequently the detection of facial and lip movements is problematic. Moreover, methods based on cross-modal temporal correlation, e.g. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref> require long sequences of audiovisual data, hence they can only be used offline such as the analysis of broadcast news, of audiovisual conferences, etc.</p><p>In the presence of simultaneous speakers, the task of diarization is more challenging because multiple-speaker infor-mation must be extracted from the audio data, one one hand, and the speech-to-face association problem must be properly addressed, on the other hand. In mixed-speech microphone signals, or dirty speech, there are many audio frames that contain acoustic features uttered by several speakers and MFCC features are not reliable anymore because they are designed to characterize acoustic signals uttered by single speakers. The multi-speech-to-multi-face association problem cannot be solved neither by performing temporal correlation between a single microphone signal and an image sequence nor by clustering MFCC features.</p><p>One way to overcome the problems just mentioned is to perform multiple speech-source localization <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> and to associate speech sources with persons. These methods, however, do not address the problems of aligning speechsource locations with visible persons and of tracking them over time. Moreover, they often use circular or linear microphone arrays, e.g. planar microphone setups, hence they provide sound-source directions with one degree of freedom, e.g. azimuth, which may not be sufficient to achieve robust audiovisual association. Hence, some form of microphone-camera calibration is needed. <ref type="bibr">Khalidov et al.</ref> propose to estimate the microphone locations into a camera-centered coordinate system <ref type="bibr" target="#b27">[28]</ref> and to use a binocular-binaural setup in order to jointly cluster visual and auditory feature via a conjugate mixture model <ref type="bibr" target="#b28">[29]</ref>. Minotto et al. <ref type="bibr" target="#b4">[5]</ref> learn an SVM classifier using labeled audio-visual features. This training is dependent on the acoustic properties of experimental setup. They combine voice activity detection with sound-source localization using a linear microphone array which provides horizontal (azimuth) speech directions. In terms of visual features, their method relies on lip movements, hence frontal speaker views are required.</p><p>Multiple-speaker scenarios were thoroughly addressed in the framework of audio-visual tracking. Gatica-Perez et al. <ref type="bibr" target="#b13">[14]</ref> proposed a multi-speaker tracker using approximate inference implemented with a Markov chain Monte Carlo particle filter (MCMC-PF). Navqi et al. <ref type="bibr" target="#b14">[15]</ref> proposed a 3D visual tracker, based as well on MCMC-PF, to estimate the positions and velocities of the participants which are then passed to blind source separation based on beamforming <ref type="bibr" target="#b29">[30]</ref>. Reported experiments of both <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> require a network of distributed cameras to guarantee that frontal views of the speakers are always available. More recently, Kilic et al. <ref type="bibr" target="#b15">[16]</ref> proposed to use audio information to assist the particle propagation process and to weight the observation model. This implies that audio data are always available and that they are reliable enough to properly relocate the particles. While audio-visual multipleperson tracking methods provide an interesting methodology, they do not address the diarization problem. Indeed, they assume that people speak continuously, which facilitates the task of the proposed audio-visual trackers. With the exception of <ref type="bibr" target="#b14">[15]</ref>, audio analysis is reduced to sound-source localization using a microphone array, and this in order to enforce spatial coincidence between faces and speech.</p><p>Recently we addressed audio-visual speaker diarization under the assumption that participants take speech turns and that there is no overlap between their emitted speech signals.</p><p>We proposed a simple model that consists of a speech-turn discrete latent variable that associates the speech signal with one of the participants <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The main idea of this work was to track multiple persons and to extract a single soundsource direction from short time intervals, e.g. using <ref type="bibr" target="#b32">[33]</ref> to map sound directions onto the image plane. Audio and visual observations can then be associated using a recently proposed weighted-data EM algorithm <ref type="bibr" target="#b33">[34]</ref>. In the present paper we propose a novel dynamic audio-visual fusion model that can deal with simultaneously speaking participants. In particular, we exploit the spectral sparsity of speech signals and we propose a novel multiple speech source localization method based on a semi-supervised complex-Gaussian mixture model in the Fourier domain. We also generalize the single speakerturn diarization model of <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> to multiple speaking persons.</p><p>Recently we addressed audio-visual speaker diarization under the assumption that participants take speech turns and that there is no overlap between their speech segments. We proposed a model that consists of a speech-turn discrete latent variable that associates the current speech signal, if any, with one of the visible participants <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The main idea was to perform multiple-person tracking in the visual domain, to extract sound-source directions (one direction at a time), and to map this sound direction onto the image plane <ref type="bibr" target="#b32">[33]</ref>. Audio and visual observations can then be associated using a recently proposed weighted-data EM algorithm <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this present paper we propose a novel DBN-based crossmodal diarization model. Unlike several recently proposed audio-visual diarization works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the proposed model can deal with simultaneously speaking participants that may wander around and turn their faces away from the cameras. Unlike <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> which require long sequences of past, present, and future frames, and hence are well suited for post-processing, our method is causal and therefore it can be used online. To deal with mixed speech signals, we exploit the sparsity of speech spectra and we propose a novel multiple speech-source localization method based on audio-visual data association implemented with a cohort of frequency-wise semi-supervised complex-Gaussian mixture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head><p>We start by introducing a few notations and definitions. Unless otherwise specified, upper-case letters denote random variables while lower-case letters denote their realizations. Vectors are in slanted bold, e.g. X, Y , while matrices are in bold, e.g. X, Y. We consider an image sequence that is synchronized with two microphone signals and let t denote the time-step index of the audio-visual stream of data.</p><p>Let N be the maximum number of visual objects, e.g. persons, available at any time t. Hence at t we have at most N persons with locations on the image plane X t = (X t,1 , . . . , X t,n , . . . , X t,N ) ∈ R 2×N , where the observed random variable X t,n ∈ R 2 is the pixel location of person n at t. We also introduce a set of binary (or control) variables</p><formula xml:id="formula_0">V t = (V t,1 , . . . V t,n , . . . V t,N ) ∈ {0, 1} N such that V t,n = 1 if person n is visible at t and V t,n = 0 if the person is not visible. Let N t = n V t,</formula><p>n denote the number of visible persons at t. The time series X 1:t = {X 1 , . . . , X t } and associated visibility binary masks V 1:t = {V 1 , . . . , V t } can be estimated using a multi-person tracker, i.e. Section IV.</p><p>We now describe the audio data. Without loss of generality, the audio signals are recorded with two microphones: let</p><formula xml:id="formula_1">Y t = (Y t,1 , . . . , Y t,k , . . . , Y t,K ) ∈ C F ×K be a binaural spectrogram containing F number of frequencies and K number of frames. Each frame is a binaural vector Y t,k ∈ C F , 1 ≤ k ≤ K.</formula><p>Binaural spectrograms are obtained in the following way. The short-time Fourier transform (STFT) is first applied to the left-and right-microphone signals acquired at time-step t such that two spectrograms, L t , R t ∈ C F ×K are associated with the left and right microphones, respectively. Each spectrogram is composed of</p><formula xml:id="formula_2">F × K complex-valued STFT coefficients. The binaural spectrograms Y t is composed of F × K complex-valued coefficients and each coefficients Y f t,k , 1 ≤ f ≤ F and 1 ≤ k ≤ K</formula><p>, can be estimated from the corresponding left-and right-microphone STFT coefficients L f t,k and R f t,k , i.e. Section V. One important characteristic of speech signals is that they have sparse spectrograms. As explained below, this sparsity is explicitly exploited by the proposed speech-source localization method. Moreover, the microphone signals are obviously contaminated by background noise and by sounds emitted by other non-speech sources. Therefore, speech activity associated with each binaural spectrogram entry Y f t,k must be properly detected and characterized with the help of a binary-mask matrix A t ∈ {0, 1} F ×K : A f t,k = 1 if the corresponding spectrogram coefficient contains speech, and A f t,k = 0 if it does not contain speech. To summarize, the binaural spectrograms Y 1:t = {Y 1 , . . . , Y t } and associated speech-activity masks A 1:t = {A 1 , . . . , A t } characterize the audio observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speaker Diarization Model</head><p>We remind that the objective of our work is to assign speech signal to persons, which amounts to one-to-one spatiotemporal associations between several speech sources (if any) and one or several observed persons. For this purpose we introduce a time series of discrete latent variables, S 1:t = {S 1 , . . . , S t } ∈ {0, 1} N ×t where the vector S t = (S t,1 , . . . , S t,n , . . . , S t,N ) ∈ {0, 1} N has binary-valued entries such that S t,n = 1 if person n speaks during the time-step t, and S t,n = 0 if person n is silent. The temporal speaker diarization problem at hand can be formulated as finding a maximum-a-posteriori (MAP) solution, namely finding the most probable configuration of the latent state S t that maximizes the following posterior probability distribution, also referred to as the filtering distribution:</p><formula xml:id="formula_3">ŝt = argmax st P (S t = s t |x 1:t , y 1:t , v 1:t , a 1:t ).</formula><p>(1)</p><p>We introduce the notation U t = (X t , Y t , A t ) for the observed variables, while the V t are referred to as control variables. The filtering distribution (1) can be expanded as:</p><formula xml:id="formula_4">P (s t |u 1:t , v 1:t ) = P (u t |s t , u 1:t-1 , v 1:t )P (s t |u 1:t-1 , v 1:t ) P (u t |u 1:t-1 , v 1:t ) = P (u t |s t , v t )P (s t |u 1:t-1 , v 1:t ) P (u t |u 1:t-1 , v 1:t ) . (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>We assumed that the observed variables U t are conditionally independent of all other variables, given the speaking state S t and control input V t ; S t is conditionally independent of S 1 , . . . , S t-2 , given S t-1 and V t-1:t . Fig. <ref type="figure">1</ref> shows the graphical model representation of the proposed model.</p><p>The numerator of ( <ref type="formula" target="#formula_4">2</ref>) is the product of two terms: the observation likelihood (left) and the predictive distribution (right). The observation likelihood can be expanded as:</p><formula xml:id="formula_6">P (u t |s t , v t ) = N n=1 P (u t |S t,n = 1, V t,n ) st,n × P (u t |S t,n = 0, V t,n ) 1-st,n . (3)</formula><p>The predictive distribution (right hand side of the numerator of ( <ref type="formula" target="#formula_4">2</ref>)) expands as:</p><formula xml:id="formula_7">P (s t |u 1:t-1 , v 1:t ) = st-1 P (s t , s t-1 |u 1:t-1 , v 1:t ) = st-1 P (s t |s t-1 , u 1:t-1 , v 1:t )P (s t-1 |u 1:t-1 , v 1:t ) = st-1 P (s t |s t-1 , v t , v t-1 )P (s t-1 |u 1:t-1 , v 1:t-1 ) = st-1 N m=1 P (s t,m |s t-1,m , v t,m , v t-1,m )<label>(4)</label></formula><formula xml:id="formula_8">× P (s t-1 |u 1:t-1 , v 1:t-1 ),<label>(5)</label></formula><p>which is the product of the state transition probabilities (4) and of the filtering distribution at t -1 (5). We now expand the denominator of (2):</p><formula xml:id="formula_9">P (u t |u 1:t-1 , v 1:t ) = st P (u t , s t |u 1:t-1 , v 1:t ) = st P (u t |s t , v t )P (s t |u 1:t-1 , v 1:t ). (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>To summarize, the evaluation of the filtering distribution at an arbitrary time-step t requires the evaluation of (i) the observation likelihood (3), i.e. Section VI, (ii) the state transition probabilities (4), i.e. Section III-B, (iii) the filtering distribution at t -1 (5), and of (iv) the normalization term <ref type="bibr" target="#b5">(6)</ref>. Notice that the number of possible state configuration is 2 N where N is the maximum number of people. For small values of N (2 to 6 persons), solving the MAP problem ( <ref type="formula">1</ref>) is computationally efficient.</p><formula xml:id="formula_11">St-1,N Vt-1,N St-1,n Vt-1,n St-1,1 Vt-1,1 Ut-1 St,N Vt,N St,n Vt,n St,1 Vt,1 Ut St+1,N Vt+1,N St+1,n Vt+1,n</formula><p>St+1,1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. State Transition Model</head><p>Priors over the dynamics of the state variables in (4) exploit the simplifying assumption that the speaking dynamics of a person is independent of all the other persons. Several existing speech-turn models rely on non-verbal cues, such as filled pauses, breath, facial gestures, gaze, etc. <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and a speech-turn classifier can be built from annotated dialogues. The state transition model of <ref type="bibr" target="#b2">[3]</ref> considers all possible transitions, e.g., speaking/non-speaking, visible/not-visible, etc., which results in a large number of parameters that need be estimated. These models cannot be easily extended when there are speech overlaps and one has to rely on features extracted from the data. To define the speaking transition priors P (s t,n |s t-1,n , v t,n , v t-1,n ), we consider three cases: (i) person n visible at t -1 and visible at t, or v t,n = v t-1,n = 1 and in this case the transitions are parametrized by a self-transition prior q ∈ [0, 1] which models the probability to remain in the same state, either speaking or not speaking, (ii) person n not visible at t -1 and visible at t, or v t,n = 1, v t-1,n = 0, in this case, the prior to be either speaking or not speaking at t is uniform, and (iii) person n not visible at t, or v t,n = 0, v t-1,n = 1, in which case the prior not to be speaking is equal to 1. The following equation summarizes all these cases:</p><formula xml:id="formula_12">P (s t,n |s t-1,n , v t,n , v t-1,n ) = v t,n v t-1,n q δs t-1,n (st,n) (1 -q) 1-δs t-1,n (st,n) + 1 2 (1 -v t-1,n )v t,n + (1 -v t,n )δ 0 (s t,n ),<label>(7)</label></formula><p>where δ i (j) = 1 if i = j and δ i (j) = 0 if i = j. Note that this does not consider the case of person n not visible at t -1 and at t for which the prior probability to be speaking is 0. In all our experiments we used q = 0.8.</p><p>The multiple-speaker tracking and diarization model proposed in this work only considers persons that are both seen and heard. Indeed, in informal scenarios there may be acoustic sources (speech or other sounds such as music) that are neither in the camera field of view, nor can they be visually detected and tracked. The proposed audio-visual association model addresses this problem, i.e. Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VISUAL OBSERVATIONS</head><p>We propose to use visual tracking of multiple persons in order to infer realizations of the random variables X 1:t introduced above. The advantage of a multiple-person tracker is that it is able to detect a variable number of persons, possibly appearing and disappearing from the visual field of view, to estimate their velocities, and to track their locations and identities. Multiple object/person tracking is an extremely well studied topic in the computer vision literature and many methods with their associated software packages are available. Among all these methods, we chose the multiple-person tracker of <ref type="bibr" target="#b36">[37]</ref>. In the context of our work, this method has several advantages: (i) it robustly handles fragmented tracks (due to occlusions, to the limited camera field of view, or simply to unreliable detections), (ii) it handles changes in person appearance, such as a person that faces the camera and then suddenly turns his/her head away from the camera, e.g. towards a speaker, and (iii) it performs online discriminative learning such that it can distinguish between similar appearances of different persons.</p><p>Visual tracking is implemented in the following way. Un upper-body detector <ref type="bibr" target="#b37">[38]</ref> is used to extract bounding boxes of persons in every frame. This allows the tracker to initialize new tracks, to re-initialize lost ones, to avoid tracking drift, and to cope with a large variety of poses and resolutions. Moreover, an appearance model, based on the color histogram of a bounding box associated with a person upper body (head and torso), is associated with each detected person. The appearance model is updated whenever the upper-body detector returns a reliable bounding box (no overlap with another bounding box). We observed that upper-body detection is more robust than face detection which yields many false positives. Nevertheless, in the context of audio-visual fusion, the face locations are important. Therefore, the locations estimated by the tracker, X 1:t , correspond to the face centers of the tracked persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. AUDIO OBSERVATIONS</head><p>In this section we present a methodology for extracting binaural features in the presence of either a single audio source or several speech sources. We consider audio signals recorded with a binaural microphone pair. As already explained in Section III, the short-time Fourier transform (STFT) is applied to the two microphone signals acquired at time-slice t and two spectrograms are thus obtained, namely L t , R t ∈ C F ×K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Audio Source</head><p>Let's assume that there is a single (speech or non-speech) signal emitted by an audio source during the time slice t. In the STFT domain, the relationships between the source-STFT spectrogram and microphone-STFT spectrograms are, for each frame k and each frequency f (for convenience we omit the time index t):</p><formula xml:id="formula_13">L f k = H f L,k T f k + N f L,k<label>(8)</label></formula><formula xml:id="formula_14">R f k = H f R,k T f k + N f R,k ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_15">T = {T f k } k=K,f =F k=1,f =1 is the unknown source spectro- gram, N L = {N f L,k } k=K,f =F k=1,f =1 and N R = {N f R,k } k=K,f =F k=1,f =1</formula><p>are the unknown noise spectrograms associated with the left and right channels, and</p><formula xml:id="formula_16">H L = {H f L,k } k=K,f =F k=1,f =1 and H R = {H f R,k } k=K,f =F k=1,f =1</formula><p>are the unknown left and right acoustic transfer functions that are frequency-dependent. The above equations correspond to the general case of a moving sound source. However, if we assume that the audio source is static during the time slice t, i.e. the source emitter is in a fixed position during the time slice t, the acoustic transfer functions are time-invariant and only depend on the source position relative to the microphones. We further define binaural features, i.e. the ratio between the left and right acoustic transfer functions,</p><formula xml:id="formula_17">H f L /H f R .</formula><p>Notice that we omitted the frame index because in the case of a static source, the acoustic transfer function is invariant over frames. Likewise the acoustic transfer function, the binaural features do not depend on k and they only contain audio-source position information <ref type="bibr" target="#b32">[33]</ref>.</p><p>One can use the estimated cross-PSD (power spectral density) and auto-PSD to extract binaural features in the following way. The cross-PSD between the two microphones is <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_18">Φ f L,R = 1 K K k=1 L f k R f k (10) ≈ 1 K H f L H f R K k=1 |T f k | 2 + 1 K K k=1 N f L,k N f * R,k , (<label>11</label></formula><formula xml:id="formula_19">)</formula><p>where A is the complex-conjugate of A and it is assumed that the signal-noise cross terms can be neglected. If the noise signals are spatially uncorrelated then the noise-noise cross terms can also be neglected. The binaural feature vector at t can be approximated with the ratio between the cross-PSD and auto-PSD functions, i.e. the vector</p><formula xml:id="formula_20">Y t = (Y 1 t , . . . Y f t , . . . Y F t ) with entries Y f t = Φ f t,L,R Φ f t,R,R<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple Speech Sources</head><p>We now consider the case of P speakers (P &gt; 1) that emit speech signals simultaneously (for convenience we omit again the time index t)</p><formula xml:id="formula_21">L f k = P p=1 H f L,p T f p,k + N f L,k<label>(13)</label></formula><formula xml:id="formula_22">R f k = P p=1 H f R,p T f p,k + N f R,k ,<label>(14)</label></formula><p>where H f L,p and H f R,p are the acoustic transfer functions from the speech-source p to the left and right microphones, respectively. The STFT based estimate of the cross-PSD for each frequency-frame point (f, k) is</p><formula xml:id="formula_23">Φ f L,R,k = L f k R f k .<label>(15)</label></formula><p>In order to further characterize simultaneously emitting speech signals, we exploit the well-known fact that speech signals have sparse spectrograms in the Fourier domain. Because of this sparsity it is realistic to assume that only one speech source p is active at each frequency-frame point of the two microphone spectrograms ( <ref type="formula" target="#formula_21">13</ref>) and <ref type="bibr" target="#b13">(14)</ref>. Therefore these spectrograms are composed of STFT coefficients that contain (i) either speech emitted by a single speaker, (ii) or noise. Using this assumption, the binaural spectrogram Y t and associated binary mask matrix A t can be estimated from the cross-PSD and auto-PSD in the following way. We start by estimating a binary mask for each frequency-frame point,</p><formula xml:id="formula_24">A f k = 0 if max(Φ f L,L,k , Φ f R,R,k ) &lt; a 1 otherwise, (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>where a is an adaptive threshold whose value is estimated based on noise statistics <ref type="bibr" target="#b40">[41]</ref>. Then, we compute the binaural spectrogram coefficients for each frequency-frame point (f, k) at time-slice t as:</p><formula xml:id="formula_26">Y f t,k =    Φ f t,L,R,k Φ f t,R,R,k if A f t,k = 1 0 if A f t,k = 0.<label>(17)</label></formula><p>It is important to stress that while these binaural coefficients are source-independent, they are location-dependent. This is to say that the binaural spectrogram only contains information about the location of the sound source and not about the content of the source. This crucial property allows one to use different types of sound sources for training a sound source localizer and for predicting the location of a speech source, as explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. AUDIO-VISUAL FUSION</head><p>In this section we propose an audio-visual spatial alignment model that will allow us to evaluate the observation likelihood (3). The proposed audio-visual alignment is weakly supervised and hence it requires training data. We start by briefly describing the audio-visual training data. The training data contain pairs of audio recordings and their associated directions. Let W = { W 1 , . . . , W m , . . . W M } ∈ C F ×M be a training dataset containing M binaural vectors. Each binaural vector is extracted from its corresponding audio recording using the method described in Section V-A, i.e.</p><formula xml:id="formula_27">W m = ( W 1 m , . . . , W f m , . . . , W F m )</formula><p>where each entry W f m is computed with <ref type="bibr" target="#b11">(12)</ref>.</p><p>Each audio sample in the training set consists of a whitenoise signal that is emitted by a loudspeaker placed at different locations, e.g. Fig. <ref type="figure" target="#fig_0">2</ref>. The PSD of a white-noise signal is significant at each frequency thus:</p><formula xml:id="formula_28">| f m | 2 &gt; a &gt; 0, ∀m ∈ [1 . . . M ], ∀f ∈ [1 . . . F ].</formula><p>A visual marker placed onto the loudspeaker allows to associate its pixel location with each sound direction, hence the M source directions correspond to an equal number of pixel locations X = { X 1 , . . . , X m , . . . X M } ∈ R 2×M . To summarize, the training data consist of M pairs of binaural features and associated pixel locations: { W m , X m } M m=1 .</p><p>We now consider the two sets of visual and auditory observations during the time slice t, namely X t = (X t,1 , . . . , X t,n , . . . ,</p><formula xml:id="formula_29">X t,N ) ∈ R 2×N , V t = (V t,1 , . . . V t,n , . . . V t,N ) ∈ {0, 1} N , Y t = (Y t,1 , . . . , Y t,k , . . . , Y t,K ) ∈ C F ×K and A t ∈ {0, 1} F ×K .</formula><p>If person n, located at X t,n , is both visible and speaks at t: the binaural features associated with the emitted speech signal depend on the person's location only, hence they must be similar to the binaural features of the training source emitting from the same location. This can be simply written as a nearest-neighbor search over the training-set of audio-source locations:</p><formula xml:id="formula_30">X n = argmin m X t,n -X m 2 (18)</formula><p>and let W n ∈ W be the binaural feature vector associated with this location. Hence, the training pair { X n , W n } ∈ X × W can be associated with person n.</p><p>We choose to model that at any frequency f ∈ [1 . . . F ], the likelihood of and observed binaural feature Y f t,k follows the following complex-Gaussian mixture model (for convenience, we omit the the time index t)</p><formula xml:id="formula_31">P (Y f k |Θ f ) = (19) N n=1 π f n N c (Y f k | W f n , σ f n ) + π f N +1 N c (Y f k |0, σ f N +1 ),</formula><p>where N c (x|µ, σ) = (πσ) -1 exp(-|x -µ| 2 /σ), x ∈ C is the complex-normal distribution and Θ f is the set of realvalued model parameters, namely the priors {π f n } N +1 n=1 with N +1 n=1 π f n = 1, and the variances {σ f n } N +1 n=1 . This model states that the binaural feature Y f k is either generated by one of the N persons, located at X n , 1 ≤ n ≤ N , hence it is an inlier generated by a complex-normal mixture model with means W f n , 1 ≤ n ≤ N , or is emitted by an unknown sound source, hence it is an outlier generated by a zero-centered complexnormal distribution with a very large variance</p><formula xml:id="formula_32">σ f N +1 σ n .</formula><p>The parameter set Θ f of ( <ref type="formula">19</ref>) can be easily estimated via a simplified variant of the EM algorithm for Gaussian mixtures: the algorithm alternates between E-step that evaluates the posterior probabilities</p><formula xml:id="formula_33">r f kn = P (z f k = n|Y f k ), z f k is assignment varaible, z f k = n means Y f k is generated by component n: r f kn = 1 C π f n N c (Y f k | W f n , σ f n ) if 1 ≤ n ≤ N 1 C π f N +1 N c (Y f k |0, σ f N +1 ) if n = N + 1,<label>(20)</label></formula><p>where</p><formula xml:id="formula_34">C = N i=1 π f i N c (Y f k | W f i , σ f i ) + π f N +1 N c (Y f k |0, σ f N +1 ),</formula><p>and M-step that estimates the variances and the priors:</p><formula xml:id="formula_35">σ f n = K k=1 A f k r f kn |Y f k -W f n | 2 K k=1 A f k r f kn ∀n, 1 ≤ n ≤ N (<label>21</label></formula><formula xml:id="formula_36">)</formula><formula xml:id="formula_37">π f n = K k=1 A f k r f kn K k=1 A f k ∀n, 1 ≤ n ≤ N + 1. (<label>22</label></formula><formula xml:id="formula_38">)</formula><p>The algorithm can be easily initialized by setting all the priors equal to 1 N +1 and by setting all the variances equal to a positive scalar σ. Because the component means are fixed, the algorithm converges in only a few iterations.</p><p>Based on these results one can evaluate (3), namely the speaking probability of person n located at X n : the probability that a visible person either speaks:</p><formula xml:id="formula_39">P (U t |S t,n = 1, V t,n = 1) = F f =1 K k=1 A f t,k r f t,kn F f =1 K k=1 A f t,k ,<label>(23)</label></formula><p>or is silent:</p><formula xml:id="formula_40">P (U t |S t,n = 0, V t,n = 1) = 1 -P (U t |S t,n = 1, V t,n ). (24)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. AUDIO-VISUAL DATASETS</head><p>In this section we describe the audio-visual datasets that are used to test the proposed method and to compare it with several state-of-the-art methods. We start by describing a novel dataset that was purposively gathered and recorded to encompass a wide number of multiple-speaker scenarios, e.g. speakers facing the camera, moving speakers, speakers looking at each other, etc. This novel dataset is referred to as AVDIAR. <ref type="foot" target="#foot_0">2</ref>In order to record both training and test data we used the following camera-microphone setup. A color camera is rigidly attached to an acoustic dummy head. The camera is a PointGrey Grasshopper3 unit equipped with a Sony Pregius IMX174 CMOS sensor of size 1.2 × 1 . The camera is equipped with a Kowa 6 mm wide-angle lens and it delivers 1920×1200 color pixels at 25 FPS. This camera-lens setup has a horizontal × vertical field of view of 97 • × 80 • .</p><p>For the audio recordings we used a binaural Senheiser MKE 2002 dummy head with two microphones plugged into its left and right ears, respectively. The orginal microphone signals are captured at 44100 Hz, we have downsampled them to 16000 Hz. The STFT, implemented with a 32 ms Hann window and 16 ms shifts between consecutive windows, is then applied separately to the left and right microphone signals. Therefore, there are 512 samples per frame and the audio frame rate is approximatively 64 FPS. Each audio frame consists of a vector composed F = 256 Fourier coefficients covering frequencies in the range 0 Hz -8 kHz.</p><p>The camera and the microphones are connected to a single PC and they are finely synchronized using time stamps delivered by the computer's internal clock. This audio-visual synchronization allows us to align the visual frames with the audio frames. The time index t corresponds to the visual-frame index. For each t we consider a spectrogram of length K = 25 frames, or a time slice of 0.4 s, hence there is an overlap between the spectrograms corresponding to consecutive time indexes.</p><p>The training data were recorded by manually moving a loudspeaker in front of the camera-microphone unit e.g. Fig. <ref type="figure" target="#fig_0">2</ref>. A visual marker placed at the center of the loudspeaker enables recording of audio signals with their associated pixel positions in the image plane. The loudspeaker is roughly moved in two planes roughly parallel to the image plane, at 1.5 m and 2.5 m, respectively. For each plane we record 800 positions lying on a uniform 20×40 grid that covers the entire field of view of the camera, hence there are M = 1600 training samples. The training data consists of 1 s of white-noise (WN) signals. Using the STFT we therefore obtain two WN spectrograms of size 256×64, corresponding to the left and right microphones, respectively. These two spectrograms are then used to compute binaural feature vectors, i.e. Section V-A (one feature vector for each loud-speaker position) and hence to build a training dataset of audio recordings and their associated image locations</p><formula xml:id="formula_41">{ W, X} = {( W 1 , X 1 ), . . . , ( W m , X m ), . . . ( W M , X M )}, i.e. Section VI.</formula><p>Similarly we gathered a test dataset that contains several scenarios. Each scenario involves participants that are either static and speak or move and speak, in front of the cameramicrophone unit at distance varying between 1.0 m and 3.5 m. In an attempt to record natural human-human interactions, participants were allowed to wonder around the scene and to interrupt each other while speaking. We recorded the following scenario categories, e.g. Fig. <ref type="figure">3:</ref> • Static participants facing the camera. This scenario can be used to benchmark diarization methods requiring the detection of frontal faces and of facial and lip movements. • Static participants facing each other. This scenario can be used to benchmark diarization methods that require static participants not necessarily facing the camera. • Moving participants. This is a general-purpose scenario that can be used to benchmark diarization as well as audio-visual person tracking.</p><p>In addition to the AVDIAR dataset, we used three other datasets, e.g. Fig. <ref type="figure" target="#fig_1">4</ref>. They are briefly described as follows:</p><p>• The MVAD dataset described in <ref type="bibr" target="#b4">[5]</ref>. The visual data were recorded with a Microsoft Kinect sensor at 20 FPS, <ref type="foot" target="#foot_1">3</ref> and the audio signals were recorded with a linear array of omnidirectional microphones sampled at 44100 Hz. The recorded sequences are from 40 s to 60 s long and contain </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recordings Description</head><p>Seq01-1P-S0M1, Seq04-1P-S0M1 , Seq22-1P-S0M1 A single person moving randomly and alternating between speech and silence.</p><p>Seq37-2P-S0M0, Seq43-2P-S0M0 Two static participants taking speech turns.</p><p>Seq38-2P-S1M0, Seq40-2P-S1M0, Seq44-2P-S2M0 Two static participants speaking almost simultaneously, i.e. there are large speech overlaps.</p><p>Seq20-2P-S1M1, Seq21-2P-S2M1 Two participants, wandering in the room and engaged in a conversation, sometime speaking simultaneously.</p><p>Seq12-3P-S2M1, Seq27-3P-S2M1 Three participants engaged in an informal conversation. They are moving around and sometimes they speak simultaneously.</p><p>Seq13-4P-S1M1, Seq32-4P-S1M1 Three to four participants engaged in a conversation. Sometimes they speak simultaneously and there are many short speech turns.</p><p>Fig. <ref type="figure">3</ref>: Examples of scenarios in the AVDIAR dataset. For the sake of varying the acoustic conditions, we used three different rooms to record this dataset.</p><p>one to three participants that speak in Portuguese. The speech and silence segments are 4 s to 8 s long. Since the diarization method proposed in <ref type="bibr" target="#b4">[5]</ref> requires frontal faces, the participants are facing the camera and remain static through all the recordings. • The AVASM dataset contains both training and test recordings used to test the single and multiple speaker localization method described in <ref type="bibr" target="#b32">[33]</ref>. The recording setup is similar to the one described above, namely a binaural acoustic dummy head with two microphones plugged into its ears and a camera placed underneath the head. The images and the audio signals were captured at 25 FPS and 44100 Hz, respectively. The recorded sequences contain up to two participants that face the camera and speak simultaneously. In addition, the dataset has audio-visual alignment data collected in a similar fashion as the AVDIAR dataset. • The AV16P3 dataset is designed to benchmark audiovisual tracking of several moving speakers without taking diarization into account <ref type="bibr" target="#b41">[42]</ref>. The sensor setup used for these recordings is composed of three cameras attached to the room ceiling, and two circular eight-microphone arrays. The recordings include mainly dynamic scenarios, comprising a single, as well as multiple moving speakers. In all the recordings there is a large overlap between the speaker-turns.</p><p>These datasets contain a large variety of recorded sce-narios, aimed at a wide range of application. e.g. formal and informal interaction in meetings and gatherings, humancomputer interaction, etc. Some of the datasets were not purposively recorded to benchmark diarization. Nevertheless they are challenging because they contain a large amount of overlap between speakers, hence they are well suited to test the limits and failures of diarization methods. Unlike recordings of formal meetings, which are composed on long single-speech segments with almost no overlap between the participants, the above datasets contain the following challenging situations e.g. Table <ref type="table" target="#tab_1">I</ref>:</p><p>• The participants do not always face the cameras, moreover, they turn their heads while they speak or listen; • The participants, rather then being static, move around and hence the tasks of tracking and diarization must be finely intertwined; • In informal meetings participants interrupt each other and hence not only that there is no silence between speech segments, but the speech segments overlap each other, and • Participants take speech turns quite rapidly which results in short-length speech segments, which makes audiovisual temporal alignment quite challenging. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Diarization Performance Measure</head><p>To effectively benchmark our model with state-of-the art methods, we use the diarization error rate (DER) to quantitatively measure the performance: smaller the DER value, better the performance. DER is defined by the NIST-RT evaluation testbed, <ref type="foot" target="#foot_2">4</ref> and corresponds to the percentage of audio frames that are not correctly assigned to one or more speakers, or to none of them in case of a silent frame. DER consists of the composition of the following measurements:</p><p>• False-alarm error, when speech has been incorrectly detected; • Miss error, when a person is speaking but the method fails to detect the speech activity, and • Speaker-labeling error, when a person-to-speech association does not correspond to the ground truth.</p><p>To compute DER, the MD-EVAL software package of NIST-RT is used, setting the forgiveness collar to a video frame of e.g. 40 ms for 25 FPS videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Diarization Algorithms and Setup</head><p>We compared our method with four methods: <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and <ref type="bibr" target="#b31">[32]</ref>. These methods are briefly explained below:</p><p>• Vijayasenan et al. <ref type="bibr" target="#b42">[43]</ref> (DiarTK) use audio information only. DiarTK allows the user to incorporate a large number of audio features. In our experiments and comparisons we used the following features: mel-frequency cepstral coefficients (MFCC), frequency-domain linear prediction (FDLP), time difference of arrival (TDOA), and modulation spectrum (MS). Notice that TDOA features can only be used with static sound-sources, hence we did not use TDOA in the case of moving speakers.</p><p>• Minotto et al. <ref type="bibr" target="#b4">[5]</ref> learn an SVM classifier based on based on labeled audio-visual features. Sound-source with diagonal covariance matrices. If it is assumed that a minimum of 50 samples are needed to properly estimate the GMM parameters, speech segments of at least 50×19×20 ms, or 19 s, are needed. Therefore it is not surprising that DiarTK performs poorly on all these datasets.</p><p>Table <ref type="table" target="#tab_2">II</ref> shows that the method of <ref type="bibr" target="#b4">[5]</ref> performs much better than DiarTK. This is not surprising, since the speech turns taken by the participants in the MVAD dataset are very brief. Minotto et al. <ref type="bibr" target="#b4">[5]</ref> use a combination of visual features extracted form frontal views of faces (lip movements) and audio features (speech-source directions) to train an SVM classifier. The method fails whenever the participants do not face the camera, e.g. sequences Two12, Two13 and Two14, where participants purposely occlude their faces several times throughout the recordings. The method proposed in this paper in combination with TREM achieves the best results on almost all the tested scenarios. This is due to the fact that the audio-visual fusion method is capable of associating very short speech segments with one or several participants. However, the performance of our method, with either TREM or GCC-PHAT, drops down as the number of people increases. This is mainly due to the limited resolution of multiple sound-source localization algorithms (of the order of 10 • horizontally) and thus, it makes it difficult to disambiguate two nearby speaking/silent persons. Notice that tracking the identity of the participants is performed by visual tracking, which is a trivial task for most of these recordings, since participants are mostly static.</p><p>Table <ref type="table" target="#tab_3">III</ref> shows the results obtained with the AVASM dataset. In these recordings the participants speak simultaneously, with the exception of the Moving-Speaker-01 recording. We do not report results obtained with DiarTK since this method yields non-meaningful performance with this dataset. The proposed method performs reasonable well in the presence of simultaneously speaking persons.</p><p>Table <ref type="table" target="#tab_4">IV</ref> shows results obtained with the AV16P3 dataset. As with the AVASM dataset we were unable to obtain meaningful results with the DiarTK method. As expected the proposed method has the same performance as <ref type="bibr" target="#b31">[32]</ref> in the presence of a single active speaker, e.g. seq11-1p-0100 and seq15-1p-0111. Nevertheless, the performance of <ref type="bibr" target="#b31">[32]</ref> rapidly degrades in the presence of two and three persons speaking almost simultaneously. Notice that this dataset was recorded to benchmark audio-visual tracking, not diarization.</p><p>Table <ref type="table" target="#tab_5">V</ref> shows the results obtained with the AVDIAR dataset. The content of each scenario is briefly described in Table <ref type="table" target="#tab_1">I</ref>. The proposed method outperforms all other methods. It is also interesting to notice that our full method performs better than with either TREM or GCC-PHAT. This is due to the robust semi-supervised audio-visual association method proposed above. Fig. <ref type="figure" target="#fig_2">5</ref>, Fig. <ref type="figure">6</ref>, and Fig. <ref type="figure">7</ref> illustrate the audiovisual diarization results obtained by our method with three scenarios. 5   5 Videos illustrating the performance of the proposed method using these scenarios are available at <ref type="url" target="https://team.inria.fr/perception/avdiarization/">https://team.inria.fr/perception/avdiarization/</ref>.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSIONS</head><p>We proposed an audio-visual diarization method well suited for challenging scenarios consisting of participants that either interrupt each other, or speak simultaneously. In both cases, the speech-to-person association problem is a difficult one. We proposed to combine multiple-person visual tracking with multiple speech-source localization in a principled spatiotemporal Bayesian fusion model. Indeed, the diarization process was cast into a latent-variable dynamic graphical model. We described in detail the derivation of the proposed model and we showed that, in the presence of a limited number of speakers (of the order of ten), the diarization formulation is efficiently solved via an exact inference procedure. Then we described a novel multiple speech-source localization method and a weakly supervised audio-visual clustering method.</p><p>We also introduced a novel dataset, AVDIAR, that was carefully annotated and that enables to assess the performance of audio-visual (or audio-only) diarization methods using scenarios that were not available with existing datasets, e.g. the participants were allowed to freely move in a room and to turn their heads towards the other participants, rather than always facing the camera. We also benchmarked our method with several other recent methods using publicly available datasets. Unfortunately, we were not able to compare our method with the methods of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> for two reasons: first, these methods require long speech segments (of the order of 10 s), and second the associated software packages are not publicly available, which would have facilitated the comparison task.</p><p>In the future we plan to incorporate richer visual features, such as head pose estimation and head-pose tracking, in order to facilitate the detection of speech turns on the basis of gaze or of people that look at each other over time. We also plan to incorporate richer audio features, such as the possibility to extract speech signals emitted by each participant (sound-source separation) followed by speech recognition, and hence to enable not only diarization but also speech-content understanding. Another extension is to consider distributed sensors, wearable devices, or a combination of both, in order to be able to deal with more complex scenarios involving tens of participants <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The AVDIAR dataset is recorded with a camera-microphone setup. (a) To record the training data, a loud-speaker that emits white noise was used. A visual marker onto the loud-speaker (circled in green) allows to annotate the training data with image locations, each image location corresponds to a loud-speaker direction. (b) The image grid of loud-speaker locations used for the training data. (c) A typical AVDIAR scenario (the camera-microphone setup is circled in green).</figDesc><graphic coords="9,229.16,57.65,149.91,111.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Example from different datasets. The MVAD dataset (top) contains recordings of one to three persons that always face the camera. The AVASM (middle) was design to benchmark audio-visual sound-source localization with two simultaneously speaking persons or with a moving speaker. The AV16P3 dataset (bottom) contains recordings of simultaneously moving and speaking persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Results obtained on sequence Seq32-4P-S1M1. Visual tracking results (first row). The raw audio signal delivered by the left microphone and the speech activity region is marked with red rectangles (second row). Speaker diarization result (third row) illustrated with a color diagram: each color corresponds to the speaking activity of a different person. Annotated ground-truth diarization (fourth row).</figDesc><graphic coords="13,117.70,135.37,372.83,109.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Results on sequence Seq12-3P-S2M1.</figDesc><graphic coords="13,117.70,385.92,372.85,118.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig.1:The Bayesian spatiotemporal fusion model used for audio-visual speaker diarization. Shaded nodes represent the observed variables, while unshaded nodes represent latent variables. Note that the visibility-mask variables Vt,n although observed, they are treated as control variables. This model enables simultaneously speaking persons, which is not only a realistic assumption but also very common in natural dialogues and applications like for example HRI.</figDesc><table><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell>Vt+1,1</cell></row><row><cell></cell><cell></cell><cell>Ut+1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Scenarios available with the AVDIAR dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>DER scores obtained with MVAD dataset (%).</figDesc><table><row><cell>Sequence</cell><cell>DiarTK [43]</cell><cell>[5]</cell><cell>[21]</cell><cell>[32]</cell><cell>Proposed with</cell><cell>Proposed with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TREM [27]</cell><cell>GCC-PHAT [26]</cell></row><row><cell>One7</cell><cell>21.16</cell><cell>8.66</cell><cell>89.90</cell><cell>5.82</cell><cell>0.91</cell><cell>1.06</cell></row><row><cell>One8</cell><cell>20.07</cell><cell>7.11</cell><cell>98.10</cell><cell>4.92</cell><cell>1.02</cell><cell>1.81</cell></row><row><cell>One9</cell><cell>22.79</cell><cell>9.02</cell><cell>94.60</cell><cell>13.66</cell><cell>0.98</cell><cell>1.58</cell></row><row><cell>Two1</cell><cell>23.50</cell><cell>6.81</cell><cell cols="2">94.90 16.79</cell><cell>2.87</cell><cell>26.00</cell></row><row><cell>Two2</cell><cell>30.22</cell><cell>7.32</cell><cell cols="2">90.60 23.49</cell><cell>3.13</cell><cell>13.70</cell></row><row><cell>Two3</cell><cell>25.95</cell><cell>7.92</cell><cell cols="2">94.50 25.75</cell><cell>8.30</cell><cell>20.88</cell></row><row><cell>Two4</cell><cell>25.24</cell><cell>6.91</cell><cell cols="2">84.10 20.23</cell><cell>0.16</cell><cell>11.20</cell></row><row><cell>Two5</cell><cell>25.96</cell><cell>8.30</cell><cell cols="2">90.80 25.02</cell><cell>4.50</cell><cell>29.67</cell></row><row><cell>Two6</cell><cell>29.13</cell><cell>6.89</cell><cell cols="2">96.70 16.89</cell><cell>6.11</cell><cell>23.57</cell></row><row><cell>Two9</cell><cell>30.71</cell><cell cols="3">11.95 96.90 15.59</cell><cell>2.42</cell><cell>34.28</cell></row><row><cell>Two10</cell><cell>25.32</cell><cell>8.30</cell><cell cols="2">95.50 21.04</cell><cell>3.27</cell><cell>15.15</cell></row><row><cell>Two11</cell><cell>27.75</cell><cell>6.12</cell><cell cols="2">84.60 21.22</cell><cell>6.89</cell><cell>18.05</cell></row><row><cell>Two12</cell><cell>45.06</cell><cell cols="3">24.60 80.40 39.79</cell><cell>12.00</cell><cell>34.60</cell></row><row><cell>Two13</cell><cell>49.23</cell><cell cols="3">27.38 64.10 25.11</cell><cell>14.49</cell><cell>48.70</cell></row><row><cell>Two14</cell><cell>27.16</cell><cell cols="3">28.81 81.10 25.75</cell><cell>6.43</cell><cell>59.10</cell></row><row><cell>Three1</cell><cell>27.71</cell><cell>9.10</cell><cell cols="2">95.80 47.56</cell><cell>6.17</cell><cell>52.63</cell></row><row><cell>Three2</cell><cell>27.71</cell><cell>9.10</cell><cell cols="2">89.20 49.15</cell><cell>13.46</cell><cell>49.66</cell></row><row><cell>Three3</cell><cell>29.41</cell><cell>5.93</cell><cell cols="2">91.50 47.78</cell><cell>13.57</cell><cell>49.09</cell></row><row><cell>Three6</cell><cell>36.36</cell><cell>8.92</cell><cell cols="2">79.70 40.92</cell><cell>12.89</cell><cell>37.78</cell></row><row><cell>Three7</cell><cell>36.24</cell><cell cols="3">14.51 86.20 47.35</cell><cell>11.74</cell><cell>40.40</cell></row><row><cell>Average</cell><cell>29.33</cell><cell>11.18</cell><cell>89.96</cell><cell>26.69</cell><cell>6.57</cell><cell>28.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>DER scores obtained with AVASM dataset (%).</figDesc><table><row><cell>Sequence</cell><cell>[21]</cell><cell>[32]</cell><cell>Proposed with</cell><cell>Proposed with</cell><cell>Proposed</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TREM [27]</cell><cell>GCC-PHAT [26]</cell><cell></cell></row><row><cell cols="2">Moving-Speaker-01 95.04</cell><cell>6.26</cell><cell>21.84</cell><cell>17.24</cell><cell>6.26</cell></row><row><cell>Two-Speaker-01</cell><cell cols="2">70.20 24.11</cell><cell>34.41</cell><cell>44.42</cell><cell>2.96</cell></row><row><cell>Two-Speaker-02</cell><cell cols="2">80.30 26.98</cell><cell>32.52</cell><cell>47.30</cell><cell>7.33</cell></row><row><cell>Two-Speaker-03</cell><cell cols="2">74.20 35.26</cell><cell>46.77</cell><cell>47.77</cell><cell>13.78</cell></row><row><cell>Average</cell><cell cols="2">79.94 23.15</cell><cell>33.89</cell><cell>39.18</cell><cell>7.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>DER scores obtained with AV16P3 dataset (%).</figDesc><table><row><cell>Sequence</cell><cell>[32]</cell><cell>Proposed with</cell><cell>Proposed with</cell></row><row><cell></cell><cell></cell><cell>TREM [27]</cell><cell>GCC-PHAT [26]</cell></row><row><cell>seq11-1p-0100</cell><cell>3.50</cell><cell>3.25</cell><cell>12.18</cell></row><row><cell>seq15-1p-0111</cell><cell>3.29</cell><cell>3.29</cell><cell>25.28</cell></row><row><cell cols="2">seq18-2p-0101 23.54</cell><cell>7.69</cell><cell>9.13</cell></row><row><cell cols="2">seq24-2p-0111 43.21</cell><cell>17.39</cell><cell>46.50</cell></row><row><cell cols="2">seq40-3p-1111 26.98</cell><cell>8.51</cell><cell>21.03</cell></row><row><cell>Average</cell><cell>20.04</cell><cell>8.02</cell><cell>22.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>DER scores obtained with AVDIAR dataset (%).</figDesc><table><row><cell>Sequence</cell><cell>DiarTK [43]</cell><cell>[21]</cell><cell>[32]</cell><cell>Proposed with</cell><cell>Proposed with</cell><cell>Proposed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TREM [27]</cell><cell>GCC-PHAT [26]</cell><cell></cell></row><row><cell>Seq01-1P-S0M1</cell><cell>43.19</cell><cell>-</cell><cell>14.36</cell><cell>61.15</cell><cell>72.06</cell><cell>3.32</cell></row><row><cell>Seq04-1P-S0M1</cell><cell>32.62</cell><cell>-</cell><cell>14.21</cell><cell>71.34</cell><cell>68.84</cell><cell>9.44</cell></row><row><cell>Seq22-1P-S0M1</cell><cell>23.53</cell><cell>-</cell><cell>2.76</cell><cell>56.75</cell><cell>67.36</cell><cell>4.93</cell></row><row><cell>Seq37-2P-S0M0</cell><cell>12.95</cell><cell>34.70</cell><cell>1.67</cell><cell>41.02</cell><cell>45.90</cell><cell>2.15</cell></row><row><cell>Seq43-2P-S0M0</cell><cell>76.10</cell><cell cols="2">79.90 23.25</cell><cell>46.81</cell><cell>56.90</cell><cell>6.74</cell></row><row><cell>Seq38-2P-S1M0</cell><cell>47.31</cell><cell cols="2">59.20 43.01</cell><cell>47.89</cell><cell>47.38</cell><cell>16.07</cell></row><row><cell>Seq40-2P-S1M0</cell><cell>48.74</cell><cell cols="2">51.80 31.14</cell><cell>42.20</cell><cell>44.62</cell><cell>14.12</cell></row><row><cell>Seq20-2P-S1M1</cell><cell>43.58</cell><cell>-</cell><cell>51.78</cell><cell>58.82</cell><cell>59.38</cell><cell>35.46</cell></row><row><cell>Seq21-2P-S2M1</cell><cell>32.22</cell><cell>-</cell><cell>27.58</cell><cell>63.03</cell><cell>60.52</cell><cell>20.93</cell></row><row><cell>Seq44-2P-S2M0</cell><cell>54.47</cell><cell>-</cell><cell>44.98</cell><cell>55.69</cell><cell>51.0</cell><cell>5.46</cell></row><row><cell>Seq12-3P-S2M1</cell><cell>63.67</cell><cell>-</cell><cell>26.55</cell><cell>28.30</cell><cell>61.20</cell><cell>17.32</cell></row><row><cell>Seq27-3P-S2M1</cell><cell>46.05</cell><cell>-</cell><cell>20.84</cell><cell>47.40</cell><cell>68.79</cell><cell>18.72</cell></row><row><cell>Seq13-4P-S1M1</cell><cell>47.56</cell><cell>-</cell><cell>43.57</cell><cell>28.49</cell><cell>48.23</cell><cell>29.62</cell></row><row><cell>Seq32-4P-S1M1</cell><cell>41.51</cell><cell>-</cell><cell>43.26</cell><cell>33.36</cell><cell>71.98</cell><cell>30.20</cell></row><row><cell>Average</cell><cell>43.82</cell><cell cols="2">56.40 27.78</cell><cell>48.72</cell><cell>58.87</cell><cell>15.32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://team.inria.fr/perception/avdiar/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Note that our method doesn't use the depth image available with this sensor</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://www.nist.gov/speech/tests/rt/2006-spring/ localization provides horizontal sound directions which</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Funding from the <rs type="funder">European Union</rs> <rs type="programName">FP7 ERC</rs> Advanced Grant VHIA (#<rs type="grantNumber">340113</rs>) and from <rs type="funder">XEROX University Affairs Committee (UAC)</rs> grant (<rs type="grantNumber">2015-2017</rs>) is greatly acknowledged.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gN8J2K4">
					<idno type="grant-number">340113</idno>
					<orgName type="program" subtype="full">FP7 ERC</orgName>
				</org>
				<org type="funding" xml:id="_du75auA">
					<idno type="grant-number">2015-2017</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>are combined with the output of a mouth tracker.</p><p>• Barzelay et al. <ref type="bibr" target="#b20">[21]</ref> calculate audio-visual correlations based on extracting onsets from both modalities and on aligning these onsets. The method consists of detecting faces and on tracking face landmarks, such that each landmark yields a trajectory. Onset signals are then extracting from each one of these trajectory as well as from the microphone signal. These onsets are used to compare each visual trajectory with the microphone signal, and the trajectories that best match the microphone signal correspond to the active speaker. We implemented this method based on <ref type="bibr" target="#b20">[21]</ref> since there is no publicly available code. Extensive experiments with this method revealed that frontal views of speakers are needed. Therefore, we tested this methods with all the sequences from the MVAD and AVASM datasets and on the sequences from the AVDIAR dataset featuring frontal images of faces.</p><p>• Gebru et al. <ref type="bibr" target="#b31">[32]</ref> track the active speaker, provided that participants take speech turns with no signal overlap. Therefore, whenever two persons speak simultaneously, this method extracts the dominant speaker.</p><p>Additionally, we used the following multiple sound-source localization methods:</p><p>• GCC-PHAT which detects the local maxima of the generalized cross-correlation method: we used the implementation from the BSS Locate Toolbox <ref type="bibr" target="#b25">[26]</ref>.</p><p>• TREM which considers a regular grid of source locations and selects the most probable locations based on maximum likelihood: we used the Matlab code provided by the authors, <ref type="bibr" target="#b26">[27]</ref>.</p><p>GCC-PHAT and TREM were used in conjunction with the proposed diarization method using the AVDIAR dataset as well as the MVAD and AV3P16 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussion</head><p>The results obtained with the MVAD, AVASM, AV16P3 and AVDIAR datasets are summarized in Table <ref type="table">II</ref>, Table <ref type="table">III</ref>, Table <ref type="table">IV</ref> and<ref type="table">Table</ref> </p><p>Overall, it can be noticed that the method of <ref type="bibr" target="#b20">[21]</ref> is the least performing method. As explained above this method is based on detecting signal onsets in the two modalities and on finding cross-modal correlations based on onset coincidence. Unfortunately, the visual onsets are unable to properly capture complex speech dynamics. The DiarTK method of <ref type="bibr" target="#b42">[43]</ref> is the second least performing method. This is mainly due to the fact that this method is designed to rely on long speech segments with almost no overlap between consecutive segments. Whenever several speech signals overlap, it is very difficult to extract reliable information with MFCC features, since the latter are designed to characterize clean speech. DiarTK is based on clustering MFCC features using a Gaussian mixture model. Consider, for example, MFCC feature vectors of dimension 19, extracted from 20 ms-long audio frames, and a GMM </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speaker diarization: A review of recent research</title>
		<author>
			<persName><forename type="first">X</forename><surname>Miro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Audio-visual synchronisation for speaker diarisation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dielmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2654" to="2657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal speaker diarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J A</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audiovisual diarization of people in video content</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">El</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sénac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia tools and applications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="747" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal on-line speaker diarization using sensor fusion through SVM</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Minotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1694" to="1705" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio-visual speaker diarization using fisher linear semi-discriminant analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="115" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal speaker clustering in full length movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kapsouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benaroya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ami meeting corpus: A pre-announcement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karaiskos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning for Multimodal Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pixels that sound</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kidron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-modal localization via sparsity</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1390" to="1404" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Audiovisual synchronization and fusion using canonical correlation analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sargin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1396" to="1403" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio-vision: Using audio-visual synchrony to locate sounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="813" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning joint statistical models for audio-visual fusion and segregation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="772" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audiovisual probabilistic tracking of multiple speakers in meetings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="601" to="616" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multimodal approach to blind source separation of moving sources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="895" to="910" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio assisted robust visual tracking with adaptive particle filtering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="200" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The icsi rt07s speaker diarization system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wooters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huijbregts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Technologies for Perception of Humans</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantitative association of vocal-tract and facial behavior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yehia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mixing audiovisual speech processing and blind source separation for the extraction of speech signals from convolutive mixtures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Onsets coincidence for cross-modal analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Barzelay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="108" to="120" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker localisation using audiovisual synchrony: An empirical study</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on image and video retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="488" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic dependency tests for audiovisual speaker association</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Siracusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP&apos;07</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">457</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On-line multi-modal speaker diarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on Multimodal interfaces</title>
		<meeting>the 9th international conference on Multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="350" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model-based expectationmaximization source separation and localization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="382" to="394" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-source tdoa estimation in reverberant audio using angular spectra and clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1950" to="1960" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tree-based recursive expectationmaximization algorithm for localization of acoustic sources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dorfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1692" to="1703" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alignment of Binocular-Binaural Data Using a Moving Audio-Visual Target</title>
		<author>
			<persName><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Multimedia Signal Processing</title>
		<meeting><address><addrLine>Pula, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conjugate mixture models for clustering multimodal data</title>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="557" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beamforming: A versatile approach to spatial filtering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Audio-visual speechturn detection and tracking</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<meeting><address><addrLine>Liberec, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tracking the active speaker based on a joint audio-visual observation model</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-localization of audio sources in images using binaural features and locally-linear regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="718" to="731" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">EM algorithms for weighted-data clustering with application to audio-visual scene analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decisions about turns in multiparty conversation: from perception to action</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Turn-taking, feedback and joint attention in situated human-robot interaction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Skantze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hjalmarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oertel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="50" to="66" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1218" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimation of relative transfer function in the presence of stationary noise based on segmental power spectral density matrix subtraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Local relative transfer function for sound source localization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-stationary noise power spectral density estimation based on regional statistics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">an audiovisual corpus for speaker localization and tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Multimodal Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="182" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Diartk: An open source toolkit for research in multistream speaker diarization and its application to meetings recordings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vijayasenan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Valente</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2170" to="2173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">No matter where you are: Flexible graph-guided multi-task learning for multi-view head pose classification under target motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Analyzing free-standing conversational groups: a multimodal approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">and a member of the PERCEPTION team at INRIA Grenoble Rhône-Alpes, Montbonnot Saint-Martin. His research interests include development of statistical methods and machine learning algorithms for multimodal signal analysis, speaker tracking and computer vision. He is particularly interested in joint processing of audio and visual data applied to human-computer and human-robot interactions</title>
	</analytic>
	<monogr>
		<title level="m">Ethiopia in 2008 and the M.Sc. degree in Telecommunication Engineering from University of</title>
		<meeting><address><addrLine>Trento, Italy; Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2011. 2008 to 2011</date>
		</imprint>
		<respStmt>
			<orgName>Israel Dejene Gebru received the B.Sc. degree in Computer Engineering from Addis Ababa University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a PhD candidate at Université Grenoble Alpes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
