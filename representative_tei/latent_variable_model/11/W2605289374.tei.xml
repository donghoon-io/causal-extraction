<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benoit</forename><surname>Massé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Silèye</forename><surname>Ba</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
						</author>
						<title level="a" type="main">Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2017.2782819</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual focus of attention</term>
					<term>eye gaze</term>
					<term>head pose</term>
					<term>dynamic Bayesian models</term>
					<term>switching state-space models</term>
					<term>multiparty interaction</term>
					<term>human-robot interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in estimating and tracking the VFOAs associated with multiparty social interactions. We note that in this type of situations the participants either look at each other or at an object of interest; therefore their eyes are not always visible. Consequently both gaze and VFOA estimation cannot be based on eye detection and tracking. We propose a method that exploits the correlation between eye gaze and head movements. Both VFOA and gaze are modeled as latent variables in a Bayesian switching state-space model (also referred switching Kalman filter). The proposed formulation leads to a tractable learning method and to an efficient online inference procedure that simultaneously tracks gaze and visual focus. The method is tested and benchmarked using two publicly available datasets, Vernissage and LAEO, that contain typical multi-party human-robot and human-human interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I n this paper we are interested in the computational analysis of social interactions. In addition to speech, people communicate via a large variety of non-verbal cues, e.g. prosody, hand gestures, body movements, head nodding, eye gaze, and facial expressions. For example, in a multi-party conversation, a common behavior consists in looking either at a person, e.g. the speaker, or at an object of current interest, e.g. a computer screen, a painting on a wall, or an object lying on a table. We are particularly interested in estimating the visual focus of attention (VFOA), or who is looking at whom or at what, which has been recognized as one of the most prominent social cues. It is used in multi-party dialog to establish faceto-face communication, to respect social etiquette, to attract someone's attention, or to signify speech-turn taking, thus complementing speech communication.</p><p>The VFOA characterizes a perceiver/target pair. It may be defined either by the line from the perceiver's face to the perceived target, or by the perceiver's direction of sight or gaze direction (which is often referred to as eye gaze or simply gaze). Indeed, one may state that the VFOA of person i is target j if the perceiver's gaze is aligned with the perceiverto-target line. From a physiological point of view, eye gaze depends on both eyeball orientation and head orientation. Both the eye and the head are rigid bodies with three and six degrees B. <ref type="bibr">Massé</ref>  of freedom respectively. The head position (three coordinates) and the head orientation (three angles) are jointly referred to as the head pose. With proper choices for the head-and eyecentered coordinate frames, one can assume that gaze is a combination of head pose and of eyeball orientation, <ref type="foot" target="#foot_0">1</ref> and the VFOA depends on head pose, eyeball orientation, and target location.</p><p>In this paper we are interested into estimating and tracking jointly the VFOAs of a group of people that communicate with each other and with a robot, or multi-party HRI (human-robot interaction), which may well be viewed as a generalization of single-user HRI. From a methodological point of view the former is more complex than the latter. Indeed, in single-user HRI the person and the robot face each other and hence a camera mounted onto the robot head provides high-resolution frontal images of the user's face such that head pose and eye orientation can both be easily and robustly estimated. In the case of multi-party HRI the eyes are barely detected since the participants often turn their faces away from the camera. Consequently, VFOA estimation methods based on eye detection and eye tracking are ineffective and one has to estimate the VFOA, indirectly, without explicit eye detection.</p><p>We propose a Bayesian switching dynamic model for the estimation and tracking gaze directions and VFOAs of several persons involved in social interaction. While it is assumed that head poses (location and orientation) and target locations can be directly detected from the data, the unknown gaze directions and VFOAs are treated as latent random variables. The proposed temporal graphical model, that incorporates gaze dynamics and VFOA transitions, yields (i) a tractable learning algorithm and (ii) an efficient gaze-and-VFOA tracking method. <ref type="foot" target="#foot_1">2</ref> The proposed method may well be viewed as a computational model of <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>The method is evaluated using two publicly available datasets, Vernissage <ref type="bibr" target="#b2">[3]</ref> and LAEO <ref type="bibr" target="#b3">[4]</ref>. These datasets consist of several hours of video containing situated dialog between two persons and a robot (Vernissage) and human-human interactions (LAEO). We are particularly interested in finding participants that either gaze to each other, gaze to the robot, or gaze to an object. Vernissage is recorded with a motion capture system (a network of infrared cameras) and with a camera placed onto the robot head. LAEO is collected from TV shows.</p><p>The remainder of this paper is organized as follows. Section II provides an overview of related work in gaze, VFOA and head-pose estimation. Section III introduces the paper's mathematical notations and definitions, states the problem formulation and describes the proposed model. Section IV presents in detail the model inference and Section V derives the learning algorithm. Section VI provides implementation details and Section VII describes the experiments and reports the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As already mentioned, the VFOA is correlated with gaze. Several methods proceed in two steps, in which the gaze direction is estimated first, and then used to estimate VFOA. In scenarios that rely on precise estimation of gaze <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> a head-mounted camera, like the one in <ref type="bibr" target="#b6">[7]</ref>, can be used to detect the iris with high accuracy. Head-mounted eye trackers provide extremely accurate gaze measurements and in some circumstances eye-tracking data can be used to estimate objects of interest in videos <ref type="bibr" target="#b7">[8]</ref>. Nevertheless, they are invasive instruments and hence not appropriate for analyzing social interactions.</p><p>Gaze estimation is relevant for a number of scenarios, such as car driving <ref type="bibr" target="#b8">[9]</ref> or interaction with smartphones <ref type="bibr" target="#b9">[10]</ref>. In these situations, either the field of view is limited, hence the range of gaze directions is constrained (car driving), or active human participation ensures that the device yields frontal views of the user's face, thus providing accurate eye measurements <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In some scenarios the user is even asked to limit head movements <ref type="bibr" target="#b12">[13]</ref>, or to proceed through a calibration phase <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Even if no specific constraints are imposed, single-user scenarios inherently facilitate the task of eye measurement <ref type="bibr" target="#b10">[11]</ref>. At the best of our knowledge, there is no gaze estimation method that can deal with unconstrained scenarios, e.g. participants not facing the cameras, partially or totally occluded eyes, etc. In general, eye analysis is inaccurate when participants are faraway from the camera.</p><p>An alternative is to approximate gaze direction with head pose <ref type="bibr" target="#b14">[15]</ref>. Unlike eye-based methods, head pose can be estimated from low-resolution images, i.e. distant cameras <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. These methods estimate gaze only approximatively since eyeball orientation can differ from head orientation by ±35° <ref type="bibr" target="#b20">[21]</ref>. Gaze estimation from head orientation can benefit from the observation that gaze shifts are often achieved by synchronously moving the head and the eyes <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The correlation between head pose and gaze has also been exploited in <ref type="bibr" target="#b22">[23]</ref>. More recently, <ref type="bibr" target="#b23">[24]</ref> combined head and eye features to estimate the gaze direction using an RGB-D camera. The method still requires that both eyes are visible.</p><p>Several methods were proposed to infer VFOAs either from gaze directions <ref type="bibr" target="#b24">[25]</ref>, or from head poses <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. For example, in <ref type="bibr" target="#b3">[4]</ref> it is proposed to build a gaze cone around the head orientation and targets lying inside this cone are used to estimate the VFOA. While this method was successfully applied to movies, its limitation resides in its vagueness: the VFOA information is limited to whether there are two people looking at each other or not.</p><p>An interesting application of VFOA estimation it the analysis of social behavior of participants engaged in meetings, e.g. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Meetings are characterized by interactions between seated people that interact based on speech and on head movements. Some methods estimate the most likely VFOA associated with a head orientation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The drawback of these approaches is that they must be purposively trained for each particular meeting layout. The correlation between VFOA and head pose was also investigated in <ref type="bibr" target="#b25">[26]</ref> where an HMM is proposed to infer VFOAs from head and body orientations. This work was extended to deal with more complex scenarios, such as participants interacting with a robot <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>. An input-output HMM is proposed in <ref type="bibr" target="#b30">[31]</ref> to enable to model the following contextual information: participants tend to look to the speaker, to the robot, or to an object which is referred to by the speaker or by the robot. The results of <ref type="bibr" target="#b30">[31]</ref> show that this improves the performance of VFOA estimation. Nevertheless, this method requires additional information, such as speaker identification or speech recognition.</p><p>The problem of joint estimation of gaze and of VFOA was addressed in a human-robot cooperation task <ref type="bibr" target="#b27">[28]</ref>. In such a scenario the user doesn't necessarily face the camera and robot-mounted cameras have low-resolution, hence the estimation of gaze from direct analysis of eye regions is not feasible. <ref type="bibr" target="#b27">[28]</ref> proposes to learn a regression between the space of head poses and the space of gaze directions and then to predict an unknown gaze from an observed head pose. The head pose itself is estimated by fitting a 3D elliptical cylinder to a detected face, while the associated gaze direction corresponds to the 3D line joining the head center to the target center. This implies that during the learning stage, the user is instructed to gaze at targets lying on a table in order to provide training data. The regression parameters thus estimated correspond to a discrete set of head-pose/gazedirection pairs (one for each target); an erroneous gaze may be predicted when the latter is not in the range of gaze directions used for training.</p><p>A summary of the proposed Bayesian dynamic model and experiments with the Vernissage [3] motion capture dataset were presented in <ref type="bibr" target="#b31">[32]</ref>. In this article we provide a detailed and comprehensive description and analysis of the proposed model, of the model inference, of the learning methodology, and of the associated algorithms. We show results with both motion capture and RGB data from Vernissage. Additionally, we show results with the LAEO dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head><p>The proposed mathematical model model is inspired from psychophysics <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In unconstrained scenarios a person switches his/her gaze from one target to another target, possibly using both head and eye movements. Quick eye movements towards a desired object of interest are called saccades. Eye movements can also be caused by the vestibuleocular reflex that compensates for head movements such that Figure <ref type="figure">1</ref>. This figure illustrates the principle of our method and displays the observed and latent variables associated with a person (left-person indexed by i). The two images were grabbed with a camera mounted onto the head of a robot and they correspond to frames t -n (left image) and t (right image), respectively. The following variables are displayed: head orientation (red arrow), H i t-n , H i t (observed variables), as well as the latent variables estimated with the proposed method, namely gaze direction (green arrow), G i t-n , G i t , VFOA, V i t-n , V i t , and head reference orientation (black arrow), R i t-n , R i t (that coincides with upper-body orientation). In this example left-person gazes towards the robot at t -n, then turns her head to eventually gaze towards right-person at t, hence her VFOA switches from</p><formula xml:id="formula_0">V i t-n = robot to V i t = right-person.</formula><p>one can maintain his/her gaze in the direction of the target of interest. Therefore, in the general case, gazing to an object is achieved by a combination of eye and head movements.</p><p>In the case of small gaze shifts, e.g. reading or watching TV, eye movements are predominant. In the case of large gaze shifts, often needed in social scenarios, head movements are necessary since eyeball movements have limited range, namely ±35° <ref type="bibr" target="#b20">[21]</ref>. Therefore, the proposed model considers that gaze shifts are produced by head movements that occur simultaneously with eye movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>We consider a scenario composed of N active targets and M passive targets. An active target is likely to move and/or to have a leading role in an interaction. Active targets are persons and robots. <ref type="foot" target="#foot_2">3</ref> Passive targets are objects, e.g. wall paintings. The set of all targets is indexed from 0 to N + M , where the index 0 designates "no target". Let i be an active target (a person or a robot), 1 ≤ i ≤ N , and j be a passive target (an object), N + 1 ≤ j ≤ N + M . A VFOA is a discrete random variable defined as follows: V i t = j means person (or robot) i looks at target j at time t. The VFOA of a person (or robot) i that looks at none of the known targets is V i t = 0. The case V i t = i is excluded. The set of all VFOAs at time t is denoted by V t = V 1 t , . . . , V N t . Two continuous variables are now defined: head orientation and gaze direction. The head orientation of person i at t is denoted with</p><formula xml:id="formula_1">H i t = [φ i H,t , θ i H,t ] ,</formula><p>i.e. the pan and tilt angles of the head with respect to some fixed coordinate frame. The gaze direction of person i is denoted with G i t and is also parameterized by pan and tilt with respect to the same coordinate frame, namely</p><formula xml:id="formula_2">G i t = [φ i G,t , θ i G,t ]</formula><p>. Although eyeball orientation is neither needed nor used, it is worth noticing that it is the difference between G i t and H i t . These variables are illustrated on Fig. <ref type="figure">1</ref>.</p><p>Finally, to establish a link between VFOAs and gaze directions, the target locations must be defined as well. Let X i t = [x i t , y i t , z i t ] be the location of target i. In the case of a person, this location corresponds to the head center while in the case of a passive target, it corresponds to the target center. These locations are defined in the same coordinate frame as above. Also notice that the direction from the active target i to target j is defined by the unit vector X ij t = (X j t -X i t )/ X j t -X i t which can also be parameterized by two angles, X ij t = [φ i,j X,t , θ i,j X,t ] . As already mentioned, target locations and head orientations are observed random variables, while VFOAs and gaze directions are latent random variables. The problem to be solved can now be formulated as a maximum a posteriori (MAP) problem:</p><formula xml:id="formula_3">Vt , Ĝt = argmax Vt,Gt P (V t , G t |H 1:t , X 1:t )<label>(1)</label></formula><p>Since there is no deterministic relationship between head orientation and gaze direction, we propose to model it probabilistically. For this purpose, we introduce an additional latent random variable, namely the head reference orientation,</p><formula xml:id="formula_4">R i t = [φ i R,t , θ i R,t</formula><p>] , which we choose to coincide with the upper-body orientation. We use the following generative model, initially introduced in <ref type="bibr" target="#b25">[26]</ref>, linking gaze direction, head orientation, and head reference orientation:</p><formula xml:id="formula_5">P (H i t |G i t , R i t ; α, Σ H ) = N (H i t ; µ i H,t , Σ H ),<label>(2)</label></formula><formula xml:id="formula_6">with µ i H,t = αG i t + (I 2 -α)R i t ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">Σ H ∈ R 2×2 is a covariance matrix, I 2 ∈ R 2×2 is the identity matrix and α = Diag (α 1 , α 2 ) is a diagonal matrix of mixing coefficients, 0 &lt; α 1 , α 2 &lt; 1.</formula><p>Also it is assumed that the covariance matrix is the same for all the persons and over time. Therefore, head orientation is an observed random</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gt-1 Gt</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ht-1 Ht</head><p>Rt-1 Rt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Xt-1 Xt</head><p>Vt-1 Vt variable normally distributed around a convex combination between two latent variables: gaze direction and head reference orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gaze Dynamics</head><p>The following model is proposed:</p><formula xml:id="formula_8">P (G i t |G i t-1 Ġi t-1 , V i t = j, X t ) = N (G i t ; µ ij G,t , Γ G ), (4) P ( Ġi t | Ġi t-1 ) = N ( Ġi t ; Ġi t-1 , Γ Ġ),<label>(5) with:</label></formula><formula xml:id="formula_9">µ ij G,t = G i t-1 + Ġi t-1 dt, if j = 0, βG i t-1 + (I 2 -β)X ij t + Ġi t-1 dt, if j = 0,<label>(6)</label></formula><p>where Ġi t = dG i t /dt is the gaze velocity, Γ G , Γ Ġ ∈ R 2×2 are covariance matrices, and β = Diag (β 1 , β 2 ) is a diagonal matrix of mixing coefficients, 0 &lt; β 1 , β 2 &lt; 1. Therefore, if a person looks at one of the targets, then his/her gaze dynamics depends on the person-to-target direction X ij t at a rate equal to β, and if a person doesn't look at one of the targets, then his/her gaze dynamics follows a random walk.</p><p>The head reference orientation dynamics can be defined in a similar way:</p><formula xml:id="formula_10">P (R i t |R i t-1 , Ṙi t-1 ) = N (R i t ; µ i R,t , Γ R ),<label>(7)</label></formula><formula xml:id="formula_11">P ( Ṙi t | Ṙi t-1 ) = N ( Ṙi t ; Ṙi t-1 , Γ Ṙ),<label>(8)</label></formula><formula xml:id="formula_12">with µ i R,t = R i t-1 + Ṙi t-1 dt,</formula><p>where Ṙi t = dR i t /dt is the head reference orientation velocity and Γ R , Γ Ṙ ∈ R 2×2 are covariance matrices. The dependencies between all the model variables are shown as a graphical representation in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>It is assumed that the gaze directions associated with different people are independent, given the VFOAs V 1:t . The crossdependency between different people is taken into account by the VFOA dynamics as detailed in section III-C below.</p><p>Similarly, head orientations, and head reference orientations associated with different people are independent, given the VFOAs. By combining the above equations with this independency assumption, we obtain:</p><formula xml:id="formula_13">P (H t |G t , R t ) = i N (H i t ; µ i H,t , Σ H )<label>(9)</label></formula><formula xml:id="formula_14">P (G t |G t-1 , Ġt-1 , V t , X t ) = i,j N (G i t ; µ ij G,t , Γ G ) δj (V i t )<label>(10)</label></formula><formula xml:id="formula_15">P (R t |R t-1 , Ṙt-1 ) = i N (R i t ; µ i R,t , Γ R ) (11)</formula><p>where the dependencies between variables are embedded in the variable means, i.e. (3) and ( <ref type="formula" target="#formula_9">6</ref>). The covariance matrices will be estimated via training. While gaze directions can vary a lot, we assume that head reference orientations are almost constant over time, which can be enforced by imposing that the total variance of gaze is much larger than the total variance of head reference orientation, namely:</p><formula xml:id="formula_16">Tr(Γ G ) Tr(Γ R ),<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. VFOA Dynamics</head><p>Using a first-order Markov approximation, the VFOA transition probabilities can be written as:</p><formula xml:id="formula_17">P (V t |V 1:t-1 ) = P (V t |V t-1 ),<label>(13)</label></formula><p>Notice that matrix</p><formula xml:id="formula_18">P (V t |V t-1 ) is of size (N + M ) N × (N + M ) N .</formula><p>Indeed, there are N persons (active targets), and N + M +1 targets (one "no" target, N active targets and M passive targets) and the case of a person that looks to him/herself is excluded. For example, if N = 2 and M = 4, matrix (13) has (2 + 4) 2×2 = 1296 entries. The estimation of this matrix would require, in principle, a large amount of training data, in particular in the presence of many symmetries. We show that, in practice, only 15 different transitions are possible. This can be seen on the following grounds. We start by assuming conditional independence between the VFOAs at t:</p><formula xml:id="formula_19">P (V t |V t-1 ) = i P (V i t |V t-1 ).<label>(14)</label></formula><p>Let's consider V i t , the VFOA of person i at t, given V t-1 , the VFOAs at t -1. One can distinguish two cases:</p><formula xml:id="formula_20">• V i t-1 = k where k is either a passive target, N &lt; k ≤ N + M , or it is none of the targets, k = 0; in this case V i t depends only on V i t-1</formula><p>, and</p><formula xml:id="formula_21">• V i t-1 = k, where k = i is a person 1 ≤ k ≤ N ; in this case V i t depends on the both V i t-1 and V k t-1 .</formula><p>To summarize, we can write that:</p><formula xml:id="formula_22">P (V i t = j|V t-1 ) = P (V i t = j|V i t-1 = k, V k t-1 = l) if 1 ≤ k ≤ N, P (V i t = j|V i t-1 = k) otherwise. (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>Based on this it is now possible to count the total number of possible VFOA transitions. With the same notations as in <ref type="bibr" target="#b14">(15)</ref>, we have the following possibilities:</p><p>• k = 0 (no target): there are two possible transitions, j = 0 and j = 0. • N &lt; k ≤ N +M (passive target): there are three possible transitions, j = 0, j = k, and j = k. • 1 ≤ k ≤ N, l = 0 (active target k looks at no target):</p><p>there are three possible transitions, j = 0, j = k, and j = k. • 1 ≤ k ≤ N, l = i (active target k looks at person i): there are three possible transitions, j = 0, j = k, and j = k. • 1 ≤ k ≤ N, l = 0, i (active target k looks at active target l different than i): there are four possible transitions, j = 0, j = k, j = l and j = k, l. Therefore, there are 15 different possibilities for P (V i t = j|V t-1 ), i.e. appendix A. Moreover, by assuming that the VFOA transitions don't depend on i, we conclude that the transition matrix may have up to 15 different entries. Moreover, the number of possible transitions is even smaller if there is no passive target (M = 0), or if the number of active targets is small, e.g. N &lt; 3. This considerably simplifies the task of estimating this matrix and makes the task of learning tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INFERENCE</head><p>We start by simplifying the notation, namely</p><formula xml:id="formula_24">L t = [G t ; Ġt ; R t ; Ṙt ] where [•; •] denotes vertical concatenation.</formula><p>The emission probabilities (9) become:</p><formula xml:id="formula_25">P (H t |L t ) = i N (H i t ; µ i H,t , Σ H ),<label>(16)</label></formula><formula xml:id="formula_26">with µ i H,t = CL i t ,<label>(17)</label></formula><p>where matrix C is obtained from the definition of L t above and from (3):</p><formula xml:id="formula_27">C = α 1 0 0 0 1 -α 1 0 0 0 0 α 2 0 0 0 1 -α 2 0 0 .</formula><p>The transition probabilities can be obtained by combining <ref type="bibr" target="#b9">(10)</ref> and ( <ref type="formula">11</ref>) with ( <ref type="formula" target="#formula_8">5</ref>) and (8):</p><formula xml:id="formula_28">P (L t |V t , L t-1 , X t ) = i j N (L i t ; µ ij L,t , Γ L ) δj (V i t ) ,<label>(18)</label></formula><p>with</p><formula xml:id="formula_29">µ ij L,t = A ij t L i t-1 + b ij t (<label>19</label></formula><formula xml:id="formula_30">)</formula><p>and</p><formula xml:id="formula_31">Γ L =     Γ G Γ Ġ Γ R Γ Ṙ    ,<label>(20)</label></formula><p>where A ij t is an 8 × 8 matrix and b ij t is an 8 × 1 vector. The indices i,j and t cannot be dropped since the transitions depend on X ij t from (6). The MAP problem (1) can now be derived in a Bayesian framework for the VFOA variables:</p><formula xml:id="formula_32">P (V t |H 1:t , X 1:t ) = P (V t , L t |H 1:t , X 1:t )dL t .<label>(21)</label></formula><p>We propose to study the filtering distribution of the joint latent variables, namely P (V t , L t |H 1:t , X 1:t ). Indeed, Bayes rule yields:</p><formula xml:id="formula_33">P (V t , L t |H 1:t , X 1:t ) = 1 c P (H t |L t )P (L t , V t |H 1:t-1 , X 1:t ). (<label>22</label></formula><formula xml:id="formula_34">)</formula><p>where c is the normalization evidence. Now we can introduce V t-1 and L t-1 using the sum rule:</p><formula xml:id="formula_35">P (L t ,V t |H 1:t-1 , X 1:t ) = Vt-1 P (L t , V t , L t-1 , V t-1 |H 1:t-1 , X 1:t )dL t-1 = Vt-1 P (L t |V t , L t-1 , X t )P (V t |V t-1 ) × P (L t-1 , V t-1 |H 1:t-1 , X 1:t-1 )dL t-1 ,<label>(23)</label></formula><p>where unnecessary dependencies were removed. Combining ( <ref type="formula" target="#formula_33">22</ref>) and ( <ref type="formula" target="#formula_35">23</ref>) we obtain a recursive formulation in P (V t , L t |H 1:t , X 1:t ). However, this model is still intractable without further assumptions. The main approximation used in this work consists of assuming local independence for the posteriors:</p><formula xml:id="formula_36">P (L t , V t |H 1:t , X 1:t ) i P (L i t , V i t |H 1:t , X 1:t ).<label>(24)</label></formula><p>A. Switching Kalman Filter Approximation</p><p>Several strategies are possible, depending upon the structure of P (L t , V t |H 1:t , X 1:t ). Commonly used strategies to evaluate this distribution include variational Bayes or Monte-Carlo. Alternatively, we propose to cast the problem into the framework of switching Kalman filters (SKF) <ref type="bibr" target="#b32">[33]</ref>. We assume the filtering distribution to be Gaussian,</p><formula xml:id="formula_37">P (L t , V t |H 1:t , X 1:t ) ∝ N (L t ; µ t , Σ t ).<label>(25)</label></formula><p>From ( <ref type="formula" target="#formula_36">24</ref>) and ( <ref type="formula" target="#formula_37">25</ref>) we obtain the following factorization:</p><formula xml:id="formula_38">P (L t , V t |H 1:t , X 1:t ) ∝ i j N (L i t ; µ ij t , Σ ij t ) δj (V i t ) .<label>(26)</label></formula><p>Thus, ( <ref type="formula" target="#formula_35">23</ref>) can be split into N components, one for each active target i:</p><formula xml:id="formula_39">P (L i t ,V i t = j|H 1:t , X 1:t ) ∝ P (H i t |L i t ) × Vt-1 N (L i t ; A ij t L i t-1 + b ij t )P (V i t |V t-1 ) × k N (L i t-1 ; µ ik t-1 , Σ ik t-1 ) δ k (V i t-1 ) dL i t-1 ,<label>(27)</label></formula><p>or, after several algebraic manipulations:</p><formula xml:id="formula_40">P (L i t , V i t = j|H 1:t , X 1:t ) ∝ k w ijk t-1,t N (L i t ; µ ijk t , Σ ijk t ).<label>(28)</label></formula><p>In this expression, µ ijk t and Σ ijk t are obtained by performing constrained Kalman filtering on µ ik t-1 , Σ ik t-1 with transition dynamics defined by A ij t and b ij t , emission dynamics defined by C, and observation H i t , i.e. <ref type="bibr" target="#b33">[34]</ref>. The weights w ijk t-1,t are defined as</p><formula xml:id="formula_41">P (V i t-1 = k | V i t = j, H 1:t , X 1:t ).</formula><p>The constraint comes from the fact that ||G i t -H i t || &lt; 35°and is achieved by projecting the mean (refer to <ref type="bibr" target="#b33">[34]</ref> for more details).</p><p>This can be rephrased as follows: from the filtering distribution at time t -1, there are N + M possible dynamics for L i t . The normal distribution at time t -1 then becomes a mixture of N + M normal distributions at time t as shown in <ref type="bibr" target="#b27">(28)</ref>. However, we expect a single Gaussian such as</p><formula xml:id="formula_42">P (L i t , V i t = j|H 1:t , X 1:t ) ∝ N (L i t ; µ ij t , Σ ij t )</formula><p>. This can be done by moment matching:</p><formula xml:id="formula_43">µ ij t = k w ijk t-1,t µ ijk t (<label>29</label></formula><formula xml:id="formula_44">)</formula><formula xml:id="formula_45">Σ ij t = k w ijk t-1,t (Σ ijk t + (µ ijk t -µ ij t )(µ ijk t -µ ij t ) ) (30)</formula><p>Finally, it is necessary to evaluate w ijk t-1,t . Let's introduce the following notations:</p><formula xml:id="formula_46">c ijk t-1,t = P (V i t = j, V i t-1 = k|H 1:t , X 1:t ),<label>(31)</label></formula><formula xml:id="formula_47">c ij t = P (V i t = j|H 1:t , X 1:t ).<label>(32)</label></formula><p>It follows that</p><formula xml:id="formula_48">c ij t = k c ijk t-1,t</formula><p>and</p><formula xml:id="formula_49">w ijk t-1,t = c ijk t-1,t c ij t .</formula><p>By applying Bayes formula to c ijk t-1,t , yields:</p><formula xml:id="formula_50">c ijk t-1,t ∝ P (H t |V i t = j, V i t-1 = k, H 1:t-1 , X 1:t ) ×c ik t-1 P (V i t = j|V i t-1 = k, H 1:t-1 , X 1:t-1 )<label>(33)</label></formula><p>Then, c ik t-1 is obtained from c ijk t-2,t-1 calculated at previous time step. The last factor in <ref type="bibr" target="#b32">(33)</ref> </p><formula xml:id="formula_51">is either equal to l c kl t-1 P (V i t = j|V i t-1 = k, V k t-1 = l) if k is an active target, or P (V i t = j|V i t-1 = k)</formula><p>otherwise. Both cases are straightforward to compute. Finally, the first factor in <ref type="bibr" target="#b32">(33)</ref>, the observation component, can be factorized as</p><formula xml:id="formula_52">P (H i t |V i t = j, V i t-1 = k, H 1:t-1 , X 1:t ) × n =i m p P (H n t |V n t = m, V n t-1 = p, H 1:t-1 , X 1:t )</formula><p>. By introducing the latent variable L, we obtain:</p><formula xml:id="formula_53">P (H n t |V n t = m, V n t-1 = p, H 1:t-1 , X 1:t ) = P (H n t |L n t ) P (L n t |L n t-1 , V n t = m, X t ) × P (L n t-1 |V n t-1 = p, H 1:t-1 , X 1:t-1 )dL n t-1 dL n t .<label>(34)</label></formula><p>All the factors (34) are normal distributions, hence it integrates in closed-form. In summary, we devised a procedure to estimate an online approximation of the joint filtering distribution of the VFOAs, V t , and of the gaze and head reference directions, L t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNING</head><p>The proposed model has two sets of parameters that must be estimated: the transition probabilities associated with the discrete VFOA variables, and the parameters associated with the Gaussian distributions. Learning is carried out using Q recordings with annotated VFOAs. Each recording is composed of T q frames, 1 ≤ q ≤ Q and contains N q active targets (the robot is the active target 1 and the persons are indexed from 2 to N q ) and M q passive targets. In addition to target locations and head poses, it is worth noticing that the learning algorithm requires VFOA ground-truth annotations, while gaze directions are still treated as latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning the VFOA Transition Probabilities</head><p>The VFOA transitions are drawn from the generalized Bernoulli distribution. Therefore, the transition probabilities can be estimated with P (V i t = j|V t-1 ) = E t-1 [δ j (V i t )], where δ j (i) is the Kronecker delta function. In Section III-C we showed that there are up to 15 different possibilities for the VFOA transition probability. This enables us to derive an explicit formula for each case, see appendix B. Consider for example one of these cases, namely</p><formula xml:id="formula_54">p 14 = P (V i t = l|V i t-1 = k, V k t-1 = l)</formula><p>, which is the conditional probability that at t person i looks at target l, given that at t -1 person i looked at person k and that person k looked at target l. This probability can be estimated with the following formula:</p><formula xml:id="formula_55">p14 = Q q=1 Nq i=2 Tq t=2 Nq k=1 k =i l =i,k δ l (V q,i t )δ k (V q,i t-1 )δ l (V q,k t-1 ) Q q=1 Nq i=2 Tq t=2 Nq k=1 k =i l =i,k δ k (V q,i t-1 )δ l (V q,k t-1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning the Gaussian Parameters</head><p>In Section IV we described the derivation of the proposed model that is based on SKF. This model requires the parameters (means and covariances) of the Gaussian distributions defined in ( <ref type="formula" target="#formula_25">16</ref>) and <ref type="bibr" target="#b17">(18)</ref>. Notice however that the mean ( <ref type="formula" target="#formula_26">17</ref>) of ( <ref type="formula" target="#formula_25">16</ref>) is parameterized by α. Similarly, the mean ( <ref type="formula" target="#formula_29">19</ref>) of ( <ref type="formula" target="#formula_28">18</ref>) is parameterized by β. Consequently, the model parameters are:</p><formula xml:id="formula_56">θ = (α, β, Γ L , Σ H ),<label>(35)</label></formula><p>and we remind that α and β are 2 × 2 diagonal matrices, Γ L is a 8 × 8 covariance and and Σ H is a 2 × 2 covariance, and that we assumed that these matrices are common to all the active targets. Hence the total number of parameters is equal to 2 + 2 + 36 + 3 = 43.</p><p>In the general case of SKF models, the discrete variables are unobserved both for learning and for inference. Here we propose a learning algorithm that takes advantage of the fact that the discrete variables, i.e. VFOAs, are observed during the learning process, namely the VFOAs are annotated. We propose an EM algorithm adapted from <ref type="bibr" target="#b34">[35]</ref>. In the case of a standard Kalman filter, an EM iteration alternates between a forward-backward pass to compute the expected latent variables (E-step), and between the maximization of the expected complete-data log-likelihood (M-step).</p><p>We start by describing the M-step. The complete-data loglikelihood is:</p><formula xml:id="formula_57">ln P (L 1 , H 1 , . . . , L Q , H Q |θ) = Q q=1 Nq i=2 Tq t=2 ln P (L q,i t |L q,i t-1 , β, Γ L ) + Q q=1 Nq i=2 Tq t=1 ln P (H q,i t |L q,i t , α, Σ H ).<label>(36)</label></formula><p>By taking the expectation w.r.t. the posterior distribution P (L 1 , . . . , L Q |H 1 , . . . , H Q , θ), we obtain:</p><formula xml:id="formula_58">Q(θ, θ old ) = E L 1 ,...,L K |θ old ln P (L 1 , H 1 , . . . , L Q , H Q |θ) ,<label>(37)</label></formula><p>which can be maximized w.r.t. to the parameters θ, which yields closed-form expressions for the covariance matrices:</p><formula xml:id="formula_59">Γ L = Q q=1 Nq i=2 Tq t=2 E[(L q,i t -µ q,ij L,t )(L i t -µ q,ij L,t ) ] Q q=1 (N q -1)(T q -1)<label>(38)</label></formula><p>where µ q,ij L,t = A q,ij t L q,i t-1 + b q,ij t , i.e. <ref type="bibr" target="#b18">(19)</ref>, and:</p><formula xml:id="formula_60">Σ H = Q q=1 Nq i=2 Tq t=1 E[(H q,i t -µ q,i H,t )(H q,i t -µ q,i H,t ) ] Q q=1 (N q -1)T q ,<label>(39)</label></formula><p>where µ q,i H,t = CL q,i t , i.e. <ref type="bibr" target="#b16">(17)</ref>. The estimation of α and of β is carried out in the following way. ∂Q(θ, θ old )/∂β 1 = 0 and ∂Q(θ, θ old )/∂β 2 = 0 yield a set of two linear equations in the two unknowns:</p><formula xml:id="formula_61">Q q=1 Nq i=2 Tq t=2 E (L q,i t -µ q,ij L,t ) Γ -1 L ∂ ∂β 1 (L q,i t -µ q,ij L,t ) = 0, Q q=1 Nq i=2 Tq t=2 E (L q,i t -µ q,ij L,t ) Γ -1 L ∂ ∂β 2 (L q,i t -µ q,ij L,t ) = 0,<label>(40)</label></formula><p>and similarly:</p><formula xml:id="formula_62">Q q=1 Nq i=2 Tq t=1 E (H q,i t -µ q,i H,t ) Σ -1 H ∂ ∂α 1 (H q,i t -µ q,i H,t ) = 0, Q q=1 Nq i=2 Tq t=1 E (H q,i t -µ q,i H,t ) Σ -1 H ∂ ∂α 2 (H q,i t -µ q,i H,t ) = 0,<label>(41)</label></formula><p>where as above, the expectation is taken w.r.t. to the posterior distribution. Once the formulas above are expanded and once the means µ q,ij L,t and µ q,i H,t are substituted with their expressions, the following terms remain to be estimated:</p><formula xml:id="formula_63">E[L q,i t ], E[L q,i t L q,i t</formula><p>] and E[L q,i t L q,i t-1 ]. The E-step provides estimates of these expectations via a forward-backward algorithm. For the sake of clarity, we drop the superscripts i (active target index) and q (recording index) up to equation (48) below. Introducing the notation P (L t |H 1 , . . . , H t ) = N (L t ; µ t , P t ), the forward-pass equations are:</p><formula xml:id="formula_64">µ t = A t µ t-1 + b t + K t (H t -C(A t µ t-1 + b t )) (42) P t = (I -K t C)P t,t-1 ,<label>(43)</label></formula><p>where:</p><formula xml:id="formula_65">P t,t-1 = A t P t-1 A t + Γ L ,<label>(44)</label></formula><formula xml:id="formula_66">K t = P t,t-1 C (CP t,t-1 C + Σ H ) -1 .<label>(45)</label></formula><p>The backward pass estimates P (L t |H 1 , . . . , H T ) = N (L t ; μt , Pt ) and leads to</p><formula xml:id="formula_67">μt = µ t + J t ( μt+1 -(A t+1 µ t + b t+1 )),<label>(46)</label></formula><formula xml:id="formula_68">Pt = P t + J t ( Pt+1 -P t+1,t )J t ,<label>(47)</label></formula><p>where:</p><formula xml:id="formula_69">J t = P t A t+1 (P t+1,t ) -1 .<label>(48)</label></formula><p>The expectations are estimated by performing a forwardbackward pass over all the persons and all the recordings of the training data. This yields the following formulas:</p><formula xml:id="formula_70">E[L q,i t ] = μq,i t (49) E[L q,i t L q,i t ] = Pq,i t + μq,i t μq,i t (50) E[L q,i t L q,i t-1 ] = Pq,i t J q,i t-1 + μq,i t μq,i t-1<label>(51)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. IMPLEMENTATION DETAILS</head><p>The proposed method was evaluated on the Vernissage dataset <ref type="bibr" target="#b2">[3]</ref> and on the Looking At Each Other (LAEO) dataset <ref type="bibr" target="#b3">[4]</ref>. We describe in detail these datasets and their annotations. We provide implementation details and we analyse the complexity of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Vernissage Dataset</head><p>The Vernissage scenario can be briefly described as follows, e.g. Fig. <ref type="figure" target="#fig_1">3</ref>: there are three wall paintings, namely the passive targets denoted with o 1 , o 2 , and o 3 (M = 3); two persons, denoted left person (left-p) and right person (right-p), interact with the robot, hence N = 3. The robot plays the role of an art guide, describing the paintings and asking questions to the two persons in front of him. Each recording is split into two roughly equal parts. The first part is dedicated to painting explanation, with a one-way interaction. The second part consists of a quiz, thus illustrating a dialog between the two participants and the robot, most of the time concerning the paintings. The scene was recorded with a camera embedded into the robot head and with a VICON motion capture system consisting of a network of infrared cameras, placed onto the walls, and of optical markers, placed onto the robot and people heads. Both were recorded at 25 frames per second (fps). There is a total of ten recordings, each lasting ten minutes. The VICON system provided accurate estimates of head positions, X 1:T and head orientations, H 1:T . Head positions and head orientations were also estimated using from the RGB images gathered with the camera embedded into the robot head. The RGB images are processed as follows. We use the OpenCV version of <ref type="bibr" target="#b35">[36]</ref> to detect faces and their bounding boxes which are then tracked over time using <ref type="bibr" target="#b36">[37]</ref>. Next, we extract HOG descriptors from each bounding box and apply a head-pose estimator, e.g. <ref type="bibr" target="#b37">[38]</ref>. This yields H1:T . The 3D head positions, X1:T , can be estimated using the line of sight through the face center and the bounding-box size, which provides a rough estimate of the depth along the line of sight.</p><p>In the remaining of this paper, X 1:T and H 1:T are referred to as Vicon Data; X1:T and H1:T as RGB Data. Because the whole setup was carefully calibrated, both Vicon and RGB Data are represented in the same coordinate frame.</p><p>In all our experiments we assumed that the passive targets are static and their positions are provided in advance. The position of the robot itself is also known in advance and one can easily estimate the orientation of the robot head from motor readings. Finally, the VFOAs of the participants were manually annotated in all the frames of all the recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The LAEO Dataset</head><p>The LAEO dataset <ref type="bibr" target="#b3">[4]</ref> is an extension of the TVHID (TV Human Interaction Dataset) <ref type="bibr" target="#b38">[39]</ref>. It consists of 300 videos extracted from TV shows. At least two actors appear in each video engaged in four human-human interactions: handshake, highfive, hug, and kiss. There are 50 videos for each interaction and 100 videos with no interaction. The videos have been grabbed at 25 fps and each video lasts from five seconds to twenty-five seconds. LAEO is further annotated, namely some of these videos are split into shots which are separated by cuts. There are 443 shots in total which are manually annotated whenever two persons look at each other, <ref type="bibr" target="#b3">[4]</ref>.</p><p>While there is no passive target in this dataset (M = 0), the number of active targets (N ) corresponds to the number of persons in each shot. In practice N varies from one to eight persons. All the faces in the dataset are annotated with a bounding box and with a coarse head-orientation label: frontal-right, frontal-left, profile-right, profile-left, backward. As with Vernissage, we use the bounding-box center and size to estimate the 3D coordinates of the heads, X 1:T . We also assigned a yaw value to each one of the five coarse head orientations, H 1:T . We also computed finer head orientations, H1:T , using <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithmic Details</head><p>The inference procedure is summarized in Algorithm 1. This is basically an iterative filtering procedure. The update step consists of applying the recursive relationship, derived in Section IV, to µ ij t , Σ ij t and c ij t , using µ ijk t , Σ ijk t and c ijk t-1,t as intermediate variables. The VFOA is chosen using MAP, given observations up to the current frame, and the gaze direction is the mean of the filtered distribution (the first two components of µ ij t are indeed the mean for the pan and tilt gaze angles).</p><p>Algorithm 1 Inference 1: procedure GAZEANDVFOA 2:</p><formula xml:id="formula_71">X 1 , H 1 ← GETOBSERVATIONS(time = 1) 3: c 1 , µ 1 , Σ 1 ← INITIALIZATION(H 1 , X 1 ) 4: V i 1 ← argmax j c ij 1 5: G i 1 ← µ ij 1 [1..2] 6:</formula><p>for t = 2..T do 7:</p><formula xml:id="formula_72">X t , H t ← GETOBSERVATIONS(time = t) 8: c t , µ t , Σ t ← UPDATE(H t , X t , c t-1 , µ t-1 , Σ t-1 )</formula><p>9: </p><formula xml:id="formula_73">V i t ← argmax j c ij</formula><formula xml:id="formula_74">G i t ← µ ij t [1..2] 11: return V 1:T , G 1:T</formula><p>Let's now describe the initialization procedure used by Algorithm 2. In a probabilistic framework, parameter intialization is generally addressed by defining an initial distribution, e.g. P (L 1 |V 1 ). Here, we did not explicitly define such a distribution. Initialization is based on the fact that, with repeated similar observation inputs, the algorithm reaches a steady-state. The initialization algorithm uses a repeated update method with initial observation to provide an estimate of gaze and of reference directions. Consequently, the initial filtering distribution </p><formula xml:id="formula_75">P (L 1 , V 1 |H 1 , X 1 )</formula><formula xml:id="formula_76">µ in ← [H 1 ; 0; H 1 ; 0] 3: Σ in ← I 4: c in ← 1 N +M (Uniform) 5:</formula><p>while Not Convergence do 6:</p><formula xml:id="formula_77">c in , µ in , Σ in ← UPDATE(H 1 , X 1 , c in , µ in , Σ in ) 7:</formula><p>return c in , µ in , Σ in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm Complexity</head><p>The computational complexity of Algorithm 1 is</p><formula xml:id="formula_78">C = T (C O + C U ) + T I C U , (<label>52</label></formula><formula xml:id="formula_79">)</formula><p>where T is the number of frames in a test video, T I is the number of iterations needed by the Algorithm 2 (initialization) to converge, C O is the computational complexity of GETOBSERVATION and C U is the computational complexity of UPDATE. The complexity of one iteration of Algorithm 1 is C O + C U . C O depends on face detection and head pose estimation algorithms. Hence we concentrate on C U . From Section IV one sees that the following values need to be computed:</p><formula xml:id="formula_80">P (H i t |V i t = j, V i t-1 = k, H 1:t-1 , X 1:t-1 ), c ijk t-1,t , µ ijk</formula><p>t , Σ ijk t , and then c ij t , µ ij t and Σ ij t , for each active target i, and for each combination of targets j and k different from i. There are N possible values for i and (N + M ) possible values for j and k. Then,</p><formula xml:id="formula_81">C U = K × N (N + M ) 2 , (<label>53</label></formula><formula xml:id="formula_82">)</formula><p>where K is a factor whose complexity can be estimated as follows. The most time-consuming part is the Kalman Filter algorithm used to estimate µ ijk t and Σ ijk t from µ ik t and Σ ik t . These calculations are dominated by several 8×8 and 2×8 matrix inversions and multiplications. By neglecting scalar multiplications and matrix additions, and by denoting with C KF the complexity of the Kalman filter, we obtain that K ≈ C KF and hence</p><formula xml:id="formula_83">C U ≈ C KF × N (N + M ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vernissage Dataset</head><p>We applied the same experimental protocol to the Vicon and RGB data. We used a leave-one-video-out strategy for training. The test is performed on the left out video. We used the frame recognition rate (FRR) metrics to quantitatively evaluate the methods. FRR computes the percentage of frames for which the VFOA is correctly estimated. One should note however that the ground-truth VFOAs were obtained by manually annotating each frame in the data. This is subject to errors since the annotator has to associate a target with each person.</p><p>The VFOA transition probabilities and the model parameters were estimated using the learning method described in Section V. Appendix B provides the formulas used for estimating the VFOA transition probabilities given the annotated data. Notice that the fifteen transitions probabilities thus estimated are identical for both data, Vicon and RGB.</p><p>The Gaussian parameters, i.e. <ref type="bibr" target="#b34">(35)</ref>, were estimated using the EM algorithm of Section V-B. This learning procedure requires head-pose estimates as well as the targets locations, estimated as just explained. Since these estimates are different for the two kinds of data (Vicon and RGB) we carried out the learning process twice, with the Vicon data and with the RGB data. The EM algorithm needs initialization. The initial parameter values for α and β are α 0 = β 0 = Diag (0.5, 0.5). Matrices Σ H and Γ L defined in <ref type="bibr" target="#b19">(20)</ref> are initialized with isotropic covariances:</p><formula xml:id="formula_84">Σ 0 H = σI 2 , Γ 0 G = Γ 0 Ġ = γI 2 , and Γ 0 R = Γ 0 Ṙ = ηI 2 with σ = 15, γ = 5</formula><p>, and η = 0.5. In particular, this initialization is consistent with <ref type="bibr" target="#b11">(12)</ref>. In practice we noticed that the covariances estimated by training remain consistent with <ref type="bibr" target="#b11">(12)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results with Vicon Data</head><p>The FRR of the estimated VFOAs for the Vicon data are summarized in Table <ref type="table" target="#tab_2">I</ref>. A few examples are shown in Figure <ref type="figure" target="#fig_4">5</ref>. The FRR score varies between 28.3% and 74.4% for <ref type="bibr" target="#b25">[26]</ref> and between 43.1% and 79.8% for the proposed method. Notice that high scores are obtained by both methods for recording #27. Similarly, low scores are obtained for recording #26. Since both methods assume that head motions and gaze shifts occur synchronously, an explanation could be that this hypothesis is only valid for some of the participants. The confusion matrices for VFOA classification using Vicon data are given in Figure <ref type="figure" target="#fig_3">4</ref>. There are a few similarities between the results obtained with the two methods. In particular, wall painting #o 2 stands just behind Nao and both methods don't always discriminate between these two targets. In addition, the head of one of the persons is often aligned with painting  #o 1 from the viewpoint of the other person. A similar remark holds for painting #o 3 . As a consequence both methods often confuse the VFOA in these cases. This can be seen in the third image of Figure <ref type="figure" target="#fig_4">5</ref>. Indeed, it is difficult to estimate whether the left person (left-p) looks at #o 1 or at right-p.</p><p>Finally, both methods have problems with recognizing the VFOA "nothing" or gaze aversion (V i t = 0). We propose the following explanation: the targets are widespread in the scene, hence it is likely that an acceptable target lies in most of the gaze directions. Moreover, Nao is centrally positioned, therefore the head orientation used to look at Nao is similar to the resting head orientation used for gaze aversion. However, in <ref type="bibr" target="#b25">[26]</ref> the reference head orientation is fixed and poorly suited for dynamic head-to-gaze mapping, hence the high error rate on painting #o 3 . Our method favors the selection of a target, either active or passive, over the no target (nothing) case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results with RGB Data</head><p>The RGB images were processed as described in section VI-A above in order to obtain head orientations, H1:T , and 3D head positions, X1:T . Table <ref type="table" target="#tab_2">II</ref> shows the accuracy of these measurements (in degrees and in centimeters), when compared with the ground truth provided by the Vicon motion capture system. As it can be seen, while the head orientation estimates are quite accurate, the error in estimating the head positions can be as large as 0.8 m for participants lying in between 1.5 m and 2.5 m in front of a robot, e.g. recordings #19 and #24. In particular this error increases as a participant is farther away from the robot. In these cases, the bounding box is larger than it should be and hence the head position is, on an average, one meter closer than the true position. These relatively large errors in 3D head position affect the overall behavior of the algorithm.</p><p>The FRR scores obtained with the RGB data are shown in Table <ref type="table" target="#tab_4">III</ref>. As expected the loss in accuracy is correlated with the head position error: the results obtained with recordings #09 and #30 are close to the ones obtained with the Vicon data, whereas there is a significant loss in accuracy for the other recordings. The loss is notable for <ref type="bibr" target="#b25">[26]</ref> in the case of the right person (right-p) for the recordings #12, #18 and #27. The confusion matrices obtained with the RGB data are shown n Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>In the case of RGB data, the comparison between our method and the method of <ref type="bibr" target="#b30">[31]</ref> is biased by the use of different head orientation and 3D head position estimators. Indeed, the RGB data results reported in <ref type="bibr" target="#b30">[31]</ref> were obtained with unpublished methods for estimating head orientations and 3D head positions, and for head tracking. Moreover, <ref type="bibr" target="#b30">[31]</ref> uses cross-modal information, namely the speaker identity based on the audio track (one of the participants or the robot) as well as the identity of the object of interest. We also note that <ref type="bibr" target="#b30">[31]</ref> reports mean FRR values obtained over all the test recordings, instead of an FRR value for each recording. Table <ref type="table" target="#tab_5">IV</ref> summarizes a comparison between the average FRR obtained with our method, with <ref type="bibr" target="#b25">[26]</ref>, and with <ref type="bibr" target="#b30">[31]</ref>. Our method yields a similar FRR score as <ref type="bibr" target="#b30">[31]</ref> using the Vicon data (first row) in which case the same head pose inputs are     <ref type="bibr" target="#b25">[26]</ref>, WITH <ref type="bibr" target="#b30">[31]</ref>  used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. LAEO Dataset</head><p>As already mentioned in Section VI-B above, the LAEO annotations are incomplete to estimate the person-wise VFOA at each frame. Indeed, the only VFOA-related annotation is whether two people are looking at each other during the shot. This is not sufficient to know in which frames they are actually looking at each other. Moreover, when more than two people appear in a shot, the annotations don't specify who are the people that look at each other. For these reasons, we decided to estimate the parameters using Vicon data of the whole Vernissage dataset, i.e. cross-validation.</p><p>We used the same pipeline as with the Vernissage RGB data to estimate 3D head positions, X1:T , from the face bounding boxes. Concerning head orientation, there are two cases: coarse head orientations (manually annotated) and fine head orientations (estimated). Coarse head orientations were obtained in the following way: pan and tilt values were associated with each head orientation label, namely the pan angles -20°, 20°, -80°, 80°, and 180°were assigned to labels frontal-left, frontal-right, profile-left, profile-right, and backwards respectively, while a tilt anble of 0°was assigned to all five labels. Fine head orientations were estimated using the same procedure as in the case of the Vernissage RGB data, namely face detection, face tracking, and head orientation estimation using <ref type="bibr" target="#b37">[38]</ref>. Algorithm 1 was used to compute the VFOA for each frame and for each person thus allowing to determine who looks at whom, e.g. Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>We used two shot-wise, not frame-wise, metrics since the LAEO annotations are for each shot: the shot recognition rate (SRR), e.g. Table <ref type="table" target="#tab_7">V</ref>, and the average precision (AP), e.g. Table <ref type="table" target="#tab_7">VI</ref>. We note that <ref type="bibr" target="#b3">[4]</ref> only provides AP scores. It is interesting to note that the proposed method yields results comparable with those of <ref type="bibr" target="#b3">[4]</ref> on this dataset. This is quite  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>In this paper we addressed the problem of estimating and tracking gaze and visual focus of attention of a group of participants involved in social interaction. We proposed a Bayesian state-space model that exploits the correlation between head movements and eye gaze on one side, and between visual focus of attention and eye gaze on the other side. We described in detail the proposed formulation. In particular we showed that the entries of the large-sized matrix of VFOA transition probabilities have a very small number of different possibilities for which we provided closed-form formulae. The immediate consequence of this simplified transition matrix is that the associated learning doesn't require a large training dataset. We showed that the problem of simultaneously inferring VFOAs and gaze directions over time can be cast in the framework of a switching Kalman filter which, in our case, yields tractable learning and inferring algorithms.</p><p>We applied the proposed method to two datasets, Vernissage and LAEO. Vernissage contains several recordings of a humanrobot interaction scenario. We experimented both with motion capture data gathered with a Vicon system and with RGB data gathered with a camera mounted onto a robot head. We also experimented with the LAEO dataset that contains several hundreds of video shots extracted from TV shows. A quite remarkable result is that the parameters obtained by training the model with the Vernissage data have been successfully used for testing the method with the LAEO data, i.e. cross-validation. This can be explained by the fact that social interactions, even in different contexts, share a lot of characteristics. We compared our method with three other methods, based on HMMs <ref type="bibr" target="#b25">[26]</ref>, on input-output HMMs <ref type="bibr" target="#b30">[31]</ref>, and on a geometric model <ref type="bibr" target="#b3">[4]</ref>. The interest of these methods (including ours) resides in the fact that eye detection, unlike many existing gaze estimation methods, is not needed. This feature makes the above methods practical and effective in a very large number of situations, e.g. social interaction.</p><p>We note that gaze inference from head orientation is an illposed problem. Indeed, the correlation between gaze and head movements is person dependent as well as context dependent. It is however important to infer gaze whenever the eyes cannot be reliably observed in images and properly analyzed. We proposed to solve the problem based on the fact that alignments often occur between gaze directions and several targets, which is a sensible assumption in practice.</p><p>Contextual information could considerably improve the results. Indeed, additional information such as speaker recognition (as in <ref type="bibr" target="#b30">[31]</ref>), speaker localization <ref type="bibr" target="#b39">[40]</ref>, speech recognition, or speech-turn detection <ref type="bibr" target="#b40">[41]</ref> may be used to learn VFOA transitions in multi-party multimodal dialog systems.</p><p>In the future we plan to investigate discriminative methods based on neural network architectures for inferring gaze directions from head orientations and from contextual information. For example one could train a deep network from input-output pairs of head pose and visual focus of attention. For this purpose, one can combine a multiple-camera system, to accurately detect the eyes of several participants and to estimate their head poses, with a microphone-array and associated algorithms in order to infer both speaker and speech information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A VFOA TRANSITION PROBABILITIES</head><p>Using the notations introduced in Section III-C let i, 1 ≤ i ≤ N , be an active target. In Section III-C we showed that in practice the entries of the probability transition matrix can have up to 15 different expressions. For completeness, these expressions are listed below.</p><p>• The VFOA of i at t -1 is neither an active nor a passive target (k = 0):</p><formula xml:id="formula_85">p 1 =P (V i t = 0|V i t-1 = 0) p 2 =P (V i t = j|V i t-1 = 0)</formula><p>• The VFOA of i at t -1 is a passive target (N &lt; k ≤ N + M ):</p><formula xml:id="formula_86">p 3 =P (V i t = 0|V i t-1 = k) p 4 =P (V i t = k|V i t-1 = k) p 5 =P (V i t = j|V i t-1 = k)</formula><p>• The VFOA of i at t -1 is an active target (1 ≤ k ≤ N, k = i):</p><formula xml:id="formula_87">p 6 =P (V i t = 0|V i t-1 = k, V k t-1 = 0) p 7 =P (V i t = k|V i t-1 = k, V k t-1 = 0) p 8 =P (V i t = j|V i t-1 = k, V k t-1 = 0) p 9 =P (V i t = 0|V i t-1 = k, V k t-1 = i) p 10 =P (V i t = k|V i t-1 = k, V k t-1 = i) p 11 =P (V i t = j|V i t-1 = k, V k t-1 = i) p 12 =P (V i t = 0|V i t-1 = k, V k t-1 = l) p 13 =P (V i t = k|V i t-1 = k, V k t-1 = l) p 14 =P (V i t = l|V i t-1 = k, V k t-1 = l) p 15 =P (V i t = j|V i t-1 = k, V k t-1 = l)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B VFOA LEARNING</head><p>This appendix provides the formulae allowing to estimate the 15 transitions probabilities as explained in Section V-A. δ j (V q,i t )δ 0 (V q,i t-1 ) δ 0 (V q,i t )δ k (V q,i t-1 ) δ k (V q,i t )δ k (V q,i t-1 ) δ j (V q,i t )δ k (V q,i t-1 ) δ 0 (V q,i t )δ k (V q,i t-1 )δ 0 (V q,k t-1 ) δ k (V q,i t-1 )δ 0 (V q,k t-1 ) p7 = δ k (V q,i t )δ k (V q,i t-1 )δ 0 (V q,k t-1 ) δ j (V q,i t )δ k (V q,i t-1 )δ 0 (V q,k t-1 ) δ k (V q,i t-1 )δ 0 (V q,k t-1 ) p9 = δ 0 (V q,i t )δ k (V q,i t-1 )δ i (V q,k t-1 ) δ k (V q,i t-1 )δ i (V q,k t-1 ) p10 = δ k (V q,i t )δ k (V q,i t-1 )δ i (V q,k t-1 ) δ j (V q,i t )δ k (V q,i t-1 )δ i (V q,k t-1 ) δ k (V q,i t-1 )δ i (V q,k t-1 ) p12 = δ 0 (V q,i t )δ k (V q,i t-1 )δ l (V q,k t-1 ) δ k (V q,i t-1 )δ l (V q,k t-1 ) p13 = δ k (V q,i t )δ k (V q,i t-1 )δ l (V q,k t-1 ) δ k (V q,i t-1 )δ l (V q,k t-1 ) p14 = δ l (V q,i t )δ k (V q,i t-1 )δ l (V q,k t-1 ) δ j (V q,i t )δ k (V q,i t-1 )δ l (V q,k t-1 ) δ k (V q,i t-1 )δ l (V q,k t-1 )</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Graphical representation showing the dependencies between the variables of the proposed Bayesian dynamic model. The discrete latent variables (visual focus of attention) are shown with squares while continuous variables are shown with circles: observed variables (head pose and target locations) are shown with shaded circles and latent variables (gaze and reference) are shown with white circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The Vernissage setup. Left: Global view of an "exhibition" showing wall paintings, two participants, i.e. left-p and right-p, and the NAO robot. Right: Top view of the room showing the Vernissage layout.</figDesc><graphic coords="8,314.77,579.93,150.64,84.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Confusion matrices for the Vicon data. Left: [26]. Right: Proposed algorithm. Row-wise: ground-truth VFOAs. Column-wise: estimated VFOAs. Diagonal terms represent the recall.</figDesc><graphic coords="10,322.79,229.54,112.97,103.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results obtained with the proposed method on Vicon data. Gaze directions are shown with green arrows, head reference directions with dark-grey arrows and observed head directions with red arrows. The ground-truth VFOA is shown with a black circle. The top row displays the image of the robot-head camera. Top views of the room show results obtained for the left-p (middle row) and for the right-p (bottom row). In the last example the left-p gazes at "nothing".</figDesc><graphic coords="11,61.30,247.31,98.71,98.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results obtained with the proposed method on RGB data. Gaze directions are shown with green arrows, head reference directions with dark-grey arrows and observed head directions with red arrows. The ground-truth VFOA is shown with a black circle. The top row displays the image of the robot-head camera. Top views of the room show results obtained for the left person (left-p, middle row) and the right person (right-p, bottom row).</figDesc><graphic coords="12,61.30,247.31,98.71,98.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Confusion matrices for the RGB data. Left: [26]. Right: Proposed algorithm. Row-wise: ground-truth VFOAs. Column-wise: estimated VFOAs. Diagonal terms represent the recall.</figDesc><graphic coords="12,52.24,399.17,120.50,110.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. This figure shows some results obtained with the LAEO dataset. The top row shows results obtained with coarse head orientation and the bottom row shows results obtained with fine head orientation. Head orientations are shown with red arrows. The algorithm infers gaze directions (green arrows) and VFOAs (blue circles). People looking at each others are shown with a dashed blue line.</figDesc><graphic coords="13,56.33,129.06,123.02,69.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>k (V q,i t-1 )δ i (V q,k t-1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>l =i,k j =i,k,l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and R. Horaud are with INRIA Grenoble Rhône-Alpes and with Université Grenoble Alpes, Montbonnot Saint-Martin, France.</figDesc><table /><note><p>S. Ba is with Dailymotion, Paris, France This work is supported by ERC Advanced Grant VHIA #340113.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is implicitly defined as the expected stationary state.</figDesc><table><row><cell>Algorithm 2 Initialization</cell></row></table><note><p>1: procedure INITIALIZATION(H 1 , X 1 ) 2:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I FRR</head><label>I</label><figDesc>SCORES OF THE ESTIMATED VFOAS FOR THE VICON DATA FOR THE LEFT AND RIGHT PERSONS (LEFT-P AND RIGHT-P).</figDesc><table><row><cell>Recording</cell><cell cols="2">Ba &amp; Odobez [26]</cell><cell cols="2">Proposed</cell></row><row><cell></cell><cell>left-p</cell><cell>right-p</cell><cell cols="2">left-p right-p</cell></row><row><cell>09</cell><cell>51.6</cell><cell>65.1</cell><cell>59.8</cell><cell>61.4</cell></row><row><cell>10</cell><cell>64.3</cell><cell>74.4</cell><cell>76.5</cell><cell>65.0</cell></row><row><cell>12</cell><cell>53.5</cell><cell>67.6</cell><cell>61.6</cell><cell>63.2</cell></row><row><cell>15</cell><cell>67.1</cell><cell>46.2</cell><cell>64.8</cell><cell>67.6</cell></row><row><cell>18</cell><cell>37.5</cell><cell>28.3</cell><cell>62.0</cell><cell>53.7</cell></row><row><cell>19</cell><cell>56.7</cell><cell>45.4</cell><cell>54.5</cell><cell>60.4</cell></row><row><cell>24</cell><cell>44.9</cell><cell>49.0</cell><cell>59.7</cell><cell>54.7</cell></row><row><cell>26</cell><cell>40.3</cell><cell>32.9</cell><cell>43.6</cell><cell>43.1</cell></row><row><cell>27</cell><cell>65.8</cell><cell>72.0</cell><cell>79.8</cell><cell>78.3</cell></row><row><cell>30</cell><cell>69.1</cell><cell>49.1</cell><cell>72.0</cell><cell>63.9</cell></row><row><cell>Mean</cell><cell></cell><cell>54.5</cell><cell>62.6</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III FRR</head><label>III</label><figDesc>SCORES OF THE ESTIMATED VFOAS OBTAINED WITH [26] AND WITH THE PROPOSED METHOD FOR THE RGB DATA. THE LAST TWO COLUMNS SHOW THE 3D HEAD POSITION ERRORS OF TABLE II.</figDesc><table><row><cell cols="3">Video Ba &amp; Odobez [26]</cell><cell cols="2">Proposed</cell><cell cols="2">Head pos. error</cell></row><row><cell></cell><cell>left-p</cell><cell>right-p</cell><cell cols="3">left-p right-p left-p</cell><cell>right-p</cell></row><row><cell>09</cell><cell>50.3</cell><cell>59.8</cell><cell>58.1</cell><cell>55.9</cell><cell>18.1</cell><cell>20.8</cell></row><row><cell>12</cell><cell>54.2</cell><cell>14.8</cell><cell>59.0</cell><cell>46.5</cell><cell>35.7</cell><cell>41.5</cell></row><row><cell>18</cell><cell>39.0</cell><cell>16.1</cell><cell>64.2</cell><cell>33.1</cell><cell>36.9</cell><cell>12.8</cell></row><row><cell>27</cell><cell>38.2</cell><cell>17.1</cell><cell>53.3</cell><cell>55.1</cell><cell>64.5</cell><cell>58.3</cell></row><row><cell>30</cell><cell>61.6</cell><cell>44.6</cell><cell>54.7</cell><cell>66.6</cell><cell>16.7</cell><cell>13.3</cell></row><row><cell>Mean</cell><cell></cell><cell>39.0</cell><cell>54.7</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV MEAN</head><label>IV</label><figDesc>FRR SCORES OBTAINED WITH</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>AND WITH THE PROPOSED METHOD. RECORDING #26 WAS EXCLUDED FROM THE FRRMEANS AS REPORTED IN<ref type="bibr" target="#b30">[31]</ref>. MOREOVER,<ref type="bibr" target="#b30">[31]</ref> USES ADDITIONAL</figDesc><table><row><cell></cell><cell cols="2">CONTEXTUAL INFORMATION.</cell><cell></cell></row><row><cell></cell><cell cols="3">Ba &amp; Odobez [26] Sheikhi [31] Proposed</cell></row><row><cell>Vicon data</cell><cell>56.5</cell><cell>66.6</cell><cell>64.7</cell></row><row><cell>RGB data</cell><cell>39.0</cell><cell>62.4</cell><cell>54.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V AVERAGE</head><label>V</label><figDesc>SHOT RECOGNITION RATE (SRR) OBTAINED WITH<ref type="bibr" target="#b25">[26]</ref> AND WITH THE PROPOSED METHOD.</figDesc><table><row><cell></cell><cell cols="3">Ba &amp; Odobez [26] Proposed</cell></row><row><cell>Coarse head orientation</cell><cell>0.535</cell><cell>0.727</cell><cell></cell></row><row><cell>Fine head orientation</cell><cell>0.363</cell><cell>0.479</cell><cell></cell></row><row><cell></cell><cell>Table VI</cell><cell></cell><cell></cell></row><row><cell cols="4">AVERAGE PRECISION (AP) OBTAINED WITH [4], WITH BA &amp; ODOBEZ</cell></row><row><cell cols="3">[26] AND WITH THE PROPOSED METHOD.</cell><cell></cell></row><row><cell cols="2">Marin-Jimenez et al. [4]</cell><cell>[26]</cell><cell>Proposed</cell></row><row><cell>Coarse head orientation</cell><cell>0.925</cell><cell>0.916</cell><cell>0.923</cell></row><row><cell>Fine head orientation</cell><cell>0.896</cell><cell>0.838</cell><cell>0.890</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that orientation generally refers to the pan, tilt and roll angles of a rigid-body pose, while direction refers to the polar and azimuth angles or, equivalently, a unit vector. Since the contribution of the roll angle to gaze is generally marginal, in this paper we make no distinction between orientation and direction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Supplementary materials, that include a software package and examples of results, are available at https://team.inria.fr/perception/research/eye-gaze/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that in case of a robot, the gaze direction and the head orientation are identical and that the latter can be easily estimated from the head motors.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank <rs type="person">Vincent Drouard</rs> for his valuable expertise in head pose estimation and tracking.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eye-head coordination during headunrestrained gaze shifts in rhesus monkeys</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Sparks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coordination of the eyes and head during visual orienting</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The vernissage corpus: A multimodal humanrobot-interaction dataset</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Jayagopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IDIAP, Tech. Rep</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting people looking at each other in videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new methodology for determining pointof-gaze in head-mounted eye tracking systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eizenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date type="published" when="2004-10">Oct 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaze guided object recognition using a head-mounted eye tracker</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ETRA Symposium</title>
		<meeting>the ETRA Symposium</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lightweight, low-cost, sidemounted mobile eye tracking system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K A</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cockburn</surname></persName>
		</author>
		<editor>IEEE WNYIPW</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual analytics for mobile eye tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hlawatsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="2017-01">Jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining driver visual attention with one camera</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Behavior recognition based on head pose and gaze direction measurement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IROS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A free-head, simple calibration, gaze tracking system that enables gaze-based interaction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ETRA Symposium</title>
		<meeting>the ETRA Symposium</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive linear regression for appearance-based gaze estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2014-10">Oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning gaze biases with head motion for head pose-free gaze estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Head pose estimation in computer vision: A survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">3D head pose estimation from multiple distant views</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarmis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Head direction estimation from low resolution images with scene adaptation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chamveha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sugimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Siriteerakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring transfer learning approaches for head pose classification from multi-view surveillance images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Vieriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multi-task learning framework for head pose estimation under target motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social grouping for multi-target tracking and head pose estimation in video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Amplitude of human head movements associated with horizontal saccades</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human eye-head coordination in two dimensions under different sensorimotor conditions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Opstal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Head orientation and gaze direction in meetings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bayesian hierarchy for robust gaze estimation in human-robot interaction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lanillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual focus of attention in non-calibrated environments using gaze estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asteriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognizing visual focus of attention from head pose in natural meetings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System Men and Cybernetics. Part B</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing the visual focus of attention for human robot interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sheikhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Behavior Understanding Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint attention by gaze interpolation and saliency</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yucel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mericli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mericli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System Men and Cybernetics. Part B</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conversation scene analysis with dynamic bayesian network based on visual head tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takemae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual focus of attention estimation with unsupervised incremental learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining dynamic head pose-gaze mapping with the robot conversational state for attention recognition in human-robot interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sheikhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simultaneous estimation of gaze direction and visual focus of attention for multi-person-to-robot interaction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Massé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Switching Kalman filters</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UC Berkeley, Tech. Rep</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kalman filtering with state constraints: a survey of linear and nonlinear algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Theory Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>IET</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust head-pose estimation based on partially-latent mixture of linear regressions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Drouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High five: Recognising human interactions in TV shows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple-speaker localization based on direct-path features and likelihood maximization with spatial sparsity regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1997" to="2012" />
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Audio-visual speaker diarization based on spatiotemporal bayesian fusion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
