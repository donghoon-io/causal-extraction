<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Bayesian Clustering and Feature Selection Through Student&apos;s t Mixtures Model</title>
				<funder ref="#_9a5SwEs">
					<orgName type="full">Key Science and Technology Project of Wuhan</orgName>
				</funder>
				<funder ref="#_cnPGdew #_JEuhXrN #_xpvvBHz">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
				<funder ref="#_YAxJ9rS">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianyong</forename><surname>Sun</surname></persName>
							<email>jysun@essex.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Aimin</forename><surname>Zhou</surname></persName>
							<email>amzhou@cs.ecnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Simeon</forename><surname>Keates</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shengbin</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Engineering and Science</orgName>
								<orgName type="institution">University of Greenwich</orgName>
								<address>
									<postCode>ME4 4TB</postCode>
									<settlement>Kent</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">National Engineering Research Centre for E-Learning</orgName>
								<orgName type="institution" key="instit2">Huazhong Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneous Bayesian Clustering and Feature Selection Through Student&apos;s t Mixtures Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian inference</term>
					<term>feature selection</term>
					<term>robust clustering</term>
					<term>tree-structured variational Bayes (VB)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we proposed a generative model for feature selection under the unsupervised learning context. The model assumes that data are independently and identically sampled from a finite mixture of Student's t distributions, which can reduce the sensitiveness to outliers. Latent random variables that represent the features' salience are included in the model for the indication of the relevance of features. As a result, the model is expected to simultaneously realize clustering, feature selection, and outlier detection. Inference is carried out by a tree-structured variational Bayes algorithm. Full Bayesian treatment is adopted in the model to realize automatic model selection. Controlled experimental studies showed that the developed model is capable of modeling the data set with outliers accurately. Furthermore, experiment results showed that the developed algorithm compares favorably against existing unsupervised probability model-based Bayesian feature selection algorithms on artificial and real data sets. Moreover, the application of the developed algorithm on real leukemia gene expression data indicated that it is able to identify the discriminating genes successfully.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of features (which is usually referred to as feature extraction), such as in principal component analysis and independent component analysis.</p><p>Existing feature selection algorithm can be categorized as supervised feature selection (on data with full class labels) <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[9]</ref>, unsupervised feature selection (on data without class labels) <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b14">[15]</ref>, and semisupervised feature selection (on data with partial labels) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Feature selection in unsupervised context is considered to be more difficult than the other two cases, since there is no target information available for training. The selected informative features must greatly preserve the distribution and the manifold structure of the data space. In this paper, we focused on unsupervised feature selection.</p><p>Various feature selection methods for unsupervised learning have been developed, which can be categorized according to different feature selection criteria. Criteria scores, such as Laplacian score <ref type="bibr" target="#b17">[18]</ref>, eigenvalue sensitive criteria <ref type="bibr" target="#b18">[19]</ref>, information entropy <ref type="bibr" target="#b19">[20]</ref>, and correlation <ref type="bibr" target="#b20">[21]</ref>, have been proposed. In <ref type="bibr" target="#b21">[22]</ref>, consistency-based feature selection methods were proposed and evaluated. To preserve pairwise similarity along data samples in the original data space, a similarity preserving feature selection framework is proposed in <ref type="bibr" target="#b10">[11]</ref>. Local learning-based feature selection methods <ref type="bibr" target="#b12">[13]</ref> have been extensively studied recently. For examples, in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref>, subspace learning based on nonnegative matrix factorization is developed, where the loading matrix is penalized by L 2 and/or L 1 norms. Moreover, L 2 , L 1 , and L 2,1 -norms have been widely applied in various feature selection methods, such as in <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, a global and local structure preservation framework that integrates global pairwise sample similarity and local geometric data structure is proposed for feature selection. In <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>, spectral learning aiming to preserve the underlying manifold structure is applied for selecting proper features. In <ref type="bibr" target="#b31">[32]</ref>, embedding learning and sparse regression are jointly applied to perform feature selection. A discrimination analysis based on a property of Fourier transform of the data density distribution is applied for feature selection via optic diffraction principle <ref type="bibr" target="#b9">[10]</ref>. A theoretically optimal criterion, namely, the discriminative optimal criterion, has been developed for feature selection in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Apart from these mentioned algorithms, clustering (which aims to discover data structure) can also be used as a criterion. Intuitively, informative feature subsets that greatly preserve the sample data distribution should vary at different clusters. In the wrapper method proposed in <ref type="bibr" target="#b3">[4]</ref>, a clustering algorithm is used to evaluate the candidate feature subsets. The performance of the wrapper method highly depends on the employed clustering algorithms. Alternatively, clustering and feature selection are embedded together with a proper objective function. Subset features can be obtained by optimizing the objective function. It is well acknowledged that the choosing of feature subsets and the clustering estimation (including the cluster statistics and the optimal number of components) are highly dependent problem <ref type="bibr" target="#b33">[34]</ref>. This clearly suggests that the two problems should be considered simultaneously.</p><p>Most of clustering-based feature selection methods were developed on finite Gaussian mixtures. Carbonetto et al. <ref type="bibr" target="#b34">[35]</ref> proposed a Bayesian shrinkage approach where shrinkage hyperpriors are placed over the component means. The shrinkage hyperpriors can lead to automatic feature selection. Pan et al. <ref type="bibr" target="#b35">[36]</ref> proposed a penalized likelihood approach where a L 1 penalty is imposed on the cluster means. The proposed approach can automatically realize feature selection through thresholding and model selection through the BIC criterion. Law et al. <ref type="bibr" target="#b36">[37]</ref> defined the saliency of feature as a probability, which is to quantify whether the data distribution with respect to the saliency features can be sufficiently represented. They proposed to fit the Gaussian mixture model to the sample data distribution using the EM algorithm, while the MML criterion is employed for model selection. Moreover, a Bayesian treatment to the finite Gaussian mixture model that benefits from automatic model selection was proposed in <ref type="bibr" target="#b33">[34]</ref>. Li et al. <ref type="bibr" target="#b37">[38]</ref> improved their work by utilizing "localized" feature saliency to address the local intrinsic property of data.</p><p>Outliers or scattered objects exist elsewhere in real data sets. As well known, Gaussian mixture models are not able to deal with outliers properly. The outliers, if exist, should seriously deteriorate the performances of Gaussian-based clustering algorithms. Moreover, the presence of outliers could also lead to selecting a false model complexity, and make the optimal selection of a subset of informative features get much more difficult. Therefore, previous clustering-based feature selection methods cannot be expected to perform well on data with outliers. It is thus indispensable to propose a principled approach to realize the selection of the most informative features and the improvement on the clustering performance, while eliminating the bad effect of outlying data. This motives us to propose a finite mixture model that is able to deal with outliers; and to develop a Bayesian inference algorithm that can carry out unsupervised clustering, feature selection, and outlier detection simultaneously.</p><p>Specifically, in this paper, we propose a hierarchical latent variable model to address the three tasks. First of all, it has been a common practice to adopt heavy-tailed distributions for handling outlier data in the literature. The Student's t distribution is such a heavy-tail distribution, and has been widely used <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In our model, we adopt a finite mixture of Student's t distributions as the backbone. Fig. <ref type="figure" target="#fig_0">1</ref> shows the difference between a Student's t distribution and a Gaussian distribution with the same mean and variance, but with different parameters (ν, also called the degree of freedom). There are other heavy-tail distribution is available, such as the Laplace distribution and the Pearson type-VII distribution <ref type="bibr" target="#b39">[40]</ref>, which can also be adopted for handling outliers. Note that the Student's t is a scalar mixture of Gaussians. This property makes the Student's t distribution convenient for inference, and hence popular for outlier detection.</p><p>Second, regarding feature selection, we propose to use a localized feature saliency similar to the approach developed in <ref type="bibr" target="#b37">[38]</ref>. The feature saliency characterizes the importance of the feature and can be used as criterion for the selection of the most informative features. Localization of the feature saliency addresses the cluster effect on relevant feature subsets. Finally, to carry out model selection, we adopt a full Bayesian treatment to the model, where proper prior distributions are assumed for the parameters, including the number of clusters, the mixing proportions, and the parameters of the cluster components. To carry out inference, we resort to a tree-structured variational Bayesian (VB), since the likelihood function of the training data with respect to the proposed model is not tractable.</p><p>In the rest of this paper, Section II presents the proposed latent variable model. The inference is presented in Section III-A-III-F, in which the tree-structured VB algorithm is described. Moreover, the interpretation of the model is described in Section III-G. The experimental study is presented in Section IV. In this paper, controlled experiments were first carried out to justify the out performance of the developed models over the model using Gaussian distributions on synthetic data sets and another state-of-the-art feature selection algorithms. Then, the developed algorithm was compared with them on some real data sets. Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MODEL</head><p>In this section, we present the proposed hierarchical latent variable model step by step starting from the introduction of saliency features to variables that are modeled to follow the mixture of Student's t. To make the description clear, Table <ref type="table" target="#tab_0">I</ref> shows the notations used.</p><p>Suppose that a vector of random variable Y = (Y 1 , . . . , Y d ) ∈ R d , where d is the dimensionality of the input data, and denote Y as the th feature. In the sequel, we use y to represent the realization of Y . To represent if a feature is relevant or not, we use a vector of random binary variable</p><formula xml:id="formula_0">= (φ 1 , . . . , φ d ).</formula><p>That is, if φ = 1, we say that the th feature is relevant, and 0 otherwise. To handle outliers, heavy-tailed probability distributions, such as Student's t distribution <ref type="bibr" target="#b40">[41]</ref> or Pearson type-VII distribution <ref type="bibr" target="#b39">[40]</ref> can be used. Taking the features' relevancy into consideration in the Student's t distribution, we result in the following model:</p><formula xml:id="formula_1">p(y| ; ) = d =1 [S t (y |θ )] φ [S t (y |γ )] 1-φ<label>(1)</label></formula><p>where S t represents the Student's t distribution.</p><p>To realize clustering, a finite mixture of p(y| ; ) can be applied. That is</p><formula xml:id="formula_2">p(y| ; ) = K j =1 π j p(y| , j )</formula><p>where = { j } and j = {θ j , γ j , 1 ≤ ≤ d} are the parameters of the cluster components. To this end, we can introduce a discrete latent variable z to specify which cluster that the data belongs to, and a Bernoulli prior over with parameter β to characterize the importance of features. To account for the case that in different clusters, features might have different relevance, we propose to impose that depends on the latent variable z. As a result, β j , 1 ≤ j ≤ K , 1 ≤ ≤ d are the parameters associated with the Bernoulli prior over depending on z, which are called feature saliency <ref type="bibr" target="#b36">[37]</ref>. Mathematically, the model can be written hierarchically for a set of training data y n , 1 ≤ n ≤ N as follows:</p><formula xml:id="formula_3">p(y n | n , z n ) = K j =1 d =1 [S t (y n |θ j )] φ n [S t (y n |γ j )] 1-φ n δ zn , j p( n |z n , β) = K j =1 d =1 β φ n j (1 -β j ) 1-φ n δ zn , j</formula><p>where n and z n are latent variables associated with each data point y n , and δ z n , j is the Kronecker delta function. Note that a similar idea has been implemented in <ref type="bibr" target="#b37">[38]</ref> which is termed as "localized feature saliency." The difference between their work and our work is that we impose dependencies between n to z n and y n , while in <ref type="bibr" target="#b37">[38]</ref>, the dependence is implemented by introducing different feature saliency variables in different classes (which results in φ nj for 1 ≤ j ≤ K rather than just φ n as in our implementation). Note that the Student's t distribution can be written as a convolution of a Gaussian and a gamma distribution as follows:</p><formula xml:id="formula_4">S t (y|θ) = N (y|μ, σ u) G u| ν 2 , ν 2 du</formula><p>where σ is the precision (inverse variance) and θ = (μ, σ, ν) is the parameters, and</p><formula xml:id="formula_5">G(x|a, b) = b a x a-1 exp(-bx) (a) .</formula><p>If we introduce u n = (u n1 , . . . , u nd ) and v n = (v n1 , . . . , v nd ) as latent variables for the Student's t components with and without relevant features, respectively, we can obtain a distribution of p(y n | n , u n , v n , z n ) as follows:</p><formula xml:id="formula_6">p(y n | n , u n , v n , z n ) = K j =1 ⎡ ⎣ d =1 N (y n |μ j , u n σ j ) φ n × N (y n |χ j , v n τ j ) 1-φ n ⎤ ⎦ δ zn , j</formula><p>.</p><p>The hierarchical latent variable model is completed by introducing conjugate prior over z n , u n and v n as follows:</p><formula xml:id="formula_7">p(u n |z n ) = K j =1 d =1 G u n ν j 2 , ν j 2 δ zn , j p(v n |z n ) = K j =1 d =1 G v n γ j 2 , γ j 2 δ zn , j p(z n ) = K j =1 π δ zn , j j .</formula><p>To realize model selection, i.e., selecting the optimal number of components, we adopt the full Bayesian treatment, which means that we need to specify conjugate priors for the parameters (i.e., ). The conjugate priors associated with the model parameters are as follows: </p><formula xml:id="formula_8">p(β) = K j =1 d =1 B(β j |κ 1 , κ 2 ) p(σ ) = j p(σ j ) = j G σ j η 0 2 , ω 0 2 p(μ) = j p(μ j ) = j N (μ j |m 0 , λ 0 ) p(χ) = j p(χ j ) = j N (χ j |m 0 , λ 0 ) p(τ ) = j p(τ j ) = j G τ j η 0 2 , ω 0 2 p(π) = D(π|α 0 ) (2)</formula><formula xml:id="formula_9">D(π|α 0 ) = K k=1 α 0 k K k=1 α 0 k K k=1 π α 0 k -1 k</formula><p>is the Dirichlet distribution. The parameters in the priors, including κ 1 , κ 2 , η 0 , ω 0 , m 0 , λ 0 , and α 0 are considered as hyperparameters. Note that in the priors, we assume the same hyperparameters for σ j and τ j and for μ j and χ j , respectively. The resultant model can be depicted using the plate diagram shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In a rectangle of the plate diagram, the bold typeset indicates the dimensions of the circled variables. For example, Kd means that there are K × d variables of ν j , 1 ≤ j ≤ K , 1 ≤ ≤ N. The arrows in the diagram indicate the variable dependencies, e.g., the arrow pointing to U from Z means that U depends on Z .</p><p>In the following, we use n, , and j to denote the index of the data point, the features, and the mixing component. We omit the typeset of parameters in the formula.</p><p>In the proposed model, bear in mind that the joint probability distribution is written as <ref type="figure">σ,</ref><ref type="figure">χ,</ref><ref type="figure">τ,</ref><ref type="figure">π,</ref><ref type="figure">β,</ref><ref type="figure">ν</ref>, γ } and it can be factorized as</p><formula xml:id="formula_10">p(y n , u n , v n , n , z n | ) where = {μ,</formula><formula xml:id="formula_11">p(y n | n , u n , v n , z n ) p( n |z n ) p(u n |z n ) p(v n |z n ) p(z n )</formula><p>and are fully factorized over the dimensions. In the sequel, we denote the latent variables as</p><formula xml:id="formula_12">h n = {u n , v n , z n , n , 1 ≤ n ≤ N}.</formula><p>According to the model, the complete likelihood of a data y n can be written as follows:</p><formula xml:id="formula_13">L C (y n , h n , ) = p(y n , h n | ) p( )<label>(3)</label></formula><p>where p( ) = p(μ) p(σ ) p(β) p(π) p(χ) p(τ ). Note that we assume the same hyperparameters of the prior distributions corresponding to the parameters with respect to all the components. We do not assume any priors for ν and γ , since there are no conjugate priors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INFERENCE</head><p>In this section, we first define some notations as listed in Table <ref type="table" target="#tab_0">II</ref>. These notations will be used in the inference. A brief introduction to the VB method is given, while the detailed inference follows. The algorithm is then summarized and interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brief Introduction to VB</head><p>The integration of p(y n , u n , v n , n , z n | ) p( ) over the latent variables and the parameters is not tractable. Therefore, exact inference is impossible. We adopt the VB algorithm for model inference <ref type="bibr" target="#b41">[42]</ref>. To apply the VB algorithm, the evidence, obtained by integrating out the latent variables (denoted by H) and the parameters (denoted by ) given a model structure M, is approximated by introducing an auxiliary distribution q. The lower bound to the evidence is as follows:</p><formula xml:id="formula_14">log p(X|M) ≥ H q(H, ) log p(Y, H, |M) q(H, ) dHd = log p(Y, H, |M) q -log q(H, ) q F (q(H), q( ), Y, M)<label>(4)</label></formula><p>where p(Y, H, |M) is the complete data likelihood and q(H, ) = q(H)q( ) is the auxiliary posterior distribution, F (q(H), q( ), Y, M) is called the free energy. In the equations, we use • q to denote the expectation with respect to q.</p><p>It is obvious that maximizing the evidence is equivalent to maximizing the free energy F .</p><p>To maximize the free energy, we apply coordinate ascent search as adopted in <ref type="bibr" target="#b42">[43]</ref>. Applying the coordinate ascent search, the auxiliary distributions of latent variable H and parameters are optimized alternatively as follows:</p><formula xml:id="formula_15">q (t +1) (H) = arg max q(H) F (q(H), q t ( ), Y, M) q (t +1) ( ) = arg max q( ) F (q t +1 (H), q( ), Y, M).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tree-Like Factorization of the Random Variables</head><p>We propose a tree-like factorization over the latent variables for the auxiliary posteriors (i.e., q(H)). Tree-like structural factorization in VB has been shown to be superior over the full factorization scheme <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The factorization can be summarized as follows:</p><formula xml:id="formula_16">q(h n , π, {β j }, {μ j , σ j }, {χ j , τ j }) = q(u n |z n )q( n |z n )q(v n |z n )q(z n )</formula><p>× q(π)q({β j })q {μ j , σ j } q {χ j , τ j } .</p><p>The tree-like factorization is reflected on the dependences between u n , v n , n , and z n . Specifically, due to the full factorization over the features and the conjugate prior we used, it can be seen that</p><formula xml:id="formula_17">q(h n , ) = q(z n ) n q(v n |z n )q(u n |z n )q(φ n |z n )</formula><p>× q(π) j q(χ j )q(τ j )q(β j )q(μ j )q(σ j ).</p><p>The auxiliary posteriors of the latent variables and the parameters can be obtained by maximizing the free energy associated with the proposed model</p><formula xml:id="formula_18">F = log L C (y n , h n , ) q -log q(h n ) q (5)</formula><p>where • q is the expectation with respect to the auxiliary posterior q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auxiliary Posteriors of the Latent Variables</head><p>The free energy associated with the auxiliary posterior q(u n |z n ) can be read as follows:</p><formula xml:id="formula_19">F = log[ p(y n , h n )] -log q(u n |z n ) q .</formula><p>According to the KKT condition, and using the Lagrange multiplier, we obtain (see the Appendix for details)</p><formula xml:id="formula_20">q(u n z n ) ∝ d =1 exp log[ p(y n |u n , z n ) p(u n |z n )] q . This shows that q(u n |z n ) = d =1 q(u n |z n ). Through mathematical manipulation, we can obtain q(u n |z n = j ) = G(u n | ānj , bnj )<label>(6)</label></formula><p>where</p><formula xml:id="formula_21">ānj = ν j + 1 2 ; bnj = ν j + (y n -μ j ) 2 σ j 2 .</formula><p>Similarly to the above calculation, the other posteriors can be computed. We find that the posterior of the latent variable v n , i.e., q(v n |z n ), is of the following form:</p><formula xml:id="formula_22">q(v n ) = G(v n |s nj , tnj )<label>(7)</label></formula><p>where</p><formula xml:id="formula_23">snj = γ j + 1 2 ; tn = γ j + (y n -χ j ) 2 τ j 2 .</formula><p>Note that (y n -μ j ) 2 = (y n -μ j ) 2 + σ j and (y n -χ j ) 2 = (y n -χ j ) 2 + ς j , where σ j and ς j are the standard deviations of the posterior q(μ j ) and q(χ j ), respectively. If we let</p><formula xml:id="formula_24">A = [ log p(y n |u n , j ) + log p(u n | j ) ] + log β j -log q(u n | j ) and B = [ log p(y n |v n , j ) + log p(v n | j ) ] + log(1 -β j ) -log q(v n | j )</formula><p>then q(φ n = 1| j ) can be written as</p><formula xml:id="formula_25">q(φ n = 1| j ) = exp{ A} exp{ A} + exp{B}<label>(8)</label></formula><p>and q(φ n = 0| j ) = 1q(φ n = 1| j ).</p><p>If we define the quantity</p><formula xml:id="formula_26">R n, j = φ n 1 j log p(y n |u n , j ) + φ n 0 j log p(y n |v n , j ) + log π j + φ n 1 j log p(u n | j ) + φ n 0 j log p(v n | j ) + φ n 1 j log β j + φ n 0 j log(1 -β j ) - φ n 1 j log q(u n | j ) + φ n 0 j log q(v n | j ) .</formula><p>Then, the responsibility q(z n = j ) can be calculated as follows:</p><formula xml:id="formula_27">q(z n = j ) = exp{R n, j } k exp{R n,k } . (<label>9</label></formula><formula xml:id="formula_28">)</formula><p>In the sequel, we use z n j to denote q(z n = j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Auxiliary Posteriors of the Parameters</head><p>The posterior of the mixing proportion π is</p><formula xml:id="formula_29">q(π) = D(π| α)<label>(10)</label></formula><p>where α j = n q(z n = j ) + α 0 and α0 = j α j and log π j = ( α j ) -( α0 ).</p><p>The posterior of the feature saliency β is</p><formula xml:id="formula_30">q(β) = j q(β ) = j B(β j | κ1 j , κ2 j ) (11)</formula><p>where κ1 j = κ 1 + n φ n 1 j z n j and κ2 j = κ 2 + n φ n 0 j z n j . The expectation log β j and log(1 -β j ) as used in the calculation of q( n | j ) can be obtained as</p><formula xml:id="formula_31">log β j = ψ( κ1 j ) -ψ( κ1 j + κ2 j ) log(1 -β j ) = ψ( κ2 j ) -ψ( κ1 j + κ2 j ).</formula><p>The posterior of variance σ j is</p><formula xml:id="formula_32">q(σ j ) = q(σ j ) = G(σ j | η j , ω j ) (<label>12</label></formula><formula xml:id="formula_33">)</formula><p>where</p><formula xml:id="formula_34">η j = η 0 + n z n j φ n 1 j 2 ω j = ω 0 + n z n j φ n 1 j (y n -μ j ) 2 u n j 2 .</formula><p>The posterior of variance of the common distribution τ is</p><formula xml:id="formula_35">q(τ ) = j q(τ j ) = G(τ j | ψ j , ξ j ) (<label>13</label></formula><formula xml:id="formula_36">)</formula><p>where</p><formula xml:id="formula_37">ψ j = η 0 + n z n j φ n 0 j 2 ξ j = ω 0 + n z n j φ n 0 j (y n -χ j ) 2 v n j 2 .</formula><p>The posterior of μ j is</p><formula xml:id="formula_38">q(μ j ) = q(μ j ) = N (μ j | μ j , σ j ) (<label>14</label></formula><formula xml:id="formula_39">)</formula><p>where</p><formula xml:id="formula_40">σ j = σ j n z n j φ n 1 j u n j + λ 0 μ j = σ -1 j σ j n z n j φ n 1 j u n j y n + λ 0 μ 0 .</formula><p>The posterior of χ is</p><formula xml:id="formula_41">q(χ) = j q(χ j ) = j N (χ j | ¯ j , ς j ) (<label>15</label></formula><formula xml:id="formula_42">)</formula><p>where</p><formula xml:id="formula_43">ς j = τ j n z n j φ n 0 j v n j + λ 0 ¯ j = ς -1 j τ j n z n j φ n 0 j v n j y n + λ 0 μ 0 .</formula><p>The degree of freedom ν j , 1 ≤ j ≤ d, γ j , 1 ≤ ≤ d can be obtained by solving the following nonlinear equations, where log v n j and log u n j denote the expectations of log q(v n | j ) and log q(u n | j ), respectively:</p><formula xml:id="formula_44">n z n j φ n 1 j × 1 + log ν j 2 + log u n j -u n j -ψ ν j 2 = 0 n, j z n j φ n 0 j × log v n -v n j + 1 + log γ j 2 -ψ γ j 2 = 0</formula><p>where ψ(•) is the digamma function. Update q(u n |z n ) according to <ref type="bibr" target="#b5">(6)</ref> 4:</p><p>Update q(v n ) according to <ref type="bibr" target="#b6">(7)</ref> 5:</p><p>Update q( n |z n ) according to <ref type="bibr" target="#b7">(8)</ref> 6:</p><p>Update q(z n ) according to <ref type="bibr" target="#b8">(9)</ref> 7:</p><p>VB M-Step 8:</p><p>Update q(π) according to <ref type="bibr" target="#b9">(10)</ref> 9:</p><p>Update q(β) according to <ref type="bibr" target="#b10">(11)</ref> 10:</p><p>Update q(σ j ), 1 ≤ j ≤ K according to <ref type="bibr" target="#b11">(12)</ref> 11:</p><p>Update q(τ ) according to <ref type="bibr" target="#b12">(13)</ref> 12:</p><p>Update q(μ j ), 1 ≤ j ≤ K according to <ref type="bibr" target="#b13">(14)</ref> 13:</p><p>Update q(ξ ) according to <ref type="bibr" target="#b14">(15)</ref> 14:</p><p>Calculate the log-likelihood bound using ( <ref type="formula" target="#formula_46">16</ref>) 15: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Log-Likelihood Bound</head><p>The optimization process can be monitored by the loglikelihood bound as shown in <ref type="bibr" target="#b4">(5)</ref>, which can be evaluated in the following. The evaluation of the expectations of the loglikelihood bound (i.e., the free energy) is summarized in the Appendix</p><formula xml:id="formula_45">F = n, j z n j φ n 1 j log[ p(y n |u n , j ) p(u n | j )] + n, j z n j φ n 0 j log[ p(y n |v n , j ) p(v n | j )] + n, j z n j log p(φ n |β j ) j + n, j</formula><p>z n j log π j + j log p(μ j ) + log p(σ j )log q(μ j )log q(σ j )</p><formula xml:id="formula_46">+ log p(χ) + log p(τ ) -log q(χ) -log q(τ ) + log p(π) -log q(π) + log p(β) -log q(β) - n, j z n j log q(u n | j ) j - n log q(v n | j ) - n, j z n j log q(φ n | j ) j - nj z n j log z n j . (<label>16</label></formula><formula xml:id="formula_47">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Algorithm</head><p>The developed VB algorithm can be summarized in Algorithm 1. To start the run, in the beginning, a large number of clusters K are given. The K -mean clustering is carried out, while the resulting centroids are used as the initial value for q(μ). Note also that the adopted Bayesian framework allows us to realize model selection, i.e., to find the optimal number of clusters. Initializing a large K cluster number, some clusters that do not have enough evidence will be pruned during the optimization process. The automatic pruning can be observed in the demo, as shown in Fig. <ref type="figure">3</ref>. Fig. <ref type="figure">3</ref>. Typical run of the developed algorithm on the example data set, while the black circles represent q(μ 1 |k), and the red circles denote q(μ 0 ). The first plot shows the data set on the first two dimensions, while the last plot shows the estimation of the third and fourth dimensions.</p><p>Since the VB algorithm is proven to be monotonically increasing, it is thus able to terminate the algorithm if there is a small difference ( in line 1) between consecutive iterations. In our implementation, we set = 1.0 -7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Interpreting the Model</head><p>Considering the time complexity of the algorithm, per iteration, computing the parameters of the posteriors of u n , z n , and n are O(Nd K ), while for q(v n |z n ), the time complexity is O(Nd). Therefore, the total time complexity is of O(N K d).</p><p>As claimed, the proposed model is supposed to deal with outliers, and to find most informative features. To detect outliers, the weighted expectation of the posteriors of u n and v n can be used as the outlier criterion. That is, if we define</p><formula xml:id="formula_48">c n = j z n j φ n 1 j ānj bnj + φ n 0 j snj tnj</formula><p>then the smaller the value of c n with respect to y n , the higher chance that the datum is an outlier.</p><p>As stated in the model, the expectation of the feature saliency variable β j , 1 ≤ ≤ d can be applied to show the informative degree of the features for each cluster, which can be obtained as follows:</p><formula xml:id="formula_49">β j = κ1 j κ1 j + κ2 j .</formula><p>The higher the β j value, the more important of feature in class j .</p><p>For the overall feature saliency, we can use the following quantity to specify:</p><formula xml:id="formula_50">ς = j π j q β j = j α j k αk β j</formula><p>which is a weighted average over the feature saliency for each cluster. The higher the ς value, the more relevant the feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthetic Data</head><p>In this section, we justify the developed model and the tree-like VB algorithm using controlled experiments. Synthetic data sets are generated that are able to accommodate the data characteristics for the justification. The proposed model and the algorithm were compared with the semi-Bayesian feature selection model and algorithm, called varFnMS <ref type="bibr" target="#b33">[34]</ref>, in which a finite mixture of Gaussian is adopted and a full-factorized VB is applied.</p><p>Synthetic data are generated by first sampling a set of data points from four well-separated bivariate clusters. The centers and the variance-covariance matrices are</p><formula xml:id="formula_51">[0 3] , [1 9] , [<label>6 4</label></formula><p>] , [7 10] , and an identity matrix. Eight "noisy" features [sampled from N (0, 1)] are then appended to this data, resulting in a 10-D patterns. 800 data points are generated, and a set of outliers uniformly sampled from [-10 30] 10 are added to the data set. Various percentages of outliers are added to the main data sets to test the performance of the algorithm on outlier detection.</p><p>The proposed algorithm was carried out for ten times with an initial cluster number K = 10. The K -means clustering algorithm is used to initialize the mean of the posterior q(μ j ),  and the feature saliency variable is initialized to be 0.5. The hyperparameters κ 1 , κ 2 , λ 0 , and α 0 are set to be 10 -5 , and m 0 is set to be the mean of all data. The algorithm terminates when the difference of log-likelihood bound is less than 10 -7 .</p><p>Fig. <ref type="figure">3</ref> shows a typical run of the developed algorithm, while the estimated mean and covariance of q(μ) in the first break two-dimension is shown at certain iterations. From the figure, we can see that the developed algorithm groups the data accurately. Moreover, it can be seen that unnecessary components are pruned automatically during the optimization process. The last plot shows the data in the third and fourth variables. The red circle demonstrates the contour of the posterior q(χ) at the third and fourth features. Fig. <ref type="figure" target="#fig_2">4</ref>(a) shows the results obtained by varFnMS at the first and second features. From the figure, it can be seen that varFnMS is not able to eliminate the effects of the outliers, and the number of clusters has not been estimated accurately.</p><p>To test the outlier detection performance, the area under curve (AUC) values obtained through the ROC analysis can be used. The higher the AUC values, the better the performance of outlier detection. Fig. <ref type="figure" target="#fig_2">4</ref>(b) shows the obtained AUC values with standard deviation by the developed algorithm for different percentages of outliers in ten runs. Unfortunately, no statistics can be derived by varFnMS for the purpose of outlier detection. From the figure, it can be observed that the developed algorithm is able to pick outliers successfully. This shows that the proposed algorithm is able to simultaneously pickup outliers from the data, and discover clusters accurately.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows the feature saliency retrieved by the proposed algorithm and varFnMS. From the figure, we can see that the saliency of the noisy variables (Y 3 -Y 8 ) obtained by the proposed algorithm is closer to the ground truth than that of the semi-Bayesian algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Real Data Sets</head><p>In this section, we used the "multiple feature database" <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b45">[46]</ref>, which consists of features of handwritten numerals ("0"-"9") extracted from a collection of Dutch utility maps. There are a total of 2000 images with 200 for each numerals. Numerals are represented in different feature sets. We used the same three data sets as in <ref type="bibr" target="#b33">[34]</ref>, that is, the Zernike moments (47 features), the Fourier coefficients (76 features),   <ref type="table" target="#tab_3">III</ref>, where the components were initialized to be 30 and 50.</p><p>From Table <ref type="table" target="#tab_3">III</ref>, we can see that our algorithm outperforms varFnMS and FnMS <ref type="bibr" target="#b45">[46]</ref> in terms of classification error. It can be seen that the developed algorithm uses less number of components than that of varFnMS, which is closer  to the true number of components. This suggests that the developed algorithm performs better in terms of recovering the true parameters. Fig. <ref type="figure" target="#fig_4">6</ref> shows the error bar plots of the saliencies obtained for the proposed model initialized with 30 components. As an comparison, Fig. <ref type="figure" target="#fig_5">7</ref> showed the saliencies by using varFnMS with the same experimental settings as the developed algorithm. From the figure, we can see that on the Fourier coefficient data set and the profile correlation data set, the feature saliency obtained by the developed algorithm has similar trends as that of varFnMS, but of smaller variances. On the Zernike moments data set, we can see that the variances of the feature saliency revealed by the developed algorithm are much less than those obtained by varFnMS. This shows that the developed algorithm is more robust than that of varFnMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Application on High-Dimensional Gene Expression Data</head><p>In this section, we apply the developed algorithm to a large-scale gene expression data set on leukemia <ref type="bibr" target="#b46">[47]</ref>. The data were obtained through the diagnostic of bone marrow samples from pediatric acute leukemia (ALL) patients corresponding to six prognostically important leukemia subtypes, including 43 T-lineage ALL, 27 E2A-PBX1, 15 BCR-ABL, 79 TEL-AML1, and 20 MLL rearrangements and 64 "hyperdiploid &gt; 50" chromosomes, and containing more than 12 600 probe sets. The resultant data set contains 248 samples, and 12 625 gene expressions. Note that in <ref type="bibr" target="#b46">[47]</ref>, A variety of statistical metrics (including χ 2 and t-statistics) are used to select discriminating genes for the subtypes.</p><p>We apply the developed algorithm to the data set to test if the developed algorithm is able to cluster the data accurately, and to find out the discriminating genes in the subtypes. Table <ref type="table" target="#tab_4">IV</ref> shows the confusion matrix obtained by the developed algorithm given K = 6. From Table <ref type="table" target="#tab_4">IV</ref>, we can see that the developed algorithm agrees with the clustering results in <ref type="bibr" target="#b46">[47]</ref> quite accurately.</p><p>On the other hand, we want to justify whether the feature saliency criterion β j , 1 ≤ ≤ d can be used to discriminate the genes in each class j . <ref type="foot" target="#foot_0">1</ref> Since these values are defined to show the relevance of the features, or in the leukemia clustering context, these values indicate the relevance of the genes to describe the clusters. Thus, it is expected that the feature saliency values obtained by the developed algorithm can also be used to discriminate genes for the cancer subtypes. Here, we use the correlation between the feature saliency and the statistics to evaluate the usefulness of the feature saliency values on discriminate the clusters, which will imply the performance of the developed algorithm.</p><p>To measure the correlation between the feature saliency values and the statistics, we use the Pearson correlation coefficients. Table V lists the average coefficients obtained by running the developed algorithm for ten times. From the table, we can see that the absolute values of these statistics and the feature saliency obtained by the developed algorithm for the genes have fairly strong correlation; the average of the coefficients is as high as 0.851, and no less than 0.613. This implies a coherence between the developed method and the method in <ref type="bibr" target="#b46">[47]</ref> in terms of selecting discriminating genes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Demonstration of the Student t-distribution with different parameters ν = 0.01, 0.1, 1, 2, ∞. Note that the Student t-distribution becomes the Gaussian distribution in case ν = ∞.</figDesc><graphic coords="2,348.67,57.25,177.69,151.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Plate diagram of the proposed hierarchical graphical model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left: typical run of the semi-Bayesian feature selection algorithm, and first and second features are shown. Right: AUC values obtained by the developed algorithm and varFnMS for different percentages of outliers with standard deviations shown.</figDesc><graphic coords="8,308.27,286.97,213.26,168.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Feature saliencies for the synthetic data with 5% percentage of outliers by the proposed algorithm (on the left) and the semi-Bayesian algorithm (on the right). The standard deviations of the ten runs were also shown in the plots.</figDesc><graphic coords="8,89.75,288.41,215.66,167.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Saliencies of different feature sets. (a) Fourier coefficients, (b) Zernike moments, and (c) profile correlations using the developed algorithm for models initialized with 30 components. and profile correlations (216 features). The classification error is used to measure the performance. For each data point, it is assigned to the class with the largest responsibility. The proposed algorithm was run 20 times, where the data set is split into half to create the training and test data set. The estimated classification error and the number of components are summarized in TableIII, where the components were initialized to be 30 and 50.From TableIII, we can see that our algorithm outperforms varFnMS and FnMS<ref type="bibr" target="#b45">[46]</ref> in terms of classification error. It can be seen that the developed algorithm uses less number of components than that of varFnMS, which is closer</figDesc><graphic coords="9,69.65,435.10,209.45,108.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Saliencies of different feature sets. (a) Fourier coefficients, (b) Zernike moments, and (c) profile correlations using varFnMS for models initialized with 30 components; reproduced from [34].TABLE VI EVALUATION OF THE LOG-LIKELIHOOD BOUND</figDesc><graphic coords="10,48.35,58.61,523.10,132.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,79.84,57.29,452.36,272.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NOTATIONS</head><label>I</label><figDesc>USED IN MODELING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1</head><label>1</label><figDesc>Proposed Tree-Like VB Algorithm for Clustering, Feature Selection, and Outlier Detection Require: training data y n , 1 ≤ n ≤ N, a cluster number K ; Ensure: the centroids, the saliency of the features and the outlier criteria; 1: while the free energy F increases less than do</figDesc><table><row><cell>2:</cell><cell>VB E-step</cell></row><row><cell>3:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AVERAGED</head><label>III</label><figDesc>CLASSIFICATION ERROR AND THE NUMBER OF COMPONENTS OBTAINED BY varFnMS AND THE PROPOSED ALGORITHM USING 30 AND 50 INITIAL COMPONENTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV CONFUSION</head><label>IV</label><figDesc>MATRIX OBTAINED BY THE DEVELOPED ALGORITHM ON THE LEUKEMIA DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V CORRELATION</head><label>V</label><figDesc>BETWEEN THE STATISTICS OBTAINED IN<ref type="bibr" target="#b46">[47]</ref> AND THE FEATURE SALIENCIES WITH RESPECT TO THE LEUKEMIA SUBTYPES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that in our method, we use a localized feature saliency rather than a global feature saliency as developed in<ref type="bibr" target="#b33">[34]</ref> and<ref type="bibr" target="#b36">[37]</ref>. This enables us to discriminate genes at different clusters.</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank anonymous reviewers for their constructive and helpful comments.</p></div>
			</div>


			<div type="funding">
<div><p>Sun was supported by the <rs type="funder">National Science Foundation of China</rs> under Grant <rs type="grantNumber">61573279</rs>, Grant <rs type="grantNumber">61573326</rs>, and Grant <rs type="grantNumber">11301494</rs>. The work of <rs type="person">A. Zhou</rs> was supported by <rs type="funder">NSFC</rs> under Grant <rs type="grantNumber">61673180</rs>. The work of S.Liao was supported by the <rs type="funder">Key Science and Technology Project of Wuhan</rs> under Grant <rs type="grantNumber">2014010202010108</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cnPGdew">
					<idno type="grant-number">61573279</idno>
				</org>
				<org type="funding" xml:id="_JEuhXrN">
					<idno type="grant-number">61573326</idno>
				</org>
				<org type="funding" xml:id="_xpvvBHz">
					<idno type="grant-number">11301494</idno>
				</org>
				<org type="funding" xml:id="_YAxJ9rS">
					<idno type="grant-number">61673180</idno>
				</org>
				<org type="funding" xml:id="_9a5SwEs">
					<idno type="grant-number">2014010202010108</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we developed a hierarchical latent variable model for feature selection and robust clustering. A full Bayesian treatment was adopted for model selection. A VB framework was used for inference. To make the inference much efficient, a tree-structured factorization of the auxiliary posteriors for the latent variables was adopted which has been shown better than the widely used full factorization approach. Quantities are proposed to detect outliers and estimate feature saliency. Controlled experiments on synthetic and real data showed that the proposed model is able to realize outlier detection and feature selection more robustly than a semi-Bayesian mixture of Gaussians model. The application of the developed algorithm to real high-dimensional data shows its applicability. In the future, unsupervised feature selection analysis on data with "big dimensionality" (i.e., the feature size is normally far beyond 10k as reviewed in <ref type="bibr" target="#b2">[3]</ref>) is our primary research avenue. The development and the application of feature selection algorithms in broadcasting <ref type="bibr" target="#b47">[48]</ref>, cloud computing <ref type="bibr" target="#b48">[49]</ref>, image processing <ref type="bibr" target="#b49">[50]</ref>, and other areas are another avenue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>In this section, we present the derivation of the posterior distribution with respect to u n , 1 ≤ n ≤ N. The derivation for the other latent variables and parameters is similar.</p><p>To derive q(u n |z n = k) (or briefly q(u n |k)), we need to maximize the free energy with respect to q(u n |k), subject to the constraint q(u n |k)du n = 1. The free energy associated with the auxiliary posterior q(u n |k) can be written as follows:</p><p>Discarding terms that are independent of u n , and using the Lagrange multiplier method, the functional to be maximized is the following:</p><p>Note here the expectation is computed with respect to the probability density functions of the parameters Taking derivatives of F u n |k with respect to q(u n |k) and λ, we have</p><p>If we let</p><p>and equating these to zero, then according to the Karush-Kuhn-Tucker conditions, we have</p><p>then taking integral with respect to q(u n |k) on both sides, we have</p><p>Finally, replacing <ref type="bibr" target="#b18">(19)</ref> into <ref type="bibr" target="#b17">(18)</ref>, we obtain</p><p>Note that the dimensions of the latent variable u n are independent, we can then obtain</p><p>which leads to the posterior presented in the main context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B</head><p>The evaluation of the free energy is presented here. Note that the main evaluation is on the computation of the expectations of the logarithms of the prior distributions for the latent variables [including p(y n |u n , j ), p(u n | j ), p(y n |v n ), p(v n ), p( n |β)]; the parameters [including p(β), p(π), p(σ ), p(μ), p(ξ ), p(τ )]; the posteriors [including q(u n |z n ), q(v n ), q(z n ), q(π), q(β), q(τ ), q(μ), q(σ ), and q(χ)]. The evaluation of these expectations is summarized in Table <ref type="table">VI</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse coding from a Bayesian perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="929" to="939" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic progamming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The emerging &apos;big dimensionality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature selection for unsupervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="845" to="889" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonconvex regularizations for feature selection in ranking with sparse SVM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Laporte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Déjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1118" to="1130" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature selection using a neural framework with controlled redundancy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="49" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FREL: A stable feature selection algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1388" to="1402" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A semidefinite programming based search strategy for feature selection with mutual information measure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Naghibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective discriminative feature selection with nontrivial solution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="796" to="808" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discrimination analysis for unsupervised feature selection via optic diffraction principle</title>
		<author>
			<persName><forename type="first">P</forename><surname>Padungweang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lursinsap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1587" to="1600" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On similarity preserving feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="632" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection via maximum projection and minimum redundancy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature selection for unsupervised learning through local learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph regularized feature selection with data reconstruction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust joint graph sparse coding for unsupervised spectral feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2016.2521602</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative semisupervised feature selection via manifold regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-T</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1033" to="1047" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global and local structure preservation for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1083" to="1095" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eigenvalue sensitive feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th ICML</title>
		<meeting>28th ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hybrid filter/wrapper approach of feature selection using information theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="835" to="846" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: A fast correlation-based filter solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Mach. Learn</title>
		<meeting>20th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Consistency measures for feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arauzo-Azofra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="292" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local learning regularized nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Joint Conf</title>
		<meeting>21st Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature selection and kernel learning for local learning-based clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1547" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature selection, L 1 vs. L 2 regularization, and rotational invariance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Mach. Learn</title>
		<meeting>21st Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2,1 -norm regularized discriminative feature selection for unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Joint Conf</title>
		<meeting>22nd Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint 2,1 -norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using nonnegative spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th AAAI Conf</title>
		<meeting>26th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised feature selection via spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th SIAM Int. Conf. Data Mining</title>
		<meeting>7th SIAM Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral feature selection for supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ICML</title>
		<meeting>24th ICML</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1151" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustering-guided sparse structural learning for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2138" to="2150" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint embedding learning and sparse regression: A framework for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative feature selection by nonparametric Bayes error minimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1422" to="1434" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian feature and model selection for Gaussian mixture models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Constantinopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1013" to="1018" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian feature weighting for unsupervised learning, with application to object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbonetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf</title>
		<meeting>9th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Penalized model-based clustering with application to variable selection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1145" to="1164" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous feature selection and clustering using mixture models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1154" to="1166" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous localized feature selection and model detection of Gaussian mixtures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Recognit. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="953" to="960" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust mixtures in the presence of measurement errors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kabán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raychaudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. Mach. Learn</title>
		<meeting>24th Int. Conf. Mach. Learn<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="847" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust mixture modeling using the Pearson type VII distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2447" to="2454" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<title level="m">Finite Mixture Models</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variational inference for Dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Soc. Bayesian Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="144" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Durham, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A fast algorithm for robust mixtures in the presence of measurement errors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1206" to="1220" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust Bayesian clustering for datasets with repeated measures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garibaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenobi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1504" to="1514" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="38" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Yeoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast motion estimation based on content property for low-complexity h.265/hevc encoder</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Broadcast</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="675" to="684" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Achieving efficient cloud search services: Multi-keyword ranked search over encrypted cloud data supporting parallel computing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image segmentation by generalized hierarchical fuzzy C-means algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byeungwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="961" to="973" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
