<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Semi-Supervised Learning with Auxiliary Deep Generative Models</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">Novo Nordisk Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
							<email>larsma@dtu.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
							<email>casper.sonderby@bio.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
							<email>soren.sonderby@bio.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Semi-Supervised Learning with Auxiliary Deep Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep generative models based upon continuous variational distributions parameterized by deep networks give state-of-the-art performance. In this paper we propose a framework for extending the latent representation with extra auxiliary variables in order to make the variational distribution more expressive for semi-supervised learning. By utilizing the stochasticity of the auxiliary variable we demonstrate how to train discriminative classifiers resulting in state-of-the-art performance within semi-supervised learning exemplified by an 0.96% error on MNIST using 100 labeled data points. Furthermore we observe empirically that using auxiliary variables increases convergence speed suggesting that less expressive variational distributions, not only lead to looser bounds but also slower model training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have recently showed impressive performance on a wide range of tasks. These improvements have been achieved by better algorithms, faster computers and increased availability of large labeled datasets in many areas, e.g. image recognition <ref type="bibr" target="#b4">(Deng et al., 2009)</ref>. In many practical situations it is relatively inexpensive to acquire data but expensive to label it. This makes semisupervised learning attractive. There exist many approaches to performing semi-supervised learning, e.g. transductive SVM (TSVM) <ref type="bibr" target="#b7">(Joachims, 1999)</ref>, EM methods <ref type="bibr" target="#b12">(Nigam et al., 2000)</ref>, graph-based methods <ref type="bibr" target="#b2">(Blum and Chawla, 2001;</ref><ref type="bibr" target="#b20">Zhu et al., 2003;</ref><ref type="bibr" target="#b13">Pitelis et al., 2014)</ref> and deep auto-encoders <ref type="bibr" target="#b17">(Rifai et al., 2011;</ref><ref type="bibr" target="#b14">Ranzato and Szummer, 2008;</ref><ref type="bibr" target="#b19">Weston et al., 2008)</ref>.</p><p>Recently several different (deep) models have significantly improved the performance on semisupervised classification tasks. <ref type="bibr">Kingma et al. (2014)</ref> introduced a deep generative model for semisupervised learning (DGM) by augmenting the auto-encoding variational Bayes (AEVB) model <ref type="bibr" target="#b10">(Kingma, 2013;</ref><ref type="bibr" target="#b16">Rezende et al., 2014)</ref> algorithm with labeled units. <ref type="bibr" target="#b11">Miyato et al. (2015)</ref> introduced an improved semi-supervised learner by applying adversarial training to deep networks. Finally <ref type="bibr" target="#b15">Rasmus et al. (2015)</ref> introduced a generalization of the ladder network <ref type="bibr" target="#b18">(Valpola, 2014)</ref> that has the ability to learn a latent classification variable.</p><p>In this article we introduce the auxiliary deep generative model (ADGM) and apply it to semisupervised learning. In the ADGM, the variational encoder model has an extra set of stochastic variables compared to the generative decoder model. These extra so-called auxiliary variables makes the variational model more flexible and thus able to improve the variational lower bound <ref type="bibr" target="#b0">(Agakov and Barber, 2004)</ref>. The auxiliary variable and the input data is fed into a variational auto-encoder and a discriminative classifier. Empirically we show that the ADGM, (i) obtain state-of-the-art results on semi-supervised classification, (ii) is trainable end-to-end without the need for any pre-training, (iii) have good convergence properties and (iv) that its stochastic auxiliary variable is essential for good discriminative classification.</p><p>2 Methods <ref type="bibr">Kingma et al. (2014)</ref> introduced a probabilistic approach to semi-supervised learning by stacking a generative feature extractor (called M1) and a generative semi-supervised model (M2) into a stacked generative semi-supervised model (M1+M2). M1 is a variational auto-encoder, where the generative model is defined as p θ (z)p θ (x|z) (decoder with parameters θ), with the variational approximation being q φ (z|x) (encoder with parameters φ), as replacement for the intractable posterior p θ (z|x) <ref type="bibr" target="#b10">(Kingma, 2013)</ref>. M2 includes labels y in the generative model: p θ (x|y, z)p θ (z)p θ (y), decoder q φ (z|y, x) and a discriminative classifier, q φ (y|x). The generative model p combining M1 and M2 called M1+M2 is (cf. fig. <ref type="figure" target="#fig_0">1a</ref>):</p><formula xml:id="formula_0">M1 : p θ (x|z 1 ) = f (x; z 1 , θ), (1) M2 : p(y) = Cat(y|π); p(z 2 ) = N (z 2 |0, I); p θ (z 1 |y, z 2 ) = f (z 1 ; y, z 2 , θ) ,<label>(2)</label></formula><p>where f (•) are the decoders and Cat(•) is the multinomial distribution. The corresponding inference model Q is (cf. fig. <ref type="figure" target="#fig_0">1b</ref>): Since latent variables z 2 and y in M2 are marginally independent, the class specific information can be modeled through y and remaining information through z 2 .</p><formula xml:id="formula_1">M1 : q φ (z 1 |x) = N (z 1 |µ φ (x), diag(σ 2 φ (x))),<label>(3)</label></formula><formula xml:id="formula_2">M2 : q φ (z 2 |y, z 1 ) = N (z 2 |µ φ (y, z 1 ), diag(σ 2 φ (y, z 1 ))); q φ (y|z 1 ) = Cat(y|π φ (z 1 )). (<label>4</label></formula><formula xml:id="formula_3">) z 2 y z 1 x (a) P model. z 2 y z 1 x (b) M1+M2 Q model. z y a x (c) P model. z y a x (d) ADGM Q model.</formula><p>However, although both M2 and M1+M2 should be powerful generative models for semi-supervised learning direct application of these models failed to deliver good results in benchmarks. In this contribution we propose an alternative formulation with two sets of stochastic variables that converges end-to-end and achieves state-of-the-art performance. The ADGM has the generative model p defined as p θ (a)p θ (y)p θ (z)p θ (x|y, z) (cf. fig. <ref type="figure" target="#fig_0">1c</ref>), where a, y, z are the auxiliary variable, class label, and latent features, respectively. Learning the posterior distribution is intractable, thus we define the approximation as q φ (a|x)q φ (z|y, x) and a classifier q φ (y|a, x) (cf. fig. <ref type="figure" target="#fig_0">1d</ref>). The distributions of the generative model p are</p><formula xml:id="formula_4">p θ (x|z, y) = f (x; z, y, θ); p(z) = N (z|0, I); p(y) = Cat(y|π); p(a) = N (a|0, I).<label>(5)</label></formula><p>And for the corresponding inference model q</p><formula xml:id="formula_5">q φ (a|x) = N (a|µ φ (x), diag(σ 2 φ (x))), q φ (z|y, x) = N (z|µ φ (y, x), diag(σ 2 φ (y, x))), q φ (y|a, x) = Cat(y|π φ (a, x)) . (6)</formula><p>The key point of the ADGM is that the auxiliary unit a introduces a class specific latent distribution between x and y allowing a more expressive discriminative distribution q(y|a, x). Further the stochasticity of a maps each input into a distribution q(a|x) used for the discriminative classifier, which is richer than a deterministic dependency between x and y <ref type="bibr" target="#b0">(Agakov and Barber, 2004</ref>). Note that it is possible to let a be conditioned on x, y, z in the generative model, but we found that this did not improve the performance. The ADGM use multi-layered perceptrons (MLP) to model q φ (a|x), q φ (z|y, x), q φ (y|a, x) and p θ (x|z, y).</p><p>For Gaussian distributions we apply the reparameterization trick, introduced in <ref type="bibr" target="#b10">(Kingma, 2013)</ref> to backpropagate the error signal through the latent variables. We approximate the expectations by drawing unbiased Monte Carlo (MC) estimates ã ∼ q φ (a|x) and z ∼ q φ (z|y, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Lower Bound</head><p>We optimize the model by maximizing the lower bound on the likelihood. The variational lower bound on the marginal likelihood for a single labeled data point is log p θ (x, y) ≥ E q φ (a,z|x,y) [log p θ (a)p θ (y)p θ (z)p θ (x|y, z)]</p><p>-KL[q φ (a|x)q φ (z|y, x)||p θ (z)p θ (a)p θ (y)] = E q φ (a,z|x,y) [log[p θ (a)p θ (y)p θ (z)p θ (x|y, z)]</p><p>-log[q φ (a|x)q φ (z|y, x)]] = -L(x, y) .</p><p>(7)</p><p>For unlabeled data we further marginalize y:</p><formula xml:id="formula_6">log p θ (x) ≥ y q φ (y|a, x)(-L(x, y)) + H(q φ (y|a, x)) = -U(x),<label>(8)</label></formula><p>where H(•) is the entropy. The discriminative classifier q φ (y|a, x) (6) is included in the objective -U(x, y), but not in -L(x, y). Since the classification loss is not part of the labeled data x l , y l lower bound we introduce:</p><formula xml:id="formula_7">-L l (x l , y l ) = -L(x l , y l ) -α • E q φ (a,z|x l ,y l ) [-log q φ (y l |a, x l )], (<label>9</label></formula><formula xml:id="formula_8">)</formula><p>where α is a weight between generative and discriminative learning. The variational lower bound for labeled x l , y l and unlabeled data x u is</p><formula xml:id="formula_9">J = (x l ,y l ) L l (x l , y l ) + (xu) U(x u ) . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>3 Results</p><p>Table <ref type="table">1</ref> shows that the ADGM outperforms all previously proposed models on the MNIST dataset.</p><p>The model convergence to around 1.5% is fast, and from that point the convergence speed declines (cf. Fig. <ref type="figure" target="#fig_1">2a</ref>). In Fig. <ref type="figure" target="#fig_1">2b</ref> we visualize 10 Gaussian distributed random samples conditioned on each class y. 100 labels AtlasRBF <ref type="bibr" target="#b13">(Pitelis et al., 2014)</ref> 8.10% (±0.95) Deep Generative Model (M1+M2) <ref type="bibr">(Kingma et al., 2014)</ref> 3.33% (±0.14) Virtual Adversarial <ref type="bibr" target="#b11">(Miyato et al., 2015)</ref> 2.12% Ladder <ref type="bibr" target="#b15">(Rasmus et al., 2015)</ref> 1.06% (±0.37) Auxiliary Deep Generative Model (1 MC) 2.25% (± 0.08) Auxiliary Deep Generative Model (10 MC) 0.96% (± 0.02)</p><p>Table <ref type="table">1</ref>: Semi-supervised benchmarks on MNIST for 100 randomly labeled data points. The ADGM was trained by performing 1 and 10 Monte Carlo samples.</p><p>Fig. <ref type="figure">3</ref> shows the information contribution from the auxiliary units a and the latent units z. Like in <ref type="bibr" target="#b3">Burda et al. (2015)</ref>, the number of contributing/activated units in z is quite low ∼ 20. The number of contributing auxiliary units a, on the other hand, is much larger. We speculate that this is due to the upweighting of the discriminative classification in the lower bound. Fig. <ref type="figure" target="#fig_1">2a</ref> shows how the ADGM outperforms a similarly optimized M2 model and an ADGM where the auxiliary unit is deterministic. We found that convergence of the M2 model was highly unstable. The result in Fig. <ref type="figure" target="#fig_1">2a</ref> is the far best achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implentation details</head><p>The ADGM is implemented in Python using Theano <ref type="bibr" target="#b1">(Bastien et al., 2012)</ref> and Lasagne <ref type="bibr" target="#b5">(Dieleman et al., 2015)</ref> libraries<ref type="foot" target="#foot_0">foot_0</ref> . For training, we have used the Adam (Kingma and Ba, 2014) optimization framework with a learning rate of 3e-4 and exponential decay rate for the 1st and 2nd moment at 0.9 and 0.999 respectively. The learning rate was annealed by .75 every 200 epochs. α was defined as 0.05 • N , where N is the number of unlabeled data points. We parametrized the MLPs with either one or two layers of 500 hidden units using rectified linear units. All stochastic layers used 100 linear hidden units to parameterize µ and log(σ 2 ). The weights and biases are initialized using Glorot and Bengio (2010) scheme. We used 1 or 10 MC samples for training and 100 MC samples for evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have shown that making the discriminative distribution more flexible by introducing extra auxiliary variables gives state-of-the-art performance on the 100 labeled examples MNIST benchmark. We are in the progress of extending this to other semi-supervised scenarios. It is also of interest to extend this approach to both fully unsupervised and supervised generative settings. Currently we are combing the proposed framework with the new tighter bound by <ref type="bibr" target="#b3">Burda et al. (2015)</ref>, where a tight bound on p(y, x) may be used directly for classification through p(y|x) ∝ p(y, x).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Probabilistic graphical models of M1+M2 and ADGM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) 100 MC classification errors for MNIST test set on Kingma et al. (2014) M2 trained with 1 MC, ADGM with deterministic auxiliary units trained with 1 MC, ADGM trained with 1 MC and ADGM trained with 10 MC. All models were trained with the same hyperparameters. (b) 100 Gaussian distributed random samples drawn from a 100-dimensional z with y fixed to each class.</figDesc><graphic coords="5,346.59,71.25,152.52,152.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>With 10 Monte Carlo samples the runtime was ∼64 s/epoch. The training converged to around 1.5% classification error in 200 epochs corresponding to 5 hours. The number of samples influences runtime and stability of training. With 1 Monte Carlo sample an epoch takes ∼16 s/epoch but in terms of predictive performance increasing the number of samples helped (cf. Fig. 2a).</figDesc><table><row><cell>We used MNIST as benchmark. In order to make a</cell></row><row><cell>fair comparison with the ladder network, we have</cell></row><row><cell>combined the training set of 50000 examples with</cell></row><row><cell>the validation set of 10000 examples. The test set</cell></row><row><cell>of 10000 remained as is. We used a batch size</cell></row><row><cell>of 200 with the first half of the batch always be-</cell></row><row><cell>ing the 100 labeled samples. The labeled data</cell></row><row><cell>are chosen randomly, but distributed evenly across</cell></row><row><cell>classes. Before each epoch the normalized MNIST</cell></row><row><cell>images were binarized by sampling Bernoulli dis-</cell></row><row><cell>tributions.</cell></row><row><cell>All experiments were carried out on GeForce GTX</cell></row><row><cell>TITAN X GPUs.</cell></row><row><cell>Figure 3: Number of active stochastic units</cell></row><row><cell>for an ADGM trained on MNIST 100 la-</cell></row><row><cell>bels. We compute KL [p(a i )||q(a i |x)] and</cell></row><row><cell>KL [p(z i )||q(z i |x)] for each stochastic unit. A</cell></row><row><cell>number close to zero indicates that q(•|x) ≈</cell></row><row><cell>p(•).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Implementation will be made available in an extension framework to the Lasagne library named Parmesan on https://github.com/larsmaaloee/auxiliary-deep-generative-models.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Durk P. Kingma</rs> for helpful discussions. This research was supported by the <rs type="funder">Novo Nordisk Foundation</rs> and <rs type="funder">NVIDIA Corporation</rs> with the donation of TITAN X and Tesla K40 GPUs.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Auxiliary Variational Method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Mudi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Parui</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3316</biblScope>
			<biblScope unit="page" from="561" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from Labeled and Unlabeled Data Using Graph Mincuts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the 18th International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance Weighted Autoencoders</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<title level="m">Lasagne: First release</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transductive Inference for Text Classification Using Support Vector Machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Machine Learning, ICML &apos;99</title>
		<meeting>the 16th International Conference on Machine Learning, ICML &apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5298</idno>
		<title level="m">Semi-Supervised Learning with Deep Generative Models</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Diederik P;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-Encoding Variational Bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional Smoothing with Virtual Adversarial Training</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Text Classification from Labeled and Unlabeled Documents Using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Machince Learning</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning Using an Unsupervised Atlas</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pitelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8725</biblScope>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning of Compact Document Representations with Deep Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02672</idno>
		<title level="m">Semi-Supervised Learning with Ladder Network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<title level="m">The Manifold Tangent Classifier. In NIPS</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">From neural pca to deep unsupervised learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7783</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
