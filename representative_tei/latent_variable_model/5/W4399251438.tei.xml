<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-12">12 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-12">12 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.19277v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2), the regularized discriminative process (b) retrofits its hierarchical latent space to the joint latent space (Section 5.2.3). Inference networks q and Generation networks p contain neural network parameters θ and ϕ. Black arrows: flows of operations. Red arrows: loss functions. MSE and WFPT stand for Mean Squared Error and Wiener First Passage Time, respectively. The heatmaps represent the probability distributions in the latent spaces. Plasma color maps are for the drift-diffusion variables (z C ∈ R 3 ), while greenery color maps are for residual neural variables (z N ∈ R 32 ). Blue blocks contain µ and σ, which are the parameters of the multivariate Gaussian latent spaces. Gray blocks contain z sampled (∼) from the distributions. The variables x and y represent EEG signals and choice-RTs, respectively. Each trapezoid represents a different convolutional neural network (see Table <ref type="table" target="#tab_11">5</ref>.2 for detailed architectures). . . . . . . . . . . . 5.3 Drift-diffusion single-trial parameter estimations from correct responses of subject s1. The parameters are constrained by the subject priors resulting from a Bayesian MCMC modeling (without EEG data). Scatter plots illustrate the relationship between the parameters and the observed choice-RTs for each trial. The top two rows are posterior inferences from neural signals, while the bottom two are from behaviors. The left column shows the drift-rate (δ) estimates, the middle column shows boundary (α) estimates, and the right column presents non-decision time (ndt) estimates. The correlations between the choice-RTs and the inferred DDM parameters are consistent with what is expected. On top of each panel are the Spearman correlation coefficients (ρ). The covariances of the inferred parameters are indicated by circles, which correspond to contours having one standard deviation. For clarity, each circle is magnified 300 times. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Performance of the model in reconstructing 98 EEG channels of subject s1 by averaging ≈ 800 predicted EEG trials from ≈ 800 choice-RTs in the test set. Time point zero denotes the time point of stimulus onset. The first row displays the original (blue) and generated (orange) trial-averaged EEG data at the pooled electrodes. The x-axis denotes the time in milliseconds from stimulus onset, and the y-axis denotes the signal amplitude. The second, third, and fourth rows are (left) frequency spectra and (right) EEG signals averaged over all test choice-RT trials (≈ 800/3 per condition). The signals on the right are low-pass filtered at 15 Hz for clarity of N200 peaks. Each colored line corresponds to one reconstructed EEG channel. In low-noise conditions, the spectra show a strong peak at the Gabor flicker frequency of 30 Hz, and the ERP waveform shows a shorter N200 latency and larger peak amplitude. Under high-noise conditions, the spectra show a strong peak at the noise flicker frequency of 40 Hz, and the ERP waveform shows a longer N200 latency and a smaller peak amplitude. . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi 5.6 Performance of the model in reconstructing single-trial N200 peaks from choice-RTs in four subjects. The dotted lines are references to the original data. The distributions of (left) single-trial N200 peak latencies across three noise conditions and (right) the N200 peak amplitude statistics are shown. Single-trial observations of the peak latency of N200 are found using the SVD method <ref type="bibr" target="#b55">(Nunez et al., 2019)</ref> for each subject and noise condition. . . . . . . . . . . . 5.8 Sensitivity analysis of choice-RTs and latent drift-diffusion parameters on EEG signal generation in four subjects. The left column presents the effects of choice-RTs on the output neural signals. The blue bars represent the power at 30 Hz, while the red bars represent the power at 40 Hz. The orange bars show the N200 latencies. The middle column shows the changes in the single-trial N200 distribution w.r.t to hypothetical changes in the cognitive parameters. The yellow distribution represents the reference data, while the blue and red ones correspond to modified parameter settings that decrease or increase the N200 latencies, respectively. The modification in subject s4 (ndt ± 0.05, δ± 0.3) is different from other subjects. The right column characterizes the changes in 30 Hz and 40 Hz peaks w.r.t to the changes in the same cognitive parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.9 Drift-diffusion parameter estimates from neural signals in a simulation of trial-level choice RTs and EEG signals. The top panels show the overlap between the recovered and the original distributions of trial-specific drift-rate and NDT. The reference values for the drift rate and NDT are drawn from the normal distributions N (1.5, 0.2) and N (0.3, 0.05), respectively. The bottom scatter plots illustrate the relationship between the recovered parameters and the original parameters each trial. ρ are the Spearman correlation coefficients. 5.10 Example stimuli of the cue and response intervals of medium noise conditions <ref type="bibr" target="#b55">(Nunez et al., 2019)</ref>. In the response phase, participants identified the spatialfrequency target represented by each Gabor, using their left hand to press a button for a target with a low spatial frequency (2.4 cpd) and their right hand for a target with a high spatial frequency (2.6 cpd). N200 waveforms were calculated time-locked to the onset of the Gabor stimulus during the response intervals. The visual noise altered at a frequency of 40 Hz, while the Gabor signal modulated at 30 Hz, inducing 40 Hz and 30 Hz electrocortical responses that monitor attention to both noise and signal. . . . . . . . . . . . . . . . . vii 5.11 The DDM is illustrated in action during a two-choice task, with non-decision time shown in green. Following the visual encoding period, the decision variable (DV) begins evidence accumulation and reaches either the upper or lower limit for each trial. The black vector depicts the average rate of evidence accumulation. The blue curve depicts the distribution of response times when choice 1 is successfully picked, while the red curve depicts the distribution of reaction times when choice 2 is correctly selected. When the DV drifts towards the incorrect boundary owing to random noise, incorrect decisions are made. The distribution of reaction times for incorrect trials is depicted by the dotted curve. EEG data for each trial, processed using singular value decomposition to highlight N200, is shown on top that track the start of evidence accumulation. . . . First of all, I would like to thank the members of my thesis committee for their invaluable feedback, constructive criticism, and unwavering support throughout this process. I am grateful to Prof. Nikil Dutt for his warm welcome to UCI and for his guidance in research from day one. His advice on presentations, paper writing, and networking has significantly improved my skills as a researcher. I thoroughly enjoyed our discussions on energy-efficient deep learning. I extend my heartfelt thanks to Prof. Hung Cao for offering me numerous opportunities for both academic and personal growth. Prof. Cao inspired my work on AI in healthcare, involving me in multidiscplinary teams where I learned extensively, leading to many fruitful collaborations and ideas for commercializing research projects. I am thankful to Prof. Ramesh Srinivasan for his guidance on challenging yet intellectually stimulating neurocognitive projects. Working with him allowed me to delve deeply into the most complex problems in AI and neuroscience. His patience, amiable advising approach, and expert knowledge have been immensely helpful in overcoming numerous significant challenges in the research. I deeply appreciate Prof. Erik Sudderth for his insightful feedback and suggestions that have greatly influenced my research. His course on Learning in Graphical Models provided the essential groundwork for the main topic of this thesis. I also would like to express my gratitude to my supervisors at Samsung Device Solution Research America, Dr. Mostafa El-Khamy and Dr. Yoojin Choi, for their extensive guidance and support, which extended beyond the duration of my internship. Portions of this dissertation were carried out during my time as an intern.</p><p>Before embarking on my PhD journey, I was privileged to have met and been inspired by several accomplished scholars, particularly Prof. Khanh Dang, Prof. Tho Quan, Mao Nguyen, and Minh Truong. Their influence motivated me to engage in research and pursue graduate studies in the United States.</p><p>A special thanks goes to my friends and fellow Ph.D. candidates at UCI, whose collaboration and camaraderie made this journey a memorable one. These include Michale Lee, Tai Le, Jenny Sun, Manoj Vishwanath, Emad Kasaeyan Naeini, Isaac Menchaca, Floranne Ellington, Amir Naderi, Alex Huang, Sadaf Sarafan, Ramses Torres, Anh Nguyen, Steven Cao, and many others. A deep latent variable model is a powerful method for capturing complex distributions. These models assume that underlying structures, but unobserved, are present within the data. In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models. First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs. This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices.</p><p>Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem. Third, we propose a framework for the joint modeling of physiological measures and behavior. Existing methods to combine multiple sources of brain data provided are limited. Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data.</p><p>Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions. The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications.</p><p>xiii Chapter 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction 1.1 Motivation</head><p>The modeling of physiological signals stands as a cornerstone in contemporary neuroscience and cardiology research, providing a powerful framework for understanding the intricate processes underlying the generation of signals in the heart and brain. These models, often constructed using computational methods, offer invaluable insights into the complex interactions among biological variables and serve as invaluable tools for hypothesis testing and experimental design. At the heart of physiological modeling lie equations that describe the interrelations among variables governing the dynamics of synthetic time series. These equations, typically derived from existing data or informed by domain expertise, describe the temporal evolution of physiological phenomena. By systematically altering parameters within these equations, researchers can explore a wide range of scenarios, uncovering novel insights and validating theoretical predictions. Previous studies in neuroscience and cardiology have leveraged computational models <ref type="bibr" target="#b53">(Niederer et al., 2019;</ref><ref type="bibr" target="#b17">Glomb et al., 2020)</ref> providing principal laws, empirically validated rules, or other domain expertise, typically 1 presented as general, time-dependent, and nonlinear partial differential equations.</p><p>With regard to electroencephalogram (EEG) signals, neural mass models represent a prominent paradigm for understanding the complex dynamics of large populations of neurons.</p><p>One such influential model is the Jansen-Rit model <ref type="bibr" target="#b31">(Jansen et al., 1993;</ref><ref type="bibr" target="#b30">Jansen and Rit, 1995)</ref>. This model emerged as a lumped parameter representation of a cortical column, specifically designed to capture the dynamics of human EEG rhythms and visual evoked potentials. Building upon earlier work by Lopes da Silva and Katznelson, the Jansen-Rit model retains non-linearities within the cortical column, thus offering a more biologically realistic description of neuronal activity. At its core, the Jansen-Rit model is based on the interaction between two distinct populations of neurons within the cortical column: pyramidal cells and local excitatory/inhibitory interneurons. Each population contributes to the generation and modulation of EEG signals through intricate synaptic connections and feedback loops.</p><p>The model describes the dynamics of postsynaptic potentials using a set of coupled differential equations, which are formulated to capture the complex interplay between excitatory and inhibitory neuronal populations. These equations are typically rewritten as a system of first-order differential equations, resulting in a six-dimensional dynamical system that encapsulates the behavior of the cortical column.</p><p>In terms of the electrocardiogram (ECG) signal, the <ref type="bibr" target="#b46">McSharry et al. (2003)</ref> model is a significant contribution, providing a framework for understanding the complex dynamics of cardiac rhythms. This model provides a mathematical description of the electrical activity of the heart through a system of three ordinary differential equations. Central to the model is the recognition of the distinct components of the ECG waveform, each of which corresponds to specific cardiac events and functions. The typical sequence begins with the P wave, representing atrial depolarization, followed by the QRS complex, reflecting ventricular depolarization, and concludes with the T wave, indicating ventricular repolarization. By capturing the temporal relationships between these waveform components, the model provides insights into the underlying physiological processes driving cardiac activity. Moreover, the constructed simulator offers the flexibility to manipulate various attributes of the produced ECG signals. Researchers can adjust parameters such as the interval between waves, the magnitude of P-waves and Q-waves, and the average and standard deviation of heart rate patterns. Additionally, the model allows for the exploration of frequency-domain aspects of heart rate variability, providing insights into the dynamic regulation of cardiac rhythm.</p><p>Although electrophysical models based on differential equations have been instrumental in understanding physiological signals, they come with inherent limitations. These models often rely on strong assumptions, can be computationally intensive, and may suffer from model misspecification issues. Consequently, there is a growing emphasis on developing more powerful and flexible models capable of handling diverse types of medical time series data. Medical time series data exhibit complex multidimensional dependencies, including spatio-temporal dependencies in biosignals and multimodal dependencies across physiological measures and behaviors. One of the key challenges is effectively modeling spatio-temporal dependencies in biosignals. Biosignals such as EEG and ECG are inherently dynamic and exhibit spatial variations across different body regions. Another challenge lies in integrating multiple modalities of medical data. In modern healthcare settings, patient data often comprise a diverse array of measurements from different sensors and modalities, including physiological signals, imaging data, and clinical observations. These presents significant challenges for modeling and analysis in biomedical research and clinical practice.</p><p>In recent decades, the exponential growth of data collection in various medical applications has paved the way for the emergence of data-driven approaches capable of unlocking valuable insights and addressing complex challenges in healthcare. These approaches leverage advanced computational techniques to analyze large volumes of data, uncover underlying structures, and utilize extracted information for tasks such as predictive modeling and pattern recognition. A significant breakthrough in this domain has been the integration of probabilistic modeling and deep learning techniques. This fusion of methodologies combines the expressive power of deep neural networks with the probabilistic framework, enabling the parameterization of rich probabilistic distributions over latent variables. By incorporating established or desired inductive biases, these models can effectively capture the complex relationships and uncertainties inherent in medical data. Two prominent classes of models that have propelled recent progress are variational autoencoders (VAEs) and generative adversarial networks (GANs), both belonging to the broader category of deep generative models.</p><p>These models offer scalable and efficient solutions for unsupervised learning of complex, high-dimensional data distributions. This thesis aims to explore a diverse category of sequential and multimodal models, with a particular focus on their application to complex spatio-temporal physiological measures. By leveraging these models, we seek to unlock valuable insights from unlabeled datasets, paving the way for a deeper understanding of dynamic physiological processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Outline and contributions</head><p>Chapter 2 first presents the pertinent techniques employed in both clinical practice and research for physiological monitoring, offering crucial insights into the body's internal condition. We focus on biosignals obtained through sensors either on or inside the body, like surface ECG and EEG. Subsequently, we introduce probability theory, graphical models, and latent variable models, which underpin all the methods discussed later in this dissertation.</p><p>Chapter 3 presents a sequential modeling of ECG signals from photoplethysmography (PPG). PPG is a cost-effective and non-invasive technique that utilizes optical methods to measure cardiac physiology. PPG has become increasingly popular in health monitoring and is used in various commercial and clinical wearable devices. Compared to electrocardiog-raphy (ECG), PPG does not provide substantial clinical diagnostic value, despite the strong correlation between the two. Here, we propose a subject-independent attention-based deep state-space model (ADSSM) to translate PPG signals to corresponding ECG waveforms.</p><p>The model is not only robust to noise but also data-efficient by incorporating probabilistic prior knowledge. To evaluate our approach, 55 subjects' data from the MIMIC-III database were used in their original form, and then modified with noise, mimicking real-world scenarios. Our approach was proven effective as evidenced by the PR-AUC of 0.986 achieved when inputting the translated ECG signals into an existing atrial fibrillation (AFib) detector.</p><p>ADSSM enables the integration of ECG's extensive knowledge base and PPG's continuous measurement for early diagnosis of cardiovascular disease.</p><p>Chapter 4 presents a deep generative model of EEG signals that provides not only a stochastic procedure that directly generates data but also insights to further understand the neurological mechanisms. Specifically, we propose a generative and inference approach that combines the complementary benefits of probabilistic graphical models and GANs for EEG signal modeling. We investigate the method's ability to jointly learn coherent generation and inverse inference models on the CHI-MIT epilepsy multi-channel EEG dataset. We further study the efficacy of the learned representations in epilepsy seizure detection formulated as an unsupervised learning problem.</p><p>Chapter 5 introduces a method for joint cognitive modeling of neural signals and human behavior. As the field of computational cognitive neuroscience continues to expand and generate new theories, there is a growing need for more advanced methods to test the hypothesis of brain-behavior relationships. Recent progress in Bayesian cognitive modeling has enabled the combination of neural and behavioral models into a single unifying framework. However, these approaches require manual feature extraction, and lack the capability to discover previously unknown neural features in more complex data. Consequently, this would hinder the expressiveness of the models. To address these challenges, we propose a Neurocogni-tive Variational Autoencoder (NCVA) to conjoin high-dimensional EEG with a cognitive model in both generative and predictive modeling analyses. Importantly, our NCVA enables both the prediction of EEG signals given behavioral data and the estimation of cognitive model parameters from EEG signals. This novel approach can allow for a more comprehensive understanding of the triplet relationship between behavior, brain activity, and cognitive processes.</p><p>Chapter 6 concludes the main contributions of this dissertation and discusses some directions for future work.</p><p>Chapter 2 Background 2.1 Physiological Signals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Electrocardiogram (ECG or EKG)</head><p>Electrocardiography (ECG) is fundamental in diagnosing and managing cardiac health . By recording the heart's electrical activity, the ECG provides crucial insights into its rhythm, rate, and muscular functionality. This non-invasive tool quantifies voltage differences between points on the body's surface over time, enabling clinicians to assess the heart's performance with precision. At its core, an ECG captures the electrical signals generated by the heart with each beat. These signals, represented graphically as waves and complexes on the ECG tracing, reflect the coordinated sequence of events during the cardiac cycle, including atrial depolarization, ventricular depolarization, and ventricular repolarization. Key among its features is the identification of the R-wave, marking the onset of ventricular contraction that propels blood from the heart into the aorta, a pivotal event in the cardiac cycle. In clinical settings, traditional 12-lead ECG devices provide a comprehensive view of the heart's electrical activity from multiple angles. By delivering 12 concurrent ECG signals, these devices offer invaluable insights into irregularities and pinpoint specific areas of concern within the heart. Recent advancements have seen the integration of single-lead ECG sensors into wrist-worn devices, bringing cardiac monitoring closer to everyday life. These compact gadgets, featuring electrodes on the wrist and an additional point of contact, offer a simplified yet effective means of evaluating heart rhythm and certain functional aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Photoplethysmogram (PPG)</head><p>Photoplethysmogram (PPG) signals are obtained through the emission of light from a lightemitting diode (LED) onto the skin, followed by the assessment of the light either reflected back from the skin surface or transmitted through bodily tissues. This fundamental principle underpins its application in a variety of devices, from wrist-worn wearables to medical-grade pulse oximeters. The PPG signal tracks variations in blood volume over time, particularly in arterial blood. As the arterial pulse wave reaches the measurement site, typically at the fingertip or earlobe, it triggers detectable changes in blood volume, generating characteristic waveforms in the PPG signal. Each heartbeat manifests as a distinct peak in the PPG waveform, reflecting the pulsatile nature of blood flow and pressure within the arteries. A major benefit of PPG sensors is their capability to record physiological signals passively, without the need for user involvement, in contrast to wrist-worn ECG sensors which require active user engagement during the collection of signals. This ease of use, coupled with their non-invasive nature, has propelled the widespread adoption of PPG sensors in consumer wearable devices for tracking heart rate, heart rate variability, and cardiac rhythm. However, it is important to recognize that the PPG signal is susceptible to noise, stemming from factors such as motion artifacts, ambient light interference, and variations in skin perfusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Electroencephalogram (EEG)</head><p>Electroencephalography (EEG) provides invaluable insights into the electrical activity of the brain. By recording the synchronized post-synaptic currents primarily in cortical pyramidal neurons <ref type="bibr" target="#b58">(Nunez and Srinivasan, 2006)</ref>, EEG offers a unique window into cognitive processes, neural dynamics, and neurological disorders. Over the years, EEG has become a cornerstone in both clinical diagnostics and cognitive research, enabling researchers and clinicians to delve deep into the complexities of brain function. Broadly categorized into spontaneous potentials, such as sleep rhythms, and evoked potentials, which are time-locked responses to external stimuli, EEG captures brain activity with high temporal resolution. Operating on the scale of milliseconds, EEG is capable of detecting rapid changes in neural activity, making it a powerful tool for studying dynamic brain processes. However, despite its temporal precision, EEG possesses inherent limitations in spatial resolution. The electrical signals detected at the scalp originate from currents that propagate through head tissues via volume conduction, resulting in a low spatial resolution. While EEG can provide insights into broad patterns of brain activity, its ability to localize specific neural sources is limited. Nevertheless, EEG has found extensive applications in both clinical and research settings. In clinical practice, EEG is used to diagnose and manage various neurological conditions, including epilepsy, sleep disorders, stroke, and Alzheimer's disease. In the realm of cognitive sciences, EEG offers insights into sensorimotor pathways, memory, language processing, and general intelligence.</p><p>One of the key advantages of EEG lies in its affordability, portability, and suitability for realtime observation. Unlike other brain imaging techniques that require specialized equipment and expertise, EEG can be deployed with minimal resources, making it accessible to a wide range of researchers and clinicians. However, EEG analysis is not without challenges. EEG signals are non-stationary, exhibit a poor signal-to-noise ratio, and exhibit high variability among individuals, posing significant obstacles to the development of generalized models for EEG analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Frequency Domain Representation</head><p>A univariate time series signal of length T consists of a sequence of real-valued data points x = (x 0 , . . . , x T -1 ) ∈ R T , each representing observations of a specific phenomenon. Observations in a time series are generally interdependent, and grasping this dependency is crucial for recognizing various phenomena as they appear. In the frequency domain, a time series is analyzed by decomposing it into sinusoids that vary in amplitude and phase.</p><p>Assuming the periodic nature of the time series x = (x 0 , . . . , x T -1 ), we can represent each element x t with an equation</p><formula xml:id="formula_0">x t = T -1 k=0 X k e j2πkt T (2.1)</formula><p>where X k ∈ C, k = 0, . . . , T -1, represent the Fourier coefficients. Each element x t in the time series is broken down into T frequency components e j2πkt/T , k = 0, . . . , T -1, each scaled by the Fourier coefficients. In the discrete FT (DFT), the time series x is expressed via the Fourier coefficients</p><formula xml:id="formula_1">X k = (1/T )v * k x, k = 0, . . . , T -1, where v k :=</formula><p>1, e j2πk(1)/T , e j2πk(2)/T , . . . , e j2πk(T -1)/T . The collection of vectors {v 0 , . . . , v T -1 } forms an orthogonal basis for the T -dimensional complex vector space. Each Fourier coefficient thus serves as an independent representation of a subcomponent of the entire time series. The conversion of the time series into the frequency domain can be efficiently performed using the fast FT (FFT) algorithm, while the transformation from the frequency domain back to the time domain is accomplished using the inverse DFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Probabilistic Graphical Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Random Variables and Probabilities</head><p>Definition 1 (Random Variable).</p><p>In a random experiment with a sample space S, a function X maps each element s ∈ S to a single real number X(s) = x. This function X is known as a random variable (r.v.).</p><p>Typically, random variables are represented by capital letters, while the values they take are denoted by lowercase letters. The term univariate distribution will be used to describe the distributions of a single random variable, indicated by the non-bold x. The term multivariate distributions will apply to distributions involving multiple random variables, typically represented as a vector with bold x.</p><p>Definition 2 (Discrete Random Variable and Probability Mass Function).</p><p>1. A random variable X is termed discrete if its range consists of countable values.</p><p>2. If X is a discrete r.v., the function P (X = x) is called the probability mass function (PMF) of X.</p><p>3. The PMF of any discrete r.v. X must adhere to these two criteria:</p><p>• Nonnegativity: P (X = x) &gt; 0 if x = x i for some i, and P (X = x) = 0 otherwise.</p><formula xml:id="formula_2">• Summation to 1: ∞ i=1 P (X = x) = 1.</formula><p>Definition 3 (Continuous Random Variable and Probability Density Function).</p><p>1. A r.v. X is defined as continuous if there is a function p(.) such that for every real number x, the cumulative distribution function (CDF) is given by F</p><formula xml:id="formula_3">X (x) = P (X ⩽ x) = x -∞ p(x) dx.</formula><p>2. For a continuous r.v. X, the function p(.) in F X (x) = p(x) dx is known as the probability density function (PDF).</p><p>3. The PDF of any continuous r.v. X must meet the following two criteria:</p><p>• Nonnegativity: p(x) ≥ 0.</p><p>• Integration to 1:</p><formula xml:id="formula_4">∞ -∞ p(x) dx = 1.</formula><p>The term "probability distribution" will be applied to both discrete probability mass functions and continuous probability density functions in our discussions. The specific use of the term will be inferred based on the context.</p><p>Definition 4 (Exponential Family of Probability Distributions).</p><p>1. The exponential family of probability distributions is a set of PDFs or PMF's characterized by the following form:</p><formula xml:id="formula_5">p(x | η) = h(x) exp T (x) T η -A(η) (2.2)</formula><p>Here, x represents a specific value of a r.v. X, T (x) is the sufficient statistic, and η is the natural parameter. The function A(η) is known as the log-partition function and h(x) is the base measure.</p><p>2. A fundamental property of PDFs or PMFs in the exponential family is expressed by:</p><formula xml:id="formula_6">E[T (x)] = ∇A(η) (2.3)</formula><p>ship between the expected value of the sufficient statistic and the gradient of the log-partition function.</p><p>The exponential family includes many well-known distributions such as the normal, exponential, Poisson, and gamma distributions, among others <ref type="bibr" target="#b72">(Wainwright et al., 2008)</ref>. One of the key advantages of distributions in the exponential family is their mathematical tractability, which simplifies parameter estimation, hypothesis testing, and model interpretation. Probabilistic Graphical Models (PGMs) <ref type="bibr" target="#b38">(Koller and Friedman, 2009)</ref> serve as a structured framework for representing and reasoning about complex probabilistic relationships among multiple variables. One of the key advantages of PGMs lies in their ability to handle uncertainty and variability efficiently. As the number of variables grows, managing probability distributions becomes increasingly complex. In many real-world scenarios, understanding these relationships and making informed decisions based on uncertain data is essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Graphical Models</head><p>At their core, PGMs leverage graphs to visually depict probabilistic relationships between variables. These graphs consist of nodes, which represent random variables, and edges, which denote probabilistic dependencies between variables. Random variables can be classified as either observed, with their values determined by the problem, or latent, with their values remaining unknown.</p><p>There are two main types of PGMs: directed graphical models, also known as Bayesian networks, and undirected graphical models, also called Markov Random Fields. Bayesian networks utilize directed edges to represent influential relationships between variables, while Markov Random Fields capture the concept of local interactions among variables through undirected edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representing domain knowledge through graphical structures</head><p>One notable aspect of PGMs is the inherent structure of graph representations, which directly implies a factorization of the joint distribution over random variables. In a graphical model, every graph structure entails a specific factorization of the joint distribution. For instance, in a directed graphical model, such as the one shown in Figure <ref type="figure" target="#fig_2">2</ref>.1a (right), the joint distribution factors according to conditional probabilities associated with each node:</p><formula xml:id="formula_7">P (X 1 , . . . , X 4 ) = P (X 1 ) P (X 2 | X 1 ) P (X 3 | X 2 ) P (X 4 | X 2 , X 3 ) (2.4)</formula><p>Similarly, considering the model depicted in Figure <ref type="figure" target="#fig_2">2</ref>.1b (left), the joint distribution can be expressed as a product of clique potentials, where each clique represents a set of variables that are directly connected:</p><formula xml:id="formula_8">P (X 1 , . . . , X 9 ) = 1 Z c i ∈C ϕ i (x c ) (2.5)</formula><p>where C represents all cliques within the graph , specifically every pair of connected nodes,</p><formula xml:id="formula_9">ϕ i (x c</formula><p>) refers to the clique potentials, which are scalar values given to each possible combination of variables within the clique x c , and Z is the normalization constant.</p><p>This factorization has practical implications, as we need only track parameters associated with each conditional probability or clique potential. By exploiting this structured representation, significant parameter savings can be achieved compared to a full join distribution. Additionally, the development of a graphical model typically requires integration with domain experts. Such integration encourages careful deliberation of the choice and interconnections of random variables in the model. For example, the grid structure observed in Figure <ref type="figure" target="#fig_2">2</ref>.1b suggests spatial correlations among variables, akin to those found in image pixels. Furthermore, graphical models enable the utilization of structural properties of the underlying data distribution to simplify the computation of probabilistic queries. One notable advantage is the simplification of independence properties, facilitated by the graph's topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Independence properties</head><p>By leveraging insights from graph theory and probability theory, graphical models offer a structured approach to probabilistic reasoning. An essential aspect of understanding these relationships lies in studying independence properties among the distributions over random variables within the graphical model:</p><p>• Marginal Independence. Marginal independence occurs when two random variables are independent of each other, irrespective of the values of other variables. In graphical models, this independence is evident when there is no direct connection (edge) between the variables. For instance, in a directed graphical model, if there is no directed path between two variables, they are considered marginally independent. Mathematically, marginal independence implies that the joint distribution factorizes into the product of marginal distributions for each variable.</p><p>• Conditional Independence. Conditional independence arises when two random variables become independent given the values of a third set of variables. In graphical models, conditional independence statements reveal when observing certain variables render others independent. This concept is crucial for understanding the influence and interaction between variables. Conditional independence relationships are often inferred from the graph's structure, where paths between variables indicate possible dependencies or influences.</p><p>In directed graphical models, a unique type of conditioning exists that causes variables, which are marginally independent, to become dependent. This concept of "explaining away" refers to a phenomenon where the evidence observed for one variable increases or decreases the probability of another variable being responsible for the same observation, depending The Markov blanket of a random variable refers to a set of variables in a graphical model that, when conditioned on, renders the random variable independent of all other variables in the graph. Specifically, for any random variables X and Y in the graphical model G, the Markov blanket (MB) of X is defined as the minimal set of variables such that conditioning on this set renders the conditional independence of X and Y :</p><formula xml:id="formula_10">P (X | MB(X), Y ) = P (X | MB(X)) (2.6)</formula><p>In undirected graphical models, the Markov blanket of a random variable consists of all its neighboring variables. In directed graphical models, the Markov blanket includes a node's parents, its children, and its children's co-parents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Latent Variable Models</head><p>In the realm of machine learning, one of the fundamental challenges is to accurately model and understand the underlying probability distributions p(x) governing high-dimensional data. High-dimensional data, such as images, text documents, and sensor readings, often exhibit complex structures and dependencies that are not easily captured by simple parametric models. Modeling these complex probability distributions is crucial for various tasks, including generative modeling, anomaly detection, and density estimation. Generative models aim to learn the underlying data distribution and generate new samples that resemble the original data. Anomaly detection algorithms rely on accurate probability estimates to identify deviations from normal behavior. Density estimation techniques seek to estimate the probability density function of the data, enabling various downstream tasks such as sampling and likelihood evaluation.</p><p>Introducing an unobserved latent variable z with lower dimensionality than observed vectors and defining a conditional distribution p(x | z) for the data is a powerful approach in probabilistic modeling, particularly in scenarios involving high-dimensional data. This framework allows us to capture complex correlations in the observed variable x by leveraging the latent variable z, which serves as a compact representation of underlying factors influencing the data. In this framework, the latent variable encodes meaningful information about the structure and content of the observed data. For example, in the context of modeling medical data, z could encapsulate latent representations of various attributes such as disease subtypes or phenotypes, biomarker signatures, and other relevant features. To formalize this probabilistic model, we introduce a prior distribution p(z) over the latent variables, representing our beliefs about the likely configurations of z. This prior distribution encodes any prior knowledge or assumptions about the latent space. We then compute the joint distribution over observed and latent variables, denoted as p(x, z), which describes the probability of observing a particular data point x along with its corresponding latent representation z:</p><formula xml:id="formula_11">p(x, z) = p(x | z)p(z) (2.7)</formula><p>Introducing a latent variable in the model enables us to express the complex marginal distribution p(x) as a more tractable joint distribution, which consists of the conditional distribution p(x | z) and the prior distribution p(z). Typically, simpler distributions such as exponential family distributions are used to define the conditional distribution p(x | z)</p><p>and the prior distribution p(z). Exponential family distributions have desirable properties, including tractable normalization constants, which make them computationally efficient for modeling purposes. Once we have the joint distribution p(x, z), we can obtain the desired data distribution p(x) by marginalizing over the latent variables:</p><formula xml:id="formula_12">p(x) = p(x, z)dz = p(x | z)p(z)dz (2.8)</formula><p>By applying Bayes' theorem, we can calculate the posterior distribution p(z | x) as</p><formula xml:id="formula_13">p(z | x) = p(x | z)p(z) p(x) (2.9)</formula><p>which allows inference of the latent variable given the observation.</p><p>Latent variable models offer a framework to describe the generative process behind the observed data. This process can be interpreted as follows:</p><p>1. Sampling Latent Variables: To generate a new data point, we first sample a latent variable z (s) from the prior distribution p(z). This latent variable captures unobserved factors or features that influence the generation of the data.</p><p>2. Generating Observations: Once we have sampled z (s) , we use it to sample a new observation x (s) from the conditional distribution p(x | z (s) ). This conditional distribution captures the relationship between the latent variables and the observed data, allowing us to generate realistic data points.</p><p>Latent variable models (LVMs) are particularly effective when the data lie in a manifold, a lower-dimensional structure embedded within the higher-dimensional data space </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Posterior Inference</head><p>In latent variable models, the posterior distribution updates our understanding of the latent variables based on the data observed. It is essential for probabilistic reasoning, facilitating prediction, inference, and the learning of model parameters. To approximate the complex posterior distribution, two main types of methods are utilized, each balancing accuracy with computational efficiency:</p><p>1. Sampling. Sampling techniques, including Markov Chain Monte Carlo (MCMC) approaches, offer an approximation of the posterior distribution through sample generation. These techniques produce samples from the posterior distribution, enabling the estimation of expectations and the execution of inference via Monte Carlo integration.</p><p>A notable advantage of sampling techniques is their ability to provide precise outcomes with unlimited computational resources. Nevertheless, they often require significant computational effort and may not efficiently handle large datasets. Moreover, assessing convergence and verifying the quality of the samples can pose difficulties.</p><p>2. Deterministic Approximation. Deterministic approximation methods approximate the posterior distribution analytically by employing parametric distribution families or specific factorizations. Techniques such as variational inference and expectation propagation are examples. These approaches are scalable and efficient, thus appropriate for handling large datasets. Nonetheless, they cannot ensure precise outcomes, even with unlimited computational resources, because of the fundamental approximations involved. Despite these constraints, deterministic approximation methods remain popular due to their computational manageability and ability to scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Inference</head><p>Variational inference (VI) <ref type="bibr" target="#b35">(Jordan et al., 1999)</ref> leverages the calculus of variations to approximate the posterior distribution p(z | x) by finding an approximate distribution q(z)</p><p>that minimizes the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior. The KL divergence between q(z) and p(z | x) is defined as:</p><formula xml:id="formula_14">KL[q(z)∥p(z | x)] = -E q(z) log p(z | x) q(z) (2.10)</formula><p>The goal of variational inference is to find a good approximation q(z) that minimizes this KL divergence. However, the intractability of the posterior p(z | x) makes direct optimization challenging. To address this, we introduce the evidence lower bound (ELBO), denoted as F(q), which is defined as:</p><formula xml:id="formula_15">-E q(z) log p(x, z) q(z) (2.11)</formula><p>The ELBO serves as a lower bound on the marginal likelihood log p(x). By maximizing the ELBO with respect to q(z), we indirectly minimize the KL divergence, as the KL divergence is equal to log p(x) -F(q).</p><p>In practice, the variational distribution q(z) is often constrained to a parametric family (e.g., Gaussian distribution) to make the optimization tractable. The parameters of this distribution are then optimized to maximize the ELBO. This trade-off between flexibility and tractability ensures that the variational approximation q(z) is both expressive enough to capture the posterior distribution and computationally feasible to work with.</p><p>In essence, VI offers a principled approach to approximate complex posterior distributions, facilitating efficient and scalable inference within probabilistic models. By framing the inference problem as an optimization task, VI inference reduces the complexity of inference to a simpler optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation Propagation</head><p>Unlike variational inference, which minimizes the KL divergence from a chosen approximation to the true posterior, expectation propagation (EP) <ref type="bibr" target="#b47">(Minka, 2013)</ref> minimizes the reverse KL divergence, which is defined as:</p><formula xml:id="formula_16">KL[p(z | x)∥q(z)] = E p(z|x) log p(z | x) q(z) (2.12)</formula><p>It is important to note that EP does not necessarily minimize the KL divergence but can be freely implemented with any divergence measure. When q(z) belongs to the exponential family, which is commonly the case, the minimization of the reverse KL divergence simplifies to the alignment of natural parameters. In EP, the goal is to approximate the posterior distribution p(z) with a set of factors Q(z) such that each factor q i (z) approximates the corresponding factor f i (z) in the exact posterior factorization. This approximation is achieved by minimizing a sequence of local Kullback-Leibler (KL) divergences between the exact factors and the EP approximating factors.</p><p>The EP approximation for p(z) is given by:</p><formula xml:id="formula_17">Q(z) = n i=0 q i (z) (2.13)</formula><p>where q i (z) represents the EP approximating factors and Q(z) is the global approximation.</p><p>To find the parameters for determining the approximate factors q i (z), EP minimizes a sequence of local KL divergences:</p><formula xml:id="formula_18">q 0 (z) = arg min q 0 (z)∈Q KL f0 (z)∥q 0 (z)Q \0 (z) q 1 (z) = arg min q 1 (z)∈Q KL f1 (z)∥q 1 (z)Q \1 (z) . . . q n (z) = arg min qn(z)∈Q KL fn (z)∥q n (z)Q \n (z) (2.14)</formula><p>where Q denotes the space of possible approximate factors, fi (z) = f i (z)Q \i (z) is the tilted distribution, and Q \i (z) is the cavity distribution obtained by removing the current KL minimizer q i (z) from Q(z).</p><p>Each KL divergence minimization problem in the above equations is solved by exclusively optimizing q i (z) instead of the EP global approximation. Therefore, expectation propagation is considered a local approximation algorithm as each KL divergence is minimized locally with respect to a selected EP approximating factor.</p><p>In EP, convergence is not guaranteed because the local KL divergence minimization does not necessarily ensure that the KL divergence is minimized from the exact posterior distribution to the EP global posterior approximation. Despite the absence of formal convergence guarantees, EP remains a widely used and effective approximate inference algorithm in probabilistic modeling due to its flexibility and applicability to a variety of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Parameter Learning</head><p>In parameter learning for latent variable models, we aim to estimate the optimal parameters θ ⋆ of the model given a training set comprising N data points {x i } N i=1 . The likelihood p θ (x | z) and the prior p θ (z) are assumed to belong to families of distributions parameterized by unknown parameters θ.</p><p>The optimal parameters θ ⋆ can be learned using Maximum Likelihood Estimation (MLE), which involves maximizing the log-likelihood function L(θ):</p><formula xml:id="formula_19">L(θ) = N i=1 log p θ (x i ) (2.15)</formula><p>Given that the latent variable z i is different for each data point x i , but the parameters θ are shared across all data points, we can express the log-likelihood as the sum of individual terms L i (θ):</p><formula xml:id="formula_20">L(θ) = N i=1 log p θ (x i , z i )dz i L i (θ)</formula><p>(2.16)</p><p>In practice, the marginal density of the observations p θ (x) is often intractable and needs to be approximated. One approach is to use the evidence lower bound (ELBO), denoted as</p><formula xml:id="formula_21">F i (θ, q)</formula><p>, which provides a lower bound to log p θ (x) for any distribution q(z) over the latent variables:</p><formula xml:id="formula_22">L i (θ) ≥ F i (θ, q) = E q(z) log p θ (x, z) q(z) (2.17)</formula><p>By maximizing the total ELBO F(θ, q) = N i=1 F i (θ, q) with respect to θ and q(z), we can learn the parameters of the model. The variational distribution q(z) can be interpreted as an approximation to the posterior distribution p θ (z | x), and the ELBO coincides with the log-likelihood only when q(z) equals the true posterior distribution.</p><p>In practice, the variational distribution q(z) is often constrained to a particular parametric family to make the optimization tractable, and the parameters of this distribution are optimized along with the parameters θ of the model. Therefore, by maximizing the ELBO, we indirectly maximize the log-likelihood, enabling parameter learning in latent variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation Maximization</head><p>The Expectation Maximization (EM) algorithm <ref type="bibr" target="#b12">(Dempster et al., 1977)</ref> provides a systematic approach for maximizing the likelihood function in models with latent variables. It iteratively alternates between two steps: the E-step, where the posterior distribution over latent variables is estimated, and the M-step, where the model parameters are updated based on the estimated posteriors.</p><p>Starting with initial parameters θ 0 , the EM algorithm iterates until convergence as follows:</p><p>1. E-step (Expectation): Given the current parameters θ k , estimate the posterior distribution over latent variables, denoted as q k+1 (z), by maximizing the ELBO F i (θ k , q) with respect to q(z). In many cases, this step involves solving a posterior inference problem, aiming to find an approximation to the true posterior distribution</p><formula xml:id="formula_23">p θ k (z | x).</formula><p>If the posterior is intractable, approximate inference methods can be employed.</p><p>2. M-step (Maximization): Fixing the estimated distribution over latent variables q k+1 (z), update the parameters θ k+1 by maximizing the ELBO F i (θ, q k+1 ) with respect to θ.</p><p>This step involves optimizing the model parameters using techniques such as gradient ascent.</p><p>For simpler classes of models where exact inference is possible, each EM iteration guarantees not to decrease the marginal likelihood after each combined step. Specifically, after the Estep, where θ k is held fixed, the ELBO equals the log-likelihood. Subsequently, maximizing the ELBO in the M-step does not decrease the log-likelihood.</p><p>In summary, the EM algorithm offers a systematic and effective approach for optimizing model parameters when latent variables are involved. By iteratively refining the estimates of the latent variables and updating the model parameters, EM enables efficient learning in latent variable models even when exact inference is not feasible. ECG is considered the preferred method for monitoring vital signs and for the diagnosis, management, and prevention of cardiovascular diseases (CVDs), which are a leading cause of death globally, accounting for approximately 32% of all deaths in 2017 according to Global Burden of Disease reports <ref type="bibr" target="#b0">(Allen, 2007)</ref>. It has also been demonstrated that sudden cardiac arrests are becoming more prevalent in young individuals, including athletes (sudden, 2020).</p><p>Regular ECG monitoring has been found to be beneficial for the early identification of CVDs <ref type="bibr" target="#b64">(Rosiek and Leksowski, 2016)</ref>. Among heart diseases, atrial fibrillation (AFib) is adults' most common rhythm disorder. Identifying AFib at an early stage is crucial for the primary and secondary prevention of cardioembolic stroke, as it is the leading risk factor for this type of stroke <ref type="bibr" target="#b59">(Olier et al., 2021)</ref>. Advancements in electronics, wearable technologies, and machine learning have made it possible to record ECGs more easily and accurately, and to analyze large amounts of data more efficiently. Despite these developments, there are still challenges associated with continuously collecting high-quality ECG data over an extended period, particularly in everyday life situations. The 12-lead ECG, considered the clinical gold standard, and simpler versions, such as the Holter ECG, can be inconvenient and bulky due to the need to place multiple electrodes on the body, which can cause discomfort. Additionally, the signals may degrade over time as the impedance between the skin and electrodes changes.</p><p>Consumer-grade products such as smartwatches have developed solutions to address these issues. However, these products require users to place their fingers on the watch to form a closed circuit, making continuous monitoring impossible. One potential solution to these issues is to use a mathematical method to derive ECG data from an alternative, highly correlated, non-invasive signal, such as the photoplethysmogram (PPG), which can be easily acquired using various wearable devices, including smartwatches.</p><p>PPG is more convenient, cost-effective, and user-friendly. PPG has been increasingly adopted in consumer-grade devices. This technique involves the use of a light source, usually an LED, and a photodetector to measure the changes in light absorption or reflection as blood flows through the tissue. ECG and PPG signals are inherently correlated as both are influenced by the same underlying cardiac activity, namely the depolarization and repolarization of the heart. These contractions lead to changes in peripheral blood volume, which are measured by PPG. Figure <ref type="figure" target="#fig_5">3</ref>.1 shows the relationship between ECG and PPG waveforms. Although there are established standards for interpreting ECG for clinical diagnosis, the use of PPG is still mostly limited to measuring heart rate and oxygen saturation <ref type="bibr" target="#b63">(Reisner et al., 2008)</ref>. By translating PPG to ECG signals, clinical diagnoses of cardiac diseases and anomalies could be made in real-time.  <ref type="formula">2020</ref>) leveraged the expressiveness and structural flexibility of neural networks to build end-to-end PPG-to-ECG algorithms. However, the models suffer from data-hungry problems as they do not explicitly model the underlying sequential structures of the data. In addition, complex deep learning models cannot run efficiently on resourceconstrained devices (e.g., wearables) due to their high computational intensity, which poses a critical challenge for real-world deployment <ref type="bibr" target="#b42">(Lee et al., 2020)</ref>. Furthermore, deterministic models face difficulties in effectively generalizing to noisy data.</p><p>To address these challenges, we propose a deep probabilistic model to accurately estimate ECG waveforms from raw PPG. The contributions of this work are three-fold:</p><p>• We present a deep generative model incorporating prior knowledge about the data structures that enable learning on small datasets. Specifically, we develop a deep latent state-space model augmented by an attention mechanism.</p><p>• The probabilistic nature of the model enhances its robustness to noise. We demonstrate this by evaluating the model on data corrupted with Gaussian and baseline wandering noise, replicating real-life situations.</p><p>• Our method is effective not only in healthy subjects but also in subjects with AFib.</p><p>It is orthogonal and complementary to existing AFib detection methods <ref type="bibr" target="#b25">(Hong et al., 2020)</ref> by simply providing the translated ECG to any pre-trained models. This would enhance the performance of existing models by enabling uninterrupted monitoring, thereby facilitating the early detection of cardiovascular disease. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology</head><formula xml:id="formula_24">≜ log p θ (y | x) -KL (q ϕ (z | x, y)∥p θ (z | x, y)) = E q ϕ (z|x,y) log p θy (y | z, x) -KL (q ϕ (z | x, y)∥p θz (z | x)) (3.2)</formula><p>Taking the likelihood model p θy (y | z, x) to be a decoder, the latent inference model q ϕ (z | x, y) to be an encoder, and the prior model p θz (z | x), a conditional variational autoencoder (CVAE) <ref type="bibr" target="#b37">(Kingma and Welling, 2013;</ref><ref type="bibr">Sohn et al., 2015)</ref> considers this objective from a deep probabilistic autoencoder perspective. Here θ and ϕ are neural network parameters, and learning takes place via stochastic gradient ascent using unbiased estimates of ∇ θ,ϕ</p><formula xml:id="formula_25">1 n n i=1 L (x i , y i ; θ z , θ y , ϕ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">State-Space Modeling of ECG from PPG Signals</head><p>In the previous section, we consider the networks that process the entire time series as a whole, which do not explicitly model the underlying sequential natures of the data. This may lead to resource-inefficient learning. Here, propose to address the problems by leveraging the quasi-periodic nature of the physiological signals.</p><p>ECG Generative (Decoding) Process from PPG</p><p>We consider nonlinear dynamical systems with observations y t ∈ R nrr , i.e., RR intervals or the time elapsed between two successive R peaks on the ECG, depending on control inputs</p><p>x t ∈ R npp , i.e., PP intervals or the time elapsed between two successive systolic peaks on the PPG. We choose the peaks to segment the signals as they are the most robust features.</p><p>Corresponding discrete-time sequences of length T are denoted as y 1:T = (y 1 , y 2 , . . . , y T ) and x 1:T = (x 1 , x 2 , . . . , x T ).</p><p>Given an input PPG x 1:T , we are interested in a probabilistic model p (y 1:T | x 1:T ). Formally, we consider</p><formula xml:id="formula_26">p (y 1:T | x 1:T ) = p (y 1:T | z 1:T , x 1:T ) p (z 1:T | x 1:T ) dz 1:T (3.3)</formula><p>where z 1:T represents the latent sequence associated with the given model. This implies that we are considering a generative model that incorporates a latent dynamical system with an emission model p (y 1:T | z 1:T , x 1:T ) and transition model p (z 1:T | x 1:T ).</p><p>To derive state-space models, we make certain assumptions regarding the state transition and emission models, as shown in Figure <ref type="figure" target="#fig_5">3</ref>.2:</p><formula xml:id="formula_27">p (z 1:T | x 1:T ) = T -1 t=0 p (z t+1 | z t , x 1:T ) (3.4) p (y 1:T | z 1:T , x 1:T ) = T t=1 p (y t | z t ) (3.5)</formula><p>Equations 3.4 and 3.5 make the assumption that the current state z t includes all the relevant information about both the current observation y t and the next state z t+1 , given the current control input x t . In contrast to the DKF model of <ref type="bibr" target="#b40">(Krishnan et al., 2015</ref><ref type="bibr" target="#b39">(Krishnan et al., , 2017))</ref>, our model takes into account the entire input signal x 1:T for each output y t via an attention mechanism <ref type="bibr" target="#b3">(Bahdanau et al., 2014)</ref>. Note that there are usually misalignments between the PPG and ECG cycles. Therefore, it is difficult to construct optimal and exact sample pairs. This attention mechanism not only helps to add more context to generate ECG segments, but also helps to address the problem of misalignment.</p><p>Let us define c t a sum of features of the input sequence (PP intervals), weighted by the alignment scores:</p><formula xml:id="formula_28">c t = T i=1 α t,i x i (3.6) α t,i = exp (s (z t-1 , x i )) n i ′ =1 exp (s (z t-1 , x i ′ )) (3.7)</formula><p>The alignment function s assigns a score α t,i to the pair of input at position i and output at position t, (x i , y t ), based on how well they match. The set of α t,i are weights defining how much of each source segment should be considered for each output interval.</p><p>Both state transition (prior) and emission models are non-linear Gaussian transformations parametrized by neural networks θ z and θ y :</p><formula xml:id="formula_29">p θz (z t+1 | z t , x 1:T ) = N (z t+1 | µ θz (z t , c t+1 ), σ 2 θz (z t , c t+1 )); (3.8) p θy (y t | z t ) = N (y t | µ θy (z t ), I) (3.9)</formula><p>where µ and σ 2 are the means and diagonal covariance matrices of the normal distributions N , I is the identity covariance matrix.</p><p>Latent State Inference (Posterior Encoding) Process Unlike a deterministic translation model, the process needs to find meaningful probabilistic embeddings of ECG segments in the latent space. We want to identify the structure of the parametetrized posterior distribution q ϕ (z 1:T | y 1:T ). Notice that we made a design choice to perform inference using only y 1:T . We chose this with the conditional independence assumption that PPG segments do not provide more information than ECG segments alone.</p><p>The graphical model in Figure <ref type="figure" target="#fig_5">3</ref>.2 shows that the z t node blocks all information coming from the past and flowing to z t+1 (i.e., z 1:t-1 and y 1:t ), leading to the following structure as in Figure <ref type="figure" target="#fig_5">3</ref>.3:</p><formula xml:id="formula_30">q ϕ (z 1:T | y 1:T ) = q ϕ (z 1 | y 1:T ) T -1 t=1 q ϕ z t+1 | z t , y t+1:T (3.10)</formula><p>where</p><formula xml:id="formula_31">q ϕ (z t+1 | z t , y t+1:T ) = N (z t+1 | µ ϕ (z t , y t+1:T ), σ 2 ϕ (z t , y t+1:T )) (3.11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Process</head><p>The objective function becomes a timestep-wise conditional variational lower bound <ref type="bibr" target="#b37">(Kingma and Welling, 2013;</ref><ref type="bibr">Sohn et al., 2015;</ref><ref type="bibr" target="#b39">Krishnan et al., 2017)</ref>:</p><formula xml:id="formula_32">log p θ (y | x) ≥ L(x, y; θ y , θ z , ϕ) ≜ T t=1 E q ϕ (zt|y t:T ) [ reconstruction log p θy (y t | z t ) emission model ] -β regularization KL (q ϕ (z 1 | y 1:T ) ∥p θz (z 1 | x 1:T )) -β T -1 t=1 E q ϕ (zt|y t:T ) [ regularization KL( q ϕ (z t+1 | z t , y t:T ) posterior inference model || p θz (z t+1 | z t , x 1:T ) prior transition model )]</formula><p>(3.12)</p><p>where β controls the regularization strength. During training, the Kullback-Leibler (KL) losses in the regularization terms "pull" the posterior distributions (which encode EEG segments) and the prior distributions (which embed PPG segments) towards each other. We learn the generative and inference models jointly by maximizing the conditional variational lower bound with respect to their parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Neural Network Architectures</head><p>Let us denote W , v, and b the weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score Model:</head><p>The alignment score α in Equation <ref type="formula">3</ref>.7 is parametrized by a feedforward network with a single hidden layer, and this network is jointly trained with other parts of the model. The score function s is in the following form:</p><formula xml:id="formula_33">s (z t-1 , x i ) = v ⊤ s tanh (W s [z t-1 ; W x x i ] + b s ) (3.13)</formula><p>Prior Transition Model: We parametrize the transition function in Equation <ref type="formula">3</ref>.8 from z t to z t+1 using a Gated Transition Function as in <ref type="bibr" target="#b39">(Krishnan et al., 2017)</ref>. The model is flexible in choosing a non-linear transition for some dimensions while having linear transitions for others. The function is parametrized as follows:</p><formula xml:id="formula_34">g t = sigmoid(W g 3 ReLU (W g 2 ReLU (W g 1 [z t ; c t+1 ] + b g 1 ) + b g 2 ) + b g 3 ) d t = W d 3 ReLU (W d 2 ReLU (W d 1 [z t ; c t+1 ] + b d 1 ) + b d 2 ) + b d 3 µ θz (z t , c t+1 ) = (1 -g t ) ⊙ (W µz [z t ; c t+1 ] + b µz ) + g t ⊙ d t σ 2 θz (z t , c t+1 ) = softplus W σ 2 z ReLU (d t ) + b σ 2 z (3.14)</formula><p>where I denotes the identity function, and ⊙ denotes element-wise multiplication.</p><p>Emission Model: We parameterize the emission function in Equation 3.9 using a twohidden layer network as: </p><formula xml:id="formula_35">µ θy (z t ) = W e 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFib Detection</head><p>Performance was measured by the Area under the Receiver Operating Characteristic (ROC-AUC), the Area under the Precision-Recall Curve (PR-AUC), and the F1 score. The PR-AUC is considered a better measure for imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Implementation and Results</head><p>ECG Translation from PPG  <ref type="bibr" target="#b33">(Johnson and Khoshgoftaar, 2019)</ref>, which arises from the fewer AFib records compared to the healthy ones, would be beneficial.</p><p>In Figure <ref type="figure" target="#fig_5">3</ref>.4, translated ECG waveforms are plotted with respect to the reference ECG waveforms of different heart rates. We can see that the model closely reconstructed the waveforms and maintained their essential properties, such as the missing P waves of the AFib ECG. In addition, we can be informed of the translation uncertainty by using a posterior on the latent embedding to propagate uncertainty from the embedding to the data. More specifically, with a distribution p(z) on the latent feature our predictions will be p θ (y | x) = p θy (y | z) p θz (z | x) dz. This would make the model more trustworthy and give patients and clinicians greater confidence in using it for medical diagnosis <ref type="bibr" target="#b6">(Begoli et al., 2019)</ref>. Future studies are expected to investigate methods to develop a fully Bayesian model and introduce a more flexible latent space <ref type="bibr">(Tran et al., 2023;</ref><ref type="bibr" target="#b7">Bendekgey et al., 2024)</ref>. Such advancements are advantageous in the medical field, particularly when data availability is limited or when uncertainty quantification and learning interpretable representations are essential. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFib Detection</head><p>We evaluated the performance of our model on the benefits of the translated ECG for the AFib detection task. To do so, we used a state-of-the-art AFib detection model, Multilevel Knowledge-Guided Attention (MINA) <ref type="bibr" target="#b24">(Hong et al., 2019)</ref>, trained on real ECG signals, each of 10 s, and tested against synthetic. It should be noted that any pre-trained AFib detection model can be used in our pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>In Our method allows for the screening and early detection of cardiovascular diseases in the home environment, saving money and labor, while supporting society in unusual pandemic situations. tions.</p><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref> provide a powerful framework and tools for machine learning, especially for deep representation learning and generative models. Over the past few years, GANs have witnessed tremendous advancements and achieved state-of-the-art performance in a variety of prominent tasks, including photo editing, video prediction, text generation, and signal synthesis <ref type="bibr" target="#b28">(Jabbar et al., 2020;</ref><ref type="bibr" target="#b69">Vo et al., 2021)</ref>. As a data-driven method, GANs are flexible and do not depend on rigid assumptions.</p><p>Therefore, GANs hold great potential in modeling the inherent stochasticity and extrinsic uncertainty of EEG signals.</p><p>Recent work <ref type="bibr" target="#b19">(Hartmann et al., 2018;</ref><ref type="bibr" target="#b2">Aznan et al., 2019;</ref><ref type="bibr" target="#b60">Pascual et al., 2019)</ref> applying</p><p>GANs in EEG synthesis tend to simply characterize the spatio-temporal characteristics of EEG data subject to latent spaces of basic distributions, e.g., Gaussian or uniform distributions. Such assumptions impose limitations in capturing the intrinsic dependence among latent variables. Also, the GANs require deeper networks to synthesize longer sequences, which are computationally expensive and challenging to train, e.g., vanishing or exploding gradient problems. Moreover, the lack of inference capability in vanilla GANs hinder insight into structural information of EEG signals. On the other hand, probabilistic graphical models <ref type="bibr" target="#b38">(Koller and Friedman, 2009;</ref><ref type="bibr" target="#b74">Wu et al., 2015)</ref> enable inference through structured representations but often lack the capability to model arbitrarily complex distributions.</p><p>To address these challenges, we propose a novel GAN-based approach for EEG signal modeling that couples deep implicit likelihoods <ref type="bibr" target="#b49">(Mohamed and Lakshminarayanan, 2016)</ref> with structured latent variable representations to combine their complementary strengths. Our method uses graphical models for representing underlying structures of the signals, and applies ideas from the Graphical-GAN <ref type="bibr" target="#b43">(Li et al., 2018)</ref> for effectively learning not only a generative model mapping from latent distributions to complex high-dimensional EEG data space but also an inverse inference model mapping from the data space to the latent space.</p><p>Our study paves the way for leveraging implicit probabilistic models to comprehensively investigate the mechanisms that generate brain waves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">EEG Signal Synthesis with GANs</head><p>A GAN is a generative model trained by a pair of neural networks in a game-theoretic approach <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref>. In GANs, a discriminator neural network D is trained to distinguish real from synthetic EEG signals, while a neural generator network G is trained to generate EEG signals from a latent space to make them indistinguishable by the discriminator. With EEG signal x drawn from data generating distribution q(x), z drawn from noise prior p z , and p(x) is the generator's distribution over synthetic data, G and D jointly optimize the following objective:</p><formula xml:id="formula_36">L GAN (G, D) = E x∼q(x) [log D(x)] + E z∼pz(z) [log(1 -D(G(z))] = E x∼q(x) [log D(x)] + E x∼p(x) [log(1 -D(x))] (4.1)</formula><p>The discriminator is expected to output a high probability for a valid EEG signal and a low probability for a synthesized one, corresponding to the values of log D(x) and log(1-D(G(z)), respectively. G and D are trained simultaneously until G is able to successfully fool D.</p><p>Following the proofs in <ref type="bibr" target="#b18">(Goodfellow et al., 2014)</ref>, given a fixed generator G, the optimal discriminator is given by D * (x) = q(x) q(x)+p(x)</p><p>Under an optimal discriminator D * , the generator minimizes the Jensen-Shannon (JS) divergence, which attains its minimum if and only if p(x) = q(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Conjoining GANs with Bayesian Networks</head><p>Generative and Inverse Inference Process As shown in Figure <ref type="figure" target="#fig_18">4</ref>.1, we model the generative process and the inverse inference process by a generative model and an inverse inference model in the Bayesian network. The framework exploits a Gaussian mixture model (GMM) to characterize the static latent variable structure with its capability to approximate arbitrary distributions, and a Markov model for the dynamic latent characterization. We use the notations p and q to denote the generative and inverse inference models.</p><p>The joint distribution of the generative model p is The distribution function p(x 1:T , v 1:T , z, k, c) is parametrized as generator neural networks.</p><formula xml:id="formula_37">p(x 1:T , v 1:T , z, k, c) = p(k)p(z | k)p(c) T t=1 p(v t | v t-1 )p(x t | z, v t , c) (4.</formula><p>It consists of three parts:</p><formula xml:id="formula_38">z p = G 1 (k p ), v p t+1 = G 2 (v p t , ϵ t ), ϵ t ∼ N (0, I), and x p t = G 3 (z, v p t , c).</formula><p>G 1 is responsible for a mapping from the input prior to a mixed Gaussian distribution with respect to k p . G 2 transitions to a new state v p t given the previous state. G 3 uses noise z p , state v p t , and condition c to generate the synthetic δ-second EEG signal x p t .</p><p>The joint distribution of the inverse inference model q is</p><formula xml:id="formula_39">q(x 1:T , v 1:T , z, k, c) = q(x 1:T )q(z | x 1:T , c)q(k | z) T t=1 q(v t | x t ) (4.3)</formula><p>where each latent variable of the Markov structure is assumed to be independent using the mean-field approximation <ref type="bibr" target="#b35">(Jordan et al., 1999)</ref>. q(x 1:T ) is the empirical data distribution, q(z | x 1:T , c), q(v t | x t ), and q(k | z) are of interest for the inference. Contrary to p(v t+1 | v t ), q(v t | x t ) models a dynamic tracing procedure for reconstructing the hidden features v t . In contrast to p(z | k), q(k | z) models a component tracing procedure for reconstructing the Gaussian mixture indicator k.</p><p>The distribution function q(x 1:T , v 1:T , z, k, c) is parametrized as extractor neural networks.</p><p>It consists of three parts: z q = E 1 (x q 1:T , c), v q t = E 2 (x q t ), and k q = E 3 (z q ). E 1 and E 2 are responsible for a mapping from original signals to noise z q and state v q t , respectively. E 3 infers within the latent space from z q to k q .</p><p>Learning Process</p><p>Our goal is to learn the parameters of the generative model p and the inverse inference model q by jointly minimizing the Jensen-Shannon (JS) divergence</p><formula xml:id="formula_40">JS(q(x 1:T , v 1:T , z, k, c)∥p(x 1:T , v 1:T , z, k, c))<label>(4.4)</label></formula><p>Expectation Propagation (EP) <ref type="bibr" target="#b47">(Minka, 2013)</ref>, a deterministic approximation algorithm, is proposed to utilize the locally structured data following <ref type="bibr" target="#b43">(Li et al., 2018)</ref>. The joint distributions can be factorized in terms of a set of factors</p><formula xml:id="formula_41">F G = {(k, z) , (v t , v t-1 ) , (x t , v t , z, c)}.</formula><p>For a factor a, the divergence of interest is</p><formula xml:id="formula_42">JS (q(a) b̸ =a q(b)∥p(a) b̸ =a p(b)) (4.5)</formula><p>EP iteratively minimize a local divergence in terms of each factor individually with the assumption that b̸ =a q(b) ≈ b̸ =a p(b). The divergence becomes</p><formula xml:id="formula_43">JS (q(a) b̸ =a q(b)∥p(a) b̸ =a q(b)) (4.6)</formula><p>Using the same proof sketch as in <ref type="bibr" target="#b43">(Li et al., 2018)</ref>, the divergence for factor a is approximated as</p><formula xml:id="formula_44">JS (q(x 1:T , v 1:T , z, k, c)∥p(x 1:T , v 1:T , z, k, c)) ≈ E q log 2q(a) p(a) + q(a) + E p log 2p(a) p(a) + q(a) (4.7)</formula><p>The divergences are further averaged over all local factors as</p><formula xml:id="formula_45">1 |F G |   E q   a∈F G log 2q(a) p(a) + q(a)   + E p   a∈F G log 2p(a) p(a) + q(a)     (4.8)</formula><p>Individual parametric discriminators D a can be employed to estimate the local divergences as follows</p><formula xml:id="formula_46">max ψ 1 |F G | E q   a∈F G log (D a (a))   + 1 |F G | E p   a∈F G log (1 -D a (a))   (4.9)</formula><p>where ψ denotes the parameters in all discriminators. The discriminative models distinguish between the variables from the generative model p and those from the inverse inference model q as synthetic and original, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Objective</head><p>Three discriminators D 3 , D 2 and D 1 receive local variable pairs, i.e., (k, z), (v t , v t-1 ), (x t , v t , z, c), from either the generative model p or the inverse inference model q, separately.</p><p>The adversarial loss is as follows In order to ensure the global consistency of an entire signal across time steps, a frequency domain loss is added as</p><formula xml:id="formula_47">L GAN (G * , E * , D * ) = E q log D 3 (k q , z q ) + log D 2 v q t , v q t-1 + log D 1 (x q t , v q t , z q , c) + E p log (1 -D 3 (k p , z p )) + log 1 -D 2 v p t , v p t-1 + log (1 -D 1 (x p t , v p t , z p , c))]</formula><formula xml:id="formula_48">L f (G * ) = ∥r(x q i,1:T ) -r(x p i,1:T )∥ 1 + ∥ φ(x q i,1:T ) -φ(x p i,1:T )∥ 1 (4.11)</formula><p>where r and φ refer to the average magnitude and phase across signals i in a batch, respectively. They are computed by a fast Fourier transform (FFT). Hence, the total objective is min</p><formula xml:id="formula_49">G * ,E * max D * L GAN + λL f (4.12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Network Architectures and Training Hyperparameters</head><p>Table <ref type="table" target="#tab_8">4</ref>.1 presents the architectures of the deep neural networks. Each time step corresponds to a 1-second EEG signal (δ = 1). All the feature maps have 96 channels. Leaky ReLU activation functions are applied to all layers, with the slope 0.1 to stimulate easier gradient  <ref type="bibr" target="#b26">(Ioffe and Szegedy, 2015)</ref> are used at each convolutional layer of the generators and extractors. Spectral normalizations (SN) <ref type="bibr" target="#b48">(Miyato et al., 2018)</ref> are applied to the discriminators to constrain their Lipschitz constants. c are subject embeddings as one-hot vectors. The sizes of z, k, and v t , and ϵ t are set at 128, 6, 32, and 16 respectively.</p><p>G 1 and E 2 are single-layer neural networks. We use the reparameterization trick <ref type="bibr" target="#b37">(Kingma and Welling, 2013)</ref> to estimate the gradients with the continuous variable z, and the Gumbel-Softmax trick <ref type="bibr" target="#b29">(Jang et al., 2016)</ref> (the temperature of 0.1) to estimate the gradients with the discrete variable k.</p><p>λ is set at 0.1 to have the training process driven mainly by the adversarial loss. In order to mitigate the issue of slow learning in regularized discriminators, a higher learning rate is provided to the discriminators than the generators and extractors by the Two Timescale Update Rule (TTUR) <ref type="bibr" target="#b21">(Heusel et al., 2017)</ref>. The models are trained with the Adam optimizer with the initial learning rate of 0.0004 for D * , the learning rate of 0.0001 for G * and E * , and the exponential decay rates β 1 = 0.5 and β 2 = 0.999. All weights are initialized using a zero-centered Gaussian distribution with a standard deviation of 0.02. We make the implementation publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Dataset</head><p>The 23-channel interictal EEG recordings from the CHB-MIT epilepsy dataset <ref type="bibr" target="#b67">(Shoeb, 2009)</ref> are used for the experiments. The dataset consists of scalp EEG from pediatric subjects with intractable seizures. We select a subset of 6 patients (chb01-03, chb05-06, chb10) having the same measurement setup, including males and females, 1.5-14 years old. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation Metrics</head><p>Sliced 2-Wasserstein distance (SWD) <ref type="bibr" target="#b8">(Bonneel et al., 2015;</ref><ref type="bibr" target="#b15">Flamary et al., 2021)</ref> quantifies the cost of transforming one distribution to another. It is an approximation to the 2-1 <ref type="url" target="https://github.com/khuongav/Graphical-Adversarial-Modeling-of-EEG">https://github.com/khuongav/Graphical-Adversarial-Modeling-of-EEG</ref> 54 Wasserstein distance using 1D projections for a closed-form solution and is defined as</p><formula xml:id="formula_50">SWD 2 (µ, ν) = E θ∼U (S d-1 ) W 2 2 (θ # µ, θ # ν) 1 2 (4.13)</formula><p>where µ and ν are two probability measures, θ # µ stands for the pushforwards of the projection R d ∋ X → ⟨θ, X⟩, and U S d-1 is the uniform distribution on the hypersphere of d dimensions.</p><p>Spectral entropy (SEN) measures the uniformity the of signal energy distribution in the frequency-domain. It is given by</p><formula xml:id="formula_51">H(x) = - fs/2 f =0 P (f ) log 2 [P (f )] (4.14)</formula><p>where P is the normalised power spectral density, and f s is the sampling frequency of signal</p><p>x.</p><p>Reconstruction error (REC) measures the differences between the values of an original signal and its reconstruction x as</p><formula xml:id="formula_52">REC = ∥x q 1:T -xq 1:T ∥ 1 (4.15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results and Discussion</head><p>Table <ref type="table" target="#tab_8">4</ref>.2 presents the performance of our proposed approaches and the comparison with the BiGAN/ALI model <ref type="bibr" target="#b14">(Dumoulin et al., 2016;</ref><ref type="bibr" target="#b13">Donahue et al., 2016)</ref>. We denote its conditional version as C-BiGAN/ALI. GMMarkov-GAN is our model characterized by Gaussian mixture and Markov latent structures, while Markov-GAN is only with the Markov structure. C-BiGAN/ALI is the GAN with an inference capability but without a latent variable structure,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusion</head><p>In this work, we proposed an EEG modeling scheme that combines the strengths of probabilistic graphical models and generative adversarial networks. Our experimental results demonstrate that our method effectively characterized EEG latent variable structure via a</p><p>Gaussian mixture and a Markov model. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. Our approach holds promise to new generative applications in neuroscience and neurology.</p><p>Future directions include generalizing learning and inference algorithms with more complicated structures to truly model the underlying relationships at different scales spanning from the single cell spike train up to macroscopic oscillations.</p><p>signals and behavioral measures. This joint latent space model is a valuable new framework for computational cognitive neuroscience, allowing for new forms of inference and hypothesis generation.</p><p>Previous work has focused on neurocognitive relationships between human neural data and behavioral data in decision-making tasks <ref type="bibr" target="#b56">(Nunez et al., 2015</ref><ref type="bibr" target="#b57">(Nunez et al., , 2017</ref><ref type="bibr" target="#b55">(Nunez et al., , 2019;;</ref><ref type="bibr" target="#b45">Lui et al., 2021;</ref><ref type="bibr">Turner et al., 2013</ref><ref type="bibr">Turner et al., , 2016))</ref>. The hierarchical Bayesian models used in these projects make strong predictions about the relationships between brain activity and the speed of decisionmaking. These models typically make use of the drift-diffusion model (DDM; <ref type="bibr" target="#b62">Ratcliff and McKoon, 2008)</ref>  <ref type="bibr">et al., 2016;</ref><ref type="bibr" target="#b57">Nunez et al., 2017)</ref>. This can be thought of as one domain of the larger field of model-based cognitive neuroscience <ref type="bibr" target="#b16">(Forstmann and Wagenmakers, 2015)</ref>.</p><p>A limitation of this approach is that we must know in advance which brain signals are possibly linked to cognitive functions. However, advances in frameworks and tools for neuroscience allow for the discovery of previously unknown neural features that we could use to explain   Nevertheless, estimating p θ (x) is typically intractable. This issue can be mitigated by introducing a parametric inference model q ϕ (z | x) to construct a variational evidence lower bound on the log-likelihood log p θ (x) as follows:</p><formula xml:id="formula_53">L(x; θ, ϕ) def = log p θ (x) -KL (q ϕ (z | x)∥p θ (z | x)) = E q ϕ (z|x) [log p θ (x | z)] -KL (q ϕ (z | x)∥p(z))</formula><p>(5.2)</p><p>Taking the likelihood model p θ (x | z) to be a decoder and the inference model q ϕ (z | x)</p><p>to be an encoder, a variational autoencoder (VAE; <ref type="bibr" target="#b37">Kingma and Welling, 2013;</ref><ref type="bibr">Sohn et al., 2015)</ref> considers this objective from a deep probabilistic autoencoder perspective. Here, θ and ϕ are neural network parameters, and learning takes place via stochastic gradient ascent using unbiased estimates of ∇ θ,ϕ</p><formula xml:id="formula_54">1 n n i=1 L (x i ; θ, ϕ).</formula><p>In the following sections, we extend the traditional VAE to create the Neurocognitive VAE (NCVA) (Figure <ref type="figure" target="#fig_22">5</ref>.1). This model allows us to model a joint distribution of neural and behavioral data. Instead of a training technique that encourages disentanglement, as in β-VAE <ref type="bibr" target="#b22">(Higgins et al., 2016)</ref>, NCVA imposes restrictions on latent space by using a cognitive model that provides interpretability and controllable generation.</p><p>Fuss, 2009) corresponding to the lower boundary:</p><formula xml:id="formula_55">p(y|z C ) = Wiener (RT | α, τ, δ) = π α 2 e -1 2 (αδ+δ 2 (RT -τ )) × +∞ k=1 k sin πk 2 e -k 2 π 2 2α 2 (RT -τ )</formula><p>(5.5)</p><p>The probability at the upper boundary is obtained by setting δ ′ = -δ. z C comprises of three parameters including drift rate δ, boundary α, non-decision time (ndt) τ . The bias towards correct or incorrect responses is fixed at 0.5, that is, the starting point is always unbiased.</p><p>The joint inference is performed using only EEG x to ensure that encoder θ C would learn to extract neural features that are tailored to cognitive parameters, without relying on choice-RT y. This has the advantage of providing more accurate trial-level parameter estimates that are associated with the EEG data.</p><p>Note that the dimension of the cognitive space is significantly lower than that of the residual neural space. This facilitates the representation of the variation in neural signals only through flexible z N . Maximizing the likelihood of observing neural signals does not guarantee decoder θ utilizing z C to output x. In the next section, we present an approach to capture the correlation between behavior and cognition, as well as the mapping of the variability of behavior and cognition to neural signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Structured EEG Modeling from Behavior</head><p>Here, we propose a discriminative model regularized by the generative model learned in the previous section. We aim to discriminatively learn the distribution of the cognitive parameters conditioned on behaviors, and the distribution of the neural latent variables</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">EEG and Behavioral Dataset</head><p>We used behavioral and EEG data collected while participants performed a two-alternative forced-choice task where they had to decide whether a Gabor patch presented with added dynamic noise is higher or lower spatial frequency (for details, see Experiment 2 by <ref type="bibr" target="#b55">Nunez et al., 2019)</ref>. Task difficulty was manipulated by adding spatial white noise to manipulate the quality of the perceptual evidence available to make the discrimination. The signal and the noise flickered at 30 and 40 Hz frequencies, respectively. 4 participants performed the task in blocks of trials at 3 added noise levels (low, medium, and high). Each subject performed approximately 3000 trials over 7 experimental sessions, while 128 channels of EEG and behavioral data were recorded. The independent component analysis (ICA)-based artifact rejection method was used on EEG data to remove eyeblinks, electrical noise, and muscle artifacts. A subset of 98 EEG channels were selected, excluding channels located in the outer ring. EEG data were bandpass filtered to 1 to 45 Hz in the frequency domain and then downsampled from 1000 Hz to 250 Hz in the time domain prior to data analysis. The data for each subject were divided into 80% for training and validation and the remaining 20% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results</head><p>To validate the neurocognitive modeling approach, we first examine the trial-by-trial variability of the parameters within each subject and the generalization of the model to unseen   the 30 Hz peak is reduced and the 40 Hz peak is enhanced. In terms of ERP waveforms, the model captures the relationships of the N200 peak latencies with respect to the additive noise conditions. Higher additive white noise in the stimulus effectively increases the latency and decreases the amplitude of the N200. We focus on the N200 signal because the original study <ref type="bibr" target="#b55">(Nunez et al., 2019)</ref> found strong relationships between N200 latency and choice-RT, and thus the N200 is a good validation of our model. These prove the convergence of the model in optimizing the lower bound of the conditional likelihood mapping from behavioral data to EEG features, which effectively encodes differences in the stimuli presented to the subjects in the latent variable space.</p><p>In addition to evaluating traditional ERP estimates (trial-averaged), we also assess the singletrial ERP estimate (channel-averaged). To increase the signal-to-noise ratio to better detect the N200, the first singular-value decomposition (SVD) component obtained from the ERP response is taken as a channel weighting function. More details of the SVD method can be seen at <ref type="bibr" target="#b55">(Nunez et al., 2019)</ref>.       to the aforementioned published joint models <ref type="bibr" target="#b56">(Nunez et al., 2015</ref><ref type="bibr" target="#b57">(Nunez et al., , 2017</ref><ref type="bibr" target="#b55">(Nunez et al., , 2019;;</ref><ref type="bibr" target="#b45">Lui et al., 2021;</ref><ref type="bibr">Turner et al., 2013</ref><ref type="bibr">Turner et al., , 2016))</ref>, our end-to-end model is capable of inferring task-relevant EEG features from behavior without prior knowledge of which features to optimize. The structured latent space allows the learning of behavioral variability to drive the EEG data generation process, leading to the prediction of the structure of EEG features in relation to the stimuli used in the experiments (N200 and SSVEP) and the behavioral performance (choice-RT). In addition, the model allows us to directly map the variability of cognitive parameters to neural signals, allowing for theoretical predictions that guide future experimental studies. It should be noted that our framework does not serve to refine the functional form of process-oriented computational models. Instead, it presumes a set of fixed assumptions;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Conclusion</head><p>in the DDM, a constant drift rate and boundary separation within trials. Importantly, our framework can be generalized to encompass any other neural measures combined with any cognitive model to explain behavior, provided that the cognitive model expresses a closedform likelihood of behavioral data. Importantly, by parameterizing the likelihood by a deep neural network receiving neural data as input, trial-level parameter inferences are made possible. In this research, we assume a DDM posterior with a diagonal covariance matrix.</p><p>This could lead to an overestimation of the variance of the marginal posteriors if the true posterior has dependencies. It would be beneficial to investigate the use of a full covariance matrix as an alternative. It is important to mention that our validation process focused on correct responses. Due to the low number of incorrect responses compared to correct ones, we lack confidence in interpreting the results in this study for the incorrect trials, although the direction of the trial-level parameter fits was consistent with the results for correct trials.</p><p>We anticipate future research to explore strategies to address the class imbalance problem in deep learning models <ref type="bibr" target="#b33">(Johnson and Khoshgoftaar, 2019)</ref>. Further work with a larger dataset is needed to demonstrate that we can extend the model to new individuals. In principle, this would potentially allow us to predict brain activity in clinical populations with known behavioral differences.</p><p>information and was associated with choice and response time.</p><p>It is evident from the results in Figure <ref type="figure" target="#fig_22">5</ref>.9 that the model can accurately recover the original distributions of trial-specific parameters. In particular, the generating and recovered distributions strongly overlap, and the correlation plots indicate that our single-trial estimates of cognitive parameters exhibit good correlations with the reference parameters.   In this dissertation, we have constructed integrated frameworks that combine concepts from latent variable models, state-space models, and deep learning to model multidimensional dependencies in physiological signals. These frameworks are capable of modeling complex data distributions across various applications. This is accomplished through the development of probabilistic models that utilize deep neural networks to parameterize the underlying conditional distributions. Deep learning architectures serve as powerful function approximators, enabling the model to automatically extract features essential for wide applicability. Recent developments in deep learning can be seamlessly integrated into this framework.</p><p>The approaches proposed in this dissertation offer versatile frameworks for representing and learning from diverse types of physiological measures. By efficiently processing unlabeled datasets, these models enable the discovery of hidden structures and patterns, facilitating data-driven hypothesis generation:</p><p>1. Deep State-Space Model for Heart Electrical Waveforms <ref type="bibr" target="#b68">(Vo et al., 2023)</ref>. This application has significant potential for clinical diagnoses, especially since it allows for heart disease assessment through wearable devices. The use of optically obtained signals as inputs adds to the innovation, potentially making diagnosis simpler and more accessible.</p><p>2. Brain Signal Modeling with Probabilistic Graphical Models and Deep Adversarial Learning <ref type="bibr" target="#b71">(Vo et al., 2022)</ref>. Combining these two approaches is promising for encoding neural oscillations' complexity while maintaining interpretability. Moreover, applying these techniques to epilepsy seizure detection as an unsupervised learning problem could lead to earlier and more accurate diagnoses, improving patient outcomes.</p><p>3. Joint Modeling of Physiological Measures and Behavior <ref type="bibr" target="#b70">(Vo et al., 2024)</ref>. This approach shows potential in tackling the modern challenge in amalgamation of diverse medical data sources. By analyzing the relationship between physiological measures and behavior, our method could uncover new insights into brain function and potentially revolutionize our understanding of neurocognitive processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Work</head><p>Every progression in a field brings forth a set of unanswered queries, usually more complex than the ones preceding them. Regarding the deep latent variable models (DLVMs) presented in this thesis, several open questions exist, the resolution of which could enhance our comprehension of their working principles as well as to better exploit their modeling power.</p><p>• Ensuring patient safety when implementing medical machine learning methods for clinical applications necessitates robust models. A key characteristic of robust models is their resilience to out-of-distribution (OOD) data, meaning they can still provide accurate predictions when encountering data that differ from the training set. Such OOD data might include samples from varied patient demographics, different medical equipment and laboratory methods. Future research should focus on assessing the adaptability of the DLVMs to OOD data and enhancing model resilience against such data by leveraging established strategies in the field <ref type="bibr" target="#b78">(Zhou et al., 2022;</ref><ref type="bibr" target="#b73">Wang et al., 2022)</ref>.</p><p>• Probabilistic graphical models offer a principled approach to incorporating prior knowledge and structured frameworks into the model, utilizing current message-passing algorithms for approximate inference. A crucial yet difficult task for broader adoption of DLVMs is developing a message-passing library that seamlessly works with existing deep learning libraries <ref type="bibr" target="#b34">(Johnson et al., 2016b;</ref><ref type="bibr" target="#b7">Bendekgey et al., 2024)</ref>.</p><p>• Choosing the optimal model parameterization for a specific application can be challenging. This category of models derives all the complexities associated with defining the precise network architecture, such as the number of layers, units, and activation functions, from its deep learning components. Therefore, identifying a systematic method for hyperparameter optimization that is effective across different applications is essential <ref type="bibr" target="#b75">(Yu and Zhu, 2020;</ref><ref type="bibr" target="#b20">He et al., 2021)</ref>.</p><p>• Recent progress in physics-informed deep learning <ref type="bibr" target="#b61">(Raissi et al., 2019;</ref><ref type="bibr" target="#b51">Nabian and Meidani, 2020)</ref> integrates the advantages of deep learning methods with physical principles to improve both model efficacy and generalization. In this approach, deep learning models are enhanced with a regularization term that serves as prior knowledge, reflecting the fundamental laws and penalizing deviations from these governing equations.</p><p>Investigating DLVM approaches that adhere to any specified law of electrophysics, as characterized by stochastic differential equations, would be advantageous.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Examples of the translated ECG signals. In each subfigure: the top panel shows the input PPG waveform and the bottom panel shows the reconstructed ECG waveform compared with the reference waveform. The average ECG waveform (dark blue) of all possible pulses overlaid on each individual pulse (light blue). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Directed graphical models for EEG signal modeling. Each time step corresponds to a δ-second multi-channel signal. Shaded nodes represent observed variables. Clear nodes represent latent variables. Directed edges indicate statistical dependencies between variables. . . . . . . . . . . . . . . . . . . . 4.2 Last 10-second of a 30-second synthetic 23-channel EEG signal by the GMMarkov-GAN model, conditioned on patient 3. 5 channels with the highest standard deviations are shown. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 t-SNE visualization of the static latent spaces. . . . . . . . . . . . . . . . . . 4.4 ROC curve for epilepsy seisure detection. . . . . . . . . . . . . . . . . . . . . v 5.1 The Neurocognitive VAE. After the generative process (a) learns the joint latent neurocognitive variables (Section 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>I</head><label></label><figDesc>am grateful to the National Science Foundation (NSF) and the National Institutes of Health (NIH) for providing the financial support that enabled me to pursue and complete this research. Without their generosity, this work would not have been possible. Funding resources: NSF grant #1917105, #1658303, #1850849, #2051186, #2126976, and NIH SBIR grant #R44OD024874. joint cognitive modeling of neural signals and human behavior NeuroImage A novel wireless ECG system for prolonged monitoring of multiple zebrafish for heart disease and drug screening studies Biosensors and Bioelectronics Deep learning-based framework for cardiac function assessment in embryonic zebrafish from heart beating videos Computers in Biology and Medicine Continuous non-invasive blood pressure monitoring: a methodological review on measurement techniques IEEE Access An efficient and robust deep learning method with 1-D octave convolution to extract fetal electrocardiogram Sensors REFEREED CONFERENCE PUBLICATIONS PPG-to-ECG signal translation for continuous atrial fibrillation detection via attention-based deep state-space modeling International Conference of the IEEE Engineering in Medicine and Biology Society Composing graphical models with generative adversarial networks for EEG signal modeling IEEE International Conference on Acoustics, Speech and Signal Processing Decision SincNet: Neurocognitive models of decision making that predict cognitive processes from neural signals International Joint Conference on Neural Networks P2E-WGAN: ECG waveform synthesis from PPG with conditional Wasserstein generative adversarial networks ACM Symposium on Applied Computing STINT: Selective transmission for low-energy physiological monitoring ACM/IEEE International Symposium on Low Power Electronics and Design xii</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2.1: Examples of different probabilistic graphical models.</figDesc><graphic coords="26,228.78,482.48,154.44,154.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on their shared dependencies. Consider a simple scenario represented by a DGM where variables A and B capture potential causes of chest pain (variable C): heart attack and indigestion, respectively, as shown in the graph A → C ← B. As the doctors gather more information about the patient's symptoms and medical history, they can start to narrow down the potential causes. For example, if the patient also shows symptoms like sweating and shortness of breath, which are commonly associated with heart attacks, this evidence increases the likelihood of a heart attack being the cause of the chest pain. Consequently, the probability of indigestion decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The measurement of the electrical activity generated by an individual's heart, known as an electrocardiogram (ECG), typically requires the placement of several electrodes on the body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>Figure 3.1: A PPG-ECG waveform pair. PPG signals can often become contaminated by noise.</figDesc><graphic coords="41,90.00,434.93,432.00,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Few</head><label></label><figDesc>research works attempted to synthesize ECG from PPG signals. In<ref type="bibr" target="#b4">(Banerjee et al., 2014)</ref>, a machine learning-based approach was proposed to estimate the ECG parameters, including the RR, PR, QRS, and QT intervals, using features from the time and frequency domain extracted from a fingertip PPG signal. Additionally,<ref type="bibr" target="#b79">Zhu et al. (2019)</ref>; Tian et al.(2020) proposed models to reconstruct the entire ECG signal from PPG in the frequency domain. However, the performance of these approaches relied on cumbersome algorithms for feature crafting. With recent advances in deep learning,<ref type="bibr" target="#b69">Vo et al. (2021)</ref>;<ref type="bibr" target="#b66">Sarkar and Etemad (2021)</ref>;Chiu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3.2.1 Probabilistic Modeling of ECG from PPG signalsWe are given a dataset D := (x 1 , y 1 ) , . . . , x N , y N with the i-th observation y i ∈ R ny , i.e., ECG signals of n y time samples, depending on x i ∈ R nx , i.e., PPG signals of n x time samples. Throughout the paper, superscript i is omitted when we refer to only one sequence or when it is clear from the context. We aim to learn a generative process with a latent-variable model comprising of a parametric non-linear Gaussian prior over latents p θz (z | x) and likelihood p θy (y | z, x). The learning process minimizes a divergence between the true data-generating distribution and the model w.r.t θ: arg min θ KL (p D (y | x)∥p θ (y | x)) = arg max θ E p D (y|x) [log p θ (y | x)] (3.1) where p θ (y | x) = p θy (y | z, x) p θz (z | x) dz is the conditional likelihood/evidence of data point y given condition x, approximated by averaging over the latent z. Nevertheless, estimating p θ (y | x) is typically intractable. This issue can be mitigated by introducing a parametric inference model q ϕ (z | x, y) to construct a conditional variational evidence lower bound on the conditional log-likelihood log p θ (y | x) as follows L(x, y; θ, ϕ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Figure 3.2: The graphical model for ECG translation from PPG. Shaded nodes represent observed variables. Clear nodes represent latent variables. Diamond nodes denote deterministic variables. Variables x t , y t , and c t represent PP intervals, RR intervals, and context vectors, respectively. α t,i are attention weights defines how well two intervals x i and y t are aligned. The attention mechanism is shown only at time step 2.</figDesc><graphic coords="46,164.25,124.14,283.50,198.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 . 3 :</head><label>33</label><figDesc>Figure 3.3: The graphical model at latent state inference time. Variables y t , h t , g t , and z t represent respectively RR intervals, backward, forward recurrent states, and latent states.</figDesc><graphic coords="47,149.40,415.66,313.20,185.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>ReLU (W e 2 ReLU (W e 1 z t + b e 1 ) + b e 2 ) + b e 3 (3.15) jects and 12 subjects having AFib, including 30 males and 25 females, 23-84 years old. The dataset is made publicly available 1 . Each record duration is 5 minutes. The first 48 s of each record were used as the training set, the next 12 s as the validation set, and the remaining 228 s as the test set. The preprocessing steps, including filtering, alignment, and normalization, were performed as described in(Tang et al., 2022). We applied HeartPy(Van Gent et al.,   2019; van Gent et al., 2019)  to identify peaks in PPG signals. Each long signal is split into 4-s chunks. Each peak-to-peak interval was linearly interpolated to a length of 90 during training, which is the mean length of the intervals in the training set. The original interval length information can be preserved by making it an additional feature along with each normalized interval. Alternatively, we can apply padding instead of interpolation. However, we found that these did not contribute to improving the performance under the experimental setting. Original PP interval lengths were used as RR interval lengths in translated ECG signals during testing. This can be justified, as PPG recordings are used to analyze heart rate variability as an alternative to ECG<ref type="bibr" target="#b44">(Lu et al., 2009;</ref><ref type="bibr" target="#b1">Aschbacher et al., 2020)</ref>. Noise was added to the signals for robustness evaluation. The amplitudes of the baseline noise signals are 0.3, 0.4, and 0.1, and the frequencies are 0.3, 0.2 and 0.9 Hz, respectively. Gaussian noise of standard deviation 0.3.3.3.2 Evaluation MetricsECG Translation from PPG Pearson's correlation coefficient (ρ) measures how much an original ECG signal y 1:T and its reconstruction ŷ1:T co-vary: ρ = (y 1:T -ȳ1:T ) ⊤ (ŷ 1:T -ȳ1:T ) ∥y 1:T -ȳ1:T ∥ 2 ∥ŷ 1:T -ȳ1:T ∥ 2 (3.17) Root Mean Squared Error (RMSE) measures the differences between the values of the original signal and its reconstruction: RMSE = ∥y 1:T -ŷ1:T ∥ 2 √ n y (3.18) Signal-to-Noise Ratio (SNR) compares the level of the desired signal to the level of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3</head><label>3</label><figDesc>Figure 3.4: Examples of the translated ECG signals. In each subfigure: the top panel shows the input PPG waveform and the bottom panel shows the reconstructed ECG waveform compared with the reference waveform. The average ECG waveform (dark blue) of all possible pulses overlaid on each individual pulse (light blue).</figDesc><graphic coords="57,87.60,463.53,210.60,166.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Figure 4.1: Directed graphical models for EEG signal modeling. Each time step corresponds to a δ-second multi-channel signal. Shaded nodes represent observed variables. Clear nodes represent latent variables. Directed edges indicate statistical dependencies between variables.</figDesc><graphic coords="61,72.00,322.07,351.00,109.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>2)where p(k) and p(c) are simple prior distributions for Gaussian mixture indicator k and condition c, e.g., a categorical distribution and a uniform distribution, p(z | k) models a component selecting procedure for sampling noise z which encodes the temporal-spatial relationships invariant across time, v t 's form a first-order Markov chain, with p(v 1 |v 0 ) ∼ N (0, I), to encodes the temporal relationships variant across time, p(x t | z, v t , c) specifies the conditional probability of the data at each time step t given noise z, state v t , and condition c, and is of interest for the final generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>trained simultaneously in an adversarial process. Let θ and ϕ denote the parameters of G * and E * , respectively. Iteratively, D * learn to maximize Equation 4.10 by updating ψ, while G * and E * learn to minimize Equation 4.10 by updating corresponding parameters θ and ϕ, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Interictal periods are extracted at least 4-hour away before a seizure onset and after the seizure ends. The signals are low-pass filtered with a cut-off frequency at 50 Hz and scaled to the range [-1, 1]. Overall, the dataset contains 43593 signals, from which 70% are used for training and validation, and the other 30% are used as the test set. Each signal is 10-second long (T=10), at a sampling rate of 256 Hz. Additionally, 339 ictal EEG signals are extracted for evaluating epilepsy seizure detection performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure 4.2: Last 10-second of a 30-second synthetic 23-channel EEG signal by the GMMarkov-GAN model, conditioned on patient 3. 5 channels with the highest standard deviations are shown.</figDesc><graphic coords="69,130.50,226.72,350.98,359.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4</head><label>4</label><figDesc>Figure 4.3: t-SNE visualization of the static latent spaces.</figDesc><graphic coords="71,153.90,295.33,304.21,202.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 4</head><label>4</label><figDesc>Figure 4.4: ROC curve for epilepsy seisure detection.</figDesc><graphic coords="72,153.90,72.00,304.22,199.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>, a widely-used cognitive model in decision-making, as their generative model of choice and reaction time data. To integrate neural signals, these models require knowledge of previously discovered features of the neural data (e.g., known functional signals in the cognitive neuroscience literature) that are then linked by prescribed (usually linear) relationships to the latent cognitive variables in a Bayesian hierarchical model. The resulting neurocognitive models test the relationship between neural signals and cognitive variables, and enhance the accuracy of predictions of behavior directly from brain signals (Turner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Consider first a data set P def =</head><label>def</label><figDesc>{D 1 , . . . , D M } containing M subjects, where each subject D m def = {x 1 , . . . , x I } consists of I trials x i ∈ R C×T that are EEG signals of C channels by T time samples. Throughout the paper, the subscript m is omitted when we refer to only one subject or when it is clear from the context.For each subject m, we aim to learn an EEG generative process with a latent-variable model comprising of a fixed Gaussian prior over latent variables p(z) = N (z | 0, I), where I is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 5</head><label>5</label><figDesc>Figure5.1: The Neurocognitive VAE. After the generative process (a) learns the joint latent neurocognitive variables (Section 5.2.2), the regularized discriminative process (b) retrofits its hierarchical latent space to the joint latent space (Section 5.2.3). Inference networks q and Generation networks p contain neural network parameters θ and ϕ. Black arrows: flows of operations. Red arrows: loss functions. MSE and WFPT stand for Mean Squared Error and Wiener First Passage Time, respectively. The heatmaps represent the probability distributions in the latent spaces. Plasma color maps are for the drift-diffusion variables (z C ∈ R 3 ), while greenery color maps are for residual neural variables (z N ∈ R 32 ). Blue blocks contain µ and σ, which are the parameters of the multivariate Gaussian latent spaces. Gray blocks contain z sampled (∼) from the distributions. The variables x and y represent EEG signals and choice-RTs, respectively. Each trapezoid represents a different convolutional neural network (see Table5.2 for detailed architectures).</figDesc><graphic coords="76,72.00,381.61,467.99,97.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>data. Figures 5.2a and 5.2c show the trial-by-trial correlations between estimated DDM posteriors and observed choice-RTs in the training data from neural signals and behavior, respectively. Spearman correlations between fitted drift rates (δ) and choice-RTs are nega-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.4a shows the average of signals generated by the neurocognitive autoencoder when given a set of approximately 800 test choice-RTs compared to the average of actual signals associated with the same choice-RTs. At the selected electrodes, the window of interest is 100 ms pre-stimulus to 500 ms post-stimulus, which captures the N200 waveform. The generated and original signals appear visually similar in the timing and amplitudes of the peaks and troughs. Figures 5.4b, 5.4c, and 5.4d depict the trial-averaged frequency spectra and corresponding ERP waveforms of the reconstructed signals. Regarding the frequency spectra, the most important features are the 30 and 40 Hz peaks, which correspond to the flicker frequency of the signal (Gabor patch) and spatial white noise, respectively. Interestingly, the generative model learns to structure output the steady-state visually evoked potentials (SSVEPs) that occur in response to a visual stimulus flickering at different frequencies, even though it was never explicitly encoded in the model. Moreover, in the low noise condition (b), the 30 Hz peak is large and the 40 Hz is small, while in the high noise condition (d),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Figure 5.6  shows the performance of the model in learning the N200 feature in each trial. As shown in Figure5.6, the distributions of the single-trial N200 peak latencies, as well as the amplitudes calculated from the generated signals, closely match those of the original signals at three different noise levels. The peak amplitude distribution is somewhat broader than the original data's generated distribution. Importantly, the model can generate the variability of the N200 latency with the experimental manipulation of low, medium, and high noise, systematically increasing the N200 latency in the generated signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 5 .</head><label>5</label><figDesc>Figure5.8 represents the sensitivity analysis of the choice-RT and drift-diffusion parameters regardless of the noise conditions. In the left column, we examine the sensitivity of the neural signals generated by the choice-RTs. We can see similar patterns across subjects where the increases in choice-RTs lead to significant declines in the 30 Hz and the rises of the N200 latencies. This confirms the minimization approach of the KL divergence between the latent spaces inferred from the behavioral data and the neural signals. Power at 40 Hz reflecting the neural response to the noise also changes according to the choice-RTs, though the pattern is not as strong as the subjects suppressed the noise signal in all conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>Figure 5.3: Drift-diffusion single-trial parameter estimations from correct responses of subject s1. The parameters are constrained by the subject priors resulting from a Bayesian MCMC modeling (without EEG data). Scatter plots illustrate the relationship between the parameters and the observed choice-RTs for each trial. The top two rows are posterior inferences from neural signals, while the bottom two are from behaviors. The left column shows the drift-rate (δ) estimates, the middle column shows boundary (α) estimates, and the right column presents non-decision time (ndt) estimates. The correlations between the choice-RTs and the inferred DDM parameters are consistent with what is expected. On top of each panel are the Spearman correlation coefficients (ρ). The covariances of the inferred parameters are indicated by circles, which correspond to contours having one standard deviation. For clarity, each circle is magnified 300 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Figure5.5: Performance of the model in reconstructing 98 EEG channels of subject s1 by averaging ≈ 800 predicted EEG trials from ≈ 800 choice-RTs in the test set. Time point zero denotes the time point of stimulus onset. The first row displays the original (blue) and generated (orange) trial-averaged EEG data at the pooled electrodes. The x-axis denotes the time in milliseconds from stimulus onset, and the y-axis denotes the signal amplitude. The second, third, and fourth rows are (left) frequency spectra and (right) EEG signals averaged over all test choice-RT trials (≈ 800/3 per condition). The signals on the right are low-pass filtered at 15 Hz for clarity of N200 peaks. Each colored line corresponds to one reconstructed EEG channel. In low-noise conditions, the spectra show a strong peak at the Gabor flicker frequency of 30 Hz, and the ERP waveform shows a shorter N200 latency and larger peak amplitude. Under high-noise conditions, the spectra show a strong peak at the noise flicker frequency of 40 Hz, and the ERP waveform shows a longer N200 latency and a smaller peak amplitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 5</head><label>5</label><figDesc>Figure 5.6: Performance of the model in reconstructing single-trial N200 peaks from choice-RTs in four subjects. The dotted lines are references to the original data. The distributions of (left) single-trial N200 peak latencies across three noise conditions and (right) the N200 peak amplitude statistics are shown. Single-trial observations of the peak latency of N200 are found using the SVD method (Nunez et al., 2019) for each subject and noise condition. 77</figDesc><graphic coords="90,140.25,530.72,327.55,118.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 5 . 8 :</head><label>58</label><figDesc>Figure 5.8: Sensitivity analysis of choice-RTs and latent drift-diffusion parameters on EEG signal generation in four subjects. The left column presents the effects of choice-RTs on the output neural signals. The blue bars represent the power at 30 Hz, while the red bars represent the power at 40 Hz. The orange bars show the N200 latencies. The middle column shows the changes in the single-trial N200 distribution w.r.t to hypothetical changes in the cognitive parameters. The yellow distribution represents the reference data, while the blue and red ones correspond to modified parameter settings that decrease or increase the N200 latencies, respectively. The modification in subject s4 (ndt ± 0.05, δ± 0.3) is different from other subjects. The right column characterizes the changes in 30 Hz and 40 Hz peaks w.r.t to the changes in the same cognitive parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 5</head><label>5</label><figDesc>Figure 5.9: Drift-diffusion parameter estimates from neural signals in a simulation of triallevel choice RTs and EEG signals. The top panels show the overlap between the recovered and the original distributions of trial-specific drift-rate and NDT. The reference values for the drift rate and NDT are drawn from the normal distributions N (1.5, 0.2) and N (0.3, 0.05), respectively. The bottom scatter plots illustrate the relationship between the recovered parameters and the original parameters each trial. ρ are the Spearman correlation coefficients.</figDesc><graphic coords="96,142.20,352.52,327.60,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 5 .</head><label>5</label><figDesc>Figure5.10: Example stimuli of the cue and response intervals of medium noise conditions<ref type="bibr" target="#b55">(Nunez et al., 2019)</ref>. In the response phase, participants identified the spatial-frequency target represented by each Gabor, using their left hand to press a button for a target with a low spatial frequency (2.4 cpd) and their right hand for a target with a high spatial frequency (2.6 cpd). N200 waveforms were calculated time-locked to the onset of the Gabor stimulus during the response intervals. The visual noise altered at a frequency of 40 Hz, while the Gabor signal modulated at 30 Hz, inducing 40 Hz and 30 Hz electrocortical responses that monitor attention to both noise and signal.</figDesc><graphic coords="97,141.12,148.04,329.76,167.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 5 .</head><label>5</label><figDesc>Figure5.11: The DDM is illustrated in action during a two-choice task, with non-decision time shown in green. Following the visual encoding period, the decision variable (DV) begins evidence accumulation and reaches either the upper or lower limit for each trial. The black vector depicts the average rate of evidence accumulation. The blue curve depicts the distribution of response times when choice 1 is successfully picked, while the red curve depicts the distribution of reaction times when choice 2 is correctly selected. When the DV drifts towards the incorrect boundary owing to random noise, incorrect decisions are made. The distribution of reaction times for incorrect trials is depicted by the dotted curve. EEG data for each trial, processed using singular value decomposition to highlight N200, is shown on top that track the start of evidence accumulation.</figDesc><graphic coords="99,126.00,72.00,360.00,288.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="91,72.00,102.67,467.97,115.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="91,72.00,253.13,467.97,115.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="91,72.00,403.59,467.97,115.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="91,72.00,554.04,467.97,115.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2.1 Examples of different probabilistic graphical models. . . . . . . . . . . . . . . 3.1 A PPG-ECG waveform pair. PPG signals can often become contaminated by noise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 The graphical model for ECG translation from PPG. Shaded nodes represent observed variables. Clear nodes represent latent variables. Diamond nodes denote deterministic variables. Variables x t , y t , and c t represent PP intervals, RR intervals, and context vectors, respectively. α t,i are attention weights defines how well two intervals x i and y t are aligned. The attention mechanism is shown only at time step 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 The graphical model at latent state inference time. Variables y t , h t , g t , and z t represent respectively RR intervals, backward, forward recurrent states, and latent states.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 viii 3.1 ECG translation performance of different models. The top three rows show models' performance on healthy subjects, while the fourth row shows the performance on both the healthy and AFib subjects. If not specified, healthy subjects and clean signals is the default setting. The LSTM model (Tang et al., 2022) is subject-dependent, while the P2E-WGAN (Vo et al., 2021) and our model are subject-independent. . . . . . . . . . . . . . . . . . . . . . 40 3.2 AFib detection performance. The performance on the translated ECG is evaluated when the MINA model (Hong et al., 2019) is trained on real ECG Neural network parametrization . . . . . . . . . . . . . . . . . . . . . . . . . 82 ix</figDesc><table /><note><p>but tested on synthetic ECG. The fusion performance is when the MINA model is extended to receive both real ECG and synthetic ECG inputs. x% random time samples are omitted, simulating intermittent ECG recording, while synthetic ECG is always available. . . . . . . . . . . . . . . . . . . . . 42 4.1 Network architectures. Models having similar architectures are grouped together. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.2 Performances of different GAN models in interictal EEG signal synthesis and reconstruction tasks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.1 Comparison of the sum of Wiener negative log-likelihood (-log Wiener (RT i | ω i )) of four subjects on the test sets. ω represents the median fitted cognitive parameters from the training set. . . . . . . . . . . . . . . . . . . . . . . . . 69 5.2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>about the generative process underlying the data. Probabilistic graphical models, such as Bayesian networks or Markov random fields, provide a principled way to encode dependencies among variables and incorporate domain knowledge into the joint distribution p(x, z).</figDesc><table><row><cell>This dissertation is centered on non-linear LVMs, specifically those utilizing deep neural</cell></row><row><cell>networks, termed Deep Latent Variable Models (DLVMs). DLVMs are adept at modeling</cell></row><row><cell>complex, high-dimensional data distributions, yet they necessitate approximate inference due</cell></row><row><cell>to the intractability of the integral in Equation (2.8), which lacks an analytic solution. Sub-</cell></row></table><note><p>. By capturing the essential characteristics of the data manifold, LVMs can effectively model the underlying data distribution while reducing the dimensionality of the representation. LVMs serve not only as black-box density models but also as interpretable frameworks for incor-porating prior knowledge sequent chapters will explore variational auto-encoders (VAEs) and generative adversarial networks (GANs), which combine principles from deep learning and latent variable models to create highly flexible distributions using deep neural networks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>.1 shows the performance of our model and compares it with other models in terms</cell></row><row><cell>of means and standard deviations of ρ, RMSE and SNR. The correlation between the signals</cell></row><row><cell>generated by our model and the reference signals is statistically strong, with a value ρ of</cell></row></table><note><p><p><p>0.858. Also, low values of RMSE (0.07) and high</p>SNR (15.365)  </p>show strong similarities between them and reference ECG signals. When the attention mechanism is not applied on</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>1: ECG translation performance of different models. The top three rows show models' performance on healthy subjects, while the fourth row shows the performance on both the healthy and AFib subjects. If not specified, healthy subjects and clean signals is the default setting. The LSTM model(Tang et al., 2022)  is subject-dependent, while the P2E-WGAN<ref type="bibr" target="#b69">(Vo et al., 2021)</ref> and our model are subject-independent. performance than our model (645,466), requiring almost six times the parameters. Our model is less affected when data is scarce, which is common in healthcare.On the other hand, the LSTM model(Tang et al., 2022) is a deep recurrent neural network that was also recently proposed and built separately for each subject. The performance of our model, trained in a cross-subject setting, surpassed that of the LSTM model trained separately for each subject. These results prove the effectiveness and efficiency of our proposed sequential data structure. Further work with a larger number of subjects having AFib is needed to demonstrate that we can extend the model to new individuals. In addition, exploring strategies to manage the class imbalance problem</figDesc><table><row><cell></cell><cell>Correlation</cell><cell>RMSE (mV)</cell><cell>SNR (dB)</cell></row><row><cell>ADSSM</cell><cell cols="3">0.858 ± 0.174 0.07 ± 0.047 15.365 ± 11.053</cell></row><row><cell>ADSSM</cell><cell></cell><cell></cell></row><row><cell>w/o attention</cell><cell cols="3">0.823 ± 0.194 0.08 ± 0.047 13.013 ± 10.537</cell></row><row><cell>ADSSM</cell><cell></cell><cell></cell></row><row><cell>(healthy sub.,</cell><cell></cell><cell></cell></row><row><cell>noisy sig.)</cell><cell cols="3">0.847 ± 0.174 0.076 ± 0.049 13.887 ± 10.58</cell></row><row><cell>ADSSM</cell><cell></cell><cell></cell></row><row><cell>(healthy &amp;</cell><cell></cell><cell></cell></row><row><cell>AFib sub.)</cell><cell cols="3">0.804 ± 0.22 0.078 ± 0.05 12.261 ± 11.328</cell></row><row><cell>P2E-WGAN</cell><cell cols="3">0.773 ± 0.242 0.091 ± 0.052 9.616 ± 9.252</cell></row></table><note><p><p>LSTM</p>(sub. dependent) 0.766 ± 0.234 0.093 ± 0.053 8.189 ± 9.560 the input PPG, there is a notable decline in performance, with ρ falling to 0.823, RMSE increasing to 0.08, and SNR decreasing to 13.013. This underscores the importance of the mechanism in providing relevant contexts for translation. The third row shows our model's performance on the noisy dataset. The negligible drop in metrics from 0.858 to 0.847 (ρ), 0.07 to 0.76 (RMSE), and 15.365 to 13.887 (SNR) demonstrates the robustness of our model. We attribute this to the probabilistic nature of the model, which better handles the measurement noise. As expected, the model performed worse on subjects with AFib due to the erratic patterns of the AFib signals (no visible P waves and an irregularly irregular QRS complex). network (4,064,769 parameters) for signal-to-signal translation, was recently proposed to translate PPG into ECG signals from a large number of subjects. P2E-WGAN achieved significantly lower</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>2: AFib detection performance. The performance on the translated ECG is evaluated when the MINA model<ref type="bibr" target="#b24">(Hong et al., 2019)</ref> is trained on real ECG but tested on synthetic ECG. The fusion performance is when the MINA model is extended to receive both real ECG and synthetic ECG inputs. x% random time samples are omitted, simulating intermittent ECG recording, while synthetic ECG is always available.</figDesc><table><row><cell></cell><cell>Real ECG</cell><cell>Translated ECG</cell><cell></cell></row><row><cell cols="2">ROC-AUC 0.995 ± 0.006</cell><cell>0.99 ± 0.004</cell><cell></cell></row><row><cell cols="2">PR-AUC 0.987 ± 0.013</cell><cell>0.986 ± 0.007</cell><cell></cell></row><row><cell>F1</cell><cell>0.985 ± 0.009</cell><cell>0.944 ± 0.014</cell><cell></cell></row><row><cell>Fusion</cell><cell>30% missing</cell><cell>50% missing</cell><cell>70% missing</cell></row><row><cell cols="2">ROC-AUC 0.992 ± 0.006</cell><cell>0.99 ± 0.006</cell><cell>0.99 ± 0.009</cell></row><row><cell cols="2">PR-AUC 0.986 ± 0.011</cell><cell>0.982 ± 0.012</cell><cell>0.981 ± 0.016</cell></row><row><cell>F1</cell><cell>0.971 ± 0.01</cell><cell>0.969 ± 0.012</cell><cell>0.956 ± 0.046</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table3.2 reveals the mean detection performance of the model in the translated ECG that is close to that of the real ECG, ROC-AUC of 0.99 vs. As shown in the bottom results of Table3.2, the performance remains almost unchanged in the fusion mode across the omission thresholds. Additionally, the model learns to utilize the sparse real ECG to marginally improve performance against only the translated ECG.</figDesc><table><row><cell>0.995, PR-AUC of 0.986 vs. 0.987, and F1 of 0.944 vs. 0.985. This implies that our model</cell></row><row><cell>allows for the combined advantages of ECG's rich knowledge base and PPG's continuous</cell></row><row><cell>measurement.</cell></row><row><cell>Furthermore, we extended the ability of the MINA model to receive real and translated ECG</cell></row><row><cell>signals by incorporating the translated frequency channels into the model. In this scenario,</cell></row><row><cell>both ECG and PPG signals can be measured simultaneously. This setting requires retraining</cell></row><row><cell>of the MINA model on the fused real and synthetic ECG signal data set. To simulate the</cell></row></table><note><p>real-life setting where ECG measurement is intermittent while PPG input is continuous, we randomly zeroed out time samples with different probabilities: 30%, 50%, and 70%.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>this work, we present a novel attention-based deep state-space model to generate ECG waveforms with PPG signals as input. The results demonstrate that our model has the potential to provide a paradigm shift in telemedicine by bringing about ECG-based clinical diagnoses of heart disease via simple PPG assessment through wearable devices. Our model,</figDesc><table /><note><p>trained on a small and noisy dataset, achieves an average Pearson's correlation of 0.847, RMSE of 0.076 mV, and SNR of 13.887 dB, demonstrating the efficacy of our approach. Significantly, our model enables the AFib monitoring capability in a continuous setting, assisting a state-of-the-art AFib detection model to achieve a PR-AUC of 0.986. Being a lightweight method also facilitates its deployment on resource-constrained devices. In our future work, we aim to validate the generalizability of the model with other pairs of physiological signals.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>1: Network architectures. Models having similar architectures are grouped together.</figDesc><table><row><cell>G 2 , D 3 , D 2</cell><cell>G 3</cell></row><row><cell cols="2">Linear 512, (SN), lReLU Linear 1536, lReLU</cell></row><row><cell>Linear 512, (SN), lReLU</cell><cell>Reshape 96x16</cell></row><row><cell>Linear 256, (SN), lReLU</cell><cell>Upsample</cell></row><row><cell>G 2 Linear 32</cell><cell cols="2">Conv 6, BN, lReLU X 4</cell></row><row><cell cols="2">D 3 Linear 1, SN, Sigmoid Conv 6, BN, lReLU</cell></row><row><cell cols="2">D 2 Linear 1, SN, Sigmoid Conv 1, Tanh</cell></row><row><cell>D 1</cell><cell>E 2 , E 1</cell></row><row><cell cols="2">Get x t or x [1,T ] (concatenated along channels)</cell></row><row><cell cols="2">Conv 1, lReLU -96x256</cell></row><row><cell cols="2">Conv 6, BN/SN, lReLU Conv 6, Stride 2, BN/SN, lReLU</cell><cell>X 4</cell></row><row><cell cols="2">Reshape 1536</cell></row><row><cell>Get v t , z, c</cell><cell>E 2 Linear 32</cell></row><row><cell>Linear 256, SN, lReLU</cell><cell>E 1 Linear 128</cell></row><row><cell>Join features of x t , v t , z, c</cell><cell></cell></row><row><cell>Linear 512, SN, lReLU</cell><cell></cell></row><row><cell>Linear 1, SN, Sigmoid</cell><cell></cell></row><row><cell>flow. Batch normalizations (BN)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>2: Performances of different GAN models in interictal EEG signal synthesis and reconstruction tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>SincNet-based neural network that made use of EEG signals to learn the latent cognitive variables of the DDM on individual decisions. This approach identifies time windows of information processing and frequency bands that can be used to predict latent processes directly from EEG data as a trial-level association between neural features, choice, and response time.This work aims to develop a deep probabilistic method for linking neural data from EEG to the latent parameters of a cognitive model. The innovation of our work lies in the use of a theoretical account of the cognitive process. This theoretical account drives the analysis of neural and behavioral measures. The framework allows for one-step, joint inference on integrative neurocognitive models that map EEG and behavior into a joint latent space.Uniquely, this new approach has the potential to allow us to generate task-relevant EEG signals from behavioral data, and predict modulation of EEG signals by cognitive model parameters. By combining the exploratory potential of modern latent variable methods with the theoretical appeal of human-interpretable cognitive model parameters, the proposed technique can be used to make predictions of brain signals and cognitive parameters in future experiments to test neurocognitive theories.</figDesc><table><row><cell>5.2 Neurocognitive Variational Autoencoders</cell></row><row><cell>5.2.1 Generative EEG Modeling with VAEs</cell></row><row><cell>latent cognitive variables. Ideally, such frameworks operate across observations, experimental</cell></row><row><cell>manipulations, and individual differences. Deterministic models that leverage deep learning</cell></row><row><cell>have been proposed for learning feature representation of EEG data to analyze and decode</cell></row><row><cell>brain activity (Roy et al., 2019). As a notable example, Sun et al. (2022) have proposed a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc>1: Comparison of the sum of Wiener negative log-likelihood (-log Wiener (RT i | ω i )) of four subjects on the test sets. ω represents the median fitted cognitive parameters from the training set. At the same time, there are strong positive correlations between boundaries (α) and choice-RTs, as well as between non-decision time and choice-RTs. The estimates in NCVA are regularized by the subject priors obtained from a Bayesian hierarchical fitting of a DDM usingMCMC Nunez et al. (2019). The model was individually fitted for each subject using choice-RT and accuracy only and accounted for between-condition variability within subjects. Clear clusters of drift rates and non-decision-time estimates depending on the noise conditions can be seen, though boundary estimates are highly overlapped. It is worth noting that uncertainties in the estimates can be inspected from the figures through the posterior covariance. Understandably, the uncertainties in the estimations from choice-RTs are significantly higher than from EEG signals, which agree with the theoretical derivations in Section 5.2.3. Figures 5.2b and 5.2d also demonstrate a satisfactory generalization to unseen data. The drift rates positively correlate with choice-RTs, whereas the boundaries and non-decision time negatively correlate with choice-RTs. The model successfully learns to extract the neural features that account for the choice-RT variability at each trial. To evaluate whether obtaining trial estimates of cognitive parameters improved the model of choice and choice-RT data, Table5.1 presents the Wiener likelihood test for the neurocognitive generalization ability to unseen data. The results show that the use of single-trial predictions of cognitive parameters ω i provides higher likelihood than the median estimates ω fitted from the training data. This implies that single-trial estimates better account for new data compared to median estimates.</figDesc><table><row><cell>Subjects</cell><cell>ω test i</cell><cell>ωtrain</cell></row><row><cell>s1</cell><cell cols="2">-0.018 0.212</cell></row><row><cell>s2</cell><cell cols="2">-0.244 0.159</cell></row><row><cell>s3</cell><cell cols="2">0.264 0.735</cell></row><row><cell>s4</cell><cell cols="2">0.031 0.230</cell></row><row><cell>tively strong.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/khuongav/dvae ppg ecg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>(a) Subject s1 (b) Subject s2 (c) Subject s3 (d) Subject s4</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This PhD journey has been the most challenging yet highly fulfilling experience of my life. Achieving this milestone would not have been possible without the individuals who have become part of it along the way.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Code Availability Statement</head><p>The dataset analyzed during the current study is available on <ref type="url" target="https://zenodo.org/record/8381751">https://zenodo.org/record/ 8381751</ref>, and the implementation of the model is in the following repository https:// github.com/khuongav/neurocognitive_vae.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIST OF FIGURES</head><p>Posterior Inference Model: We use a Bi-directional Gated Recurrent Unit network <ref type="bibr" target="#b11">(Chung et al., 2014)</ref> (GRU) to process the sequential order of RR intervals backward from y T to y t+1 and forward from y t+1 to y T . The GRUs are denoted here as h t = GRU W y y T , . . . , W y y t+1 and g t = GRU W y y t+1 , . . . , W y y T , respectively. The hidden states of the GRUs parametrize the variational distribution, which are combined with the previous latent states for the inference in Equation <ref type="formula">3</ref>.11 as follows:</p><p>All the hidden layer sizes are 256, and the latent space sizes are 128. Input and output segments at each timestep are of size 90. We use Adam <ref type="bibr" target="#b36">(Kingma and Ba, 2014)</ref> for optimization, with a learning rate of 0.0008, exponential decay rates β 1 = 0.9, and β 2 = 0.999.</p><p>We train the models for 5000 epochs, with a minibatch size 128. We set the regularization hyperparameter β = 0 at the beginning of training and gradually increase it until β = 1 is reached at epoch 1250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dataset</head><p>The MIMIC-III Waveform Database Matched Subset <ref type="bibr" target="#b50">(Moody et al., 2020;</ref><ref type="bibr" target="#b32">Johnson et al., 2016a)</ref> was used for the experiments. The database contains recordings collected from patients at various hospitals. Each session has multiple physiological signals, including PPG and ECG signals, sampled at a frequency of 125 Hz. We used the records of 43 healthy sub- To demonstrate the efficacy of our generative and inverse mapping approach for auxiliary tasks, we further evaluated our approach in epilepsy seizure detection. As the model is trained on the interictal EEG signals, seizure segments are detected with reconstruction error thresholds in an anomaly detection framework. Figure <ref type="figure">4</ref>.4 shows a high detection performance from our model by the ROC curve with the area under the curve of 0.92, competitive with contemporary approaches in supervised learning <ref type="bibr">(Siddiqui et al., 2020)</ref>.</p><p>We plan to build on these results in our future work for interpreting more encoded features in the low-dimensional manifolds and further investigate the partial mode collapse issue of GANs <ref type="bibr" target="#b5">(Bau et al., 2019)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction</head><p>Current approaches to understanding brain function emphasize the search for statistical relationships between human behavior and individual physiological measures (EEG, fMRI, fNIRS, etc.; e.g. <ref type="bibr" target="#b27">Itthipuripat et al., 2019)</ref>. Behavioral measures, such as accuracy and speed of responses, reflect latent cognitive processes that underlie decision making that are not observed directly and must be inferred by cognitive models <ref type="bibr" target="#b41">(Lee and Wagenmakers, 2014</ref>).</p><p>An ongoing challenge in computational cognitive neuroscience research is formulating the link between brain activity and latent cognitive processes. Here, we present a novel approach that allows a theoretical account of the cognitive process of decision-making, and artificial neural networks to estimate a joint latent space to link cognitive parameters to both neural</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Disentangled Cognitive Latent Space of EEG</head><p>Now consider the data D m def = {(x 1 , y 1 ) , . . . , (x I , y I )}, consisting, on the one hand, of N trials of the EEG data x i and, on the other hand, of the corresponding choice response times (choice-RT) y i . Both x i and y i are associated with a context vector c i (where the applicable context might be an experimental condition; say, noise conditions c i ). For mathematical simplicity, the context vector c is not mentioned when we refer to one of the data modalities.</p><p>Crucially, we propose a generative model with two sources of variation: z C , which is cognitively specific, and z N , which captures any residual neural variations left in x. We assume the approximate posterior q ϕ (z N , z C | x) has the following fully factorized form:</p><p>(5.3)</p><p>A Gaussian prior over latent variables p(z C ) can be chosen for each subject. We use subject priors obtained from a Bayesian hierarchical fitting of a DDM using the Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b55">(Nunez et al., 2019)</ref>.</p><p>We learn the generative model by maximizing the lower bound on log p θ (x, y) as:</p><p>) and p(y|z C ) can be any neurocognitive likelihood. This work applies the Wiener First Passage Time distribution (WFPT; Navarro and conditioned on cognitive parameters. The joint latent space inferred from the behavior can be factorized into the two-level latent space as follows:</p><p>Inspired by Suzuki et al. ( <ref type="formula">2016</ref>), we learn the following approximations, w.r.t parameter ϕ 1 B :</p><p>(5.7) and w.r.t parameter ϕ 2 B : </p><p>As there is little posterior uncertainty once conditioned on an EEG signal x i , the approximations are close to the average posterior induced by each of the EEG x i associated with similar y.</p><p>Having fit both the generative and discriminative models, we can now explore the three-way relationship between behavior, brain activity, and cognitive processes. all layers, with a slope of 0.1 to stimulate easier gradient flow. Batch normalizations (BN) <ref type="bibr" target="#b26">(Ioffe and Szegedy, 2015)</ref> are used in each convolutional layer of the encoders and decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Supplementary Materials</head><p>Self-attention layers <ref type="bibr" target="#b76">(Zhang et al., 2019)</ref> are applied in the encoders and decoders to better account for long-range relationships in time series. c are noise condition embeddings as onehot vectors (size 3). The size of z N is set at 32 as increasing the dimension did not lead to any improvement in performance on a validation set.</p><p>In Equation <ref type="formula">5</ref>.4, the term log p(y | z C ) is weighted by λ = 2 to scale up the likelihood of low-dimensional behavior. The KL terms are weighted by β = 20. The KL terms are normalized to balance the KL divergence loss and the reconstruction loss. Please refer to Sections 4.2 and A6 of <ref type="bibr" target="#b22">(Higgins et al., 2016)</ref> for further information. The optimization of q ϕ C (z C | x) is divided into two stages. We first optimize the network w.r.t drift rate δ and boundary α, while non-decision time τ is set to 0.93 • RT min for each subject, approximating the results of the Bayesian MCMC modeling <ref type="bibr" target="#b55">Nunez et al. (2019)</ref>. Having trained ϕ C for δ and α, we can proceed to train only the last fully connected layer that predicts τ . This procedure is to circumvent the difficulty of simultaneously optimizing the network for the boundary and the non-decision time on the experimental data. We used Adam <ref type="bibr" target="#b36">(Kingma and Ba, 2014)</ref> for optimizations, with a learning rate of 5e-4 and exponential decay rates β 1 = 0.9 and β 2 = 0.999.  Linear 1 (logvar ndt) specifically focused on N200 due to the significant associations between N200 latency and NDT reported by <ref type="bibr" target="#b55">Nunez et al. (2019)</ref>. In our new experiments, we additionally observed a substantial relationship between drift rate and N200 latency, which we included in the simulation. Boundary separation was not included in the simulation, as we did not find any neural correlates of variability in boundary separation, and those are usually only found in tasks with trial-level accuracy feedback <ref type="bibr" target="#b9">(Cavanagh and Frank, 2014;</ref><ref type="bibr" target="#b54">Nunez et al., 2024)</ref>.</p><p>To simulate single-trial EEG signals, we shifted the true averaged ERP waveform based on each sample of trial-level NDT, using a linear regression slope of 1, as in <ref type="bibr" target="#b55">Nunez et al. (2019)</ref>.</p><p>EEG noise was obtained from the original data, using independently sampled segments that did not include responses to stimuli. The resulting ERP and EEG waveforms were then combined to generate artificial EEG signals for each trial that carried the N200 latency</p><p>The primary objective was to assess whether N200 peak-latencies recorded by EEG reflected VET across varying visual noise conditions, thereby shedding light on the timing involved in perceptual decision-making processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Decision-Making Models -The Drift-Diffusion Model (DDMs)</head><p>The Drift-Diffusion Model (DDM) is a sequential sampling model of decision making. The model assumes that decision-making is the result of the accumulation of evidence in favor of one option or another. The evidence is represented by a random walk process, where the evidence accumulates over time, and the decision is made when the accumulated evidence exceeds a threshold.</p><p>Mathematically, the DDM is described by a set of equations that govern the accumulation of evidence and the decision-making process. The basic equation for the DDM following a Wiener process (Figure <ref type="figure">5</ref>.11) is dx = δdt + ςdW (5.9)</p><p>where dx is the evidence step, dt is the time step and ςdW denotes a Gaussian noise with a scale ς. The drift rate δ and the diffusion coefficient ς are parameters that describe the average rise in change over a unit interval during evidence accumulation and the instantaneous variation in the rate of change, respectively. The variance can be fixed at 1 for mathematical explicitness and simplicity. The distance between two options is described as the boundary separation, or α. The beginning position of evidence accumulation, which shows a bias toward one of the two options, is encoded by the parameter β. When β is 0.5, the beginning point is halfway between the two borders, and the evidence-building process can begin unbiasedly between the two options. Visual encoding time prior to evidence accumulation and motor execution time following evidence accumulation could be written as τ e and τ m , approximated as</p><p>(5.10)</p><p>Efficient methods for the computation of the Wiener diffusion model density and distribution functions exist <ref type="bibr" target="#b52">(Navarro and Fuss, 2009)</ref>, making it a highly tractable model. In this work,</p><p>β is set at 0.5, so that the starting point is always unbiased at z = βα.</p><p>Siddiqui, M. K., Morales-Menendez, R., Huang, X., and Hussain, N. (2020). A review of epileptic seizure detection using machine learning classifiers. Brain informatics, 7:1-18.</p><p>Sohn, K., <ref type="bibr">Lee, H., and Yan, X. (2015)</ref>. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28.</p><p>sudden (Retrieved on September 2020). Sudden death in young people: Heart problems often blamed.</p><p><ref type="url" target="https://www.mayoclinic.org/diseases-conditions/sudden-cardiac-arrest/in-depth/sudden-death/art-20047571">https://www.mayoclinic.org/diseases-conditions/ sudden-cardiac-arrest/in-depth/sudden-death/art-20047571</ref>. Sun, Q. J., Vo, K., Lui, K., Nunez, M., Vandekerckhove, J., and Srinivasan, R. ( <ref type="formula">2022</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Photoplethysmography and its application in clinical physiological measurement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiol Meas</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Atrial fibrillation detection from raw photoplethysmography waveforms: A deep learning application</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aschbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kerem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Benaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Tison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Olgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart rhythm O</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simulating brain signals: Creating synthetic eeg data via neural-based generative models for improved ssvep classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K N</forename><surname>Aznan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photoecg: Photoplethysmographyto estimate ecg parameters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Visvanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4404" to="4408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The need for uncertainty quantification in machine-assisted medical decision making</title>
		<author>
			<persName><forename type="first">E</forename><surname>Begoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kusnezov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unbiased learning of deep generative models with structured discrete representations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Bendekgey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sliced and radon wasserstein barycenters of measures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="45" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frontal theta as a mechanism for cognitive control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="414" to="421" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reconstructing qrs complex from ppg by transformed attentional neural networks</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="12374" to="12383" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society: series B (methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pot: Python optimal transport</title>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boisbunon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Corenflos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">78</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An Introduction to Model-Based Cognitive Neuroscience</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Forstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Glomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cattani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Franceschiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08385</idno>
		<title level="m">Computational models in electroencephalography</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Eeg-gan: Generative adversarial networks for electroencephalograhic (eeg) brain signals</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Schirrmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01875</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automl: A survey of the state-of-the-art. Knowledgebased systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106622</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in Advances in Approximate Bayesian Inference, NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11333</idno>
		<title level="m">Mina: multilevel knowledge-guided attention for modeling electrocardiography signals</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opportunities and challenges of deep learning methods for electrocardiogram data: A systematic review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">103801</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>pmlr</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Functional MRI and EEG index complementary attentional modulations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Itthipuripat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Serences</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="6162" to="6179" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jabbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Omar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05132</idno>
		<title level="m">A survey on generative adversarial networks: Variants, applications, and training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Rit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neurophysiologically-based mathematical model of flash visual evoked potentials</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zouridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mimic-iii, a freely accessible critical care database</title>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic graphical models: principles and techniques</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured inference networks for nonlinear state space models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05121</idno>
		<title level="m">Deep kalman filters</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<title level="m">Bayesian cognitive modeling: A practical course</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stint: selective transmission for low-energy physiological monitoring</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dutt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design</title>
		<meeting>the ACM/IEEE International Symposium on Low Power Electronics and Design</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graphical generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comparison of photoplethysmography and ecg recording to analyse heart rate variability in healthy subjects</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical engineering &amp; technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="634" to="641" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Timing of readiness potentials reflect a decision-making process in the human brain</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="283" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A dynamical model for generating synthetic electrocardiogram signals</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Mcsharry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="294" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Expectation propagation for approximate bayesian inference</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.2294</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<title level="m">Learning in implicit generative models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Mimic-iii waveform database matched subset. MIMIC-III Waveform Database Matched Subset v1</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Silva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Physics-driven regularization of deep neural networks for enhanced engineering design and analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meidani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing and Information Science in Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast and accurate calculations for first-passage times in wiener diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Fuss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Computational models in cardiology</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Niederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lumens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Trayanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cardiology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="111" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A tutorial on fitting joint models of m/eeg and behavior to understand cognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>PsyArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The latency of a visual evoked potential tracks the onset of decision making</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gosai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Individual differences in attention influence perceptual decision making</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How attention influences perceptual decision making: Single-trial EEG correlates of drift-diffusion model parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Electric fields of the brain: the neurophysics of EEG</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How machine learning is impacting research in atrial fibrillation: implications for risk prediction and future management</title>
		<author>
			<persName><forename type="first">I</forename><surname>Olier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ortega-Martorell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pieroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Lip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cardiovascular Research</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1700" to="1717" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Synthetic epileptic brain activities using gans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aminifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Atienza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ryvlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ML4H) at NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The diffusion decision model: theory and data for two-choice decision tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Utility of the photoplethysmogram in circulatory monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Shaltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mccombie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Asada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anesthesiology: The Journal of the American Society of Anesthesiologists</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="950" to="958" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The risk factors and prevention of cardiovascular disease: the importance of electrocardiogram in the diagnosis and treatment of acute coronary syndrome</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leksowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Therapeutics and clinical risk management</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1223</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep learning-based electroencephalography analysis: a systematic review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Banville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51001</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cardiogan: Attentive generative adversarial network with dual discriminators for synthesis of ecg from ppg</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Application of machine learning to epileptic seizure onset detection and treatment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Shoeb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Ppg to ecg signal translation for continuous atrial fibrillation detection via attention-based deep state-space modeling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15375</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">P2e-wgan: Ecg waveform synthesis from ppg with conditional wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual ACM Symposium on Applied Computing</title>
		<meeting>the 36th Annual ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1030" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep latent variable joint cognitive modeling of neural signals and human behavior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="page">120559</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Composing graphical models with generative adversarial networks for EEG signal modeling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1231" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: A survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="8052" to="8072" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bayesian machine learning: Eeg\/meg signal processing measurements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="36" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05689</idno>
		<title level="m">Hyper-parameter optimization: A review of algorithms and applications</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Domain generalization: A survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4396" to="4415" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ecg reconstruction via ppg: A pilot study</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE EMBS International Conference on Biomedical &amp; Health Informatics (BHI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
