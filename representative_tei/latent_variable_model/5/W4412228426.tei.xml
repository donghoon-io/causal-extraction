<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Latent Variable Model for Graph Learning and Inference</title>
				<funder ref="#_fUPwQXe">
					<orgName type="full">NextGenerationEU</orgName>
				</funder>
				<funder ref="#_MaK9XCy">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_x5jwtKk">
					<orgName type="full">MUR</orgName>
				</funder>
				<funder>
					<orgName type="full">TAILOR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-10-14">October 14, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manfred</forename><forename type="middle">;</forename><surname>Jaeger</surname></persName>
							<email>jaeger@cs.aau.dk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><forename type="middle">;</forename><surname>Longa</surname></persName>
							<email>antonio.longa@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steve</forename><forename type="middle">;</forename><surname>Azzolin</surname></persName>
							<email>steve.azzolin@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><forename type="middle">;</forename><surname>Schulte</surname></persName>
							<email>oschulte@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Passerini</surname></persName>
							<email>andrea.passerini@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalborg University</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<orgName type="institution" key="instit3">University of Trento</orgName>
								<orgName type="institution" key="instit4">Simon Fraser University</orgName>
								<orgName type="institution" key="instit5">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Latent Variable Model for Graph Learning and Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-14">October 14, 2025</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a probabilistic latent variable model for graphs that generalizes both the established graphon and stochastic block models. This naive histogram AHK model is simple and versatile, and we demonstrate its use for disparate tasks including complex predictive inference usually not supported by other approaches, and graph generation. We analyze the tradeoffs entailed by the simplicity of the model, which imposes certain limitations on expressivity on the one hand, but on the other hand leads to robust generalization capabilities to graph sizes different from what was seen in the training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most state-of-the-art machine learning techniques for graph data are highly specialized towards specific tasks such as node classification or synthetic graph generation. This is the case, in particular, for the graph neural network technology. In contrast, one may be interested in more flexible approaches that support a variety of different inference tasks based on a single model. This is the tradition of probabilistic graphical models <ref type="bibr" target="#b15">[16]</ref>, which have been adapted to the handling of graph data in the field of statistical relational learning (SRL) by frameworks like relational Bayesian networks <ref type="bibr" target="#b11">[12]</ref>, Markov logic networks <ref type="bibr" target="#b20">[21]</ref>, or ProbLog <ref type="bibr" target="#b6">[7]</ref>. Conceptually extremely powerful, these SRL approaches face numerous challenges in the form of computational complexity of inference and learning, and the need to provide some structural model specifications based on domain expertise. One cause for the high computational complexity of SRL models lies in the fact that the answer to a probabilistic query involving a certain set of named entities often depends on how many other entities there exist in the domain, even if nothing is known about their properties, and their connections to the query entities. In the worst case, the complexity of computing a query is exponential in the number of these additional entities <ref type="bibr" target="#b10">[11]</ref>. Technically, these SRL models are not projective in the sense of Shalizi and Rinaldo <ref type="bibr" target="#b22">[23]</ref>.</p><p>Investigating projectivity in a more general setting of multi-relational graphs, Jaeger and Schulte <ref type="bibr" target="#b12">[13]</ref> have introduced the AHK model, which can be seen as a generalization of the classic graphon model for random graphs developed by Aldous <ref type="bibr" target="#b2">[3]</ref>, Hoover <ref type="bibr" target="#b9">[10]</ref>, and Kallenberg <ref type="bibr" target="#b13">[14]</ref>. As introduced in <ref type="bibr" target="#b12">[13]</ref>, the AHK model is a very general, abstract mathematical model that does not directly provide algorithmic learning and inference solutions. In this paper we develop a concrete, limited, class of AHK models, together with practical learning and inference approaches. We call this model class the naive histogram AHK (NH-AHK -) model, because it incorporates a probabilistic independence assumption similar in nature to the naive Bayes assumption, and it employs non-parametric histogrambased specifications of conditional probability distributions.</p><p>Our model class can be seen as a lightweight alternative to SRL frameworks. Compared to the latter, it loses a substantial amount of modeling capabilities (though, as we shall see, it also provides some capabilities not found in current SRL), but gains in terms of computational tractability, and does not depend on model components defined using elements of logic or programming languages, which are very difficult to learn fully automatically from data. Compared to many other machine learning techniques, the naive histogram AHK model retains the capability to answer a wide range of probabilistic queries, to handle incomplete data both in learning and inference, and to function as a generator for synthetic graphs.</p><p>We introduce the NH-AHK -model in Section 4 and discuss some basic theoretical properties in Section 5. The main algorithmic challenges and solutions are described in Section 6. In Section 7 we demonstrate the range of possible applications of the NH-AHK -model for graph modeling and reasoning, and graph generation. An implementation of NH-AHK -and scripts for the reproduction of our experiments are available at <ref type="url" target="https://github.com/manfred-jaeger-aalborg/AHK">https://github.com/manfred-jaeger-aalborg/AHK</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several authors have proposed methods for learning a graphon model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. Often the graphon is required to be given by a smooth function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>, but histograms or stochastic block models are also often considered, at least as an approximation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. Learning is almost exclusively performed for descriptive analysis of a single network, and the learned graphon is often presented as a heatmap representation of the network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. Applications to predictive inference then are limited to transductive link prediction <ref type="bibr" target="#b17">[18]</ref>. Graphon learning from multiple graphs has been considered under the assumption that training data contains graphs over the same set of nodes, which all have a fixed setting of their latent variables in repeated realizations of the random graph structure <ref type="bibr" target="#b1">[2]</ref>. Earlier work on graphon learning only considered graphs without node attributes. Numeric node attributes have recently been considered in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. All of these works differ substantially from ours in that they do not consider graphon learning from multiple graphs for applications in inductive predictive inference and graph generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphs.</head><p>We only consider graphs without numeric attributes or edge weights. A signature S = A ∪ E then consists of categorical node attributes A = {A 0 , . . . , A |A|-1 } and edge relations</p><formula xml:id="formula_0">E = {E 0 , . . . , E |E|-1 }. A S-graph ω = ([n], A, E) consists of a set of nodes [n]</formula><p>for some n ∈ N , an n × |A|-dimensional matrix A containing the attribute values of all nodes, and a n × n × |E|dimensional tensor E where E[:, :, l] is the adjacency matrix for relation E l . We refer to n as the size of ω. We denote by Ω (n) the set of all graphs for a given signature S with domain [n]. The relevant signature is usually implicit from the context, and not made explicit in the notation. An incomplete graph ω is a graph with ? entries in A or E. We denote by ∆Ω (n) the set of probability distributions over Ω (n) . A random graph model defines for every n ∈ N a distribution P n ∈ Ω (n) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Permutations. P[n] stands for the set of permutations of [n].</head><p>For n = 2, this set contains two elements: the identity mapping, and the permutation with π(0) = 1. For the latter permutation we use the special symbol π 0↔1 .</p><p>Types. We introduce the key concept of types. While the exact definitions are a bit technical, the underlying intuition is very simple: the 1-type of a node represents all the data available for that node in isolation, which consists of its attribute values, and its possible self-loops of relations. The (strict) 2-type of a pair of nodes contains all the data about edges connecting the two nodes. Both 1and 2-types can again be represented as graphs over the canonical domains <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>, respectively. Formally: for a graph ω of size n, and i ∈ [n], we denote with ω ↓ i the graph ( <ref type="bibr" target="#b0">[1]</ref>, A[i, :], E[i, i, :]). We also call ω ↓ i the 1-type of i in ω, and denote by T 1 the space of all possible 1-types. For a pair (i, j) ∈ [n] 2 = we define ω ↓ (i, j) as the graph ([2], ∅, E i,j ), where E i,j stands for the 2 × 2 × |E|dimensional tensor in which for k &lt; |E|:</p><formula xml:id="formula_1">E i,j [0, 1, k] = E[i, j, k], E i,j [1, 0, k] = E[j, i, k], and E i,j [0, 0, k] = E i,j [1, 1, k] = 0.</formula><p>We also call ω ↓ (i, j) the strict 2-type of (i, j) in ω, and denote by T 2 the space of all strict 2-types (we add the qualifier "strict" in order to emphasize that these 2-types omit the information about i, j that is already contained in their respective 1-types). There is a one-to-one correspondence between graphs and mappings of nodes i and pairs (i, j) to 1-types and strict 2-types, respectively. The class of probabilistic models introduced in the following makes essential use of this correspondence.</p><p>Projectivity. A random graph model is projective, if for all n 0 &lt; n 1 ∈ N the following holds: P n0 is equal to the marginal distribution of P n1 on induced subgraphs of size n 0 (see <ref type="bibr" target="#b12">[13]</ref> for a full technical definition). Projectivity has important benefits: for inference, a query related to a specific set of nodes does not depend on the size of the graph the nodes are embedded in (assuming no further information on the nodes other than those named in the query is given). For learning: if the actual data-generating distribution can be represented by a member of a certain class of projective random graph models (such as the class of NH-AHK -models we introduce below), then the model learned from a full dataset (consisting of possibly very large graphs), and the model learned from a dataset consisting of (smaller) sampled induced sub-graphs of size n will agree on the probabilities for graphs up to size n (in the large sample limit, and assuming the true maximum likelihood solution is identified).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Naive Histogram AHK -Model</head><p>We formally introduce our model. We start by recapitulating the general definition of <ref type="bibr" target="#b12">[13]</ref> constrained to signatures and graphs as introduced in Section 3, and limited to what in <ref type="bibr" target="#b12">[13]</ref> is called the AHK - model, which lacks an additional global latent mixture variable of the full AHK model. In the following ∆T i stands for the set of probability distributions over T i (i = 1, 2). • For i ∈ [n]: random variables T i with values in T 1 , and for (i, j) ∈ n 2 : random variables T (i,j) with values in T 2 .</p><p>• A measurable function</p><formula xml:id="formula_2">f 1 : [0, 1] → ∆T 1 ,<label>(1)</label></formula><p>and a measureable function</p><formula xml:id="formula_3">f 2 : [0, 1] 2 → ∆T 2<label>(2</label></formula><p>) that is permutation equivariant in the following sense: for (u, u ) ∈ [0, 1] 2 and t ∈ T 2 :</p><formula xml:id="formula_4">f 2 (u, u )(t) = f 2 (u , u)(π 0↔1 t).</formula><p>The AHK --model defines a probability distribution over graphs Ω (n) with n nodes as the expectation over latent u i variables</p><formula xml:id="formula_5">P (ω) = u∈[0,1] n i∈[n] f 1 (u i )(ω ↓ i) (i,j)∈ n 2 f 2 (u i , u j )(ω ↓ (i, j))du.<label>(3)</label></formula><p>A simple strategy for satisfying the permutation equivariance condition for f 2 is to only define a function f &lt; 2 (u, u ) for arguments u &lt; u , and let</p><formula xml:id="formula_6">f 2 (u, u ) = f &lt; 2 (u, u ) if u &lt; u π 0↔1 f &lt; 2 (u , u) if u &lt; u.<label>(4)</label></formula><p>With f 2 given in this form, and denoting by [0, 1] n the set of all ordered n-tuples of values from [0, 1], we can re-write (3) by replacing the integral over [0, 1] n with a combination of a sum over node permutations, and an integral only over [0, 1] n .</p><formula xml:id="formula_7">P (ω) = π∈P[n] u∈ [0,1] n p∈[n] f 1 (u p )(ω ↓ π(p)) (p,q)∈ n 2 f &lt; 2 (u p , u q )(ω ↓ (π(p), π(q)))du. (5)</formula><p>We use p, q to denote positions in a permutation π of [n]. Then π(p) stands for the node i ∈ [n] with the pth smallest U i . In order to turn this purely mathematical construction into an operational model, we need to design suitable classes of functions f 1 and f &lt; 2 for which learning and inference methods can be developed. In the following, we limit our descriptions and definitions to the f &lt; 2 functions. The f 1 are treated in an analogous manner. We base our design on two assumptions: A1 The naive assumption: conditional on the latent variables U i , U j , all relations defining ω ↓ (i, j) are independent (in the spirit of the naive Bayes assumption). A2 The histogram assumption:</p><formula xml:id="formula_8">f &lt; 2 is piecewise constant on a grid partition of its domain [0, 1] × [0, 1], i.e., f &lt; 2 (u, u ) = f &lt; 2 (b(u), b(u ))</formula><p>where b(u) is the bin index of u in a fixed given partioning of [0, 1] into interval bins b 1 , . . . , b g with a granularity parameter g ∈ N.</p><p>Under these assumptions, f &lt; 2 can be represented as a g × g × |E| × 2-dimensional array F 2 , where</p><formula xml:id="formula_9">F 2 [k, h, l, 0] and F 2 [k, h, l, 1] specify the probabilities of an edge E l (i, j), respectively E l (j, i), given that u i &lt; u j , k = b(u i ), and h = b(u j ).</formula><p>We can then substitute in (5):</p><formula xml:id="formula_10">f &lt; 2 (u p , u q )(ω ↓ (π(p), π(q))) = |E| l=1 d∈{0,1} E[π(p), π(q), l] • F 2 [b p , b q , l, d] + (1 -E[π(p), π(q), l] • (1 -F 2 [b p , b q , l, d]), (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where E is the edge relation tensor of ω. The right-hand side here depends on π and u only through the relative order and bin membership of the two nodes π(p), π(q). The same term will be encountered in many evaluations of π, u pairs of the outer summation/integration in (5). We collect these values in a g × g × n × n-dimensional table BP ω , so that BP ω (k, h, i, j) contains the multiplicative factor contributed to P (ω) by the nodes i, j, whenever b(u i ) = k, b(u j ) = h, and π -1 (i) &lt; π -1 (j). Replacing the integral over u by a sum over possible induced bin membership vectors, then (5) can be written as (but still omitting f 1 ):</p><formula xml:id="formula_12">P (ω) = π∈P[n] b∈ g n (p,q)∈ n 2 BP ω (b p , b q , π(p), π(q))V (b),<label>(7)</label></formula><p>where V (b) is the volume of the n-dimensional hypercube defined by the bin sequence b, intersected with [0, 1] n .</p><p>All definitions here are given for directed graphs. Undirected graphs are handled by a slightly simplified model where the table F 2 only needs to be |E| × g × g-dimensional, omitting the argument that encodes the direction of an edge. The probability of an incomplete graph is computed in exactly the same way by taking the product in (6) only over the relations A l for which E(π(p), π(q), l) =?.</p><p>Importantly, due to A1, no summation over possible values of unobserved edges or attributes is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expressivity and Complexity</head><p>The general AHK -model generalizes the basic graphon model with the ability to model discrete node attributes and multiple directed edge relations. The NH-AHK -model with its restriction to histogram representations generalizes the stochastic block model in a similar manner. Though NH-AHK -is conceived as a lightweight alternative to more expressive SRL languages, its latent variables enable some modeling capabilities not commonly found in existing SRL frameworks. An example of this will be given in Section 7.1.</p><p>While the shared nature of being generative random graph models supports a more direct expressivity comparison between NH-AHK -and stochastic block models or SRL frameworks, one can also compare the capabilities of NH-AHK -vs. graph neural networks in terms of discriminating different structures. Figure <ref type="figure" target="#fig_3">2</ref> shows a standard example illustrating limitations of graph neural networks: the two graphs and their nodes are indistinguishable by standard GNN architectures in the sense that every such GNN will construct node or graph feature vectors that are identical for the two graphs <ref type="bibr" target="#b0">[1]</ref>. A stochastic block model (and hence a NH-AHK -model) with two blocks and high intra-block and low inter-block probabilities, on the other hand, will assign a much higher probability to the right than the left graph, and thus distinguish these two graphs. Even though NH-AHK -makes the significant simplifying assumptions A1 and A2, it still leads to high complexity of probabilistic inference. A direct evaluation of ( <ref type="formula" target="#formula_12">7</ref>) is exponential in n, and hence intractable for all but very small graphs. As the following proposition shows, this is an inherent complexity that is unlikely to be avoidable by other inference designs. The proof is by an easy reduction of the 3-colorability problem: consider an NH-AHK -model that represents a simple stochastic block model with 3 blocks, where intra-block links have probability zero, and inter-block links have probabilities &gt; 0. Then a graph has a nonzero probability under this model, iff it is 3-colorable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference and Learning</head><p>For inference we need to evaluate <ref type="bibr" target="#b6">(7)</ref>. For learning we need to compute the gradient of ( <ref type="formula" target="#formula_12">7</ref>) with respect to the parameters contained in the array F 2 . Given the gradients, and assuming a fixed bin partitioning, learning is performed by stochastic gradient descent using Adam <ref type="bibr" target="#b14">[15]</ref>. If the granularity is given, but the boundary points defining the bins are to be optimized, then the gradient of ( <ref type="formula" target="#formula_12">7</ref>) also needs to be computed with respect to these boundary points (which affect <ref type="bibr" target="#b6">(7)</ref> only via the volume factors V (b)). The granularity itself can either be viewed as a hyper-parameter that can be set or optimized using the common approaches to hyperparameter tuning, or one can use an iterative refinement approach where learning starts at g = 1, and when learning at a given g value has terminated, then the currently widest bin is split into two, and learning continues at g + 1. This is iterated until the log-likelihood obtained at a g level does not exceed the likelhood at g -1 by more than a specified factor. Key to making both inference and learning tractabel is to replace the outer two sums over π ∈ P[n] and b ∈ g n by an expectation over sampling with a proposal distribution Q(π, b), such that Q(π, b) approximates the posterior P (π, b|ω) ∼ P (ω|π, b)V (b) (from Bayes's rule, and using that P (π, b) = n!V (b) is constant in π). We sample π, b by iteratively inserting a next node into already partially constructed π and b tuples. We make the probability of inserting the next node i at a given position with a given bin assignment proportional to the product of:</p><p>• the BP ω values we obtain from pairs i, j for nodes j already contained in the current partial π.</p><p>Note that inserting node i into π does not change any of the BP ω factors contributed by pairs of nodes j, j already contained in π, since these values only depend on the relative order of j and j and their bin memberships, but not on their absolute positions in π. • an estimate of the BP ω values we will obtain from pairs i, j for nodes j that are not yet contained in pi. For this we take a maximum over the BP ω values that can be obtained by a future insertion of j before or after the position of i with the corresponding possible bin assignments for j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query edge(s) Probability</head><formula xml:id="formula_13">1 → 0 0.43 0 → 3 0.29 3 → 0 0.16 3 → 2 0.005 1 → 0&amp;0 → 3 0.134 0 → 3&amp;3 → 0 0.0017</formula><p>The basic idea can be summarized as replacing an exact optimization over n! many possible permutations π by an approximation that only takes pairwise orderings of elements in π into account. A detailed description of the samping algorithm and an illustration of its effectiveness is contained in Appendix A.</p><p>Drawing a single sample from Q still is cubic in n (see Appendix A). This implies strict limitations on the sizes of graphs that can be handled. In the case of probabilistic inference, these limitations are often not so problematic, since queries often only refer to a small number of nodes of interest.</p><p>Training data, on the other hand, will often consist of graphs that are intractable for approximate gradient computations. In this case, we use a subsampling strategy, and learn from small induced subgraphs obtained by random uniform sampling of nodes in the original training graphs.</p><p>7 Examples and Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Directed Acyclic Graphs</head><p>In this section we demonstrate the capabilities of the NH-AHK -model by learning and reasoning with directed acyclic graphs. We created a training set containing 100 small directed acyclic graphs with 3 to 7 nodes. The graphs are directed Erdős-Rényi random graphs filtered on being acyclic. The sampled graphs had an average edge density of 0.43 (w.r.t. to the maximal number of n • (n -1)/2 edges in an acyclic graph).</p><p>We apply the learned model to answer several queries about the graphs shown in Figure <ref type="figure" target="#fig_5">3</ref>. The edges shown in the graphs are known to exist. All other edges are considered unknown, and we query the probability of their existence. The first three queries for the graph on the left are about edges that are consistent with acyclicity and the observed edges. Their probabilities depend on the number of complete orderings of the nodes that are consistent with the partial order imposed by the observed edges. The edge 1 → 0 is consistent with every possible complete ordering, and its probability is the overall edge density in the training set. The relative order between nodes 0 and 3 is not fully determined by the observed edges, but there are more complete orderings in which 0 preceds 3 than vice-versa, which is why the edge 0 → 3 has a higher probability. The edge 3 → 2 is inconsistent with acyclicity, and therefore has a very low probability. The final two queries are about the probabilities of joint occurrences of two edges. The first combination of edges is still consistent with acyclicity, and the probability for the combination is close to the product of the individual edge probabilities. The last combination introduces a cycle, and therefore it has a much lower probability than the product of individual edge probabilities.</p><p>Inference for the small graph on the left can be performed exactly. Figure <ref type="figure" target="#fig_5">3</ref> on the right shows a larger query graph for which approximate inference has to be used. This graph also is twice as big as the largest training graph. We query the model for the probabilities of the edges 1 → 13, 13 → 1, and 7 → 11. The plot at the bottom right shows for N = 100, 500, 1000, 5000, 20000 samples the computed probabilities for the three queries. The plots show average and standard deviation over 5 inference runs. We observe that at N = 5000 the approximate inference results are already quite stable. The estimated probabilities obtained at N = 20000 are 0.342, 0.001, and 0.255, respectively, for the three queries, again reflecting the proportion of total node orderings that are consistent with the given partial topological order and the query edge direction. The time for computing a single sample was approximately 0.006 seconds, or 30s for one inference run at N = 5000 (jointly for all 3 queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Molecules</head><p>The well-known MUTAG dataset consists of 188 molecules of 10-28 atoms (nodes). Nodes have an attribute element ∈ {C, N, O, F, I, Cl, Br}, and molecules carry a Boolean class label mutagenic. The standard task is to predict the class labels. However, we here use this dataset to learn a model for probabilistic inference. To this end, we use the first 187 molecules for training, and consider the last molecule as a subject for inference. Due to the small size of the molecules, no sub-sampling of the training graphs here is necessary. We learn with a fixed granularity of g = 10.</p><p>Figure <ref type="figure" target="#fig_6">4</ref> at the top shows the query molecule. Like most molecules in the dataset, it only contains the elements C,N and O. We consider atoms 0,1,2 and 13,14,15 as query atoms and set their element attribute to unknown. Given the known graph structure (different from Figure <ref type="figure" target="#fig_5">3</ref>, edges not shown in Figure <ref type="figure" target="#fig_6">4</ref> are known not to exist) and the observed elements of the atoms 3,. . . ,12, we want to infer the elements of the query atoms. The trained NH-AHK -model allows us to both query for the marginal probabilities of each query atom individually, and to query for the probabilities of joint configurations of multiple query atoms. We pose these joint queries for the two groups of query atoms, computing the probabilities of all joint configurations that involve the three elements C,N,O, and in which the two atoms 0,2, respectively 14,15, are the same element.</p><p>We perform importance sampling inference with N = 5000 samples, repeated for 5 runs. Figure <ref type="figure" target="#fig_6">4</ref> shows boxplots of the obtained probability estimates in the 5 runs. For nodes 0,1,2 we obtain for the single node probabilities a high probability for C, and much lower probabilities for N and O (top 3 left plots). This is quite in line with the result for the joint configuration, where the C-C-C configuration is about five times as likely as the second most probable N-O-N configuration (top right plot; joint configurations not shown in the boxplots all have very low probabilities). For the 13,14,15 group we observe in the single node inferences that N for 13 and O of 14,15 are the most probable elements. However, in all cases C is almost as likely. Here joint inference provides a quite different picture in that now the joint configuration O-N-O is about twice as likely as the C-C-C configuration. This example demonstrates how the NH-AHK -model can model mutual dependencies that can be exploited for better predictions of joint configurations than what can be obtained by just combining independent individual predictions (in other cases it may very well be the case that the most probable joint configuration is not the combination of the most probable individual attributes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Graph Generation</head><p>The NH-AHK -model allows efficient sampling of graphs of arbitrary size. In this section we explore the capability of NH-AHK -to serve as a simple baseline approach to learning graph generators. Observe that one must distinguish graph generators and generative models: the former are designed only to support sampling of random graphs; the latter are defined by the semantic property of specifying a full joint distribution, and are traditionally intended to support probabilistic and predictive inference. Dedicated graph generators usually do not support any inference, and not every generative model supports efficient sampling from the distribution it defines.</p><p>Datasets and settings: We evaluate our model using two well-known graph benchmarks: Community <ref type="bibr" target="#b26">[27]</ref> and EGO <ref type="bibr" target="#b26">[27]</ref> networks. The Community dataset consists of 500 synthetic two-community graphs. Community fits the modeling capabilities of the NH-AHK -model very well. The EGO dataset comprises 816 2-hop ego networks extracted from the Citeseer network <ref type="bibr" target="#b21">[22]</ref>. The sizes of the graphs range between 4 ≤ n ≤ 262. EGO appears like a reasonable challenge for NH-AHK -. We note that other common benchmarks like Grid <ref type="bibr" target="#b26">[27]</ref> represent sparse graph distributions that are inherently out of scope for NH-AHK -. Our primary focus is on a generator's extrapolation capability: to generate larger graphs than seen in the training data. We therefore use separate test sets that contain graphs of the same size as the training graphs, and graphs that are (on average) larger by scaling factors (sf ) of 1.5, 2, 4, and 8. For Community we just create synthetic examples of the appropriate sizes. The EGO graphs are partitioned into subsets according to their size, such that the different sets differ (approximately) by our scaling factors. The training set then contains graphs of sizes 4 ≤ n ≤ 18.</p><p>Other approaches: We compare NH-AHK -against two popular architectures for graph generation: GraphRNN <ref type="bibr" target="#b26">[27]</ref> generates new graphs in an incremental, auto-regressive manner. DiGress <ref type="bibr" target="#b25">[26]</ref> is a diffusion-based, one-shot generative model based on graph transformers. We have selected these two models because of their complementary designs, and because they are able, in principle, to generate graphs of arbitrary size. As a very simple baseline we include the Erdős-Rényi (ER) model <ref type="bibr" target="#b7">[8]</ref>.</p><p>Metrics: A common score to evaluate graph generators is the maximum mean discrepancy (MMD) of degree, clustering, and orbit distributions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. However, it has recently been shown that MMD is quite sensitive to the choice its parameters <ref type="bibr" target="#b19">[20]</ref>. Moreover, degree, clustering, and orbit distribution do not capture well the characteristic structure of Community and EGO networks. We therefore introduce metrics that are more tailored towards these particular benchmarks. For Community we measure similarity between generated and test set by comparing the distribution of the number of communities and their modularity scores <ref type="foot" target="#foot_0">1</ref> . A characteristic feature of ego-networks is the distribution of pair-wise distances between nodes, which (for our 2-hop networks) is bounded by 4, and, for at least one node, all distances to other nodes are bounded by 2. We use the radius and the diameter to capture essential distance properties of a graph. All our base metrics, thus, are graph-level measures, and we compare two graph sets by the earth-movers distance (EMD) between the distributions of these measures. In Appendix B.5 we also report results using the standard MMD metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>In Table <ref type="table" target="#tab_1">1</ref>, we present the Earth Mover's Distance (EMD) between the generated and test networks for each scaling factor for Community and EGO networks, left and right of Table <ref type="table" target="#tab_1">1</ref>, respectively. A scaling factor equal to 1 indicates that the generated networks are of the same size as the training set. Notably, certain entries of GraphRNN are left unfilled due to the models' inability to generate larger networks (for further details, refer to Appendix B.4). While DiGress encounters memory limitations when generating Community networks with a scaling factor of 4 and 8.</p><p>Examining the Community results, it is evident that despite competitors being tailored specifically for graph generation, NH-AHK -demonstrates comparable results in network generation and surpasses them in generating larger networks. Furthermore, it is important to note that the NH-AHK -model for this dataset required only 3 parameters, while GraphRNN and DiGress require almost 400,000 and 4.6 million parameters, respectively. On the other hand, within the EGO networks, the NH-AHK - model can capture the radius but fails in capturing the diameter. Once again, the main objective here is to demonstrate that the NH-AHK -model, with only 6 parameters, stands in comparison with specific graph generation models: GraphRNN with nearly 400,000 parameters and DiGress with 2 million parameters. We provide a visualization of generated graphs in Appendix B.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have introduced theory and implementation of the NH-AHK -model. The model is conceived as a highly versatile, lightweight graph learning and inference tool that combines key features of SRL frameworks on the one hand, and GNNs on the other. Like SRL models it is fully generative and supports a wide range of probabilistic queries beyond simple classification tasks. We have demonstrated the capabilities of answering joint queries for multiple edges, or multiple node attributes. Like GNNs, NH-AHK -does not rely on expert knowledge to specify the logical structure of a model in some form of probabilistic programming language. For the purpose of graph generation we found that NH-AHK -can compete with, and partly outperform highly engineered specialized GNN models on tasks that are compatible with the inherently dense nature of NH-AHK -models.</p><p>Other advantages of NH-AHK -beyond what has been demonstrated in this paper include the support for multi-relational graphs, and the ability to learn from incomplete data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Importance Sampling</head><p>Given an NH-AHK -model and a graph ω, the goal is to sample a random pair of a permutation π and a bin assignment b, such that the sampling probability Q(π, b) approximates the posterior distribution P (π, b|ω). For a full description of the method, we need to also provide the details of the handling of the f 1 factors in <ref type="bibr" target="#b4">(5)</ref>, which in ( <ref type="formula" target="#formula_10">6</ref>) and ( <ref type="formula" target="#formula_12">7</ref>) were omitted. Without loss of generality, we assume that categorical attribute A l has values in [m l ] for some m l ∈ N. Under assumptions A1,A2, the function f 1 can be represented by g × m h -dimensional tables F 1,l (l = 1, . . . , |A|), such that F 1,l [k, v] is the probability that a node i with k = b(u i ) has value v for attribute A l (i.e., the rows of F 1,l are multinomial distributions over the values of A l ).</p><p>For a given graph ω, we define in analogy to BP ω :</p><formula xml:id="formula_14">UP ω (k, i) = |A| l=1 v∈[m l ] I[A[i, l] = v]F 1,l [k, v],<label>(8)</label></formula><p>where The importance sampling algorithm receives as input the pre-computed tables BP ω , UP ω . The highlevel structure of the algorithm is shown in Algorithm 1. We start by creating an initial random   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Generated graph evaluation B.1 Graph Generation Baselines</head><p>The official repository of GraphRNN <ref type="bibr" target="#b26">[27]</ref> and DiGress <ref type="bibr" target="#b25">[26]</ref> were used for implementing the two baselines in Section 7.3. For GraphRNN we stuck to the original configuration provided in the repository for all datasets, while for DiGress on the EGO dataset we reduced the number of layers to 5 to account for the smaller size of the networks. For model selection, we resorted to the heuristics already provided in the repositories, selecting as the best epoch for DiGress over EGO and Community respectively epoch 799 and 699, while for GraphRNN 1600 and 1600. The experiments were launched on a single A100 80GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Data Details</head><p>Community: Each community is generated by the Erdos-Renyi model with N ∼ U (30, 80) many nodes, and intra-community edge probability p = 0.3. The communities are connected by random edges with edge probability of 0.0025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EGO:</head><p>The collection of ego-graphs is divided as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Generating Bigger Graphs</head><p>While the number of edges is usually upper-bounded by the maximum size of the BFS queue found in the train set, GraphRNN <ref type="bibr" target="#b26">[27]</ref> can generate an arbitrary number of nodes by unrolling the trained RNN.</p><p>DiGress <ref type="bibr" target="#b25">[26]</ref>, on the other hand, can generate bigger graphs by sampling the number of nodes from a desired node distribution. In Table <ref type="table" target="#tab_4">2</ref> we reported the average number of nodes for the reference test set and the generated graphs. The results show an inherent limitation of GraphRNN in generating larger networks, in contrast to all the other methods. We claim that this behavior can be due to the local auto-regressive nature of the architecture, which seems to suffer from overfitting on the training distribution, which in turn generally shows a pretty narrow variability in the number of nodes. The same does not happen for DiGress, which first samples a noisy graph of size n and then reverts the noise via the trained Graph Transformer <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 MMD Evaluation</head><p>To ensure a rigorous evaluation, we assess the performance of the NH-AHK -model using the Maximum Mean Discrepancy (MMD) metric across degree distributions, clustering distributions, and orbit distributions. We compute these statistical measures and the MMD score employing the same framework as utilized by GraphRNN. The corresponding source code can be accessed at the following link   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Additional analysis</head><p>Here we report additional analysis on the Community dataset and samples of generated graphs. Given that the original Community networks consist of precisely two communities, each with an intra-community edge probability of 0.3 and inter-community edges with a probability of 0.0025, we partition the input graphs using the Clauset-Newman-Moore greedy modularity maximization algorithm <ref type="bibr" target="#b5">[6]</ref>. Subsequently, we evaluate the intra-community and inter-community edge probabilities.</p><p>The summarized average values are presented in Table <ref type="table" target="#tab_7">4</ref>. As we present the average values, we also include the values from the test set. Consequently, the optimal model is the one with a value closest to that of the test set. Discerning from the table, it becomes evident that exclusively the NH-AHK - model maintains its proximity to the test set, even as the scaling factor increases.</p><p>Scaling     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The set of integers {0, . . . , n -1} is denoted [n]. For any d ≥ 1, [n] d then is the set of d-tuples with elements from [n]. With [n] d = we denote the set of d-tuples with all distinct elements. We write n d for the set of d-tuples in which elements appear in their natural order, and n d = for ordered tuples with all distinct elements. Thus, n d and n d = are the outcome spaces of drawing an unordered sample of size d from [n], with and without replacement, respectively. Tuples are denoted in boldface letters u, b, . . ., and components of such tuples by corresponding subscripted normal face letters u p , b i , . . ..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 . 1</head><label>41</label><figDesc>Let S be a signature. An AHK --model for S and graphs of size n is given by • A family of i.i.d. random variables {U i |i ∈ [n]}, where each U i is uniformly distributed on [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plate representation of AHK - model</figDesc><graphic coords="5,351.72,169.94,146.17,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: GNN indistinguishable graphs</figDesc><graphic coords="6,329.64,234.94,150.70,56.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proposition 5 . 1</head><label>51</label><figDesc>Computing the probability P (ω) of a graph ω in an NH-AHK -model is NP-hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Queries on DAG model</figDesc><graphic coords="7,289.94,217.00,193.00,133.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top: Query molecule; brown: carbon (C), green: nitrogen (N), blue: oxygen (O). Box plots of computed probabilities for single node and multiple-node queries.</figDesc><graphic coords="8,139.75,271.39,332.50,105.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 4 5 next=π init [i] 6 remaining=π init [i + 1 : n] 7 insertions=list 9 sample an insertion position/bin according to sampleprobs 10 insert</head><label>3567910</label><figDesc>I[] is the indicator function. ISAMPLE (n,g,BP,UP) 1 π init = random permutation of [n] 2 π = [], b = [] /* Initialize as empty lists */ for i=1,. . . ,n do of possible combinations of insertion position for next into π, and associated bin assignement.8 sampleprobs=GETSAMPLEPROBS(insertions,π,b,remaining,BP,UP) next at the sampled position/bin 11 end 12 return π, b Algorithm 1: High-level importance sampling algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Exact and importance sampling distribution for the DAG of Figure3</figDesc><graphic coords="14,108.00,72.00,396.00,149.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 : 4 Illustration. 5 = 6</head><label>6456</label><figDesc>Figure 6: Exact and importance sampling distribution for the ego graph of radius 2 of node 13 in Figure 4</figDesc><graphic coords="14,108.00,255.09,395.97,170.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 6 =</head><label>66</label><figDesc>Figure6gives a similar illustration for the MUTAG example of Section 7.2. We use as ω the ego-graph of radius 2 of node 13, which consists of 6 nodes. With a model granularity of g = 10, this gives us 6! = 720 permutations and 6+10-1 6 = 5005 different b vectors. Figure6displays a detail of size 50 × 50 of the resulting 720 × 5005-dimensional heatmaps. Again we observe a very good match between the true and the sampling distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :Figure 8</head><label>78</label><figDesc>Figure 7: Samples from the test set, NH-AHK -, DiGress, and GraphRNN, for Community and EGO networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Samples from the test set, NH-AHK -, DiGress, and GraphRNN, for Community as the scaling factor increases.</figDesc><graphic coords="18,108.00,208.52,396.00,343.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Samples from the test set, NH-AHK -, DiGress, and GraphRNN, for EGO as the scaling factor increases.</figDesc><graphic coords="19,108.00,208.52,396.00,343.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Jaeger et al., A Simple Latent Variable Model for Graph Learning and Inference. Proceedings of the Second Learning on Graphs Conference (LoG 2023), PMLR 231, Virtual Event, November 27-30, 2023.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Community and ego network results: EMD between number of communities (top-left), modularity (bottom-left), diameter (top-right) and radius (bottom-right) of generated and test networks, for increasing values of the scaling factor between training and generated/test networks. Best results are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Community</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EGO</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Stat.</cell><cell>1</cell><cell>Scaling factor 1.5 2</cell><cell>4</cell><cell>8</cell><cell>Stat.</cell><cell>1</cell><cell>Scaling factor 1.5 2</cell><cell>4</cell><cell>8</cell></row><row><cell>NH-AHK -</cell><cell></cell><cell cols="4">0.02 0.00 0.00 0.00 0.00</cell><cell></cell><cell cols="4">0.47 0.75 1.00 1.15 1.73</cell></row><row><cell cols="4">DiGress GraphRNN com. 1.05 nb of 0.00 0.20 0.53 --</cell><cell>--</cell><cell>--</cell><cell>dia.</cell><cell cols="4">0.32 0.78 0.33 0.35 0.40 2.08 ----</cell></row><row><cell>ER</cell><cell></cell><cell cols="4">2.42 2.07 1.90 1.25 1.23</cell><cell></cell><cell cols="4">0.40 2.03 1.48 0.96 2.05</cell></row><row><cell>NH-AHK -</cell><cell></cell><cell cols="4">0.03 0.03 0.02 0.02 0.02</cell><cell></cell><cell cols="4">0.39 0.13 0.03 0.10 0.05</cell></row><row><cell>DiGress GraphRNN</cell><cell>mod.</cell><cell cols="2">0.00 0.08 0.14 0.06 --</cell><cell>--</cell><cell>--</cell><cell>rad.</cell><cell cols="4">0.08 0.29 0.05 0.85 1.05 1.11 ----</cell></row><row><cell>ER</cell><cell></cell><cell cols="4">0.29 0.34 0.36 0.40 0.43</cell><cell></cell><cell cols="4">0.39 1.13 1.29 1.77 1.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The learned NH-AHK -model for the EGO data is defined by the binbounds 0, 0.25, 0.5, 1.0 and</figDesc><table><row><cell cols="4">Scaling factor Number training graphs Number test graphs Size range</cell></row><row><cell>1.0</cell><cell>94</cell><cell>24</cell><cell>4-18</cell></row><row><cell>1.5</cell><cell>-</cell><cell>227</cell><cell>19-24</cell></row><row><cell>2.0</cell><cell>-</cell><cell>134</cell><cell>25-33</cell></row><row><cell>4.0</cell><cell>-</cell><cell>198</cell><cell>34-67</cell></row><row><cell>8.0</cell><cell>-</cell><cell>139</cell><cell>70-262</cell></row><row><cell>B.3 Learned Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0.24311648 0.87151754 0.68273159</cell><cell></cell></row><row><cell>F 2 =</cell><cell></cell><cell>0.01519822 0.05864193</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.15766381</cell><cell></cell></row><row><cell>F F 2 =</cell><cell cols="2">0.30327248 0.00788412 0.25864134</cell><cell></cell></row><row><cell cols="3">which corresponds to a simple stochastic block model for two communities.</cell><cell></cell></row></table><note><p><p>2 [k, h] is the probability of an edge between nodes belonging to bins k and h (the graphs here are undirected, so that only the upper triangular matrix is needed) According to this model, nodes belonging to the first bin [0, 0.25] are highly connected to nodes in the other two bins; nodes in the second bin are sparsely connected to nodes in any other bin than the first, and nodes in the third bin also have a certain edge density (0.157) among themselves, apart from having a high connectivity with the nodes in the first bin.</p>The learned NH-AHK -model for the Community data is defined by the binbounds 0, 0.5, 1.0 and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2 .</figDesc><table><row><cell>Scaling Factor</cell><cell>Test set</cell><cell>NH-AHK -</cell><cell>GraphRNN</cell><cell>DiGress</cell><cell>ER</cell></row><row><cell>1.0</cell><cell>8.9 (±4.1)</cell><cell>9.7 (±3.7)</cell><cell>14.8 (±1.3)</cell><cell>7.5 (±4.7)</cell><cell>8.9 (±4.1)</cell></row><row><cell>1.5</cell><cell>20.3 (±2.0)</cell><cell>20.6 (±1.8)</cell><cell>15.0 (±1.1)</cell><cell>14.6 (±4.9)</cell><cell>20.3 (±2.0)</cell></row><row><cell>2.0</cell><cell>29.4 (±2.7)</cell><cell>28.5 (±2.9)</cell><cell>14.9 (±1.5)</cell><cell>24.4 (±3.7)</cell><cell>29.4 (±2.7)</cell></row><row><cell>4.0</cell><cell>45.4 (±8.8)</cell><cell>45.8 (±10.2)</cell><cell>15.0 (±1.5)</cell><cell>57.3 (±3.6)</cell><cell>45.4 (±8.8)</cell></row><row><cell>8.0</cell><cell cols="5">118.8 (±27.8) 115.0 (±20.9) 14.8 (±1.4) 127.1 (±3.3) 118.8 (±27.8)</cell></row><row><cell></cell><cell cols="4">(a) Average number of nodes of EGO networks</cell><cell></cell></row><row><cell>Scaling Factor</cell><cell>Test set</cell><cell>NH-AHK -</cell><cell>GraphRNN</cell><cell>DiGress</cell><cell>ER</cell></row><row><cell>1.0</cell><cell>98.4 (±28.8)</cell><cell cols="3">102.8 (±26.0) 157.4 (±2.2) 110.8 (±30.2)</cell><cell>98.4 (±28.8)</cell></row><row><cell>1.5</cell><cell>167.3 (±42.2)</cell><cell cols="4">164.2 (±43.8) 158.8 (±2.4) 173.9 (±26.3) 167.3 (±42.2)</cell></row><row><cell>2.0</cell><cell>235.6 (±55.9)</cell><cell cols="4">217.3 (±61.2) 160.3 (±1.1) 264.5 (±26.1) 235.6 (±55.9)</cell></row><row><cell>4.0</cell><cell cols="3">445.4 (±117.3) 446.6 (±120.3) 160.8 (±2.8)</cell><cell>-</cell><cell>445.4 (±117.3)</cell></row><row><cell>8.0</cell><cell cols="3">889.5 (±198.2) 910.4 (±209.5) 165.6 (±2.1)</cell><cell>-</cell><cell>889.5 (±198.2)</cell></row></table><note><p>(b) Average number of nodes of Comm networks</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Average number of nodes and standard deviation for reference test sets of EGO and Community, and of the generated graphs</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EGO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Community</cell><cell></cell></row><row><cell cols="2">Scaling Factor</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0</cell><cell>4.0</cell><cell>8.0</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0</cell><cell>4.0</cell><cell>8.0</cell></row><row><cell></cell><cell cols="5">NH-AHK -0.17 0.73 0.78 0.58</cell><cell>0.46</cell><cell cols="5">0.00 0.00 0.00 0.00 0.00</cell></row><row><cell>deg.</cell><cell cols="5">DiGress GraphRNN 0.02 0.07 0.06 0.59 1.09</cell><cell>1.12</cell><cell cols="3">0.00 0.01 0.01 0.15</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ER</cell><cell cols="3">0.09 0.33 0.50</cell><cell>0.47</cell><cell>0.89</cell><cell cols="5">0.01 0.01 0.01 0.01 0.01</cell></row><row><cell>clust.</cell><cell cols="5">NH-AHK -0.03 0.24 0.23 0.25 digress 0.04 0.01 0.08 0.17 GraphRNN 0.04</cell><cell>0.17 0.20</cell><cell cols="5">0.06 0.07 0.05 0.09 0.06 0.06 0.12 0.23 --0.01</cell></row><row><cell></cell><cell>ER</cell><cell cols="4">0.02 0.03 0.05 0.07</cell><cell>0.12</cell><cell cols="5">0.02 0.02 0.02 0.02 0.03</cell></row><row><cell></cell><cell cols="5">NH-AHK -0.01 0.02 0.03 0.04</cell><cell>0.12</cell><cell>0.06</cell><cell cols="4">0.02 0.04 0.09 0.16</cell></row><row><cell>orb.</cell><cell cols="5">DiGress GraphRNN 0.02 0.05 0.00 0.02 0.10</cell><cell>0.28</cell><cell cols="3">0.01 0.06 0.15 0.00</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ER</cell><cell cols="4">0.04 0.08 0.11 0.13</cell><cell>0.29</cell><cell cols="5">0.10 0.20 0.28 0.46 0.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of degree distributions, clustering distributions, and orbit distributions using the Maximum Mean Discrepancy (MMD) metric for each model and network.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>(±0.00) 0.002 (±0.00) 0.002 (±0.00) 0.003 (±0.00) 0.002 (±0.00) NH-AHK -0.004 (±0.00) 0.004 (±0.00) 0.004 (±0.00) 0.004 (±0.00) 0.004 (±0.00) Digress 0.003 (±0.00) 0.014 (±0.02) 0.021 (±0.02) (±0.01) 0.085 (±0.01) 0.085 (±0.00) 0.085 (±0.00) 0.084 (±0.00)</figDesc><table><row><cell></cell><cell>factor</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>intra-comm.</cell><cell cols="2">Test NH-AHK -Digress GraphRNN 0.27 (±0.08) 0.30 (±0.01) 0.28 (±0.02) 0.29 (±0.01) ER 0.34 (±0.13)</cell><cell>0.30 (±0.01) 0.28 (±0.02) 0.27 (±0.13) -0.29 (±0.13)</cell><cell>0.30 (±0.01) 0.28 (±0.02) 0.28 (±0.17) -0.27 (±0.12)</cell><cell>0.30 (±0.00) 0.28 (±0.02) --0.26 (±0.17)</cell><cell>0.30 (±0.00) 0.28 (±0.02) --0.22 (±0.12)</cell></row><row><cell>inter-comm.</cell><cell cols="5">Test GraphRNN 0.010 (±0.00) 0.003 ----ER 0.083</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Intra-community and inter-community edge probability on the Community dataset Figure 7 depicts test and generated random samples. The top row presents two Community networks for each model, while the bottom row displays two EGO networks for each model.</figDesc><table><row><cell>Original</cell><cell>Original</cell><cell>NH-AHK</cell><cell>NH-AHK</cell><cell>DiGress</cell><cell>DiGress</cell><cell>GraphRNN</cell><cell>GraphRNN</cell></row><row><cell>Community</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original</cell><cell>Original</cell><cell>NH-AHK</cell><cell>NH-AHK</cell><cell>DiGress</cell><cell>DiGress</cell><cell>GraphRNN</cell><cell>GraphRNN</cell></row><row><cell>EGO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>computed with the Clauset-Newman-Moore greedy modularity maximization algorithm<ref type="bibr" target="#b5">[6]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/snap-stanford/GraphRNN</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by <rs type="funder">TAILOR</rs>, a project funded by the <rs type="funder">EU</rs> <rs type="programName">Horizon 2020 research and innovation program</rs> under GA No 952215. AL acknowledges the support of the <rs type="funder">MUR</rs> <rs type="projectName">PNRR</rs> project <rs type="projectName">FAIR -Future AI Research</rs> (<rs type="grantNumber">PE00000013</rs>) funded by the <rs type="funder">NextGenerationEU</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MaK9XCy">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funded-project" xml:id="_x5jwtKk">
					<orgName type="project" subtype="full">PNRR</orgName>
				</org>
				<org type="funded-project" xml:id="_fUPwQXe">
					<idno type="grant-number">PE00000013</idno>
					<orgName type="project" subtype="full">FAIR -Future AI Research</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>permutation π init of the nodes, and then take nodes in the order of π init to iteratively insert them into the final permutation π. Given partially constructed π, b, the list of possible combinations of insertion positions and bin assignments is computed in line 7. For example, if n = 5, g = 3 and currently π = (4, 1), b = (1, 2), then insertions = ((0, 0), (0, 1), <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>, meaning that the next node can be inserted as the first element of π with bin assignment 0 or 1, in the middle between nodes 4 and 1 with bin assignment 1 or 2, or at the end with bin assignment 2 (this only depends on g and the current b, not the current π). The core of the importance sampling procedure then consists of line 8 where a probability distribution over insertions is calculated by Algorithm 7.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GETSAMPLEPROBS (insertions</head><p>In our example, let remaining</p><p>, BP[2, 2, 3, i]} Line 4 multiplies the weight computed so far with the factor UP[bin, next] that represents the contribution to P (ω|π, b) that derives from the probabilities of the node attributes. This only depends on the bin membership of next, not the permutation π.</p><p>Complexity. (Sketch) The total number of insertions calculated in line 7 of ISAMPLE is O(n(n+g)) (over all iterations of the for-loop). The weight computation for one candidate insertion in lines 3-5 of GETSAMPLEPROBS is O(ng) (the combined computation of leftweight,rightweight, remainweight effectively iterates over all nodes other than next, and the computation of remainweight also involves an iteration over the bins). This gives a total complexity of O(n 3 g + n 2 g 2 ), which also dominates the complexity of the pre-computation of BP, which is O(n 2 g 2 ).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ismail Ilkan Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2021</title>
		<meeting>IJCAI 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">Thiago B</forename><surname>Edo M Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><forename type="middle">H</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representations for partially exchangeable arrays of random variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="581" to="598" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A consistent histogram estimator for exchangeable graph models</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local linear graphon estimation using covariates</title>
		<author>
			<persName><forename type="first">Swati</forename><surname>Chandna</surname></persName>
		</author>
		<author>
			<persName><surname>Sc Olhede</surname></persName>
		</author>
		<author>
			<persName><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="721" to="734" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding community structure in very large networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristopher</forename><surname>Mark Ej Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Problog: A probabilistic prolog and its application in link discovery</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelika</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannu</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2468" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfréd</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. math. inst. hung. acad. sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rate-optimal graphon estimation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2624" to="2652" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relations on probability spaces and arrays of random variables. HPreprint, Institute for Advanced Study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><surname>Hoover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Princeton, NJ, 2</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Liftability of probabilistic inference: Upper and lower bounds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Statistical Relational AI</title>
		<meeting>the 2nd International Workshop on Statistical Relational AI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relational bayesian networks</title>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;97</title>
		<meeting>the Thirteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;97<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A complete characterization of projectivity for statistical relational models</title>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>of International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic symmetries and invariance principles</title>
		<author>
			<persName><forename type="first">Olav</forename><surname>Kallenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph normalizing flows</title>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random function priors for exchangeable arrays with applications to graphs and relational data</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Orbanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR, 2020. 9</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="4474" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01098</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consistency under sampling of exponential random graph models</title>
		<author>
			<persName><forename type="first">Rohilla</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName><surname>Rinaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Em-based smooth graphon estimation using mcmc and spline-based approaches</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sischka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Göran</forename><surname>Kauermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="295" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network estimation via graphon with node features</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Kw</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas Cm</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2078" to="2089" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Krawczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Siraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14734</idno>
		<title level="m">Digress: Discrete denoising diffusion for graph generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
