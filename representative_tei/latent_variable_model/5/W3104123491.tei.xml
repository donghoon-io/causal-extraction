<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation</title>
				<funder ref="#_BCT7DWG">
					<orgName type="full">Key Programs of the Chinese Academy of Sciences</orgName>
				</funder>
				<funder ref="#_SQQPjws">
					<orgName type="full">Chinese Academy of Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
							<email>chenxiuyi2017@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
							<email>fandongmeng@tencent.com</email>
							<affiliation key="aff3">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
							<email>chenfeilong2018@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
							<email>shuang.xu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<address>
									<country>CASIA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-theart latent variable model and their combination shows further improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge-grounded dialogue <ref type="bibr" target="#b8">(Ghazvininejad et al., 2018)</ref> which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>I just got a husky puppy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Pool</head><p># Knowledge Sentence 0 Husky is a general name for a sled type of dog used in northern regions, differentiated from other sled-dog types by their fast pulling style.</p><p>1</p><p>Huskies are also today kept as pets, and groups work to find new pet homes for retired racing and adventure trekking dogs. 2 Huskies are used in sled dog racing.</p><p>3 The use of "husk" is recorded from 1852 for dogs kept by Inuit people ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>Child of the Wolves is a children's novel, published in 1996, about a Siberian husky puppy that joins a wolf pack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response a</head><p>It sounds cute! Huskies are known amongst sled-dogs for their fast pulling style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response b</head><p>It sounds cute! I have read a novel about a husky puppy joining a wolf pack.</p><p>Is your husky puppy wolf-like in appearance? Table <ref type="table">1</ref>: An example shows the diversity of knowledge selection in knowledge-grounded dialogue. Here, we show two different responses with two possible selection decisions. For the same context, there may be diverse knowledge sentences to generate different responses which help their selection decisions in turn.</p><p>The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response <ref type="bibr" target="#b16">(Lian et al., 2019)</ref>. <ref type="bibr">(Zhao et al., 2019b;</ref><ref type="bibr" target="#b28">Sun et al., 2020;</ref><ref type="bibr" target="#b17">Madotto et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2019;</ref><ref type="bibr" target="#b33">Yavuz et al., 2019;</ref><ref type="bibr" target="#b29">Tang and Hu, 2019;</ref><ref type="bibr" target="#b15">Li et al., 2019;</ref><ref type="bibr" target="#b40">Zheng and Zhou, 2019;</ref><ref type="bibr" target="#b22">Niu et al., 2019;</ref><ref type="bibr" target="#b18">Meng et al., 2019;</ref><ref type="bibr" target="#b27">Ren et al., 2019;</ref><ref type="bibr" target="#b34">Ye et al., 2020)</ref>. However, they usually use the pre-identified knowledge <ref type="bibr" target="#b35">(Zhang et al., 2018;</ref><ref type="bibr" target="#b20">Moghe et al., 2018;</ref><ref type="bibr" target="#b25">Qin et al., 2019)</ref> which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data <ref type="bibr" target="#b2">(Chaudhuri et al., 2018;</ref><ref type="bibr" target="#b23">Parthasarathi and Pineau, 2018;</ref><ref type="bibr" target="#b41">Zhou et al., 2018;</ref><ref type="bibr" target="#b9">Gopalakrishnan et al., 2019)</ref>. Recently, <ref type="bibr" target="#b7">Dinan et al. (2019)</ref> propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b19">Min et al., 2018;</ref><ref type="bibr" target="#b21">Nie et al., 2019)</ref> and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue <ref type="bibr" target="#b7">(Dinan et al., 2019)</ref>. Knowledge selection plays an important role in open-domain knowledge-grounded dialogue <ref type="bibr">(Di-nan et al., 2019)</ref> since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately <ref type="bibr" target="#b16">(Lian et al., 2019)</ref>, or even lead to an inappropriate response. The example in Table <ref type="table">1</ref> shows two phenomena: (1) There may exist one-to-many relations between the dialogue context and the knowledge, resulting in the diversity of knowledge selection <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>;</p><p>(2) The posterior knowledge selection with context and response is much easier than the prior knowledge selection only depending on context. It is rather intuitive that we can dramatically reduce the scope of knowledge selection when we know the knowledge contained in the response, while such posterior information is not available at inference. Recently, latent variable models <ref type="bibr" target="#b16">(Lian et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>, using the posterior distribution to guide the prior distribution, have been proposed to deal with the diversity of knowledge selection. They can jointly model knowledge selection with response generation and achieve promising performance. Despite their success, latent variable models suffer from a huge gap between prior and posterior knowledge selection as discussed below.</p><p>We analyze the gap in latent variable models on two aspects: (1) Compared with the posterior selection module, the prior selection module has no access to the necessary posterior information. As a result, it is hard for the prior distribution to approximate the posterior distribution correctly at training, so that the prior selection module may not select knowledge properly at inference. (2) Response generation of latent variable models is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. This discrepancy, also named exposure bias, leads to a gap between training and inference <ref type="bibr" target="#b26">(Ranzato et al., 2015;</ref><ref type="bibr" target="#b36">Zhang et al., 2019;</ref><ref type="bibr">Zhao et al., 2019a)</ref>, and therefore the decoder may have to generate a response with inappropriate knowledge selected from an unfamiliar prior distribution.</p><p>In this paper, we propose to bridge the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation. Firstly, we enhance the prior selection module with the necessary posterior information which is obtained by the specially designed Posterior Information Prediction Module (PIPM). Secondly, inspired by knowledge distillation <ref type="bibr" target="#b11">(Hinton et al., 2015)</ref>, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected by the prior module, removing the exposure bias of knowledge selection. Experimental results show that both PIPM and KDBTS bring performance improvement on two knowledge-grounded dialogue datasets, i.e., Wizard of Wikipedia <ref type="bibr" target="#b7">(Dinan et al., 2019)</ref> and Holl-E <ref type="bibr" target="#b20">(Moghe et al., 2018)</ref>. And the combination of PIPM and KDBTS obtains the new state-of-the-art performance with further improvement.</p><p>Our contributions are summarized as follows:<ref type="foot" target="#foot_0">foot_0</ref> </p><p>• We clearly point out the gap between prior and posterior knowledge selection of latent variable models and propose to enhance the prior selection module with the necessary posterior information. Moreover, we explore several variants of posterior information.</p><p>• We focus on the exposure bias of knowledge selection for knowledge-grounded dialogue, and design a knowledge distillation based training strategy to deal with it.</p><p>• Experimental results show that both PIPM and KDBTS bring performance improvement, and their combination achieves the state-of-the-art performance with further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation</head><p>For a dialogue with T turns, each turn is a pair of message x t and response y t . Besides, each turn is associated with a knowledge pool</p><formula xml:id="formula_0">K t = {k l t } = {k 1 t , • • • , k L t } consisting of L sentences. The con- text ctx t consists of the dialogue history hist t = [x 1 , y 1 , • • • , x t-1 , y t-1 ] and the message x t .</formula><p>Given the context ctx t , we firstly select a knowledge sentence k sel t ∈ K t from the knowledge pool, then leverage the selected knowledge to generate an informative response y t . The selection history at the t-th turn is</p><formula xml:id="formula_1">kh t = [k sel 1 , • • • , k sel t-1 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Knowledge Selection For Response Generation</head><p>To obtain the likelihood of response y t , latent variable models treat knowledge k as the latent variable and marginalize over all possible knowledges K t :</p><formula xml:id="formula_2">p (y t |ctx t ) = E k∼π θ (Kt) p θ (y t |ctx t , k) ,<label>(1)</label></formula><p>Figure <ref type="figure">1</ref>: Latent knowledge selection for response generation. We train the decoder with the selected knowledge from posterior distribution with the red line, while we have to infer with the knowledge selected by the prior selection module without access to the response.</p><p>where p θ (y t |ctx t , k) is the decoder network, and π θ (K t ), short for π θ (K t |ctx t , kh t ), is the prior distribution over the knowledge pool K t based on the context ctx t and selection history kh t . The evidence lower bound (ELBO) is written as:</p><formula xml:id="formula_3">L ELBO = -L post NLL -L KL ≤ log p (y t |ctx t ) L post NLL = -E k∼q φ (Kt) [log p θ (y t |ctx t , k)] L KL = D KL (q φ (K t ) π θ (K t )) ,<label>(2)</label></formula><p>where q φ (K t ), short for q φ (K t |ctx t , y t , kh t ), is an inference network to approximate the true posterior distribution p (K t |ctx t , y t , kh t ).</p><p>The Gap between Prior and Posterior Knowledge Selection. Firstly, compared with the posterior selection module, the prior selection module has no access to the posterior information as shown in Figure <ref type="figure">1</ref>. As a result, it is hard for the prior distribution to approximate the posterior distribution correctly by minimizing the KL divergence in Equation 2 at training. Hence, the prior selection module may not select knowledge properly at inference. Secondly, comparing the two expectation terms in Equation 1 and 2, we see that the selected knowledge for response generation at training and inference is drawn from different distributions, i.e., the posterior distribution k ∼ q φ (K t ) at training and the prior distribution k ∼ π θ (K t ) at inference. Figure <ref type="figure">1</ref> clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sequential Knowledge Transformer</head><p>Recently, <ref type="bibr" target="#b13">Kim et al. (2020)</ref> propose Sequential Knowledge Transformer (SKT), the state-of-theart latent variable model for knowledge selection. Here, we briefly describe SKT, based on which we validate the effectiveness of our approach.</p><p>Sentence Encoding. For any sentence sent t with N w words at t-th turn, SKT uses a shared BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> to obtain the context aware word representations H sent t with d dims and then converts them into the sentence representation h sent t by mean pooling <ref type="bibr">(Cer et al., 2018)</ref>:</p><formula xml:id="formula_4">H sent t = BERT (sent t ) ∈ R Nw×d h sent t = Mean H sent t ∈ R d . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>As a result, we obtain H x t and h x t for the message x t , H y t and h y t for the response<ref type="foot" target="#foot_1">foot_1</ref> y t , and H k l t t and h k l t t for any knowledge sentence k l t ∈ K t . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs <ref type="bibr" target="#b5">(Cho et al., 2014)</ref> are used to summarize them as corresponding states s hist t and s kh t with zero initialization:</p><formula xml:id="formula_6">s hist t = GRU dial [h x t ; h y t ] , s hist t-1 ∈ R d s kh t = GRU sel h k sel t t , s kh t-1 ∈ R d ,<label>(4)</label></formula><p>where h x t , h y t and h k sel t t are sentence vectors of message x t , response y t and the selected knowledge k sel t and [•; •] denotes concatenation. Then, the prior query q prior t and posterior query q post t are obtained:</p><formula xml:id="formula_7">q prior t = s kh t-1 ; s hist t-1 ; h x t q post t = s kh t-1 ; s hist t .<label>(5)</label></formula><p>Note that s kh t-1 summarizes the selection history</p><formula xml:id="formula_8">kh t = [k sel 1 , • • • , k sel t-1 ],</formula><p>and s hist t contains current response information, i.e., h y t , not available at inference. The distribution a P t (K t ) over knowledge pool K t is obtained by the dot-product attention:</p><formula xml:id="formula_9">a P t (K t ) = softmax v P t h k 1 t t , • • • , h k L t t v P t = W P q P t ∈ R d ,<label>(6)</label></formula><p>where "P" denotes either prior or posterior, W P is the projection matrix and a P t (K t ) ∈ R L . Finally, the knowledge k sel t is selected by sampling from the posterior distribution q φ (K t ) = a post t (K t ) at training while selected with the highest probability over the prior distribution π θ (K t ) = a prior t (K t ) at inference. Generation with Knowledge. SKT takes the concatenation of message x t and selected knowledge sentence k sel t as input and generates responses by the Transformer decoder <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> with copy mechanism <ref type="bibr" target="#b10">(Gu et al., 2016)</ref>. Though there are various models studying how to improve the generation quality based on the given knowledge, here, we simply follow the decoder of SKT and mainly focus on the knowledge selection issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we show how to bridge the gap between prior and posterior knowledge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Posterior Information Prediction Module</head><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref>, we design a Posterior Information Prediction Module (PIPM) to predict the necessary posterior information. The main motivation is that we want to enhance the prior selection module with the necessary posterior information, so that it could approximate the posterior distribution better at training and leverage the posterior information for knowledge selection at inference. Following the typical setting in latent variable models <ref type="bibr" target="#b16">(Lian et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>, we use the response in bag-of-words (BOW) format <ref type="bibr" target="#b38">(Zhao et al., 2017)</ref> as the posterior information. Here we take the dialogue context and the knowledge pool as input to generate the posterior information.</p><p>We firstly summarize the context as the query of this module q PI t = s hist t-1 ; h x t and use it to get the attention distribution a PI t (K t ) over the knowledge pool K t by Equation <ref type="formula" target="#formula_9">6</ref>. Then, we summarize the knowledge representation in the knowledge pool with the weights in a PI t (K t ) considered:</p><formula xml:id="formula_10">h PI t = h k 1 t t , • • • , h k L t t • a PI t (K t ) ∈ R d .<label>(7)</label></formula><p>Secondly, we take the summarization of the dialogue context and the knowledge pool as input and use a position wise feed-forward network (FFN) <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> to generate the posterior information Ît in BOW format as:</p><formula xml:id="formula_11">Ît = softmax FFN q PI t ; h PI t ∈ R |V | . (8)</formula><p>Finally, we use the generated posterior information Ît to obtain the updated prior query qpost t as:</p><formula xml:id="formula_12">qpost t = q prior t ; Ît • E ,<label>(9)</label></formula><p>where E ∈ R |V |×d is the embedding matrix and |V | is the vocabulary size. Compared with q prior t in Equation <ref type="formula" target="#formula_7">5</ref>, qpost t additionally contains the predicted posterior information, and we replace q prior t with qpost t for knowledge selection in Equation <ref type="formula" target="#formula_9">6</ref>. We supervise this module by an addition loss L PIPM with the grounded posterior information I, i.e., the bag of tokens in the golden response:</p><formula xml:id="formula_13">L PIPM = -1/|I| w∈I log Îw t .<label>(10)</label></formula><p>Note that we remove the context information from I because this information is already contained in the prior query q prior t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Distillation Based Training Strategy</head><p>Current latent variable models <ref type="bibr" target="#b16">(Lian et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref> suffer from the exposure bias of knowledge selection as shown in Figure <ref type="figure">1</ref>. Therefore, the decoder may have to generate a response with inappropriate knowledge selected from an unfamiliar prior distribution. Inspired by knowledge distillation <ref type="bibr" target="#b11">(Hinton et al., 2015)</ref>, we design the Knowledge Distillation Based Training Strategy (KDBTS) to deal with this exposure bias. KDBTS is a two-stage training strategy that we firstly train the posterior selection module as the teacher and then leverage the well-trained posterior module to train the prior selection module as the student.</p><p>First Training Stage. We train a teacher at this stage, which is used to guide the student at the next stage. We can obtain a teacher as the by-product, i.e., the posterior selection module, from the training process of latent variable models in Figure <ref type="figure">1</ref>. However, we find the posterior selection module is affected by the prior distribution when minimizing the KL term in Equation <ref type="formula" target="#formula_3">2</ref>, and experiments in Section 5.2 show that it is usually not good enough.</p><p>As a result, we introduce a "fix" operation to make sure that the posterior selection module can not be affected by the prior distribution, and replace the KL term L KL with the fixed KL term L fix KL :</p><formula xml:id="formula_14">L fix KL = D KL (fix (q φ (K t )) π θ (K t )) . (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>The total loss for training the teacher is as follows:</p><formula xml:id="formula_16">L 1 = L post NLL + L fix KL -log q φ (k a t ) + λL PIPM , (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where L post NLL is defined in Equation 2, k a t ∈ K t is the golden selected knowledge for knowledge loss -log q φ (k a t ) which is proposed by <ref type="bibr" target="#b13">(Kim et al., 2020)</ref> and λ is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second Training Stage.</head><p>Once we obtain the teacher, we could leverage the posterior distribution from the well-trained teacher to deal with the diversity of knowledge selection. At this training stage, we feed the knowledge selected by the prior module into the decoder as shown in Figure <ref type="figure" target="#fig_0">2</ref>, which is the same as the inference process. In this way, KDBTS removes the exposure bias of knowledge selection. Here, we only update the prior selection module and the decoder (the green blocks in Figure <ref type="figure" target="#fig_0">2</ref>) because the encoder is shared by the student and the teacher. The total loss for training the student at this stage is:</p><formula xml:id="formula_18">L 2 = L prior NLL + L fix KL -log π θ (k a t ) ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_19">L prior NLL = -E k∼π θ (Kt) [log p θ (y t |ctx t , k)] and -log π θ (k a t )</formula><p>is the knowledge loss. Compared with L post NLL defined in Equation <ref type="formula" target="#formula_3">2</ref>, L prior NLL optimizes the decoder with knowledge selected from the prior distribution. Figure <ref type="figure" target="#fig_0">2</ref> clearly shows that KDBTS removes the exposure bias of knowledge selection by feeding the knowledge selected from prior distribution into the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We adopt two multi-turn knowledge-grounded dialogue datasets for experiments. Wizard of Wikipedia <ref type="bibr" target="#b7">(Dinan et al., 2019)</ref>  There are two versions of the test set: one with a single golden reference, the other with multiple golden references. Each dialogue is assigned with a document of about 60 sentences on average as the knowledge pool. Here, we use the modified version <ref type="bibr" target="#b13">(Kim et al., 2020)</ref> which fits for knowledge selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>We compare our approach with a set of competitive baselines: TMN is short for E2E Transformer MemNet <ref type="bibr" target="#b7">(Dinan et al., 2019)</ref>. TMN uses a Transformer memory network for knowledge selection and a Transformer decoder with copy mechanism for utterance prediction. Knowledge selection is trained based on the knowledge label without posterior distribution. TMN BERT , is short for TMN+BERT, implemented by <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>. TMN BERT replaces the Transformer memory network with a pre-trained BERT. PostKS <ref type="bibr" target="#b16">(Lian et al., 2019)</ref> only uses the posterior knowledge distribution as a pseudo-label for knowledge selection. PostKS uses GRU-based encoder and decoder without copy mechanism and does not use the knowledge label at the training stage. TMN BERT+PostKS+CP is short for TMN+BERT+ PostKS+Copy, implemented by <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>. Compared with TMN BERT , it additionally uses the the posterior distribution in PostKS. SKT <ref type="bibr" target="#b13">(Kim et al., 2020)</ref> is the current state-ofthe-art latent variable model. Compared with TMN BERT +PostKS cp , SKT leverages the posterior distribution by sequential latent modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We validate the effectiveness of our approach based on current state-of-the-art model SKT <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>, using the same datasets and dataprocessing codes<ref type="foot" target="#foot_2">foot_2</ref> . A shared encoder initialized with BERT BASE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> is used to encode dialogue and knowledge sentences. Then, a 5-layer Transformer decoder with copy mechanism is used to generate the response. We use a FFN with 512 hidden dims to generate the posterior information in BOW formats. The hidden size d is 768 and the vocabulary size |V | is 30, 522. Each batch consists of dialogues rather than individual turns, and the batch size is 1. The hyperparameter λ in Equation 13 is set to 0.5 for all experiments without searching. The "fix" operation in Equation 11 is implemented by gradient stoping.</p><p>Models are trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with gradient clipping at 0.4 and the learning rate is 0.00002. And we apply label smoothing <ref type="bibr" target="#b24">(Pereyra et al., 2017)</ref> and set 0.1 for knowledge selection and 0.05 for response generation. We approximate the expectation in Equation 1 and 2 by drawing one sample with Gumbel-Softmax function <ref type="bibr" target="#b12">(Jang et al., 2016)</ref> with temperature τ = 0.1. We train our teacher with 5 epochs, then select the teacher to teach the student according to the prior knowledge selection accuracy rather than posterior selection accuracy, because the shared encoder and decoder may be optimized overly for the posterior selection module, which is not a good initialization for the student. We train other models 20 epoches and select them according to the R-1 score as the final goal is to generate high-quality responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>Automatic Evaluation. Following <ref type="bibr" target="#b7">(Dinan et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>, we use accuracy (Acc) to evaluate the knowledge selection and use perplexity (PPL), unigram F1 (R-1) and bigram F1 (R-2) to evaluate the quality of response generation automatically. Following <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>, we remove all the punctuation and (a, an, the) before computing the R-1 and R-2 scores. Note that lower perplexity and higher R-1 and R-2 indicate better generation quality.</p><p>Human Evaluation. We firstly select 100 samples from each test set on the Wizard of Wikipedia dataset for human evaluation. Then, we ask three annotators to judge whether the response makes sense (Sensibleness) or is specific (Specificity) with the dialogue context. Finally, we obtain Sensibleness and Specificity Average (SSA) scores, which could penalize boring and vague responses and is suitable for the goal of knowledge-grounded dialogue <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref>. Moreover, compared with Engagingness and Knowledgeability used in <ref type="bibr" target="#b7">(Dinan et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>, SSA, evaluated in 0/1 format, is more objective and easier to conduct <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Quantitative Results. We report automatic results on the Wizard of Wikipedia dataset in Table <ref type="table" target="#tab_1">2</ref> and we have the following observations: (1) From row 4 and 5 we can see that the PIPM indeed provides some necessary posterior information which is helpful for knowledge selection. (2) Comparing row 4 and 6, we see that the KDBTS brings about significant improvement on generation quality by removing the exposure bias of knowledge selection.</p><p>(3) The combination of PIPM and KDBTS achieves further improvement on most metrics, except the knowledge selection accuracy. We think the reason is that there may exist several similar knowledge sentences leading to the same response. As a result, SKT+PIPM+KDBTS may select a reasonable  knowledge but not the golden one to generate informative and fluent responses, because models are selected according to the generation quality rather than the selection accuracy <ref type="bibr" target="#b13">(Kim et al., 2020)</ref>.</p><formula xml:id="formula_20"># Method Test Seen Test Unseen Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ 0 TMN</formula><formula xml:id="formula_21"># Method Single Reference Multi Reference Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ 0 TMN</formula><p>Results in Table <ref type="table" target="#tab_2">3</ref> lead to consistent observations on the Holl-E dataset that both PIPM and KDBTS bring performance improvement over the state-ofthe-art latent variable model SKT, and their combination achieves further improvement, resulting in the new state-of-the-art performance.</p><p>Qualitative Results. We report human evaluation results of the generated responses in Table <ref type="table" target="#tab_3">4</ref>. We see that our approach brings about consistent improvement over the state-of-the-art model SKT. Our approach can leverage the selected knowledge appropriately to generate more sensible and spe-Name Details I = y BOW bag-of-word information in the response, i.e., the words.</p><formula xml:id="formula_22">I = y x BOW y BOW -x BOW I = yk x BOW y BOW + k a BOW -x BOW I = y Seq the response</formula><p>Table <ref type="table">5</ref>: Several variants of posterior information.</p><p>cific responses, which are fluent and informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>PIPM. We explore several variants of posterior information in Tabel 5 to better study this module. Besides the default y x BOW , we also explore two variants in BOW format: (1) y BOW does not remove the context information; (2) yk x BOW additionally considers another source of posterior information, i.e., the golden selected knowledge k a t . And we consider the sequence information in y Seq , as the BOW format discards the word order. Note that we use the FFN in Equation <ref type="formula">8</ref>to obtain the posterior information in BOW formats, and we take H x t +h PI t as input and use a 3-layer Transformer decoder to generate y Seq . Moreover, we also perform an ablation study to investigate the effectiveness of generated posterior information for prior knowledge selection. We remove the predicted posterior information Î in Equation <ref type="formula" target="#formula_12">9</ref>, but still use L PIPM in Equation 10 for comparison. These results are reported in the upper part (row 1 ∼ 5) of Table <ref type="table" target="#tab_4">6</ref> and the observations are stated as follows: (1) Compared with SKT in row 0, the variants of posterior information in row 1 ∼ 4 bring improvement on selection accuracy, though some generation metrics are slightly lower because of the exposure bias of knowledge selection. (2) From row 1 and 2, we see that the sequence information in y Seq contributes to the knowledge selection. However, it is inefficient to generate y Seq word by word, and y Seq is not better than y x BOW in row 4. And there is no significant difference between y x BOW and yk x BOW which combines another source of posterior information.</p><formula xml:id="formula_23"># Method Test Seen Test Unseen # Parameters Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ Acc ↑ PPL ↓ R-1 ↑ R-2 ↑ 0 SKT</formula><p>(3) We report the ablation result in row 5 to investigate the effectiveness of generated posterior information for knowledge selection. We see that this information improves the selection accuracy (compared with row 4).</p><p>KDBTS. We investigate the training strategy in the lower part (row 6 ∼ 9) of Table <ref type="table" target="#tab_4">6</ref> and the observations are stated as follows: (1) Comparing row 6 and 7, we see that SKT (KLfix) † is a good teacher with a much higher selection accuracy because L fix KL in Equation 11 guarantees the posterior selection module not affected by the prior distribution. When using L KL in Equation 2, SKT † achieves the lower accuracy (32.8 vs 52.0) and the lower KL divergence (0.31 vs 1.41), which indicates that the posterior module is affected by the prior distribution. (2) Despite doing well in knowledge selection, SKT (KLfix) in row 8 performs worse in generation than SKT. Because SKT (KLfix) has a larger KL divergence (1.41 vs 0.31) than SKT<ref type="foot" target="#foot_3">foot_3</ref> , it has to generate the response with the knowledge selected from a much more unfamiliar prior distribution.</p><p>(3) As our KDBTS in row 9 does not suffer from the exposure bias of knowledge selection, the generation quality is improved significantly. We are amazed at finding that the selection accuracy is also improved and we argue that the KDBTS drives the prior distribution to approximate the posterior distribution at most by minimizing the KL term with the posterior distribution fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>Table <ref type="table">7</ref> shows two example response on the Wizard of Wikipedia dataset. We see that the posterior information Î, e.g., "rock band" and "formed", provided by the PIPM, is helpful for knowledge selection, although there is some noise information, e.g., "american". And quantitative results in Section 5.1 show that this module is effective although not perfect. We see that the responses generated by SKT have repeated words in case 1 and do not reply directly in case 2. Our method can leverage the selected knowledge sufficiently to generate a fluent and informative response because our KDBTS removes the exposure bias of knowledge selection by training the decoder with knowledge selected in the same way as the inference process.</p><p>Case 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>I used to listen to the rolling stones a lot when i was a child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Golden Knowledge</head><p>The rolling stones are an English rock band formed in london, england in 1962.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>Me too. I can't believe they formed in London as far back as 1962! what's your favorite song? SKT</p><p>The rolling stones are an English rock band formed in London in London.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Posterior Information in BOW format (Top 20)</p><p>[PAD] in the rock band , they formed is are ? of a you i ' ! from and american</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>I love the rolling stones! They are an English rock band formed in 1962.</p><p>Case 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>How do teams score points? Golden Knowledge Points are primarily scored by advancing the ball into the opposing team's end zone for a touchdown or kicking the ball through the opponent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>Points are scored by advancing the ball into the opposing teams end zone for a touchdown or kicking it through the opponents goalposts for a field goal. good question!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKT</head><p>The object is to score by getting the ball into the opposing goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Posterior Information in BOW format (Top 20)</p><p>the players [PAD] of , is goal a ball and to team on it each in score points most field</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>Points are scored by advancing the ball into the opposing team's end zone.</p><p>Table <ref type="table">7</ref>: Examples of generated responses on the Wizard of Wikipedia dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We mainly focus on knowledge selection in open domain knowledge-grounded dialogue, and there are several work studying this issue <ref type="bibr" target="#b7">(Dinan et al., 2019;</ref><ref type="bibr" target="#b16">Lian et al., 2019;</ref><ref type="bibr" target="#b13">Kim et al., 2020)</ref>. Here, we point out the gap between prior and posterior knowledge selection and try to deal with this gap.</p><p>The PIPM has some relations with several work. Deliberation Decoder <ref type="bibr" target="#b32">(Xia et al., 2017;</ref><ref type="bibr" target="#b31">Wang et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2019)</ref> leverages two decoders for two-pass generation. We also use two decoders in PIPM (y Seq ), but the first decoder is used to generate posterior information for the knowledge selection rather than the second-pass generation. BOW loss, proposed by <ref type="bibr" target="#b38">(Zhao et al., 2017)</ref>, is adopted to supervise the posterior module <ref type="bibr" target="#b16">(Lian et al., 2019)</ref>. Here, we have three different aspects: (1) We use the BOW loss for the prior module rather than the posterior module; (2) We use posterior information in BOW format to enhance the prior selection module; (3) We explore several BOW variants.</p><p>Our KDBTS is inspired by knowledge distillation <ref type="bibr" target="#b11">(Hinton et al., 2015)</ref>. Instead of using more complex structure as the teacher for model compression, we treat the posterior selection module with additional input information (e,g., the response) as the teacher, and deal with the exposure bias of knowledge selection. Lite ELBO <ref type="bibr">(Zhao et al., 2019a)</ref> is proposed to remove the exposure bias at latent space for a different task. However, Lite ELBO naturally does not leverage the posterior distribution as it sets the posterior module the same as the prior module. Our KDBTS is a two-stage training strategy that uses the posterior distribution as the teacher to guide the prior selection module and uses the knowledge selected from the prior distribution to train the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we firstly analyze the gap between prior and posterior knowledge selection for opendomain knowledge-grounded dialogue. Then, we deal with it on two aspects: (1) We enhance the prior selection module with the posterior information obtained by the PIPM and we explore several variants of posterior information. (2) We design the KDBTS to train the decoder with knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experiments show that both PIPM and KDBTS improve the state-of-the-art latent variable model and their combination achieves further improvement. In the future, we would explore three aspects: (1) more efficient posterior information representation and corresponding prediction module, (2) the interpretability of knowledge selection and (3) knowledge selection without knowledge label.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our Framework. Firstly, we enhance the prior selection module with the necessary posterior information Î obtained from the Posterior Information Prediction Module (PIPM). Secondly, we design a two-stage Knowledge Distillation Based Training Strategy (KDBTS). At the first stage, KDBTS trains the posterior selection module (the teacher) with the red line. At the second stage, response generation is based on the knowledge selected by the prior module, which is guided by the well-trained teacher, and we only update the green blocks at this stage.</figDesc><graphic coords="4,117.35,62.81,362.84,122.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on the Wizard of Wikipedia dataset. Both PIPM and KDBTS bring about improvement over the state-of-the-art model (SKT) and their combination obtains further improvement on most metrics.</figDesc><table><row><cell></cell><cell>22.5</cell><cell>63.5</cell><cell>16.9</cell><cell>NA</cell><cell>12.2</cell><cell>97.3</cell><cell>14.4</cell><cell>NA</cell></row><row><cell>1 PostKS</cell><cell>4.8</cell><cell>79.1</cell><cell>13.0</cell><cell>1.0</cell><cell cols="2">4.2 193.8</cell><cell>13.1</cell><cell>1.0</cell></row><row><cell>2 TMN BERT</cell><cell>23.7</cell><cell>53.5</cell><cell>16.8</cell><cell>4.5</cell><cell cols="2">13.6 105.7</cell><cell>13.5</cell><cell>2.2</cell></row><row><cell>3 TMN BERT+PostKS+CP</cell><cell>25.5</cell><cell>52.2</cell><cell>19.0</cell><cell>6.5</cell><cell>14.4</cell><cell>83.4</cell><cell>15.6</cell><cell>3.9</cell></row><row><cell>4 SKT</cell><cell>26.8</cell><cell>52.0</cell><cell>19.3</cell><cell>6.8</cell><cell>18.3</cell><cell>81.4</cell><cell>16.1</cell><cell>4.2</cell></row><row><cell>5 SKT+PIPM (Ours)</cell><cell>27.9</cell><cell>53.1</cell><cell>19.6</cell><cell>7.0</cell><cell>19.6</cell><cell>78.5</cell><cell>17.0</cell><cell>4.9</cell></row><row><cell>6 SKT+KDBTS (Ours)</cell><cell>27.3</cell><cell>46.0</cell><cell>19.9</cell><cell>7.3</cell><cell>20.6</cell><cell>68.8</cell><cell>17.2</cell><cell>5.1</cell></row><row><cell>7 SKT+PIPM+KDBTS (Ours)</cell><cell>27.7</cell><cell>42.7</cell><cell>19.9</cell><cell>7.3</cell><cell>19.4</cell><cell>65.7</cell><cell>17.6</cell><cell>5.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on the Holl-E dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">22.7 140.6</cell><cell>20.1</cell><cell>10.3</cell><cell>32.3</cell><cell>83.6</cell><cell>24.3</cell><cell>12.8</cell></row><row><cell cols="2">1 PostKS</cell><cell></cell><cell cols="2">1.5 196.6</cell><cell>15.2</cell><cell>6.0</cell><cell cols="2">3.2 114.1</cell><cell>19.2</cell><cell>7.9</cell></row><row><cell cols="2">2 TMN BERT</cell><cell></cell><cell cols="2">28.2 112.6</cell><cell>25.9</cell><cell>18.3</cell><cell>37.5</cell><cell>66.9</cell><cell>31.1</cell><cell>22.7</cell></row><row><cell cols="2">3 TMN BERT+PostKS+CP</cell><cell></cell><cell>27.8</cell><cell>47.4</cell><cell>29.2</cell><cell>22.3</cell><cell>37.8</cell><cell>27.9</cell><cell>35.9</cell><cell>29.0</cell></row><row><cell cols="2">4 SKT</cell><cell></cell><cell>29.2</cell><cell>48.9</cell><cell>29.8</cell><cell>23.1</cell><cell>39.2</cell><cell>28.5</cell><cell>36.5</cell><cell>29.7</cell></row><row><cell cols="2">5 SKT+PIPM (Ours)</cell><cell></cell><cell>31.0</cell><cell>47.5</cell><cell>30.2</cell><cell>23.8</cell><cell>41.1</cell><cell>27.0</cell><cell>37.4</cell><cell>31.0</cell></row><row><cell cols="2">6 SKT+KDBTS (Ours)</cell><cell></cell><cell>30.3</cell><cell>39.7</cell><cell>30.8</cell><cell>23.8</cell><cell>40.1</cell><cell>23.2</cell><cell>37.4</cell><cell>30.5</cell></row><row><cell cols="3">7 SKT+PIPM+KDBTS (Ours)</cell><cell>30.6</cell><cell>39.2</cell><cell>30.8</cell><cell>23.9</cell><cell>40.6</cell><cell>23.1</cell><cell>37.7</cell><cell>30.7</cell></row><row><cell>Method</cell><cell>Test Seen</cell><cell cols="2">Test Unseen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SKT</cell><cell cols="3">48.2 (52.7&amp;43.7) 45.5 (50.7&amp;40.3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="3">60.8 (65.0&amp;56.7) 55.2 (61.3&amp;50.0)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Human 81.3 (81.7&amp;81.0) 80.7 (81.0&amp;80.3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Qualitative results on Wizard of Wikipedia. Ours denotes SKT+PIPM+KDBTS. The scores in each cell are "SSA (Sensibleness&amp;Specificity)".</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results for model analysis on the Wizard of Wikipedia dataset. The default setting of PIPM is y x BOW . Models with † are teachers which are not comparable as they use the posterior knowledge selection at inference, and their results can be regarded as the upper bound. KLfix means that we replace L KL with L fix KL .</figDesc><table><row><cell></cell><cell>26.8</cell><cell>52.0</cell><cell>19.3</cell><cell>6.8</cell><cell>18.3</cell><cell>81.4</cell><cell>16.1</cell><cell>4.2</cell><cell>174056704</cell></row><row><cell>1 SKT+PIPM (y Seq )</cell><cell>27.9</cell><cell>51.9</cell><cell>19.1</cell><cell>6.7</cell><cell>18.8</cell><cell>77.5</cell><cell>16.0</cell><cell>4.0</cell><cell>192361216</cell></row><row><cell>2 SKT+PIPM (y BOW )</cell><cell>27.3</cell><cell>51.5</cell><cell>19.3</cell><cell>7.0</cell><cell>18.4</cell><cell>78.0</cell><cell>16.1</cell><cell>4.3</cell><cell>192664890</cell></row><row><cell>3 SKT+PIPM (yk x BOW ) 4 SKT+PIPM (y x BOW )</cell><cell>27.6 27.9</cell><cell>52.2 53.1</cell><cell>19.0 19.6</cell><cell>6.8 7.0</cell><cell>19.8 19.6</cell><cell>78.1 78.5</cell><cell>16.3 17.0</cell><cell>4.6 4.9</cell><cell>192664890 192664890</cell></row><row><cell>5 SKT+PIPM (w/o update)</cell><cell>26.9</cell><cell>51.8</cell><cell>19.3</cell><cell>7.2</cell><cell>19.0</cell><cell>77.0</cell><cell>16.6</cell><cell>4.7</cell><cell>192075066</cell></row><row><cell>6 SKT  † -teacher</cell><cell>32.8</cell><cell>47.4</cell><cell>20.4</cell><cell>7.5</cell><cell>20.2</cell><cell>75.0</cell><cell>17.0</cell><cell>4.8</cell><cell>174056704</cell></row><row><cell>7 SKT (KLfix)  † -teacher</cell><cell>52.0</cell><cell>43.1</cell><cell>24.1</cell><cell>10.4</cell><cell>38.3</cell><cell>61.6</cell><cell>20.3</cell><cell>7.1</cell><cell>174056704</cell></row><row><cell>8 SKT (KLfix)</cell><cell>28.0</cell><cell>73.3</cell><cell>18.5</cell><cell>6.5</cell><cell>20.6</cell><cell>99.3</cell><cell>15.8</cell><cell>4.4</cell><cell>174056704</cell></row><row><cell>9 SKT+KDBTS</cell><cell>27.4</cell><cell>45.9</cell><cell>20.0</cell><cell>7.3</cell><cell>20.5</cell><cell>68.6</cell><cell>17.2</cell><cell>5.1</cell><cell>174056704</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code is available at https://github.com/ youngornever/bridge_latent_knowledge_ selection_gap_for_conversation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Response is only used for the posterior selection module at the training stage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We thank the authors for releasing their code and datasets at https://github.com/bckim92/ sequential-knowledge-transformer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that SKT and SKT † are the same network tested with prior and posterior knowledge selection, respectively. Hence, the KL divergence is same.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments. This work was supported by the <rs type="funder">Key Programs of the Chinese Academy of Sciences</rs> (<rs type="grantNumber">ZDBS-SSW-JSC006</rs>) and the <rs type="programName">Strategic Priority Research Program</rs> of the <rs type="funder">Chinese Academy of Sciences</rs> (Grant No. <rs type="grantNumber">XDB32070000</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BCT7DWG">
					<idno type="grant-number">ZDBS-SSW-JSC006</idno>
					<orgName type="program" subtype="full">Strategic Priority Research Program</orgName>
				</org>
				<org type="funding" xml:id="_SQQPjws">
					<idno type="grant-number">XDB32070000</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Chris Tar, et al. 2018. Universal sentence encoder</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge</title>
		<author>
			<persName><forename type="first">Debanjan</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustinus</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A working memory model for task-oriented dialog response generation</title>
		<author>
			<persName><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1258</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2687" to="2693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wizard of Wikipedia: Knowledge-Powered Conversational Agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gottardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anu</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raefer</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1891" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue. arXiv: Computation and Language</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental Transformer with Deliberation Decoder for Document Grounded Conversations</title>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5081" to="5087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06449</idno>
		<title level="m">RefNet: A reference-aware network for background based conversation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient and Robust Question Answering from Minimal Context over Documents</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards Exploiting Background Knowledge for Building Conversation Systems</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Moghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2322" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revealing the Importance of Semantic Retrieval for Machine Reading at Scale</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</title>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1782" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extending Neural Generative Conversational Model using External Knowledge Sources</title>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5427" to="5436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Thinking Globally, Acting Locally: Distantly Supervised Global-to-Local Knowledge Selection for Background Based Conversation</title>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09528</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">History-adaption Knowledge Incorporation Mechanism for Multi-turn Dialogue System</title>
		<author>
			<persName><forename type="first">Yajing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge-Aware Self-Attention Networks for Document Grounded Dialogue Generation</title>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="400" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Soft Prototype</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Xiang Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6313" to="6322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DEEP-COPY: Grounded Response Generation with Hierarchical Pointer Networks</title>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan-Lin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amazon</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge-grounded response generation with deep attentional latent-variable model</title>
		<author>
			<persName><forename type="first">Kai-Lin</forename><surname>Hao-Tong Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="page">101069</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Personalizing Dialogue Agents: I have a dog, do you have pets too?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the gap between training and inference for neural machine translation</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A documentgrounded matching network for response selection in retrieval-based chatbots</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5443" to="5449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enhancing Conversational Dialogue Models with Grounded Knowledge</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="709" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Commonsense Knowledge Aware Conversation Generation with Graph Attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
