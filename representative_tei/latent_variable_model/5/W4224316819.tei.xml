<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intent Contrastive Learning for Sequential Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-02-05">5 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongjun</forename><surname>Chen</surname></persName>
							<email>yongjun.chen@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
							<email>zhiweiliu@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<email>jia.li@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<email>jmcauley@eng.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Salesforce Research Palo Alto</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Salesforce Research Palo Alto</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Salesforce Research Palo Alto</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UC</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Salesforce Research Palo Alto</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intent Contrastive Learning for Sequential Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-05">5 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3512090</idno>
					<idno type="arXiv">arXiv:2202.02519v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Personalization; Recommender systems Latent Factor Modeling</term>
					<term>Self-Supervised Learning</term>
					<term>Contrastive Learning</term>
					<term>Robustness</term>
					<term>Sequential Recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Users' interactions with items are driven by various intents (e.g., preparing for holiday gifts, shopping for fishing equipment, etc.). However, users' underlying intents are often unobserved/latent, making it challenging to leverage such latent intents for Sequential recommendation (SR). To investigate the benefits of latent intents and leverage them effectively for recommendation, we propose Intent Contrastive Learning (ICL), a general learning paradigm that leverages a latent intent variable into SR. The core idea is to learn users' intent distribution functions from unlabeled user behavior sequences and optimize SR models with contrastive self-supervised learning (SSL) by considering the learnt intents to improve recommendation. Specifically, we introduce a latent variable to represent users' intents and learn the distribution function of the latent variable via clustering. We propose to leverage the learnt intents into SR models via contrastive SSL, which maximizes the agreement between a view of sequence and its corresponding intent. The training is alternated between intent representation learning and the SR model optimization steps within the generalized expectationmaximization (EM) framework. Fusing user intent information into SR also improves model robustness. Experiments conducted on four real-world datasets demonstrate the superiority of the proposed learning paradigm, which improves performance, and robustness against data sparsity and noisy interaction issues 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems have been widely used in many scenarios to provide personalized items to users over massive vocabularies of items. The core of an effective recommender system is to accurately predict users' interests toward items based on their historical interactions. With the success of deep learning, deep Sequential Recommendation (SR) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> models, which aims at dynamically characterizing the behaviors of users with different deep neural networks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46]</ref>, arguably represents the current state-of-theart <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. In general, a deep SR model is trained based on users' interaction behaviors via a deep neural network, assuming users' interests depending on historical behaviors. However, consuming behaviour of users can be affected by other latent factors, i.e., driven by their underlying intents. Consider the example illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Two users purchased a series of different items on Amazon in the past. Given such distinct interaction behaviors, the system will recommend different items to them. However, both of them are fishing enthusiasts and are shopping for fishing activities. As a result, they both purchase 'fishing swivels' in the future. If the system is aware that these two users are shopping for fishing activities, then commonly purchased items for fishing, such as 'fishing swivels' can be suggested. This motivates us to mine underlying intents that are shared across users and use the learnt intents to guide system providing recommendations.</p><p>Precisely discovering the intents of users, however, is underexplored. Most existing works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref> of user intent modeling require side information. ASLI <ref type="bibr" target="#b36">[37]</ref> leverages user action types (e.g., click, add-to-favorite, etc.) to capture users' intentions, whereas such information is not always available in system. CoCoRec <ref type="bibr" target="#b0">[1]</ref> utilizes item category information. But we argue that categorical feature is unable to accurately represents users' intents. For example, intents like 'shopping for holiday gifts' may involve items from multiple different categories. DSSRec <ref type="bibr" target="#b26">[27]</ref> proposes a seq2seq training strategy, which optimizes the intents in latent spaces. However, those intents in DSSRec are inferred solely based on individual sequence representation, while ignoring the underlying correlations of the intents from different users.</p><p>Effectively modeling latent intents from user behaviors poses two challenges. First, it is extremely difficult to learn latent intents accurately because we have no labelling data for intents. The only available supervision signals for intents are the user behavior data. Nevertheless, as aforementioned example indicates, distinct behaviors may reflect the same intent. Besides, effectively fusing intent information into a SR model is non-trivial. The target in SR is to predict next items in sequences, which is solved by encoding sequences. Leveraging latent intents of sequences into the model requires the intent factors to be orthogonal to the sequence embeddings, which otherwise would induce redundant information.</p><p>To discover the benefits of latent intents and address challenges, we propose the Intent Contrastive Learning (ICL), a general learning paradigm that leverages the latent intent factor into SR. It learns users' intent distributions from all user behavior sequences via clustering. And it leverages the learnt intents into the SR model via a new contrastive SSL, which maximizes the agreement between a view of sequence and its corresponding intent. The intent representation learning module and the contrastive SSL module are mutually reinforced to train a more expressive sequence encoder. We tackle the challenge of intent mining problem by introducing a latent variable to represent users' intents and learn them alternately along with the SR model optimization through an expectation-maximization (EM) framework to ensure convergence. We suggest fusing learnt intent information into SR via the proposed contrastive SSL, as it can improve model's performance as well as robustness. Extensive experiments conducted on four real-world datasets further verify the effectiveness of the proposed learning paradigm, which improves performance and robustness, even when recommender systems face heavy data sparsity issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Sequential Recommendation</head><p>Sequential recommendation aims to accurately characterize users' dynamic interests by modeling their past behavior sequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. Early works on SR usually model an item-to-item transaction pattern based on Markov Chains <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>. FPMC <ref type="bibr" target="#b33">[34]</ref> combines the advantages of Markov Chains and matrix factorization to fuse both sequential patterns and users' general interest.</p><p>With the recent advances of deep learning, many deep sequential recommendation models are also developed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Such as Convolutional Neural Networks (CNN)-based <ref type="bibr" target="#b35">[36]</ref> and RNNbased <ref type="bibr" target="#b11">[12]</ref> models. The recent success of Transformer <ref type="bibr" target="#b39">[40]</ref> also motivates the developments of pure Transformer-based SR models. SASRec <ref type="bibr" target="#b12">[13]</ref> utilizes unidirectional Transformer to assign weights to each interacted item adaptively. BERT4Rec <ref type="bibr" target="#b34">[35]</ref> improves that by utilizing a bidirectional Transformer with a Cloze task <ref type="bibr" target="#b37">[38]</ref> to fuse user behaviors information from left and right directions into each item. LSAN <ref type="bibr" target="#b20">[21]</ref> improves SASRec on reducing model size perspective. It proposes a temporal context-aware embedding and twin-attention network, which are light weighted. ASReP <ref type="bibr" target="#b23">[24]</ref> further alleviates the data-sparsity issue by leveraging a pre-trained Transformer on the revised user behavior sequences to augment short sequences. In this paper, we study the potential of addressing data sparsity issues and improving SR via self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">User Intent for Recommendation</head><p>Recently, many approaches have been proposed to study users' intents for improving recommendations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41]</ref>. MCPRN <ref type="bibr" target="#b40">[41]</ref> designs mixture-channel purpose routing networks to adaptively learn users' different purchase purposes of each item under different channels (sub-sequences) for session-based recommendation. MITGNN <ref type="bibr" target="#b24">[25]</ref> proposes a multi-intent translation graph neural network to mine users' multiple intents by considering the correlations of the intents. ICM-SR <ref type="bibr" target="#b30">[31]</ref> designs an intent-guided neighbor detector to retrieve correct neighbor sessions for neighbor representation. Different from session-based recommendation, another line of works focus on modeling the sequential dynamics of users' interaction behaviors in a longer time span. DSSRec <ref type="bibr" target="#b26">[27]</ref> proposes a seq2seq training strategy using multiple future interactions as supervision and introducing an intent variable from her historical and future behavior sequences. The intent variable is used to capture mutual information between an individual user's historical and future behavior sequences. Two users of similar intents might be far away in representation space. Unlike this work, our intent variable is learned over all users' sequences and is used to maximize mutual information across different users with similar learned intents. ASLI <ref type="bibr" target="#b36">[37]</ref> captures intent via a temporal convolutional network with side information (e.g., user action types such as click, add-to-favorite, etc.), and then use the learned intents to guide SR model to predict the next item. Instead, our method can learn users' intents based on user interaction data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Self-Supervised Learning</head><p>Contrastive Self-Supervised Learning (SSL) has brought much attentions by different research communities including CV <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> and NLP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50]</ref>, as well as recommendation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>. The fundamental goal of contrastive SSL is to maximize mutual information among the positive transformations of the data itself while improving discrimination ability to the negatives. In reccommendation, A two-tower DNN-based contrastive SSL model is proposed in <ref type="bibr" target="#b46">[47]</ref>. It aims to improving collaborative filtering based recommendation leveraging item attributes. SGL <ref type="bibr" target="#b43">[44]</ref> adopts a multi-task framework with contrastive SSL to improve the graph neural networks (GCN)-based collaborative filtering methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49]</ref> with only item IDs as features. Specific to SR, S 3 -Rec <ref type="bibr" target="#b50">[51]</ref> adopts a pre-training and fine-tuning strategy, and utilizes contrastive SSL during pre-training to incorporate correlations among items, sub-sequences, and attributes of a given user behavior sequence. However, the two-stage training strategy prevents the information sharing between next-item prediction and SSL tasks and restricts the performance improvement. CL4SRec <ref type="bibr" target="#b44">[45]</ref> and CoSeRec <ref type="bibr" target="#b22">[23]</ref> instead utilize a multi-task training framework with a contrastive objective to enhance user representations. Different from them, our work is aware of users' latent intent factor when leveraging contrastive SSL, which we show to be beneficial for improving recommendation performance and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES 3.1 Problem definition</head><p>Assume that a recommender system has a set of users and items denoted by U and V respectively. Each user ğ‘¢ âˆˆ U has a sequence of interacted items sorted in chronological order In practice, sequences are truncated with maximum length ğ‘‡ . If the sequence length is greater than ğ‘‡ , the most recent ğ‘‡ actions are considered. If the sequence length is less than ğ‘‡ , 'padding' items will be added to the left until the length is ğ‘‡ <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>. For each user ğ‘¢, the goal of next item prediction task is to predict the next item that the user ğ‘¢ is most likely to interact with at the |ğ‘† ğ‘¢ | + 1 step among the item set V, given sequence S ğ‘¢ .</p><formula xml:id="formula_0">ğ‘† ğ‘¢ = [ğ‘  ğ‘¢ 1 , . . . ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep SR Models for Next Item Prediction</head><p>Modern sequential recommendation models commonly encode user behavior sequences with a deep neural network to model sequential patterns from (truncated) user historical behavior sequences. Without losing generality, we define a sequence encoder ğ‘“ ğœƒ (â€¢) that encodes a sequence S ğ‘¢ and outputs user interest representations over all position steps H ğ‘¢ = ğ‘“ ğœƒ (S ğ‘¢ ). Specially, h ğ‘¢ ğ‘¡ represents user's interest at position ğ‘¡. The goal can be formulated as finding the optimal encoder parameter ğœƒ that maximizes the log-likelihood function of the expected next items of given ğ‘ sequences on all positional steps:</p><formula xml:id="formula_1">ğœƒ * = arg max ğœƒ ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =2 ln ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ ).<label>(1)</label></formula><p>which is equivalent to minimizing the adapted binary crossentropy loss as follows:</p><formula xml:id="formula_2">L NextItem = ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =2 L NextItem (ğ‘¢, ğ‘¡),<label>(2)</label></formula><formula xml:id="formula_3">L NextItem (ğ‘¢, ğ‘¡) = -log(ğœ (h ğ‘¢ ğ‘¡ -1 â€¢ s ğ‘¢ ğ‘¡ )) - âˆ‘ï¸ ğ‘›ğ‘’ğ‘” log(1 -ğœ (h ğ‘¢ ğ‘¡ -1 â€¢ s ğ‘¢ ğ‘›ğ‘’ğ‘” )),<label>(3)</label></formula><p>where s ğ‘¢ ğ‘¡ and s ğ‘¢ ğ‘›ğ‘’ğ‘” denote the embeddings of the target item ğ‘  ğ‘¡ and all items not interacted by ğ‘¢. The sum operator in Eq. 3 is computationally expensive because |ğ‘‰ | is large. Thus we follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51]</ref> to use a sampled softmax technique to randomly sample a negative item for each time step in each sequence. ğœ is the sigmoid function. And ğ‘ is refers to the mini-batch size as the SR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive SSL in SR</head><p>Recent advances in contrastive SSL have inspired the recommendation community to leverage contrastive SSL to fuse correlations among different views of one sequence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>, following the mutual information maximization (MIM) principle. Existing approaches in SR can be seen as instance discrimination tasks that optimize a lower bound of MIM, such as InfoNCE <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. It aims to optimize the proportion of gap of positive pairs and negative pairs <ref type="bibr" target="#b21">[22]</ref>. In such an instance discrimination task, sequence augmentations such as 'mask', 'crop', or 'reorder' are required to create different views of the unlabeled data in SR <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr">52]</ref>. Formally, given a sequence ğ‘† ğ‘¢ , and a pre-defined data transformation function set G, we can create two positive views of ğ‘† ğ‘¢ as follows:</p><formula xml:id="formula_4">Sğ‘¢ 1 = ğ‘” ğ‘¢ 1 (ğ‘† ğ‘¢ ), Sğ‘¢ 2 = ğ‘” ğ‘¢ 2 (ğ‘† ğ‘¢ ), s.t. ğ‘” ğ‘¢ 1 , ğ‘” ğ‘¢ 2 âˆ¼ G,<label>(4)</label></formula><p>where ğ‘” ğ‘¢ 1 and ğ‘” ğ‘¢ 2 are transformation functions sampled from G to create a different view of sequence ğ‘  ğ‘¢ . Commonly, views created from the same sequence are treated as positive pairs, and the views of any different sequences are considered as negative pairs. The augmented views are first encoded with the sequence encoder ğ‘“ ğœƒ (â€¢) to Hğ‘¢ 1 and Hğ‘¢ 2 , and then be fed into an 'Aggregation' layer to get vector representations of sequences, denoted as hğ‘¢ 1 and hğ‘¢ 2 . In this paper, we 'concatenate' users' interest representations over time steps for simplicity. Note that sequences are prepossessed to have the same length (See Sec. 3.1), thus their vector representations after concatenation have the same length too. After that, we can optimize ğœƒ via InfoNCE loss:</p><formula xml:id="formula_5">L SeqCL = L SeqCL ( hğ‘¢ 1 , hğ‘¢ 2 ) + L SeqCL ( hğ‘¢ 2 , hğ‘¢ 1 ),<label>(5)</label></formula><p>and</p><formula xml:id="formula_6">L SeqCL ( hğ‘¢ 1 , hğ‘¢ 2 ) = -log exp(sim( hğ‘¢ 1 , hğ‘¢ 2 )) ğ‘›ğ‘’ğ‘” exp(sim( hğ‘¢ 1 , hğ‘›ğ‘’ğ‘” )) ,<label>(6)</label></formula><p>where ğ‘ ğ‘–ğ‘š(â€¢) is dot product and hğ‘›ğ‘’ğ‘” are negative views' representations of sequence ğ‘† ğ‘¢ . Figure <ref type="figure" target="#fig_1">2</ref> (a) illustrates how SeqCL works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Latent Factor Modeling in SR</head><p>The main goal of next item prediction task is to optimize Eq. ( <ref type="formula" target="#formula_1">1</ref>).</p><p>Assume that there are also ğ¾ different user intents (e.g., purchasing holiday gifts, preparing for fishing activity, etc.) in a recommender system that forms the intent variable ğ‘ = {ğ‘ ğ‘– } ğ¾ ğ‘–=1 , then the probability of a user interacting with a certain item can be rewritten as follows:</p><formula xml:id="formula_7">ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ) = E (ğ‘) ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ , ğ‘) .<label>(7)</label></formula><p>However, users intents are latent by definition. Because of the missing observation of variable ğ‘, we are in a 'chicken-and-eggs' situation that without ğ‘, we cannot estimate parameter ğœƒ , and without ğœƒ we cannot infer what the value of ğ‘ might be.</p><p>Later, we will show that a generalized Expectation-Maximization framework provides a direction to address above problem with a convergence guarantee. The basic idea of optimizing Eq. ( <ref type="formula" target="#formula_7">7</ref>) via </p><formula xml:id="formula_8">Sequences â€¦ Aggregation Encoder ğ‘“ ! (#) Mask i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 5 i 8 i 9 i 1 i 2 i 4 i 1 i 3 i 5 i 1 i 2 i 4 i 3 i 6 i 7 i 5 i 1 i 9 i 2 i 3 i 1 i 4 i 5</formula><p>Sequence Augmentations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-batch Sequences</head><p>User Intent Retrieval:</p><p>Positive pair: Negative pair:</p><formula xml:id="formula_9">Encoder ğ‘“ ! (#) Mini-batch Sequences Sequence Augmentations (b) (a)</formula><p>Sequences with same intent are pushed away EM is to start with an initial guess of the model parameter ğœƒ and estimate the expected values of the missing variable ğ‘, i.e., the Estep. And once we have the values of ğ‘, we can maximize the Eq. ( <ref type="formula" target="#formula_7">7</ref>) w.r.t the parameter ğœƒ , i.e., the M step. We can repeat this iterative process until the likelihood cannot increase anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>The overview of the proposed ICL within EM framework is presented in Figure <ref type="figure" target="#fig_1">2</ref>  In the following sections, we first derive the objective function in order to model the latent intent variable ğ‘ into an SR model, and how to alternately optimize the objective function w.r.t. ğœƒ and estimate the distribution of ğ‘ under a generalized EM framework in Section 4.1. Then we describe the overall training strategy in Section 4.2. We provide detailed analyses in Section 4.3 followed by experimental studies in Section 5. ğ‘–=1 that affect users' decisions to interact with items, then based on Eq. ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_7">7</ref>), we can rewrite objective as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intent Contrastive Learning</head><formula xml:id="formula_10">ğœƒ * = arg max ğœƒ ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ln E (ğ‘) ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ) ,<label>(8)</label></formula><p>which is however hard to optimize. Instead, we construct a lowerbound function of Eq. ( <ref type="formula" target="#formula_10">8</ref>) and maximize the lower-bound. Formally, assume intent ğ‘ follows distribution ğ‘„ (ğ‘), where ğ‘ ğ‘„ (ğ‘ ğ‘– ) = 1 and</p><formula xml:id="formula_11">ğ‘„ (ğ‘ ğ‘– ) â‰¥ 0. Then we have ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ln E (ğ‘) ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ) = ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ln ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ) = ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ln ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘„ (ğ‘ ğ‘– ) ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ) ğ‘„ (ğ‘ ğ‘– ) .<label>(9)</label></formula><p>Based on the Jensen's inequality, the term in Eq. ( <ref type="formula" target="#formula_11">9</ref>) is</p><formula xml:id="formula_12">â‰¥ ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘„ (ğ‘ ğ‘– ) ln ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ) ğ‘„ (ğ‘ ğ‘– ) âˆ ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘„ (ğ‘ ğ‘– ) â€¢ ln ğ‘ƒ ğœƒ (ğ‘  ğ‘¢ ğ‘¡ , ğ‘ ğ‘– ),<label>(10)</label></formula><p>where the âˆ stands for 'proportional to' (i.e. up to a multiplicative constant). The inequality will hold with equality when ğ‘„ (ğ‘ ğ‘– ) = ğ‘ƒ ğœƒ (ğ‘ ğ‘– |ğ‘  ğ‘¢ ğ‘¡ ). For simplicity, we only focus on last positional step when optimize the lower-bound, which is defined as:</p><formula xml:id="formula_13">ğ‘ âˆ‘ï¸ ğ‘¢=1 ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘„ (ğ‘ ğ‘– ) â€¢ ln ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ , ğ‘ ğ‘– ),<label>(11)</label></formula><p>where</p><formula xml:id="formula_14">ğ‘„ (ğ‘ ğ‘– ) = ğ‘ƒ ğœƒ (ğ‘ ğ‘– |ğ‘† ğ‘¢ ).</formula><p>So far, we have found a lower-bound of Eq. ( <ref type="formula" target="#formula_10">8</ref>). However, we cannot directly optimize Eq. ( <ref type="formula" target="#formula_13">11</ref>) because ğ‘„ (ğ‘) is unknown. Instead, we alternately optimize the model between the Intent Representation Learning (E-step) and the Intent Contrastive SSL with FNM (M-step), which follows a generalized EM framework. We term the whole processes Intent Contrastive Learning (ICL). In each iteration, ğ‘„ (ğ‘) and the model parameter ğœƒ are updated. to obtain ğ¾ clusters. After that, we can define the distribution function ğ‘„ (ğ‘ ğ‘– ) as follows:</p><formula xml:id="formula_15">ğ‘„ (ğ‘ ğ‘– ) = ğ‘ƒ ğœƒ (ğ‘ ğ‘– |ğ‘† ğ‘¢ ) = 1 if ğ‘† ğ‘¢ in cluster ğ‘– 0 else. (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>We denote c i as the vector representation of intent ğ‘ ğ‘– , which is the centroid representation of the ğ‘– ğ‘¡â„ cluster. In this paper, we use 'aggregation layer' to denote the the mean pooling operation over all position steps for simplicity. We leave other advanced aggregation methods such as attention-based methods for future work studies. Figure <ref type="figure" target="#fig_1">2</ref> (b) illustrates how the E-step works.</p><p>4.1.3 Intent Contrastive SSL with FNM. We have estimated the distribution function ğ‘„ (ğ‘). To maximize Eq. ( <ref type="formula" target="#formula_13">11</ref>), we also need to define ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ , ğ‘ ğ‘– ). Assuming that the prior over intents follow the uniform distribution and the conditional distribution of ğ‘† ğ‘¢ given ğ‘ is isotropic Gaussian with ğ¿2 normalization, then we can rewrite ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ , ğ‘ ğ‘– ) as follows:</p><formula xml:id="formula_17">ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ , ğ‘ ğ‘– ) = ğ‘ƒ ğœƒ (ğ‘ ğ‘– )ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ |ğ‘ ğ‘– ) = 1 ğ¾ â€¢ ğ‘ƒ ğœƒ (ğ‘† ğ‘¢ |ğ‘ ğ‘– ) âˆ 1 ğ¾ â€¢ exp(-(h ğ‘¢ -c ğ‘– ) 2 ) ğ¾ ğ‘—=1 exp(-(h ğ‘¢ ğ‘– -c ğ‘— ) 2 ) âˆ 1 ğ¾ â€¢ exp(h ğ‘¢ â€¢ c ğ‘– ) ğ¾ ğ‘—=1 exp(h ğ‘¢ â€¢ c ğ‘— ) ,<label>(13)</label></formula><p>where h ğ‘¢ and c ğ‘¢ are vector representations of ğ‘† ğ‘¢ and ğ‘ ğ‘– , respectively. Based on Eq. ( <ref type="formula" target="#formula_13">11</ref>), ( <ref type="formula" target="#formula_15">12</ref>), ( <ref type="formula" target="#formula_17">13</ref>), maximizing Eq. ( <ref type="formula" target="#formula_13">11</ref>) is equivalent to minimize the following loss function:</p><formula xml:id="formula_18">- ğ‘ âˆ‘ï¸ ğ‘£=1 log exp(sim(h ğ‘¢ , c ğ‘– )) ğ¾ ğ‘—=1 exp(sim(h ğ‘¢ , c ğ‘— )) ,<label>(14)</label></formula><p>where sim(â€¢) is a dot product. We can see that Eq. ( <ref type="formula" target="#formula_18">14</ref>) has a similar form as Eq. ( <ref type="formula" target="#formula_6">6</ref>), where Eq. ( <ref type="formula" target="#formula_6">6</ref>) tries to maximize mutual information between two individual sequences. While Eq. ( <ref type="formula" target="#formula_18">14</ref>) maximizes mutual information between one individual sequence and its corresponding intent. Note that, sequence augmentations are required in SeqCL to create positive views for Eq. ( <ref type="formula" target="#formula_6">6</ref>). While in ICL, sequence augmentations are optional, as the view of a given sequence is its corresponding intent that learnt from original dataset. In this paper, we apply sequence augmentations for enlarging training set purpose and optimize model w.r.t ğœƒ based on Eq. ( <ref type="formula" target="#formula_18">14</ref>). Formally, given a batch of training sequences {ğ‘  ğ‘¢ } ğ‘ ğ‘¢=1 , we first create two positive views of a sequence via Eq. ( <ref type="formula" target="#formula_4">4</ref>), and then optimize the following loss function:</p><formula xml:id="formula_19">L ICL = L ICL ( hğ‘¢ 1 , c ğ‘¢ ) + L ICL ( hğ‘¢ 2 , c ğ‘¢ ),<label>(15)</label></formula><p>and</p><formula xml:id="formula_20">L ICL ( hğ‘¢ 1 , c ğ‘¢ ) = -log exp(sim( hğ‘¢ 1 , c ğ‘¢ )) ğ‘›ğ‘’ğ‘” exp(sim( hğ‘¢ 1 , c ğ‘›ğ‘’ğ‘” )) ,<label>(16)</label></formula><p>where ğ‘ ğ‘›ğ‘’ğ‘” are all the intents in the given batch. However, directly optimizing Eq. ( <ref type="formula" target="#formula_20">16</ref>) can introduce false-negative samples since users in a batch can have same intent. To mitigate the effects of falsenegatives, we propose a simple strategy to mitigate the effects by not contrasting against them:</p><formula xml:id="formula_21">L ICL ( hğ‘¢ 1 , c ğ‘¢ ) = -log exp(sim( hğ‘¢ 1 , c ğ‘¢ )) ğ‘ ğ‘£=1 1 ğ‘£âˆ‰F exp(sim( h1 , c ğ‘£ )) , (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>where F is a set of users that have same intent as ğ‘¢ in the minibatch. We term this False-Negative Mitigation (FNM). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Task Learning</head><p>We train the SR model with a multi-task training strategy to jointly optimize ICL via Eq. ( <ref type="formula" target="#formula_21">17</ref>), the main next-item prediction task via Eq. ( <ref type="formula" target="#formula_2">2</ref>) and a sequence level SSL task via Eq. ( <ref type="formula" target="#formula_5">5</ref>). Formally, we jointly train the SR model ğ‘“ ğœƒ as follows:</p><formula xml:id="formula_23">L = L NextItem + ğœ† â€¢ L ICL + ğ›½ â€¢ L SeqCL ,<label>(18)</label></formula><p>where ğœ† and ğ›½ control the strengths of the ICL task and sequence level SSL tasks, respectively. Appendix A provides the pseudo-code of the entire learning pipeline. Specially, we build the learning paradigm on Transformer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40]</ref> encoder to form the model ICLRec.</p><p>ICL is a model-agnostic objective, so we also apply it to S 3 -Rec [51] model, which is pre-trained with several L SeqCL objectives to capture correlations among items, associated attributes, and subsequences in a sequence and fine-tuned with the L NextItem objective, to further verify its effectiveness (see 5.4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Connections with Contrastive SSL in SR.</head><p>Recent methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref> in SR follow standard contrastive SSL to maximize mutual information between two positive views of sequences. For example, CL4SRec encodes sequences with Transformer and maximizes mutual information between two sequences that are augmented (cropping, masking, or reordering) from the original sequence. However, if the item relationships of a sequence are vulnerable to random perturbation, two views of this sequence may not reveal the original sequence correlations. ICLRec maximizes mutual information between a sequence and its corresponding intent prototype. Since the intent prototype can be considered as a positive view of a given sequence that learnt by considering the semantic structures of all sequences, which reflects true sequence correlations, the ICLRec can outperform CL4SRec consistently. Fortunately, the model can be effectively parallelized because ğ‘“ ğœƒ is Transformer and we leave it in future work. In the testing phase, the proposed ICL as well as the SeqCL objectives are no longer needed, which yields the model to have the same time complexity as SASRec (ğ‘‚ (ğ‘‘ |ğ‘‰ |)). The empirical time spending comparisons are reported in Sec. 5.2. The convergence of ICL is guaranteed under the generalized EM framework. Proof is provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setting</head><p>5.1.1 Datasets. We conduct experiments on four public datasets. Sports, Beauty and Toys are three subcategories of Amazon review data introduced in <ref type="bibr" target="#b27">[28]</ref>. Yelp<ref type="foot" target="#foot_0">foot_0</ref> is a dataset for business recommendation.</p><p>We follow <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref> to prepare the datasets. In detail, we only keep the '5-core' datasets, in which all users and items have at least 5 interactions. The statistics of the prepared datasets are summarized in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Metrics.</head><p>We follow <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref> to rank the prediction on the whole item set without negative sampling. Performance is evaluated on a variety of evaluation metrics, including Hit Ratio@ğ‘˜ (HR@ğ‘˜), and Normalized Discounted Cumulative Gain@ğ‘˜ (NDCG@ğ‘˜) where ğ‘˜ âˆˆ {5, 20}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Baseline Methods. Four groups of baseline methods are included for comparison.</head><p>â€¢ Non-sequential models: BPR-MF <ref type="bibr" target="#b32">[33]</ref> characterizes the pairwise interactions via a matrix factorization model and optimizes through a pair-wise Bayesian Personalized Ranking loss. â€¢ Standard sequential models. We include solutions that train the models with a next-item prediction objective. Caser <ref type="bibr" target="#b35">[36]</ref> is a CNN-based approach, GRU4Rec <ref type="bibr" target="#b11">[12]</ref> is an RNN-based method, and SASRec <ref type="bibr" target="#b12">[13]</ref> is one of the state-of-the-art Transformer-based baselines for SR. â€¢ Sequential models with additional SSL: BERT4Rec <ref type="bibr" target="#b34">[35]</ref> replaces the next-item prediction with a Cloze task <ref type="bibr" target="#b37">[38]</ref> to fuse information between an item (a view) in a user behavior sequence and its contextual information. S<ref type="foot" target="#foot_1">foot_1</ref> -Rec <ref type="bibr" target="#b50">[51]</ref> uses SSL to capture correlation-ship among item, sub-sequence, and associated attributes from the given user behavior sequence. Its modules for mining on attributes are removed because we don't have attributes for items, namely S 3 -Rec ğ¼ğ‘†ğ‘ƒ . CL4SRec <ref type="bibr" target="#b44">[45]</ref> fuses contrastive SSL with a Transformer-based SR model. â€¢ Sequential models considering latent factors: We include DSSRec <ref type="bibr" target="#b26">[27]</ref>, which utilizes seq2seq training and performs optimization in latent space. We do not directly compare ASLI <ref type="bibr" target="#b36">[37]</ref>, as it requires user action type information (e.g., click, add-tofavorite, etc). Instead, we provide a case study in Sec. 5.6 to evaluate the benefits of the learnt intent factor with additional item category information.</p><p>5.1.4 Implementation Details. Caser 3 , BERT4Rec<ref type="foot" target="#foot_2">foot_2</ref> , and S3-Rec<ref type="foot" target="#foot_3">foot_3</ref> are provided by the authors. BPRMF<ref type="foot" target="#foot_4">foot_4</ref> , GRU4Rec<ref type="foot" target="#foot_5">foot_5</ref> , and DSSRec<ref type="foot" target="#foot_6">foot_6</ref> are implemented based on public resources. We implement SASRec and CL4SRec in PyTorch. The mask ratio in BERT4Rec is tuned from {0.2, 0.4, 0.6, 0.8}. The number of attention heads and number of self-attention layers for all self-attention based methods (SASRec, S 3 -Rec, CL4SRec, DSSRec) are tuned from {1, 2, 4}, and {1, 2, 3}, respectively. The number of latent factors introduced in DSSRec is tuned from {1, 2, . . . , 8}.</p><p>Our method is implemented in PyTorch. Faiss<ref type="foot" target="#foot_7">foot_7</ref> is used for ğ¾means clustering to speed up the training and query stages. For the encoder architecture, we set self-attention blocks and attention heads as 2, the dimension of the embedding as 64, and the maximum sequence length as 50. The model is optimized by an Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a learning rate of 0.001, ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, and batch size of 256. For hyper-parameters of ICLRec, we tune ğ¾, ğœ† and ğ›½ within {8, 64, 128, 256, 512, 1024, 2048}, {0.1, 0.2, â€¢ â€¢ â€¢ , 0.8}, and {0.1, 0.2, â€¢ â€¢ â€¢ , 0.8} respectively. All experiments are run on a single Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the results of different methods on all datasets. We have the following observations. First, BPR performs worse than sequential models in general, which indicates the importance of mining the sequential patterns under user behavior sequences. As for standard sequential models, SASRec utilizes a Transformerbased encoder and achieves better performance than Caser and GRU4Rec. This demonstrates the effectiveness of Transformer for capturing sequential patterns. DSSRec further improves SASRec's performance by using a seq2seq training strategy and reconstructs the representation of the future sequence in latent space for alleviating non-convergence problems.</p><p>Moreover, though BERT4Rec and S 3 -Rec and adopt SSL to provide additional training signals to enhance representations , we observe that both of them exhibit worse performance than SASRec in some datasets (e.g., in the Toys dataset). The reason might be that both BERT4Rec and S 3 -Rec aim to incorporate context information of given user behavior sequences via masked item prediction. Such a goal may not align well with the next item prediction target, and it requires that each user behavior sequence is long enough to provide comprehensive 'context' information. Thus their performances are degenerated when most sequences are short. Besides, S 3 -Rec is proposed to fuse additional contextual information. Without such features, its two-stage training strategy prevents information sharing between the next-item prediction and SSL tasks, thus leading to poor results. CL4SRec consistently performs better than other baselines, demonstrating the effectiveness of enhancing sequence representations via contrastive SSL on an individual user level.</p><p>Finally, ICLRec consistently outperforms existing methods on all datasets. The average improvements compared with the best baseline ranges from 7.47% to 33.33% in HR and NDCG. The proposed ICL estimates a good distribution of intents and fuses them into SR model by a new contrastive SSL, which helps the encoder discover a good semantic structure across different user behavior sequences.</p><p>We also report the model efficiency on Sports. SASRec is the most efficient solution. It spends 3.59 s/epoch on model updates. CL4SRec and the proposed ICLRec spend 6.52 and 11.75 s/epoch,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness Analysis</head><p>Robustness w.r.t. user interaction frequency. The user 'coldstart' problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48]</ref> is one of the typical data-sparsity issues that recommender systems often face, i.e., most users have limited historical behaviors. To check whether ICL improves the robustness under such a scenario, we split user behavior sequences into three groups based on their behavior sequences' length, and keep the total number of behavior sequences the same. Models are trained and evaluated on each group of users independently. Figure <ref type="figure" target="#fig_7">3</ref> shows the comparison results on four datasets. We observe that: (1) The proposed ICLRec can consistently performs better than SASRec among all user groups while CL4SRec fails to outperform SAS-Rec in Beauty and Yelp when user behavior sequences are short. This demonstrates that CL4SRec requires individual user behavior sequences long enough to provide 'complete' information for auxiliary supervision while ICLRec reduces the need by leveraging user intent information, thus consistently benefiting user representation learning even when users have limited historical interactions.</p><p>(2) Compared with CL4SRec, we observe that the improvement of ICLRec is mainly because it provides better recommendations to users with low interaction frequency. This verifies that user intent information is beneficial, especially when the recommender system faces data-sparsity issues where information in each individual user sequence is limited.  Robustness to Noisy Data. We also conduct experiments on the Sports and Yelp datasets to verify the robustness of ICLRec against noisy interactions in the test phase. Specifically, we randomly add a certain proportion (i.e., 5%, 10%, 15%, 20%) of negative items to text sequences. From Figure <ref type="figure" target="#fig_9">4</ref> we can see that adding noisy data deteriorates the performance of CL4SRec and ICLRec. However, the performance drop ratio of ICLRec is consistently lower than CL4SRec, and its performance with 15% noise proportion can still outperforms CL4SRec without noisy dataset on Sports. The reason might be the leveraged intent information is collaborative information that distilled from all the users. ICL helps the SR model capture semantic structures from user behavior sequences, which increases the robustness of ICLRec to noisy perturbations on individual sequences. Since ICL is a model-agostic learning paradigm, we also add ICL to the S 3 -Rec ğ¼ğ‘†ğ‘ƒ <ref type="bibr" target="#b50">[51]</ref> model in the fine-tuning stage to further verify its effectiveness. Results are shown in Table . 2 (G)-(H). We find that the S 3 -Rec ğ¼ğ‘†ğ‘ƒ model also benefits from the ICL objective. The average improvement over the four datasets is 41.11% in NDCG@20, which further validate the effectiveness and practicality of ICLRec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter Sensitivity</head><p>The larger of the intent class number ğ¾ means users can have more diverse intentions. The larger value of the strength of SeqCL objective ğ›½ means the ICL task contributes more to the final model. The results on Yelp is shown in Figure <ref type="figure" target="#fig_10">5</ref>. We find that: (1) ICLRec reaches its best performance when increasing ğ¾ to 512, and then it starts to deteriorate as ğ¾ become larger. When ğ¾ is very small, the number of users under each intent prototype can potentially be large. As a result, false-positive samples (i.e., users that actually have different intents are considered as having the same intent erroneously) are introduced to the contrastive SSL, thus affecting learning. On the other hand, when ğ¾ is too large, the number of users under each intent prototype is small, the introduced falsenegative samples will also impair contrastive SSL. In Yelp, 512 user intents summarize users' distinct behaviors best. (2) A 'sweet-spot' of ğœ† = 0.5 can also be found. It indicates that the ICL task can benefit the recommendation prediction as an auxiliary task. The impact of the batch size and ğ›½ are provided in Appendix D.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study</head><p>The Sports dataset <ref type="bibr" target="#b27">[28]</ref> contains 2,277 fine-grained item categories, and the Yelp dataset provides 1,001 business categories. We utilize these attributes to study the effectiveness of the proposed ICLRec both quantitatively and qualitatively. Note that we did not use this information during the training phrase. The detailed analysis results are in Appendix E. 6  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><formula xml:id="formula_24">Sğ‘¢ 1 = ğ‘” ğ‘¢ 1 (ğ‘† ğ‘¢ ), Sğ‘¢ 2 = ğ‘” ğ‘¢ 2 (ğ‘† ğ‘¢ ), ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘” ğ‘¢ 1 , ğ‘” ğ‘¢ 2 âˆ¼ G // Encoding via ğ‘“ ğœƒ (â€¢) 7 h ğ‘¢ = ğ‘“ ğœƒ (ğ‘† ğ‘¢ ) 8 hğ‘¢ 1 = ğ‘“ ğœƒ ( Sğ‘¢ 1 ), hğ‘¢ 2 = ğ‘“ ğœƒ ( Sğ‘¢ 2 ) // Optimization 9 L = L NextItem + ğœ† â€¢ L ICL + ğ›½ â€¢ L SeqCL</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF CONVERGENCE</head><p>Take the expectation in term of ğ‘ condition over ğ‘† on both sides, then we have:</p><formula xml:id="formula_26">E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) [ln ğ‘ƒ ğœƒ (ğ‘†)] = E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†, ğ‘) -E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘ |ğ‘†) .<label>(20)</label></formula><p>Based on Eq. ( <ref type="formula" target="#formula_15">12</ref>), and 20, the term on left side equal to:</p><formula xml:id="formula_27">E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†) = ğ¾ âˆ‘ï¸ ğ‘–=1 ğ‘„ (ğ‘ ğ‘– ) â€¢ ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†) = ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†).<label>(21)</label></formula><p>Thus, proof</p><formula xml:id="formula_28">ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†) â‰¥ ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†) is equivalent to proof ln ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†) â‰¥ ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†),<label>(22)</label></formula><p>which is equivalent to: </p><formula xml:id="formula_29">E (ğ‘</formula><p>Combining Eq. ( <ref type="formula">23</ref>), <ref type="bibr" target="#b23">(24)</ref>,and (25), we show that ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†) â‰¥ ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†). Thus, the algorithm will converge.   The impact of ğ›½ is shown in Figure <ref type="figure" target="#fig_14">8</ref>. We can see that, ğ›½ does help ICLRec improve the performance when it is small (e.g., ğ›½ â‰¤ 0.1). However, when ğ›½ continually increase, the model performance drop significantly. This phenomenon also indicates the limitation of SeqCL, since focusing on maximize mutual information between individual sequence pairs may break the global relationships among users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DATASET INFORMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CASE STUDY</head><p>Quantitative analysis. We study how ICLRec will perform by considering the item categories of users interacted items as their intents. Specifically, given a user behavior sequence ğ‘† ğ‘¢ , we consider the mean of its corresponding trainable item category embeddings as the intent prototype c, aiming to replace the intent representation learning described in Sec. 4.1.2. We run the corresponding model named ICLRec-A and show the comparison results in Table <ref type="table" target="#tab_9">4</ref>. We observe that on Sports (1) ICLRec-A performs better than CL4SRec, which shows the potential benefits of leveraging item category information.</p><p>(2) ICLRec achieves similar performance as ICLRec-A's when ğ¾ = 2048. Joint analysis with the above qualitative results indicates that ICL can capture meaningful user intents via SSL. (3) ICLRec can outperform ICLRec-A when ğ¾ = 1024. We hypothesize that users' intents can be better described by the latent variables when ğ¾ = 1024 thus improving performance. (e.g., parents of the existing item categories.) Similar observations in Yelp.</p><p>Qualitative analysis We also compare the proposed ICLRec with CL4SRec by visualizing the learned users' representations via t-SNE <ref type="bibr" target="#b38">[39]</ref>. Specifically, we sampled 100 users for whom used to interact with one category of items or the other category. These 100 users also interacted with other categories of items in the past. We visualize the learned users' representations via t-SNE <ref type="bibr" target="#b38">[39]</ref> in Figure <ref type="figure" target="#fig_12">7</ref>. From Figure <ref type="figure" target="#fig_12">7</ref> we can see, users' representations learned by ICLRec intent to pull users that interacted with the same category of items closer to each other while pushing others further away in the representation space than CL4SRec. It reflects that representations learned by ICL can capture more semantic structures, therefore, improves the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Users' purchasing behaviors can be driven by underlying intents that are not observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of ICL. (a) An individual sequence level SSL for SR. (b) The proposed ICL for SR. It alternately performs intent representation learning and intent contrastive SSL with FNM within the generalized EM framework to maximizes mutual information (MIM) between a behavior sequence and its corresponding intent prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b). It performs E-step and M-step alternately to estimate the distribution function ğ‘„ (ğ‘) over the intent variable ğ‘ and optimize the model parameters ğœƒ . In E-step, it estimates ğ‘„ (ğ‘) via clustering. In M-step, it optimizes ğœƒ with considering the estimated ğ‘„ (ğ‘) via mini-batch gradient descent. In each iteration, ğ‘„ (ğ‘) and ğœƒ are updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 . 1</head><label>11</label><figDesc>Modeling Latent Intent for SR. Assuming that there are ğ¾ latent intent prototypes {ğ‘ ğ‘– } ğ¾</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 . 2</head><label>12</label><figDesc>Intent Representation Learning. To learn the intent distribution function ğ‘„ (ğ‘), we encode all the sequences {ğ‘† ğ‘¢ } |U | ğ‘¢=1 with the encoder ğœƒ followed by an 'aggregation layer', and then we perform ğ¾-means clustering over all sequence representations {h ğ‘¢ } |U | ğ‘¢=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 2 (b) illustrates how the M-step works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 3 . 2</head><label>32</label><figDesc>Time Complexity and Convergence Analysis. In every iteration of the training phase, the computation costs of our proposed method are mainly from the E-step estimation of ğ‘„ (â€¢) and M-step optimization of ğœƒ with multi-tasks training. For the E-step, the time complexity is ğ‘‚ (|ğ‘ˆ |ğ‘šğ¾ğ‘‘) from clustering, where ğ‘‘ is the dimensionality of the embedding and ğ‘š is the maximum iteration number in clustering (ğ‘š = 20 in this paper). For the M-step, since we have three objectives to optimize the network ğ‘“ ğœƒ (â€¢), the time complexity is ğ‘‚ (3 â€¢ (|ğ‘ˆ | 2 ğ‘‘ + |ğ‘ˆ |ğ‘‘ 2 ). The overall complexity is dominated by the term ğ‘‚ (3 â€¢ (|ğ‘ˆ | 2 ğ‘‘)), which is 3 times of Transformerbased SR with only next item prediction objective, e.g., SASRec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison on different user groups among SASRec, CL4SRec and ICLRec (Upper left: Beauty, Upper right: Yelp, Lower left: Sports, Lower right: Toys).)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison w.r.t. noise ratio on Sports and Yelp. The bar chart shows the performance in NDCG@5 and the line chart shows the corresponding drop rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of intent class numbers ğ¾ and the intent contrastive learning strength ğœ† on Yelp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison w.r.t. Batch Size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>10Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of the learned users' representations by CL4SRec and ICLRec on Sports.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Impact of SeqCL task strength ğ›½ on Beauty (left) and Yelp (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ğ‘  ğ‘¢ ğ‘¡ , . . . , ğ‘  ğ‘¢ |ğ‘† ğ‘¢ | ] where |ğ‘† ğ‘¢ | is the number of interacted items and ğ‘  ğ‘¢ ğ‘¡ is the item ğ‘¢ interacted at step ğ‘¡. We denote S ğ‘¢ as embedded representation of ğ‘† ğ‘¢ , where s ğ‘¢ ğ‘¡ is the d-dimensional embedding of item ğ‘  ğ‘¢ ğ‘¡ .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons of different methods. The best score is bolded in each row, and the second best is underlined. The last two columns are the relative improvements compared with the best baseline results.</figDesc><table><row><cell cols="3">Dataset Metric</cell><cell cols="4">BPR GRU4Rec Caser SASRec DSSRec BERT4Rec S 3 -Rec ğ¼ğ‘†ğ‘ƒ</cell><cell>CL4SRec</cell><cell>ICLRec</cell><cell>Improv.</cell></row><row><cell></cell><cell></cell><cell>HR@5</cell><cell cols="4">0.0141 0.0162 0.0154 0.0206 0.0214</cell><cell>0.0217</cell><cell>0.0121</cell><cell>0.0217Â±0.0021 0.0283Â±0.0006 30.48%</cell></row><row><cell cols="2">Sports</cell><cell cols="5">HR@20 NDCG@5 0.0091 0.0103 0.0114 0.0135 0.0142 0.0323 0.0421 0.0399 0.0497 0.0495</cell><cell>0.0604 0.0143</cell><cell>0.0344 0.0084</cell><cell>0.0540Â±0.0024 0.0638Â±0.0023 18.15% 0.0137Â±0.0013 0.0182Â±0.0001 33.33%</cell></row><row><cell></cell><cell></cell><cell cols="5">NDCG@20 0.0142 0.0186 0.0178 0.0216 0.0220</cell><cell>0.0251</cell><cell>0.0146</cell><cell>0.0227Â±0.0016 0.0284Â±0.0008 24.89%</cell></row><row><cell></cell><cell></cell><cell>HR@5</cell><cell cols="4">0.0212 0.0111 0.0251 0.0374 0.0410</cell><cell>0.0360</cell><cell>0.0189</cell><cell>0.0423Â±0.0031 0.0493Â±0.0013 16.43%</cell></row><row><cell cols="2">Beauty</cell><cell cols="5">HR@20 NDCG@5 0.0130 0.0058 0.0145 0.0241 0.0261 0.0589 0.0478 0.0643 0.0901 0.0914</cell><cell>0.0984 0.0216</cell><cell>0.0487 0.0115</cell><cell>0.0994Â±0.0028 0.1076Â±0.0001 0.0281Â±0.0018 0.0324Â±0.0017 15.51% 8.30%</cell></row><row><cell></cell><cell></cell><cell cols="5">NDCG@20 0.0236 0.0104 0.0298 0.0387 0.0403</cell><cell>0.0391</cell><cell>0.0198</cell><cell>0.0441Â±0.0018 0.0489Â±0.0013 10.90%</cell></row><row><cell></cell><cell></cell><cell>HR@5</cell><cell cols="4">0.0120 0.0097 0.0166 0.0463 0.0502</cell><cell>0.0274</cell><cell>0.0143</cell><cell>0.0526Â±0.0034 0.0590Â±0.0012 12.07%</cell></row><row><cell cols="2">Toys</cell><cell cols="5">HR@20 NDCG@5 0.0082 0.0059 0.0107 0.0306 0.0337 0.0312 0.0301 0.0420 0.0941 0.0975</cell><cell>0.0688 0.0174</cell><cell>0.0235 0.0123</cell><cell>0.1038Â±0.0041 0.1150Â±0.0016 10.74% 0.0362Â±0.0025 0.0403Â±0.0002 11.34%</cell></row><row><cell></cell><cell></cell><cell cols="5">NDCG@20 0.0136 0.0116 0.0179 0.0441 0.0471</cell><cell>0.0291</cell><cell>0.0162</cell><cell>0.0506Â±0.0025 0.0560Â±0.0004 10.57%</cell></row><row><cell></cell><cell></cell><cell>HR@5</cell><cell cols="4">0.0127 0.0152 0.0142 0.0160 0.0171</cell><cell>0.0196</cell><cell>0.0101</cell><cell>0.0229Â±0.0003 0.0257Â±0.0007 12.23%</cell></row><row><cell cols="2">Yelp</cell><cell cols="5">HR@20 NDCG@5 0.0082 0.0091 0.0346 0.0371 0.0406 0.0443 0.0464 0.008 0.0101 0.0112</cell><cell>0.0564 0.0121</cell><cell>0.0314 0.0068</cell><cell>0.0630Â±0.0009 0.0677Â±0.0016 0.0144Â±0.0001 0.0162Â±0.0003 12.50% 7.47%</cell></row><row><cell></cell><cell></cell><cell cols="5">NDCG@20 0.0143 0.0145 0.0156 0.0179 0.0193</cell><cell>0.0223</cell><cell>0.0127</cell><cell>0.0256Â±0.0003 0.0279Â±0.0006</cell><cell>8.98%</cell></row><row><cell>NDCG@20</cell><cell>0.02 0.03</cell><cell>SASRec CL4SRec ICLRec</cell><cell></cell><cell>NDCG@20</cell><cell>0.03</cell><cell>SASRec CL4SRec ICLRec</cell></row><row><cell>NDCG@20</cell><cell>0.01 0.02</cell><cell cols="2">=5 User Interaction Frequency 6-8 &gt;8 SASRec CL4SRec ICLRec</cell><cell>NDCG@20</cell><cell>0.03 0.04 0.05 0.06</cell><cell>=5 User Interaction Frequency 6-8 &gt;8 SASRec CL4SRec ICLRec</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell></row><row><cell></cell><cell></cell><cell cols="2">=5 User Interaction Frequency 6-8 &gt;8</cell><cell></cell><cell></cell><cell>=5 User Interaction Frequency 6-8 &gt;8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of ICLRec (NDCG@20).</figDesc><table><row><cell>Model</cell><cell>Dataset Sports Beauty Toys</cell><cell>Yelp</cell></row><row><cell>(A) ICLRec</cell><cell cols="2">0.0287 0.0480 0.0554 0.0283</cell></row><row><cell>(B) w/o FNM</cell><cell cols="2">0.0283 0.0465 0.0524 0.0266</cell></row><row><cell>(C) only ICL</cell><cell cols="2">0.0263 0.0429 0.0488 0.0267</cell></row><row><cell>(D) w/o ICL</cell><cell cols="2">0.0238 0.0428 0.0505 0.0258</cell></row><row><cell cols="3">(E), is (C) w/o seq. aug 0.0242 0.0414 0.0488 0.0213</cell></row><row><cell>(F) SASRec</cell><cell cols="2">0.0216 0.0387 0.0441 0.0179</cell></row><row><cell>(G) ICL + S 3 -Rec ğ¼ğ‘†ğ‘ƒ</cell><cell cols="2">0.0157 0.0264 0.0266 0.0205</cell></row><row><cell>(H) S 3 -Rec ğ¼ğ‘†ğ‘ƒ</cell><cell cols="2">0.0146 0.0198 0.0162 0.0127</cell></row><row><cell>5.4 Ablation Study</cell><cell></cell><cell></cell></row><row><cell cols="3">Our proposed ICLRec contains a novel ICL objective, a false-negative</cell></row><row><cell cols="3">noise mitigation (FNM) strategy, a SeqCL objective, and sequence</cell></row><row><cell cols="3">augmentations. To verify the effectiveness of each component, we</cell></row><row><cell cols="3">conduct an ablation study on four datasets and report results in</cell></row><row><cell cols="3">Table 2. (A) is our final model, and (B) to (F) are ICLRec removed cer-</cell></row><row><cell cols="3">tain components. From (A)-(B) we can see that the FNM leverages</cell></row><row><cell cols="3">the learned intent information to avoid users with similar intents</cell></row><row><cell cols="3">pushing away in their representation space which helps the model</cell></row><row><cell cols="3">to learn better user representations. Compared with (A)-(D), we</cell></row><row><cell cols="3">find that without the proposed ICL, the performance drops signif-</cell></row><row><cell cols="3">icantly, which demonstrates the effectiveness of ICL. Compared</cell></row><row><cell cols="3">with (A)-(C), we find that individual user level mutual information</cell></row><row><cell cols="3">also helps to enhance user representations. As we analyze in Sec. 5.3,</cell></row><row><cell cols="3">it contributes more to long user sequences. Compared with (E)-(F),</cell></row><row><cell cols="3">we find that ICL can perform contrastive SSL without sequence</cell></row><row><cell cols="3">augmentations and outperforms SASRec. While CL4SRec requires</cell></row><row><cell cols="3">the sequence augmentation module to perform contrastive SSL.</cell></row><row><cell cols="3">Comparison between (C) and (E) indicates sequence augmentation</cell></row><row><cell cols="3">enlarges training set, which benefits improving performance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>In this work, we propose a new learning paradigm ICL that can model latent intent factors from user interactions and fuse them into a sequential recommendation model via a new contrastive SSL objective. ICL is formulated within an EM framework, which guarantees convergence. Detailed analyses show the superiority of ICL and experiments conducted on four datasets further demonstrate the effectiveness of the proposed method. for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. 1893-1902. [52] Yao Zhou, Jianpeng Xu, Jun Wu, Zeinab Taghavi, Evren Korpeoglu, Kannan Achan, and Jingrui He. 2021. PURE: Positive-Unlabeled Recommendation with Generative Adversarial Network. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. 2409-2419.</figDesc><table><row><cell cols="2">A PSEUDO-CODE OF ICL FOR SR</cell></row><row><cell cols="2">Algorithm 1: ICL for SR</cell></row><row><cell></cell><cell>Input: training dataset {ğ‘  ğ‘¢ } ğ‘¢=1 , sequence encoder ğ‘“ ğœƒ , batch | U |</cell></row><row><cell></cell><cell>size ğ‘ , hyper-parameters ğ¾, ğœ†, ğ›½.</cell></row><row><cell></cell><cell>Output: ğœƒ .</cell></row><row><cell cols="2">1 while ğ‘’ğ‘ğ‘œğ‘â„ â‰¤ ğ‘€ğ‘ğ‘¥ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ¸ğ‘ğ‘œğ‘â„ do</cell></row><row><cell></cell><cell>// E-step: Intent Representation Learning</cell></row><row><cell>2</cell><cell>ğ‘ = ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘”({ğ‘“ ğœƒ (ğ‘† ğ‘¢ )} ğ‘¢=1 , ğ¾) | U |</cell></row></table><note><p>3 Update distribution function ğ‘„ (ğ‘ ğ‘– ) = ğ‘ƒ ğœƒ (ğ‘ ğ‘– |ğ‘† ğ‘¢ ) // M-step: Multi-Task Learning 4 for a minibatch {ğ‘  ğ‘¢ } ğ‘ ğ‘¢=1 do 5 for ğ‘¢ âˆˆ {1, 2, â€¢ â€¢ â€¢ , ğ‘ } do // Construct 2 views.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>To proof the convergence of ICL under the generalized EM framework, we just need to proof ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†) â‰¥ ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†), where ğ‘š indi-</figDesc><table /><note><p>cates the number of training iterations. Based on Eq. (7), we have ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†) = ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†, ğ‘ ğ‘– ) ğ‘ƒ ğœƒ (ğ‘š) (ğ‘ ğ‘– |ğ‘†) = ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†, ğ‘ ğ‘– )ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘ ğ‘– |ğ‘†).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>|ğ‘†,ğœƒ (ğ‘š+1) ) ln ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†, ğ‘) -E (ğ‘ |ğ‘†,ğœƒ (ğ‘š+1) ) ln ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘ |ğ‘†) â‰¥ E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†, ğ‘) -E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘ |ğ‘†) .Because we try to optimize ğœƒ at M-step, thus we haveE (ğ‘ |ğ‘†,ğœƒ (ğ‘š+1) ) ln ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘†, ğ‘) â‰¥ E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘†, ğ‘) .(24)And based on Jsnson's inequality, we haveE (ğ‘|ğ‘†,ğœƒ (ğ‘š+1) ) ln ğ‘ƒ ğœƒ (ğ‘š+1) (ğ‘ |ğ‘†) â‰¤ E (ğ‘ |ğ‘†,ğœƒ (ğ‘š) ) ln ğ‘ƒ ğœƒ (ğ‘š) (ğ‘ |ğ‘†) .</figDesc><table><row><cell>(23)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Dataset information.Performance w.r.t. batch size on Yelp between CL4SRec and the proposed ICLRec are shown in Figure.6. We observe that with the batch size increases, CL4SRec's performance does not continually improve. The reason might because of larger batch sizes introduce false-negative samples, which harms learning. While ICLRec is relatively stable with different batch sizes, and out performs CL4SRec in all circumstances. Because the intent learnt can be seen as a pseudo label of sequences, which helps identify the true positive samples via the proposed contrastive SSL with FNM.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Sports Beauty</cell><cell>Toys</cell><cell>Yelp</cell></row><row><cell>|U|</cell><cell>35,598</cell><cell>22,363</cell><cell cols="2">19,412 30,431</cell></row><row><cell>|V |</cell><cell>18,357</cell><cell>12,101</cell><cell cols="2">11,924 20,033</cell></row><row><cell># Actions</cell><cell>0.3m</cell><cell>0.2m</cell><cell>0.17m</cell><cell>0.3m</cell></row><row><cell>Avg. length</cell><cell>8.3</cell><cell>8.9</cell><cell>8.6</cell><cell>8.3</cell></row><row><cell>Sparsity</cell><cell cols="4">99.95% 99.95% 99.93% 99.95%</cell></row><row><cell cols="5">D IMPACT OF BATCH SIZE AND THE</cell></row><row><cell cols="4">STRENGTH OF SEQCL TASK ğ›½</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Quantitative Analysis Results. (NDCG@20)</figDesc><table><row><cell cols="4">Datasets SASRec CL4SRec ICLRec-A</cell><cell>ICLRec</cell></row><row><cell>Sports</cell><cell>0.0216</cell><cell>0.0238</cell><cell>0.0272</cell><cell>0.0275 (ğ¾ = 2048) 0.0287 (ğ¾ = 1024)</cell></row><row><cell>Yelp</cell><cell>0.0179</cell><cell>0.0258</cell><cell>0.0264</cell><cell>0.0271 (ğ¾ = 1024) 0.0283 (ğ¾ = 512)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.yelp.com/dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/graytowne/caser_pytorch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/FeiSun/BERT4Rec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/RUCAIBox/CIKM2020-S3Rec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/xiangwang1223/neural_graph_collaborative_filtering</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/slientGe/Sequential_Recommendation_Tensorflow</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://github.com/abinashsinha330/DSSRec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://github.com/facebookresearch/faiss</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Category-aware Collaborative Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Renqin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jibang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<title level="m">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controllable multi-interest framework for recommendation</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2942" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Anderle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11654</idno>
		<title level="m">Modeling Dynamic Attributes for Next Basket Recommendation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01403</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Alexis Conneau, and Ves Stoyanov</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fusing similarity models with markov chains for sparse sequential recommendation</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<title level="m">Session-based recommendations with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised contrastive learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Sampled Metrics for Item Recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1748" to="1757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-interest network with dynamic routing for recommendation at Tmall</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2615" to="2623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intention-aware Sequential Recommendation with Structured Intent Transition</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Time Interval Aware Self-Attention for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno>WSDM. 322-330</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightweight Self-Attentive Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="967" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Contrastive self-supervised sequential recommendation with robust augmentation</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06479</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmenting Sequential Recommendation with Pseudo-Prior Items via Reversely Pre-training Transformer</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00522</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Basket recommendation with multi-intent translation graph neural network</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Achan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02100</idno>
		<title level="m">Deoscillated Graph Collaborative Filtering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disentangled self-supervision in sequential recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An intentguided collaborative machine for session-based recommendation</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentive sequential models of latent intent for next item recommendation</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Mehrab Tanjim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzhe</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2528" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953">1953. 1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling multi-purpose sessions for next-item recommendations via mixture-channel purpose routing networks</title>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Orgun</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent recommender networks</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM international conference on web search and data mining</title>
		<meeting>the tenth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14395</idno>
		<title level="m">Contrastive Learning for Sequential Recommendation</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">CosRec: 2D convolutional neural networks for sequential recommendation</title>
		<author>
			<persName><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2173" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12865</idno>
		<title level="m">Self-supervised Learning for Large-scale Item Recommendations</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferrable parameters for long-tailed sequential user behavior modeling</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Ch</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stacked Mixed-Order Graph Convolutional Networks for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 SIAM International Conference on Data Mining</title>
		<meeting>the 2020 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan</forename><surname>Hui Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12061</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>S3-rec: Self-supervised learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
