<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS</title>
				<funder>
					<orgName type="full">Google Brain</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guangzhi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FULLY-HIERARCHICAL FINE-GRAINED PROSODY MODELING FOR INTERPRETABLE SPEECH SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text-to-speech</term>
					<term>Tacotron 2</term>
					<term>fine-grained VAE</term>
					<term>hierarchical</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a hierarchical, fine-grained and interpretable latent variable model for prosody based on the Tacotron 2 text-tospeech model. It achieves multi-resolution modeling of prosody by conditioning finer level representations on coarser level ones. Additionally, it imposes hierarchical conditioning across all latent dimensions using a conditional variational auto-encoder (VAE) with an auto-regressive structure. Evaluation of reconstruction performance illustrates that the new structure does not degrade the model while allowing better interpretability. Interpretations of prosody attributes are provided together with the comparison between word-level and phone-level prosody representations. Moreover, both qualitative and quantitative evaluations are used to demonstrate the improvement in the disentanglement of the latent dimensions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Significant developments have taken place in the neural end-to-end text-to-speech (TTS) synthesis models for generating high fidelity speech with a simplified pipeline <ref type="bibr" target="#b1">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref>. Such systems usually incorporate an encoder-decoder neural network architecture <ref type="bibr" target="#b5">[5]</ref> that maps a given text sequence to a sequence of acoustic features. More recent advancement in such models enables the use of crowd-sourced data by disentangling and controlling different attributes such as speaker identity, noise, recording channels as well as prosody <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr">[8]</ref>. The focus of this paper, prosody, is a collection of attributes including fundamental frequency (F0), energy and duration <ref type="bibr" target="#b9">[9]</ref>. Efforts have been made to model and control these attributes by factorizing the latent attributes (e.g. prosody) from observed attributes (e.g. speaker). Although most of these works use latent representations at utterance level which captures the salient features of the utterance <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, fine-grained prosody that are aligned with the phone sequence can be captured using techniques recently proposed in <ref type="bibr" target="#b13">[13]</ref>. This model provides a localized prosody control that achieves more variability and higher robustness to speaker perturbations.</p><p>Even though prosody attributes such as F0 and energy can be treated as latent features, interpreting the phone-level latent space is still difficult since latent dimensions can be entangled with each other. Moreover, coherence of the prosody within a word (e.g. accented syllables), noise level and channel properties are important attributes not captured at the phone-level alone. Respecting the hierarchical nature of spoken language and aiming at interpretation of prosody at fine-scale such as F0 for a vowel, this paper aims to achieve disentangled control of each prosody attribute at different levels.</p><p>This paper proposes a multilevel model based on Tacotron 2 [14] integrated with a hierarchical latent variable model. In addition to the prosody representation at utterance level, the representation is also extracted at word and phone levels. Apart from utterance-level characteristics such as noise and channel properties, phone-level prosodic features are expected to capture fine-grained information associated with each phone, and word-level features are expected to capture the prosody at each word while maintaining a natural prosody structure within the word. To better interpret the representation of each latent dimension, the original VAE is replaced by a conditional VAE driven by the information contained in the previous latent dimensions. This setup gives a hierarchy where finer level features are conditioned on coarser, and latent variables at each level are hierarchically factorized. The proposed model is thus referred to as a fully-hierarchical VAE. Furthermore, imposing a training schedule for each latent dimension results in a phone-level representation which reflects a consistent ordering of prosody attributes. Finally, we assess the disentanglement property of our model on three most significant attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRIOR WORK</head><p>Abundant research has been performed on learning latent representations for styles and prosody <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> such as the use of an utterancelevel VAE in <ref type="bibr" target="#b17">[17]</ref>. Our multilevel model is based on the fine-grained VAE structure which extends the idea in <ref type="bibr" target="#b13">[13]</ref>, and is closely related to the hierarchical VQVAE model in <ref type="bibr" target="#b18">[18]</ref>. While the latter uses down sampling to extract coarser features for image processing, our proposed model takes advantage of the hierarchical structure of spoken language. The multilevel alignment is also similar to the multilevel information extraction model proposed by <ref type="bibr" target="#b19">[19]</ref>.</p><p>Meanwhile, exhaustive exploration has been made in the unsupervised learning of disentangled latent representations these years in various scenarios including speech recognition <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. Progress have been made mostly in the direction of learning independently distributed latent variables without associating them with the actual latent factors <ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>. However, <ref type="bibr" target="#b25">[25]</ref> demonstrated the impossibility of unsupervised learning of disentangled representations without any inductive bias. They pointed out that there exists an infinite number of bijective mappings from the learned latent space to another space with the same marginal distribution, but the two spaces are fully entangled. Other works try to learn disentangled representations via semi-supervised learning <ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref> that guides a subset of latent variables to learn some labelled features, or via adversarial training <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. The most similar hierarchical decomposition to our approach is proposed by <ref type="bibr" target="#b31">[31]</ref>, but the intention of this decomposition is to facilitate learning statistically independent random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTILEVEL PROSODY MODELING STRUCTURE</head><p>Different from utterance-level VAE <ref type="bibr" target="#b6">[6]</ref> where a single latent feature is extracted for each utterance, the fine-grained VAE <ref type="bibr" target="#b13">[13]</ref>  target spectrogram with the phone sequence and extract a sequence of phone-level latent prosody features. These latent prosody features are concatenated with their corresponding phone encodings before sending to the decoder. Extending the fine-grained VAE, an illustration of our proposed multilevel model is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>This structure is integrated with the encoder of the Tacotron-2 <ref type="bibr" target="#b14">[14]</ref>. Location-sensitive attention <ref type="bibr" target="#b32">[32]</ref> is used to align the target spectrogram with encodings of each phone at step 1. After the aligned target spectrogram is obtained, the average of spectrograms associated with phones in each word is calculated using the known phone-word alignment. The word-level latent prosody features are then extracted from these averaged spectrograms. Phone-level latent features are extracted conditioning on word-level latent features, and both features are concatenated using the phone-word alignment again. These features are used by the decoder for reconstruction. The system is optimized with the multilevel evidence lower bound (ELBO):</p><formula xml:id="formula_0">L(p, q) = E q(z|X) [log p(X | Y, z)] -β1 N n=1 E q(z w f (n) |X) DKL q(z p n | X, z w f (n) ) p(z p n ) -β2 M m=1 DKL(q(z w m | X) p(z w m )) ,<label>(1)</label></formula><p>where M is the number of words, N is the number of phones and βs have the same function as <ref type="bibr" target="#b22">[22]</ref>. z refers to the sequence of concatenated phone and word-level latent features. f (n) maps the phone index n to the corresponding word index. z w and z p represent latent features associated with each word and each phone respectively. The model incorporates the utterance-level feature z u by conditioning other fine-grained latent features on the utterance level latent one, and subtracting β3 DKL(q(z u | X) p(z u )) in Eq.( <ref type="formula" target="#formula_0">1</ref>) accordingly. For a more complete model one could also introduce the dependency on text and speaker information for the posterior, e.g. q(z p n | X, Y p n , S, z w n ) and q(z w m | X, Y w m , S) where S is the speaker embedding and Y w and Y p for phone and word encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INTERPRETABLE CONDITIONAL VAE STRUCTURE</head><p>In the previous multilevel structure, the VAE layer models the data distribution in the latent space as a multi-dimensional Gaussian distribution with diagonal covariance matrices, which is based on the assumption that different latent dimensions have independent effects to the prosody attributes. A corresponding graphical model is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> where the graph with label 1 shows the generation shows the generation process, while 2 and 3 show the inference process. Right: conditional VAE structure where Proj. is a projection layer mapping each latent dimension to a higher dimension.</p><p>sub-graph and label 2 shows the corresponding inference sub-graph.</p><p>When inferring the posterior of z2, the model in label 2 where z are independent of each other indicates when the observation X is given, knowing what z1 represents does not provide any further information. However, if latent dimensions control disentangled factors and z1 is known to represent the energy of the phone, it adds extra information to z2 indicating that z2 captures attributes other than energy. Therefore, this conditional dependency as shown by the graph with label 3 should be modeled when inferring the posterior distribution.</p><p>To incorporate the dependency into inference, we extend the hierarchical structure described in the previous section further to include a conditional VAE according to an auto-regressive decomposition of the posterior. The model structure is shown in the right part of Fig. <ref type="figure" target="#fig_1">2</ref>. Unlike auto-regressive density estimators <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref> which uses recurrent structures directly on top of the latent variables, our recurrent model conditions on the projection of latent variables and extract latent dimensions one at a time. Specifically, when extracting the k-th latent dimension, the input to the VAE is the aligned spectrogram concatenated with the summation of all previously extracted latent features. Because these latent dimensions after projection are directly used by the decoder, they are effectively representing the prosody attributes being captured. Using these features with the aligned spectrogram as inputs to the VAE implicitly encourages the current latent dimension k to extract information about the prosody attribute other than what has already been represented. The training objective for this VAE model can still be written in the ELBO form as shown in Eq. ( <ref type="formula" target="#formula_1">2</ref>), where expectation is estimated by single sample for the KL-divergence between the auto-regressive posterior q(z | X) and the prior p(z).</p><formula xml:id="formula_1">L(p, q) = E q(z|X) [log p(X | Y, z)] -β d k=1 DKL(q(z k | Z 1:k-1 , X) p(z k )) ,<label>(2)</label></formula><p>where Z 1:k-1 are samples of dimensions 1 to k -1 from their posterior distributions. The prior uses the isometric standard normal distribution for each latent dimension. When applied to the multilevel framework, this loss function is minimized at each time step for each level and the subscription n and m in Eq. ( <ref type="formula" target="#formula_0">1</ref>) for phone and word indices can be directly added to each latent variable. Combining the two approaches where auto-regressive decomposition of the posterior is applied across different levels and different latent dimensions, the model thus covers the full hierarchy from phone to utterance level.</p><p>Finally, disentangled prosody features are observed to be extracted following an energy-duration-F0 order guided by scheduled training across latent dimensions. Scheduled training refers to the process where the first latent dimension is trained for a certain number of steps before the second dimension starts to train. The rest of the dimensions are started consecutively in the same way. Therefore, when scheduling is imposed together with the conditional VAE, the first latent dimension will capture the energy information, and the second dimension being aware that energy has been represented by the first dimension, will seek for representations of the duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>The proposed models are evaluated on the LibriTTS multi-speaker audiobook dataset <ref type="bibr" target="#b35">[35]</ref> and the Blizzard Challenge 2013 single-speaker audiobook dataset <ref type="bibr" target="#b36">[36]</ref>. LibriTTS includes approximately 585 hours of read English audiobooks at 24kHz sampling rate. It covers a wide range of speakers, recording conditions and speaking styles. The latent space is expected to control the prosody without affecting speaker characteristics. On the other hand, the Blizzard Challenge 2013 dataset contains 147 hours US English speech with highly varying prosody, recorded by a female professional speaker.</p><p>Three attributes are considered in this paper for fine-grained prosody interpretation including F0, energy and duration. To quantitatively evaluate each attributes, we leverage the decoder alignment attention weights to obtain the duration of the phone by counting the frames which have a peak value at that specific phone in their attention weights. After obtaining the duration frames n1 to n2 and also converting to signal sample indices t1 to t2, the energy can be estimated using the average signal magnitude in [t1, t2 -1] divided by the average signal magnitude of the whole utterance. F0 can be similarly measured using the average F0 estimated from an F0 tracker <ref type="bibr" target="#b37">[37]</ref> among the frames in [n1, n2 -1]. To decrease the variance due to bad alignments, we exclude 50 samples at both margins.</p><p>Finally, the mel-cepstral distortion (MCD), the F0 Frame Error (FFE) <ref type="bibr" target="#b38">[38]</ref>, which is a combination of the Gross Pitch Error (GPE) and the Voicing Decision Error (VDE), are used to quantify the reconstruction performance. FFE evaluates the reconstruction of the F0 track, and MCD evaluates the timbral distortion. We strongly recommend readers to listen to the samples on the demo page <ref type="bibr" target="#b39">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Reconstruction Performance</head><p>Table <ref type="table">1</ref> shows reconstruction performance measured using FFE and MCD13 for the first 13 MFCCs. Lower is better for both metrics. Both the fine-grained VAE with 3-dimensional latent features and the fully-hierarchical VAE achieve similar reconstruction performance. Furthermore, progressive improvements from global to the system with 3-dimensional latent space can be interpreted: By introducing fine-grained VAE, there is a significant drop in VDE as the fine-grained VAE could capture phone-level energy and duration information. Conditioning the posterior distribution on the speaker embedding at the encoder side significantly reduced GPE because the speaker identity is closely related to the average F0. Increasing the latent space size to 3 again significantly reduces the F0 error, which confirms the fact that F0 information is the last to be captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multilevel Controllability</head><p>The model selected for the demonstration is trained on the LibriTTS dataset, and both phone-level and word-level latent spaces are 3dimensional. To illustrate the effect of controlling a single attribute at different levels clearly, we traverse one dimension of the latent features to control a certain attribute while keeping other dimensions  <ref type="table">1</ref>. Reconstruction performance results. 2d and 3d refers to the dimension of the latent space. 3-dimensional latent space is used for the conditional and the fully-hierarchical VAE. If not specified, the posterior is conditioned on the speaker embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>constant. We demonstrate the control of a single vowel or a word using phone-level or word-level latent features in Fig <ref type="figure">4</ref>.</p><p>The effect on a single vowel is clear: Increasing F0 raised harmonic frequencies within that phone. Increasing energy brightens the area in the box while darkens the rest as signals are normalized. Increasing duration stretched the corresponding area. Because the influence at the word level contains a mixture of effects on vowels and consonants, when changing the dimension controlling energy, the duration of the phone n also changes which affects the duration of the word. However, the most significant changes can still be interpreted as the same three attributes exerted on all the vowels in the word. Meanwhile, the traversing of each latent dimension for a phone at different word prosody level is shown in Fig. <ref type="figure">3</ref>. The control of each latent attribute is linear when traversing each dimension from -1 to +1. The word-level control shifts the phone-level curves up and down as the phone-level latent features are conditioned on the word-level.</p><p>Furthermore, adjusting word-level prosody features also retains the prosody distribution within a word while phone-level adjustment required a manual assignment to keep it natural. This effect can be evaluated by generating utterances with phone-level or word-level independent sampling of one latent feature. Subjective mean opinion score (MOS) test results are shown in Table <ref type="table">2</ref> where random samples of the latent dimension controlling the F0 were used. Consequently, the word-level independent sampling sounds more natural, as the prosody structure within each word is retained as neutral prosody.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling level MOS</head><p>Neutral-prosody 4.04 ±0.06 Phone-level 3.75 ± 0.09 Word-level 3.91 ± 0.10 Table <ref type="table">2</ref>. MOS evaluation of speech generated with the phone/word level independent F0 sampling. When sampling at one level, the other is set to all zero to give neutral prosody.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Improved phone-level interpretability</head><p>The interpretability is improved by the disentanglement for the three prosody attributes with the conditional VAE model. To illustrate this improvement, a vowel was selected and its F0, energy and duration were measured with the method in Sec. 5. For each model, 100 samples were generated by drawing from a standard Gaussian distribution for one latent dimension while keeping other dimensions constant. Then, the standard deviations for measured attributes were calculated and scaled to lie in a similar range. Next, the ratio of standard deviations between the attribute under control and the attribute with the second-largest deviation was obtained to represent the disentanglement in that dimension. This was repeated for each latent dimension and the sum of the ratios for each repetition is used to generally represent the degree of disentanglement. Though the ratio is not capturing the exact entanglement, it still reflects the degree of disentanglement because disentangled systems should have a much larger variation in one factor than the other two when only one latent dimension is varying. The experiment with each model was repeated 5 times with different random seeds to show a consistent improvement in disentanglement. Results are displayed in the form of µ ± σ in Table <ref type="table">3</ref> and Table <ref type="table">4</ref> respectively, where µ is the average of summed ratios and σ is the standard deviation across repetitions. Even though the standard deviation increases, using the conditional VAE model on average improves the degree of disentanglement. As the variation in prosody is found to be linearly correlated with the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Average variance ratio</head><p>Baseline fine-grained VAE 7.5 ± 1.0 DIP-VAE I 9.9 ± 1.9 Fully-hierarchical VAE 11.5 ± 2.3</p><p>Table <ref type="table">3</ref>. Variance ratio for different influencing control factors associated with a vowel on LibriTTS. DIP-VAE-I refers the model proposed in <ref type="bibr" target="#b24">[24]</ref> which essentially enforces the covariance matrix of the marginal posterior q(z) to be diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Average variance ratio</head><p>Baseline fine-grained VAE 5.8 ± 0.8 Fully-hierarchical VAE 8.0 ± 2.9</p><p>Table <ref type="table">4</ref>. Variance ratio for different influencing control factors associated with a vowel on the single-speaker audio book dataset. latent dimension, each attribute is adjusted by traversing the corresponding latent dimension. Additionally, when training schedule is imposed on latent dimensions, the order of prosody attributes being captured is always found to be energy, duration, and F0 on both datasets. Energy is the amplitude of the signal which is directly related to the reconstruction loss and is easier to be captured, and F0 is the last which coincides with the findings from the reconstruction evaluation. Moreover, the first dimension captures the duration of silence since that is the most significant attribute. The effect of latent features for a consonant in spectrograms can also be categorized into these three attributes but is hard to interpret directly from the audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>A fully-hierarchical model to achieve multilevel control of prosody attributes is proposed in this paper. The model consists of a hierarchical structure across different levels covering phone, word and utterance. Besides, a conditional VAE is applied at the phone and word-level which also adopts a hierarchical structure across all latent dimensions. Experimental results demonstrate improved interpretability by showing improved disentanglement, and the order of prosody attributes to be extracted is explained. Furthermore, the difference in phone and word level control effects is also analyzed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multilevel fine-grained VAE structure on the encoder side. Phone-level alignment implemented with a location-sensitive attention. Utterance-level latent features can be extracted separately and used as input to steps 3 , 4 and 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Left: graphical models for a 3-dimensional latent space. 1 shows the generation process, while 2 and 3 show the inference process. Right: conditional VAE structure where Proj. is a projection layer mapping each latent dimension to a higher dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Prosody variations when traversing the nominated controlling dimension in the latent space under different word latent values.</figDesc><graphic coords="4,381.59,71.53,142.46,111.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table</figDesc><table><row><cell></cell><cell cols="4">GPE VDE FFE MCD</cell></row><row><cell>Global VAE</cell><cell>0.39</cell><cell>0.34</cell><cell>0.52</cell><cell>16.0</cell></row><row><cell cols="2">Phone-level VAE 2d (no spk.) 0.33</cell><cell>0.18</cell><cell>0.35</cell><cell>10.5</cell></row><row><cell>Phone-level VAE 2d</cell><cell>0.25</cell><cell>0.15</cell><cell>0.28</cell><cell>9.0</cell></row><row><cell>Phone-level VAE 3d</cell><cell>0.10</cell><cell>0.12</cell><cell>0.18</cell><cell>8.6</cell></row><row><cell>Phone-level conditional VAE</cell><cell>0.10</cell><cell>0.13</cell><cell>0.20</cell><cell>9.2</cell></row><row><cell>Fully-hierarchical VAE</cell><cell>0.10</cell><cell>0.12</cell><cell>0.19</cell><cell>8.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGEMENTS</head><p>The authors thank <rs type="person">Daisy Stanton</rs>, <rs type="person">Eric Battenberg</rs>, and the <rs type="funder">Google Brain</rs> and Perception teams for their helpful feedback and discussions.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Char2wav: End-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tacotron: Towards end-toend speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Voice 3: 2000-speaker neural text-to-speech</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical generative modeling for controllable speech synthesis</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A generative adversarial network for style modeling in a text-to-speech system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experimental and theoretical advances in prosody: A review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language and Cognitive Processes</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="905" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Effective use of variational embedding capacity in expressive end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03402</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron</title>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust and fine-grained prosody control of end-toend speech synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning latent representations for style control and transfer in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6945" to="6949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep encoder-decoder models for unsupervised learning of controllable speech synthesis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenzo-Trueba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11470</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Expressive speech synthesis via modeling expressions with variational autoencoder</title>
		<author>
			<persName><forename type="first">K</forename><surname>Akuzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3067" to="3071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02882</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised training for improving data efficiency in end-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2649" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational inference of disentangled latent concepts from unlabeled observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning disentangled representations with semi-supervised deep generative models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5925" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised learning by disentangling and self-ensembling over stochastic latent space</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09607</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised generative modeling for controllable speech synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01709</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation in deep representations using adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured disentangled representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Intelligence and Statistics</title>
		<meeting>Int. Conf. on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2525" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RNADE: The real-valued neural autoregressive density-estimator</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LibriTTS: A corpus derived from LibriSpeech for text-to-speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The blizzard challenge 2013</title>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karaiskos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blizzard Challenge Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">YIN, a fundamental frequency estimator for speech and music</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Audio samples from &quot;Fully-hierarchical Fine-grained Prosody Modelling for Interpretable Speech Synthesis</title>
		<ptr target="https://google.github.io/tacotron/publications/hierarchical_prosody" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
