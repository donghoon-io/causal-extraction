<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling User Exposure in Recommendation</title>
				<funder ref="#_uHPwzs8">
					<orgName type="full">John Templeton Foundation</orgName>
				</funder>
				<funder ref="#_yQ2wN9A">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_qDmrEb2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-02-04">4 Feb 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
							<email>dliang@ee.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
							<email>lcharlin@cs.mcgill.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
							<email>david.blei@columbia.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling User Exposure in Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-02-04">4 Feb 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1510.07025v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>recommender systems</term>
					<term>collaborative filtering</term>
					<term>matrix factorization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering analyzes user preferences for items (e.g., books, movies, restaurants, academic papers) by exploiting the similarity patterns across users. In implicit feedback settings, all the items, including the ones that a user did not consume, are taken into consideration. But this assumption does not accord with the common sense understanding that users have a limited scope and awareness of items. For example, a user might not have heard of a certain paper, or might live too far away from a restaurant to experience it. In the language of causal analysis [9], the assignment mechanism (i.e., the items that a user is exposed to) is a latent variable that may change for various user/item combinations. In this paper, we propose a new probabilistic approach that directly incorporates user exposure to items into collaborative filtering. The exposure is modeled as a latent variable and the model infers its value from data. In doing so, we recover one of the most successful state-of-theart approaches as a special case of our model <ref type="bibr" target="#b7">[8]</ref>, and provide a plug-in method for conditioning exposure on various forms of exposure covariates (e.g., topics in text, venue locations). We show that our scalable inference algorithm outperforms existing benchmarks in four different domains both with and without exposure covariates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Making good recommendations is an important problem on the web. In the recommendation problem, we observe how a set of users interacts with a set of items, and our goal is to show each user a set of previously unseen items that she will like. Broadly speaking, recommendation systems use historical data to infer users' preferences, and then use the inferred preferences to suggest items. Good recommendation systems are essential as the web grows; users are overwhelmed with choice.</p><p>Traditionally there are two modes of this problem, recommendation from explicit data and recommendation from implicit data. With explicit data, users rate some items (positively, negatively, or along a spectrum) and we aim to predict their missing ratings. This is called explicit data because we only need the rated items to infer a user's preferences. Positively rated items indicate types of items that she likes; negatively rated items indicate items that she does not like. But, for all of its qualities, explicit data is of limited use. It is often difficult to obtain.</p><p>The more prevalent mode is recommendation from implicit data. In implicit data, each user expresses a binary decision about items-for example this can be clicking, purchasing, viewing-and we aim to predict unclicked items that she would want to click on. Unlike ratings data, implicit data is easily accessible. While ratings data requires action on the part of the users, implicit data is often a natural byproduct of their behavior, e.g., browsing histories, click logs, and past purchases.</p><p>But recommendation from implicit data is also more difficult than its explicit counterpart. The reason is that the data is binary and thus, when inferring a user's preferences, we must use unclicked items. Mirroring methods for explicit data, many methods treat unclicked items as those a user does not like. But this assumption is mistaken, and overestimates the effect of the unclicked items. Some of these items-many of them, in large-scale settings-are unclicked because the user didn't see them, rather than because she chose not to click them. This is the crux of the problem of analyzing implicit data: we know users click on items they like, but we do not know why an item is unclicked.</p><p>Existing approaches account for this by downweighting the unclicked items. In Hu et al. <ref type="bibr" target="#b7">[8]</ref> the data about unclicked items are given a lower "confidence", expressed through the variance of a Gaussian random variable. In Rendle et al. <ref type="bibr" target="#b24">[25]</ref>, the unclicked items are artificially subsampled at a lower rate in order to reduce their influence on the estimation. These methods are effective, but they involve heuristic alterations to the data.</p><p>In this paper, we take a direct approach to solving this problem. We develop a probabilistic model for recommendation called Exposure MF (abbreviated as ExpoMF) that separately captures whether a user has been exposed to an item from whether a user has ultimately decided to click on it. This leads to an algorithm that iterates between estimating the user preferences and estimating the exposure, i.e., why the unclicked items were unclicked. When estimating preferences, it naturally downweights the unclicked items that it expected the user will like, because it imagines that she was not exposed to them.</p><p>Concretely, imagine a music listener with a strong preference for alternative rock bands such as Radiohead. Imagine that, in a dataset, there are some Radiohead tracks that this user has not listened to. There are different reasons which may explain unlistened tracks (e.g., the user has a limited listening budget, a particular song is too recent or is unavailable from a particular online service). According to that user's listening history these unlistened tracks would likely make for good recommendations. In this situation our model would assume that the user does not know about these tracks-she has not been exposed to them-and downweight their (negative) contribution when inferring that user's preferences.</p><p>Further, by separating the two sides of the problem, our approach enables new innovations in implicit recommendation models. Specifically, we can build models of users' exposure that are guided by additional information such as item content, if exposure to the items typically happens via search, or user/item location, if the users and items are geographically organized.</p><p>As an example imagine a recommender system for diners in New York City and diners in Las Vegas. New Yorkers are only exposed to restaurants in New York City. From our model's perspective, unvisited restaurants in New York are therefore more informative in deriving a New Yorker's preferences compared to unvisited restaurants in Las Vegas. Accordingly for New York users our model will upweight unvisited restaurants in New York while downweighting unvisited Las Vegas restaurants.</p><p>We studied our method with user listening history from a music intelligence company, clicks from a scientific e-print server, user bookmarks from an online reference manager, and user checkins at venues from a location-based social network. In all cases, ExpoMF matches or surpasses the state-of-the-art method of Hu et al. <ref type="bibr" target="#b7">[8]</ref>. Furthermore, when available, we use extra information to inform our user exposure model. In those cases using the extra information outperforms the simple ExpoMF model. Further, when using document content information our model also outperforms a method specially developed for recommending documents using content and user click information <ref type="bibr" target="#b29">[30]</ref>. Finally, we illustrate the alternative-rock-listener and the New-Yorkdinner examples using real data fit with our models in Figure <ref type="figure">2</ref> and Figure <ref type="figure">3</ref>.</p><p>This paper is organized as follows. We first review collaborative filtering models in Section 2. We then introduce Ex-poMF in Section 3 and location and content specific models in the subsequent subsection. We draw connections between ExpoMF and causal inference as well as other recommendation research paths in Section 4. Finally we present an empirical study in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>Matrix factorization for collaborative filtering. Useritem preference data, whether implicit or not, can be encoded in a user by item matrix. In this paper we refer to this user by item matrix as the click matrix or the consumption matrix. Given the observed entries in this matrix the recommendation task is often framed as filling in the unobserved entries. Matrix factorization models, which infer (latent) user preferences and item attributes by factorizing the click matrix, are standard in recommender systems <ref type="bibr" target="#b10">[11]</ref>. From a generative modeling perspective they can be understood as first drawing user and item latent factors corresponding, respectively, to user preferences and item attributes. Then drawing observations from a specific distribution (e.g., a Poisson or a Gaussian) with its mean parametrized by the dot product between the user and the item factors. Formally, Gaussian matrix factorization is <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_0">θu ∼ N (0, λ -1 θ IK ) β i ∼ N (0, λ -1 β IK ) yui ∼ N (θ u β i , λ -1 y ),</formula><p>where θu and β i represent user u's latent preferences and item i's attributes respectively. We use the mean and covariance to parametrize the Gaussian distribution. λ θ , λ β , and λy are treated as hyperparameters. IK stands for the identity matrix of dimension K.</p><p>Collaborative filtering for implicit data. Weighted matrix factorization (WMF), the standard factorization model for implicit data (also known as one-class collaborative filtering <ref type="bibr" target="#b19">[20]</ref>), selectively downweights evidence from the click matrix <ref type="bibr" target="#b7">[8]</ref>. WMF uses a simple heuristic where all unobserved user-item interactions are equally downweighted visa-vis the observed interactions. Under WMF an observation is generated from:</p><formula xml:id="formula_1">yui ∼ N (θ u β i , c -1 y ui ),</formula><p>where the "confidence" c is set such that c1 &gt; c0. This dependency between a click and itself is unorthodox; because of it WMF is not a generative model. As we will describe in Section 3 we obtain a proper generative model by adding an exposure latent variable. WMF treats the collaborative filtering problem with implicit data as a regression problem. Concretely, consumed user-item pairs are assigned a value of one and unobserved user-item pairs are assigned a value of zero. Bayesian personalized ranking (BPR) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> instead treats the problem as a one of ranking consumed user-item pairs above unobserved pairs. In a similar vein, the weighted approximateranking pairwise (WARP) loss proposed in Weston et al. <ref type="bibr" target="#b30">[31]</ref> approximately optimizes Precision@k. To deal with the non-differentiable nature of the ranking loss, these methods typically design specific (stochastic optimization) methods for parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPOSURE MATRIX FACTORIZATION</head><p>We present exposure matrix factorization (ExpoMF). In Section 3.1, we describe the main model. In Section 3.2 we discuss several ways of incorporating external information into ExpoMF (i.e., topics from text, locations). We derive inference procedures for our model (and variants) in Section 3.3. Finally we discuss how to make predictions given our model in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Description</head><p>For every combination of users u = 1, . . . , U and items i = 1, . . . , I, consider two sets of variables. The first matrix A = {aui} indicates whether user u has been exposed to item i. The second matrix Y = {yui} indicates whether or not user u clicked on item i.</p><p>Whether a user is exposed to an item comes from a Bernoulli. Conditional on being exposed, user's preference comes from a matrix factorization model. Similar to the standard methodology, we factorize this conditional distribution to K user preferences θi,1:K and K item attributes βu,1:K ,</p><formula xml:id="formula_2">θu ∼ N (0, λ -1 θ IK ) β i ∼ N (0, λ -1 β IK ) aui ∼ Bernoulli(µui) yui | aui = 1 ∼ N (θ u β i , λ -1 y ) yui | aui = 0 ∼ δ0,<label>(1)</label></formula><p>where δ0 denotes that p(yui = 0 | aui = 0) = 1, and we introduced a set of hyperparameters denoting the inverse variance (λ θ , λ β , λy). µui is the prior probability of exposure, we discuss various ways of setting or learning it in subsequent sections. A graphical representation of the model in Equation ( <ref type="formula" target="#formula_2">1</ref>) is given in Figure <ref type="figure" target="#fig_1">1a</ref>. We observe the complete click matrix Y . These have a special structure. When yui &gt; 0, we know that aui = 1. When yui = 0, then aui is latent. The user might have been exposed to item i and decided not to click (i.e., aui = 1, yui = 0); or she may have never seen the item (i.e., aui = 0, yui = 0). We note that since Y is usually sparse in practice, most aui will be latent.</p><p>The model described in Equation (1) leads to the following log joint probability 1 of exposures and clicks for user u and item i,</p><formula xml:id="formula_3">log p(aui, yui | µui, θu, β i , λ -1 y ) = log Bernoulli(aui | µui) + aui log N (yui | θ u β i , λ -1 y ) + (1 -aui) log I[yui = 0],<label>(2)</label></formula><p>where I[b] is the indicator function that evaluates to 1 when b is true, and 0 otherwise. What does the distribution in Equation 2 say about the model's exposure beliefs when no clicks are observed? When the predicted preference is high (i.e., when θ u β i is high) then the log likelihood of no clicks log N (0 | θ u β i , λ -1 y ) is low and likely non-positive. This feature penalizes the model for placing probability mass on aui = 1, forcing us to believe that user u is not exposed to item i. (The converse argument also holds for low values of θ u β i ). Interestingly, a low value of aui downweights the evidence for θu and β i (this is clear by considering extreme values: when aui = 0, the user and item factors do not affect the log joint in Eq. 2 at all; when aui = 1, we recover standard matrix factorization). Like weighted matrix factorization (WMF) <ref type="bibr" target="#b7">[8]</ref>, ExpoMF shares the same feature of selectively downweighting evidence from the click matrix.</p><p>In ExpoMF, fixing the entries of the exposure matrix to a single value (e.g., aui = 1, ∀u, i) recovers Gaussian probabilistic matrix factorization <ref type="bibr" target="#b17">[18]</ref> (see <ref type="bibr">Section 2)</ref>. WMF is also a special case of our model which can be obtained by fixing ExpoMF's exposure matrix using c0 and c1 as above.</p><p>The intuitions we developed for user exposure from the joint probability do not yet involve µui, the prior belief on exposure. As we noted earlier, there are a rich set of choices available in the modeling of µui. We discuss several of these next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Modeling of Exposure</head><p>We now discuss methods for choosing and learning µui. One could fix µui at some global value for all users and items, 1 N.B., we follow the convention that 0 log 0 = 0 to allow the log joint to be defined when yui &gt; 0.  meaning that the user factors, item factors, and clicks would wholly determine exposure (conditioned on variance hyperparameters). One could also fix µui for specific values of u and i. This can be done when there is specific extra information that informs us about exposure (denoted as exposure covariates), e.g. the location of a restaurant, the content of a paper. However, we found that empirical performance is highly sensitive to this choice, motivating the need to place models on the prior for µui with flexible parameters. We introduce observed exposure covariates xi and exposure model parameters ψ u and condition µui | ψ u , xi according to some domain-specific structure. The extended graphical model with exposure covariates is shown in Fig- <ref type="figure" target="#fig_1">ure 1b</ref>. Whatever this exposure model looks like, conditional independence between the priors for exposure and the more standard collaborative filtering parameters (given exposure) ensures that the updates for the model we introduced in Section 3.1 will be the same for many popular inference procedures (e.g., expectation-maximization, variational inference, Gibbs sampling), making the extension to exposure covariates a plug-in procedure. We discuss two possible choices of exposure model next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-item µi.</head><p>A direct way to encode exposure is via item popularity: if a song is popular, it is more likely that you have been exposed to it. Therefore, we choose an itemdependent conjugate prior on µi ∼ Beta(α1, α2). This model does not use any external information (beyond clicks).</p><p>Text topics or locations as exposure covariates. In the domain of recommending text documents, we consider the exposure covariates as the set of words for each document. In the domain of location-based recommendation, the exposure covariates are the locations of the venues being recommended. We treat both in a similar way.</p><p>Consider a L-dimensional (L does not necessarily equal the latent space dimension K in the matrix factorization model) representation xi of the content of document i obtained through natural language processing (e.g., word embeddings <ref type="bibr" target="#b16">[17]</ref>, latent Dirichlet allocation <ref type="bibr" target="#b1">[2]</ref>), or the position of venue i obtained by first clustering all the venues in the data set then finding the expected assignment to L clusters for each venue. In both cases, xi is all positive and normalizes to 1. Denoting σ as the sigmoid function, we set</p><formula xml:id="formula_4">µui = σ(ψ u xi),<label>(3)</label></formula><p>where we learn the coefficients ψ u for each user u. Furthermore, we can include intercepts with various levels and interactions <ref type="bibr" target="#b5">[6]</ref>.</p><p>How to interpret the coefficients ψ u ? The first interpretation is that of logistic regression, where the independent variables are xi, the dependent binary variables are aui, and the coefficients to learn are ψ u .</p><p>The second interpretation is from a recommender systems perspective: ψ u represents the topics (or geographical points of interest) that a user is usually exposed to, restricting the choice set to documents and venues that match ψ u . For example, if the l th topic represents neural networks, and x il is high, then the user must be an avid consumer of neural network papers (i.e., ψ ul must be high) for the model to include an academic paper i in the exposure set of u. In the location domain if the l th cluster represents Brooklyn, and x il is high, then the user must live in or visit Brooklyn often for the model to include venues near there in the exposure set of u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>We use expectation-maximization (EM) <ref type="bibr" target="#b4">[5]</ref> to find the maximum a posteriori estimates of the unknown parameters of the model. 2 The algorithm is summarized in Algorithm 1.</p><p>The EM inference procedure for the basic model, Ex-poMF, can be found by writing out the full log likelihood of the model, then alternating between finding the expectations of missing data (exposure) in the E(xpectation)-step and finding maximum of the likelihood with respect to the parameters in the M(aximization)-step. This procedure is analytical for our model because it is conditionally conjugate, meaning that the posterior distribution of each random variable is in the same family as its prior in the model.</p><p>Furthermore, as we mentioned in Section 3.2, conditional independence between the priors for µui and the rest of the model (given µui) means that the update for the latent exposure variables and user and item factors are not altered for any exposure model we use. We present these general updates first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-step.</head><p>In the E-step, we compute expectation of the exposure latent variable E[aui] for all user and item combinations (u, i) for which there are no observed clicks (recall that the presence of clicks yui &gt; 0 means that aui = 1 deterministically),</p><formula xml:id="formula_5">E[aui | θu, β i , µui, yui = 0] = µui • N (0|θ u β i , λ -1 y ) µui • N (0|θ u β i , λ -1 y ) + (1 -µui) . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>2 There are various other inference methods we could have used, such as Markov chain Monte Carlo <ref type="bibr" target="#b26">[27]</ref> or variational inference <ref type="bibr" target="#b28">[29]</ref>. We chose EM for reasons of efficiency and simplicity, and find that it performs well in practice. Compute expected exposure A (Equation <ref type="formula" target="#formula_5">4</ref>) 5:</p><p>Update user factors θ1:U (Equation <ref type="formula" target="#formula_7">5</ref>) 6:</p><p>Update item factors β 1:I (Equation <ref type="formula" target="#formula_9">6</ref>) 7:</p><p>ExpoMF with per-item µi: Update priors µi (Equation <ref type="formula" target="#formula_10">7</ref>) 8:</p><p>ExpoMF with exposure model µui = σ(ψ u xi): Update coefficients ψ u (Equation 8 or (10)) 9: end while where N (0 | θ u β i , λ -1 y ) stands for the probability density function of N (θ u β i , λ -1 y ) evaluated at 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-step.</head><p>For notational convenience, we define pui = E[aui | θu, βi, µui, yui = 0] computed from the E-step. Without loss of generality, we define pui = 1 if yui = 1. The update for the latent collaborative filtering factors is:</p><formula xml:id="formula_7">θu ← (λy i puiβ i β i + λ θ IK ) -1 ( i λypuiyuiβ i ) (<label>5</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">β i ← (λy u puiθuθ u + λ β IK ) -1 ( u λypuiyuiθu),<label>(6)</label></formula><p>Inference for the Exposure Prior µui</p><p>We now present inference for the hierarchical variants of Exposure MF. In particular we highlight the updates to µui under the various models we presented in Section 3.2.</p><p>Update for per-item µi. Maximizing the log likelihood with respect to µi is equivalent to finding the mode of the complete conditional Beta(α1 + u pui, α2 + Uu pui), which is:</p><formula xml:id="formula_10">µi ← α1 + u pui -1 α1 + α2 + U -2<label>(7)</label></formula><p>Update for exposure covariates (topics, location).</p><p>Setting µui = σ(ψ u xi), where xi is given by pre-processing (topic analysis or clustering), presents us with the challenge of maximizing the log likelihood with respect to exposure model parameters ψ u . Since there is no analytical solution for the mode, we resort to following the gradients of the log likelihood with respect to ψ u ,</p><formula xml:id="formula_11">ψ new u ← ψ u + η∇ ψ u L,<label>(8)</label></formula><p>for some learning rate η, where</p><formula xml:id="formula_12">∇ ψ u L = 1 I i (pui -σ(ψ u xi))xi.<label>(9)</label></formula><p>This can be computationally challenging especially for large item-set sizes I. Therefore, we perform (mini-batch) stochastic gradient descent: at each iteration t, we randomly subsample a small batch of items Bt and take a noisy gradient steps:</p><formula xml:id="formula_13">ψ new u ← ψ u + ηtg t (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>for some learning rate ηt, where gt = 1</p><formula xml:id="formula_15">|B t | i∈B t (pui -σ(ψ u xi))xi.<label>(11)</label></formula><p>For each EM iteration, we found it sufficient to do a single update to the exposure model parameter ψ u (as opposed to updating until it reaches convergence). This partial Mstep <ref type="bibr" target="#b18">[19]</ref> is much faster in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity and Implementation Details</head><p>A naive implementation of the weighted matrix factorization (WMF) <ref type="bibr" target="#b7">[8]</ref> has the same complexity as ExpoMF in terms of updating the user and item factors. However, the trick that is used to speed up computations in WMF cannot be applied to ExpoMF due to the non-uniformness of the exposure latent variable aui. On the other hand, the factor updates are still independent across users and items. These updates can therefore easily be parallelized.</p><p>In ExpoMF's implementation, explicitly storing the exposure matrix A is impractical for even medium-sized datasets. As an alternative, we perform the E-step on the fly: only the necessary part of the exposure matrix A is constructed for the updates of the user/item factors and exposure priors µui. As shown in Section 5, with parallelization and the onthe-fly E-step, ExpoMF can be easily fit to medium-to-large datasets. <ref type="foot" target="#foot_0">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction</head><p>In matrix factorization collaborative filtering the prediction of yui is given by the dot product between the inferred user and item factors θ u β i . This corresponds to the predictive density of ExpoMF p(yui | Y ) using point mass approximations to the posterior given by the EM algorithm <ref type="foot" target="#foot_1">4</ref> . However, ExpoMF can also make predictions by integrating out the uncertainty from the exposure latent variable aui:</p><formula xml:id="formula_16">Ey[yui | θu, β i ] = Ea Ey[yui | θu, β i , aui] = a ui ∈{0,1} P(aui)Ey[yui | θu, β i , aui] = µui • θ u β i<label>(12)</label></formula><p>We experimented with both predictions in our study and found that the simple dot product works better for ExpoMF with per-item µi while E[yui | θu, β i ] works better for Ex-poMF with exposure covariates. We provide further insights about this difference in Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>In this section we highlight connections between ExpoMF and other similar research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal inference.</head><p>Our work borrows ideas from the field of causal inference <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>. Causal inference aims at understanding and explaining the effect of one variable on another.</p><p>One particular aim of causal inference is to answer counterfactual questions. For example, "would this new recommendation engine increase user click through rate?". While online studies may answer such a question, they are typically expensive even for large electronic commerce companies. Obtaining answers to such questions using observa-tional data alone (e.g., log data) is therefore of important practical interest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>We establish a connection with the potential outcome framework of Rubin <ref type="bibr" target="#b25">[26]</ref>. In this framework one differentiates the assignment mechanism, whether a user is exposed to an item, from the potential outcome, whether a user consumes an item. In potential outcome terminology our work can thus be understood as a form a latent assignment model. In particular, while consumption implies exposure, we do not know which items users have seen but not consumed. Further the questions of interest to us, personalized recommendation, depart from traditional work in causal inference which aims at quantifying the effect of a particular treatment (e.g., the efficacy of a new drug).</p><p>Biased CF models. Authors have recognized that typical observational data describing user rating items is biased toward items of interest. Although this observation is somewhat orthogonal to our investigation, models that emerged from this line of work share commonalities with our approach. Specifically, Marlin et al. <ref type="bibr" target="#b15">[16]</ref>, Ling et al. <ref type="bibr" target="#b13">[14]</ref> separate the selection model (the exposure matrix) from the data model (the matrix factorization). However, their interpretation, rooted in the theory of missing data <ref type="bibr" target="#b14">[15]</ref>, leads to a much different interpretation of the selection model. They hypothesize that the value of a rating influences whether or not a user will report the rating (this implicitly captures the effect that users mostly consume items they like a priori). This approach is also specific to explicit feedback data. In contrast, we model how (the value of) the exposure matrix affects user rating or consumption.</p><p>Modeling exposure with random graphs. The useritem interaction can also be encoded as a bipartite graph. Paquet and Koenigstein <ref type="bibr" target="#b20">[21]</ref> model exposure using a hidden consider graph. This graph plays a similar role as our exposure variable. One important difference is that during inference, instead of directly inferring the posterior as in Ex-poMF (which is computationally more demanding), an approximation is developed whereby a random consider graph is stochastically sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exposure in other contexts.</head><p>In zero-inflated Poisson regression, a latent binary random variable, similar to our exposure variable is introduced to "explain away" the structural zeros, such that the underlying Poisson model can better capture the count data <ref type="bibr" target="#b11">[12]</ref>. This type of model is common in Economics where it is used to account for overly frequent zero-valued observations. ExpoMF can also be considered as an instance of a spikeand-slab model <ref type="bibr" target="#b9">[10]</ref> where the "spike" comes from the exposure variables and the matrix factorization component forms the flat "slab" part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Versatile CF models.</head><p>As we show in Section 3.2, Ex-poMF's exposure matrix can be used to model external information describing user and item interactions. This is in contrast to most CF models which are crafted to model a single type of data (e.g., document content when recommendation scientific papers <ref type="bibr" target="#b29">[30]</ref>). An exception is factorization machines (FM) of Rendle <ref type="bibr" target="#b22">[23]</ref>. FM models all types of (numeric) user, item or user-item features. FM considers the interaction between all features and learns specific parameters for each interaction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EMPIRICAL STUDY</head><p>In this section we study the recommendation performance of ExpoMF by fitting the model to several datasets. We provide further insights into ExpoMF's performance by exploring the resulting model fits. We highlight that:</p><p>• ExpoMF performs comparably better than the stateof-the-art WMF <ref type="bibr" target="#b7">[8]</ref> on four datasets representing user clicks, checkins, bookmarks and listening behavior.</p><p>• When augmenting ExpoMF with exposure covariates its performance is further improved. ExpoMF with location covariates and ExpoMF with content covariates both outperform the simpler ExpoMF with peritem µi. Furthermore, ExpoMF with content covariates outperforms a state-of-the-art document recommendation model <ref type="bibr" target="#b29">[30]</ref>.</p><p>• Through posterior exploration we provide insights into ExpoMF's user-exposure modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Throughout this study we use four medium to large-scale user-item consumption datasets from various domains: 1) taste profile subset (TPS) of the million song dataset <ref type="bibr" target="#b0">[1]</ref>; 2) scientific articles data from arXiv<ref type="foot" target="#foot_2">foot_2</ref> ; 3) user bookmarks from Mendeley<ref type="foot" target="#foot_3">foot_3</ref> ; and 4) check-in data from the Gowalla dataset <ref type="bibr" target="#b3">[4]</ref>. In more details:</p><p>• Taste Profile Subset (TPS): contains user-song play counts collected by the music intelligence company Echo Nest. <ref type="foot" target="#foot_4">7</ref> We binarize the play counts and interpret them as implicit preference. We further pre-process the dataset by only keeping the users with at least 20 songs in their listening history and songs that are listened to by at least 50 users.</p><p>• ArXiv: contains user-paper clicks derived from log data collected in 2012 by the arXiv pre-print server. Multiple clicks by the same user on a single paper are considered to be a single click. We pre-process the data to ensure that all users and items have a minimum of 10 clicks.</p><p>• Mendeley: contains user-paper bookmarks as provided by the Mendeley service, a "reference manager". The behavior data is filtered such that each user has at least 10 papers in her library and the papers that are bookmarked by at least 20 users are kept. In addition this dataset contains the content of the papers which we pre-process using standard techniques to yield a 10K words vocabulary. In Section 5.6 we make use of paper content to inform ExpoMF's exposure model.</p><p>• Gowalla: contains user-venue checkins from a locationbased social network. We pre-process the data such that all users and venues have a minimum of 20 checkins. Furthermore, this dataset also contains locations for the venues which we will use to guide locationbased recommendation (Section 5.6).</p><p>The final dimensions of these datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setup</head><p>For each dataset we randomly split the observed user-item interactions into training/test/validation sets with 70/20/10 proportions. In all the experiments, the dimension of the latent space for collaborative filtering model K is 100. The model is trained following the inference algorithm described in Section 3.3. We monitor the convergence of the algorithm using the truncated normalized discounted cumulative gain (NDCG@100, see below for details) on the validation set. Hyper-parameters for ExpoMF-based models and baseline models are also selected according to the same criterion.</p><p>To make predictions, for each user u, we rank each item using its predicted preference y * ui = θ u β i , i = 1, • • • , I. We then exclude items from the training and validation sets and calculate all the metrics based on the resulting ordered list. Further when using ExpoMF with exposure covariates we found that performance was improved by predicting missing preferences according to E[yui|θu, β i ] (see Section 3.4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance measures</head><p>To evaluate the recommendation performance, we report both Recall@k, a standard information retrieval measure, as well as two ranking-specific metrics: mean average precision (MAP@k) and NDCG@k. 8  We denote rank(u, i) as the rank of item i in user u's predicted list and y test u as the set of items in the heldout test set for user u.</p><p>• Recall@k: For each user u, Recall@k is computed as follows:</p><p>Recall@k =</p><formula xml:id="formula_17">i∈y test u 1{rank(u, i) ≤ k} min(k, |y test u |)</formula><p>8 Hu et al. <ref type="bibr" target="#b7">[8]</ref> propose mean percentile rank (MPR) as an evaluation metric for implicit feedback recommendation. Denote perc(u, i) as the percentile-ranking of item i within the ranked list of all items for user u. That is, perc(u, i) = 0% if item i is ranked first in the list, and perc(u, i) = 100% if item i is ranked last. Then MPR is defined as:</p><formula xml:id="formula_18">MPR = u,i y test u,i • perc(u, i) u,i y test u,i</formula><p>We do not use this metric because MPR penalizes ranks linearly -if one algorithm ranks all the heldout test items in the middle of the list, while another algorithm ranks half of the heldout test items at the top while the other half at the bottom, both algorithms will get similar MPR, while clearly the second algorithm is more desirable. On the other hand, the metrics we use here will clearly prefer the second algorithm.  <ref type="bibr" target="#b7">[8]</ref> and ExpoMF. While the differences in performance are generally small, ExpoMF performs comparably better than WMF across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPS</head><p>where 1{•} is the indicator function. In all our experiments we report both k = 20 and k = 50. We do not report Precision@k due to the noisy nature of the implicit feedback data: even if an item i / ∈ y test u , it is possible that the user will consume it in the future. This makes Precision@k less interpretable since it is prone to fluctuations.</p><p>• MAP@k: Mean average precision calculates the mean of users' average precision. The (truncated) average precision for user u is:</p><formula xml:id="formula_19">Average Precision@k = k n=1</formula><p>Precision@n min(n, |y test u |) .</p><p>• NDCG@k: Emphasizes the importance of the top ranks by logarithmically discounting ranks. NDCG@k for each user is computed as follows:</p><formula xml:id="formula_20">DCG@k = k i=1 2 rel i -1 log 2 (i + 1)</formula><p>; NDCG@k = DCG@k IDCG@k IDCG@k is a normalization factor that ensures NDCG lies between zero and one (perfect ranking). In the implicit feedback case the relevance is binary: reli = 1 if i ∈ y test u , and 0 otherwise. In our study we always report the averaged NDCG across users.</p><p>For the ranking-based measure in all the experiments we set k = 100 which is a reasonable number of items to consider for a user. Results are consistent when using other values of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines</head><p>We compare ExpoMF to weighted matrix factorization (WMF), the standard state-of-the-art method for collaborative filtering with implicit data <ref type="bibr" target="#b7">[8]</ref>. WMF is described in Section 2.</p><p>We also experimented with Bayesian personalized ranking (BPR) <ref type="bibr" target="#b24">[25]</ref>, a ranking model for implicit collaborative filtering. However preliminary results were not competitive with other approaches. BPR is trained using stochastic optimization which can be sensible to hyper-parameter values (especially hyper-parameters related to the optimization procedure). A more exhaustive search over hyper-parameters could yield more competitive results.</p><p>We describe specific baselines relevant to modeling exposure covariates in their dedicated subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Studying Exposure MF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical evaluation.</head><p>Results comparing ExpoMF to WMF on our four datasets are given in Table <ref type="table" target="#tab_2">2</ref>. Each metric is averaged across all the users. We notice that ExpoMF performs comparably better than WMF on most datasets (the standard errors are on the order of 10 -4 ) though the difference in performance is small. In addition, higher values of NDCG@100 and MAP@100 (even when Recall@50 is lower) indicate that the top-ranked items by ExpoMF tend to be more relevant to users' interests.</p><p>Exploratory analysis. We now explore posterior distributions of the exposure latent variables of two specific users from the TSP dataset. This exploration provides insights into how ExpoMF infers user exposure.</p><p>The top figure of Figure <ref type="figure">2</ref> shows the inferred exposure latent variable E[aui] corresponding to yui = 0 for user A. E[aui] is plotted along with the empirical item popularity (measured by number of times a song was listened to in the training set). We also plot the interpolated per-item consideration prior µi learned using Equation <ref type="formula" target="#formula_10">7</ref>. There is a strong relationship between song popularity and consideration (this is true across users). User A's training data revealed that she has only listened to songs from either Radiohead or Interpol (both are alternative rock bands). Therefore, for most songs, the model infers that the probability of user A considering them is higher than the inferred prior, i.e., it is more likely that user A did not want to listen to them (they are true zeros). However, as pointed out by the rectangular box, there are a few "outliers" which mostly contain songs from Radiohead and Interpol that user A did not listen to (some of them are in fact held out in the test set). Effectively, a lower posterior E[aui] than the prior indicates that the model downweights these unlistened songs more. In contrast, WMF downweights all songs uniformly.</p><p>A second example is shown in the bottom figure of Figure <ref type="figure">2</ref>. User B mostly listens to indie rock bands (e.g. Florence and the Machine, Arctic Monkeys, and The Kills). "Dog Days are Over" by Florence and the Machine is the second most popular song in this dataset, behind "Fireworks" by Katy Perry. These two songs correspond to the two rightmost dots on the figure. Given the user's listening history, the model clearly differentiates these two similarly popular songs. The fact that user B did not listen to "Dog Days are Over" (again in the test set) is more likely due to her not having been exposed to it. In contrast the model infers that the user probably did not like "Fireworks" even though it is popular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Incorporating Exposure Covariates</head><p>Figure <ref type="figure">2</ref> demonstrates that ExpoMF strongly associates user exposure to item popularity. This is partly due to the fact that the model's prior is parametrized with a per-item term µi.</p><p>Here we are interested in using exposure covariates to provide additional information about the (likely) exposure of users to items (see Figure <ref type="figure" target="#fig_1">1b</ref>). LG A" 5DGLRKHDG: "JLJsDw )DOOLnJ InWR 3ODFH" 5DGLRKHDG: "6DLO 7R 7KH 0RRn" 5DGLRKHDG: "6LW 'Rwn. 6WDnG 8S" 5DGLRKHDG: "BORw 2uW" InWHUSRO: "C'PHUH" InWHUSRO: "1RW (vHn JDLO" InWHUSRO: "3uEOLF 3HUvHUW" InWHUSRO: "1H[W (vLO" InFuEus: "BDss 6ROR" GUHDW :KLWH: "HRusH 2I BURNHn LRvH" 8sHU A (5DGLRKHDG DnG InWHUSRO OLsWHnHU) 3RsWHULRU RI a ui LHDUnHG SULRU µ i Recall that the role of these exposure covariates is to allow the matrix factorization component to focus on items that the user has been exposed to. In particular this can be done in the model by upweighting (increasing their probability of exposure) items that users were (likely) exposed to and downweighting items that were not. A motivating example with restaurant recommendations and New York City versus Las Vegas diners was discussed in Section 1.</p><p>In the coming subsections we compare content-aware and location-aware versions of ExpoMF which we refer to as Content ExpoMF and Location ExpoMF respectively. Studying each model in its respective domain we demonstrate that the exposure covariates improve the quality of the recommendations compared to ExpoMF with per-item µi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Covariates</head><p>Scientists-whether through a search engine, a personal recommendation or other means-have a higher likelihood of being exposed to papers specific to their own discipline. In this section we study the problem of using the content of papers as a way to guide inference of the exposure component of ExpoMF.</p><p>In this use case, we model the user exposure based on the topics of articles. We use latent Dirichlet allocation (LDA) <ref type="bibr" target="#b1">[2]</ref>, a model of document collections, to model article content. Assuming there are K topics Φ = φ 1:K , each of which is a categorical distribution over a fixed set of vocabulary, LDA treats each document as a mixture of these topics where the topic proportion xi is inferred from the data. One can understand LDA as representing documents in a low-dimensional "topic" space with the topic proportion xi being their coordinates. We also compare collaborative topic regression (CTR) <ref type="bibr" target="#b29">[30]</ref>, a model makes use of the same additional information as Content ExpoMF.</p><p>We use the topic proportion xi learned from the Mendeley dataset as exposure covariates. Following the notation of Section 3.2, our hierarchical ExpoMF is:</p><formula xml:id="formula_21">µui = σ(ψ u xi + γu)</formula><p>where we include a per-user bias term γu. Under this model, a molecular biology paper and a computer science paper that a computer scientist has not read will likely be treated differently: the model will consider the computer scientist has been exposed to the computer science paper, thus higher E[aui], yet not to the molecular biology paper (hence lower E[aui]). The matrix factorization component of the model will focus on modeling computer science papers since that are more likely to be have been exposed.</p><p>Our model, Content ExpoMF, is trained following the algorithm in Algorithm 1. For updating exposure-related model parameters ψ u and γu, we take mini-batch gradient steps with a batch-size of 10 users and a constant step size of 0.5 for 10 epochs. Figure <ref type="figure">3</ref>: We compare the inferred exposure posterior of ExpoMF (top row) and Content ExpoMF (bottom row). On the left are the posteriors of user A who is interested in statistical machine learning while on the right user B is interested in computer system research. Neither users have read the "Latent Dirichlet Allocation" paper. ExpoMF infers that both users have about equal probability of having been exposed to it. As we discussed in Section 5.5 (and demonstrated in Figure <ref type="figure">2</ref>) this is mostly based on the popularity of this paper. In contrast, Content ExpoMF infers that user A has more likely been exposed to this paper because of the closeness between that paper's content and user A's interest. Content ExpoMF therefore upweights the paper. Given user B's interests the paper is correctly downweighted by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study.</head><p>We evaluate the empirical performance of Content ExpoMF and report results in Table <ref type="table" target="#tab_3">3</ref>. We compare to collaborative topic regression (CTR), a state-of-the-art method for recommending scientific papers <ref type="bibr" target="#b29">[30]</ref> combining both LDA and WMF. <ref type="foot" target="#foot_5">9</ref> We did not compare with the more recent and scalable collaborative topic Poisson factorization (CTPF) <ref type="bibr" target="#b6">[7]</ref> since the resulting performance differences may have been the result of CTPF Poisson likelihood (versus Gaussian likelihood for both ExpoMF and WMF).</p><p>We note that CTR's performance falls in-between the performance of ExpoMF and WMF (from Table <ref type="table" target="#tab_1">1</ref>). CTR is particularly well suited to the cold-start case which is not the data regime we focus on in this study (i.e., recall that we have only kept papers that have been bookmarked by at least 20 users).</p><p>Figure <ref type="figure">3</ref> highlights the behavior of Content ExpoMF compared to that of regular ExpoMF. Two users are selected: User A (left column) is interested in statistical machine learning and Bayesian statistics. User B (right column) is interested in computer systems. Neither of them have read "Latent Dirichlet Allocation" (LDA) a seminal paper that falls within user A's interests. On the top row we show the posterior of the exposure latent variables E[aui] for two users (user A and user B) inferred from ExpoMF with per-item µi. LDA is shown using a white dot. Overall both users' estimated exposures are dominated by the empirical item popularity.</p><p>In contrast, on the bottom row we plot the results of Content ExpoMF. Allowing the model to use the documents' content to infer user exposure offers greater flexibility compared to the simple ExpoMF model. This extra flexibility may also explain why there is an advantage in using inferred exposure to predict missing observations (see Section 3.4). Namely when exposure covariates are available the model can better capture the underlying user exposures to items. In contrast using the inferred exposure to predict with the simple ExpoMF model performs worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Location Covariates</head><p>When studying the Gowalla dataset we can use venue location as exposure covariates.</p><p>Recall from Section 3.2 that location exposure covariates are created by first clustering all venues (using K-means) and then finding the representation of each venue in this clustering space. Similarly as in Content ExpoMF (Section 5.6), Location ExpoMF departs from ExpoMF: µui = σ(ψ u xi + γu) where x ik is the venue i's expected assignment to cluster k and γu is a per-user bias term. <ref type="foot" target="#foot_6">10</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study.</head><p>We train Location ExpoMF following the same procedure as Content ExpoMF. We report the empirical comparison between WMF, ExpoMF and Location ExpoMF For comparison purposes we also developed a simple baseline FilterWMF which makes use of the location covariates. FilterWMF filters out venues recommended by WMF that are inaccessible (too far) to the user. Since user location is not directly available in the dataset, we estimate it using the geometric median of all the venues the user has checked into. The median is preferable to the mean because it is better at handling outliers and is more likely to choose a typical visit location. However, the results of this simple FilterWMF baseline are worse than the results of the regular WMF. We attribute this performance to the fact that having a single focus of location is too strong an assumption to capture visit behavior of users well. In addition, since we randomly split the data, it is possible that a user's checkins at city A and city B are split between the training and test set. We leave the exploration of better location-aware baselines to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we presented a novel collaborative filtering mechanism that takes into account user exposure to items. In doing so, we theoretically justify existing approaches that downweight unclicked items for recommendation, and provide an extendable framework for specifying more elaborate models of exposure based on logistic regression. In empirical studies we found that the additional flexibility of our model helps it outperform existing approaches to matrix factorization on four datasets from various domains. We note that the same approach can also be used to analyze explicit feedback.</p><p>There are several promising avenues for future work. Consider a reader who keeps himself up to date with the "what's new" pages of a website, or a tourist visiting a new city looking for a restaurant recommendation. The exposure processes are more dynamic in these scenarios and may be different during training and test time. We therefore seek new ways to capture exposure that include ever more realistic assumptions about how users interact with items.</p><p>Finally, we would like to evaluate our proposed model in a more realistic setting, e.g., in an online environment with user interactions. It would be instructive to evaluate the performance of ExpoMF in environments where it may be possible to observe items which users have been exposed to.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of the exposure MF model (both with and without exposure covariates). Shaded nodes represent observed variables. Unshaded nodes represent hidden variables. A directed edge from node a to node b denotes that the variable b depends on the value of variable a. Plates denote replication by the value in the lower corner of the plate. The lightly shaded node aui indicates that it is partially observed (i.e., it is observed when yui = 1 and unobserved otherwise).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8 .Figure 2 :</head><label>82</label><figDesc>Figure2: We compare the inferred posteriors of the exposure matrix for two users (denoted by blue dots) and compare against the prior probability for exposure (red dashed lined). On the top, user A is a fan of the bands Radiohead and Interpol. Accordingly, the model downweights unlistened songs from these two bands. User B has broader interests and notably enjoys listening to the very popular band Florence and the Machine. Similarly as for user A, unlistened tracks of Florence and the Machine get downweighted in the posterior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,53.80,53.80,502.13,219.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Inference for ExpoMF 1: input: click matrix Y , exposure covariates x1:I (topics or locations, optional). 2: random initialization: user factors θ1:U , item factors β 1:I , exposure priors µ1:I (for per-item µi), OR exposure model parameters ψ 1:U (with exposure model). 3: while performance on validation set increases do 4:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Attributes of datasets after pre-processing. Interactions are non-zero entries (listening counts, clicks, and checkins). % interactions refers to the density of the useritem consumption matrix (Y ).</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Mendeley Gowalla ArXiv</cell></row><row><cell># of users</cell><cell>221,830</cell><cell>45,293</cell><cell>57,629</cell><cell>37,893</cell></row><row><cell># of items</cell><cell>22,781</cell><cell>76,237</cell><cell>47,198</cell><cell>44,715</cell></row><row><cell cols="2"># interactions 14.0M</cell><cell>2.4M</cell><cell>2.3M</cell><cell>2.5M</cell></row><row><cell>% interactions</cell><cell>0.29%</cell><cell>0.07%</cell><cell>0.09%</cell><cell>0.15%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between WMF</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mendeley</cell><cell cols="2">Gowalla</cell><cell cols="2">ArXiv</cell></row><row><cell></cell><cell cols="8">WMF ExpoMF WMF ExpoMF WMF ExpoMF WMF ExpoMF</cell></row><row><cell>Recall@20</cell><cell>0.195</cell><cell>0.201</cell><cell>0.128</cell><cell>0.139</cell><cell>0.122</cell><cell>0.118</cell><cell>0.143</cell><cell>0.147</cell></row><row><cell>Recall@50</cell><cell>0.293</cell><cell>0.286</cell><cell>0.210</cell><cell>0.221</cell><cell>0.192</cell><cell>0.186</cell><cell>0.237</cell><cell>0.236</cell></row><row><cell cols="2">NDCG@100 0.255</cell><cell>0.263</cell><cell>0.149</cell><cell>0.159</cell><cell>0.118</cell><cell>0.116</cell><cell>0.154</cell><cell>0.157</cell></row><row><cell>MAP@100</cell><cell>0.092</cell><cell>0.109</cell><cell>0.048</cell><cell>0.055</cell><cell>0.044</cell><cell>0.043</cell><cell>0.051</cell><cell>0.054</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between Content ExpoMF and Ex-poMF on Mendeley.</figDesc><table><row><cell></cell><cell cols="3">ExpoMF Content ExpoMF CTR [30]</cell></row><row><cell>Recall@20</cell><cell>0.139</cell><cell>0.144</cell><cell>0.127</cell></row><row><cell>Recall@50</cell><cell>0.221</cell><cell>0.229</cell><cell>0.210</cell></row><row><cell>NDCG@100</cell><cell>0.159</cell><cell>0.165</cell><cell>0.150</cell></row><row><cell>MAP@100</cell><cell>0.055</cell><cell>0.056</cell><cell>0.049</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between Location ExpoMF and Ex-poMF with per-item µi on Gowalla. Using location exposure covariates outperforms the simpler ExpoMF and WMF according to all metrics. in Table4. We note that Location ExpoMF outperforms both WMF and the simpler version of ExpoMF.</figDesc><table><row><cell></cell><cell cols="3">WMF ExpoMF Location ExpoMF</cell></row><row><cell>Recall@20</cell><cell>0.122</cell><cell>0.118</cell><cell>0.129</cell></row><row><cell>Recall@50</cell><cell>0.192</cell><cell>0.186</cell><cell>0.199</cell></row><row><cell cols="2">NDCG@100 0.118</cell><cell>0.116</cell><cell>0.125</cell></row><row><cell>MAP@100</cell><cell>0.044</cell><cell>0.043</cell><cell>0.048</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The source code to reproduce all the experimental results is available at: https://github.com/dawenl/expo-mf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>This quantity is also the treatment effect E[yui | aui = 1, θu, β i ] -E[yui | aui = 0, θu, β i ] in the potential outcomes framework, since aui = 0 deterministically ensures yui = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://arxiv.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://mendeley.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>http://the.echonest.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>Note that to train CTR we first learned a document topic model, fixed it and then learned the user preference model. It was suggested by its authors that this learning procedure provided computational advantages while not hindering performance significantly<ref type="bibr" target="#b29">[30]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>We named Content ExpoMF and Location ExpoMF differently to make it clear to the reader that they condition on content and location features respectively. Both models are in fact mathematically equivalent.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by <rs type="funder">IIS</rs>-<rs type="grantNumber">1247664</rs>, <rs type="grantNumber">ONR N00014-11-1-0651</rs>, <rs type="grantNumber">DARPA FA8750-14-2-0009</rs>, <rs type="institution">Facebook, Adobe, Amazon</rs>, and the <rs type="funder">John Templeton Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yQ2wN9A">
					<idno type="grant-number">1247664</idno>
				</org>
				<org type="funding" xml:id="_qDmrEb2">
					<idno type="grant-number">ONR N00014-11-1-0651</idno>
				</org>
				<org type="funding" xml:id="_uHPwzs8">
					<idno type="grant-number">DARPA FA8750-14-2-0009</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Society for Music Information Retrieval Conference</title>
		<meeting>the 12th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quiñonero Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Friendship and mobility: user movement in location-based social networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1082" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Data Analysis Using Regression and Multilevel/Hierarchical Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contentbased recommendations with Poisson factorization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note>Data Mining, 2008. ICDM&apos;08</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causal Inference in Statistics, Social, and Biomedical Sciences</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spike and slab variable selection: frequentist and Bayesian strategies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishwaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="730" to="773" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<idno type="ISSN">0018-9162</idno>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-inflated Poisson regression, with an application to defects in manufacturing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW &apos;10</title>
		<meeting>the 19th International Conference on World Wide Web, WWW &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Response aware model-based collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Catalina Island, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">August 14-18, 2012. 2012</date>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J A</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical Analysis with Missing Data</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative filtering and the missing at random assumption</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2007, Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">July 19-22, 2007. 2007</date>
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-class collaborative filtering with random graphs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISBN 052189560X</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">9780521895606</biblScope>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE International Conference on Data Mining</title>
		<meeting>the 2010 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Counterfactual risk minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15 Companion</title>
		<meeting>the 24th International Conference on World Wide Web, WWW &apos;15 Companion</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="939" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">WSABIE: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
