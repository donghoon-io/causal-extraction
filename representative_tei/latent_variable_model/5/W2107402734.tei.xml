<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Eye Movements and Collaborative Filtering for Proactive Information Retrieval</title>
				<funder ref="#_jcNAE7s">
					<orgName type="full">Academy of Finland</orgName>
				</funder>
				<funder ref="#_EWvKCXQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Puolam</surname></persName>
							<email>kai.puolamaki@hut.fi</email>
						</author>
						<author>
							<persName><forename type="first">Jarkko</forename><surname>Saloj</surname></persName>
							<email>jarkko.salojarvi@hut.fi</email>
						</author>
						<author>
							<persName><forename type="first">Eerika</forename><surname>Savia</surname></persName>
							<email>eerika.savia@hut.fi</email>
						</author>
						<author>
							<persName><forename type="first">Jaana</forename><surname>Simola</surname></persName>
							<email>jaana.simola@hkkk.fi</email>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
							<email>samuel.kaski@hut.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Laboratory of Computer and Information Science</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
								<address>
									<postBox>P.O. Box 5400</postBox>
									<postCode>FIN-02015 HUT</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Helsinki</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Laboratory of Computer and Information Science</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
								<address>
									<postBox>P.O. Box 5400</postBox>
									<postCode>FIN-02015 HUT</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Laboratory of Computer and Information Science</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
								<address>
									<postBox>P.O. Box 5400</postBox>
									<postCode>FIN-02015 HUT</postCode>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Knowledge and Innovation Research Helsinki School of Economics and Business Administration Tammasaarenkatu 3</orgName>
								<address>
									<postCode>FIN-00180</postCode>
									<settlement>Helsinki</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Eye Movements and Collaborative Filtering for Proactive Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1076034.1076062</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Systems]: Information Storage and Retrieval-Information Search and Retrieval</term>
					<term>I.5.1 [Computing Methodologies]: Pattern Recognition-Models</term>
					<term>H.1.2 [Information Systems]: Models and Principles-User/Machine Systems Algorithms, Experimentation, Theory Collaborative filtering, eye movements, hidden Markov model, latent variable model, mixture model, proactive information retrieval, relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a new task, proactive information retrieval by combining implicit relevance feedback and collaborative filtering. We have constructed a controlled experimental setting, a prototype application, in which the users try to find interesting scientific articles by browsing their titles. Implicit feedback is inferred from eye movement signals, with discriminative hidden Markov models estimated from existing data in which explicit relevance feedback is available. Collaborative filtering is carried out using the User Rating Profile model, a state-of-the-art probabilistic latent variable model, computed using Markov Chain Monte Carlo techniques. For new document titles the prediction accuracy with eye movements, collaborative filtering, and their combination was significantly better than by chance. The best prediction accuracy still leaves room for improvement but shows that proactive information retrieval and combination of many sources of relevance feedback is feasible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In a typical information retrieval setup users formulate queries that express their interests. The task of an information retrieval system then is to identify documents that best match the query terms, based on the contents of the documents. Alternatively, documents can be used as very complex queries to find other relevant documents, when similarity measures have been defined between documents. The systems may additionally collect explicit relevance feedback from the user, by asking which of the retrieved documents were relevant, and combine the results to a new search.</p><p>Information retrieval systems would be much more userfriendly if the number of successive explicit queries and explicit relevance evaluations could be reduced-or eliminated altogether. Our work is a feasibility study on how far we can go by measuring implicit feedback signals from the user, and by combining them with existing data about preferences of a group of similar-minded users. The task is to predict relevance; if the predictions are successful they can be used in a variety of proactive applications, including proactive information retrieval.</p><p>We infer user interest from eye movements with probabilistic models that predict whether a user finds a text relevant, given her eye movement trajectory while reading the text. The key assumption motivating the use of eye movements is that attention patterns correlate with relevance, and that attention patterns are reflected in eye movements (see <ref type="bibr" target="#b18">[19]</ref>). At the simplest, people tend to pay more attention to objects they find relevant or interesting.</p><p>Gaze direction is an indicator of the focus of attention, since accurate viewing is possible only in the central fovea area (only 1-2 degrees of visual angle) where the density of photoreceptor cells is highly concentrated. A detailed inspection of a scene is carried out in a sequence of saccades (rapid eye movements) and fixations (during which the eye is fairly motionless). Information about the environment is mostly gathered during fixations. The physiology suggests that eye movements can provide a rich source of information about the attention and interest patterns of the user. Indeed, psychologists have studied eye movements as an indicator of different cognitive processes for decades <ref type="bibr" target="#b17">[18]</ref>, and a recent feasibility study <ref type="bibr" target="#b18">[19]</ref> showed that relevance can be inferred from eye movements, at least to a certain degree.</p><p>Our key contribution is that we do not assume anything about the details of this relationship between the attention and eye movement patterns; we infer everything we need from data, using machine learning methods.</p><p>Collaborative filtering is another, complementary source of relevance information. The goal of collaborative filtering is to predict the relevance of a document to a given user, based on a database of explicit or implicit relevance ratings from a large population of users. In this work we complement the rich but noisy eye movement-based relevance feedback with collaborative filtering, using a probabilistic latent variable model.</p><p>Finally, we combine the predictions from eye movements and collaborative filtering, again with a probabilistic model. The system is modular in the sense that new better components can easily be plugged in later, to replace the ones we use in this feasibility study.</p><p>In our prototype application the users browse titles of scientific articles and their eye movements are measured. We then combine the relevance predictions of the collaborative filtering model with the model that predicts the relevance from implicit feedback information.</p><p>The main research questions of this paper are:</p><p>1. How does the eye movement model perform in inferring which articles are interesting?</p><p>2. How does the collaborative filtering model perform in the same task?</p><p>3. How do the models compare against each other?</p><p>4. Is it feasible to combine relevance predictions from implicit feedback (eye movements), and other sources (collaborative filtering), and how to do the combination?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>To our knowledge, the combination of using implicit feedback from eye movements and relevance prediction from a collaborative filtering model is new. However, earlier work exists in several separate fields: inferring relevance implicitly from eye movements <ref type="bibr" target="#b18">[19]</ref>, extending queries or modeling user preferences by estimating relevance from implicit feedback <ref type="bibr" target="#b6">[7]</ref>, and using user modeling to determine documents that may be relevant to a group of users <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b11">12]</ref>. There have also been various studies on combining collaborative filtering and content-based filtering in general (e.g. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b13">14]</ref>).</p><p>Eye movements have earlier been utilized as alternative input modalities for either pointing at icons or typing text in human-computer interfaces (the most recent application being <ref type="bibr" target="#b24">[25]</ref>). The first application where user interest was inferred from eye movements was an interactive story teller <ref type="bibr" target="#b22">[23]</ref>. The story teller concentrated more on items that the user was gazing at on a display. Rudimentary relevance determination is needed also in <ref type="bibr" target="#b5">[5]</ref>, where a proactive translator is activated if the reader encounters a word which she has difficulties in understanding. These difficulties are inferred from eye movements.</p><p>Traditionally implicit feedback in IR has been derived from document reading time, or by monitoring user behavior: saving, printing, and selecting of documents (see <ref type="bibr" target="#b6">[7]</ref> for a good overview on different approaches). Use of eye movements as a source of implicit feedback for IR is a relatively new concept. A prototype attentive agent application (Simple User Interest Tracker, Suitor) is introduced in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. The agent monitors eye movements while the user views web pages, in order to determine whether the user is reading or just browsing. If reading is detected, the document is defined relevant, and more information on the topic is sought and displayed. The feasibility of the application was not experimentally verified, however. To our knowledge the only study with statistically tested significance is <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which is a simple feasibility study. The experimental setup is close to ours, but the task is much simpler. The user is presented a question and a list of possible answers, some of which are relevant to the question and one provides the answer. The eye movements are then used to infer the relevant lines, as well as the correct answer.</p><p>Traditionally collaborative filtering has been performed by memory-based techniques, in which one first identifies users similar to a given user and then gives predictions based on interests of those users (see e.g. GroupLens <ref type="bibr">[8]</ref>, or Ringo <ref type="bibr" target="#b21">[22]</ref>). However, the time and memory requirements of the memorybased techniques do not scale well as the number of users and documents increases.</p><p>We propose a model-based approach, which is based on the User Rating Profile model (URP) <ref type="bibr" target="#b11">[12]</ref>. We have extended the previous work by optimizing the URP by using Markov Chain Monte Carlo (MCMC) integration instead of the variational approximation used earlier. The model structure of URP is closely related to a probabilistic latent variable model introduced by Pritchard et al. <ref type="bibr" target="#b15">[16]</ref>, and Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">[2]</ref> which is also known as Multinomial PCA (mPCA) <ref type="bibr" target="#b3">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Eye Movement Modeling</head><p>Eye movements were modeled using hidden Markov models (HMMs); they are simple yet reliable models for sequential data. We use two kinds of HMMs: ordinary and discriminative, both modeling word-level eye movement data. In eye movement research, HMMs have earlier been used for segmenting the low-level eye movement signal to detect focus of attention and for implementing (fixed) models of cognitive processing <ref type="bibr" target="#b20">[21]</ref>. Discriminative HMMs have been previously applied to eye movement data in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Prediction of known classes with machine learning methods is based on a labeled data set from which the predictive model is learned. We collected such a set by measuring eye movements in a setting where relevance was known: explicit feedback for presented sentences (titles of scientific documents) was collected from the user during the recording session. The experimental setup is described in more detail in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden Markov Models</head><p>The simplest model that takes the sequential nature of eye movement data into account is a two-state HMM. We optimized one model individually for each of the two classes, in our case relevant (R) and irrelevant (I) sentences. In a prediction task a maximum a posteriori (MAP) estimate was computed. The two HMMs were fitted to data by the Baum-Welch (BW) algorithm that maximizes the log-likelihood of the data Y given the model and its parameters ψ, that is, log p(Y |ψ) <ref type="bibr" target="#b16">[17]</ref>. The model is described in more detail in <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative Hidden Markov Models</head><p>In discriminative modeling we want to predict the relevance r = {I, R} of a sentence, given the observed eye movements Y . Formally, we optimize log p(r|Y, ψ) .</p><p>In speech recognition, where HMMs have been extensively used for decades, the current state-of-the-art HMMs are discriminative. The parameters of the discriminative HMM can be optimized with an Extended Baum-Welch (EBW) algorithm <ref type="bibr" target="#b14">[15]</ref>, which is a modification of the original BW.</p><p>Eye movements are modeled with a two-level discriminative HMM, where the first level models transitions between sentences whereas the second level models transitions between words within a sentence. The topology of the model is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>In our implementation, the first level Markov model has two states, each modeling one sentence class (I or R). Each state has the following exponential family emission distributions: (1) A multinomial distribution emitting the relevance of the line, r. This distribution is fixed; for each state one of the probabilities is one and the other is zero. (2) A Viterbi distribution emitting the probability of the sequence of words in a sentence. The Viterbi distribution is defined by the probability of a Viterbi path <ref type="bibr" target="#b23">[24]</ref> trough a two-state Markov model forming the second level in our model. The two states of the second level model emit the exponential observation distributions. The modeled eye movement features are described in Section 4.1.</p><p>When optimizing the model the most likely path through the second level model is sought by the Viterbi approxima-tion <ref type="bibr" target="#b23">[24]</ref>. The discriminative Extended Baum-Welch algorithm optimizes the full model, keeping the Viterbi path in the second level model fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collaborative Filtering Model</head><p>Collaborative filtering was carried out with a state-ofthe-art latent topic model, the User Rating Profile model (URP) <ref type="bibr" target="#b11">[12]</ref>. This model was used because in <ref type="bibr" target="#b11">[12]</ref> it outperformed several other latent topic models. It was originally optimized with variational Bayesian methods (variational URP ). We also implemented a potentially more accurate Markov Chain Monte Carlo (MCMC) integration method to compute the predictions from the model, using Gibbs sampling (Gibbs URP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Rating Profile Model</head><p>URP is a generative model which generates a binary rating r for a given (user, document) pair. <ref type="foot" target="#foot_0">1</ref> Our notation is summarized in Table <ref type="table" target="#tab_0">1</ref>. We estimate the posterior distribution P (r|u, d, D) by Gibbs sampling where D denotes the training data that consists of observations (u, d, r). The model assumes that there are a number of latent user groups whose preferences on the documents vary, and the users belong to these groups probabilistically. Alternatively, the groups can be interpreted as different "attitudes" of the user, and the attitude may be different for different documents.</p><p>The generative process proceeds according to the following steps (see also Figure <ref type="figure" target="#fig_2">2</ref>):</p><p>• For each user, a vector of multinomial parameters θ(u) is drawn from Dirichlet(α). <ref type="foot" target="#foot_1">2</ref> The parameter vector θ(u) contains the probabilities for the user to have different attitudes Z, i.e., to belong to different user groups Z.</p><p>• For each user u, a user group or attitude Z is drawn for each document d, from the user's Multinomial(θ(u)). The value of Z in effect selects the parameters β(Z, d) from the set of parameters in the node labeled by β in Figure <ref type="figure" target="#fig_2">2</ref>.  • For each pair (Z, d), a binary relevance value r is drawn from the Binomial(β(Z, d)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Other Latent Topic Models</head><p>In the URP model each user is assigned a distribution of multinomial parameters θ and the latent user group ("topic" in text modeling) Z is sampled repeatedly for each document. A user can therefore belong to many groups with varying degrees. In URP, the multinomial parameters θ are marginalized out from the maximum likelihood cost function. In the well-known latent topic model called Probabilistic Latent Semantic Analysis <ref type="bibr" target="#b4">[4]</ref>, the number of parameters grows with the number of since each user is given a fixed set of multinomial parameters θ. URP is closely related to Pritchard's latent variable model <ref type="bibr" target="#b15">[16]</ref> and Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">[2]</ref> (also known as multinomial PCA). URP can be seen as an extension to LDA with one extra dimension in the parameter matrix β to represent the possible different rating values. In our case we only have two values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating Gibbs URP</head><p>In Gibbs URP a five-fold cross-validation within the training set was first carried out to determine the optimal number of user groups in the range {1, 2, . . . , NU }. In our experiments the optimal number of user groups was found to be two, which was later used when computing the predictions for the final test set.</p><p>The duration of the burn-in period was determined by running three MCMC chains in parallel and monitoring the convergence of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dumb Model and Document Frequency Model</head><p>We introduced two simple models to give baseline results.</p><p>The dumb model classifies all documents to the largest class, P (r = 0) = 1. The document frequency model does not take into account differences between users or user groups. It simply models the probability of a document being relevant as the frequency of r = 1 in the training data for the document,</p><formula xml:id="formula_0">P (r = 1 | d) = P u #(u, d, r = 1) P u,r #(u, d, r) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Models</head><p>We started by examining the prediction performance of each of the models separately. Since the models use different sources of information, the natural extension is to combine their predictions.</p><p>Both models produce a probability of relevance for each given (user, document) pair. The simplest way to combine the models is to train the models independently and combine the predicted probabilities to produce the final prediction. This approach has the advantage of being modular and easily extensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative Dirichlet Mixture Model</head><p>We formulated a generative model for combining probabilities. Let us denote the prediction of the collaborative filtering model by Purp and the prediction of the eye movement model by Peye.</p><p>We first define a model that generates the observed relevances r ∈ {0, 1} and the (noisy) predictions Purp and Peye. Our goal is to find an expression for P (r|Purp, Peye, ϕ), where ϕ denotes all parameters of the model.</p><p>The generative process of the discriminative dirichlet mixture model is (see Figure <ref type="figure" target="#fig_3">3</ref>) as follows:</p><p>• For each (user, document) pair, a binary relevance r is drawn from Binomial(π).</p><p>• For each X ∈ {urp, eye}, a vector of multinomial (in this case binomial) parameters PX is drawn from Dirichlet(α r X ).</p><p>The observed variables of our model are the binary relevances rn and the prediction probabilities P X,u,d , where the indices u, d denote all (user, document) pairs. The parameters of the model are given by ϕ = {π, α r urp , α r eye }. We have ignored the priors of the parameters, since we assume the prior to be flat, i.e., P (r, PX |ϕ) = P (r, PX , ϕ), up to a normalization factor.</p><p>We optimize the model by maximum likelihood, and since the task is to predict relevances we build a discriminative model by maximizing the conditional log-likelihood of the relevances,</p><formula xml:id="formula_1">L = X u,d log P (r u,d |P urp,u,d , P eye,u,d , ϕ) .<label>(1)</label></formula><p>Values of the parameters ϕ can be found using standard optimization methods, for instance gradient ascent.</p><p>Besides giving predictions of relevance, the Dirichlet mixture reveals how useful the different sources of relevance information are relative to each other. Some of the feedback channels may produce smaller prediction probabilities PX than others for the observed relevances r. Some of the relevance feedback channels may additionally be noisy, that is, the prediction probabilities PX for a given relevance r have a large variance. After optimization, the mixture parameters α r X will contain information about magnitude and noisiness of the probability predictions. The magnitude of the prediction is contained in the relative magnitudes of the Dirichlet components. The information of the noisiness is contained in the sum of the Dirichlet parameters: if the sum of Dirichlet parameters is large, P i∈{0,1} α r Xi 1, the prediction probabilities PX have smaller variance, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Mixture Model</head><p>To serve as a baseline, we constructed a linear mixture model in which the final probability is a linear combination of the predictions of the various models (here of two models),</p><formula xml:id="formula_2">P (r|Purp, Peye, q) = qPurp(r)+(1 -q) Peye(r) , q ∈ [0, 1] .</formula><p>The parameter q is optimized by maximizing the conditional log-likelihood (Equation ( <ref type="formula" target="#formula_1">1</ref>)) using standard optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The test subjects were shown 80 pages, each containing titles of scientific articles. On each page the subject was instructed to choose the two most interesting titles in the order of preference.</p><p>The subjects participating in the experiment were researchers in vision research, artificial intelligence, and machine learning. The stimuli consisted of titles of scientific articles published during autumn 2004 in major journals in the fields of vision research (VR), artificial intelligence (AI), machine learning (ML), and general science (see Appendix A). On each page there was a randomly selected list of titles always containing two VR titles and one title from a general science journal. Half of the lists contained additionally one AI title and two ML titles, and half vice versa. Each list consisted of six titles, resulting in a total of 480 titles. The lists were shown in a randomized order to each subject, but the pages themselves were identical to all subjects.</p><p>Data was gathered in two different modalities. 22 of the subjects were asked to give their feedback explicitly via a web form, and three of the subjects participated in an eye movement experiment (Figure <ref type="figure" target="#fig_4">4</ref>). In this paper we refer to these subjects as the web-subjects and eye-subjects, respectively. The web-subjects were given the full publication information of the most interesting paper as a reward to encourage them to find the truly most interesting titles.</p><p>Three of the subjects (the eye-subjects) were shown the same stimuli in a controlled setting where eye movements were recorded. In the experiment, the subject was instructed in a similar manner to choose the two most interesting titles from the list of six titles (the eye movements were measured during this part), then press "enter" to proceed to another display, and finally to type in the numbers corresponding to the interesting titles. Hence both explicit ratings and eye movement trajectories were available for these subjects. Eye movements were measured with a Tobii 1750 eye tracker with a screen resolution of 1280x1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>Randomly chosen 21 of the lists (26 %) and the corresponding ratings from the eye-subjects formed a common test data set for all the models. Seven of the titles in the test set were not read by the eye-subjects. They were discarded, leaving us a total of 371 titles in the test set. Test data set was not touched before computing the final results.</p><p>Eye movement models were trained with the remaining feedback data from the three eye-subjects. <ref type="foot" target="#foot_2">3</ref> For training the URP model we used the eye-subjects' explicit ratings that were not included in the test data set, and all the explicit feedback data from the web-subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Movement Data</head><p>For the eye-subjects, nine of the measured lists had to be discarded from the data sets for technical reasons, thus leaving a set of 71 lists where both explicit and implicit feedback was available. The explicit ratings were, however, not discarded.</p><p>The raw eye movement data (consisting of x and y coordinates of the gaze direction, measured with a sampling rate of 50 Hz) was segmented into a sequence of fixations and saccades by a window-based algorithm (software from Tobii), with a 20-pixel window size and a minimum duration of 80 ms, used for defining fixations. An example of an eye movement trajectory in a case where relevance can easily be determined is shown in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>Feature extraction from the fixation-level data was then carried out as in <ref type="bibr" target="#b19">[20]</ref>. Each fixation was first assigned to the nearest word, which segmented the eye movement trajectory into words. The following features were then computed from the segmented data to be modeled with hidden Markov models: (1) One or many fixations within the word (modeled with a binomial distribution). ( <ref type="formula">2</ref>) Logarithm of the total fixation duration on the word (assumed to be Gaussian). (3) Reading behavior (multinomial): skip next word, go back to already read words, read next word, jump to an unread line, or the last fixation in an assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit Feedback Data</head><p>In the explicit feedback data all the selected titles were assumed to be relevant (r = 1) for the user, resulting in one third of all the ratings being "relevant." In other words, we did not model the users' preference order for the titles.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>For all the models we used perplexity and prediction accuracy in the test data set as measures of performance. Perplexity measures the probabilistic quality of the prediction, 4   perplexity = e -L N , where L = X Here ψ denotes the parameters of the model under evaluation, the sum is taken over the test set, and N is the size of the test set. We further computed the accuracy, that is, 4 The best possible performance yields perplexity = 1 and random guessing (coin flipping) yields perplexity = 2. If perplexity is greater than 2 the model is doing worse than random guessing. Theoretically, it can grow without a limit if the model predicts zero probability for some item in the test data set. However, we actully clipped the probabilities to the range [e -10 , 1] implying maximum perplexity of e 10 ≈ 22, 000 the fraction of the items in the test data set for which the prediction was correct for all the models, and the precision and recall measures. Precision is defined as the fraction of relevance predictions that were correct. Recall is defined as the fraction of relevant items correctly predicted. The results are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The discriminative HMM produced a reasonable, though rather noisy, prediction of the relevance<ref type="foot" target="#foot_3">foot_3</ref> . The difference in classification accuracy versus the dumb model was statistically significant (McNemar's test, P 0.01). The performances of the Document Frequency Model and URP cannot be directly compared to the HMM, since the HMM prediction is only based on the eye movements from the three eye-subjects, whereas the other models utilize the explicit feedback given by the 22 web-subjects. Consequently, it is not surprising that the URP outperforms the pure HMM in terms of perplexity and accuracy measures.</p><p>As expected, URP was able to distinguish two different user groups and provide a reasonable 83 % accuracy. The accuracies of the variational version and the Gibbs version were practically equal. However, the perplexity of the Gibbs URP is better. The reason is that the variational URP finds a maximum likelihood point estimate of the model parameters, whereas the Gibbs URP integrates properly over all model parameters, resulting in a more robust probability prediction. The difference of Gibbs URP to Document Frequency Model and HMM was tested by the Wilcoxon signed rank test, applied to the negative log-likelihoods given by the models for individual test samples. The differences were significant (P 0.01). The discriminative Dirichlet mixture model did succeed in combining the predictions of the different models. The difference of Dirichlet Mixture Model (HMM+Gibbs URP) to Gibbs URP was statistically significant (P 0.01) using the Wilcoxon signed rank test. The linear mixture of predictions performed poorly, placing all the weight on the prediction of the URP and ignoring the more noisy HMM al- Finally, we wish to point out that, for relatively small data sets, the classification accuracy is a noisy measure, as compared to perplexity. However, the difference between the accuracies of the Mixing Model (HMM+variational URP) and Gibbs URP is significant even for this noisy measure, at a moderate P -value of 0.04 (with McNemar's test). The difference between the accuracies of variational URP and the combination is not statistically significant (P = 0.06).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND DISCUSSION</head><p>We have set up a controlled experimental framework where the test subjects rated the relevance of titles of scientific articles. Eye movements were measured from a subset of the test subjects. The experimental setup was designed to resemble closely a real-world information retrieval scenario, where the user browses the output of, e.g., a web search engine in an attempt to find interesting documents. In our scenario a database of user preferences is combined with the measured implicit relevance feedback, resulting in more accurate relevance predictions. Collaborative filtering and implicit feedback can be used alone, or to complement standard textual content-based filtering.</p><p>We applied a discriminative time series model that produced a reasonable, though rather noisy, prediction of document relevance based on eye movement measurements. We also applied a probabilistic collaborative filtering model that produced a quite robust document relevance prediction. Thirdly, we introduced a probabilistic mixture model that can be used to combine the predictions. The mixture model clearly outperformed a simple linear method and was found necessary for making use of several information sources, the quality of which varied.</p><p>Our work provides the next step towards proactive information retrieval systems. The obvious extension is to incorporate the textual content of the documents to the models; in this work we do not utilize it at all. The second extension is to supplement or replace the eye movements by other sources of implicit feedback, such as measurements by biopotential sensors, e.g., from autonomic nervous system signals <ref type="bibr" target="#b8">[9]</ref> or electromyographic activity <ref type="bibr" target="#b12">[13]</ref>, and respective probabilistic models.</p><p>The models for inferring relevance could also be developed further. A good opportunity is provided by our Pascal EU Network of Excellence challenge (Inferring Relevance from Eye Movements, [6]), a competition where participants are invited to develop methods that best predict relevance from eye movement data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The topology of the discriminative hidden Markov model. The first level models transitions between sentences having relevance r ∈ {I, R} and the second level (within the boxes) models transitions between the words in a sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>For each (user group, document) pair (Z, d), a vector of binomial parameters β(Z, d) is drawn from Dirichlet(α β (Z, d)). The parameters β(Z, d) define the probability of the user group Z to consider document d relevant (or irrelevant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A graphical model representation of URP. The grey circle indicates an observed value. The boxes are "plates" representing replicates and the index at the bottom right corner of each plate indicates the number of replicates. The lowest plate, labeled with NU , represents users. The plate labeled with ND represents the repeated choice of user group and document. The plate labeled with KU represents the multinomial models of relevance for the different user groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A graphical model representation of the discriminative Dirichlet mixture model. X is the index of the model that predicts relevance, in our case X ∈ {eye, urp}. The grey circles indicate observed values. In our model we observe triplets (r, Peye, Purp) for each user-document pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A total of 25 test subjects were shown 80 lists, each with six article titles. On each page the subjects chose the two most interesting titles. 22 of the subjects were asked to give their feedback explicitly via web forms (sample shown on the left). Eye movements of three subjects were measured with a Tobii 1750 eye tracker (right).</figDesc><graphic coords="7,314.07,53.80,225.95,169.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A reconstruction of the eye movement trajectory of a test subject during one of the assignments. Fixations are denoted by circles. The relevant sentences (R) are on lines 1 and 4. The trajectories naturally varied across the users and experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>log P (r u,d | ψ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>: Notation</cell></row><row><cell cols="2">Symbol Description</cell></row><row><cell>u</cell><cell>user index</cell></row><row><cell>d</cell><cell>document index</cell></row><row><cell>r</cell><cell>binary relevance</cell></row><row><cell>Z</cell><cell>user group index</cell></row><row><cell>NU</cell><cell>number of users</cell></row><row><cell>ND</cell><cell>number of documents</cell></row><row><cell>KU</cell><cell>number of user groups</cell></row><row><cell>Purp</cell><cell>relevance prediction of the URP</cell></row><row><cell>Peye</cell><cell>relevance prediction of the HMM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results. Small perplexity and large accuracy, precision, and recall are better. The differences of Dirichlet Mixture Model (HMM+Gibbs URP) to Gibbs URP, as well as Gibbs URP to Document Frequency Model and HMM were tested statistically and found significant (Wilcoxon signed rank test).</figDesc><table><row><cell>Model</cell><cell cols="4">Perplexity Accuracy (%) Precision (%) Recall (%)</cell></row><row><cell>Dumb Model</cell><cell>-</cell><cell>66.6</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Document Frequency Model</cell><cell>1.80</cell><cell>69.1</cell><cell>55.8</cell><cell>35.0</cell></row><row><cell>Linear Mixture (HMM+Gibbs URP)</cell><cell>1.50</cell><cell>83.0</cell><cell>76.8</cell><cell>69.9</cell></row><row><cell>HMM (eye movements)</cell><cell>1.78</cell><cell>73.3</cell><cell>70.0</cell><cell>34.1</cell></row><row><cell>Variational URP (collaborative filtering)</cell><cell>1.62</cell><cell>83.3</cell><cell>77.5</cell><cell>69.9</cell></row><row><cell>Gibbs URP (collaborative filtering)</cell><cell>1.50</cell><cell>83.0</cell><cell>76.8</cell><cell>69.9</cell></row><row><cell>Dirichlet Mixture (HMM+Var. URP)</cell><cell>1.56</cell><cell>85.7</cell><cell>83.7</cell><cell>70.7</cell></row><row><cell>Dirichlet Mixture (HMM+Gibbs URP)</cell><cell>1.48</cell><cell>85.2</cell><cell>81.5</cell><cell>71.5</cell></row><row><cell cols="2">together. The precision of the best mixture (83.7 %) is quite</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">good, taking into account that the content of documents was</cell><cell></cell><cell></cell><cell></cell></row><row><cell>not modeled in any way.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the model allows also multiple-valued ratings if the binomial is replaced with a multinomial.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We denote distributions with capitalized words followed by their parameters in parentheses.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The number of eye-subjects was chosen to be three for practical reasons. Three subjects were sufficient to train the HMM, to form the test set, and to obtain a statistically significant result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The performance of simple two-state HMMs optimized for each class was similar to discriminative HMM.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>The authors would like to thank the people at <rs type="institution">Tampere Unit for Computer-Human Interaction</rs>, mainly <rs type="person">Aulikki Hyrskykari</rs>, <rs type="person">Päivi Majaranta</rs>, and <rs type="person">Kari-Jouko Räihä</rs> for useful discussions and for providing us with the measurement time for the eye movement experiments. We would also like to thank all the persons that (more or less) voluntarily took part in our experiment.</p><p>This work was supported by the <rs type="funder">Academy of Finland</rs>, decision no. <rs type="grantNumber">79017</rs>, and by the <rs type="programName">IST Programme</rs> of the European Community, under the <rs type="programName">PASCAL Network of Excellence</rs>, <rs type="grantNumber">IST-2002-506778</rs>. This publication only reflects the authors' views. The authors acknowledge that access rights to the data sets and other materials produced in the <rs type="projectName">PRIMA</rs> project are restricted due to other commitments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jcNAE7s">
					<idno type="grant-number">79017</idno>
					<orgName type="program" subtype="full">IST Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_EWvKCXQ">
					<idno type="grant-number">IST-2002-506778</idno>
					<orgName type="project" subtype="full">PRIMA</orgName>
					<orgName type="program" subtype="full">PASCAL Network of Excellence</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. LIST OF JOURNALS</head><p>-Letters to Nature -Science</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unifying collaborative and content-based filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Basilico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML&apos;04, Twenty-first International Conference on Machine Learning</title>
		<meeting>ICML&apos;04, Twenty-first International Conference on Machine Learning<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational extensions to EM and multinomial PCA</title>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML&apos;02, 13th European Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Elomaa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</editor>
		<meeting>ECML&apos;02, 13th European Conference on Machine Learning<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent semantic models for collaborative filtering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="89" to="115" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proactive response to eye movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyrskykari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Räihä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERACT&apos;03</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">W M</forename><surname>Rauterberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Menozzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Wesson</surname></persName>
		</editor>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit feedback for inferring user preference: a bibliography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grouplens: Applying collaborative filtering to usenet news</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maui: a multimodal affective user interface</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nasoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MULTIMEDIA&apos;02: Proceedings of the tenth ACM international conference on Multimedia</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Suitor: an attentive information system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Intelligent User Interfaces</title>
		<meeting>the 5th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentive agents</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="47" to="51" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling user rating profiles for collaborative filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person-independent estimation of emotional experiences from facial expressions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Partala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Surakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vanhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUI&apos;05: Proceedings of the 10th International Conference on Intelligent User Interfaces</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI-2001</title>
		<meeting>UAI-2001</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative map for acoustic model adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="312" to="315" />
		</imprint>
	</monogr>
	<note>Proceedings. (ICASSP&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inference of population structure using multilocus genotype data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="945" to="959" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="372" to="422" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can relevance be inferred from eye movements in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSOM&apos;03, Workshop on Self-Organizing Maps</title>
		<meeting>WSOM&apos;03, Workshop on Self-Organizing Maps<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="261" to="266" />
		</imprint>
		<respStmt>
			<orgName>Kyushu Institute of Technology, Kitakyushu</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relevance feedback from eye movements for proactive information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salojärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Processing Sensory Information for Proactive Systems (PSIPS 2004)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Silvén</surname></persName>
		</editor>
		<meeting><address><addrLine>Oulu, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated eye-movement protocol analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Salvucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="39" to="86" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Social information filtering: Algorithms for automating &apos;word of mouth</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shardanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Human Interaction</title>
		<meeting>Computer Human Interaction</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A gaze-responsive self-disclosing display</title>
		<author>
			<persName><forename type="first">I</forename><surname>Starker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast hands-free writing by gaze direction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">418</biblScope>
			<biblScope unit="page">838</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
