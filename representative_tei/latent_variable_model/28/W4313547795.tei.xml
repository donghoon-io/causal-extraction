<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-04-01">1 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liwen</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peixi</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><roleName>IEEE</roleName><forename type="first">Yonghong</forename><forename type="middle">Tian</forename><surname>Fellow</surname></persName>
						</author>
						<title level="a" type="main">MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-01">1 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.00746v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Traffic Signal Control</term>
					<term>Reinforcement Learning</term>
					<term>Meta Reinforcement Learning</term>
					<term>Variational Autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. On the one hand, the policy of the current traffic signal is often heavily influenced by its neighbor agents, and the coordination between the agent and its neighbors needs to be considered. Hence, the control of a road network composed of multiple traffic signals is naturally modeled as a multi-agent system, and all agents' policies need to be optimized simultaneously. On the other hand, once the policy function is conditioned on not only the current agent's observation but also the neighbors', the policy function would be closely related to the training scenario and cause poor generalizability because the agents in various scenarios often have heterogeneous neighbors. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. Then, a learned latent variable is introduced to represent the task's specific information and is further brought into the policy for learning. In addition, to make the policy learning stable, a novel intrinsic reward is designed to encourage each agent's received rewards and observation transition to be predictable only conditioned on its own history. Extensive experiments conducted on CityFlow demonstrate that the proposed method substantially outperforms existing approaches and shows superior generalizability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T RAFFIC signals that direct traffic movements play an important role in efficient transportation. Most conventional methods aim to control traffic signals by fixed-time plans <ref type="bibr" target="#b0">[1]</ref> or hand-crafted heuristics <ref type="bibr" target="#b1">[2]</ref>. However, the predefined rules cannot adapt to dynamic and uncertain traffic conditions. Recently, deep reinforcement learning (RL) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> has been applied in Intelligent Transportation Systems (ITS) and demonstrated promising performance. Specifically, recent works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> use RL for traffic signal control, and provide a promising way to handle dynamic and uncertain traffic conditions, where a RL agent employs a deep neural network to control an intersection and the network is learned by directly interacting with the environment.</p><p>The most straightforward RL baseline considers each</p><p>• Corresponding authors: Peixi Peng and Yonghong Tian. intersection independently and models the task as a single agent RL problem <ref type="bibr" target="#b11">[12]</ref>. However, the observation, received reward and dynamics of each traffic signal are closely related to its neighbors, and the coordination between signals should be modeled. Hence, optimizing traffic signal control in a large-scale road network could be modeled as a multiagent reinforcement learning (MARL) problem. Prior works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> in this domain resort to centralized training to ensure that agents learn to coordinate, they demonstrate that communication among agents could help coordination. However, as the joint action space grows exponentially with the number of agents, it is infeasible or costly in realistic deployment, and training emergent communication protocols also remains a challenging problem. In addition, once the policy function is conditioned on not only the current agent's observation but also the neighbors', the policy function would be closely related to the training scenario and cause poor generalizability because the agents in various scenarios often have heterogeneous neighbors. Therefore, learning decentralized policies opens up a window of new opportunities for advanced MARL research in this area.</p><p>To learn effective decentralized policies, there are two main challenges. Firstly, it is impractical to learn an individual policy for each intersection in a city or a district containing thousands of intersections. Parameter sharing may help. However, each intersection has a different traffic pattern, and a simple shared policy hardly learns and acts optimally at all intersections. To handle this challenge, we formulate the policy learning in a road network as a meta-learning problem, where traffic signal control at each intersection corresponds to a task, and a policy is learned to adapt to various tasks. Reward function and state transition of these tasks vary but share similarities since they follow the same traffic rules and have similar optimization goals. Therefore, we represent each task as a learned and lowdimensional latent variable obtained by encoding the past trajectory in each task. The latent variable is a part of the input of the policy, which captures task-specific information and helps improve the policy adaption.</p><p>Secondly, even for a specific task, the received rewards and observations are uncertain to the agent, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, which make the policy learning unstable and nonconvergent. Even if the agent performs the same action on the same observation at different timesteps, the agent may receive different rewards and observation transitions because of neighbor agents' different actions. In this case, the received rewards and observation transitions of the current agent could not be well predicted only conditioned on its own or partial neighbors' observations and performed actions. To avoid this situation, four decoders are introduced to predict the next observations and rewards without neighbor agents' policies or with partially neighbor agents, respectively. In addition, an intrinsic reward is designed to reduce the bias among different predictions and enhance learning stability. In other words, the design of the decoders and intrinsic reward is similar to the law of contra-positive. The unstable learning will cause the predicted rewards and observation transitions unstable in a decentralized way, while our decoders and intrinsic reward encourage the prediction convergent. In addition, from the perspective of information theory, the intrinsic reward design makes the policy of each agent robust to neighbours' polices, which could make the learned policy easy to transfer.</p><p>Intrinsic motivation refers to reward functions that allow agents to learn useful behaviors across various tasks. Previous approaches to intrinsic motivation often focus on curiosity <ref type="bibr" target="#b16">[17]</ref>, imagination <ref type="bibr" target="#b17">[18]</ref> or synergy <ref type="bibr" target="#b18">[19]</ref>, these approaches rely on hand-crafted rewards specific to the environment, or limit to the bimanual manipulation tasks. Such structural constraints make it impossible to achieve independent training of MARL agents across multiple environments. Here, we consider the problem of deriving intrinsic motivation as an exploration bias from other agents in MARL.</p><p>Our key idea is that a good guiding principle for intrinsic motivation is to make approximations robust to a dynamic environment. Intrinsic motivation is assessed using comparison reasoning: at each timestep, an agent measures the effect of another agent's policy on its model's predictions. Neighbor's policies that lead to a relatively higher change in an agent's own model estimates are considered highly influential, and because the explicit effect of neighbor's influence in this task is to increase incoming traffic volumes, which exacerbates the agent's own traffic load. Therefore, we minimize this influence bias to make the model's estimations free from interference from others. We show that this inductive bias will drive agents to learn effective decentralized policies.</p><p>We conduct extensive experiments on CityFlow <ref type="bibr" target="#b19">[20]</ref> in public datasets Hangzhou (China), Jinan (China), New York (USA), and our derived dataset Shenzhen (China) road networks under various traffic patterns, and empirically demonstrate that our proposed method can achieve state-ofthe-art performances over the above scenarios. Furthermore, our method shows superior adaptivity in transfer experiments. In summary, the main contributions of this paper are concluded as three aspects:</p><p>1) The traffic signal control is modeled as a meta-learning problem over a set of related tasks, and a novel method MetaVIM is proposed to learn decentralized policy to handle large-scale traffic lights.</p><p>2) A learnable latent variable is introduced to represent task-specific information by performing approximate inference on the task, and make the policy function shareable across the tasks.</p><p>3) A novel intrinsic reward is designed to tackle the challenge of unstable policy learning in dynamically changing traffic environments. We show that this design will drive agents to learn effective decentralized policies.</p><p>The paper is structured as follows. We describe the related work in Section 2, and the MARL setting in Section 3. Section 4 introduces the details of the proposed method. Section 5 presents experimental results that empirically demonstrate the efficacy of MetaVIM. Finally, conclusions and future work are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional and Adaptive Traffic Signal Control</head><p>Most conventional traffic signal control methods are designed based on fixed-time signal control <ref type="bibr" target="#b20">[21]</ref>, actuated control <ref type="bibr" target="#b21">[22]</ref> or self-organizing traffic signal control <ref type="bibr" target="#b22">[23]</ref>. These approaches rely on expert knowledge and often perform unsatisfactorily in complicated real-world situations. To solve this problem, several optimization-based methods have been proposed to optimize average travel time, throughput, etc., which decide the traffic signal plans according to the dynamical observed data. Specifically, the method <ref type="bibr" target="#b1">[2]</ref> calculates the difference between the number of upstream and downstream vehicles and then sorts the values to determine the phase order. Besides, several methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> consider the scheduling of urban traffic light as the model-based optimization problem and employ search based methods to solve it. Furthermore, the dynamic programming <ref type="bibr" target="#b27">[28]</ref>, mixed-integer linear programming <ref type="bibr" target="#b28">[29]</ref> and non-linear programming models <ref type="bibr" target="#b29">[30]</ref> are also used. Please see <ref type="bibr" target="#b30">[31]</ref> for more details. Although effective in some situations, these approaches typically rely on prior assumptions which may fail in some cases, or require environment model which is unavailable and unreliable in complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RL-based Traffic Signal Control</head><p>RL-based traffic signal control methods aim to learn the policy from interactions with the environment. Earlier studies use tabular Q-learning <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> where the states are required to be discretized and low-dimensional. To handle more complex continuous state, recent advances employ deep RL to map the high-dimensional state representations (such as images or feature vectors) into actions.</p><p>Efforts have been made to design strategies that formulate the task as a single agent <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> or some isolated intersections <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> , i.e., each agent makes decision for its own. This type of methods is usually easy to scale, but may have difficulty to achieve global optimal performance due to the lack of collaboration. To address the problem, another way is to jointly model the action among learning agents with centralized optimization <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, as the number of agents increases, joint optimization usually leads to dimensional explosion, which has inhibited the widespread adoption of such methods to a large-scale traffic signal control. To overcome the difficulty, another type of methods are implemented in a decentralized manner. For example, the methods proposed in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b43">[44]</ref> directly add neighboring information into states, and the neighbors' hidden features are merged into states in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Compared with them, our method uses neighbor information to form intrinsic motivation rather than as additional input of the policy. It makes our method easy to transfer to a new scenario which may have different neighbour topology with the training scenario. Besides, the neighborhood travel time is optimized in <ref type="bibr" target="#b47">[48]</ref> as an additional reward. However, simple concatenation of neighboring information is not reasonable enough because the influence of neighboring intersections is not balanced.</p><p>To make the policy transferable, traffic signal control is also modeled as a meta-learning problem in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Specifically, the method in <ref type="bibr" target="#b13">[14]</ref> performs meta-learning on multiple independent MDPs and ignores the influences of neighbor agents. A data augmentation method is proposed in <ref type="bibr" target="#b48">[49]</ref> to generates diverse traffic flows to enhance meta-RL, and also regards agents as independent individuals, without explicitly considering neighbors. In addition, a model-based RL method is proposed in <ref type="bibr" target="#b35">[36]</ref> for high data efficiency. However it may introduce cumulative errors due to error of the learned environment model and it is hard to achieve the asymptotic performance of model-free methods. Our method both belongs to meta-RL paradigms, the main advantages are two main aspects Firstly, we consider the neighbour information during the meta-learning, which is critical for the multi-agent coordination. Secondly, our method learns a latent variable to represent task-specific information, which can not only balance exploration and exploitation <ref type="bibr" target="#b49">[50]</ref>, but also help to learn the shared structures of reward and transition across tasks. As far as we know, our work is the first to propose an intrinsic motivation to enhance the robustness of the policy on traffic signal control. See Appendix F for a brief overview of the above methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Intrinsic Reward</head><p>Intrinsic motivation methods have been widely studied in the literature, such as handling the difficult-to-learn dilemma in sparse reward environments <ref type="bibr" target="#b50">[51]</ref> or trading off the exploration and exploitation in non-sparse reward scenarios <ref type="bibr" target="#b49">[50]</ref>. Most of the intrinsic reward approaches can be classified into two classes. The first class is countedbased paradigm, where agents are incentivized to reach infrequently visited states by maintaining state visitation counts <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> or density estimators <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. However, this paradigm is challenging in continuous or high-dimensional state space. The second is curiosity-based paradigm, in which agents are rewarded for high prediction error in a learned reward <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b55">[56]</ref> or inverse dynamics model <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b56">[57]</ref>. The uncertainty of the agent's assessment of its behavior can be measured as a curiosity for environmental exploration.</p><p>Besides the above two classes, other intrinsic reward methods are mainly task-oriented and for a specific purpose. For example, the method in <ref type="bibr" target="#b18">[19]</ref> uses the discrepancy between the marginal policy and the conditional policy as the intrinsic reward for encouraging agents to have a greater social impact on others. The errors between the joint cooperative behaviors and the individual actions are defined in <ref type="bibr" target="#b57">[58]</ref> as an intrinsic reward, which is suitable for agentpair tasks that rely heavily on collaboration, such as dualarm robot tasks. Similar with them, the proposed intrinsic reward is specially designed for the traffic signal control task, and we adopt the discrepancy with/without neighbor agent's policies as a motivation for decentralized control in dynamically changing multi-agent scenarios, making the policy robust to the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Latent Variable in RL</head><p>A number of prior works have explored how RL can be cast in the framework of variational inference. Latent variable could transform the dynamically updated task-related information such as trajectories into a continuous lower-dimensional space. For example, <ref type="bibr" target="#b58">[59]</ref> shows that exploring in latent space can enhance the representation of the environment. In meta-RL, the agent is not given prepared task-specific data to adapt to, and it must fully explore the environment to collect useful information. Privileged information generated during the learning phase can be used during meta-training to guide exploration, such as expert trajectories <ref type="bibr" target="#b59">[60]</ref>, dense reward for meta-training but not testing <ref type="bibr" target="#b60">[61]</ref>, or ground-truth task IDs / descriptions <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Similar work <ref type="bibr" target="#b63">[64]</ref> introduces temporally-extended exploration with latent variable. A series of methods <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b60">[61]</ref> use variational auto-encoders (VAE) <ref type="bibr" target="#b64">[65]</ref> structure to help explore the environment. A branch of context-based methods automatically learns to trade-off exploration and exploitation by maximizing average adaptation performance <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b65">[66]</ref>. Differently, we learn a dynamical latent variable for each task to present taskspecific information and indicate the correspond agent's belief.      </p><formula xml:id="formula_0">U i 3 A 2 q 4 F I o 3 U K D k 7 V h z G g a S t 4 L x z d R v P X F t R K T u c R J z P 6 R D J Q a C U b T S g 3 x M h c p 6 a T X r l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 i C U h V 8 g k N a b j u T H 6 K d U o m O R Z s Z s Y H l M 2 p k P e s V T R k B s / n V 2 c k V O r 9 M k g 0 v Y p J D P 1 9 0 Z K Q 2 M m Y W A n Q 4 o j s + h N x f + 8 T o K D a 9 9 m i h P k i s 0 / G i S S Y E S m 8 U l f a M 5 Q T i y h T A t 7 K 2 E j q i l D W 1 L R l u A t R</formula><formula xml:id="formula_1">W p E k r 3 Q m K Y 4 J K 1 g Y N g v U Q z E o e C d c P J 7 c z v P j F t u J I P M E 1 Y E J O R 5 B G n B K z k i 8 d M p Z A P s k Y + q N b c u j s H X i V e Q W q o Q G t Q / e o P F U 1 j J o E K Y o z v u Q k E G d H A q W B</formula><formula xml:id="formula_2">W p E k r 3 Q m K Y 4 J K 1 g Y N g v U Q z E o e C d c P J 7 c z v P j F t u J I P M E 1 Y E J O R 5 B G n B K z k i 8 d M p Z A P s k Y + q N b c u j s H X i V e Q W q o Q G t Q / e o P F U 1 j J o E K Y o z v u Q k E G d H A q W B 5 p Z 8 a l h A 6 I S P m W y p J z E y Q z U / O 8 Z l V h j h S 2 p Y E P F d / T 2 Q k N m Y a h 7 Y z J j A 2 y 9 5 M / M / z U 4 h u g o z L J A U m 6 W J R l A o M C s / + x 0 O u G Q U x t Y R Q z e 2 t m I 6 J J h R s S h U b g r f 8 8 i r p X N a 9 6 7 p 3 f 1 V r n h d x l N E J O k U X y E M N 1 E R 3 q I X a i C K F n t E</formula><formula xml:id="formula_3">W p E k r 3 Q m K Y 4 J K 1 g Y N g v U Q z E o e C d c P J 7 c z v P j F t u J I P M E 1 Y E J O R 5 B G n B K z k i 8 d M p Z A P s k Y + q N b c u j s H X i V e Q W q o Q G t Q / e o P F U 1 j J o E K Y o z v u Q k E G d H A q W B 5 p Z 8 a l h A 6 I S P m W y p J z E y Q z U / O 8 Z l V h j h S 2 p Y E P F d / T 2 Q k N m Y a h 7 Y z J j A 2 y 9 5 M / M / z U 4 g a Q c Z l k g K T d L E o S g U G h W f / 4 y H X j I K Y W k K o 5 v Z W T M d E E w o 2 p Y o N w V t + e Z V</formula><formula xml:id="formula_4">X i Z B Q a q o Q K N f + e o N F L U J k 0 A F M a Y b + C m E G d H A q W B 5 u W c N S w k d k y H r O i p J w k y Y z W 7 O 8 a l T B j h W 2 p U E P F N / T 2 Q k M W a S R K 4 z I T A y i 9 5 U / M / r W o h v w o z L 1 A K T d L 4 o t g K D w t M A 8 I B r R k F M H C F U c 3 c r p i O i C Q U X U 9 m F E C y + v E x a F 7 X g q h b c X 1 b r Z 0 U c J X S M T t A 5 C t A 1 q q M 7 1 E B N R F G K n t E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM STATEMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>In this paper, we investigate traffic signal control of multiintersection as illustrated in Fig. <ref type="figure" target="#fig_3">6</ref>. In order to explain the basic concepts, we use a 4-way intersection as an example, depicted in Fig. <ref type="figure">2</ref>. Note that the concepts are easily generalized to different intersection structures (e.g., different numbers of entering approaches or phases). Definition 1 (Incoming/Outgoing Lanes) For an intersection, the incoming lanes refer to the lanes where the vehicles are about to enter the intersection. In real world, most intersections are equipped with 4-way entering approaches, but some are 3-way or 5-way intersections. A standard 4way intersection is shown in Fig. <ref type="figure">2</ref>, which consists of four approaches, i.e., "east", "south", "west" and "north". Each approach consists of three types of lanes, representing "leftturn", "straight" and "right-turn" directions from inner to outer. The outgoing lanes refer to the lanes where the vehicles are about to leave the intersection. Notes that vehicles on the incoming lanes are affected directly by the traffic signal at the current intersection. Therefore, we adopt the traffic information on the incoming lanes as part of the observation, which is the same as most existing works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>Definition 2 (Phase) Phase is a controller timing unit associated with the control of one or more movements, representing the permutation and combination of different traffic flows. At each phase, vehicles in the specific lanes can continue to drive. The 4-phase setting is the most common configuration in reality, but the number of phases can vary due to different intersection topologies (3-way, 5way intersections, etc.). Fig. <ref type="figure">2</ref> illustrates a standard 4-phase setting: "north-south-straight", "north-south-left", "east-weststraight" and "east-west-left", "north-south-straight" means that the signal on the corresponding lanes are green. Note that the signal on the right-turn lanes is always green for consistency with real world.</p><p>Definition 3 (Average Travel Time) The travel time of a vehicle is the time discrepancy between entering and leaving a particular area. A vehicle from the origin to the destination (OD) is regarded as a travel. Average travel time of all vehicles in a road network is the most frequently used measure to evaluate the performance of traffic signal control <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>The traffic signal control in a road network is modeled as a multi-agent system, where each signal is controlled by an agent. Before formulating the problem, we firstly design the learning paradigm by analyzing the characteristics of the traffic signal control (TSC). Due to the coordination among different signals, the most direct paradigm may be centralized learning. However, the global state information in TSC is not only highly redundant and difficult to obtain in realistic deployment, but also likely suffers from dimensional explosion. Moreover, once the policy function relies on the global state information or neighbors on execution, it is hard to transfer the policy from the training scenario to other unseen scenarios containing different road networks. Hence, it is natural to resort to the decentralized policy, which controls each signal only conditioned on its own history. However, the fully decentralized learning ignores the coordination. If agents are behaved independently, agents maximize their own rewards and may sacrifice the interests of others, it is difficult for the entire system to reach the optimum. Therefore, we model the task as Decentralized Partially Observable Markov Decision Process (Dec-POMDP) <ref type="bibr" target="#b66">[67]</ref>. The neighbors' information is considered, all agents' policies are optimized synchronously in training, while only the agent's observation history is used in the execution.</p><p>Specifically, a Dec-POMDP could be denoted by a tuple G =&lt; S, N , A, O, Z, P, R, H, γ &gt;. Each intersection in the scenario is controlled by an agent. S is the state space, and s t ∈ S denotes the state at time step t. Since the environment is partially observed, each agent i only has access to its local observation o i,t ∈ O that is acquired through an observation function Z(s) : S → O. At current time, all agents perform joint action a = {a i , . . . , a N }, a i ∈ A and cause the state transition dynamics P (s t+1 |s t , a) := S × A N → S where s t+1 is the next state. The observation-action history of agent i at time t is denoted as τ i,:t . R = {R i } N i=1 is the reward for each agent. As stated in Sec. 3.3, the reward is calculated by the partial observation (queue length) and the observation transition may be unstable in a multi-agent system. That is, even if the agent performs the same action on the same observation at different timesteps, the agent may receive different observation transitions because neighbor agents may perform different actions. Hence, we define the reward function of each agent as r i,t = R i (o i,t , a i , o i,t+1 ). Our goal is to learn the decentralized policy π i (a i | o i,t ) to maximize its cumulative rewards: t γ t r i,t where γ is the discounted factor. In the traffic signal task, the next observation of each agent not only relies on current agent's observation and performed action, but also is associated with neighbors' actions. Therefore, there exists an observation transition function T i o i,t+1 | o i,t , a i , a -i for each agent i where a -i is the joint action of neighbors.</p><p>Based on the above formulation, there are two key issues that need to be considered:</p><p>• Since training policy for each realistic scenario is timeconsuming even impossible, hence our goal is to learn the decentralized policy π i (a i | o i,t ) from the given training scenario (i.e., a road network), which can be generalized to unseen scenarios. Thus the meta-learning framework is employed where the policy learning of each agent corresponds to a task. The reward function R i and observation transition T i of these tasks vary but also share similarities since they follow the same traffic rules and have similar optimization goals. Therefore, we represent each task as a learned and low dimensional latent variable m i . By incorporating m i , we assume the reward, observation transition and policy functions could be shareable across tasks:</p><formula xml:id="formula_5">T i o i,t+1 | o i,t , a i , a -i ≈ T o i,t+1 | o i,t , a i , a -i , m i , R i (o i,t , a i , o i,t+1 ) ≈ R (o i,t , a i , o i,t+1 , m i ) , π i (a i | o i,t ) ≈ π (a i | o i,t , m i ) ,<label>(1)</label></formula><p>which make the meta-learning possible. • Since R i not only rely on o i,t and a i but also o i,t+1</p><p>which is generated by T i and related with a -i , hence learning π i from R i may cause learning non-stationary because the agent may receive different rewards and observation transitions for the same action at the same observation. In this case, the received rewards and observation transitions of the current agent could not be well predicted only conditioned on its own observations and performed actions. Conversely, to avoid suffering such non-stationary, we hope the learned decentralized policy could make the observation transition and reward predictable. That is, based on the learned π(a i |o i,t ), there exists functions T ′ i and R ′ i satisfy that:</p><formula xml:id="formula_6">T ′ i (o i,t+1 | o i,t , a i ) ≈ T i o i,t+1 | o i,t , a i , a -i , R ′ i (o i,t+1 , o i,t , a i ) ≈ R i o i,t+1 , o i,t , a i , a -i , where a i ∼ π(a i |o i,t ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Agent Design</head><p>Each intersection in the system is controlled by an agent, and here we present the detailed definitions of the agent.</p><p>• Observation. Each agent has its own local observation, including the number of vehicles on each incoming lane and the current phase of the intersection, where phase is the part of the signal cycle allocated to any combination of traffic movements, as explained in Section 3.1. Observation of agent i is defined by</p><formula xml:id="formula_7">o i,t = {V 1 , V 2 , . . . , V M , p},<label>(3)</label></formula><p>where M is the total number of incoming lanes and V M means the number of vehicles in the M th incoming lanes, p is the current phase and represented as a one-hot vector. • Action. At time t, each agent i chooses a phase p as its action a i , indicating the traffic signal should be set to phase p. Note that the phases may organize in a sequential way in reality, while directly selecting a phase makes the traffic control plan more flexible. In a grid road network, each agent has four phases as described in Section 3.1. For a complex and heterogeneous road network, we unite the phases of all structures together as the full action space, and mask the unavailable phases of each traffic signal as unavailable actions of the agent. • Reward. We define the reward for agent i as the negative of the queue length on incoming lanes. Note that optimizing queue length has been proved to be equivalent to optimizing average travel time in <ref type="bibr" target="#b37">[38]</ref> under certain assumptions. Average travel time is a global criteria which cannot be optimized directly by an agent. Hence, queue length is widely used as reward in traffic signal control. Reward of agent i is defined by</p><formula xml:id="formula_8">r i = - M m q m ,<label>(4)</label></formula><p>where q m is the queue length on the incoming lane m, and M is the total number of incoming lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>In this section, we propose Meta Variationally Intrinsic Motivated (MetaVIM) method to achieve Eq. 1 and Eq. 2, as illustrated in Fig. <ref type="figure">3</ref>. MetaVIM employs latent variable to represent each task to make the reward, observation transition and policy functions shareable. At the same time, MetaVIM makes the approximations and policy robust to neighbors by intrinsic reward design. From now on, we drop the sub-and superscript i to denote the current agent for ease of notation. We start by describing the overall architecture in Sec. 4.1, and then elaborate the policies with latent variable in Sec. 4.2 and intrinsic reward design in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>MetaVIM consists of a multi-head VAE (mVAE) and a policy network, where the mVAE consists of an encoder and four decoders:</p><p>• Encoder. The encoder takes the past trajectories τ :t up until t as input to calculate the latent variable m in Eq. 1, parameterised by ψ. The encoder could be:</p><formula xml:id="formula_9">e ψ (m | τ :t ) .<label>(5)</label></formula><p>• Decoder. Four decoders are used to predict the environmental dynamics, and parameterised by ϕ r , ϕ r, ϕ o , ϕ õ respectively, where the former two are used to predict next reward with/without neighbors and the latter two are used to predict next observation with/without neighbors:</p><formula xml:id="formula_10">p ϕr (r t+1 | o t+1 , o t , a, m) ,<label>(6)</label></formula><formula xml:id="formula_11">p ϕo (o t+1 | o t , a, m) ,<label>(7)</label></formula><formula xml:id="formula_12">p ϕr (r t+1 | o t+1 , o t , a, a j , m) ,<label>(8)</label></formula><formula xml:id="formula_13">p ϕõ (õ t+1 | o t , a, a j , m) , (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where j is any neighbor agent of current intersection, a j is the neighbor action, and m is the latent variable generated by the encoder. • Policy. The policy takes local observations o t and latent variable as input, parameterised by θ and dependent on ψ, which is defined by</p><formula xml:id="formula_15">π θ (a | o t , m) , (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where m is derived by encoder. The policy is conditioned on both observation and posterior over m, which is similar to the formulation of Bayesian RL <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. </p><formula xml:id="formula_17">n 3 J x C s P 6 F o E r 8 Y l + R 3 c P y J b y 0 = " &gt; A A A B 6 3 i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 G p I 6 O u 2 u 4 M Z l B f u A d i i Z N G 1 D M 5 k h y Q h l 6 C + 4 c a G I W 3 / I n X 9 j p q 2 g o g c u H M 6 5 l 3 v v C R P B t U H o w y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t o 5 T R V m L x i J W 3 Z B o J r h k L c O N Y N 1 E M R K F g n X C 6 X X u d + 6 Z 0 j y W d 2 a W s C A i Y 8 l H n B K T S / 1 E 8 0 G 5 g l w P + z 7 y I X I v U B 3 X q 5 Y g X L 3 E H s Q u W q A C V m g O y u / 9 Y U z T i E l D B d G 6 h 1 F i g o w o w 6 l g 8 1 I / 1 S w h d E r G r G e p J B H T Q b a 4 d Q 7 P r D K E o 1 j Z k g Y u 1 O 8 T G Y m 0 n k W h 7 Y y I m e j f X i 7 + 5 f V S M 6 o F G Z d J a p i k y 0 W j V E A T w / x x O O S K U S N m l h C q u L 0 V 0 g l R h B o b T 8 m G 8 P U p / J + 0 q y 6 + c r 1 b r 9 K o r e I o g h N w C s 4 B B j 5 o g B v Q B C 1 A w Q Q 8 g C f w 7 E T O o / P i v C 5 b C 8 5 q 5 h j 8 g P P 2 C X + 1 j o g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V C d K F 2 U D 4 5 M r + j Q y i P j H d b 1 F F 4 U = " &gt; A A A B 7 3 i c d V D J S g N B E K 1 x j X G L e v T S G A R P w 3 Q y W Q 4 e A l 4 8 R j A L J E P o 6 f Q k T X o W u 3 u E M O Q n v H h Q x K u / 4 8 2 / s b M I K v q g 4 P F e F V X 1 / E R w p R 3 n w 1 p b 3 9 j c 2 s 7 t 5 H f 3 9 g 8 O C 0 f H b R W n k r I W j U U s u z 5 R T P C I t T T X g n U T y U j o C 9 b x J 1 d z v 3 P P p O J x d K u n C f N C M o p 4 w C n R R u r 2 k z E f Z H I 2 K B Q d u 1 Q t l c s Y O b b r V u r Y N Q R X K j W M E b a d B Y q w Q n N Q e O 8 P Y 5 q G L N J U E K V 6 2 E m 0 l x G p O R V s l u + n i i W E T s i I 9 Q y N S M i U l y 3 u n a F z o w x R E E t T k U Y L 9 f t E R k K l p q F v O k O i x + q 3 N x f / 8 n q p D u p e x q M k 1 S y i y 0 V B K p C O 0 f x 5 N O S S U S 2 m h h A q u b k V 0 T G R h G o T U d 6 E 8 P U p + p + 0 S z a u 2 u 6 N W 2 x c r u L I w S m c w Q V g q E E D r q E J L a A g 4 A G e 4 N m 6 s x 6 t F + t 1 2 b p m r W Z O 4 A e s t 0 / B R 5 B y &lt; / l a t e x i t &gt; r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 w 0 n 1 I U G H 7 k S V r R 0 s Z Q N t r F o m G I = " &gt; A A A B 7 3 i c d V D L S g M x F M 3 4 r P V V d e k m W A R X w 6 S d P h Y u C m 5 c V r A P a I e S S T N t a C Y Z k 4 x Q h v 6 E G x e K u P V 3 3 P k 3 p g 9 B R Q 9 c O J x z L / f e E y a c a e N 5 H 8 7 a + s b m 1 n Z u J 7 + 7 t 3 9 w W D g 6 b m u Z K k J b R H K p u i H W l D N B W 4 Y Z T r u J o j g O O e 2 E k 6 u 5 3 7 m n S j M p b s 0 0 o U G M R 4 J F j G B j p W 4 / G b N B J m e D Q t F z S 9 V S u Y y g 5 / p + p Y 5 8 S 1 C l U k M I I t d b o A h W a A 4 K 7 / 2 h J G l M h S E c a 9 1 D X m K C D C v D C K e z f D / V N M F k g k e 0 Z 6 n A M d V B t r h 3 B s + t M o S R V L a E g Q v 1 + 0 S G Y 6 2 n c W g 7 Y 2 z G + r c 3 F / / y e q m J 6 k H G R J I a K s h y U Z R y a C S c P w + H T F F i + N Q S T B S z t 0 I y x g o T Y y P K 2 x C + P o X / k 3 b J R V X X v / G L j c t V H D l w C s 7 A B U C g B h r g G j R B C x D A w Q N 4 A s / O n f P o v D i v y 9 Y 1 Z z V z A n 7 A e f s E v L i Q b w = = &lt; / l a t e x i t &gt; o &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P i K O Z 1 H S 5 b Z u h k g 9 W 1 N U O s V K k H w = " &gt; A A A B 7 n i c d V D J S g N B E O 1 x j X G L e v T S G A R B G K a T y X L w E P D i M Y J Z I B l C T 6 c n a d K z 0 F 0 j h C E f 4 c W D I l 7 9 H m / + j Z 1 F U N E H B Y / 3 q q i q 5 y d S a H C c D 2 t t f W N z a z u 3 k 9 / d 2 z 8 4 L B w d t 3 W c K s Z b L J a x 6 v p U c y k i 3 g I B k n c T x W n o S 9 7 x J 9 d z v 3 P P l R Z x d A f T h H s h H U U i E I y C k T p q k M E l m Q 0 K R c c u V U v l M s G O 7 b q V O n E N I Z V K j R B M b G e B I l q h O S i 8 9 4 c x S 0 M e A Z N U 6 x 5 x E v A y q k A w y W f 5 f q p 5 Q t m E j n j P 0 I i G X H v Z 4 t w Z P j f K E A e x M h U B X q j f J z I a a j 0 N f d M Z U h j r 3 9 5 c / M v r p R D U v U x E S Q o 8 Y s t F Q S o x x H j + O x 4 K x R n I q S G U K W F u x W x M F W V g E s q b E L 4 + x f + T d s k m V d u 9 d Y u N q 1 U c O X S K z t A F I q i G G u g G N V E L M T R B D + g J P V u J 9 W i 9 W K / L 1 j V r N X O C f s B 6 + w R i l Y + b &lt; / l a t e x i t &gt; r t+1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m K l k k l W 5 + o / / r G 0 S 9 d S j E I 8 / Q I U = " &gt; A A A B 7 n i c d V D L S g M x F M 3 4 r P V V d e k m W A R B G C b t 9 L F w U X D j s o J 9 Q D u U T J p p Q z P J k G S E M v Q j 3 L h Q x K 3 f 4 8 6 / M X 0 I K n r g w u G c e 7 n 3 n j D h T B v P + 3 D W 1 j c 2 t 7 Z z O / n d v f 2 D w 8 L R c V v L V B H a I p J L 1 Q 2 x p p w J 2 j L M c N p N F M V x y G k n n F z P / c 4 9 V Z p J c W e m C Q 1 i P B I s Y g Q b K 3 X k I D O X a D Y o F D 2 3 V C 2 V y w h 6 r u 9 X 6 s i 3 B F U q N Y Q g c r 0 F i m C F 5 q D w 3 h 9 K k s Z U G M K x 1 j 3 k J S b I s D K M c D r L 9 1 N N E 0 w m e E R 7 l g o c U x 1 k i 3 N n 8 N w q Q x h J Z U s Y u F C / T 2 Q 4 1 n o a h 7 Y z x m a s f 3 t z 8 S + v l 5 q o H m R M J K m h g i w X R S m H R s L 5 7 3 D I F C W G T y 3 B R D F 7 K y R j r D A x N q G 8 D e H r U / g / a Z d c V H X 9 W 7 / Y u F r F k Q O n 4 A x c A A R q o A F u Q B O 0 A A E T 8 A C e w L O T O I / O i / O 6 b F 1 z V j M n 4 A e c t 0 9 d 9 4 + Y &lt; / l a t e x i t &gt; o t+1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x L n E I W W J c A V Y p G j v v O L K Z L H R y y M = " &gt; A A A B + 3 i c d V D J S g N B E O 2 J W 4 x b j E c v j U E Q l G H G r A c P A S 8 e I 5 g F k m H o 6 X S S N j 0 L 3 T V i G O Z X v H h Q x K s / 4 s 2 / s b M I K v q g 4 P F e F V X 1 v E h w B Z b 1 Y W R W V t f W N 7 K b u a 3 t n d 2 9 / H 6 h r c J Y U t a i o Q h l 1 y O K C R 6 w F n A Q r B t J R n x P s I 4 3 u Z z 5 n T s m F Q + D G 5 h G z P H J K O B D T g l o y c 0 X + s D F g C U y d Z P b M w y n d u r m i 5 Z Z q p b K F Q t b p m 2 V K r W y J p V K 1 a 6 X s G 1 a c x T R E k 0 3 / 9 4 f h D T 2 W Q B U E K V 6 t h W B k x A J n A q W 5 v q x Y h G h E z J i P U 0 D 4 j P l J P P b U 3 y s l Q E e h l J X A H i u f p 9 I i K / U 1 P d 0 p 0 9 g r H 5 7 M / E v r x f D s O 4 k P I h i Y A F d L B r G A k O I Z 0 H g A Z e M g p h q Q q j k + l Z M x 0 Q S C j q u n A 7 h 6 1 P 8 P 2 m f m 3 b V L F + X i 4 2 L Z R x Z d I i O 0 A m y U Q 0 1 0 B V q o h a i 6 B 4 9 o C f 0 b K T G o / F i v C 5 a M 8 Z y 5 g D 9 g P H 2 C e n Z l F w = &lt; / l a t e x i t &gt; rj,t+1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8    </p><formula xml:id="formula_18">+ P 3 J W p p Y g 5 2 I x r m c A I 8 t B o u B a Y = " &gt; A A A B + 3 i c d V D J S g N B E O 2 J W 4 x b j E c v j U E Q l G H G r A c P A S 8 e I 5 g F k m H o 6 X S S N j 0 L 3 T V i G O Z X v H h Q x K s / 4 s 2 / s b M I K v q g 4 P F e F V X 1 v E h w B Z b 1 Y W R W V t f W N 7 K b u a 3 t n d 2 9 / H 6 h r c J Y U t a i o Q h l 1 y O K C R 6 w F n A Q r B t J R n x P s I 4 3 u Z z 5 n T s m F Q + D G 5 h G z P H J K O B D T g l o y c 0 X + s D F g C V h 6 i a 3 Z x h O 7 d T N F y 2 z V C 2 V K x a 2 T N s q V W p l T S q V q l 0 v Y d u 0 5 i i i J Z p u / r 0 / C G n s s w C o I E r 1 b C s C J y E S O B U s z f V j x S J C J 2 T E e p o G x G f K S e a 3 p / h Y K w M 8 D K W u A P B c / T 6 R E F + p q e / p T p / A W P 3 2 Z u J f X i + G Y d 1 J e B D F w A K 6 W D S M B Y Y Q z 4 L A A y 4 Z B T H V h F D J 9 a 2 Y j o k k F H R c O R 3 C 1 6 f 4 f 9 I + N + 2 q W b 4 u F x s X y z i y 6 B A d o R N k o x p q o C v U R C 1 E 0 T 1 6 Q E / o 2 U i N R + P F e F 2 0 Z o z l z A H 6 A e P t E + U v l F k = &lt; / l a t e x i t &gt; õj,t+1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T e v u Z H m U / A 2 B h + O / q g 3 l a v w r D 8 E = " &gt; A A A C E n i c d V C 7 S g N B F J 3 1 G e M r a m k z G A R F D b s m m h Q W g o 2 l g l E h G 5 b Z y d 1 k z O y D m b t C W P M N N v 6 K j Y U i t l Z 2 / o 2 T G E F F D w w c z j m X O / f 4 i R Q a b f v d G h u f m J y a z s 3 k Z + f m F x Y L S 8 v n O k 4 V h z q P Z a w u f a Z B i g j q K F D C Z a K A h b 6 E C 7 9 7 N P A v r k F p E U d n 2 E u g G b J 2 J A L B G R r J K 2 y 6 E g J 0 b 5 S X 4 Z b T 3 3 F R y B Z k q u 9 l V 9 t 0 I L l K t D s m 4 R W K d q m 6 b 9 e c M r V L 9 h C G l P e q l b 0 y d U Z K k Y x w 4 h X e 3 F b M 0 x A i 5 J J p 3 X D s B J s Z U y i 4 h H 7 e T T U k j H d Z G x q G R i w E 3 c y G J / X p u l F a N I i V e R H S o f p 9 I m O h 1 r 3 Q N 8 m Q Y U f / 9 g b i X 1 4 j x a D W z E S U p A g R / 1 w U p J J i T A f 9 0 J Z Q w F H 2 D G F c C f N X y j t M M Y</formula><formula xml:id="formula_19">k = " &gt; A A A C E n i c d V B N a 9 t A F F y 5 a Z q 4 T a o 2 x 1 y W m E J L U i P V X z n 0 Y O i l R x d q x 2 A Z s V o / 2 Z u s t G L 3 K W A U / 4 Z c 8 l d 6 y a G l 5 J p T b / 0 3 X d k O J K E Z W B h m 5 v H 2 T Z R J Y d D z / j q V Z x v P N 1 9 s b V d f v t r Z f e 2 + e T s w K t c c + l x J p Y c R M y B F C n 0 U K G G Y a W B J J O E k O v t S + i f n o I 1 Q 6 X e c Z z B O 2 D Q V s e A M r R S 6 H w I J M Q Y X K i z w 0 F 9 8 D F D I C R R q E R a n R 7 S U A i 2 m M 5 s I 3 Z p X 7 7 S 9 Y 7 9 B v b q 3 h C W N V q f Z a l B / r d T I G r 3 Q / R N M F M 8 T S J F L Z s z I 9 z I c F 0 y j 4 B I W 1 S A 3 k D F + x q Y w s j R l C Z h x s T x p Q d 9 Z Z U J j p e 1 L k S 7 V + x M F S 4 y Z J 5 F N J g x n 5 r F X i v / z R j n G x + N C p F m O k P L V o j i X F B U t + 6 E T o Y G j n F v C u B b 2 r 5 T P m G Y c b Y t V W 8 L d p f R p M v h U 9 9 v</formula><formula xml:id="formula_20">c E S V n y 2 P n a M z o 4 x Q G E t T Q q O l + n 0 i w 5 F S s y g w n R H W E / X b W 4 h / e f 1 U h y 0 / Y y J J N R V k t S h M O d I x W n y O R k x S o v n M E E w k M 7 c i M s E S E 2 3 y K Z k Q v j 5 F / 5 O 7 C 9 t t 2 L W b W q V 9 m c d R h B M 4 h X N w o Q l t u I Y O e E C A w Q M 8 w b M l r E f r x X p d t R a s f O Y Y f s B 6 + w R 0 u 4 8 d &lt; / l a t e x i t &gt; o t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f S W S U 7 E K U 4 z G V q L M m G B q Z b 7 c r g 4 = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k T w 8 z P R 8 W K 4 4 d r P h t N w q c m x n C U O q 9 W a t X k V u r l Q g R 2 d Y f h + M Y p J G V G j C s V J 9 1 0 m 0 n 2 G p G e F 0 X h q k i i a Y T P G Y 9 g 0 V O K L K z 5 b H z t G Z U U Y o j K U p o d F S / T 6 R 4 U i p W R S Y z g j r i f r t L c S / v H 6 q w 5 a f M Z G k m g q y W h S m H O k Y L T 5 H I y Y p 0 X x m C C a S m V s R m W C J i</formula><formula xml:id="formula_21">= " &gt; A A A B 7 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 U Q h J D Y 5 8 J F w Y 3 L C v Y B b S i T 6 a Q d O p m E m R u h h H 6 E G x e K u P V 7 3 P k 3 T t s I K n r g w u G c e 7 n 3 H j 8 W X I P j f F i 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N F R o i h r 0 0 h E q u c T z Q S X r A 0 c B O v F i p H Q F 6 z r T 6 8 X f v e e K c 0 j e Q e z m H k h G U s e c E r A S N 1 o m M K F O x 8 W S 4 5 d r z k N t 4 w d 2 1 n C k H K 1 X q m W s Z s p J Z S h N S y + D 0 Y R T U I m g Q q i d d 9 1 Y v B S o o B T w e a F Q a J Z T O i U j F n f U E l C p</formula><formula xml:id="formula_22">c E S V n y 2 P n a M z o 4 x Q G E t T Q q O l + n 0 i w 5 F S s y g w n R H W E / X b W 4 h / e f 1 U h y 0 / Y y J J N R V k t S h M O d I x W n y O R k x S o v n M E E w k M 7 c i M s E S E 2 3 y K Z k Q v j 5 F / 5 O 7 C 9 t t 2 L W b W q V 9 m c d R h B M 4 h X N w o Q l t u I Y O e E C A w Q M 8 w b M l r E f r x X p d t R a s f O Y Y f s B 6 + w R 0 u 4 8 d &lt; / l a t e x i t &gt; o t</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S R r Y m t q 7 q C 5 y g p E z c r H j i u r S w y      </p><formula xml:id="formula_23">4 = " &gt; A A A B 7 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w Y 0 j s c + G i 4 M Z l B f u A N p T J d N I O n U z C z I 1 Q Q j / C j Q t F 3 P o 9 7 v w b p 2 0 E F T 1 w 4 X D O v d x 7 j x 8 L r s F x P q z c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 6 u g o U Z S 1 a S Q i 1 f O J Z o J L 1 g Y O g v V i x U j o C 9 b 1 p 9 c L v 3 v P l O a R v I N Z z L y Q j C U P O C V g p K 4 a p n D h z o f F k m P X a 0 7 D L W P H d p Y w p F y t V 6 p l 7 G Z K C W V o D Y v v g 1 F E k 5 B J o I J o 3 X e d G L y U K O B U s H l h k G g W E z o l Y 9 Y 3 V J K Q a S 9 d n j v H Z 0 Y Z 4 S B S p i T g p f p 9 I i W h 1 r P Q N 5 0 h g Y n + 7 S 3 E v 7 x + A k H D S 7 m M E 2 C S r h Y F i c A Q 4 c X v e M Q V o y B m h h C q u L k V 0 w l R h I J J q G B C + P o U / 0 8 6 l 7 Z b s y u 3 l V L z K o s j j 0 7 Q K T p H L q q j J r p B L d R G F E 3 R A 3 p C z 1 Z s P V o v 1 u u q N W d l M 8 f o B 6 y 3 T 1 W / j 5 I = &lt; / l a t e x i t &gt; r t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c P 7 i v D U f H x s F O S j C 0 R 2 X U r 1 Y 5 e 4 = " &gt; A A A B 7 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w Y 0 j s c + G i 4 M Z l B f u A N p T J d N I O n U z C z I 1 Q Q j / C j Q t F 3 P o 9 7 v w b p 2 0 E F T 1 w 4 X D O v d x 7 j x 8 L r s F x P q z c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 6 u g o U Z S 1 a S Q i 1 f O J Z o J L 1 g Y O g v V i x U j o C 9 b 1 p 9 c L v 3 v P l O a R v I N Z z L y Q j C U P O C V g p C 4 Z p n D h z o f F k m P X a 0 7 D L W P H d p Y w p F y t V 6 p l 7 G Z K C W V o D Y v v g 1 F E k 5 B J o I J o 3 X e d G L y U K O B U s H l h k G g W E z o l Y 9 Y 3 V J K Q a S 9 d n j v H Z 0 Y Z 4 S B S p i T g p f p 9 I i W h 1 r P Q N 5 0 h g Y n + 7 S 3 E v 7 x + A k H D S 7 m M E 2 C S r h Y F i c A Q 4 c X v e M Q V o y B m h h C q u L k V 0 w l R h I J J q G B C + P o U / 0 8 6 l 7 Z b s y u 3 l V L z K o s j j 0 7 Q K T p H L q q j J r p B L d R G F E 3 R A 3 p C z 1 Z s P V o v 1 u u q N W d l M 8 f o B 6 y 3 T z u V j 4 E = &lt; / l a t e x i t &gt; a t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E u 9 U s F q 2 7 9 J R C + m x J 1 J D d g i A G Q 0 = " &gt; A A A B 7 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w Y 0 j s c + G i 4 M Z l B f u A N p T J d N I O n U z C z I 1 Q Q j / C j Q t F 3 P o 9 7 v w b p 2 0 E F T 1 w 4 X D O v d x 7 j x 8 L r s F x P q z c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 6 u g o U Z S 1 a S Q i 1 f O J Z o J L 1 g Y O g v V i x U j o C 9 b 1 p 9 c L v 3 v P l O a R v I N Z z L y Q j C U P O C V g p G 4 0 T O H C n Q + L J c e u 1 5 y G W 8 a O 7 S x h S L l a r 1 T L 2 M 2 U E s r Q G h b f B 6 O I J i G T Q A X R u u 8 6 M X g p U c C p Y P P C I N E s J n R K x q x v q C Q h 0 1 6 6 P H e O z 4 w y w k G k T E n A S / X 7 R E p C r W e h b z p D A h P 9 2 1 u I f 3 n 9 B I K G l 3 I Z J 8 A k X S 0 K E o E h w o v f 8 Y g r R k H M D C F U c X M r p h O i C A W T U M G E 8 P U p / p 9 0 L m 2 3 Z l d u K 6 X m V R Z H H p 2 g U 3 S O X F R H T X S D W q i N K J q i B / S E n q 3 Y e r R e</formula><formula xml:id="formula_24">c m x n C U O q 9 W a t X k V u r l Q g R 2 d Y f h + M Y p J G V G j C s V J 9 1 0 m 0 n 2 G p G e F 0 X h q k i i a Y T P G Y 9 g 0 V O K L K z 5 b H z t G Z U U Y o j K U p o d F S / T 6 R 4 U i p W R S Y z g j r i f r t L c S / v H 6 q w 5 a f M Z G k m g q y W h S m H O k Y L T 5 H I y Y p 0 X x m C C a S m V s R m W C J i</formula><formula xml:id="formula_25">x Q G E t T Q q O l + n 0 i w 5 F S s y g w n R H W E / X b W 4 h / e f 1 U h y 0 / Y y J J N R V k t S h M O d I x W n y O R k x S o v n M E E w k M 7 c i M s E S E 2 3 y K Z k Q v j 5 F / 5 O 7 C 9 t t 2 L W b W q V 9 m c d R h B M 4 h X N w o Q l t u I Y O e E C A w Q M 8 w b M l r E f r x X p d t R</formula><formula xml:id="formula_26">c m x n C U O q 9 W a t X k V u r l Q g R 2 d Y f h + M Y p J G V G j C s V J 9 1 0 m 0 n 2 G p G e F 0 X h q k i i a Y T P G Y 9 g 0 V O K L K z 5 b H z t G Z U U Y o j K U p o d F S / T 6 R 4 U i p W R S Y z g j r i f r t L c S / v H 6 q w 5 a f M Z G k m g q y W h S m H O k Y L T 5 H I y Y p 0 X x m C C a S m V s R m W C J i</formula><formula xml:id="formula_27">x Q G E t T Q q O l + n 0 i w 5 F S s y g w n R H W E / X b W 4 h / e f 1 U h y 0 / Y y J J N R V k t S h M O d I x W n y O R k x S o v n M E E w k M 7 c i M s E S E 2 3 y K Z k Q v j 5 F / 5 O 7 C 9 t t 2 L W b W q V 9 m c d R h B M 4 h X N w o Q l t u I Y O e E C A w Q M 8 w b M l r E f r x X p d t R</formula><formula xml:id="formula_28">c m x n C U O q 9 W a t X k V u r l Q g R 2 d Y f h + M Y p J G V G j C s V J 9 1 0 m 0 n 2 G p G e F 0 X h q k i i a Y T P G Y 9 g 0 V O K L K z 5 b H z t G Z U U Y o j K U p o d F S / T 6 R 4 U i p W R S Y z g j r i f r t L c S / v H 6 q w 5 a f M Z G k m g q y W h S m H O k Y L T 5 H I y Y p 0 X x m C C a S m V s R m W C J i</formula><formula xml:id="formula_29">h W E 6 A F a v u t X O u R T V K 9 q Q J U = " &gt; A A A B 7 H i c d V D J S g N B E O 2 J W 4 x b 1 K O X x i B 4 G n q y H z w E v H i M Y B Z I h t D T 6 S R N e n q G 7 h o h D P k G L x 4 U 8 e o H e f N v 7 C y C i j 4 o e L x X R V W 9 I J b C A C E f T m Z j c 2 t 7 J 7 u b 2 9 s / O D z K H 5 + 0 T Z R o x l s s k p H u B t R w K R R v g Q D J u 7 H m N A w k 7 w T T 6 4 X f u e f a i E j d w S z m f k j H S o w E o 2 C l F h 2 k M B / k C 8 Q l l X L N K 2 L i l u q E l C q W V G u k X q x g z y V L F N A a z U H + v T + M W B J y B U x S Y 3 o e i c F P q Q b B J J / n + o n h M W V T O u Y 9 S x U N u f H T 5 b F z f G G V I R 5 F 2 p Y C v F S / T 6 Q 0 N G Y W B r Y z p D A x v 7 2 F + J f X S 2 B U 9 1 O h 4 g S 4 Y q t F o 0 R i i P D i c z w U m j O Q M 0 s o 0 8 L e i t m E a s r A 5 p O z I X x 9 i v 8 n 7 a L r V d 3 y b b n Q u F r H k U V n 6 B x d I g / V U A P d o C Z q I Y</formula><p>Y E e k B P 6 N l R z q P z 4 r y u W j P O e u Y U / Y D z 9 g l z S I 8 d &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K f 7 t L w r F 8 e t g u c s d u w g m X w      </p><formula xml:id="formula_30">M v + x U = " &gt; A A A B 7 H i c d V D J S g N B E O 2 J W 4 x b 1 K O X x i B 4 G n q y H z w E v H i M Y B Z I h t D T 6 S R N e n q G 7 h o h D P k G L x 4 U 8 e o H e f N v 7 C y C i j 4 o e L x X R V W 9 I J b C A C E f T m Z j c 2 t 7 J 7 u b 2 9 s / O D z K H 5 + 0 T Z R o x l s s k p H u B t R w K R R v g Q D J u 7 H m N A w k 7 w T T 6 4 X f u e f a i E j d w S z m f k j H S o w E o 2 C l l h 6 k M B / k C 8 Q l l X L N K 2 L i l u q E l C q W V G u k X q x g z y V L F N A a z U H + v T + M W B J y B U x S Y 3 o e i c F P q Q b B J J / n + o n h M W V T O u Y 9 S x U N u f H T 5 b F z f G G V I R 5 F 2 p Y C v F S / T 6 Q 0 N G Y W B r Y z p D A x v 7 2 F + J f X S 2 B U 9 1 O h 4 g S 4 Y q t F o 0 R i i P D i c z w U m j O Q M 0 s o 0 8 L e i t m E a s r A 5 p O z I X x 9 i v 8 n 7 a L r V d 3 y b b n Q u F r H k U V n 6 B x d I g / V U A P d o C Z q I Y Y E</formula><formula xml:id="formula_31">O B Z W + W V 7 w 5 Q = " &gt; A A A B 7 X i c d V D J S g N B E O 2 J W 4 x b 1 K O X x i B 4 G n p i t o O H g B e P E c w C y R B 6 O p 2 k T c / 0 0 F 0 j h C H / 4 M W D I l 7 9 H 2 / + j Z 1 F U N E H B Y / 3 q q i q F 8 R S G C D k w 8 m s r W 9 s b m W 3 c z u 7 e / s H + c O j l l G J Z r z J l F S 6 E 1 D D p Y h 4 E w R I 3 o k 1 p 2 E g e T u Y X M 3 9 9 j 3 X R q j o F q Y x 9 0 M 6 i s R Q M A p W a v V g z I H 2 8 w X i k n K p 6 h U x c S 9 q h F y U L a l U S a 1 Y x p 5 L F i i g F R r 9 / H t v o F g S 8 g i Y p M Z 0 P R K D n 1 I N g k k + y / U S w 2 P K J n T E u 5 Z G N O T G T x f X z v C Z V Q Z 4 q L S t C P B C / T 6 R 0 t C Y a R j Y z p D C 2 P z 2 5 u J f X j e B Y c 1 P R R Q n w C O 2 X D R M J A a F 5 6 / j g d C c g Z x a Q p k W 9 l b M x l R T B j a g n A 3 h 6 1 P 8 P 2 k V X a / i l m 5 K h f r l K o 4 s O k G n</formula><formula xml:id="formula_32">c E S V n y 2 P n a M z o 4 x Q G E t T Q q O l + n 0 i w 5 F S s y g w n R H W E / X b W 4 h / e f 1 U h y 0 / Y y J J N R V k t S h M O d I x W n y O R k x S o v n M E E w k M 7 c i M s E S E 2 3 y K Z k Q v j 5 F / 5 O 7 C 9 t t 2 L W b W q V 9 m c d R h B M 4 h X N w o Q l t u I Y O e E C A w Q M 8 w b M l r E f r x X p d t R a s f O Y Y f s B 6 + w R 0 u 4 8 d &lt; / l a t e x i t &gt; o t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F S M 4 9 D N 0 / 9 F F O 2 g E I a / B e w g v 8 A g = " &gt; A A A B 7 3 i c d V D J S g N B E O 1 x j X G L e v T S G A Q P M v R k P 3 g I e P E Y w S y Q D K G n 0 5 O 0 6 V n s r h H C k J / w 4 k E R r / 6 O N / / G z i K o 6 I O C x 3 t V V N X z Y i k 0 E P J h</formula><formula xml:id="formula_33">Q o B z 9 X v E y k N t J 4 E n u k M K I z 0 b 2 8 m / u V 1 E / B r b i r C O A E e s s U i P 5 E Y I j x 7 H g + E 4 g z k x B D K l D C 3 Y j a i i j I w E W V N C F + f 4 v 9 J q 2 A 7 F b t 0 X c r X L 5 Z x Z N A x O k F n y E F V V E d X q I G a i C G J H t A T e</formula><formula xml:id="formula_34">Q o B z 9 X v E y k N t J 4 E n u k M K I z 0 b 2 8 m / u V 1 E / B r b i r C O A E e s s U i P 5 E Y I j x 7 H g + E 4 g z k x B D K l D C 3 Y j a i i j I w E W V N C F + f 4 v 9 J q 2 A 7 F b t 0 X c r X L 5 Z x Z N A x O k F n y E F V V E d X q I G a i C G J H t A T e</formula><formula xml:id="formula_35">Q o B z 9 X v E y k N t J 4 E n u k M K I z 0 b 2 8 m / u V 1 E / B r b i r C O A E e s s U i P 5 E Y I j x 7 H g + E 4 g z k x B D K l D C 3 Y j a i i j I w E W V N C F + f 4 v 9 J q 2 A 7 F b t 0 X c r X L 5 Z x Z N A x O k F n y E F V V E d X q I G a i C G J H t A T e</formula><formula xml:id="formula_36">V h i d G g t n v u p / s D l w = " &gt; A A A B + X i c d V D L S s N A F J 3 4 r P U V d e l m s A i u Q t I 0 1 Y K L g h u X F e w D m h A m k 0 k 7 d P J g Z l I o I X / i x o U i b v 0 T d / 6 N 0 4 e g o g c u H M 6 5 l 3 v v C T J G h T T N D 2 1 t f W N z a 7 u y U 9 3 d 2 z 8 4 1 I + O e y L N O S Z d n L K U D w I k C K M J 6 U o q G R l k n K A 4 Y K Q f T G 7 m f n 9 K u K B p c i 9 n G f F i N E p o R D G S S v J 1 3 c 3 G 1 C 9 c S V l I C l 6 W v l 4 z D d t p O U 4 L m o Z l 2 n X H V s R p m v W G D S 3 D X K A G V u j 4 + r s b p j i P S S I x Q 0 I M L T O T X o G 4 p J i R s u r m g m Q I T 9 C I D B V N U E y E V y w u L + G 5 U k I Y p V x V I u F C / T 5 R o F i I W R y o z h j J s f j t z c W / v G E u o y u v o E m W S 5 L g 5 a I o Z 1 C m c B 4 D D C k n W L K Z I g h z q m 6 F e I w 4 w l K F V V U h f H 0 K / y e 9 u m E 1 j c Z d o 9 a + X s V R A a f g D F w A C 1 y C N r g F H d A F G E z B A 3 g C z 1 q h P W o v 2 u u y d U 1 b z Z y A H 9 D e P g G 4 c 5 R g &lt; / l a t e x i t &gt; r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w j V w j 9 C I 7 O K i V S w k z h a 1 K 6 Z c t e k = " &gt; A A A B + X i c d V D L S s N A F J 3 4 r P U V d e l m s A i u Q t I 0 1 Y K L g h u X F e w D m h A m k 0 k 7 d P J g Z l I o I X / i x o U i b v 0 T d / 6 N 0 4 e g o g c u H M 6 5 l 3 v v C T J G h T T N D 2 1 t f W N z a 7 u y U 9 3 d 2 z 8 4 1 I + O e y L N O S Z d n L K U D w I k C K M J 6 U o q G R l k n K A 4 Y K Q f T G 7 m f n 9 K u K B p c i 9 n G f F i N E p o R D G S S v J 1 3 c 3 G 1 C 9 c S V l I i r Q s f b 1 m G r b T c p w W N A 3 L t O u O r Y j T N O s N G 1 q G u U A N r N D x 9 X c 3 T H E e k 0 R i h o Q Y W m Y m v Q J x S T E j Z d X N B c k Q n q A R G S q a o J g I r 1 h c X s J z p Y Q w S r m q R M K F + n 2 i Q L E Q s z h Q n T G S Y / H b m 4 t / e c N c R l d e Q Z M s l y T B y 0 V R z q B M 4 T w G G F J O s G Q z R R D m V N 0 K 8 R h x h K U K q 6 p C + P o U / k 9 6 d c N q G o 2 7 R q 1 9 v Y q j A k 7 B G b g A F r g E b X A L O q A L M J i C B / A E n r V C e 9 R e t N d l 6 5 q 2 m j k B P 6 C 9 f Q K z 4 Z R d &lt; / l a t e x i t &gt; õ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 G 9 f t J B N W E a L 7 4 n E 9 l u X T P H b l U = " &gt; A A A B 6 H i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 6 a d d i q 4 K L h x 2 Y J 9 Q D u U T J p p Y z O Z I c k I p f Q L 3 L h Q x K 2 f 5 M 6 / M d N W U N E D F w 7 n 3 M u 9 9 w Q J Z 0 o j 9 G G t r W 9 s b m 3 n d v K 7 e / s H h 4 W j 4 7 a K U 0 l o i 8 Q 8 l t 0 A K 8 q Z o C 3 N N K f d R F I c B Z x 2 g s l 1 5 n f u q V Q s F r d 6 m l A / w i P B Q k a w N l I z G h S K y H a d a s k r Q 2 S X k V f x L g 1 B q F Z x y 9 A x J E M R r N A Y F N 7 7 w 5 i k E R W a c K x U z 0 G J 9 m d Y a k Y 4 n e f 7 q a I J J h M 8 o j 1 D B Y 6 o 8 m e L Q + f w 3 C h D G M b S l N B w o X 6 f m O F I q W k U m M 4 I 6 7 H 6 7 W X i X 1 4 v 1 W H N n z G R p J o K s l w U p h z q G G Z f w y G T l G g + N Q Q T y c y t k I y x x E S b b P I m h K 9 P 4 f + k X b K d q u 0 2 3 W L 9 a h V H D p y C M 3 A B H O C B O r g B D d A C B F D w A J 7 A s 3 V n P V o v 1 u u y d c 1 a z Z y A H 7 D e P g F B x 4 0 + &lt; / l a t e x i t &gt; m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j + 4 k W 0 A G 1 Y f P a E Q s h a I 9 h q k n V V A = " &gt; A A A C F n i c b Z D L S g M x F I Y z 9 V b r b d S l m 2 A R K m i Z K U V d u C i 4 c S U V 7 A U 6 t W T S T B u a z A z J G a E M f Q o 3 v o o b F 4 q 4 F X e + j e l l o a 0 / B H 6 + c w 4 n 5 / d j w T U 4 z r e V W V p e W V 3 L r u c 2 N r e 2 d + z d v b q O E k V Z j U Y i U k 2 f a C Z 4 y G r A Q b B m r B i R v m A N f 3 A 1 r j c e m N I 8 C u 9 g G L O 2 J L 2 Q B 5 w S M K h j n 8 p O O s K e 5 h L f e I I F U P B k Y t D J m P U k M f Y + L Y 0 8 x X t 9 O O 7 Y e a f o T I Q X j T s z e T R T t W N / e d 2 I J p K F Q A X R u u U 6 M b R T o o B T w U Y 5 L 9 E s J n R A e q x l b E g k 0 + 1 0 c t Y I H x n S x U G k z A s B T + j v i Z R I r Y f S N 5 2 S Q F / P 1 8 b w v 1 o r g e C i n f I w T o C F d L o o S A S G C I 8 z w l 2 u G A U x N I Z Q x c 1 f M e 0 T R S i Y J H M m B H f + 5 E V T L x X d s 2 L 5 t p y v X M 7 i y K I D d I g K y E X n q I K u U R X V E E W P 6 B m 9 o j f r y X q x 3 q 2 P a W v G m s 3 s o z + y P n 8 A M / W f Z g = = &lt; / l a t e x i t &gt; m ⇠ N µ, 2</formula><p>Fig. <ref type="figure">3</ref>: MetaVIM consists of a mVAE and a policy network. RL agent is augmented with a latent variable m, obtained by encoding over past trajectories τ :t to represent the task-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Latent Variable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Design</head><p>From the perspective of meta-RL, there exists a true variable to represent a certain task (i.e., an agent). However, we do not have access to this information. Alternatively, we aim to learn variable m from trajectories of the current task.</p><p>Considering the uncertainty of the task, we model the task as a Gaussian distribution N (µ, σ 2 ), where the mean µ and the standard deviation σ are generated by the encoder. Specifically, a RNN is employed to track the characteristics of trajectory τ :t and calculate µ and σ respectively. Then, m is randomly sampled from N (µ, σ 2 ), that is, m ∼ N (µ, σ 2 ). Note m ∼ N (µ, σ 2 ) is underivable, we alternatively use the reparameterization technique <ref type="bibr" target="#b64">[65]</ref> for backpropagation:</p><formula xml:id="formula_37">m = µ + σm 0 ,<label>(11)</label></formula><p>where m 0 is randomly sampled from the standard normal distribution m 0 ∼ N (µ, I), and I is the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Analysis</head><p>Besides making the meta-learning possible as shown in Eq. 1, the latent variable brings two additional advantages:</p><p>(1) Trade-off of Exploration and Exploitation: Latent variable is an additional input of the policy network except for the observation, which could be regarded as prior knowledge about the task. Previous research <ref type="bibr" target="#b63">[64]</ref> has shown that random Gaussian distribution could be regarded as the noise disturbance and provide randomness of the policy. Specifically, the regular policy network outputs action probability based on the current observation only, and the distribution of sampled action is certain once the current observation is given. In contrast, our method takes the latent variable m as the additional input of the policy network besides the observation, where m is randomly sampled from the learnable Gaussian distribution N (µ, δ 2 ). Hence, m could bring randomness to the distribution of the sampled action in the learning procedure. Hence, it helps to explore the tasks. Since our Gaussian distribution changes dynamically in training rather than keeps as constant. It not only helps to explore but also helps to trade off the exploration and exploitation <ref type="bibr" target="#b49">[50]</ref>. With the converging of the model, the randomness gradually decreases. Specifically, the mean µ tends to be stable and the variance δ 2 tends to zero. Fig. <ref type="figure">4</ref> illustrates the adaptation of latent variable. It has a dynamic guiding effect on policy: randomness is the largest at the beginning, the dispersive distribution of m necessitates the agent to perform diverse actions to explore, and then randomness decreases as the posterior distribution N (µ, σ 2 ) gradually converges. In this phase, the policy network encourages the agent to exploit the environment. In addition, it also makes the final learned policy stable with very little randomness. This can be analogous to the impact of ϵ-greedy in DQN on trade-off of the exploration and exploitation. The intuitive idea is: random noise (random latent variable) is equivalent to the fixed ϵ in ϵ-greedy, and the learnable Gaussian distribution (learnable latent variable) is equivalent to the adjustable ϵ in ϵ-greedy, which has a dynamic guiding effect on the trade-off of the exploration and exploitation.</p><p>(2) Modelling Latent Coordination with Neighbors: In our method, the neighbor information is available only in training, and the decoders are abandoned in execution. Only using individual observation as the input of policy may ignore the latent neighbors information. As shown in Eq. 1, the observation transition is caused by not only the current agent but also its neighbors. In turn, the latent variable could reflect the latent neighbor's information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Intrinsic Reward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Design</head><p>As stated in Eq. 2, the non-stationary learning often causes the observation transition and received rewards unpre- dictable only conditioned on individual observation and action. Conversely, we hope the learned policy makes them be predicted stably. To achieve this goal, we design a novel intrinsic reward based on VAE, as illustrated in Fig. <ref type="figure">5</ref>, which is the negative of the prediction bias with/without neighbor agents' policies:</p><formula xml:id="formula_38">r int t = - j ∥r t+1 -rj,t+1 ∥ + ∥o t+1 -õj,t+1 ∥ ,<label>(12)</label></formula><p>where r t+1 and rj,t+1 are the predicted rewards with/without neighbor agents' policies respectively, o t+1 and õj,t+1 are predicted next observations with/without neighbor agent's policies respectively. They are generated by the corresponding decoders in Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Analysis</head><p>Here, will give an interpretation from the perspective of information theory why the intrinsic reward design can lead to a stable policy robust to neighbors' policy, which could make the learned policy easy to transfer. From the perspective of the current agent, the r t+1 is defined as</p><formula xml:id="formula_39">r t+1 = R (o t+1 , o t , u t , a t ; m) ,<label>(13)</label></formula><p>for any neighbor agent j, the unobservable part u t consists of two parts, the part containing j and the part not containing j, that is u t = {o j,t , u -j t }, and a j,t ∼ π j . Then add the neighbor agent's policy, we can obtain:</p><formula xml:id="formula_40">rj,t+1 = R(o t+1 , o t , o j,t , u -j t unobs.</formula><p>, a t , a j,t ; m).</p><p>Similar, o t+1 is defined as</p><formula xml:id="formula_42">o t+1 = T (o t+1 | o t , a t , u t ; m) ,<label>(15)</label></formula><p>we split the unobservable part u t+1 into the part containing j and the part not containing j, then add the neighbor agent's policy, we have</p><formula xml:id="formula_43">õj,t+1 = T (o t+1 | o t , a t , o j,t , u -j t unobs.</formula><p>, a j,t ; m).</p><p>According to Eq. 12, our intrinsic reward is:</p><formula xml:id="formula_45">r int t = - j ∥r t+1 -rj,t+1 ∥ + ∥o t+1 -õj,t+1 ∥ (17) = - j R (o t+1 , o t , u t , a t ; m) - R(o t+1 , o t , o j,t , u -j t , a t , a j,t ; m) 2 2 + T (o t+1 | o t , a t , u t ; m) - T (o t+1 | o t , a t , o j,t , u -j t , a j,t ; m) 2 2 = - j R r t+1 | z 2 t -R r t+1 | a j,t , z 2 t 2 2 + T o t+1 | z 1 t -T o t+1 | a j,t , z 1 t 2 2</formula><p>where z 1 t represents the conditioning variable 1 at timestep t, z 1 t = ⟨o t , a t , u t , m⟩, and z 2 t represents the conditioning variable 2 at timestep t, z 2 t = ⟨o t+1 , o t , u t , a t , m⟩. The mutual information(MI) between the predicted state o t+1 and another agent's policy a j is:</p><formula xml:id="formula_46">I o t+1 ; a j,t | z 1 t (18) = ot+1,aj,t p o t+1 , a j,t | z 1 t log p o t+1 , a j,t | z 1 t p (o t+1 | z 1 t ) p (a j,t | z 1 t ) = ot+1,aj,t p o t+1 | a j,t , z 1 t • p a j,t | z 1 t • log p o t+1 | a j,t , z 1 t • p a j,t | z 1 t p (o t+1 | z 1 t ) p (a j,t | z 1 t ) = aj,t p a j,t | z 1 t D KL p o t+1 | a j,t , z 1 t ∥p o t+1 | z 1 t = aj,t p a j,t | z 1 t D KL T o t+1 | a j,t , z 1 t ∥T o t+1 | z 1 t .</formula><p>The mutual information(MI) between the predicted reward r t+1 and another agent's policy a j,t is as follows:</p><formula xml:id="formula_47">I r t+1 ; a j,t | z 2 t (19) = rt+1,aj,t p r t+1 , a j,t | z 2 t log p r t+1 , a j,t | z 2 t p (r t+1 | z 2 t ) p (a j,t | z 2 t ) = rt+1,aj,t p r t+1 | a j,t , z 2 t • p a j,t | z 2 t log p r t+1 | a j,t , z 2 t • p a j,t | z 2 t p (r t+1 | z 2 t ) p (a j,t | z 2 t ) = aj,t p a j,t | z 2 t D KL p r t+1 | a j,t , z 2 t ∥p r t+1 | z 2 t = aj,t p a j,t | z 2 t D KL R r t+1 | a j,t , z 2 t ∥R r t+1 | z 2 t .</formula><p>Combine the Eq. 18 and Eq. 19, we have</p><formula xml:id="formula_48">I o t+1 ; a j,t | z 1 t + I r t+1 ; a j,t | z 2 t (20) = aj,t p a j,t | z 1 t D KL T o t+1 | a j,t , z 1 t ∥T o t+1 | z 1 t + aj,t p a j,t | z 2 t D KL R r t+1 | a j,t , z 2 t ∥R r t+1 | z 2 t ,</formula><p>By sampling N independent trajectories τ N from the environment, we perform a Monte-Carlo approximations of the MI:</p><formula xml:id="formula_49">I o t+1 ; a j,t | z 1 t + I r t+1 ; a j,t | z 2 t (21) = E τ D KL T o t+1 | a j,t , z 1 t ∥T o t+1 | z 1 t | z 1 t + E τ D KL R r t+1 | a j,t , z 2 t ∥R r t+1 | z 2 t | z 2 t , ≈ 1 N N T o t+1 | a j,t , z 1 t -T o t+1 | z 1 t + R r t+1 | a j,t , z 2 t -R r t+1 | z 2 t .</formula><p>The last approximation is because the predictions in our implementation are deterministic rather than stochastic.</p><p>Thus, in expectation, the intrinsic reward is the negative of MI above. As each agent maximizes the long-term cumulative reward, which therefore minimizes MI. As a result, agents become independent. This can be an interpretation from the information-theoretical perspective. Note that the prediction results are only used to form intrinsic rewards, and our method tries to minimize them. That means our method mainly relies on the trend of change of predicted results, not the predicted value. Therefore, we expect our method is resilient to the decoders' modeling error accumulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning</head><p>The parameters of MetaVIM consist of a policy network π θ (a t | o t , m) and a mVAE which includes a encoder e ψ (m | τ :t ) and four decoders p ϕr (r t+1 | o t+1 , o t , a t , m), p ϕo (o t+1 | o t , a t , m), p ϕr (r t+1 | o t+1 , o t , a t , a j,t , m) and p ϕō (õ t+1 | o t , a t , a j,t , m), where θ, ψ, ϕ r , ϕ o , ϕ r and ϕ õ are the corresponding network weights. The learning mainly contains two parts:</p><p>(1) Maximizing the Cumulative Rewards: For the policy network θ, the learning objective is to maximize the cumulative reward as well as the intrinsic reward r int t in Eq. 12:</p><formula xml:id="formula_50">max θ J(θ) = E at∼π θ (at|ot,m) m∼N (µ,σ 2 ) H t=0 (r t + αr int t ),<label>(22)</label></formula><p>where α is a positive weight. To achieve Eq. 22, the Proximal Policy Optimization (PPO) <ref type="bibr" target="#b70">[71]</ref> is employed. PPO is an on-policy actor-critic method which adds a soft constraint that can be optimized by a first-order optimizer, effectively using existing data to take step that leads to the biggest possible improvement on a policy, without stepping too far to accidentally cause performance collapse.</p><p>(2) Training mVAE: For current agent, the past trajectory up to time step t is collected as </p><formula xml:id="formula_51">τ :t = (o 0 , a 0 , r 1 , o 1 , a 1 , r 2 , . . . , o t-1 , a t-1 , r t , o t ) ,<label>(23)</label></formula><p>Since the original optimized objective of VAE can't be optimized directly, instead of optimising Eq. 24, we use a variational evidence lower bound (ELBO) <ref type="bibr" target="#b64">[65]</ref> which is widely used to optimize the VAE-based network. Maximizing ELBO can approximately maximize the log p ϕr,ϕr,ϕo,ϕõ (τ :t ), the ELBO can be derived as follows:</p><p>E τ:t∼ρ log p ϕr,ϕr,ϕo,ϕõ (τ :t )</p><p>= E τ:t∼ρ log p ϕr (r t+1 ) + log p ϕr (r t+1 ) </p><formula xml:id="formula_54">+</formula><p>The first 4 terms are often referred to as the reconstruction loss, and the term KL(e ψ (m|τ :t )||q(m)) is the KL-divergence between the variational posterior and the prior distribution q(m), which is set to standard normal distribution N (0, I) initially, where I means the identity matrix. Suppose ρ i is the trajectory distribution induced by our policy and the transition function T (•), then the learning objective of the mVAE is to maximize the ELBO over ρ: max ψ,ϕr,ϕr,ϕo,ϕõ E τ:t∼ρ ELBO(ψ, ϕ r , ϕ r , ϕ o , ϕ õ|τ :t ).</p><p>(</p><formula xml:id="formula_56">)<label>27</label></formula><p>The overall training algorithm is provided in Alg. 1. In our experiments, we train the policy and the mVAE using different replay buffers: B π only collects recent trajectories since we use on-policy RL algorithms, and for the B mVAE we maintain a larger buffer of observed trajectories. At meta-test time, we roll out the policy to evaluate the performance, meta-testing procedure is provided in Alg. 2. The decoders are not used at test time, and no gradient adaptation is done: θ and ψ are shared across different tasks and the policy has learned to act approximately optimal during meta-training. See Appendix B for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct the experiments on CityFlow <ref type="bibr" target="#b19">[20]</ref>, an city-level open-source simulation platform for traffic signal control. The simulator is used as the environment to provide state for traffic signal control, the agents execute actions by changing the phase of traffic lights, and the simulator returns feedback. Specifically, we conduct additional experiments on another platform SUMO 1 and under different lane configurations to demonstrate the robustness of the method in Appendix D and Appendix C respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Road Networks</head><p>The evaluation scenarios come from four real road network maps of different scales, including Hangzhou (China), Jinan (China), New York (USA) and Shenzhen (China), illustrated in Fig. <ref type="figure" target="#fig_3">6</ref>. The road networks and data of Hangzhou, Jinan and New York are from the public datasets 2 . The road network map of Shenzhen is made by ourselves which is derived from OpenStreetMap 3 . The road networks of Jinan and Hangzhou contain 12 and 16 intersections in 4 × 3 and 4 × 4 grids, respectively. The road network of New York includes 48 intersections in 16 × 3 grid. The road network of Shenzhen contains 33 intersections, which is not grid compared to other three maps, illustrated in Fig. <ref type="figure" target="#fig_4">7</ref>. In addition, an additional experiment are conducted on a much larger road network to validate the scalability, more details are listed in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Flow configurations</head><p>We run the experiments under three traffic flow configurations: real traffic flow, mixed low traffic flow and mixed high traffic flow. The real traffic flow is real-world hourly statistical data with slight variance in vehicle arrival rates, as shown in Tab. 1. Since the real-world strategies tend to break down during bottleneck period (peak hour), to better  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Criteria</head><p>Following existing studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>, we use the average travel time to evaluate the performance of different methods for traffic signal control. The average travel time indicates the overall traffic situation in an area over a period of time. For a detailed definition of average travel time, see   Section 3.1. Since the number of vehicles and the origindestination (OD) positions are fixed, better traffic signal control strategies result in less average travel time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Testing Mode</head><p>The method is evaluated in two modes: (1) Common Testing Mode: the model trained on one scenario with one traffic flow configuration is tested on the same scenario with the same configuration. It is used to validate the ability of the RL algorithm to find the optimal policy. (2) Meta-Test Mode: we train the model in the Hangzhou road network and transfer the model to the other three networks directly. It is used to validate the generality of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines</head><p>Our method is compared with the following two categories of methods: conventional transportation methods and RL methods 5 . Note that for a fair comparison all the RL methods are learned without any pre-trained parameters and the methods are evaluated under the same settings. The results are obtained by running the source codes 6 . All the baselines are run with three random seeds, and the mean is taken as the final result. The action interval is five seconds for each method, and the horizon is 3600 seconds for each episode. Specifically, the compared methods contain:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Conventional methods</head><p>• Random selects available phase randomly.</p><p>• MaxPressure [2] is a leading conventional method, which greedily chooses the phase with the maximum pressure. The pressure is defined as the difference of vehicle density between the incoming lane and the outgoing lane.</p><p>• Fixedtime <ref type="bibr" target="#b0">[1]</ref> with random offset <ref type="bibr" target="#b71">[72]</ref> executes each phase in a phase loop with a pre-defined span of phase duration, which is widely used for steady traffic.</p><p>• FixedtimeOffset <ref type="bibr" target="#b0">[1]</ref> where multiple intersections use the same synchronized fix-time plan. The offset is the time interval between the start time of the green light among intersections.</p><p>5. Some existing RL based methods, such as AttendLight <ref type="bibr" target="#b41">[42]</ref> and SD-MaCAR <ref type="bibr" target="#b2">[3]</ref>, evaluate their methodS under different experimental settings (e.g., road network or traffic flow), and the source codes are not available yet. Therefore, they are not compared.</p><p>6. <ref type="url" target="https://github.com/traffic-signal-control/RL_signals">https://github.com/traffic-signal-control/RL_signals</ref>  • SlidingFormula <ref type="bibr" target="#b71">[72]</ref> is designed based on the expert experience, which dynamically divides the duration of each phase according to the number of vehicle arriving.</p><p>• SOTL <ref type="bibr" target="#b22">[23]</ref> specifies a pre-defined threshold for the number of waiting vehicles on approaching lanes. Once the waiting vehicles exceeds the threshold, it will switch to the next phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">RL-based methods</head><p>• Individual RL <ref type="bibr" target="#b11">[12]</ref> where each intersection is controlled by one agent. The replay buffer and network parameters are not shared, and the model update is independent.</p><p>There is no information transfer between agents.</p><p>• MetaLight [14] is a value-based meta RL method via parameter initialization based on MAML <ref type="bibr" target="#b72">[73]</ref>. MetaLight is originally a single-agent approach for meta-learning on multiple separate tasks. Here we extend it to a multiagent scenario without considering neighbor information.</p><p>• PressLight <ref type="bibr" target="#b12">[13]</ref> combines the traditional traffic method MaxPressure <ref type="bibr" target="#b1">[2]</ref> with RL together, and optimizes the pressure of each intersection.</p><p>• CoLight <ref type="bibr" target="#b45">[46]</ref> uses graph convolution and attention mechanism to model the neighbor information, and then further uses this neighbor information to optimize the queue length.</p><p>• GeneraLight [49] is a meta RL method which uses generative adversarial network to generate diverse traffic flows and uses them to build training environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Evaluation on Common Testing</head><p>Tab. 4 lists the comparative results on the common testing mode, and it is evident that: 1) In general, RL methods perform better than conventional methods, and it indicates the advantage of the RL. The reason is that the conventional methods often rely on prior knowledge which may fails in some cases. A typical case is MaxPressure. It shows good performances on several cases including Hangzhou with the real configuration, Jinan with the real,mixed l configurations, NewYork with the real,mixed l configurations, and Shenzhen with the real,mixed l configurations. However, it dramatically drops in other scenarios. That is, once the scenarios meet the assumption well, the method performs well and vice versa. Moreover, MetaVIM is comprehensively superior to other methods clearly in all scenarios and configurations, which demonstrates the effectiveness of the method.</p><p>2) MetaVIM shows good generalization for different scenarios and configurations. MetaVIM performs the second best in Hangzhou with the mixed l configuration, Jinan with the real configuration and Shenzhen with the mixed l configuration, and performs best in other scenarios. Overall, MetaVIM has the best mean performance. Except Max-Pressure analysed above, GeneraLight achieves the best in Hangzhou with the mixed l configuration, while performs poorly in other scenarios. The reason is that GeneraLight trains several models on diverse generated traffic flows, and select the model in testing by matching the flow. Hence, it limits the generalization once the testing flow differs largely from the training flows.</p><p>3) MetaVIM outperforms Individual RL, MetaLight and PrssLight with 827, 423 and 411, respectively. The main reason is that they learn the traffic signal's policy only using its own observation and ignore the influence of the neighbors, while MetaVIM considers the neighbors as the unobserved part of the current signal to help learning. In addition, Individual RL performs relatively worst in all scenarios because it employs independent replay buffer and neural network parameters among agents. In traffic signal control task, different signals vary but also share similarities since they follow the same traffic rules and have similar optimization goals. Hence, sharing the replay buffer and neural network is helpful.</p><p>4) The neighbors' information is modeled in CoLight and it performs well.It indicates modeling neighbors is critical for the coordination. The results of MetaVIM is superior to CoLight on each scenario and configuration, resulting mean 43 improvement. Compared to Colight, MetaVIM proposes an intrinsic reward to help the policies learning stable, and use latent variable to better trade off the exploration and exploitation. In addition, Colight needs the agents' communications in testing, which is unnecessary in MetaVIM. It makes MetaVIM easy to deploy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Evaluation on Meta-Test</head><p>The comparative results evaluated on the meta-test mode are shown in Tab. 5. The "original" means the model is trained on the current testing scenario, and the "transfer" stands for the model is trained on the road map of Hangzhou. From the results, we can obtain follow findings:</p><p>1) Colight needs full state information in both training and testing, hence it cannot be used for a new scenario which contains different number intersections compared with the training scenario. That is, the heterogeneous scenarios will cause heterogeneous inputs of the policy network, which makes the network fail to work. Hence, its results are unavailable in this setting. In contrast, the input of decentralized policy is the current agent's observation, and it is easy to adapt to a new scenario. Therefore, it is necessary to develop the decentralized policy for cross-scenario generalization.</p><p>2) The performances of Individual RL and PressLight drop 38% and 41% when the model is transferred. It shows that the models learned by the regular RL algorithms indeed rely on the training scenario. MetaLight is more robust to various scenarios than Individual RL and PressLight, and it indicates the advantage of the meta-learning framework. The meta-learning framework could help to learn taskshared model. Overall, MetaVIM achieves the state-of-theart performances and only drops 9% when transferring the model. The main reason is that: the task-specific information is modeled by the latent variable in our method, and the learned policy function could be adaptive to diverse latent the policy learned from a training scenario generalizable to new unseen scenarios. MetaVIM learns the decentralized policy for each intersection which considers neighbor information in a latent way. We conduct extensive experiments and demonstrate the superior performance of our method over the state-of-the-art. We have collected and released more complex scenarios containing different structures 7 , and will improve the method based on these scenarios in the future. In addition, the utilization of latent variable in model-based RL for traffic signal control will also be explored to improve sample efficiency. The results are shown in Tab. 7, and it is evident that: Among these baselines, the performance of Fixedtime is the worst because it can not adapt to the dynamics in the road network. RL-based methods show advantages than conventional traffic method. Among the RL-based method, MetaVIM outperforms Individual RL, MetaLight, PressLight and CoLight in both left-2 / straight-4 /right-1 and left-1 / straight-2 /right-1 scenarios. It indicates the method can handle the intersections with different lane configurations well. To validate that the method is robust to different traffic simulation platforms, several experiments are conducted on SUMO 8 . For fair comparisons, 4 recent methods which provides the source code on SUMO are evaluated at the same setting, including:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D EXPERIMENTS ON SUMO</head><p>• MA2C <ref type="bibr" target="#b43">[44]</ref>: a fully scalable and decentralized MARL alogrithm for the deep RL agent: advantage actor critic (A2C), within the context of adaptive traffic signal control (ATSC).</p><p>• IA2C <ref type="bibr" target="#b43">[44]</ref>: an independent advantage actor critic (A2C) method in multi-agent scenario.</p><p>• IQL-LR <ref type="bibr" target="#b73">[74]</ref>: a control algorithm based on dynamic programming, using an approximation ot the value function of the dynamic programming and reinforcement learning to update the approximation.</p><p>• IQL-DNN <ref type="bibr" target="#b43">[44]</ref>: the same as IQL-LR but uses DNN for fitting the Q-function.</p><p>We choose a 5x5 large traffic grid as the experimental scenario, which is the same as <ref type="bibr" target="#b43">[44]</ref>. The scenario is formed by two-lane arterial streets with speed limit 20m/s and one-lane avenues with speed limit 11m/s. The evaluation metrics are provided by SUMO as follows:</p><p>• Reward The rewards received by all agents across the entire simulation time.</p><p>• Avg. Queue Length Average queue length is the average numbers of vehicles in the queue over all intersections. If a vehicle is waiting at an intersection and the speed less than 0.1m/s, we think the car is on the queue.</p><p>• Avg. intersection delay If a vehicle is waiting at an intersection and the speed less than 0.1m/s, the extra time the vehicle stays at the intersection is the delay time. We calculate the average time over all vehicles, then we get the avg. intersection delay.</p><p>8. <ref type="url" target="http://sumo.dlr.de/index.html">http://sumo.dlr.de/index.html</ref> </p><p>• Avg. vehicle speed The average speed of all vehicles in the road network for the entire simulation time.</p><p>• Trip completion flow Trip completion flow is the number of vehicles that complete the trip. Trip completion flow is calculated by dividing the total number of vehicles that complete the trip during the entire simulation time by the horizon.</p><p>• Trip delay Trip delay is the extra time wasted for all vehicles that complete the trip in the road network. Tab. 8 lists the comparative results on the common testing mode over SUMO, and it is evident that: In general, MetaVIM performs better than advantage actor critic and Q learning in both decentralized MARL and independent control, and it indicates the advantage of the MetaVIM in various evaluation criterias. MetaVIM achieves higher reward in evaluation. Moreover, MetaVIM is superior to other methods clearly in average queue length, average intersection delay, average vehicle speed, trip completion flow and trip delay, which demonstrates the effectiveness of the method in SUMO platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F RL-BASED TRAFFIC SIGNAL CONTROL SURVEY</head><p>A survey on RL-based traffic signal control methods is shown in Tab. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 10: RL-based Methods Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classes</head><p>Methods Descriptions MARLIN-ATSC <ref type="bibr" target="#b31">[32]</ref> real-time adjustment of signal timing plans based on traffic fluctuations, with each agent coordinating with adjacent intersections to generate actions tabular Q-learning holonic Q-learning <ref type="bibr" target="#b32">[33]</ref> the traffic network is divided into multiple zones with two phases of control DWL <ref type="bibr" target="#b33">[34]</ref> collaboration-based agent self-optimizes for multiple strategies Q-learning based <ref type="bibr" target="#b34">[35]</ref> apply traditional Q-learning method to multi-agent systems Intellilight <ref type="bibr" target="#b11">[12]</ref> use a variety of different combinations of traffic indicators as the states RLTSC <ref type="bibr" target="#b74">[75]</ref> use current phase index, phase duration, and queue length as states single agent AttendLight <ref type="bibr" target="#b41">[42]</ref> a single general model is designed, and two attention models are used to deal with multiple topologies MT-GAD <ref type="bibr" target="#b36">[37]</ref> use a group attention structure to reduce the number of required parameters and to achieve a better generalizability PG Time EWMA <ref type="bibr" target="#b75">[76]</ref> a policy gradient method based on episodic conditions and a timedependent baseline to learn an optimal policy for traffic signal control in congested conditions LIT <ref type="bibr" target="#b37">[38]</ref> analyze which traffic indicators are suitable as states or reward FRAP <ref type="bibr" target="#b38">[39]</ref> introduce phase competition to select a more suitable phase isolated multi-agent DemoLight <ref type="bibr" target="#b39">[40]</ref> demonstrations can be collected from classical methods to accelerate learning PressLight <ref type="bibr" target="#b12">[13]</ref> combine the traditional traffic method MaxPressure with RL technology together</p><p>MPLight <ref type="bibr" target="#b40">[41]</ref> experiment on a large dataset using a shared strategy PlanLight <ref type="bibr" target="#b42">[43]</ref> learn from the demonstration of the rollouts centralized DQN-TP <ref type="bibr" target="#b15">[16]</ref> combination of DQN method and transfer algorithm to overcome instability problem max-plus <ref type="bibr" target="#b14">[15]</ref> coordination by max-plus algorithm to obtain optimal joint actions LQF <ref type="bibr" target="#b76">[77]</ref> a distributed static control approach that integrates adaptive RL systems and LQF NFQI with GCNN <ref type="bibr" target="#b44">[45]</ref> the traffic characteristics of long-distance roads are considered by using graph neural network decentralized CoLight <ref type="bibr" target="#b45">[46]</ref> use graph convolution and attention mechanism to model the neighbor information HiLight <ref type="bibr" target="#b47">[48]</ref> use hierarchical reinforcement learning to allow agents to optimize different short-term sub-goals MA2C <ref type="bibr" target="#b43">[44]</ref> use neighbor information to improve observability and reduce the learning difficulty of local agents MetaLight <ref type="bibr" target="#b13">[14]</ref> a value-based meta reinforcement learning method via parameter initialization metalearning GeneraLight <ref type="bibr" target="#b48">[49]</ref> generate diversified traffic flows using GAN, and then improve generalization ability through clustering ModelLight <ref type="bibr" target="#b35">[36]</ref> a model-based meta-reinforcement learning framework, meta-learning method can improve data efficiency and reduce the number of interactions required with real-world environments</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration of the multi-intersection control scenario where policies and rewards are affected by neighboring policies, i.e., the instability of the multi-agent scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>in 1 &lt;</head><label>1</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " J K 8 6 T a D 4 N u g k 3 5 + T 9 j h 4 U f K tZ d U = " &gt; A A A B 8 X i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v i q s y I q M u C G 5 c V 7 A P b s W T S 2 z Y 0 k x m S j F C G + Q s 3 L h R x 6 9 + 4 8 2 9 M 2 1 l o 6 4 H A 4 Z x 7 y T 0 n i A X X x n W / n c L K 6 t r 6 R n G z t L W 9 s 7 t X 3 j 9 o 6 i h R D B s s E p F q B 1 S j 4 B I b h h u B 7 V g h D Q O B r W B 8 M / V b T 6 g 0 j + S 9 m c T o h 3 Q o + Y A za q z 0 I B 5 T L r N e 6 m W 9 c s W t u j O Q Z e L l p A I 5 6 r 3 y V 7 c f s S R E a Z i g W n c 8 N z Z + S p X h T G B W 6 i Y a Y 8 r G d I g d S y U N U f v p 7 O K M n F i l T w a R s k 8 a M l N / b 6 Q 0 1 H o S B n Y y p G a k F 7 2 p + J / X S c z g 2 r e Z 4 s S g Z P O P B o k g J i L T + K T P F T I j J p Z Q p r i 9 l b A R V Z Q Z W 1 L J l u A t R l 4 m z f O q d 1 n 1 7 i 4 q t d O 8 j i I c w T G c g Q d X U I N b q E M D G E h 4 h l d 4 c 7 T z 4 r w 7 H / P R g p P v H M I f O J 8 / y 4 u Q 6 Q = = &lt; / l a t e x i t &gt; l l a t e x i t s h a 1 _ b a s e 6 4 = " p n L 0 C V u 2 G B a 7 G j B w 9 E s U O a y Z B e 0 = " &gt; A A A B 8 X i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y F F c l Z k i 6 r L g x m U F + 8 B 2 L J k 0 b U M z m S G 5 I 5 R h / s K N C 0 X c + j f u / B v T d h b a e i B w O O d e c s 8 J Y i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k o 0 4 w 0 W y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>l 4 m z W r F u 6 x 4 d x f l 2 l l e R w G O 4 Q T O w Y M r q M E t 1 K E B D B Q 8 w y u 8 O c Z 5 c d 6 d j / n o i p P v H M E f O J 8 / z R C Q 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N V V Y Z c 1 2 O R v 1 p a m y D A W N a e + 2 1 l s = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b F U 0 l E q s e C F 4 8 V 7 A e k s W y 2 m 3 b p Z j f s T o Q S 8 j O 8 e F D E q 7 / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B H c g O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U M S r V l L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>out 6 &lt;</head><label>6</label><figDesc>5 p Z 8 a l h A 6 I S P m W y p J z E y Q z U / O 8 Z l V h j h S 2 p Y E P F d / T 2 Q k N m Y a h 7 Y z J j A 2 y 9 5 M / M / z U 4 h u g o z L J A U m 6 W J R l A o M C s / + x 0 O u G Q U x t Y R Q z e 2 t m I 6 J J h R s S h U b g r f 8 8 i r p X N a 9 R t 2 7 v 6 o 1 z 4 s 4 y u g E n a I L 5 K F r 1 E R 3 q I X a i C K F n t E r e n P A e X H e n Y 9 F a 8 k p Z o 7 R H z i f P 8 B h k X k = &lt; / l a t e x i t &gt; l l a t e x i t s h a 1 _ b a s e 6 4 = " P o p h S M 7 K D E x i K C i v l S B s O Y 6 a 1 / A = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b F U 0 l E r M e C F 4 8 V 7 A e k s W y 2 m 3 b p Z j f s T o Q S 8 j O 8 e F D E q 7 / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B H c g O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U M S r V l L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>out 7 &lt;</head><label>7</label><figDesc>r e n P A e X H e n Y 9 F a 8 k p Z o 7 R H z i f P 8 H m k X o = &lt; / l a t e x i t &gt; l l a t e x i t s h a 1 _ b a s e 6 4 = " Y L / g p 7 e F f m 7 + M r Q 8 W R T 6 9 F M m 2 P E = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b F U 0 l E t M e C F 4 8 V 7 A e k s W y 2 m 3 b p Z j f s T o Q S 8 j O 8 e F D E q 7 / G m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B H c g O t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U M S r V l L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>out 8 &lt;out 9 &lt;</head><label>89</label><figDesc>0 L u v e d d 2 7 v 6 o 1 z 4 s 4 y u g E n a I L 5 K E b 1 E R 3 q I X a i C K F n t E r e n P A e X H e n Y 9 F a 8 k p Z o 7 R H z i f P 8 N r k X s = &lt; / l a t e x i t &gt; l l a t e x i t s h a 1 _ b a s e 6 4 = " L 4 x + 8 n 7 5 O G P q 8 k CE 7 s H R / p H m R J s = " &gt; A A A B 8 n i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V x V R I R H 7 u C G 5 c V 7 A P S W C b T S T t 0 M h N m b o Q S 8 h l u X C j i 1 q 9 x 5 9 8 4 b b P Q 1 g M X D u f c y 7 3 3 h I n g B l z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D t l G p p q x F l V C 6 G x L D B J e s B R w E 6 y a a k T g U r B O O b 6 d + 5 4 l p w 5 V 8 g E n C g p g M J Y 8 4 J W A l X z x m K o W 8 n 9 3 k / W r N r b s z 4 G X i F a S G C j T 7 1 a / e Q N E 0 Z h K o I M b 4 n p t A k B E N n A q W V 3 q p Y Q m h Y z J k v q W S xM w E 2 e z k H J 9 Y Z Y A j p W 1 J w D P 1 9 0 R G Y m M m c W g 7 Y w I j s + h N x f 8 8 P 4 X o O s i 4 T F J g k s 4 X R a n A o P D 0 f z z g m l E Q E 0 s I 1 d z e i u m I a E L B p l S x I X i L L y + T 9 n n d u 6 x 7 9 x e 1 x m k R R x k d o W N 0 h j x 0 h R r o D j V R C 1 G k 0 D N 6 R W 8 O O C / O u / M x b y 0 5 x c w h + g P n 8 w f E 8 J F 8 &lt; / l a t e x i t &gt; l l a t e x i t s h a 1 _ b a s e 6 4 = " F Y p 5 7 m q j L 6 c J Q O x L W h z 2 r H D G U L M = " &gt; A A A B 8 3 i c b V B N S w M x E M 3 6 W e t X 1 a O X Y F E 8 l V 0 R 9 V j w 4 r G C / Y B 2 L d k 0 2 4 Z m k y W Z C G X Z v + H F g y J e / T P e / D e m 7 R 6 0 9 c H A 4 7 0 Z Z u Z F q e A G f P / b W 1 l d W 9 / Y L G 2 V t 3 d 2 9 / Y r B 4 c t o 6 y m r E m V U L o T E c M E l 6 w J H A T r p J q R J B K s H Y 1 v p 3 7 7 i W n D l X y A S c r C h A w l j z k l 4 K S e e M y U h b y f B X 7 e r 1 T 9 m j 8 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>out 10 &lt;out 11 &lt;Fig. 2 :</head><label>10112</label><figDesc>Fig. 2: Illustration of incoming, outgoing lanes and 4-phase diagram.</figDesc><graphic coords="4,216.86,177.56,59.00,53.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>6 m x b w p 4 e t S + j 8 5 3 y 0 5 + 6 X K a a V 4 e D C q I 0 d W y R r Z I A 6 p k k N y T E 5 I n X B y S + 7 J I 3 m y 7 q w H 6 9 l 6 + Y y O W a O Z F f I D 1 u s H v y 2 e K A = = &lt; / l a t e x i t &gt; kr t+1 rj,t+1 k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 P T B V e m m 2 h 8 7 b d j d 1 Y k g g B 7 0 g T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>1 5 r d m r f t 5 X c c W 2 S c H 5 D 3 x S Y d 0 y V f S I 3 3 C y S X 5 Q X 6 S X 8 6 V c + 3 8 d m 5 W 0 Y q z n t k j D + D c / g O 1 f J 4 i &lt; / l a t e x i t &gt; ko t+1 õj,t+1 k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f i 8 v O B 6 x H z v e t I l X 5 c w a S 4 m q E z I = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k b x 4 m O n 5 s F x x 7 G b D a b l V 5 N j O E o Z U 6 8 1 a v Y r c X K l A j s 6 w / D 4 Y x S S N q N C E Y 6 X 6 r p N o P 8 N S M 8 L p v D R I F U 0 w m e I x 7 R s q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>T b 5 l E w I X 5 + i / 8 n d h e 0 2 7 N p N r d K + z O M o w g m c w j m 4 0 I Q 2 X E M H P C D A 4 A G e 4 N k S 1 q P 1 Y r 2 u W g t W P n M M P 2 C 9 f Q J f S 4 8 P &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t U p T r 0 g S e 1 R p 9 M u g B L V p c e s r w q U</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>r 1 0 e e 4 c n x l l h I N I m Z K A l + r 3 i Z S E W s 9 C 3 3 S G B C b 6 t 7 c Q // L 6 C Q Q N L + U y T o B J u l o U J A J D h B e / 4 x F X j I K Y G U K o 4 u Z W T C d E E Q o m o Y I J 4 e t T / D / p X N p u z a 7 c V k r N q y y O P D p B p + g c u a i O m u g G t V A b U T R F D + g J P V u x 9 W i 9 W K + r 1 p y V z R y j H 7 D e P g F O F Y + N &lt; / l a t e x i t &gt;o t+1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f i 8 v O B 6 x H z v e t I l X 5 c w a S 4 m q E z I = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k b x 4 m O n 5 s F x x 7 G b D a b l V 5 N j O E o Z U 6 8 1 a v Y r c X K l A j s 6 w / D 4 Y x S S N q N C E Y 6 X 6 r p N o P 8 N S M 8 L p v D R I F U 0 w m e I x 7 R s q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>t 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 =</head><label>114</label><figDesc>r N d V a 8 7 K Z o 7 R D 1 h v n 1 E h j 4 8 = &lt; / l a t e x i t &gt; o " f S W S U 7 E K U 4 z G V q L M m G B q Z b 7 c r g 4 = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k T w 8 z P R 8 W K 4 4 d r P h t N w q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>T b 5 l E w I X 5 + i / 8 n d h e 0 2 7 N p N r d K + z O M o w g m c w j m 4 0 I Q 2 X E M H P C D A 4 A G e 4 N k S 1 q P 1 Y r 2 u W g t W P n M M P 2 C 9 f Q J f S 4 8 P &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f i 8 v O B 6 x H z v e t I l X 5 c w a S 4 m q E z I = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k b x 4 m O n 5 s F x x 7 G b D a b l V 5 N j O E o Z U 6 8 1 a v Y r c X K l A j s 6 w / D 4 Y x S S N q N C E Y 6 X 6 r p N o P 8 N S M 8 L p v D R I F U 0 w m e I x 7 R s q c E S V n y 2 P n a M z o 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>a s f O Y Y f s B 6 + w R 0 u 4 8 d &lt; / l a t e x i t &gt; o t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f S W SU 7 E K U 4 z G V q L M m G B q Z b 7 c r g 4 = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k T w 8 z P R 8 W K 4 4 d r P h t N w q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>T b 5 l E w I X 5 + i / 8 n d h e 0 2 7 N p N r d K + z O M o w g m c w j m 4 0 I Q 2 X E M H P C D A 4 A G e 4 N k S 1 q P 1 Y r 2 u W g t W P n M M P 2 C 9 f Q J f S 4 8 P &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t U p T r 0 g S e 1 R p 9 M u g B L V p c e s r w q U = "&gt; A A A B 7 n i c d V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 U Q h J D Y 5 8 J F w Y 3 L C v Y B b S i T 6 a Q d O p m E m R u h h H 6 E G x e K u P V 7 3 P k 3 T t s I K n r g w u G c e 7 n 3 H j 8 W X I P j f F i 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N F R o i h r 0 0 h E q u c T z Q S X r A 0 c B O v F i p H Q F 6 z r T 6 8 X f v e e K c 0 j e Q e z m H k h G U s e c E r A S N 1 o m M K F O x 8 W S 4 5 d r z k N t 4 w d 2 1 n C k H K 1 X q m W s Z s p J Z S h N S y + D 0 Y R T U I m g Q q i d d 9 1 Y v B S o o B T w e a F Q a J Z T O i U j F n f U E l C p r 1 0 e e 4 c n x l l h I N I m Z K A l + r 3 i Z S E W s 9 C 3 3 S G B C b 6 t 7 c Q / / L 6 C Q Q N L + U y T o B J u l o U J A J D h B e / 4 x F X j I K Y G U K o 4 u Z W T C d E E Q o m o Y I J 4 e t T / D / p X N p u z a 7 c V k r N q y y O P D p B p + g c u a i O m u g G t V A b U T R F D + g J P V u x 9 W i 9 W K + r 1 p y V z R y j H 7 D e P g F O F Y + N &lt; / l a t e x i t &gt; o t+1&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f i 8 v O B 6 x H z v e t I l X 5 c w a S 4 m q E z I = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s<ref type="bibr" target="#b4">5</ref> 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k b x 4 m O n 5 s F x x 7 G b D a b l V 5 N j O E o Z U 6 8 1 a v Y r c X K l A j s 6 w / D 4 Y x S S N q N C E Y 6 X 6 r p N o P 8 N S M 8 L p v D R I F U 0 w m e I x 7 R s q c E S V n y 2 P n a M z o 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>a s f O Y Y f s B 6 + w R 0 u 4 8 d &lt; / l a t e x i t &gt; o t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f S W SU 7 E K U 4 z G V q L M m G B q Z b 7 c r g 4 = " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k T w 8 z P R 8 W K 4 4 d r P h t N w q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>T b 5 l E w I X 5 + i / 8 n d h e 0 2 7 N p N r d K + z O M o w g m c w j m 4 0 I Q 2 X E M H P C D A 4 A G e 4 N k S 1 q P 1 Y r 2 u W g t W P n M M P 2 C 9 f Q J f S 4 8 P &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K d d w a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>e k B P 6 N l R z q P z 4 r y u W j P O e u Y U / Y D z 9 g m N U I 8 u &lt; / l a t e x i t &gt; r t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m X m u q p m e n m h h b 4 + m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>6 B x 5 q I r q 6 B o 1 U B M x d I c e 0 B N 6 d p T z 6 L w 4 r 8 v W j L O a O U Y / 4 L x 9 A g R i j 2 0 = &lt; / l a t e x i t &gt; ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f i 8 v O B 6 x H z v e t I l X 5 c w a S 4 m q E z I= " &gt; A A A B 7 H i c d V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 j s 5 8 F D w Y v H C s Y W 2 l A 2 2 0 2 7 d L M J u x u h h P 4 G L x 4 U 8 e o P 8 u a / c d t G U N E H A 4 / 3 Z p i Z F y S c K e 0 4 H 1 Z h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e 3 a k 4 l Y R 6 J O a x 7 A V Y U c 4 E 9 T T T n P Y S S X E U c N o N p l c L v 3 t P p W K x u N W z h P o R H g s W M o K 1 k b x 4 m O n 5 s F x x 7 G b D a b l V 5 N j O E o Z U 6 8 1 a v Y r c X K l A js 6 w / D 4 Y x S S N q N C E Y 6 X 6 r p N o P 8 N S M 8 L p v D R I F U 0 w m e I x 7 R s q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>r a y u r W 9 s Z r a y 2 z u 7 e / u 5 g 8 O W j h L F e J N F M l I d j 2 o u R c i b I E D y T q w 4 D T z J 2 9 7 4 c u a 3 7 7 n S I g p v Y B J z N 6 D D U P i C U T B S h / b T 2 3 M M 0 3 4 u T 2 x S L l W d A i Z 2 s U Z I s W x I p U p q h T J 2 b D J H H i 3 R 6 O f e e 4 O I J Q E P g U m q d d c h M b g p V S C Y 5 N N s L 9 E 8 p m x M h 7 x r a E g D r t 1 0 f u 8 U n x p l g P 1 I m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>r b u r E f r x X p d t K 5 Y y 5 k j 9 A P W 2 y f 8 F o / x &lt; / l a t e x i t &gt; a j,t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F S M 4 9 D N 0 / 9 F F O 2 g E I a / B e w g v 8 A g = " &gt; A A A B 73 i c d V D J S g N B E O 1 x j X G L e v T S G A Q P M v R k P 3 g I e P E Y w S y Q D K G n 0 5 O 0 6 V n s r h H C k J / w 4 k E R r / 6 O N / / G z i K o 6 I O C x 3 t V V N X z Y i k 0 E P J hr a y u r W 9 s Z r a y 2 z u 7 e / u 5 g 8 O W j h L F e J N F M l I d j 2 o u R c i b I E D y T q w 4 D T z J 2 9 7 4 c u a 3 7 7 n S I g p v Y B J z N 6 D D U P i C U T B S h / b T 2 3 M M 0 3 4 u T 2 x S L l W d A i Z 2 s U Z I s W x I p U p q h T J 2 b D J H H i 3 R 6 O f e e 4 O I J Q E P g U m q d d c h M b g p V S C Y 5 N N s L 9 E 8 p m x M h 7 x r a E g D r t 1 0 f u 8 U n x p l g P 1 I m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>r b u r E f r x X p d t K 5 Y y 5 k j 9 A P W 2 y f 8 F o / x &lt; / l a t e x i t &gt; a j,t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F S M 4 9 D N 0 / 9 F F O 2 g E I a / B e w g v 8 A g = " &gt; A A A B 73 i c d V D J S g N B E O 1 x j X G L e v T S G A Q P M v R k P 3 g I e P E Y w S y Q D K G n 0 5 O 0 6 V n s r h H C k J / w 4 k E R r / 6 O N / / G z i K o 6 I O C x 3 t V V N X z Y i k 0 E P J hr a y u r W 9 s Z r a y 2 z u 7 e / u 5 g 8 O W j h L F e J N F M l I d j 2 o u R c i b I E D y T q w 4 D T z J 2 9 7 4 c u a 3 7 7 n S I g p v Y B J z N 6 D D U P i C U T B S h / b T 2 3 M M 0 3 4 u T 2 x S L l W d A i Z 2 s U Z I s W x I p U p q h T J 2 b D J H H i 3 R 6 O f e e 4 O I J Q E P g U m q d d c h M b g p V S C Y 5 N N s L 9 E 8 p m x M h 7 x r a E g D r t 1 0 f u 8 U n x p l g P 1 I m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>r b u r E f r x X p d t K 5 Y y 5 k j 9 A P W 2 y f 8 F o / x &lt; / l a t e x i t &gt; a j,t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d o k s I f 5 P C d b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Visualisation of the latent space. (a) and (b) denote the mean and variance of the latent variable respectively.The latent variable has 5 dimensions, each line is one latent dimension, the black line is the average.</figDesc><graphic coords="7,57.90,212.42,232.20,82.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The illustration of the road networks. The first row shows the road networks of Jinan (China), Hangzhou (China) and New York (USA), containing 12, 16 and 48 traffic signals respectively, and the second row shows the road network of Shenzhen containing 33 traffic signals.</figDesc><graphic coords="9,48.00,43.70,252.84,184.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: Overview of the Shenzhen (China) road network. There are total 33 traffic signal intersections across the area.</figDesc><graphic coords="10,65.27,36.28,214.51,139.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1</head><label>1</label><figDesc>MetaVIM: Meta-training Phase Require: A set of meta-training tasks {κ i } i∈N drawn from intersections N = 1, 2, . . . , N in a multi-agent scenario, number of training episodes E Initialize policy replay buffers B π Initialize mVAE replay buffers B mVAE Initialize encoder e ψ , decoders p ϕr , p ϕ , p ϕr , p ϕõ Initialize policy π θ while not done do for episodes= 1, 2, . . . , E do {Data collection} Compute latent variable m from the current rollouts Add latent variable m to B π for steps in training steps do {Training} Take action according to π θ Get neighbor actions {a j } j=1,...,J Act to the environment and get environmental reward r Compute intrinsic reward r int using Eq. (12) Update latent variable m by e ψ Add trajectories τ and latent variable m to B mVAE Add trajectories τ to B π Train encoder e ψ and decoders p ϕr , p ϕ , p ϕr , p ϕõ by maximising ELBO in Eq. (26) Train policy π θ by maximizing reward in Eq. ∼ π θ a t | o t , e ψ (m | τ :t ) . Based on τ :t , at any given time step t, our mVAE learning objective is thus to maximise E τ:t∼ρ log p ϕr,ϕr,ϕo,ϕõ (τ :t ) ,</figDesc><table><row><cell>(22)</cell></row><row><cell>clean up B π end for</cell></row><row><cell>end for</cell></row><row><cell>end while</cell></row><row><cell>Algorithm 2 MetaVIM: Meta-testing Phase</cell></row><row><cell>Require: A set of meta-testing tasks {κ i } i∈M drawn from intersections M = 1, 2, . . . , M in a multi-agent scenario, number of testing episodes E</cell></row><row><cell>Load policy model π θ</cell></row><row><cell>Load encoder model e ψ</cell></row><row><cell>for episodes=1,2, ...,E do {Rollouts}</cell></row><row><cell>Compute latent variable m from the current rollouts</cell></row><row><cell>for step in testing steps do</cell></row><row><cell>Take action according to π θ</cell></row><row><cell>Act to the environment</cell></row><row><cell>Compute latent variable m by e ψ</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>for step in testing steps do {Testing}</cell></row><row><cell>Take action according to π θ (a | o, m) end for</cell></row><row><cell>where a t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>log p ϕo (o t+1 ) + log p ϕõ (o t+1 ) E τ:t∼ρ, e ψ (m|τ:t) log p ϕr (r t+1 |m) + log p ϕr (r t+1 |m) + log p ϕo (o t+1 |m) + log p ϕõ (o t+1 |m)log e ψ (m|τ :t ) ϕr (r t+1 ) + log p ϕr (r t+1 ) + log p ϕo (o t+1 ) + log p ϕõ (o t+1 )) -KL(e ψ (m|τ :t )||q(m)). = ELBO(ψ, ϕ r , ϕ r , ϕ o , ϕ õ|τ :t ).</figDesc><table><row><cell cols="6">= E τ:t∼ρ log E e ψ (m|τ:t) [</cell><cell cols="2">p ϕr (r t+1 , m) e ψ (m|τ :t )</cell></row><row><cell>+</cell><cell cols="3">p ϕr (r t+1 , m) e ψ (m|τ :t )</cell><cell cols="3">+</cell><cell>p ϕo (o t+1 , m) e ψ (m|τ :t )</cell><cell>+</cell><cell>p ϕõ (o t+1 , m) e ψ (m|τ :t )</cell><cell>]</cell></row><row><cell cols="5">≥ E τ:t∼ρ, e ψ (m|τ:t) log</cell><cell cols="3">p ϕr (r t+1 , m) e ψ (m|τ :t )</cell><cell>+ log</cell><cell>p ϕr (r t+1 , m) e ψ (m|τ :t )</cell></row><row><cell cols="2">+ log</cell><cell cols="6">p ϕo (o t+1 , m) e ψ (m|τ :t )</cell><cell>+ log</cell><cell>p ϕõ (o t+1 , m) e ψ (m|τ :t )</cell></row><row><cell cols="8">t-1 t ′ =0 (log p Then the mVAE learning objective is thus to maximise: = = E e ψ (m|τ:t)</cell></row><row><cell cols="8">ELBO(ψ, ϕ r , ϕ r , ϕ o , ϕ õ|τ :t )</cell></row><row><cell></cell><cell></cell><cell>t-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">= E e ψ (m|τ:t)</cell><cell cols="5">(log p ϕr (r τ t+1 ) + log p ϕr (r τ t+1 )</cell></row><row><cell></cell><cell></cell><cell>t=0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">+ log p ϕo (o τ t+1 ) + log p ϕõ (o τ t+1 ))</cell></row><row><cell></cell><cell></cell><cell cols="6">-KL(e ψ (m|τ :t )||q(m)).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1. http://sumo.dlr.de/index.html 2. https://traffic-signal-control.github.io/ 3. https://github.com/zhuliwen/RoadnetSZ</figDesc><table><row><cell>Traffic Flow</cell></row><row><cell>Traffic Lights</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 1 :</head><label>1</label><figDesc>Arrival rate of real-world traffic dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Arrival rate (vehicles/300s)</cell></row><row><cell>Dataset</cell><cell># Intersections</cell><cell>Mean</cell><cell cols="3">Std Max Min</cell></row><row><cell cols="2">Hangzhou 16 (4 × 4)</cell><cell>248.58</cell><cell>42.25</cell><cell>333</cell><cell>212</cell></row><row><cell>Jinan</cell><cell>12 (4×3)</cell><cell cols="2">524.58 102.91</cell><cell>672</cell><cell>256</cell></row><row><cell>NewYork</cell><cell>48 (16×3)</cell><cell>235.33</cell><cell>5.84</cell><cell>244</cell><cell>224</cell></row><row><cell>Shenzhen</cell><cell>33 (Non-grid)</cell><cell>147.92</cell><cell>79.35</cell><cell>255</cell><cell>22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 :</head><label>2</label><figDesc>Data statistics of synthetic traffic dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Arrival rate (vehicles/300s)</cell></row><row><cell>Dataset</cell><cell>Total vehicles</cell><cell>Mean</cell><cell>Std</cell><cell>Max</cell><cell>Min</cell></row><row><cell>mixed l</cell><cell>2570</cell><cell>214.17</cell><cell>198.41</cell><cell>600</cell><cell>60</cell></row><row><cell>mixed h</cell><cell>4770</cell><cell>397.50</cell><cell>420.75</cell><cell>1200</cell><cell>60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>Route statistics of real-world traffic dataset</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Route (lanes/route)</cell><cell></cell></row><row><cell>Dataset</cell><cell># Intersections</cell><cell>Mean</cell><cell>Std</cell><cell>Max</cell><cell>Min</cell></row><row><cell>Hangzhou</cell><cell>16 (4 × 4)</cell><cell>4.65</cell><cell>2.15</cell><cell>16</cell><cell>2</cell></row><row><cell>Jinan</cell><cell>12 (4×3)</cell><cell>4.37</cell><cell>1.88</cell><cell>17</cell><cell>2</cell></row><row><cell>NewYork</cell><cell>48 (16×3)</cell><cell>10.00</cell><cell>4.63</cell><cell>21</cell><cell>3</cell></row><row><cell>Shenzhen</cell><cell>33 (Non-grid)</cell><cell>7.57</cell><cell>3.91</cell><cell>41</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>Experimental performance on intersections with different topologies number of left-turn lanes, straight lanes and right-turn lanes are 1, 2, and 1 respectively. Similarly, left-1 / straight-2 /right-1 indicates the roads in the network is formed by 4 lanes, where the number of left-turn lanes, straight lanes and right-turn lanes are 1, 1, and 1 respectively.</figDesc><table><row><cell>Model</cell><cell>l-2 / s-4 / r-1</cell><cell>l-1 / s-2 / r-1</cell></row><row><cell>Random</cell><cell>828.00</cell><cell>866.33</cell></row><row><cell>MaxPressure</cell><cell>829.06</cell><cell>867.16</cell></row><row><cell>Fixedtime</cell><cell>1591.83</cell><cell>1529.09</cell></row><row><cell>FixedtimeOffset</cell><cell>830.85</cell><cell>867.97</cell></row><row><cell>SOTL</cell><cell>1258.67</cell><cell>1285.66</cell></row><row><cell>Individual RL</cell><cell>835.64</cell><cell>860.37</cell></row><row><cell>MetaLight</cell><cell>775.65</cell><cell>790.00</cell></row><row><cell>PressLight</cell><cell>798.87</cell><cell>819.36</cell></row><row><cell>CoLight</cell><cell>501.37</cell><cell>516.55</cell></row><row><cell>MetaVIM</cell><cell>485.36</cell><cell>490.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Verification on the SUMO simulation platform</figDesc><table><row><cell>Metrics</cell><cell>MA2C</cell><cell>IA2C</cell><cell>IQL-LR</cell><cell>IQL-DNN</cell><cell>MetaVIM</cell></row><row><cell>reward</cell><cell>-366.13</cell><cell>-1062.23</cell><cell>-1057.19</cell><cell>-2619.42</cell><cell>-298.58</cell></row><row><cell>avg. queue length [veh]</cell><cell>1.88</cell><cell>3.74</cell><cell>2.78</cell><cell>4.48</cell><cell>3.97</cell></row><row><cell>avg. intersection delay [s/veh]</cell><cell>11.98</cell><cell>52.52</cell><cell>74.56</cell><cell>166.03</cell><cell>10.56</cell></row><row><cell>avg. vehicle speed [m/s]</cell><cell>2.40</cell><cell>1.60</cell><cell>3.18</cell><cell>1.48</cell><cell>2.86</cell></row><row><cell>trip completion flow [veh/s]</cell><cell>0.63</cell><cell>0.38</cell><cell>0.73</cell><cell>0.23</cell><cell>0.79</cell></row><row><cell>trip delay [s]</cell><cell>322.81</cell><cell>560.61</cell><cell>186.67</cell><cell>477.43</cell><cell>157.23</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>variables. That is, given a novel or unseen task, the taskspecific information would be represented as latent variable rather than acting as distractors. Hence, the latent variable helps to learn the across-task shared policy function better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Ablations</head><p>To better validate the contribution of each component, four variants of MetaVIM are evaluated on the common testing mode in the Shenzhen road network, including:</p><p>• Base only keeps the policy network and removes the mVAE and latent variable m. • Base+m where the encoder and m are introduced.</p><p>Base+m contains policy network and encoder, which keeps latent variable but discards intrinsic rewards. • Base+m+tran_RS contains policy network, encoder and transition decoders but discards reward decoders. Transition decoders p ϕ0 (o t+1 ) and p ϕõ (õ t+1 ) are used and only ∥o t+1õj,t+1 ∥ is remained in Eq. 17 to form the intrinsic reward. • Base+m+rew_RS contains policy network, encoder and reward decoders but discards transition decoders. Reward encoders p ϕr (r t+1 ) and p ϕr (r t+1 ) are used and only ∥r t+1rj,t+1 ∥ is remained in Eq. 17 to form the intrinsic reward.</p><p>Overall, MetaVIM contains the whole modules: policy network, encoder and decoders.</p><p>The qualitative evaluation results are listed in Tab. 4 and the learning curves are shown in Appendix A. We can obtain the following findings: 1) Among these 5 models, the performance of Baseline is the worst. The reason is that it is hard to learn the effective decentralized policy independently in the multi-agent traffic signal control task, where one agent's reward and transition are affected by its neighbors.</p><p>2) Compared with the baseline, the improvement of Baseline + m demonstrates the effectiveness of the latent variable m. The latent variable not only identifies the POMDPspecific information and helps to learn POMDP-shared policy network, but also trades off the exploration and exploitation during the RL procedure. 3) The tran_RS and rew_RS are both effective because each of them encourages the policy learning stable. Compared to them, the superiority of MetaVIM indicates tran_RS and rew_RS are complementary to each other. 4) Overall, all of the proposed components contribute positively to the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORKS</head><p>In this paper, we propose a novel Meta RL method MetaVIM for multi-intersection traffic signal control, which can make </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A LEARNING CURVES</head><p>Fig. <ref type="figure">9</ref>: Learning curves of MetaVIM and baselines in the Shenzhen road network map. The first, second and third columns correspond to the real, mixed l and mixed h configurations respectively. The final model MetaVIM outperforms all other models and shows great stability. In addition, the learning curves of ablations indicate that each component contributes positively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B IMPLEMENTATION DETAILS OF METAVIM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C EXPERIMENTS ON DIFFERENT LANE CONFIGURATIONS</head><p>To validate that the method is robust to diverse lane configurations, we modify the 4x4 Hangzhou road network using different lane configurations: left-2 / straight-4 /right-1 indicates the roads in the network is formed by 7 lanes, where the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E SCALABILITY VALIDATION</head><p>To validate scalability of the method, an additional experiment is conducted on a public dataset of 196 intersections in New York City 9 , as illustrated in Figure <ref type="figure">10</ref>. It is one of the largest public dataset in Cityflow to our knowledge. As shown in Tab. 9, MetaVIM achieves better results than compared methods, which demonstrates the superior scalability of MetaVIM. The reason is that MetaVIM is a decentralized method and doesn't need the joint action of all agents and full state. Hence, MetaVIM could avoid the dimensional explosion of large scale of agents. In addition, as the number of agents increases, more sample data will be collected. It leads to a significant increase in the number of training samples, rather than an increase in dimensionality. Therefore, MetaVIM could scale well.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Traffic signal timing manual</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koonce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rodegerdts</surname></persName>
		</author>
		<idno>FHWA- HOP-08-024</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>United States. Federal Highway Administration</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The max-pressure controller for arbitrary networks of signalized intersections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Dynamic Network Modeling in Complex Transportation Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Urban traffic light control via active multi-agent communication and supply-demand modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>early access</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to delay in ride-sourcing systems: A multi-agent deep reinforcement learning framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2280" to="2292" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatio-temporal meta learning for urban traffic prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1462" to="1476" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal capsule-based reinforcement learning for mobility-on-demand coordination</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1446" to="1461" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combinatorial optimization meets reinforcement learning: Effective taxi order dispatching at large-scale</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>early access</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for spatio-temporal data mining: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3681" to="3700" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting interpretable patterns for flow prediction in dockless bike sharing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="640" to="652" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Urban flow pattern mining based on multi-source heterogeneous data fusion and knowledge graph embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>early access</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Periodic weather-aware lstm with event mechanism for parking behavior prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5896" to="5909" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intellilight: A reinforcement learning approach for intelligent traffic light control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Presslight: Learning max pressure control to coordinate traffic signals in arterial network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gayah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Metalight: Value-based meta-reinforcement learning for traffic signal control</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1153" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning for urban traffic control using coordination graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coordinated deep reinforcement learners for traffic light control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2016 Workshop on Learning, Inference and Control of Multi-Agent Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intrinsic motivation for encouraging synergistic behavior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cityflow: A multi-agent reinforcement learning environment for large scale city traffic scenario</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Traffic signal settings, road research technical paper no. 39</title>
		<author>
			<persName><forename type="first">F</forename><surname>Webster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
		</imprint>
		<respStmt>
			<orgName>Road Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive traffic signal control using fuzzy logic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Vehicles Symposium</title>
		<meeting>the Intelligent Vehicles Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-organizing traffic lights: A realistic simulation</title>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Cools</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gershenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>D'hooghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in applied selforganizing systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing urban traffic light scheduling problem using harmony search with ensemble of local search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solving traffic signal scheduling problems in heterogeneous traffic network by using meta-heuristics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3272" to="3282" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time traffic signal control with swarm optimization methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Celtek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E M</forename><surname>Alı</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">108206</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monte carlo tree search-based mixed traffic flow control algorithm for arterial intersections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">2674</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An improved adaptive signal control method for isolated signalized intersection based on dynamic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Traffic signal control with partial grade separation for oversaturated conditions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kamineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation research part C: emerging technologies</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="267" to="283" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal network-level traffic signal control: A benders decomposition-based solution algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mohebifard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hajbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="252" to="274" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">State-of-art review of traffic signal control methods: challenges and opportunities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S S M</forename><surname>Qadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gökçe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Öner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European transport research review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning for integrated network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto</title>
		<author>
			<persName><forename type="first">S</forename><surname>El-Tantawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelgawad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1140" to="1150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holonic multiagent system for traffic signals control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mozayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="1575" to="1587" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distributed w-learning: Multi-policy optimization in self-organizing systems,&quot; in self-adaptive and selforganizing systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dusparic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cahill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic light control in non-stationary environments based on multi agent q-learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mozayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bazzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modellight: Modelbased meta-reinforcement learning for traffic signal control</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boulet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08067</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic lane traffic signal control with group attention and multi-timescale reinforcement learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Diagnosing reinforcement learning for traffic signal control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gayah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning phase competition for traffic signal control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning traffic signal control from demonstrations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="3414" to="3421" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attendlight: Universal attention-based reinforcement learning model for traffic signal control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oroojlooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hajinezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05772</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Planlight: Learning to optimize traffic signal control with planning and iterative policy improvement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kafouros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="219" to="244" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-agent deep reinforcement learning for large-scale traffic signal control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Codecà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1086" to="1095" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Traffic signal control based on reinforcement learning with graph convolutional neural nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Otaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hayakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Colight: Learning network-level cooperation for traffic signal control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Macar: Urban traffic light control via active multi-agent communication and action rectification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchically and cooperatively learning traffic signal control</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalight: Improving environment generalization of traffic signal control via meta reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Varibad: A very good method for bayes-adaptive deep rl via meta-learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Igl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ciexplore: Curiosity and influence-based exploration in multi-agent cooperative scenarios with sparse rewards</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main"># exploration: A study of count-based exploration for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName><surname>Abbeel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Countbased exploration with neural density models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2721" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Large-scale study of curiosity-driven learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04355</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Surprise-based intrinsic motivation for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01732</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning to play with intrinsically-motivated self-aware agents</title>
		<author>
			<persName><forename type="first">N</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07442</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Social influence as intrinsic motivation for multi-agent deep reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Strouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3040" to="3049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploration in approximate hyper-state space for meta reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Igl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Offline meta learning of exploration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02598</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient off-policy meta-reinforcement learning via probabilistic context variables</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5331" to="5340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning adaptive exploration strategies in dynamic environments through informed policy regularization</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Kamienny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Metacure: Meta reinforcement learning with empowerment-driven exploration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">610</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Metareinforcement learning of structured exploration strategies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5302" to="5311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A concise introduction to decentralized POMDPs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning for multi-robot decentralized patrolling in uncertain environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">703</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An analytic solution to discrete bayesian reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Regan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Roess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Prassas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Mcshane</surname></persName>
		</author>
		<title level="m">Traffic engineering</title>
		<imprint>
			<publisher>Pearson/Prentice Hall</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adaptive traffic signal control using approximate dynamic programming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Heydecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="456" to="474" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An experimental review of reinforcement learning algorithms for adaptive traffic signal control</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Howley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomic road transport support systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Time critic policy gradient methods for traffic signal control in complex and congested scenarios</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Her current research interests include reinforcement learning and intelligent transportation system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Urbanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Kohls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Liwen Zhu received the B.S. degree from Beijing University of Technology</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
		</imprint>
		<respStmt>
			<orgName>School of Electronic and Computer Engineering, Peking University</orgName>
		</respStmt>
	</monogr>
	<note>Reinforcement learning-based multi-agent system for network traffic signal control</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
