<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Combinational Game Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-20">20 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anurag</forename><surname>Sarkar</surname></persName>
							<email>sarkar.an@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Khoury College of Computer Sciences</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seth</forename><surname>Cooper</surname></persName>
							<email>se.cooper@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Khoury College of Computer Sciences</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Combinational Game Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-20">20 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.14203v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>procedural content generation</term>
					<term>combinational creativity</term>
					<term>game blending</term>
					<term>variational autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present latent combinational game design-an approach for generating playable games that blend a given set of games in a desired combination using deep generative latent variable models. We use Gaussian Mixture Variational Autoencoders (GMVAEs) which model the VAE latent space via a mixture of Gaussian components. Through supervised training, each component encodes levels from one game and lets us define blended games as linear combinations of these components. This enables generating new games that blend the input games as well as controlling the relative proportions of each game in the blend. We also extend prior blending work using conditional VAEs and compare against the GMVAE and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture which lets us generate whole blended levels and layouts. Results show that these approaches can generate playable games that blend the input games in specified combinations. We use both platformers and dungeon-based games to demonstrate our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION Methods for Procedural Content Generation via Machine</head><p>Learning (PCGML) <ref type="bibr" target="#b0">[1]</ref> primarily focus on learning distributions of individual games, but an emerging body of work <ref type="bibr" target="#b1">[2]</ref> has focused on methods for recombining learned models and distributions across multiple games to generate content for entirely new games. These have been motivated by limited training data, wanting to leverage design knowledge in one set of games to apply in another and exploring creative PCGML applications beyond level generation. One such application is game blending-the generation of new games by blending the levels and/or mechanics of existing games, inspired by designers building new games by combining ideas from existing ones <ref type="bibr" target="#b2">[3]</ref>. An extensively used approach for blending levels has been the variational autoencoder (VAE) <ref type="bibr" target="#b3">[4]</ref>, an encoder-decoder architecture that learns latent representations of data. Prior works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> have demonstrated VAEs to be capable of such blending but offer limited means to control the blending, learning a combined latent space spanning all games without affording the ability to sample specific blends or games. Given a design space spanning a set of games, one may wish to blend only a certain subset of the games and in desired proportions relative to one another. To this end, prior works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have shown via conditional VAEs that supervision via labels is promising for obtaining such desired blend combinations. However, these works only examined binary combinations (a game is either included or not) rather than relative blend ratios. Further, the playability of such blended levels has been previously evaluated only by using mechanics for the original games as proxies, rather than by blending the mechanics themselves.</p><p>In this work, we introduce a framework for generating playable blended games that blend desired proportions of games. We call this latent combinational game design-latent since we use latent representations, combinational since game blending is a combinational creativity <ref type="bibr" target="#b8">[9]</ref> process and game design since we generate novel, playable games. Overall, the framework consists of:</p><p>• Training a latent variable model to learn a design space that captures the set of k games to be blended • Defining k weights specifying blend proportions • Using the weights to generate blended levels by sampling from the relevant parts of the learned design space • Using the weights to derive a blended agent that combines the mechanics of the original game agents We apply this framework using supervised Gaussian Mixture VAEs (GMVAE) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and conditional VAEs (CVAE) <ref type="bibr" target="#b11">[12]</ref> which we train using levels from the platformers Super Mario Bros., Kid Icarus, Mega Man and Metroid. Moreover, unlike prior approaches, we test playability using blended agents derived by combining the jump physics of these games using the same weights used to blend the levels. Overall, we find that the framework (with both models) enables generating playable blended games based on the weights. Further, since these models generate segments, we introduce a novel hybrid conditional GMVAE (CGMVAE) architecture which enables generating whole blended levels as well as layouts that blend levels of the dungeon games The Legend of Zelda, DungeonGrams and a repurposed version of Metroid, thereby generalizing to beyond platformers. This work thus contributes:</p><p>• a controllable combinational creativity framework for blending games in terms of both levels and mechanics • implementations of the framework using both GMVAEs and CVAEs • to our knowledge, the first use of blended jump agents • a novel CGMVAE model combining GMVAEs and CVAEs II. RELATED WORK While PCGML <ref type="bibr" target="#b0">[1]</ref> methods typically focus on modeling individual games, a body of work has emerged which instead recombines models from different games to generate new forms of content and explore design spaces across multiple games. Such works have recently been coined as methods for PCG via Knowledge Transformation <ref type="bibr" target="#b1">[2]</ref> and include game generation by recombining learned game graphs <ref type="bibr" target="#b12">[13]</ref> and learning mappings between different platformers <ref type="bibr" target="#b13">[14]</ref>, to name a few. A specific focus has been game blending i.e., generating new games by blending the levels and/or mechanics of two or more games, introduced in <ref type="bibr" target="#b2">[3]</ref> where specifications of Frogger and Zelda were blended manually. Several works have since implemented blending using variational autoencoders (VAEs) <ref type="bibr" target="#b3">[4]</ref> which learn continuous latent representations that Fig. <ref type="figure">1</ref>: Latent combinational game design: 1) a dataset of k games for training 2) a model which learns a latent space on which 3) blend weights are applied to yield 4) a blended design space for sampling content satisfying 5) a constraints module such as a gameplay agent. span all input games and can be sampled and searched for blended content, having been used for blending platformers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> as well as platformers with dungeons <ref type="bibr" target="#b7">[8]</ref>. These works however learn a fixed blend space without readily allowing the specification of different blend combinations such as blending a subset of the input games and in specific proportions. In this work, we implement controllable blending using Gaussian Mixture (GMVAE) and conditional VAEs (CVAE). GMVAEs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> use a mixture of Gaussians as the prior distribution for the latent space unlike regular VAEs which use a standard Gaussian. This enables GMVAEs to cluster data in an unsupervised manner, where each GM component encodes a specific cluster and can be used to generate data belonging to it. Previously, GMVAEs have been used to discover clusters of platformer levels <ref type="bibr" target="#b14">[15]</ref>. We instead use supervised GMVAEs since we want each GM component to cluster levels of one of the input games and we know which game each level belongs to. To our knowledge, this is the first use of a supervised GMVAE in a games context, though prior works have applied supervised <ref type="bibr" target="#b15">[16]</ref> and semi-supervised GMVAEs <ref type="bibr" target="#b16">[17]</ref> outside of games. CVAEs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref> add supervision to regular VAEs through labeling. Prior works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have used CVAEs to control blending using multi-hot labels that specify which games to include in the blend. We additionally apply continuous labels to control the relative ratios of each game.</p><p>Game blending is a conceptual blending <ref type="bibr" target="#b18">[19]</ref> method, falling under combinational creativity <ref type="bibr" target="#b8">[9]</ref>, the branch of creativity concerned with generating new concepts by recombining existing ones. A prior method for generating new games similar to our approach is conceptual expansion <ref type="bibr" target="#b12">[13]</ref>. Our works differ in that their system does automated game recombination and uses game graphs and video input whereas we generate specific combinations of games and use VAE latent representations and text-based input. In being a creative ML-based method for game design, our work can also be viewed as a GDCML (Game Design via Creative Machine Learning) approach <ref type="bibr" target="#b19">[20]</ref>.</p><p>Few previous works have attempted blending gameplay. Prior work <ref type="bibr" target="#b20">[21]</ref> has demonstrated the use of a constraintbased method for blending the reachability rules of different games but the only prior instance of blending jumps can be found in <ref type="bibr" target="#b21">[22]</ref> where blended jumps are extracted from paths in generated blended levels. We blend jumps of the original games directly using jump models from <ref type="bibr" target="#b22">[23]</ref> which learned hybrid automata to describe jumps of NES-era platformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK</head><p>We discuss our overall methodology in two sections. In this section, we introduce the framework and discuss its implementation via GMVAEs and CVAEs and its use for platformer level and jump blending. In the section after, we will discuss the CGMVAE and CCVAE architectures and how we use them to generate whole blended levels and extend to dungeons. The framework (Figure <ref type="figure">1</ref>) consists of: 1) Data comprised of levels from k different games 2) A model that learns a representation that enables working with different combinations of the k games 3) A set of k weights to specify how each game should be combined in the final blend 4) A blended design space obtained by blending the k games using the weights 5) A constraints module that checks if outputs satisfy constraints such as playability.</p><p>A dataset of k games defines the full blending possibility space and is used to train a latent variable model capable of learning a disentangled<ref type="foot" target="#foot_0">foot_0</ref> representation of the games. We use k-component GMVAEs and CVAEs with k-dimensional labels which allow manipulating each game via a separate GM component and label dimension respectively (note that this could be any model that allows reasoning about the k subsets of data separately, such as a conditional GAN <ref type="bibr" target="#b23">[24]</ref>). To blend the games, we use k weights defining the desired proportion of each game. Applying these gives us the final blended space with the k games blended in the specified proportions. For the GMVAE, we linearly combine the k GM components to obtain a new blended distribution. For the CVAE, we use the k weights as the k-dimensional label when generating a level. The final blended design space is then sampled to produce the blended outputs. Lastly, a module checks if the sampled output satisfies certain constraints. Here, the constraint is that outputs must be playable by blended agents, however this is a general module not restricted to playtesting e.g., it could be a module that uses evolutionary algorithms to search the design space for specific content. Borrowing linear algebra vocabulary, we can consider the original k games as the basis games for blending and each blended game is a linear combination of these basis games, specified by the blend weights. The set of all such linear combinations is the span of these basis games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Level Data</head><p>Training data consisted of levels of Super Mario Bros. (SMB), Kid Icarus (KI), Mega Man (MM) and Metroid (Met), taken from the Video Game Level Corpus (VGLC) <ref type="bibr" target="#b24">[25]</ref>. VGLC levels use a text format with unique characters mapping Fig. <ref type="figure">2</ref>: GMVAE architecture. The label-assigning network (dotted box) is part of the unsupervised GMVAE (as in prior work <ref type="bibr" target="#b14">[15]</ref>) but not required in the supervised GMVAE setting used in this work.</p><p>to specific tiles. To show blending, we used a distinct mapping for each game, modifying the original tiles when games shared the same tile characters (e.g., enemies in both SMB and Met use 'E'). We extracted 15x16 non-overlapping segments from each level based on the dimensions of playable areas in MM and Met where horizontal and vertical sections are 15 and 16 tiles high and wide respectively. KI levels are also 16 tiles wide. The 14-tile high SMB levels were padded with a row of background tiles on top. For Met, we filtered out entirely solid segments. We extracted 172, 80, 143 and 435 segments for SMB, KI, MM and Met respectively and upsampled the non-Met segments to obtain a total of 1740 segments. Each segment had a length-4 one-hot label indicating which game it belonged to, with 〈1000〉, 〈0100〉, 〈0010〉 and 〈0001〉 indicating SMB, KI, MM and Met respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gaussian Mixture VAEs</head><p>VAEs <ref type="bibr" target="#b3">[4]</ref> are deep latent variable models consisting of encoder and decoder networks. For a z-dimensional latent space, for each input training instance, the encoder outputs z pairs of means and variances which parameterize the latent distribution. A z-dimensional latent vector sampled from this distribution is then forwarded through the decoder to obtain outputs. Training is done by minimizing a loss function composed of-1) the reconstruction error between encoder inputs and decoder outputs and 2) the KL-divergence between the latent distribution and a tractable prior (typically standard Gaussian). Rather than use a standard Gaussian as the prior for the latent distribution, GMVAEs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> use a mixture of Gaussians. For a mixture of k Gaussians, this is done via: 1) using a k-dimension one-hot label indicating the component for an input, 2) training an additional network that maps a onehot label to pairs of means and variances that parameterize the 1 out of k components the input belongs to and 3) modifying the 2nd loss term above to instead compute KL-divergence between the latent and component distributions. GMVAEs are typically unsupervised with a label-assigning network used to learn the one-hot labels. This was used in <ref type="bibr" target="#b14">[15]</ref> for discovering clusters of platformer levels. For blending however, we want each component to encode the segments of a specific game and since we know which game each segment belongs to, we use a supervised GMVAE, manually supplying one-hot labels for each segment, making the label-assigning network unnecessary and simplifying training, as shown in Figure <ref type="figure">2</ref>. Thus, a k-component supervised GMVAE trained on levels from k games results in each component encoding the levels for one game. Each game is thus modeled by a separate Gaussian distribution parametrized by a mean and a variance learned through training. For generating a blended game, we accept k weights and model the blended game as a new distribution defined by the linear combination of the k GMs, as specified by the weights. For k = 4 as in this work, after training we obtain 4 GM components with means and variances given by:</p><formula xml:id="formula_0">M = [µ 1 , µ 2 , µ 3 , µ 4 ] and V = [σ 2 1 , σ 2 2 , σ 2 3 , σ 2 4 ].</formula><p>We then accept a weight vector W = [w 1 , w 2 , w 3 , w 4 ] indicating the weight of each GM desired in the new blended game. We then define a new blended game distribution as the linear combination of the 4 GMs with a mean given by the inner product ⟨M, W ⟩ and variance given by the inner product ⟨V, W 2 ⟩. This new distribution is the blended design space from the framework and is sampled to generate levels for the new blended game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Conditional VAEs</head><p>Another approach for blending different combinations of games explored in past works involves conditional VAEs (CVAE) which are supervised VAEs that use labeled inputs. The encoder and decoder learn to use labels to encode inputs and produce outputs respectively. New outputs are generated by sampling a latent vector, concatenating the desired label and forwarding it through the decoder. Blending games using CVAEs involves using one-hot labels for training but multihot labels for generation e.g., 〈1011〉 indicates a blend of SMB/MM/Met without KI. Prior works have used CVAEs to blend games using multi-hot labels despite being trained only using one-hot labels. However, these labels only allow a game to be included or excluded from the blend. Since our goal is to generate arbitrary blends of games, we test if CVAEs trained using the same one-hot labels can work with arbitrary labels i.e., if similar blend weights used to linearly combine the learned components of the GMVAE, could be used by the CVAE to produce blended games that combine the games in accordance with the weights. After training the CVAE on the same data and labels as the GMVAE, we generate blended levels by: 1) sampling vectors from the CVAE latent space 2) accepting k blend weights as before but using them as the kelement label 3) concatenating it to each sampled vector and 4) forwarding the concatenated vectors through the decoder to obtain the output blended levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Blending Jumps</head><p>Prior works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b25">[26]</ref> have evaluated playability of blended platformer levels using separate A* agents for each game. By seeing which game's agent can play through a blended level, we can conclude if a blended level is e.g., more Mario-like or Metroid-like. However, since our goal is generating new blended games, we blend gameplay in addition to blending levels. We define a blended jump model as a linear combination of the parameters of the original jump models, using the same weights used to blend levels. Thus, the jump for a blended game is a linear combination of the jumps of the original games. For jump models, we use the models from prior work <ref type="bibr" target="#b22">[23]</ref> that learned hybrid automata for describing the jump physics of several NES platformers, including the four we use. The automata consist of parameters defining the jump arcs and vary depending on the game. We obtain blended jump models by linearly combining these parameters using the blend weights. The automata can be used to derive jump arcs for the respective games. These jumps, when represented as lists of (x,y) coordinates, can in turn be used to learn the impulse and gravity parameters that define the jump, as described in <ref type="bibr" target="#b21">[22]</ref>. This has two specific benefits-1) jump arcs derived via blending can be used with the Summerville tilebased A* agent <ref type="bibr" target="#b26">[27]</ref> to quickly test if they can be used to traverse blended levels and 2) in the future, the impulse and gravity parameters could be used to define the mechanics for a player controller so that the blended levels can be played in an interactive environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments</head><p>All models were implemented using PyTorch <ref type="bibr" target="#b27">[28]</ref>. The GMVAE consists of an encoder, decoder and prior-assigning network. The encoder and decoder consisted of 4 and 3 fully-connected layers respectively, with both using ReLU activation. The last encoder layer was split into 2 parts, each having 1 fully-connected layer with the latter additionally using softplus activation. These two layers output the means and variances of the latent distribution respectively. The prior assigning network consisted of 2 independent sub-networks each having 1 fully-connected layer with the second additionally using Softplus activation. The two networks output the component means and variances respectively. Training was done for 1000 epochs using the Adam optimizer with an initial learning rate of 0.001, decayed by 0.1 every time training plateaued for 50 epochs. For the CVAE, the encoder and decoder consisted of 4 and 3 fully-connected layers respectively, both using ReLU activation. Training lasted 10000 epochs using Adam with an initial learning rate of 0.001, set to decay by 0.1 every 2500 epochs. The weight on the KL term of the loss function was annealed from 0 to 1 for the first 2500 epochs. Hyperparameters were determined based on prior works using these models and through experimentation. For evaluation, we conducted a 3-part study measuring 1) how accurately the output of the blend models adhered to the blend weights, 2) playability of the blended levels using the blended agents and 3) comparing the tile patterns and content in the generated blended levels with those in the original levels. Additionally, for all evaluations, we used two types of blend weights-1) binary weights in the form of length-4 onehot and multi-hot vectors indicating single-game and multigame blends respectively and 2) fractional weights consisting of length-4 floating-point vectors, representing any arbitrary combination of games. For binary weights, we evaluate all possible length-4 binary vectors except 〈0000〉 since it is unclear how to reason about a blend containing none of the games, giving us 2 4 -1 = 15 vectors. For fractional weights, we define 4 vectors to test against to cover a range of blend  combinations: 〈0.5, 0.3, 0.2, 0.0〉, 〈0.1, 0.1, 0.1, 0.7〉, 〈0.1, 0.6, 0.2, 0.1〉 and 〈0.0, 0.2, 0.3, 0.5〉. For each of the CVAE and GMVAE, we tested 4 different latent dimensionalities of 8, 16, 32 and 64 but found none to be clearly preferable across all experiments, with different sizes producing best results for different metrics. For clarity and space, instead of presenting results for all 8 models, we present results from the GMVAE-32 and CVAE-64 models as they performed the best on the blend accuracy evaluation which is the most relevant for the main focus of this work i.e., controllable blending of games.</p><p>1) Blend Accuracy: For evaluating how well the outputs of the blended models adhere to the blend weights, we used scikit-learn <ref type="bibr" target="#b28">[29]</ref> to train a random forest classifier on the segments using the game of that segment as the class label. We achieved a 98.19% accuracy on an 80-20 train-test split. The parameters for the classifier were determined via grid search. This classifier approach for evaluating blending has been used in prior CVAE works and acts as a proxy since we lack ground-truth blended levels to compare generated output. The expectation is that when using single game weights (e.g., 〈0,0,0,1〉, 〈1,0,0,0〉), classifier predictions for a game will be high when its bit is set to 1 and low when set to 0. Similarly, when using blended weights (e.g., 〈1,1,0,0〉, 〈0,1,0,1〉), the predictions should be more spread out across the games whose bit is set to 1. When using fractional weights (e.g., 〈0.5, 0.2, 0.3, 0.0〉), the predictions should be spread out in accordance with these weights. For classifying binary weights, for each model, we sampled 1000 latent vectors for each possible 4digit binary weight, leaving out 〈0,0,0,0〉. We applied the classifier on each generated segment and tracked the percentage of times each of the 4 games was predicted, per 4-digit weight. To determine how well the outputs matched a given weight, for each weight, we computed a score based on the following formula: 2 where w i is the ith weight element indicating the desired weight of the ith game, f is a factor equal to 100 divided by the number of ones in the weight and p i is the percentage of the 1000 segments for which the classifier predicted the ith game. The lower the score, the better the model matched outputs with the given weights with 0 representing a perfect match, e.g., if the weights are 〈1,0,1,0〉, the factor f is 100/2=50 and a perfect score of 0 is achieved when the classifier predicts games 1 and 3 50% of the time each. For fractional weights, we generated 1000 latents per weight per model and computed scores similarly, except here f is set to a constant 100. Results are given in Table <ref type="table" target="#tab_1">I</ref>. For both binary and fractional weights, the GMVAE does better than the CVAE for all latent dimensions except for 64 where the CVAE does better. Looking at the latent sizes, for the GMVAE and CVAE, 32 and 64 dimensions respectively give the lowest score when combining binary and fractional weights and thus for the remainder of our evaluations, we will only   present and discuss results obtained using the 32-dimensional GMVAE and 64-dimensional CVAE. Full classification results are shown in Table <ref type="table" target="#tab_3">II</ref>. We see that in most cases, the game predicted most often is one whose bit is set to 1 in the binary case. When using fractional weights, the amounts that levels are classified as different games corresponds to the value of the weights, though very loosely. Interestingly, the CVAE is able to reason with fractional weights despite having only been trained using one-hot binary weights as labels. Overall, this suggests that both GMVAE and CVAE models are able to generate output that corresponds to the desired blend weights. These results give some intuition about why the GMVAE outperforms the CVAE in Table <ref type="table" target="#tab_1">I</ref>, namely that the GMVAE is better able to blend the individual games in accordance with the weights while the CVAE is more prone to pushing the blend towards the game(s) with highest weight, as shown (in Table <ref type="table" target="#tab_3">II</ref>), by how for most multi-game blends, the CVAE scores higher (worse) than the GMVAE, usually due to putting too much weight on one of the games whose bit is set to 1. This may be due to how these models perform blending. GMVAEs learn separate distributions per game which are then combined using the weights where as CVAEs learn one distribution spanning all games and blends are produced by applying labels (i.e., weights) to samples from this distribution. If this single distribution approximates some games better than others, the labels may be insufficient in producing accurate blends.</p><formula xml:id="formula_1">S = 4 i=1 (w i * f -p i )</formula><p>2) Playability: We evaluated playability of generated blended levels using jump arcs obtained by blending the automata models using given blend weights and the Summerville tile-based A* agent <ref type="bibr" target="#b29">[30]</ref>. The original VGLC version worked with solid and non-solid tiles and was updated in <ref type="bibr" target="#b21">[22]</ref> to work with 4 affordances <ref type="bibr" target="#b30">[31]</ref>: solids, hazards, passables and climbables. We further updated the agent to find start and goal tiles in a segment. A segment is then playable if the agent can find a path from start to goal. If no path or goal is found, it is unplayable. The jumps used by the blend agent are derived from blended automata models which in turn are obtained by blending the hybrid automata models of the original 4 games using the blend weights. Details about these automata and jump extraction can be found in <ref type="bibr" target="#b22">[23]</ref>. Blended jump arcs are shown in Figure <ref type="figure" target="#fig_0">3</ref>. Since the blend agent simulates an agent that can play blends of all 4 games, it knows the tile-to-affordance mappings of all 4 games. For each model, we sampled 1000 segments for each possible 4-digit binary weight and computed the percentage that were playable by the blended agent obtained by blending the original jump models according to the corresponding weight. Since games can progress both horizontally and vertically, for each segment, the agent looks for both a valid horizontal and vertical path. If either one is found, the segment is playable. To simplify computations, we only look for left-to-right and bottom-to-top paths. Thus, our evaluations underestimate playability since e.g., MM and Met segments that progress downward but do not have an upward moving path would be deemed unplayable. Full playability results for the blended agent on binary and fractional-weighted blends are shown in Tables <ref type="table" target="#tab_5">III</ref> and<ref type="table" target="#tab_6">IV</ref>. Table IV also shows how well the original agents did in the blended levels to depict that performance roughly corresponds to the game's weight in the blend e.g., for 〈0.5, 0.3, 0.2, 0〉, the SMB agent completes more levels than the KI agent which completes more than the MM agent which does better than the Met agent. We show the performance of the original agents to show that all models generate levels in accordance with the weights and not to compare playability of the blend agent with the original agents since such comparisons are not useful. Essentially, there are two broad ways to compare. First, we can assume that the original agent for a game should not reason about the tiles of another e.g., an SMB agent should not know to avoid MM spikes. But if only the blend agent knows all tiles across all games, then it will trivially do the best since in most blended levels, the original agents would face tiles that are undefined for them and they would not be able to find paths. Thus, playability reduces to tile knowledge. Second, we can instead assume that all agents work at the affordance level, agnostic of game-specific tiles. Here, playability is completely determined by an agent's jumpsize. Higher the jump, higher the playability when all agents work with the same tile information. Since the blend agent combines the jumps of all games, its jump by definition can never be bigger than that of Metroid. Thus, playability reduces to jumpsize. Hence, in either case, such comparisons are uninformative. Overall, we observe satisfactory playability of 61.97% and 65.58% for the GMVAE, averaged across binary and fractional weights respectively and 70.14% and 68.03% for the CVAE.</p><p>3) Tile Metrics: To compare the content of generated blended levels with those in the original levels, we used the Tile Pattern KL-Divergence (TPKLDiv) <ref type="bibr" target="#b31">[32]</ref> metric which measures the similarity between two sets of levels in terms    of the KL-divergence between their tile pattern distributions. For each model and set of blend weights, we generate 1000 levels and compare the average TPKLDiv between them and each set of original levels, separately for each of the 4 games, with the values averaged over 2x2, 3x3 and 4x4 patterns. We expect lower values when comparing the original levels of a game with blended levels produced by weights that include that game, and higher values when they do not, e.g., using 〈1,1,0,0〉 should lead to lower TPKLDiv values compared to SMB and KI than with MM and Met. Full results are in Table <ref type="table" target="#tab_8">V</ref> and are true to this expectation, giving further support that the models can produce blends in accordance with the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visual Inspection</head><p>Figure <ref type="figure" target="#fig_1">4</ref> shows example blended segments. Note that a sampled segment may not appear blended since weights apply to whole distributions and not individual segments. Thus it is possible to sample segments that contain content from only 1  game, highlighting the need to use more sophisticated methods beyond just random sampling. That said, these examples depict the games being blended. SMB can be identified by its pipes and goombas, KI by its blue platforms, red hazards, brown tiles and doors, MM via orange tiles, spikes and ladders and Metroid via dark blue tiles, orange lava and metroid creatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GENERATING AND BLENDING WHOLE LEVELS</head><p>In this section, we describe our approach for extending the LCGD framework in 2 ways-1) generating whole levels and 2) blending levels from dungeon-based games. For this, we combine the GMVAE and CVAE to introduce a novel Conditional Gaussian Mixture VAE (CGMVAE) architecture which uses a supervised GM prior for the VAE latent space thus still enabling games to be encoded as GM components but now, we also condition the encoder/decoder with labels indicating segment orientation i.e., the direction(s) in which movement is possible across the segment. Directional labeling was introduced in prior work <ref type="bibr" target="#b7">[8]</ref> for generating whole levels. Using labels for progression enables generating segments that have desired orientations and can be reliably connected together to form whole levels. For dungeons, such labels can control the direction(s) in which rooms have doors/openings and enable generating dungeons with appropriately interconnected rooms. This in turn lets us use the CGMVAE to apply the LCGD framework on dungeon-based games. We compare this with a CVAE where a label controlling direction is concatenated to the label controlling blending. To distinguish it from the prior CVAEs, we refer to it as CCVAE to indicate that the labels here control both blend ratio and directionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model and Architecture</head><p>The architectures and training for both CGMVAE and CC-VAE were identical to the GMVAE and CVAE but augmented by the length-4 multi-hot labels. For both models, these labels were concatenated to both the encoder and decoder inputs. The CGMVAE architecture is shown in Figure <ref type="figure" target="#fig_2">5</ref>. The reasoning for this labeling is that whole levels can be considered to be composed of segments that are connected together so as  to enable progressing through them. Platformer and dungeon levels can be viewed as made up of discrete segments and rooms respectively. For the CCVAE, we concatenated the new directional label to the game label used with the CVAE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Level Data</head><p>The training data included levels from both platformers and dungeon-based games. The platformer models were trained using the same data as before, with directional labels added to the game labels. Directional labels were length-4 multi-hot vectors with elements corresponding to up, down, left and right respectively and set to 1/0 to indicate if progression in the corresponding direction was/was not possible e.g., 〈1,1,0,0〉 indicates a segment where progress can be made upward or downward but not left or right. The label for each segment was determined manually via visual inspection and indicated the directions in which a segment could be entered or exited by a player. For Metroid segments, directions having a door were considered open. A challenge for training on dungeonbased games is the lack of data in the VGLC where the only such game with text-based level data is The Legend of Zelda. Thus, to blend multiple such games, we also used levels from DungeonGrams (DGG), a roguelike dungeon crawler developed for prior research <ref type="bibr" target="#b32">[33]</ref>. We also leveraged the previously used Metroid levels. While Metroid is a platformer, its sprawling, interconnected world can be seen as a dungeon from a top-down perspective. We trained on 15x16 segments from each game. Since Zelda and DGG levels are 11x16, we padded each by duplicating the outermost rows. We also added vertically and horizontally flipped versions of each Zelda room, if not already present. We obtained 502 Zelda rooms, 522 DGG levels and 435 Metroid segments but upsampled Zelda and Metroid to get a total of 1566 segments. Example segments and directional labels are shown in Table <ref type="table" target="#tab_9">VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Layout Algorithm</head><p>To arrange sampled segments/rooms into whole levels, we use the layout algorithm from <ref type="bibr" target="#b7">[8]</ref>. This starts with an initial segment location closed in all four directions. In each iteration, we randomly select a closed side of the current location and place a new location next to that side if there is none. The closed side just selected is then labeled open to connect the new location to the prior one. This is repeated until a desired number of interconnected locations are generated. Generating a platformer layout is simpler since movement is overall leftto-right, with/without vertical sections in between. For this, we randomly set an upward or rightward direction for the initial segment and set directions for subsequent segments such that progress is possible from start to finish (e.g., segment following a rightward/upward segment should be open on the left/bottom respectively). There are many ways for arranging segments into whole levels and in the context of the framework, the layout algorithm could be viewed as part of the constraints module. To produce the final level, we loop through each location, determine the label based on the sides that are open/closed, sample a latent vector, concatenate the label and forward it through the decoder to generate the segment/room. For the CGMVAE, the directional label is concatenated as in Figure <ref type="figure" target="#fig_2">5</ref>. For the CCVAE, both the game blend label and the directional label are concatenated to the sampled latent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments</head><p>For evaluations, we tested that 1) the new directional labels do not prevent the new models from blending in accordance with the blend weights and 2) the generated outputs are open/closed in accordance with the directional labels.</p><p>1) Blend Accuracy: We performed an identical classifierbased evaluation as for the prior models. We re-used the random forest classifier trained previously for the platformer models and trained a new random forest classifier for the dungeon models. We achieved a 98.8% accuracy on a 80-20 traintest split for the new classifier. Results for platformers and dungeon games are given in Tables VII and VIII respectively. Similar to the prior models, for both platformers and dungeon games, the game classifications using the new models are in accordance with the blend weights in a majority of cases. That is, the game predicted most frequently by the classifier is one whose bit is set to 1 in the binary case. The models do less well for the fractional weights with most predictions being for the game with the highest weight in only 5 out of 8 and 4 out of 6 model-weight pairing for platformers and dungeons respectively with CCVAE doing worse than CGMVAE for dungeons for both types of weights. To compare the two models, we performed the same evaluation as done for the prior models (i.e., the one in Table <ref type="table" target="#tab_1">I</ref>) using the same approach described previously. Results are shown in Table <ref type="table" target="#tab_14">IX</ref> and confirm that in all but one case, the CGMVAE outperforms the CCVAE, especially when using binary weights. Note that these results are consistent with the corresponding evaluation for the initial GMVAE and CVAE models, thus suggesting that the additional labels for controlling directionality do not negatively impact the ability of these models to blend games and that like the GMVAE outperforming the CVAE, here the CGMVAE outperforms the CCVAE, with the CCVAE only doing better for fractional weights in the platformer case. This worse performance may stem from the difference in how they enable blending compared to CGMVAE, as previously noted.   2) Directional Accuracy: For this, we wanted to test if generated segments had directionality/orientation true to the labels used to generate them. Similar to the blend label evaluation, we trained a random forest classifier on the segments of the three games to predict their directional labels, obtaining 97.13% accuracy using an 80-20 train-test split. For our evaluation, for each generated segment, we compared the label predicted by this classifier with the directional label used to condition its generation. Ideally, we would like the predicted label to exactly match the label used for generation i.e., the generated segment would be open/closed in the precise directions indicated by the label. However, as noted in <ref type="bibr" target="#b7">[8]</ref>, it is sufficient for the generated segment to match in only the required open directions to ensure playability, irrespective of whether it is open/closed in the desired closed directions. We only really want to avoid the opposite case i.e., the generated segment being closed off in a desired open direction. Borrowing terminology from <ref type="bibr" target="#b7">[8]</ref>, we use the notion of exact and admissible matches where the former refers to cases where the predicted label exactly    matches the conditioning label and the latter to situations where the bits set to 1 in the conditioning label are also 1 in the predicted label, regardless of the value of the bits set to 0 in the conditioning label. Thus, by definition, an exact match is also admissible. For our evaluation, we sampled 1000 latent vectors for each blend weight and conditioned the generation of each using all 15 directional labels. We computed the mean percentage of exact, admissible and inadmissible matches for each weight, averaged across the 15 labels. Results are shown in Table <ref type="table" target="#tab_15">X</ref>. While exact matches are not high for any model, the percentage of admissible matches (which includes exact matches) is about 80% in most cases, with the number being lower for the binary platformer models, albeit still much higher than the inadmissible matches. Thus, using the hybrid models, we can both reliably obtain desired blended distributions and also control the direction of segments sampled from them.</p><p>3) Tile Metrics: We did a TPKLDiv-based evaluation for the new models with results shown in Tables XI and XII. In essentially all cases, games with the highest/joint highest and lowest/joint lowest weight have the smallest/highest TPKLDiv to the corresponding original game respectively. This suggests that, in terms of tile patterns, the blended distributions are closer/farther to the original games in accordance with the blend weights, thus indicating that incorporating directional conditioning to the GMVAE and CVAE does not negatively impact blending, as also suggested by the blending evaluation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visual Inspection</head><p>Sample whole levels are shown in Figures <ref type="figure">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref>, generated using the layout algorithm described previously with equal weights for each game i.e., 1/4 each for platformers and 1/3 each for dungeons. While these are manually selected, they are representative of the merits of the GMVAE over CVAE in better respecting the blend weights as seen in Figures <ref type="figure">6</ref> and<ref type="figure">8</ref> which blend content from all platformers and dungeon games while there is little Mario and DGG content (brown tiles) in Figures <ref type="figure">7</ref> and<ref type="figure">9</ref> respectively. This reflects how the game classifications for the CGMVAE are more spread out in accordance with the weights compared to the CCVAE as shown in Table <ref type="table" target="#tab_14">IX</ref>. These visualizations show some amount of noise in generated levels which is expected when blending games. Future work could reduce noise by testing different architectures/training. Our framework is not specific to VAEs so we could try models such as CGANs <ref type="bibr" target="#b23">[24]</ref> and GMGANs <ref type="bibr" target="#b33">[34]</ref>. We could also conduct user studies to test how preferences and qualitative evaluations match with quantitative evaluations e.g., a model with better metrics might produce levels that a user finds less aesthetically pleasing and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND LIMITATIONS</head><p>In this section we discuss limitations and some future directions for more thoroughly investigating the nature of blending. We note the classifier-based approach is a proxy for the lack of ground-truth blended levels and recognize its issues, e.g., given a blend label 〈1001〉, a perfect classification score of 0 can be achieved by a generator outputting a Marioonly level or a Metroid-only level 50% of the time each. While problematic in a vacuum, our models were trained without any signal from a classifier and thus do not learn this adversarial behavior. Combined with the other experiments, the classifier evaluations lend support that the models respect the blend weights. A limitation we do observe is that some games can be poorly represented in the blends, particularly Metroid for platformers and Zelda for dungeons. This may be due to Metroid being the most structurally different from other platformers and thus its latent encodings end up further from those of the other games. When blending the learned game distributions using equal weight, it is possible that the final blend is furthest from the Metroid distribution, leading to fewer Metroid-like segments being sampled. Similarly, for dungeon blends, Zelda with its discrete, self-contained rooms is different than both DGG and Metroid with their more open structures. This could be fixed by scaling the blend weights for these outlier games so that the final blend is closer to the desired combination. Further, we want to more precisely investigate the nature of blending, both in terms of how the levels are blended across games as well as the interaction of level and mechanic blending. Based on evaluations, visual inspection and our intuition, in blending, the models seem to generate game-specific tiles in game-agnostic semanticallyequivalent locations in terms of affordances, which is a reasonable analog to how a designer might blend such games. Thus, in addition to our TPKLDiv evaluations, we could evaluate the underlying affordances and compute affordance-based KL-Divergence as in <ref type="bibr" target="#b34">[35]</ref>. We could also decouple the level and mechanic blends i.e., test different jump blends on a given level blend and vice-versa, which could enable mixing and matching different level and mechanic blends to produce an even greater variety of games. Relatedly, our approach could benefit from additional methods of controlling outputs sampled from the blended distribution since currently one can only sample at random. While some control is provided by the CGMVAE and CCVAE in terms of orientation, additional means of control could leverage prior work that combines similar models with evolutionary algorithms to search for specific content <ref type="bibr" target="#b35">[36]</ref> or evolve an archive of playable levels <ref type="bibr" target="#b36">[37]</ref> based on specified behavioral features. A method for more reliably sampling playable levels is also needed. For this, we could incorporate into the constraints module an A* agent that repairs unplayable levels as in <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>We introduced latent combinational game design, a framework for controllably blending games, demonstrated it with a supervised GMVAE and a CVAE and proposed a new CGM-VAE architecture which combines the two models to generate whole blended levels. Our future goal is to build a system where the blended games can be played. We also wish to blend mechanics beyond jumps by incorporating affordances and try alternatives besides A* agents for the constraints module, such as an ASP solver. We could also try learning mechanics for a given blended domain by, for example, training an RL agent to learn actions given a set of levels and tile affordances. Future work could also use evolution to search the blended space instead of sampling randomly. Finally, the framework could be used as a combinational creativity approach for artistic domains in general e.g., applied on a dataset of different styles of paintings to produce a custom blend of art styles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Jump arcs for various combinations of games.</figDesc><graphic coords="5,90.96,264.89,167.08,95.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Example generated levels using the GM and CVAE models for a number of different blend weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Conditional GMVAE (CGMVAE) architecture.</figDesc><graphic coords="7,71.68,56.07,205.62,175.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 : 32 Fig. 7 :</head><label>6327</label><figDesc>Fig. 6: Sample platform level generated using CGMVAE-32</figDesc><graphic coords="9,78.11,294.55,192.77,90.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 : 32 Fig. 9 :</head><label>8329</label><figDesc>Fig. 8: Sample dungeon level generated using CGMVAE-32</figDesc><graphic coords="9,353.97,56.07,167.07,93.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Binary Fractional 8-dim 16-dim 32-dim 64-dim 8-dim 16-dim 32-dim 64-dim CVAE 1021.99 741.14 939.04 700.14 1113.8 995.08 1171.03 939.46 GM 659.83 707.9 293.79 310.94 904.21 937.26 851.84 947.31</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Classification results for blended levels generated using binary and fractional weights. Lower scores are better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>〈0101〉 11.7 21.1 2.1 65.1 1204.5 17.7 5.5 0.6 76.2 2980.3 〈0110〉 11.8 38 50 0.2 283.3 23.8 45.7 30.5 0 965.2 〈0111〉 15.6 20.4 34.1 29.9 422 35.5 7.5 30.9 26.1 1983.5 〈0.1, 0.1, 0.1, 0.7〉 2.3 0.7 1.8 95.2 848.1 2.4 0.2 0.5 96.9 967.7 〈0.1, 0.6, 0.2, 0.1〉 12.6 77.3 9.3 0.8 505.2 9.8 88.9 1.2 0.1 1286.7 〈0, 0.2, 0.3, 0.5〉 8.3 3.1 14.7 73.9 1159.8 12.3 2.5 18.3 66.9 880</figDesc><table><row><cell></cell><cell cols="2">GM-32 dim</cell><cell></cell><cell></cell><cell>CVAE-64 dim</cell></row><row><cell cols="6">Blend Weights SMB KI MM Met Score SMB KI MM Met Score</cell></row><row><cell>〈0001〉</cell><cell cols="3">0.7 0.1 0.4 98.8 2.1</cell><cell>0.1</cell><cell>0</cell><cell>0 99.9 0.02</cell></row><row><cell>〈0010〉</cell><cell cols="2">5.2 1.8 93.0 0</cell><cell>79.3</cell><cell cols="2">1.5 0.9 97.6 0</cell><cell>8.8</cell></row><row><cell>〈0011〉</cell><cell cols="4">4.4 1.3 40.5 53.8 125.7 6.2</cell><cell>0 58.4 35.4 322.2</cell></row><row><cell>〈0100〉</cell><cell cols="3">2.3 96.5 1 0.2 18.6</cell><cell cols="2">0.4 99.6 0</cell><cell>0</cell><cell>0.3</cell></row><row><cell>〈1000〉</cell><cell cols="2">94.6 3.8 1.6 0</cell><cell cols="3">46.2 98.6 1.4 0</cell><cell>0</cell><cell>3.9</cell></row><row><cell>〈1001〉</cell><cell cols="5">34.8 2.1 2.3 60.8 357.4 39.9 1.4 0 58.7 179.7</cell></row><row><cell>〈1010〉</cell><cell cols="2">48.1 5.3 46.6 0</cell><cell cols="3">43.3 42.7 9.3 48</cell><cell>0 143.8</cell></row><row><cell>〈1011〉</cell><cell cols="5">39 2.8 38.8 19.4 263.8 40.8 6.1 36.9 16.2 398.8</cell></row><row><cell>〈1100〉</cell><cell cols="2">56 42.8 1.2 0</cell><cell cols="3">89.3 64.5 35.5 0</cell><cell>0 420.5</cell></row><row><cell>〈1101〉</cell><cell cols="5">50.6 25.7 3 20.7 524.8 55.9 22.1 0.2 21.8 768.5</cell></row><row><cell>〈1110〉</cell><cell cols="5">47.8 26.4 25.7 0.1 315.6 30.2 62.6 7.2 0 1549.3</cell></row><row><cell>〈1111〉</cell><cell cols="5">42.5 19.4 29.6 8.5 631 40.5 37.1 13.4 9 777.2</cell></row><row><cell cols="2">〈0.5, 0.3, 0.2, 0〉 74.4 18.6 7</cell><cell cols="4">0 894.3 65.9 33.1 1</cell><cell>0 623.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Full classification results for both blend weight types.Lower the score, more aligned are predictions with weights.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Percentage of playable levels for each binary blend. GM-32 dim CVAE-64 dim Blend SMB KI MM Met Blend SMB KI MM Met 〈0.5, 0.3, 0.2, 0〉 75.3 67.3 8.5 6.5 0.2 77.9 66.4 21.8 4.6 2.2 〈0.1, 0.1, 0.1, 0.7〉 62.1 0.2 0.6 0.7 66.4 61.8 0.5 0.3 1.2 64.8 〈0.1, 0.6, 0.2, 0.1〉 71.3 9.9 58 6.9 1.5 71.9 3.1 63.2 8.2 3 〈0, 0.2, 0.3, 0.5〉 53.6 0.7 3.7 9.7 51.3 60.5 0.6 2.5 14.7 58.9</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Percentage of playable levels for fractional blends using the blended agent and original agents for each game.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>GM-32 dim CVAE-64 dim Blend Weights SMB KI MM Met SMB KI MM Met 〈0001〉 12.68 13.02 12.83 1.59 13.51 12.97 13.16 1.37 〈0010〉 10.95 10.64 2.1 11.28 9.87 10.23 1.64 11.03 〈0011〉 11.28 10.69 7.59 5.59 8.7 8.78 6.37 5.2 〈0100〉 10.77 2.6 10.55 11.7 11.02 1.62 10.97 11.61 〈0101〉 10.84 8.57 11.07 4.46 9.04 8.16 9.16 2.96 〈0110〉 8.58 6.74 5.34 10.03 7.95 4.79 5.14 8.52 〈0111〉 9.01 7.61 5.78 6.35 6.74 6.65 5.15 4.26 〈1000〉 0.8 4.67 4.31 4.49 0.65 4.43 4.26 4.64 , 0.1, 0.1, 0.7〉 11.77 12.04 11.66 1.63 11.4 11.05 11.15 1.46 〈0.1, 0.6, 0.2, 0.1〉 8.7 3.32 8.24 9.54 8.26 2.11 8.19 8.87 〈0, 0.2, 0.3, 0.5〉 11.19 10.5 10.01 3.79 9.48 9.28 8.26 3.64</figDesc><table><row><cell>〈1001〉</cell><cell cols="3">6.76 7.73 7.49 2.36 5.03 5.84 5.78 1.9</cell></row><row><cell>〈1010〉</cell><cell cols="3">3.53 6.15 2.51 6.4 2.94 4.09 1.81 4.49</cell></row><row><cell>〈1011〉</cell><cell cols="3">5.31 6.29 4.25 4.42 3.95 4.56 3.18 3.22</cell></row><row><cell>〈1100〉</cell><cell cols="3">3.6 4.25 5.97 6.51 2.44 2.48 4.12 4.06</cell></row><row><cell>〈1101〉</cell><cell cols="3">4.78 4.06 6.1 4.32 2.58 3.03 3.13 1.49</cell></row><row><cell>〈1110〉</cell><cell>4</cell><cell cols="2">5.29 3.94 6.56 2.19 2.02 2.46 3.54</cell></row><row><cell>〈1111〉</cell><cell cols="3">4.37 5.66 4.08 5.36 3.26 3.85 2.76 2.3</cell></row><row><cell cols="2">〈0.5, 0.3, 0.2, 0〉 2.01</cell><cell>5</cell><cell>4.5 5.38 2.38 3.5 4.34 4.81</cell></row><row><cell>〈0.1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>TPKLDiv values between original games and the different blend configurations. Lower the value, closer the tile-pattern distribution of that blend is to the corresponding game.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Example</figDesc><table /><note><p>segments with corresponding directional labels indicating doors/openings in 〈Up, Down, Left, Right〉 directions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Full classification results on platformers. Lower the score, more aligned are predictions with weights.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CGM-32 dim</cell><cell></cell><cell></cell><cell cols="2">CCVAE-64 dim</cell></row><row><cell cols="8">Blend Weights Zelda DGG Met Score Zelda DGG Met Score</cell></row><row><cell>〈001〉</cell><cell>0.2</cell><cell cols="3">0.3 99.7 0.2</cell><cell>0</cell><cell>0</cell><cell>100</cell><cell>0</cell></row><row><cell>〈010〉</cell><cell cols="3">0.1 99.8 0.1</cell><cell>0.1</cell><cell cols="3">0.1 99.8 0.1</cell><cell>0.1</cell></row><row><cell>〈011〉</cell><cell cols="5">0.3 57.8 41.9 126.5 0.7</cell><cell cols="2">9.9 89.4 3160.9</cell></row><row><cell>〈100〉</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">1.8 70.1 28.1 15346.9</cell></row><row><cell>〈101〉</cell><cell cols="4">74.3 1.8 23.9 1275</cell><cell>0.1</cell><cell cols="2">0.2 99.7 4960.1</cell></row><row><cell>〈110〉</cell><cell cols="7">42.1 57.6 0.3 120.3 0.2 99.4 0.4 4920.6</cell></row><row><cell>〈111〉</cell><cell cols="4">26 46.2 27.8 250</cell><cell cols="3">2.4 61.1 36.5 1737.9</cell></row><row><cell cols="8">〈0.5, 0.3, 0.2〉 39.5 59.3 1.2 1322.2 0.6 68.5 30.9 4041.4</cell></row><row><cell cols="5">〈0.2, 0.5, 0.3〉 0.4 95.3 4.3 1642.6</cell><cell>1</cell><cell cols="2">78.1 20.9 1233.4</cell></row><row><cell cols="6">〈0.3, 0.2, 0.5〉 3.7 31.4 64.9 1043.7 0.1</cell><cell cols="2">0.1 99.9 3770.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VIII :</head><label>VIII</label><figDesc>Full classification results on dungeon games.Lower the score, more aligned are predictions with weights.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE IX :</head><label>IX</label><figDesc>Classification results for blended levels generated by the CGM-32 and CC-64 models. Lower scores are better.</figDesc><table><row><cell>Genre</cell><cell>Model</cell><cell>Binary Exact Adm Inadm Exact Adm Inadm Fractional</cell></row><row><cell>Platformers</cell><cell cols="2">CGM-32 23.03 63.4 CC-64 27.05 71.01 28.99 36.24 82.28 17.72 36.6 33.34 84.41 15.59</cell></row><row><cell>Dungeons</cell><cell cols="2">CGM-32 47.33 83.77 16.23 49.67 96.29 3.71 CC-64 33.43 79.31 20.69 36.77 83.54 16.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE X</head><label>X</label><figDesc>0〉 4.76 3.77 5.4 6.68 3.7 3.89 4.42 5.86 〈0.1, 0.1, 0.1, 0.7〉 11.32 10.98 11.35 2.53 11.23 11.12 11.27 1.65 〈0.1, 0.6, 0.2, 0.1〉 7.46 3.74 7.42 8.38 7.92 3.08 7.05 8.75 〈0, 0.2, 0.3, 0.5〉 9.42 8.82 8.65 4.29 8.88 8.81 7.77 3.62</figDesc><table><row><cell cols="4">: Percentage of exact, admissible (adm) and inadmissible</cell></row><row><cell cols="4">(inadm) matches for models using both weight types.</cell></row><row><cell></cell><cell></cell><cell>CGM-32 dim</cell><cell>CCVAE-64 dim</cell></row><row><cell>Blend Weights</cell><cell cols="3">SMB KI MM Met SMB KI MM Met</cell></row><row><cell>〈0001〉</cell><cell cols="3">12.67 12.35 12.65 2.22 13.16 12.92 13.2 1.51</cell></row><row><cell>〈0010〉</cell><cell cols="3">10.67 10.49 2.39 11.55 11.1 10.99 1.99 11.92</cell></row><row><cell>〈0011〉</cell><cell cols="3">10.39 10.23 6.61 6.94 8.52 8.61 5.55 5.69</cell></row><row><cell>〈0100〉</cell><cell cols="3">9.11 3.06 9.37 10.03 10.82 2 10.86 11.67</cell></row><row><cell>〈0101〉</cell><cell cols="3">10.58 8.95 10.67 4.28 8.63 6.95 8.74 4.07</cell></row><row><cell>〈0110〉</cell><cell cols="3">9.27 7.55 4.29 10.17 9.54 7.64 4.41 10.35</cell></row><row><cell>〈0111〉</cell><cell cols="3">9.36 8.42 6.77 6.62 5.72 5.33 3.56 5.18</cell></row><row><cell>〈1000〉</cell><cell cols="3">2.88 4.1 5.06 5.68 0.94 5.26 5.12 5.7</cell></row><row><cell>〈1001〉</cell><cell>8</cell><cell cols="2">8.53 8.75 2.75 6.64 7.12 7.1 1.59</cell></row><row><cell>〈1010〉</cell><cell cols="3">6.12 6.68 2.29 7.42 5.17 6.06 1.97 6.53</cell></row><row><cell>〈1011〉</cell><cell cols="3">6.83 7.34 5.07 4.98 4.52 4.99 2.92 3.6</cell></row><row><cell>〈1100〉</cell><cell cols="3">5.04 3.21 6.43 7.05 3.3 2.52 4.88 5.36</cell></row><row><cell>〈1101〉</cell><cell cols="3">6.94 6.52 7.82 4.14 3.45 2.97 3.88 2.62</cell></row><row><cell>〈1110〉</cell><cell cols="3">5.8 5.46 3.54 7.22 4.5 3.65 3.02 5.5</cell></row><row><cell>〈1111〉</cell><cell cols="3">6.33 6.39 5.16 5.27 3.86 3.97 2.92 3.21</cell></row><row><cell>〈0.5, 0.3, 0.2,</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XI :</head><label>XI</label><figDesc>TPKLDiv values between each original platformer and the different blend configurations. Lower the value, closer the tilepattern distribution of that blend is to the corresponding game.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>CGM-32 dim CCVAE-64 dim Blend Weights Zelda DGG Met Zelda DGG Met 〈001〉 23.37 23.4 1.77 23.64 23.68 1.38 〈010〉 21.92 3.73 21.78 21.85 3.96 21.71 〈011〉 20.45 13.94 12.52 21.03 19.95 8.3 〈100〉 1.28 22.8 22.62 10.87 18.73 18.69 〈101〉 8.35 20.79 17.61 23.16 23.27 2.76 〈110〉 10.45 15.5 20.07 16.42 12.07 19.84 〈111〉 14.48 15.72 15.68 16.45 17.82 15.83 〈0.5, 0.3, 0.2〉 7.17 19.1 20.3 16.91 16.71 15.39 〈0.2, 0.5, 0.3〉 20.29 7.87 19.04 18.89 14.47 15.53 〈0.3, 0.2, 0.5〉 19.83 20.1 7.9 22.47 22.54 3.83</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XII :</head><label>XII</label><figDesc>TPKLDiv values between each original dungeon game and the different blend configurations. Lower the value, closer the tile-pattern distribution of that blend is to the corresponding game.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use disentangled in the literal sense (i.e., the model learns separate representations for each game) and not to refer to the specific technique of making latent dimensions learn independent factors of variation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Procedural content generation via machine learning (PCGML)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Isaksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Procedural content generation via knowledge transformation (PCG-KT)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards generating novel games using conceptual blending</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corneli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment (AIIDE)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Controllable level blending between games using variational autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring level blending across platformers via paths and affordances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Osborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment (AIIDE)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional level generation and game blending</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dungeon and platformer level blending and generation using conditional vaes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Games (CoG)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The creative mind: Myths and mechanisms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Boden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian mixture VAE: Lessons in variational inference, generative models, and deep nets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<ptr target="http://ruishu.io/2016/12/25/gmvae/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conceptual game expansion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An approach to domain transfer in procedural content generation of two-dimensional videogame levels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Game level clustering and generation using gaussian mixture VAEs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment (AIIDE)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open-set recognition with gaussian mixture variational autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klabjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised gaussian mixture variational autoencoder for pulse shape discrimination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Di Fulvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00570</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conceptual integration networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fauconnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="187" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards game design via creative machine learning (GDCML)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Games (CoG)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constraint-based 2D tile game blending in the Sturgeon system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting physics from blended platformer game levels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Osborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mechanics automatically recognized via interactive observation: Jumping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Osborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Foundations of Digital Games</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The VGLC: The video game level corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mateas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontañón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Workshop on PCG at 1st Joint International Conference of DiGRA and FDG</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating and blending game levels via quality-diversity in the latent space of a variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on the Foundations of Digital Games</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Super Mario as a string: Platformer level generation via LSTMs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mateas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Joint Conference of DiGRA and FDG</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MCMCTS PCG 4 SMB: Monte Carlo tree search to guide platformer level generation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mateas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment (AIIDE)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The videogame affordances corpus</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Osborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tile pattern KL-divergence for analysing and evolving game levels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Volz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gram-elites: N-gram based quality-diversity search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Biemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hervella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on the Foundations of Digital Games</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10356</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">tile2tile: learning game filters for platformer style transfer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Interactive Digital Entertainment (AIIDE)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepmasterprints: Generating masterprints for dictionary attacks via latent variable evolution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Procedural content generation through quality diversity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Games (CoG)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pathfinding agents for platformer level repair</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EXAG Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
