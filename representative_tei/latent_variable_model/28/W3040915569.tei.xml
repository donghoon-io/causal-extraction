<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Generative Modelling in VAEs using Multimodal Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Vinayak</forename><surname>Abrol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pulkit</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arijit</forename><surname>Patra</surname></persName>
						</author>
						<title level="a" type="main">Improving Generative Modelling in VAEs using Multimodal Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative modelling</term>
					<term>autoencoders</term>
					<term>matching network</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a conditional generative modelling (CGM) approach for unsupervised disentangled representation learning using variational autoencoder (VAE). CGM employs a multimodal/categorical conditional prior distribution in the latent space to learn global uncertainty in data by modelling the variations at local level. Thus, the proposed framework enforces the model to independently estimate the inherent patterns within each category, which improves the interpretability of the latent representations learned by the VAE model. The evidence lower bound objective for training the generative model is maximized using a mutual information criterion between the global latent categorical variable and the encoded inputs. Further, the approach has a built-in mechanism for bounding the information flow between the encoder and the decoder which addresses the problems of posterior collapse in conventional VAE models. Experiments on a variety of datasets demonstrate that our objective can learn disentangled representations and the proposed approach achieves competitive results on various task such as generative modelling, image classification and image denoising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>G ENERATIVE models have shown great promise for unsupervised learning via capturing rich distribution of complex data such as natural images, text and speech <ref type="bibr" target="#b0">[1]</ref>. In particular, the aim is to extract semantically meaningful lowlevel (region-oriented) and high-level (object-oriented) hidden attributes of the given data. Recently, neural network based generative models have become successful frameworks for this class of problems e.g., image retrieval <ref type="bibr" target="#b1">[2]</ref>, super-resolution <ref type="bibr" target="#b2">[3]</ref>, image recognition <ref type="bibr" target="#b3">[4]</ref>, video captioning <ref type="bibr" target="#b4">[5]</ref>, video dialog <ref type="bibr" target="#b5">[6]</ref> and music transcription <ref type="bibr" target="#b6">[7]</ref>. Popular approaches include, variational autoencoders (VAEs) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref>. VAEs are probabilistic graphical models for latent modelling with the ability to approximate (at-least theoretically) a desired distribution <ref type="bibr" target="#b7">[8]</ref>. The data generated using VAEs is of good quality and VAEs naturally collapse most dimensions in the latent space, which Manuscript received August 14, 2019; revised January 15, 2020 and March 16, 2020; accepted June 23, 2020. Date of publication 2020; date of current version 2020. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Heng Tao Shen. (Corresponding author: V. Abrol)</p><p>V. Abrol is with the Mathematical Institute, University of Oxford, Oxford OX26GG, UK. (e-mail: abrol@maths.ox.ac.uk).</p><p>P. Sharma, and Arijit Patra are with Department of Engineering Science, University of Oxford, Oxford OX13PJ, U.K. (e-mail: pulkit.sharma@eng.ox.ac.uk,arijit.patra@exeter.ox.ac.uk).</p><p>Digital Object Identifier 10.1109/TMM.2020.</p><p>result in interpretable latent space <ref type="bibr" target="#b11">[12]</ref>. Thus autoencoders are more suitable for data compression and generating meaningful semantic features. In contrast, GANs are explicitly set up to optimize for generative tasks and are typically better deep generative models as compared to VAEs <ref type="bibr" target="#b10">[11]</ref>. GANs use another network (so-called Discriminator) to compare generated and real data with the aim of achieving an equilibrium between Generator and Discriminator. However, this makes them difficult to work with as they require a lot of training data and tuning <ref type="bibr" target="#b12">[13]</ref>. Some efforts have also been taken to learn more flexible generative models by combining adversarial training of GANs with VAEs <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>In unsupervised representation learning, generative models encode the observed data such as images in an informative latent space. In particular, VAEs are designed to have an isotropic Gaussian prior latent distribution, and are trained by maximizing the likelihood of the observed data by marginalizing over the latent variables <ref type="bibr" target="#b8">[9]</ref>. This is achieved via optimizing the evidence lower bound consisting of a data reconstruction error term, and a divergence term between latent inference and prior distribution. In practice, most VAEs are inadequate from the viewpoint of matching between the prior and true distribution, and suffers from their inability to learn latent features that are disentangled <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The main reason for this is the original formulation of VAE, which is designed to learn a generative model (via an encoder-decoder pair), and not to produce informative latent features. This results in two major problems 1) uninformative latent representations and 2) variance over-estimation <ref type="bibr" target="#b11">[12]</ref>. The former problem occurs when the decoder is too expressive and models the data distribution on its own without the use of latent representation, which drives the divergence term in evidence lower bound to zero <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The latter problem occurs due to unbounded information flow from input to latent space <ref type="bibr" target="#b19">[20]</ref>. These problems cause VAEs to overfit on the dataset thus driving the inference distribution away from prior distribution on the latent variable <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Recently, many studies have proposed solutions for improving the representation learning capabilities of VAEs via a modified objective function <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this work, we propose an alternative yet simple generative modelling framework with multimodal prior distribution targeted towards image applications such image generation, classificaton and denoising. In order to train such models, we define the evidence lower bound objective (ELBO) with categorical conditional prior. We call this conditional generative modelling (CGM) and show that it is capable of extracting interpretable hidden attributes of data. The disentanglement in latent representations is achieved by introduction of global Accepted for publication c 2020 IEEE D R A F T latent variables over various categories of available training images. In unsupervised settings, the CGM model induces clustering of images in latent space while supervised setting can be used if label information is available. In addition, a mutual information criterion is used to ensure that the latent variables provide useful information about the input image space, which is finally used by the decoder. Each global latent variable (similar to a clustering regime) represents a learned categorical context conditioned over that category. Such latent variables are aimed to capture the global/local uncertainty of the data arising due to inter/intra class variations. This ensures that the encoder match to at least one of the categorical global latent variable, and the information between latent and input space is maximized.</p><p>The variance over-estimation problem is addressed by maximizing the likelihood of the data with a bounded information rate between encoder and decoder. The proposed approach ensures the inference distribution of the encoder for a given input is a Gaussian distribution with a learned mean but fixed pre-determined standard deviation. Further, this work shows that contrary to more complex and flexible models, CGM models are expressive enough without the need of specifically fine-tuning neural network architectures.</p><p>The rest of the paper is organized as follows: Section II briefly introduces VAEs followed by the proposed CGM approach and the associated model training procedure. In Section III we discuss and provide a comparison with the relevant related works. Section IV presents experimental results and finally the paper is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED APPROACH</head><p>We first provide a brief background on generative modelling using variational inference. Next, we describe the proposed CGM approach. The section discusses an approach to maximize mutual information between input and latent space in order to learn categorical prior distributions. Finally, we describe the overall loss function for training the proposed CGM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>We consider the problem of probabilistic generative modelling where the goal is to learn a probability distribution p(x|θ) from data points x parameterized by θ <ref type="bibr" target="#b8">[9]</ref>. The generation of x via 'generator' model is achieved with introduction of latent variables z such that p(x|θ) = p(z|θ)p(x|z, θ)dz <ref type="bibr" target="#b23">[24]</ref>. Here, p(z|θ) is the prior distribution commonly restricted to tractable standard Gaussian distribution N (0, I) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Since, marginal distribution p(x|θ) can not be computed analytically, typically variational inference using an 'encoder' model parameterized by φ with amortized inference distribution q(z|x, φ), is employed to approximate it with a lower bound</p><formula xml:id="formula_0">p(x|θ) ≥ E q(z|x,φ) [p(x|z, θ)] -λD[q(z|x, φ) p(z|θ)]. (1)</formula><p>The first term on right hand side of eq(1) measures how accurate is the generator, the second term denotes the divergence loss (KL divergence being a popular choice) that measures how closely the encoded latent variables match a unit Gaussian and λ is the scaling parameter <ref type="bibr" target="#b7">[8]</ref>. In GANs, this divergence term is replaced by an adversarial network which tries to discriminate between original and generated samples <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conditional Generative Modelling (CGM)</head><p>The proposed CGM aims to model conditional generative distributions of the form</p><formula xml:id="formula_1">p(x|C, θ) = p(z|C, θ)p(x|z, C, θ)dz, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where C is the data-dependent conditional variable(s) used to estimate the conditional prior p(z|C, θ) and likelihood p(x|z, C, θ). The evidence lower bound in this case will be</p><formula xml:id="formula_3">E q(z|x,φ) [p(x|z, θ)] -λD[q(z|x, φ) p(z|C, θ)],<label>(3)</label></formula><p>where the conditional prior p(z|C, θ) being intractable is approximated via the variational posterior q(z|C, φ). The CGM model is designed to inherently solve the latent space exploding and variance over-estimation problem of conventional variational models. The first problem is addressed by introduction of global latent variables and maximizing a mutual information criteria to ensure that the latent variables provide useful information about the input to decoder (Section II-C).</p><p>The variance over-estimation problem is solved by bounding the information flow from the input to latent space. A given input is first encoded to approximate the mean µ of inference distribution q(z|x, φ), followed by addition of noise z = µ + , where is i.i.d Gaussian with variance less than one. This is in contrast to conventional variational models where variance is also estimated. An alternative approach towards bounding the information rate is by using a variational infomax bound <ref type="bibr" target="#b28">[29]</ref>. But this requires training a separate auxiliary network and thus, there are no guarantees on tightness of the mutual information bound. Moreover, it increases the model complexity and training time. On similar lines, we came across the work in <ref type="bibr" target="#b21">[22]</ref>, which also uses a variational inference to construct a lower bound on the information bottleneck objective. Recently, work in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> showed that bounding the variance prevents posterior collapse or equivalently prevents the divergence term from vanishing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Moment Matching in Local and Global Latent Variables</head><p>In previous section, for simplicity the conditional variable is denoted by C instead of individual categorical variables i.e., [g 1 . . . g K ] (see Fig. <ref type="figure" target="#fig_0">1</ref>). This results in a K-categorical Ndimensional latent embedding space ∈ R K×N which captures the global uncertainty of the data, while allowing us to still sample the distribution at the local level. As the model trains on more observations the latent distribution encoded by model amounts to the posterior p(z|C, θ) instead of the conventional zero-information prior p(z|θ). Since, the latent space is discrete due to K-categories, the actual conditional prior distribution q(z|x, g K , φ) for a category is continuous and deterministic.</p><p>Further, defining a simple uniform prior over all categories we obtain a constant KL divergence term over the prior p(g K |θ) which can be ignored while training (see Proposition 1).</p><p>In the CGM, an example is first locally encoded and then is used to update network parameters such that it match with global conditional encoding of a category. In unsupervised setting example assignment/relevance can be achieved by nearest neighbour search or a similarity metric, while in supervised setting it is done using class labels. The matching between local and global encoding is calculated using Maximum-Mean Discrepancy (MMD) which quantifies the match between examples from two distributions by comparing their moments <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The basic idea is that the distances between distributions can be represented as distances between mean embedding, and in its general form MMD is defined as</p><formula xml:id="formula_4">D R A F T Encoder Descriptor Latent Mapper q(z|x, g K , φ), q(z|x, φ) Decoder p(x|z, θ) x r + r z x g 2 g 1 g K</formula><formula xml:id="formula_5">M (p q) = sup f ∈F (E p [f (a)] -E q [f (b)]),<label>(4)</label></formula><p>where divergence M (p q)=0 if p=q, only when F ={f, f H ≤ 1} is a unit ball in a Reproducing Kernel Hilbert Space H <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Hence, given samples a 1 . . . a S ∼ p and b 1 . . . b T ∼ q, and with a positive semi-definite kernel K(•, •), under the unit ball assumptions on the evaluation function, we have</p><formula xml:id="formula_6">M (p q) = µ p -µ q 2 H = E p(a),p(b) [K(a, b)] -2 E p(a),q(b) [K(a, b)] + E q(a),q(b) [K(a, b)] = 1 S 2 S i S i K(a i , a i ) - 2 ST S i T j K(a i , b j ) + 1 T 2 T j T j K(b j , b j ).</formula><p>(5) Its implementation requires the conventional kernel trick where we compute the distance between the means µ p , µ q of the two distributions mapped into a reproducing kernel Hilbert space <ref type="bibr" target="#b35">[36]</ref>. Note that computing MMD is equivalent to computing an energy distance with respect to some negativetype semimetric <ref type="bibr" target="#b36">[37]</ref>. Hence, for a given domain-specific notion of distance (negative type semimetric) we can define an equivalent similarity (kernel) which results in a computationally attractive criterion for training the models. This approach was recently studied in <ref type="bibr" target="#b37">[38]</ref> for MMD based GANs.</p><p>In CGM, we compute the empirical estimates of MMD metric for samples from the posterior distribution q(z i |x i , φ) using encoded inputs depending on sample assignment, and corresponding global categorical distribution q(z i |x i , g K , φ) using the reparametrization trick. Considering eq <ref type="bibr" target="#b4">(5)</ref>, µ p is the sample mean of kernel transformed latent representations of inputs and similarly µ q is the sample mean corresponding to global prototypes. This is to ensures that the information between latent and input space is maximized, and to encourage disentanglement improving discriminative power of latent features. The MMD metric is also a pseudo measure which maximizes the mutual information between input and latent space. The use of mutual information based objectives is popular in various works such as information maximizing GANs or InfoGANs <ref type="bibr" target="#b12">[13]</ref>, variational lossy autoencoders <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref> and representation learning <ref type="bibr" target="#b0">[1]</ref>. The proposed CGM model is not a standard VAE as the conditional prior is categorical i.e., a mixture of Gaussians with hard assignment <ref type="foot" target="#foot_0">1</ref> . However, it can be shown that it still maximizes a variational lower bound of the marginal log-likelihood i.e.,</p><formula xml:id="formula_7">Proposition 1. E p(x) [log p(x|θ)] ≥ L E + L M .</formula><p>where p(x) is the empirical data distribution, L E is the data error loss and L M is the MMD loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The CGM Model</head><p>In our implementation the CGM model has three core components as shown in Fig. <ref type="figure" target="#fig_0">1:</ref> 1) An encoder E(•) parameterised as a neural network, that encodes the examples x i from input space to a representation r i = E(x i ). 2) A descriptor A(•) that summarizes the encoded inputs to learn K global latent prototype representations. To achieve order invariance in global representation we employ the mean operation g K = 1 R R i r K i which worked well in our initial experiments. We observed marginal performance improvement with a weighted average using attention mechanism, however, further studies are required to make any claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D R A F T</head><p>3) A Latent mapper L(•) that generate samples from the posterior distribution q(z i |x i , φ) and q(z i |x i , g K , φ) using the reparametrization trick. 4) A decoder D(•) parameterised as a neural network, that takes as input the sampled local latent variable z i and estimates the distribution p(x i |z i , θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Training</head><p>The training of a CGM model is achieved by jointly optimizing the overall loss function:</p><formula xml:id="formula_8">L CGM = L E + λL M + βL D ,<label>(6)</label></formula><p>where L E is the data error loss for optimizing the encoder and decoder networks, L M is the MMD loss, and L D is the descriptor loss for updating global prototypes. If output distribution is chosen to be Gaussian, L E corresponds to mean squared error loss, while for the case of Bernoulli (e.g., for binary MNIST images) it corresponds to binary cross entropy loss. λ, β are constants, and in our experiments, we found the model training to be robust and stable w.r.t. these constants.</p><p>For MMD loss we considered the Gaussian kernel</p><formula xml:id="formula_9">K(a, b) = exp -a-b 2 2N ,<label>(7)</label></formula><p>although other handcrafted kernels can also be explored as long as they are positive semi-definite so as to enable a projection into a reproducing kernel Hilbert space. In order to update the K global prototypes of the descriptor we considered the following loss after computing the example assignments <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_10">L D = r i -g K 2 2 if i K max(0, m -r i -g K 2 ) otherwise. (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>The network encodes the examples x i from input space to a representation r i = E(x i ), which are matched with a global prototype. Ideally, prototypes g K should be updated (using back-propagation) along with latent features, on the entire training set. To implement this efficiently, updates are performed on mini-batches. If all classes are well-separated with the margin m, then g K will approximately correspond to the means of features in each class <ref type="bibr" target="#b40">[41]</ref>. Thus, the loss in eq <ref type="bibr" target="#b7">(8)</ref> ensures that each encoded example match to at least one of the global categorical representation, and this encourages disentanglement in latent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORKS</head><p>In this section we compare the proposed CGM approach with the existing closely related generative modelling and representation learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Information Maximizing VAE (InfoVAE)</head><p>The closest work related to the proposed CGM approach is of the InfoVAE proposed in <ref type="bibr" target="#b11">[12]</ref>. The family of InfoVAE models are also based on using a mutual information maximization term with the standard VAE loss term</p><formula xml:id="formula_12">L IV AE = L V AE -γI q(x,z,φ) (x, z),<label>(9)</label></formula><p>where for γ &lt; 1, L IV AE = L E + L M . Hence, InfoVAE is a special case of CGM model with β = 0 i.e., without considering global latent variables. In other words, for β &lt; 1, the learned latent space in CGM tends to behave like InfoVAE. However, it is not possible to train an InfoVAE model having γ = 1 without bounding the mutual information between input and latent space. In other words, the mutual information can be maximized to infinity by making q(z|x, φ) a deterministic mapping with zero variance<ref type="foot" target="#foot_1">foot_1</ref> . Although this problem was highlighted in <ref type="bibr" target="#b11">[12]</ref>, there was no explicit solution presented to restrict such modelling behaviour. While CGM also maps the latent representations in a deterministic way to one of the K global distributions, each input is encoded with a fixed variance determined by the bound on the information flow designed into the model. Further, the CGM model uses conditional prior instead of standard Gaussian prior used in InfoVAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conditional VAE (CVAE)</head><p>In CVAE, conditioning on the context is done by adding the dependence both in the inference distribution q(z|x, c, φ) and the decoder p(x|z, c, θ), so they can be considered as deterministic functions of the context <ref type="bibr" target="#b41">[42]</ref>. Here, context variable c is usually the class label. This helps in controlling the data generation process as the decoder can generate examples from a specific class. The usual way of implementing this is to concatenate the sampled latent variable z ∼ N (0, I) with a specified label and pass through the decoder. This again can lead to the problem of uninformative latent representations where the decoder is powerful enough to just focus on the label rather than the latent representation <ref type="bibr" target="#b42">[43]</ref>. This issue is more prominent when the latent dimension is small. CGM model eliminates this problem by introduction of global latent variables (in contrast to local latent variables for computing divergence) such that we generate new examples by sampling one of the learned K-conditional prior distributions. Further, the encoder of CGM model doesn't use the class label. As described in Section II-C the model can also be trained in unsupervised settings where no label information is available, and the K-categorical modelling is performed not in the label space but rather in the prototype space which itself is defined by the model and learned during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Meta Learning</head><p>Meta learning aims at inducing knowledge (e.g., a set of rules) such that an existing learning method can be evolved to perform on different learning problems <ref type="bibr" target="#b43">[44]</ref>. As an example, in metric space meta learning such an idea of 'learning to learn' can be attempted by efficient optimization on small subsets of multiple tasks in a metric space and that metric space can be used for evaluating the extent or quality of the learning process. Models that include a conditioning class-variable can be used for classification as well, instead of usual generative tasks. For instance, for few-shot classification problem we usually D R A F T employ some distance metric to compare target examples to the available prototype observations, with such a metric computed between feature representations obtained through a transformation of the image space into a lower dimensional space. Such a behaviour is inherent in CGM due to the matching between local and global prototype latent variables. A similar effort in this direction is of matching networks <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b44">[45]</ref> which at an abstract level can be interpreted as a CGM model where the descriptor is another network whose parameters are optimized using MMD after fixing a baseline VAE <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section we present various experimental studies to show the effectiveness of the proposed CGM approach, and it with existing state-of-the-art approaches. The performance is evaluated for various tasks namely density estimation, image generation, classification and image completion using standard datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>NLL* NLL** Error VLAE (PixelCNN) <ref type="bibr" target="#b17">[18]</ref> 78.5 79.0 1.55 InfoVAE (DCGAN) <ref type="bibr" target="#b11">[12]</ref> 80.7 81.2 1.62 MAE (PixelCNN) <ref type="bibr" target="#b19">[20]</ref> 77.9 78.4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Setup</head><p>All of the networks for this work were trained using the PyTorch toolkit, where weights were initialized using the Xavier initialization scheme and biases were initialized to zero <ref type="bibr" target="#b46">[47]</ref>. The Adam optimizer was used with an initial learning rate of 10 -4 . All the networks were trained for 200 epochs. A batch size of 64, 64, 100, and 256 was used for MNIST <ref type="bibr" target="#b47">[48]</ref>, fashion MNIST <ref type="bibr" target="#b48">[49]</ref>, CIFAR-10 [50], and ImageNet <ref type="bibr" target="#b50">[51]</ref> experiments, respectively. Parameter m is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Behaviour On MNIST</head><p>In this section, the performance of the proposed CGM approach is evaluated on the MNIST, and fashion-MNIST datasets. The scaling constants are set as λ = 1000, β = 1, and is sampled with variance 0.02. For a fair comparison, we use the DCGAN architecture 3 for training the model and the choice of λ is kept similar to InfoVAE. We compared two variations of our CGM model, 1) CGM1 trained in unsupervised setting where example relevance is computed using nearest neighbour search; 2) CGM2 trained in supervised setting where example relevance is just the class label. In all experiments the training parameters are set empirically after experimentation. 3 DCGAN refers to a fully convolutional neural network architecture replacing pooling operations with spatial downsampling convolutions and eliminating fully connected layers <ref type="bibr" target="#b51">[52]</ref>. 1) Density estimation: In this experiment we evaluated CGM on density estimation problem with a fixed information rate of 15 bits per image (BPI). The latent dimension N is set to 10. The performance of the proposed approach is compared with existing best performing approaches namely InfoVAE, Variational lossy autoencoder (VLAE) <ref type="bibr" target="#b17">[18]</ref>, and mutual posterior-divergence autoencoder (MAE) <ref type="bibr" target="#b19">[20]</ref>. Both VLAE and MAE employ popular PixelCNN architecture for model training <ref type="bibr" target="#b52">[53]</ref>. The results are reported in Table I in terms of marginal negative log-likelihood (NLL) in nats evaluated via importance sampling. It can be observed that the proposed approach achieves comparable and/or better performance to existing approaches.</p><p>2) Image generation: Next, in Fig. <ref type="figure" target="#fig_1">2</ref> we demonstrate image reconstruction and generation behavior of the proposed CGM approach on MNIST and fashion-MNIST datasets. The latent dimension N is set to 10, with previously described setting of all other parameters. It can be observed that the reconstructed images are of high quality, while the generated images are slightly blurry but sharp enough.</p><p>To investigate further, Fig. <ref type="figure">3</ref> further plots the latent variables for N = 2 on the test set of MNIST. As a comparison we have also plotted the same for InfoVAE and CVAE. In case of CVAE, the latent variables are highly overlapping, while for InfoVAE and CGM one can observe good disentanglement among latent features of different classes. This is because, in CVAE we model p(z|c), which is inferred variationally and hence the posterior q(z|x, c) for any class is roughly N (0, I), as evident from the plot. CGM inherits the benefits of both conditional modelling so as to be able to generate class specific examples, and informative latent modelling like InfoVAE. Note that the disentanglement is expected to improve as the latent dimension increases.</p><p>3) Classification: In order to quantify the clustering behaviour of latent space we calculated the classification error by computing class assignment S(q(y|f (z))), where f a network that maps the latent variable to the logits and S(•) is the softmax function. These results for the case of dynamically binarized images are also reported in Table <ref type="table" target="#tab_0">I</ref>.</p><formula xml:id="formula_13">(•) is D R A F T</formula><p>Clearly, CGM achieves the best performance in both supervised and unsupervised setting due to its ability to learn disentangled representations. 4) Effect of bounding the information rate: As mentioned earlier the idea of introducing a mutual information constraint between the input and its latent variable is not new and many existing works employ this in some form to overcome the shortcomings of regular VAEs. However, mutual information itself is not a quantity that is easy to comprehend and specify. In practice, mutual information is always data dependent and it is difficult to access how much mutual information is enough to achieve a desired modelling behavior. Hence, for representation learning the usual way is to qualitatively inspect the model training, which makes existing methods less practical. This is not the case with the CGM approach, as it uses a fixed variance while encoding which determines the information rate, and for a fixed rate I it is given as σ 2 = 1/(4 I N ). As shown in Fig. <ref type="figure">4</ref>, with increase in the information rate from 0 to 15 BPI, the quality of both the generated and reconstructed images increases. In comparison, approaches such as InfoVAE rely on manual inspection of vanishing variance. However, we observed that for a given information rate both CGM and InfoVAE achieve similar MSE, except that CGM results in better disentangled latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Behaviour on ImageNet and CIFAR-10</head><p>This section investigates CGM1 for ImageNet <ref type="bibr" target="#b50">[51]</ref> and CGM2 for CIFAR-10, two popular benchmark datasets of natural images. For ImageNet, we used the residual-CNN <ref type="bibr" target="#b53">[54]</ref> architecture of comparable size to the one in <ref type="bibr" target="#b52">[53]</ref>, with latent dimension N = 600, number of categories K = 100, and information rate of 200 BPI. The scaling constants are set as λ = 1200, β = 1, and variable is sampled with variance 0.02. For CIFAR-10, we used the DCGAN architecture with a latent dimension of N = 300 and an information rate of 120 BPI. The scaling constants are set as λ = 1100, β = 1, and is sampled with variance 0.02. In all experiments, the training hyperparameters are obtained empirically.</p><p>Table <ref type="table" target="#tab_2">II</ref> reports the performance of the proposed approach on the density estimation problem. As a comparison we have considered both likelihood-based auto-regressive generative models (e.g. Pixel-RNN) and variationally trained latent models (e.g. δ-VAE). It can be observed that CGM achieves comparable performance to state-of-the-art approaches on challenging datasets with simpler network architecture. We expected to improve upon this by using a more powerful VAE architecture. Our initial experiments with complex architectures in particular, with PixelSNAIL decoder resulted in significant gains. Further, the generated images shown in Fig. <ref type="figure">5</ref> on the ImageNet dataset using CGM2 (PixelSNAIL) demonstrate the effective modelling capability of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Completion</head><p>In this experiment, we evaluate the performance of the proposed CGM2 approach for image completion task. We carried out the experiments using MNIST <ref type="bibr" target="#b47">[48]</ref> and CIFAR-10 [50], both containing 10 classes with approximately 60,000 images in the dataset. For each dataset we tested reconstruction abilities of a trained denoising CGM2 network on input images corrupted by additive Gaussian noise. For both datasets, we used the DCGAN architecture, number of categories K = 10, and the noise variance was set to 0.5. For MNIST, the latent D R A F T Model CIFAR-10 (Test) ImageNet (Valid) Pixel-RNN <ref type="bibr" target="#b52">[53]</ref> 3.06 3.63 Gated Pixel-RNN <ref type="bibr" target="#b54">[55]</ref> 3.02 3.57 PixelSNAIL <ref type="bibr" target="#b55">[56]</ref> 2.85 3.56 δ-VAE (PixelSNAIL) <ref type="bibr" target="#b29">[30]</ref> 2.83 3.58 MAE (PixelCNN) <ref type="bibr" target="#b19">[20]</ref> 2.95 As shown in Fig. <ref type="figure">6</ref> the model learns to make good predictions of the underlying images on MNIST dataset. Also, for a given epoch as the number of categorical examples increases, the better the estimate of categorical conditional distribution become and hence, the predictions look more similar to the underlying ground truth. Similarly, Fig. <ref type="figure" target="#fig_4">7</ref> shows the results on some images from CIFAR-10 dataset. It's worth mentioning that the goal of this experiment is not to surpass a state-of-the-art image recovery approach but just to show the capability of the proposed generative modelling approach. The recovery performance is expected to improve, especially in terms of edge details by using state-of-the-art CNN architectures designed for such tasks with additional image priors and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>Uninformative latent variable due to behaviour of KL divergence in ELBO and variance overestimation problems are well known, where existing works solves the former by carefully choosing parameters (e.g., β-VAE), label conditioning (e.g, CVAE) or redefining ELBO objective using MMD based ELBO. However, this still requires manually checking the variance, a theoretical proof/justification of which is presented in InfoVAE-Proposition2. CGM improves upon these by mapping each input to a categorical prior and fixed variance to bound the information flow. The main contributions of the paper are multimodal prior and information bounding method which learns an informative latent space with benefits of MMD based ELBO objective e.g., Fig. <ref type="figure">4</ref> confirms the effect of information rate controlling behaviour in image generation. Our initial attempt in this work is to show an easy way of training a generative model which eliminated the existing problems without compromising much on overall performance. Again our preliminary results confirm that with better architectures significant performance gains can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have introduced CGM, a categorically conditional generative modelling approach with fixed multimodal priors. We have demonstrated that this model can learn D R A F T disentangled representation of data. The overall generative framework is regularized by restricting the information rate of the encoder network, and by maximizing a mutual information criterion between the global latent categorical variable and the encoded inputs. This ensures that the latent conditional prior distribution doesn't have vanishing variances, and information between latent and input space is maximized efficiently. We evaluated our model on various tasks ranging from unsupervised clustering, classification to image completion using popular datasets and achieved competitive results compared to the current state-of-the-art. In future applications to sequence prediction or data (such as audio or video) that requires modelling of temporal dynamics using a conditional generative approach would be worth considering. Additionally, the proposed CGM method's applications towards categorically defined rehearsal and pseudo-rehearsal approaches for alleviating catastrophic forgetting may be attempted to enable continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOF OF PROPOSITION 1:</head><p>Assuming K categories with conditional variables g K , the marginal distribution can be defined in terms of summation over categorical distributions as: q(z, g k |x)</p><p>≥ E z,g k log p(x|z, g k , θ) + log p(z|g k , θ) q(z|x, g k , φ) + log p(g k |θ) q(g k |x, φ) (10) where the relation follows from Jensen's inequality. The third term is a constant since we have assumed a uniform prior p(g|θ) over all categories. The second term is the KL divergence between inference distribution and conditional prior distribution. From eq <ref type="bibr" target="#b8">(9)</ref>, for γ ≤ 1 and bounded information flow between latent and input space, KL divergence can be replaced by MMD divergence. Taking the expectation over data distribution p(x) proves our result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D R A F T</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the proposed CGM approach. Input vector x is encoded to a local representation vector r via an Encoder. A Descriptor summarizes representations of similar inputs as one of the global categorical prototypes g K . With all classes well-separated g K corresponds to the means of encoded features in each class. Both r and g K are used by the Latent Mapper to generate samples (with a fixed variance) from the posterior distribution and underlying latent prior distribution. Finally, a Decoder using the sampled latent variable attempts to reconstruct the input. The overall training of CGM model involve a joint loss comprising of individual losses corresponding to Encoder/Decode, Descriptor and Latent Mapper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Reconstructed and generated images for (a,b) MNIST and (c,d) Fashion-MNIST datasets using CGM2.</figDesc><graphic coords="5,438.65,166.20,88.80,88.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Visualization of 2D latent representations on MNIST test set.</figDesc><graphic coords="6,71.57,23.27,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Generated images for ImageNet dataset using CGM1.</figDesc><graphic coords="7,83.32,203.14,182.35,183.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Noisy and Denoised images for CIFAR-10 dataset using CGM2.</figDesc><graphic coords="7,339.51,177.73,196.00,55.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, z,g k [i = k] |θ)dz = log E z,g k p(x|z, g k , θ)p(z|g k , θ)p(g k |θ) q(z, g k |x) ≥ E z,g k log p(x|z, g k , θ)p(z|g k , θ)p(g k |θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I LOG</head><label>I</label><figDesc>LIKELIHOOD ESTIMATES FOR DIFFERENT MODELS ON THE MNIST DATASET, AND % CLASSIFICATION ERROR ON THE TEST SET OF MNIST DATASET. * AND ** SCORES ARE FOR THE CASE OF DYNAMICALLY AND STATISTICALLY BINARIZED IMAGES, RESPECTIVELY.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II LOG</head><label>II</label><figDesc>LIKELIHOOD ESTIMATES (BITS/DIM) FOR DIFFERENT MODELS ON CIFAR-10 (32×32 IMAGES) AND IMAGENET (64×64 IMAGES).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>During submission process we came across a related article<ref type="bibr" target="#b38">[39]</ref> which uses a GMM prior in latent space. The regularization with GMM prior rely upon ad-hoc parameters, namely eta (in NLL) and alpha (in regularizer), to which it is sensitive and are difficult to tune. Full ELBO objective is based on KL divergence which is assumed to ensure an uniform approximate posterior; this is only true for small data, and in practice the NLL term in the ELBO will overpower the regularizer. Also it is unclear if the mean-field approximation actually constraints the Monte Carlo sampling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The publicly available implementation by the authors doesn't uses the conventional reparametrization trick to sample the latent variable. In other words the posterior q(z|x, φ) is just a point mass. This is not the case with CGM where reparametrization is used as in case of VAEs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">August 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative model for concurrent image retrieval and roi segmentation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gonzlez-Daz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Baz-Hormigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Daz-De-Mara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auto-embedding generative adversarial networks for high resolution image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2726" to="2737" />
			<date type="published" when="2019-11">November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust face recognition via multimodal deep face representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="2015-11">November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From deterministic to generative: Multimodal stochastic RNNs for video captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3047" to="3058" />
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video dialog via multi-grained convolutional self-attention context networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An end-to-end neural network for polyphonic piano music transcription</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sigtia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="927" to="939" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">β-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning cascaded deep autoencoder networks for face alignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2066" to="2078" />
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Infovae: Information maximizing variational autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference of Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019-01">January 2019</date>
			<biblScope unit="page" from="5885" to="5892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Snderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarially approximated autoencoder for image generation and manipulation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keshmiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2387" to="2396" />
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangling disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="4402" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation with cycle-consistent variational auto-encoders</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veeravasarapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="829" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MAE: mutual posterior-divergence regularization for variational autoencoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">f-VAEGAN-D2: A feature generating framework for any-shot learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="10" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auxiliary guided autoregressive variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="443" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">IntroVAE: Introspective variational autoencoders for photographic image synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual adversarial autoencoders for clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FuseGAN: learning to fuse multi-focus image via conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1982" to="1996" />
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkbmWqxCZ" />
		<title level="m">The mutual autoencoder: Controlling information in latent code representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-VAEs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bounded information rate variational autoencoders</title>
		<author>
			<persName><forename type="first">D</forename><surname>Braithwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning day of ACM Conference on Knowledge Discovery And Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018-08">August 2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007-12">December 2007</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convergence de la répartition empirique vers la répartition théorique</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fortet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mourier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales scientifiques de l&apos; École Normale Supérieure</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="285" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Step size adaptation in reproducing kernel hilbert space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1107" to="1133" />
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Moore-closed spaces, completeness and centered bases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">General Topology and its Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="313" />
			<date type="published" when="1974-05">May 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and RKHS-based statistics in hypothesis testing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013-10">October 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Demystifying MMD GANs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2017-10">October 2017</date>
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Latent constraints: Learning to generate conditionally from unconditional generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Been there, done that: Meta-learning with episodic recall</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="4354" to="4363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast adaptation in generative models with generative matching networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="1704" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR-2009</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="4797" to="4805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PixelSNAIL: An improved autoregressive generative model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
