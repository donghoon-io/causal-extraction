<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiscale Latent-Guided Entropy Model for LiDAR Point Cloud Compression</title>
				<funder ref="#_sJHwMsb">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_k4YETBK">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-14">14 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tingyu</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Linyao</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yilin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Dong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Multiscale Latent-Guided Entropy Model for LiDAR Point Cloud Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-14">14 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.12512v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>point cloud compression</term>
					<term>end-to-end learning</term>
					<term>octree</term>
					<term>deep entropy model</term>
					<term>sparse convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The non-uniform distribution and extremely sparse nature of the LiDAR point cloud (LPC) bring significant challenges to its high-efficient compression. This paper proposes a novel end-to-end, fully-factorized deep framework that represents the original LiDAR point cloud into an octree structure and hierarchically constructs the octree entropy model in layers. The proposed framework utilizes a hierarchical latent variable as side information to encapsulate the sibling and ancestor dependence, which provides sufficient context information for the modeling of point cloud distribution while enabling the parallel encoding and decoding of octree nodes in the same layer. Besides, we propose a residual coding framework for the compression of the latent variable, which explores the spatial correlation of each layer by progressive downsampling, and model the corresponding residual with a fully-factorized entropy model. Furthermore, we propose soft addition and subtraction for residual coding to improve network flexibility. The comprehensive experiment results on the LiDAR benchmark SemanticKITTI and MPEG-specified dataset Ford demonstrate that our proposed framework achieves stateof-the-art performance among all the previous LPC frameworks. Besides, our end-to-end, fully-factorized framework is proved by experiment to be high-parallelized and time-efficient, which saves more than 99.8% of decoding time compared to previous stateof-the-art methods on LPC compression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D UE to the rapid development of 3D sensors, point clouds (PC) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> have become a promising data structure with broad applications in autonomous driving <ref type="bibr" target="#b3">[4]</ref>, robotic sensing <ref type="bibr" target="#b4">[5]</ref>, and AR/VR. Among various use cases, a type of point cloud captured by the Light Detection And Ranging (LiDAR) sensor, called the LiDAR point cloud (LPC) <ref type="bibr" target="#b5">[6]</ref>, has recently received much attention. An LPC represents the scene information centered on a driving vehicle and is usually collected in real-time, with over 100,000 points per frame <ref type="bibr" target="#b6">[7]</ref>. However, the non-uniform distribution of the LiDAR point cloud and the noise brought by real-time acquisition make the exploration of spatial correlation extremely difficult, bringing significant challenges to its compression. Therefore, this paper focuses on the geometry of LiDAR point clouds, aiming to design an efficient end-to-end deep framework for the compression of LiDAR point clouds.</p><p>Prior works on LiDAR point cloud compression use multiple data structures to regularize the non-uniform point cloud sequence, e.g., range images <ref type="bibr" target="#b7">[8]</ref>, KD-trees <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and Octrees <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Among them, the Moving Picture Expert Group (MPEG) develops a Geometry-based Point Cloud Compression (G-PCC) standard <ref type="bibr" target="#b12">[13]</ref>. G-PCC is implemented based on the octree structure that recursively divides the current 3D cube into eight sub-cubes. G-PCC reports state-of-theart performance among all rule-based methods. However, the above methods are highly dependent on hand-crafted context selection and entropy model without the optimization by traversing the whole dataset, leading to unsatisfactory coding efficiency.</p><p>Thanks to the application of deep learning techniques in image/video compression <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b18">[19]</ref>, learning-based point cloud compression methods <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> have emerged. For LPC, most of the methods are based on the octree entropy model. Given the octree sequence x = {x 1 , x 2 , • • • , x N }, the lower bound of bit rate can be formulated using the information entropy of the data distribution p(x) <ref type="bibr" target="#b23">[24]</ref>, which is assumed intractable due to its high dimensionality. Therefore, the goal is to approximate p(x) with an estimated distribution q(x) computed by a deep neural network. Given an estimation q(x), the lower bound of bit rate is the cross-entropy between q(x) and p(x), i.e.,</p><formula xml:id="formula_0">H(p, q) = E x∼p [-log 2 q(x)] ,<label>(1)</label></formula><p>which is a tight lower bound achievable by arithmetic coding algorithms. Due to the high-dimensionality of x, the probability model q(x) is often constructed in an auto-regressive <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> fashion:</p><formula xml:id="formula_1">q(x) = N i q(x i |context(x i )),<label>(2)</label></formula><p>where context(x i ) is a subset of all decoded nodes that are considered to have dependence with x i . The strategy to determine the context set varies among different methods. Oct-Squeeze <ref type="bibr" target="#b19">[20]</ref> assumes the conditional independence of sibling nodes given their ancestors and applies a hierarchical coding structure that selects ancestry nodes as context, bringing low computational complexity at the cost of violating the original distribution. To mitigate the huge information loss brought by the sibling independence assumption, VoxelContext-Net <ref type="bibr" target="#b21">[22]</ref> utilizes a 3D convolutional neural network (CNN) to capture the dependencies between nodes in the bottom layers of the octree. OctAttention <ref type="bibr" target="#b22">[23]</ref> further encodes and decodes the octree in a breadth-first order and maintains a context window {x i-N +1 , • • • , x i-1 } for attention-based context learning, which reports state-of-the-art performance among all LPC compression methods. However, selecting the decoded sibling nodes causes the context to vary during the decoding process. Thus the model needs to be constantly re-run to update the semantic information of the current context, leading to excessive decoding complexity and non-parallelism. Although the author proposes mask operation for complexity reduction, 00100001 <ref type="bibr" target="#b32">(33)</ref> ...... ...... the model still requires even more than 10 minutes to decode one frame of point cloud <ref type="bibr" target="#b22">[23]</ref>.</p><p>To summarize, two major challenges must be compatibly addressed to design an efficient LPC compression network: 1) The context selection algorithm needs to thoroughly explore the dependence of the whole octree to avoid suboptimality. 2) The decoding process of nodes should be independent enough, such that the model runs only once during decoding for the sake of complexity and parallelism.</p><p>To this end, we propose a new end-to-end, fully-factorized LPC entropy model. Inspired by Ballé et al. <ref type="bibr" target="#b14">[15]</ref>, we address the above two issues by transmitting side information, which encodes additional information to reduce the mismatch of the entropy model. The proposed side information is a latent variable encapsulating the dependency information of the octree, through which the octree nodes are assumed to be independent layer-wisely. To fully explore the spatial correlation, we design the latent variable in a hierarchical form, progressively increasing the perceptive field to capture deeper correlation information. The model we propose encapsulates the sibling correlation through the latent variable to achieve optimal coding efficiency. Meanwhile, compared with Fu et al., the independence of sibling nodes conditioned on the latent variable significantly decreases the computational complexity and brings decoding parallelism among nodes in the same layer, such that the model runs only once during decoding. To summarize, our contributions are highlighted as follows:</p><p>• We propose an end-to-end, fully-factorized LPC compression framework that encapsulates the spatial correlation of point clouds by introducing a hierarchical latent variable. The proposed framework explores the dependence of sibling nodes to avoid suboptimality while maintaining low complexity and a high degree of parallelism. The model runs only once during decoding. • We propose to replace the ordinary addition/subtraction operation with soft addition/subtraction to improve the model's flexibility. • The experiment result shows that our proposed network achieves state-of-the-art performance on LPC datasets Se-manticKitti and Ford. Furthermore, due to the enormous complexity reduction, our model saves 99.8% of runtime compared with the previous state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Convolution</head><p>Compared with well-structured 2D images/videos, point clouds are of unordered and non-uniform characteristics. A straightforward way to process 3D point clouds is by voxelization and 3D convolutional neural networks (CNN), which runs counter to the sparsity nature of point clouds, resulting in huge time and memory consumption. Therefore, Choy et al. propose the generalized sparse convolution <ref type="bibr" target="#b26">[27]</ref>, with the original point cloud P = {(x i , y i , z i , f i )} i expressed as the coordinate matrix C and the associated feature matrix F :</p><formula xml:id="formula_2">C =      b 1 x 1 y 1 z 1 b 2 x 2 y 2 z 2 . . . b N x N y N z N      , F =      f 1 f 2 . . . f N     <label>(3)</label></formula><p>where b i is the batch index and f i is the associated feature. The generalized sparse convolution is defined as:</p><formula xml:id="formula_3">x out u = i∈N 3 (u,C in ) W i x in u+i for u ∈ C out<label>(4)</label></formula><p>where x in u is the input feature-vector defined at u, C in and C out are predefined input and output coordinates and</p><formula xml:id="formula_4">N 3 (u, C in ) = {i|u+i ∈ C in , i ∈ N 3 }</formula><p>is the set of offsets from u that exist in C in . The sparse CNN utilizes the same convolutional structure as the dense CNN for each effective point while significantly reducing the time and memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rule-based LiDAR Point Cloud Compression</head><p>Tree structures are widely applied in LPC compression due to its ability to regularize sparse geometry data, e.g., KD-tree <ref type="bibr" target="#b9">[10]</ref>, quadtree <ref type="bibr" target="#b27">[28]</ref> and octree <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Among them, the octree-based standard G-PCC <ref type="bibr" target="#b12">[13]</ref> developed by the MPEG group achieves state-of-the-art performance. G-PCC relies on the decomposition of octree geometry. Specifically, a LiDAR point cloud P is first quantized into a 2 L × 2 L × 2 L cube:</p><formula xml:id="formula_5">P = round P -bias qs , qs = bounding 2 L -1 ,<label>(5)</label></formula><p>where bias = [min(P x ), min(P y ), min(P z )], L is the target depth of the octree, and bounding is the size of the bounding box of P. Correspondingly, the inverse quantization is defined as follows:</p><formula xml:id="formula_6">P = P * qs + bias, (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where P is the reconstructed point cloud. As Figure <ref type="figure" target="#fig_0">1</ref> shows, the octree is constructed by recursively dividing the current cube of P into 8 subcubes until the current cube is empty or the recursion reaches the target depth L. An 8-bit occupancy code is assigned for each cube to indicate the occupancy situation (0 for unoccupied and 1 for occupied). However, the above methods (including G-PCC) rely on the hand-crafted context model that cannot be optimized by traversing largescale data, leading to inferior performance compared with deep-learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning-based Point Cloud Compression</head><p>Due to the applications of deep learning techniques in image/video compression, learning-based point cloud compression methods have recently emerged. In general, these learning-based methods can be summarized as point-based, voxel-based and octree-based.</p><p>1) Point-based methods: The point-based methods directly process and compress the original point cloud, e.g., Huang et al. <ref type="bibr" target="#b28">[29]</ref> utilizes Pointnet++ <ref type="bibr" target="#b29">[30]</ref> structure for feature extraction and point cloud reconstruction. Gao et al. <ref type="bibr" target="#b30">[31]</ref> further propose graph-based feature extraction and attentive sampling to improve compression efficiency. However, due to the complexity factor, the point-based methods are limited to small-scale datasets like ShapeNetCorev2 <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) Voxel-based methods: The voxel-based methods process the point clouds by voxelization. Among them, Wang et al. <ref type="bibr" target="#b32">[33]</ref> utilize a 3D CNN-based network for voxelized point cloud compression. The experiment result shows remarkable coding gain; however, the voxelization process runs counter to the sparsity nature of point clouds, leading to excessive time and memory consumption. Therefore, Wang et al. further utilize the generalized sparse convolution by Choy et al. <ref type="bibr" target="#b26">[27]</ref> and propose an end-to-end multi-scale point cloud compression framework based on the sparse CNN <ref type="bibr" target="#b33">[34]</ref>. The experiment result shows state-of-the-art performance on dense point cloud datasets 8i Voxelized Full Bodies <ref type="bibr" target="#b34">[35]</ref> (8iVFB) and Microsoft Voxelized Upper Bodies (MVUB) <ref type="bibr" target="#b35">[36]</ref>. However, the above voxel-based methods are limited to dense point clouds, which are not yet implementable on LPC <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> due to the extreme sparsity and noise brought by real-time acquisition.</p><p>3) Octree-based methods: As mentioned in Section I, The octree-based methods utilize the octree structure to regularize the point cloud structure, which is widely used in LiDAR point cloud compression due to the ability to express sparse point clouds. The auto-regressive probability model is widely used to infer the high-dimensional octree sequence. However, due to the inefficiency of the auto-regressive inference, existing octree-based methods <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> fail to balance the performance and complexity (more details in Section I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expand Expand Expand</head><p>Fig. <ref type="figure">2</ref>. The decoding pipeline of the proposed method. The solid and dashed lines represent the dependencies brought by the first-and secondorder Markovian properties. The decoding follows the order of</p><formula xml:id="formula_8">• • • → f (l) → x (l) → f (l+1) → x (l+1) → • • • .</formula><p>The "Expand" denotes the operation that expands the 8-dimensional occupancy map into the corresponding point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>The proposed framework encodes and decodes the octree in layers to ensure parallelism. The octree is alternatively expressed as an ordered set of layers: x = {x (1) , x (2) , • • • , x (L) }, where the decoding of the l-th layer x (l) depends on the previously decoded layers, i.e., x (1:l-1) . The corresponding hierarchical probability model is expressed as follows:</p><formula xml:id="formula_9">q(x) = L l q(x (l) |x (1:l-1) ).<label>(7)</label></formula><p>The high dimensionality of x (l) makes its probability distribution intractable to compute. Instead of relying on the inefficient auto-regressive models for the node-wise decomposition of x (l) , we introduce a latent variable f (l) , conditioned on which the nodes in x (l) are assumed to be independent <ref type="bibr" target="#b36">[37]</ref>. Figure <ref type="figure">2</ref> shows the decoding pipeline with the introduction of f (l) . Through f (l) , the spatial correlation of x (l) is captured as follows:</p><formula xml:id="formula_10">q(x (l) |f (l) , x (1:l-1) ) = N l i q(x (l) i |f (l) , x (1:l-1) ),<label>(8)</label></formula><p>where N l is the number of nodes in the l-th layer. f (l) serves as side information to signal modifications to the probability distribution x (l) that requires extra bits to express. Since f (l)  is to be encoded and decoded together with x (l) , we express the the joint probability distribution of x (l) and f (l) :</p><formula xml:id="formula_11">q(x (l) , f (l) |x (1:l-1) , f (1:l-1) ; ψ) = q(x (l) |x (1:l-1) , f (1:l) ; ψ x )q(f (l) |x (1:l-1) , f (1:l-1) ; ψ f ),<label>(9)</label></formula><p>where ψ x and ψ f are the parameters for decoding x (l) and f (l) . By assuming the Markovian property, the joint distribution is reduced to the product of the following two terms: For each layer, x (l) is the l-th octree layer and e (l) is the corresponding occupancy embedding. f (l) is the latent variable of the lth layer. r(l) is the reconstruction residual. f (l) and f (l) are respectively the predicted and reconstructed latent variable.and + denote the soft subtraction and soft addition operator. Q denotes quantization.</p><formula xml:id="formula_12">q (l) x = q(x (l) |x (l-1) , f (l) , x (l-2) , f (l-1) ; ψ x ) q (l) f = q(f (l) |x (l-1) , f (l-1) , x (l-2) , f (l-2) ; ψ f ).<label>(10)</label></formula><p>Note that we assume second-order Markovian property to exploit the context of lower layers. q (l)</p><p>x can be derived by a classification network with 256 dimensions, each indicating the probability of an occupancy code. However, q (l) f is unable to be derived in the same way, because different from x (l) in one-hot representation, f (l) is a numerical vector whose distribution cannot be explicitly represented by a classification network. Therefore, we first apply residual coding to remove the redundancy in f (l) :</p><formula xml:id="formula_13">f (l) = f (l) + r (l) , with f (l) = g d (x (l-1) , f (l-1) , x (l-2) , f (l-2) ; ψ f ),<label>(11)</label></formula><p>where g d is a neural network with ψ f as its parameters, f (l) is the predictable component given its context, which requires no extra bits; and r (l) is the residual. As we have no prior beliefs about r (l) , we assume it to be independent and identically distributed (i.i.d.), and model it with a fully-factorized entropy model <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_14">q (l) f = q(r (l) ) = N i q(r (l) i ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK ARCHITECTURE</head><p>The overall architecture is shown in Figure <ref type="figure" target="#fig_1">3</ref>. The proposed network follows a 5-layer architecture, where each layer consists of an embedding network, an encoder network, and a decoder network. Each octree layer x (l) passes through the embedding network (chapter IV-A) to generate the occupancy embedding e (l) . The encoder network (chapter IV-B) analyses the correlation between e (l) and f (l+1) (assume f (L+1) = 0 for consistency) and outputs the latent variable f (l) encapsulating the spatial correlation of input variables. The decoder network (chapter IV-D) decodes the l-th octree layer x (l) from the previously decoded context, including f (l) , e (l-1) , f (l-1) and e (l-2) , where f (i) is the reconstructed latent variable of the i-th layer. Meanwhile, the decoder network generates the predicted latent variable f (l+1) for the higher layer, which is softly subtracted (chapter IV-C) from f (l+1) to generate the residual r (l+1) . r (l+1) is quantized into r(l+1) with the quantization step of 1 before entropy coding. During training, the quantization is replaced by adding a uniformly distributed noise µ ∼ U(- 1  2 , 1 2 ) to maintain the differentiablity of the network. Due to the i.i.d. assumption illustrated in Equation ( <ref type="formula" target="#formula_14">12</ref>), we utilize a factorized entropy model <ref type="bibr" target="#b13">[14]</ref> (chapter IV-E) to compress the quantized residual r(l+1) , which is softly added (chapter IV-C) with f (l+1) to generate the reconstructed latent variable f (l+1) for the higher layer.</p><p>Due to the negligible spatial correlation after downsampling, f (L-5) is compressed by a factorized entropy model, and x (L-6) is compressed by OctSqueeze <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Occupancy Embedding</head><p>The architecture of the occupancy embedding module is shown in Figure <ref type="figure" target="#fig_2">4</ref>(e). The occupancy embedding module takes the one-hot octree layer x (l) as input and outputs the corresponding occupancy embedding e (l) . The network aims to encode the semantic information of x (l) into a densely distributed representation that can be better processed by a neural network. As shown in Figure <ref type="figure" target="#fig_2">4</ref>(e), the network consists of three sparse convolution layers, which are trained jointly with the entire network. Ablation studies in chapter VI-D demonstrate the effectiveness of the occupancy embedding network, where we can observe that occupancy codes geometrically closer are more similar in the vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder Network</head><p>The architecture of the encoder network is shown in Figure <ref type="figure" target="#fig_2">4</ref>(a). Equation <ref type="bibr" target="#b9">(10)</ref> shows that the probability distributions of x (l) and f (l+1) is correlated with latent variable f (l) of the l-th layer. Therefore, the encoder network (Figure <ref type="figure" target="#fig_2">4(a)</ref>) is designed as a deterministic transform on x (l) and f (l+1) to encapsulate their spatial correlation into f (l) :</p><formula xml:id="formula_15">f (l) = g e (x (l) , f (l+1) ; ψ e ),<label>(13)</label></formula><p>where ψ e is the encoder parameters. Inspired by Wang et al.</p><p>[34], we adopt sparse-CNN to design g e (•) for high-efficiency octree feature extraction. The octree sequence is modeled as a sparse tensor mentioned in equation (??), where the coordinate matrix C corresponds to the coordinates of the octree nodes, and the feature matrix F corresponds to the occupancy code.</p><p>The down-scaling of the octree layers is achieved by a stridetwo and kernel-size-two sparse convolution layer. Specifically, f (l+1) is first down-sampled by a stride-two sparse convolution layer, followed by several Inception Residual Network (IRN) blocks <ref type="bibr" target="#b37">[38]</ref> (Figure <ref type="figure" target="#fig_3">5</ref>) for neighborhood feature extraction and aggregation. For the feature fusion of f (l+1) and x (l) , the down-sampled f (l+1) is concatenated with the occupancy embedding e (l) and passes through a sparse convolution layer to generate the latent variable f (l) for the l-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Subtraction/Addition</head><p>The quantization of the residual r (l) introduces error to decoding, leading to noise between the latent variable f (l) and its reconstruction f (l) . Therefore, instead of directly adding the quantized residual r(l) to the predicted latent variable f (l) shown in equation <ref type="bibr" target="#b10">(11)</ref>, we propose soft addition (denoted as ⊕) to compensate for the information loss during quantization:</p><formula xml:id="formula_16">f (l) = f (l) ⊕ r(l) = f (l) + h a (r (l) , f (l) ; ψ a ),<label>(14)</label></formula><p>where h a (•) is a transform with ψ a as its parameters. Similarly, the soft subtraction (denoted as ) is defined as follows:</p><formula xml:id="formula_17">r (l) = f (l) f (l) = f (l) -h s ( f (l) , f (l) ; ψ s ).<label>(15)</label></formula><p>Note that the difference between f (l) and f (l) can be arbitrarily close to zero provided that with additional bit rate to encode r(l) and the corresponding proper choice of h a (•), h s (•).</p><p>The network dynamically adjusts the quantization precision of each node in f (l) to balance the reconstruction quality of f (l) and the corresponding bit rate consumption in an end-toend manner. h a (•) and h s (•) (Figure <ref type="figure" target="#fig_2">4</ref>(c)(d)) are designed as sparse convolution layers to fit arbitrary transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Decoder Network</head><p>The design of the decoder network follows equation ( <ref type="formula" target="#formula_12">10</ref>) and <ref type="bibr" target="#b10">(11)</ref>. When constructing the network, we assume secondorder Markovian property for the system stability, as the lower layer context is potentially informative for the entropy coding of the current layer. To demonstrate the necessity of secondorder Markov property, we compare the performance of firstand second-order Markov assumptions in chapter VI-B.</p><p>Figure <ref type="figure" target="#fig_2">4</ref>(b) shows the architecture of the decoder network, which is divided into two branches corresponding to the decoding of x (l) and f (l+1) respectively:</p><p>1) The decoding of x (l) : The context of x (l) , including x (l-1) , f (l) , x (l-2) and f (l-1) are concatenated together and pass through a 2-layer sparse CNN. Finally, a 256-dimensional softmax layer is used to generate the probability distribution of x (l) (denoted as p (l) ∈ R N l ×256 ). During training, we directly represent the bit rate of entropy coding with the cross entropy between x (l) and p (l) , since cross entropy is a tight lower bound achievable using arithmetic coding algorithms. When evaluating the model, an arithmetic coder is used to encode the difference between p (l) and x (l) to ensure that the decoding of x (l) is lossless.</p><p>2) The decoding of f (l+1) : The context x (l) , f (l) , x (l-1) and f (l-1) are concatenated together and up-sampled to generate the predicted latent variable f (l+1) . We adopt a stride-two and kernel-size-two sparse transposed convolution layer to achieve the octree up-sampling. Similar to the encoder network, we utilize several IRN modules for local feature analysis and aggregation and a stride-one sparse convolution layer to adjust the dimension of output.</p><p>Note that the nodes in the same layer share the same context, meaning that the decoding process of each node is independent of other nodes in the same layer. This independence brings superior parallelism to the entire network. Compared with Oc-tAttention, which requires re-running the model multiple times to overcome the continuous varying of context, our proposed model needs only run once during decoding. Therefore, one can observe significant runtime improvement in the subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Factorized Deep Entropy Model</head><p>We adopt the factorized deep entropy model <ref type="bibr" target="#b13">[14]</ref> for entropy coding. Two operations in the entropy encoding system block the back-propagation of gradients: 1) quantizing the input variable y into ŷ, 2) encoding the quantized variable to and from the bitstream. For 1), the quantization operation is replaced by adding a uniformly distributed noise µ ∼ U(-1 2 , 1 2 ) to ensure differentiability during training.</p><p>For 2), thanks to the development of arithmetic coding, we can approximate the bit rate of encoding ŷ without the actual encoding and decoding process. The estimated size of bitstream is the entropy of ŷ, which is a tight lower bound achievable by arithmetic coding algorithms:</p><formula xml:id="formula_18">R ŷ = E ŷ [-log q ŷ(ŷ)] (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>We follow <ref type="bibr" target="#b13">[14]</ref> and model the probability distribution of ŷ with a fully factorized density model:</p><formula xml:id="formula_20">q ŷ|φ (ŷ|φ) = i q yi|φ (i) (φ (i) ) * U(- 1 2 , 1 2 ) (ŷ i ), (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>where q yi|φ (i) is the univariate distribution of the i-th channel and φ (i) is the corresponding parameters. In general, adding a hyperprior variable z upon ŷ can further capture the spatial correlation of ŷ for more bit rate reduction <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However, it is unnecessary for our proposed model due to the i.i.d. assumption of the residual r (l) in equation ( <ref type="formula" target="#formula_14">12</ref>). More information can be found in Section VI-C that plots the visualization of r (l) . One can observe the strong independence of r (l) among nodes in the same layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Loss function</head><p>The model is trained in an end-to-end manner with the following loss function:</p><formula xml:id="formula_22">L = α L l=L-k R (l) r + L l=L-k R (l) x , (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>where k is the number of layers, R</p><p>r is the bits per output point bpop to encode the quantized residual r(l) (let r(L-k) = f (L-k) for consistency) of the l-th layer, R (l)</p><p>x is the bpop to entropy encode x (l) . In experiments, we find that the model easily converges to a local optimum where the bit rate of f (l) is extremely low. Therefore, we add a scaling factor α to encourage the model to output more informative f (l) . α is set as 0.5 for the first several epochs and 0.95 after that. R x is computed by the binary cross entropy (BCE) between the probability p (l) and the ground truth x (l) :</p><formula xml:id="formula_25">R (l) x = - 1 N l N l i=1 256 j=1 x (l) ij log 2 p (l) ij ,<label>(19)</label></formula><p>where N l is the number of nodes in the l-th layer, and N is the number of points in the output point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Datasets</head><p>• SemanticKITTI <ref type="bibr" target="#b39">[40]</ref> is a public LiDAR dataset for autonomous driving, containing 22 sequences with 43352 scans collected from a Velodyne HDL-64 sensor. We SemanticKITTI Ford Fig. <ref type="figure">6</ref>. The visualization of datasets. We randomly choose six scans from two datasets. The upper row is the scans from SemanticKITTI. The lower row is the scans from Ford. Test Conditions (CTC) <ref type="bibr" target="#b40">[41]</ref>. The dataset contains three sequences with 4500 frames. The precision of the original point cloud is 18 bits. We divide sequence 01 as the training set and sequences 02 and 03 as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Strategy</head><p>• For SemanticKITTI, we train four models for octree depth L = 12, 11, 10, 9. We utilize an Adam optimizer with β = (0.9, 0.999) and the initial learning rate set as 0.0006. We further utilize a learning rate scheduler with a decay rate of 0.7 for every 20 epochs. We train the proposed model for 200 epochs, with 1200 iterations for each epoch. For the first 10 epochs, α in equation ( <ref type="formula" target="#formula_22">18</ref>) is set as 0.5. The batch size is set as 4 during training. We conduct all the experiments on a GeForce RTX 3090 GPU with 24GB memory.</p><p>• For Ford, we also train four models for octree depth L = 12, 11, 10, 9. As Ford is a small dataset with only 1500 frames for training, we first train the L = 12 model for 40 epochs. α in equation ( <ref type="formula" target="#formula_22">18</ref>) is set as 0.5 for the first two epochs and 0.95 for the remaining. For the models of L = 11, 10, 9, since the amount of quantized data is too small to support the end-to-end training, we perform four epochs of finetuning on the L = 12 model on Ford dataset with the initial finetuning learning rate 0.0002. α is set as 0.95 during finetuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Setup</head><p>We compare our proposed model with G-PCC and other learning-based LPC compression network including Oct-Squeeze, VoxelContext-Net and state-of-the-art method OctAttention:</p><p>• For G-PCC, we follow the MPEG CTC to generate the results on SemanticKITTI and Ford with the latest TMC13-v14. • For OctSqueeze, as the source code is not publicly available, we re-implement their model and train for 200 epochs on SemanticKITTI, 40 epochs on Ford 01. • For VoxelContext-Net, as the source code is also unavailable, we adopt the re-implemented result by Muhammad et al. <ref type="bibr" target="#b41">[42]</ref>, which reports better results than <ref type="bibr" target="#b21">[22]</ref>. We remove the coordinate refinement module (CRM) in VoxelContext-Net for the fairness of comparison. It is a post-processing module that enhances the quantized point cloud to reduce distortion but is irrelevant from the entropy coding of point cloud geometry. • For OctAttention, we adopt the result on SemanticKITTI in the paper. Since they have no available results on the Ford dataset, we retrain their model on Ford 01 for 8 epochs (the same number of epochs as SemanticKITTI) to generate the result on Ford 02 and 03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>The bit rate is evaluated using bits per input point (bpip), and the distortion is evaluated using point-to-point geometry (D1) Peak Signal-to-Noise Ratio (PSNR) and point-to-plane  geometry (D2) PSNR, which are computed by the software pc error developed by MPEG. For Ford dataset, we follow the MPEG CTC that sets the PSNR peak value p = 30000.</p><p>For the floating-point SemanticKITTI dataset, we follow the evaluation metric in <ref type="bibr" target="#b22">[23]</ref> that scales the point clouds into [-1, 1] 3 , and sets the PSNR peak value p = 1. We also compare the chamfer distance (CD) among all methods:</p><p>CD(P, P) = max(CD (P, P), CD ( P, P)),</p><formula xml:id="formula_26">CD (P, P) = 1 |P| i min j p i -pj 2 .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Results on SemanticKITTI:</head><p>The rate-distortion (D1-PSNR, D2-PSNR, chamfer distance) curves of different methods on SemanticKITTI are shown in Figure <ref type="figure">7</ref>. We can observe that the four learning-based methods outperform G-PCC due to the optimization by traversing the whole dataset. Besides, OctSqueeze and Voxelcontext-Net assume different degrees of node independence considering the time complexity; thus, neither fully explores the sibling correlation, leading to suboptimal performance. Our method dramatically surpasses the two methods due to the hierarchical latent variable, which encapsulates the correlation of sibling nodes to remove spatial redundancy. Meanwhile, our model surpasses OctAttention, which considers sibling dependence despite the excessive complexity. We also report the corresponding BD-Rate gain of the four learning-based methods against G-PCC in Table <ref type="table" target="#tab_0">I</ref>. The experiment result shows that our model achieves state-ofthe-art coding efficiency on SemanticKITTI, with the highest -28.27% BD-Rate gain against G-PCC.</p><p>2) Results on Ford: The rate-distortion curves of different methods on Ford are also shown in Figure <ref type="figure">7</ref>, and the corresponding BD-rate gain against G-PCC is shown in Table <ref type="table" target="#tab_0">I</ref>. Ford is a dataset suggested by MPEG CTC. Our proposed model also achieves state-of-the-art performance on Ford, with the highest -25.55% BD-Rate gain against G-PCC. The experiment result demonstrates the extensibility of our model.</p><p>3) Complexity Comparison: In this section, we evaluate the complexity. Figure <ref type="figure">8</ref> compares the decoding time of each method. We can observe that:</p><p>• The complexity of OctAttention is extraordinarily high compared to other methods. This is due to the sibling context selection strategy, which causes the context to change during the decoding process and requires multiple runs of the model to update the context. OctAttention requires more than 5 minutes to decode one point cloud frame, which is unaffordable in practical applications. loss and the corresponding bit rate of f (l) , thus achieving 3.47% BD-Rate gain against the ordinary addition/subtraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation study on the Markovian property</head><p>In equation <ref type="bibr" target="#b9">(10)</ref>, we assume the Markovian property of f (l) and x (l) for simplification. When establishing the network, we instead assume second-order Markovian property for system stability in the hope that the lower layer context is also informative for the entropy coding of the current layer. We separately train two models of first-order and second-order Markovian properties on the training split of SemanticKITTI and evaluate the result on the test split. In Figure <ref type="figure" target="#fig_6">9</ref>, we show the performance comparison of the model based on first-order and second-order Markovian properties. The experimental result proves that the assumption of second-order Markovian property is effective, which achieves 1.09% BD-Rate gain against first-order Markovian property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis on Latent Variable</head><p>Defined by equation <ref type="bibr" target="#b10">(11)</ref>, the latent variable f (l) of the lth octree layer is decomposed into two parts: the predictable component f (l) given the previously decoded context, and the independent residual component r(l) with no prior knowledge. In this section, we visualize the decomposition process of f (l) in Figure <ref type="figure" target="#fig_0">11</ref>. The middle-left column is the visualization of f (l) , with clearly visible spatial dependence among each node. The middle-right column is the visualization of the residual r(l) that captures the unpredictable component of f (l) . We can observe that r(l) consists of a large number of noisy points, which demonstrates the i.i.d. nature of r(l) in equation <ref type="bibr" target="#b11">(12)</ref>. The rightmost column is the visualization of predictable component f (l) that captures the sibling dependence of f (l) , thus we can observe stronger spatial correlation than f (l) . The visualization indicates that the residual coding successfully captures the dependence of high-scale data into a low-scale variable, and guarantees the i.i.d. nature of the remaining information.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows the bit rate consumption to encode the latent variable and occupancy, and the effect of the latent variable on encoding occupancy. The bit rate of the latent variable increases slowly with the overall bit rate, accounting for less than 10%. However, the benefit of introducing latent variables as side information is significant. The bit rate of encoding the occupancy is reduced by more than 40% due to the latent variable. Note that the network autonomously balances the bit rate of each part via end-to-end training, with the final bit rate as the optimization goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis on Occupancy Embedding Module</head><p>To validate the effectiveness of the occupancy embedding module, we compute the feature embedding of different occupancy codes. In Figure <ref type="figure" target="#fig_8">13</ref>, we compare the Euclidean distance of three occupancy codes from the base occupancy code 11110000 (240). Note that the input occupancy codes are one-hot vectors whose distance is one between each other. However, after being converted by the embedding module, occupancy codes that are geometrically closer are similar in the feature space, demonstrating that the occupancy embedding module learns the geometric semantics of the occupancy codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper proposes an end-to-end, fully-factorized LiDAR point cloud compression framework based on the hierarchical decomposition of the octree entropy model. The network utilizes a latent variable to encapsulate the sibling and ancestor dependence, through which the sibling nodes are conditionally independent to be decoded in parallel. We further propose a residual coding framework for the entropy coding of the latent variable. The latent variable is hierarchically downsampled to capture the deeper spatial correlation, and the residual is modeled by a factorized entropy model. We propose to use soft addition/subtraction to substitute ordinary addition/subtraction for the flexibility of the network. The experiment result shows that our framework achieves state-of-the-art performance among all the LPC compression frameworks. It is noted that our framework also shows superior time complexity, saving more than 99.8% decoding time compared with the previous state-of-the-art framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The demonstration of octrees. The left three graph shows the hierarchical construction process of the octree sequence with depths 9, 8, and 7. The rightmost graph shows a part of an octree sequence, where black indicates occupied, and white indicates unoccupied. The occupied nodes need to be subdivided into smaller cubes until reaching the specified depth.</figDesc><graphic coords="2,52.00,56.53,115.96,116.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The overall architecture of the proposed model. The model consists of 5 layers. For each layer, x (l) is the l-th octree layer and e (l) is the corresponding occupancy embedding. f (l) is the latent variable of the lth layer. r(l) is the reconstruction residual. f (l) and f (l) are respectively the predicted and reconstructed latent variable.and + denote the soft subtraction and soft addition operator. Q denotes quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The detailed network architecture of (a) the encoder module, (b) the decoder module, (c) the soft addition module, (d) the soft subtraction module, (e) the occupancy embedding module. Conv(C, K, S) denotes a sparse convolution layer with output channel size C, kernel size K, stride S.IRN stands for the Inception Residual Network in<ref type="bibr" target="#b33">[34]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The Inception Residual Network (IRN) block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>r</head><label></label><figDesc>is computed by the factorized entropy model mentioned in chapter IV-E. R (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Rate-distortion curves on SemanticKITTI and Ford. From left to right: D1-PSNR, D2-PSNR and chamfer distance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Ablation study of second-order Markovian property and first-order Markovian property.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Ablation study of the model with soft addition/subtraction and w/o addition/subtraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Analysis on occupancy embedding module. The left column is the occupancy code of octree nodes. The middle column is the visualization of the octree node. The right column is the Euclidean distance of the feature of octree nodes from the octree node 11110000 (240) in feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DIFFERENT</head><label>I</label><figDesc>BD-RATE(%) GAINS AGAINST G-PCC.</figDesc><table><row><cell></cell><cell cols="4">Ours OctAttention Voxelcontext-Net OctSqueeze</cell></row><row><cell cols="2">SemanticKITTI -28.27</cell><cell>-25.33</cell><cell>-14.37</cell><cell>-3.95</cell></row><row><cell>Ford</cell><cell>-25.55</cell><cell>-22.01</cell><cell>-11.68</cell><cell>-2.98</cell></row><row><cell cols="5">adopt the official data splitting that takes sequences 00 to</cell></row><row><cell cols="5">10 as the training set and 11 to 21 as the test set.</cell></row></table><note><p>• Ford is a LiDAR dataset suggested by MPEG Common</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig.12. The overall bit rate of the entropy model with latent variable (orange line), the bit rate of the occupancy with latent variable (green line), the bit rate of the latent variable (red line), and the overall bit rate of the entropy model if no latent variable is introduced to capture the sibling dependency (blue line). The x-axis is the depth of the octree, i.e., L. The performance comparison is made on SemanticKITTI-00-000000.</figDesc><table><row><cell></cell><cell>4 5</cell><cell></cell><cell>bpip w/o latent overall bpip with latent occupancy bpip latent bpip</cell></row><row><cell>bpip</cell><cell>2 3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>9</cell><cell>10 Octree Depth 11</cell><cell>12</cell></row></table><note><p>• On the contrary, our model is end-to-end and fullyfactorized, where the nodes in the same layer are independently decoded. Compared with OctAttention, our model is highly parallelized and requires running only once, thus</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This paper is supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61971282</rs>, <rs type="grantNumber">U20A20185</rs>). The corresponding author is Yiling Xu(e-mail: yl.xu@sjtu.edu.cn).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sJHwMsb">
					<idno type="grant-number">61971282</idno>
				</org>
				<org type="funding" xml:id="_k4YETBK">
					<idno type="grant-number">U20A20185</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. The color is visualized by random projection <ref type="bibr" target="#b42">[43]</ref>, which projects the high dimensional latent variable and residual into 3D color space, then normalizes the 3D vectors to 0-255 to represent the RGB value. The dependence can be judged by color. The area with similar colors can be regarded as highly spatial-correlated.</p><p>saving more than 99.8% of decoding time.</p><p>• Due to the complexer network structure, our model is slightly slower than OctSqueeze, which assumes the independence of sibling nodes and also decodes layerwisely. But our model explores the correlation of sibling nodes, thus our model achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ANALYSIS AND ABLATION STUDY</head><p>A. Ablation study on soft addition/subtraction</p><p>In chapter IV-C, we propose substituting the ordinary addition/subtraction operator for the soft addition/subtraction. Therefore, we perform an ablation study on its effectiveness in Figure <ref type="figure">10</ref>. We separately train the two models on the training split of SemanticKITTI and evaluate the result on the test split. The experiment result proves that the soft addition/subtraction enables the network to dynamically balance the reconstruction</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introduction to point cloud compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ZTE Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View-dependent dynamic point cloud compression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lossy point cloud geometry compression via region-wise processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4575" to="4589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low latency scalable point cloud communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kathariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2369" to="2373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attan: Attention adversarial networks for 3d point cloud semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="789" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A regularized projection-based geometry compression scheme for lidar point cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frame-level rate control for geometrybased lidar point cloud compression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d point cloud compression using conventional image compression for efficient data transmission</title>
		<author>
			<persName><forename type="first">H</forename><surname>Houshiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nüchter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 XXV International Conference on Information, Communication and Automation Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric compression for interactive transmission</title>
		<author>
			<persName><forename type="first">O</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Gandoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization 2000. VIS 2000</title>
		<meeting>Visualization 2000. VIS 2000</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
	<note>Cat. No. 00CH37145</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Octree-based point-cloud compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PBG@ SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric modeling using octree encoding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics and image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="147" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emerging mpeg standards for point cloud compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Preda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Baroncini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krivokuća</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="148" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end optimized image compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dvc: An end-to-end deep video compression framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fvc: A new framework towards deep video compression in feature space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1502" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coarse-tofine deep video coding with hyperprior-guided mode prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5921" to="5930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextual video compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="114" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Octsqueeze: Octree-structured entropy model for lidar compression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1313" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Muscle: Multi sweep compression of lidar using deep entropy models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voxelcontext-net: An octree based framework for point cloud compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Que</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6042" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Octattention: Octree-based large-scale contexts model for point cloud compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="625" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOBILE mobile computing and communications review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable point cloud geometry coding with binary tree embedded quadtree</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kathariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d point cloud geometry compression on deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="890" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point cloud geometry compression via neural graph sampling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3373" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lossy point cloud geometry compression via end-to-end learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4909" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale point cloud geometry compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 Data Compression Conference (DCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">8i voxelized full bodies-a voxelized point cloud dataset</title>
		<author>
			<persName><forename type="first">E</forename><surname>D'eon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Philip</surname></persName>
		</author>
		<idno>ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) input document WG11M40059/WG1M74006</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft voxelized upper bodies -a voxelized point cloud dataset</title>
		<author>
			<persName><forename type="first">L</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISO/IEC MPEG m38673</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Latent variable models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="371" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Coarse-to-fine hyper-prior modeling for learned image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Common test conditions for point cloud compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martin-Cocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
		<idno>IEC JTC1/SC29/WG11 w17766</idno>
	</analytic>
	<monogr>
		<title level="j">Document ISO</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Ljubljana, Slovenia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Point cloud geometry compression using learned octree entropy coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<idno>ISO/IEC JTC1/SC29/WG7 m59528</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random projection in dimensionality reduction: applications to image and text data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
