<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Multi-Dimensional Model for Global and Time-Series Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-05-06">6 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Anil</forename><surname>Ramakrishna</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
						</author>
						<title level="a" type="main">Joint Multi-Dimensional Model for Global and Time-Series Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-06">6 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.03117v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Annotation fusion</term>
					<term>Emotion annotations</term>
					<term>Multi-dimensional annotations</term>
					<term>Time series annotation modeling</term>
					<term>Expectation Maximization</term>
					<term>Factor Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowdsourcing is a popular approach to collect annotations for unlabeled data instances. It involves collecting a large number of annotations from several, often naive untrained annotators for each data instance which are then combined to estimate the ground truth. Further, annotations for constructs such as affect are often multi-dimensional with annotators rating multiple dimensions, such as valence and arousal, for each instance. Most annotation fusion schemes however ignore this aspect and model each dimension separately. In this work we address this by proposing a generative model for multi-dimensional annotation fusion, which models the dimensions jointly leading to more accurate ground truth estimates. The model we propose is applicable to both global and time series annotation fusion problems and treats the ground truth as a latent variable distorted by the annotators. The model parameters are estimated using the Expectation-Maximization algorithm and we evaluate its performance using synthetic data and real emotion corpora as well as on an artificial task with human annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Crowdsourcing is a popular tool used in collecting human judgments on subjective constructs such as emotion. Typical examples include annotations of images and video clips with categorical emotions or with continuous affective dimensions such as valence or arousal. Online platforms such as Amazon Mechanical Turk 1 (MTurk) and Crowdflower 2 have risen in popularity owing to their inexpensive annotation costs and their ability to scale efficiently.</p><p>Crowdsourcing is also a popular approach in collecting labels for training supervised machine learning algorithms. Such labels are typically obtained from domain experts, which can be slow and expensive. For example, in the medical domain, it is often expensive to collect diagnosis information given laboratory tests since this requires judgments from trained professionals. On the other hand, unlabeled patient data may be easily available. Crowdsourcing has been particularly successful in such settings with easy availability of unlabeled data instances since we can collect a large number of annotations from untrained and inexpensive workers over the Internet, which when combined together may be comparable or even better than expert annotations <ref type="bibr" target="#b0">[1]</ref>.</p><p>A typical crowdsourcing setting involves collecting annotations from a large number of workers; hence there is a need to robustly combine them to estimate the ground truth. The most common approach for this is to take simple averages for continuous annotations or perform majority voting for categorical annotations. However, this assumes uniform competency across all the workers which is not always guaranteed or justified. Several alternative approaches have been proposed to address this challenge, each assuming 1. www.mturk.com 2. www.crowdflower.com a specific function modeling the annotators' behavior. In practice, it is common to collect annotations on multiple questions for each data instance in order to reduce costs, the annotators' mental load or even to improve annotation accuracy. For example, if we're annotating valence and arousal for a given data instance (such as a single image or video segment), collecting annotations on both these dimensions in one session per instance may be preferred over collecting valence annotations for all instances followed by arousal.</p><p>Such a joint annotation task may entail task specific or annotator specific dependencies between the annotated dimensions. In the aforementioned example, task specific dependencies may occur due to inherent correlations between the valence and arousal dimensions depending on the experimental setup. Annotator specific dependencies may occur due to a given annotator's (possibly incorrect or incomplete) understanding of the annotation dimensions. Hence it is of relevance to model the dimensions jointly. However, most state of the art models in annotation fusion combine the annotations by treating the different dimensions independently.</p><p>Joint modeling of the annotation dimensions may result in more accurate estimates of the ground truth as well as in giving a better picture of the annotators' behavior. In this work, we address this goal by proposing a multidimensional model which makes use of any potential relationships between the annotation dimensions while combining them. The model we propose is applicable to both the global annotation setting (such as while collecting emotion annotations on a picture, judgment about the overall tone of a conversation, etc.) as well as time series annotations (for example, time continuous annotations of audio/video clips on dimensions such as engagement or affect). Our model treats the hidden ground truth as latent variables and estimates them jointly with the annotator parameters using the Expectation Maximization (EM) algorithm <ref type="bibr" target="#b1">[2]</ref>. We evaluate the model in both settings with both synthetic and real emotion corpora. We also create an artificial annotation task with controlled ground truth which is used in the model evaluation for both settings.</p><p>The main contributions of this work are as follows:</p><p>1) We propose a unified model to capture relationships between annotation dimensions. For ease of exposition we focus on the linear case in this paper.</p><p>2) The linear model we propose results in an annotator specific matrix which captures this annotator level relationship between the annotation dimensions. 3) We create a novel multi-dimensional annotation task with controlled ground truth and use it to evaluate both the global and time series annotation settings of the model.</p><p>The rest of the paper is organized as follows. In Section 2, we review related work and motivate the problem in Section 3. In Section 4, we describe the proposed model and provide equations for parameter estimation using EM algorithm (derivations are deferred to the appendix). We evaluate the model in Section 5 and provide conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several authors, most notably <ref type="bibr" target="#b0">[1]</ref>, assert the benefits of aggregating opinions from many people which is often believed to be better than those from a small number of experts, under certain conditions. Often referred to as the wisdom of crowds, this approach has been remarkably popular in recent times, specially in fields such as psychology and behavioral sciences where a ground truth may not be easily accessible or may not exist. This popularity can be largely attributed to online crowdsourcing platforms such as Mturk that connect researchers with low cost workers from around the globe. Along with cost, scalability is another major appeal with such tools leading to their frequent use in machine learning, leveraging large scale annotation of data instances such as images <ref type="bibr" target="#b2">[3]</ref>, audio/video clips <ref type="bibr" target="#b3">[4]</ref> and text snippets <ref type="bibr" target="#b4">[5]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a common setting in the crowdsourcing paradigm. For each data instance m, annotator k provides a noisy annotation a m,d k which depends on the ground truth a m,d * where d is the dimension being annotated. Since we collect several annotations for each m, we need to aggregate them to estimate the unknown ground truth. The most common technique used in this aggregation is to take the average value in case of numeric annotations or perform majority voting in the case of categorical annotations as shown in Equation <ref type="formula" target="#formula_0">1</ref>.</p><formula xml:id="formula_0">a m,d * = argmax j k 1{a m,d k == j}<label>(1)</label></formula><p>where, 1{} is the indicator function.</p><p>While simple and easy to implement, this approach assumes consistent reliability among the different annotators which seems unreasonable, especially in online platforms such as Mturk. To address this, several approaches have been suggested that account for annotator reliability in estimating the ground truth.</p><p>Early efforts to capture reliability in annotation modeling <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> assumed specific structure to the functions modeled by each annotator. Given a set of annotations a m,d k along with the corresponding function parameters, the ground truth is estimated using the Maximum A Posteriori (MAP) estimator.</p><formula xml:id="formula_1">a m,d * = argmax j k log p(a m,d k |a m,d * = j) + log p(a m,d * = j)<label>(2)</label></formula><p>where p(a m,d * ) is the prior probability of ground truth. In <ref type="bibr" target="#b5">[6]</ref>, the categorical ground truth label a m,d * = i is modified probabilistically by annotator k using a stochastic matrix Π k as shown in Equation 3 in which each row is a multinomial conditional distribution given the ground truth.</p><formula xml:id="formula_2">P (a m,d k = j|a m,d * = i) = π k ij (3)</formula><p>Given annotations from K different annotators, their parameters Π k and prior distribution of labels p j = P (a m,d * = j), the ground truth is estimated using MAP estimation as before.</p><formula xml:id="formula_3">a m,d * = argmax j k log π j(a m,d k ) + log p j<label>(4)</label></formula><p>The above expression makes a conditional independence assumption for annotations given the ground truth label. Since we do not typically have the annotator parameters Π k , these are estimated using the EM algorithm.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows an extension of the model in Figure <ref type="figure" target="#fig_0">1</ref> in which we learn a predictor (classifier/regression model) for the ground truth jointly with annotator parameters. Such a predictor may be used to estimate the ground truth for new unlabeled data instances. This strategy of jointly modeling the annotator functions as well as the ground truth predictor has been shown to have better performance when compared to predictors trained independently using the estimated ground truth <ref type="bibr" target="#b7">[8]</ref>. The ground truth estimate in this model is given by </p><formula xml:id="formula_4">a m,d * = argmax a m,d * k log p(a m,d k |a m,d * ) + log p(a m,d * |x m ) (5)</formula><p>Recently, several additional extensions have been proposed to the model in Figure <ref type="figure" target="#fig_1">2</ref>; For example, in <ref type="bibr" target="#b8">[9]</ref>, the authors assume varying regions of annotator expertise in the data feature space and account for this using different probabilities for label confusion for each region. The authors show that this leads to a better estimation of annotator reliability and ground truth.</p><p>The models described so far have been designed for annotation tasks in which the task is to rate some global property of the data. For example, in image based emotion annotation, the task may be to provide annotations on affective dimensions such as valence and arousal conveyed by each image. However, human interactions often involve variations of these dimensions over time <ref type="bibr" target="#b9">[10]</ref> which are captured using time series annotations from audio/video clips. Various tools have been developed to collect such annotations, including Anvil <ref type="bibr" target="#b10">[11]</ref>, Feeltrace <ref type="bibr" target="#b11">[12]</ref>, EMuJoy <ref type="bibr" target="#b12">[13]</ref>, Gtrace <ref type="bibr" target="#b13">[14]</ref> and DARMA <ref type="bibr" target="#b14">[15]</ref> (for a review of available tools and their properties, see <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b14">[15]</ref>). In fusing such time series annotations, the previously mentioned models are applicable only if annotations from each frame are treated independently. However, this entails several unrealistic assumptions such as independence between frames, zero lag in the annotators and synchronized response in the annotators to the underlying stimulus.</p><p>Several works have been proposed to capture the underlying reaction lag in the annotators. <ref type="bibr" target="#b16">[17]</ref> proposed a generalization of Probabilistic Canonical Correlation Analysis (PCCA) <ref type="bibr" target="#b17">[18]</ref> named Dynamic PCCA which captures temporal dependencies of the shared ground truth space in a generative setting, and incorporated a latent time warping process to implicitly handle the reaction lags in annotators. They have further proposed a supervised extension of their model which jointly learns a predictor function for the latent ground truth signal similar to <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b18">[19]</ref> address the reaction lag by explicitly finding the time shift that maximizes the mutual information between expressive behaviors and their annotations. <ref type="bibr" target="#b19">[20]</ref> generalize the work of <ref type="bibr" target="#b18">[19]</ref> by using a linear time invariant (LTI) filter which can also handle any bias or scaling the annotators may introduce.</p><p>More recent works in annotation fusion include <ref type="bibr" target="#b20">[21]</ref> in which the authors propose a variant of the model in Figure <ref type="figure" target="#fig_0">1</ref> with various annotator functions to capture four specific types of annotator behavior. <ref type="bibr" target="#b21">[22]</ref> describes a mechanism named approval voting that allows annotators to provide multiple answers instead of one for instances where they are not confident. <ref type="bibr" target="#b22">[23]</ref> uses repeated sampling for opinions from annotators over the same data instances to increase reliability in annotations.</p><p>Most of the models described above focus on combining annotations on each dimension separately. However, the annotation dimensions are often related. For example, many studies in emotion literature have reported interrelationships between discrete emotion categories <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The circumplex model <ref type="bibr" target="#b25">[26]</ref>, which attempts to capture these relationships by modeling the emotions as points on a two dimensional space, has also been noted to exhibit vshaped patterns in the joint distribution of valence and arousal <ref type="bibr" target="#b26">[27]</ref>. In addition, in most practical applications, the annotation tasks themselves are multi-dimensional. For example, while collecting ratings on affective dimensions it is routine to collect annotations on valence, arousal and dominance together. Further, there may be dependencies between the internal definitions the annotators hold for the annotation dimensions; for example, while annotating emotional dimensions, a particular annotator may associate certain valence values with only a certain range of arousal. Hence it may be beneficial to model the different dimensions jointly while performing annotation fusion. However, research in this direction has been limited. <ref type="bibr" target="#b27">[28]</ref> proposed a model which assumes joint Gaussian noise between the annotation dimensions, but their model fails to capture structural dependencies described above between the annotation and ground truth dimensions. The model proposed in <ref type="bibr" target="#b16">[17]</ref> can indeed be generalized to combine the different annotation dimensions together but they do not evaluate with joint annotated dimensions from a real dataset as that is not the focus of their work. <ref type="bibr" target="#b28">[29]</ref> jointly model continuous annotations on valence and arousal using personalized basis spline functions, on which functional PCA is applied to identify the dominant spline functions. Using this model, they estimate the ground truth for each data instance using a heuristic algorithm, but their model does not include a jointly trained ground truth predictor. It is therefore of relevance to model multi-dimensional annotation fusion as part of the unified annotator function and predictor modeling paradigm.</p><p>In this work, we propose a joint multi-dimensional model to address many of the gaps mentioned above. Our model captures annotator specific linear relationships between different annotation dimensions, and is an extension of the Factor Analysis model <ref type="bibr" target="#b29">[30]</ref>. It incorporates an annotator specific transformation matrix parameter F k , which explicitly captures the relationship between the annotation dimensions and enables clear interpretations of the estimated relationships; the matrix F k is jointly estimated with a predictor for the ground truth signal. We further provide generalizations of our model to both global and time series annotation settings. We begin with a motivation followed by a detailed description of the model and its parameter estimation in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>To examine the relationships between the annotation dimensions, we created a plot of absolute values of Pearson's correlation between annotation dimensions from four commonly studied emotional corpora in Figure <ref type="figure">3</ref>: IEMOCAP <ref type="bibr" target="#b30">[31]</ref>, SEMAINE <ref type="bibr" target="#b31">[32]</ref>, RECOLA <ref type="bibr" target="#b32">[33]</ref> and the movie emotion corpus from <ref type="bibr" target="#b26">[27]</ref>. Each of these corpora include annotations over affective dimensions such as valence, arousal, dominance and power. For the IEMOCAP corpus, we used global annotations while the others include time series annotations of the affective dimensions from videos. In each case, the correlations were computed from concatenated annotation values between all the dimensions.</p><p>As is evident, in almost all cases, the annotation dimensions exhibit non-zero correlations. We attribute the inconsistent correlations between the dimensions across corpora to varying underlying affective narratives as well as differences in perceptions and biases introduced by individual annotators themselves (see Section A.1). The non-zero correlations highlight the benefit of modeling the annotation dimensions jointly. The model we propose is aimed at addressing this. We explain the model in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JOINT MULTI-DIMENSIONAL ANNOTATION MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The proposed model is shown in Figure <ref type="figure">4</ref>. Each data instance m has a feature vector x m and an associated multidimensional ground truth a m * , which is defined as follows,</p><formula xml:id="formula_5">a m * = f (x m ; Θ) + m<label>(6)</label></formula><p>We assume that from a pool of K annotators, a subset operates on each data instance and provides their annotation</p><formula xml:id="formula_6">a m k . a m k = g(a m * ; F k ) + η k<label>(7)</label></formula><p>where index k corresponds to the k th annotator; F k is an annotator specific matrix that defines his/her linear weights for each output dimension; m and η k are noise terms defined individually in the next sections along with the functions f and g. In the global annotation setting, both a m * and a m k ∈ IR D where D is the number of items being annotated; for the time series setting a m * and a m k ∈ IR T×D , where T is the total duration of the data instance (audio/video signal). In all subsequent definitions, we use uppercase letters M, K, T, D to denote various counts and lowercase letters m, k, t, d to denote the corresponding index variables.</p><p>We make the following assumptions in our model.</p><p>A1 Annotations are independent for different data instances. A2 The annotations for a given data instance are independent of each other given the ground truth. A3 The model ground truths for different annotation dimensions are assumed to be conditionally independent of each other given the features x m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Global annotation model</head><p>In this setting, the ground truth and annotations are d dimensional vectors for each data instance. We define the ground truth a m * and annotations a m k as follows.</p><formula xml:id="formula_7">a m * = Θ T x m + m (8) a m k = F k a m * + η k (9)</formula><p>where,</p><formula xml:id="formula_8">x m ∈ IR P ; Θ ∈ IR P×D ; m ∼ N (0, σ 2 I); σ 2 ∈ IR. The annotator noise η k is defined as η k ∼ N (0, τ 2 k I); τ 2 k ∈ IR. F k ∈ IR D×D</formula><p>is the annotator specific weight matrix. Each annotation dimension value a m,d k for annotator k is defined as a weighted average of the ground truth vector a m * with weights given by the vector F k (d, :).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Parameter Estimation</head><p>The model parameters Φ = {F k , Θ, σ 2 , τ 2 k } are estimated using Maximum Likelihood Estimation (MLE) in which they are chosen to be the values that maximize the likelihood function L.</p><formula xml:id="formula_9">log L = M m=1 log p(a m 1 . . . a m K ; Φ) = M m=1 log a m * p(a m 1 . . . a m K |a m * ; F k , τ 2 k )p(a m * ; Θ, σ 2 ) da m *<label>(10)</label></formula><p>Optimizing Equation <ref type="formula" target="#formula_9">10</ref>directly is intractable because of the presence of the integral within the log term, hence we use the EM algorithm. Note that the model we propose assumes that only some random subset of all available annotators provide annotations on a given data instance, as shown in Figure <ref type="figure">4</ref>. However, for ease of exposition, we overload the variable K and use it here to indicate the number of annotators that attempt to judge the given data instance m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">EM algorithm</head><p>The Expectation Maximization (EM) algorithm to estimate the model parameters is shown below. It is an iterative algorithm in which the E and M-steps are executed repeatedly until an exit condition is encountered. Complete derivation of the model can be found in Appendix B. Initialization We initialize by assigning the expected values and covariance matrices for the m ground truth vectors a m * to their sample estimates (i.e. sample mean and sample covariance) from the corresponding annotations. We then estimate the parameters as described in the maximization step using these estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-step</head><p>In this step we take expectation of the log likelihood function with respect to p(a m * |a m 1 . . . a m K ) and the resulting objective is maximized with respect to the model parameters in the M-step. Equations to compute the expected value and covariance matrices for the latent variable a m * in the E-step are listed below. </p><formula xml:id="formula_10">E a m * |a m 1 ...a m K [a m * ] = Θ T x m + Σ a m</formula><formula xml:id="formula_11">. Θ = (X T X) -1 (X T E[a m * ]) F k = M k m=1 a m K E[(a m * ) T ] M k m=1 E[a m * (a m * ) T ] -1 σ 2 = 1 md M m=1 E[(a m * ) T a m * ] -2tr Θ T x m E[(a m * ) T ] +tr(x T m Θ Θ T x m ) τ 2 k = 1 m k d M k m=1 (a m K ) T a m K -2tr F T k a m K E[(a m * ) T ] +tr F T k F k E[a m * (a m * ) T ]</formula><p>Note the similarity of the update equation for Θ with the familiar normal equations. We are using the soft estimate of a m * to find the expression for Θ in each iteration. Here, X is the feature matrix for all data instances; it includes individual feature vectors x m in its rows. Θ and F k are parameters from the previous iteration.</p><p>Termination We run the algorithm until convergence, and stop model training when the change in log-likelihood falls below a threshold of 0.001%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time series annotation model</head><p>In this setting, the ground truth and the annotations are matrices with T rows (time) and D columns (annotation dimensions). The ground truth matrix a m * is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vec(a</head><formula xml:id="formula_12">m * ) = vec(X m Θ) + m<label>(11)</label></formula><p>where a m * ∈ IR T×D , X m ∈ IR T×P and Θ ∈ IR P×D ; T represents the time dimension and is the length of the time series. X m is the feature matrix where each row corresponds to features extracted from the data instance for one particular time stamp. vec(.) is the vectorization operation which flattens the input matrix in column first order to a vector. m ∼ N (0, σ 2 I) ∈ IR TD is the additive noise vector with σ ∈ IR.</p><p>In <ref type="bibr" target="#b19">[20]</ref>, the authors propose a linear model where the annotation function g(a m * ; F k ) is a causal linear time invariant (LTI) filter of fixed width. The advantage of using an LTI filter is that it can capture scaling and time-delay biases introduced by the annotators.</p><p>The filter width W is chosen such that W T , where T is the number of time stamps for which we have the annotations. The annotation function for dimension d can be viewed as the left multiplication of a filter matrix B d k ∈ IR T×T as shown in Equation <ref type="formula" target="#formula_13">12</ref>.</p><formula xml:id="formula_13">B d k =              b d 1 0 0 0 0 . . . 0 b d 2 b d 1 0 0 0 . . . 0 b d 3 b d 2 b d 1 0 0 . . . 0 . . . . . . . . . . . . . . . 0 b d W . . . b d 1 0 . . . 0 . . . . . . . . . . . . . . . 0 0 0 0 b d W . . . b d 1             <label>(12)</label></formula><p>We extend this model in our work to combine information from all of the annotation dimensions. Specifically, the ground truth is left multiplied by D horizontally concatenated filter matrices, each ∈ IR T×T corresponding to a different dimension as shown below.</p><formula xml:id="formula_14">a m,d k = F d k vec(a m * ) + η k<label>(13)</label></formula><p>where,</p><formula xml:id="formula_15">F d k = [B d,1 k , B d,2 k , . . . , B d,D k ]<label>(14)</label></formula><formula xml:id="formula_16">F d k ∈ IR T×TD with W D unique parameters. η k ∼ N (0, τ 2 k I) ∈ IR T with τ 2 k ∈ IR.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Parameter Estimation</head><p>Estimating the model parameters similar to the global model requires computing the expectations over a vector of size T D. Since T is the number of time stamps in the task and can be arbitrarily long, this may not be feasible in all tasks. For example, in the movie emotions corpus <ref type="bibr" target="#b26">[27]</ref>, annotations are computed at a rate of 25 frames per second with each file of duration ∼30 minutes or of ∼45k annotation frames. To avoid this we use a variant of EM named Hard EM in which instead of taking expectations over the entire conditional distribution of a m * we find its mode. This variant has been shown to be comparable in performance to the classic EM (Soft EM) despite being significantly faster and simple <ref type="bibr" target="#b33">[34]</ref>. This approach is similar to the parameter estimation strategy devised by <ref type="bibr" target="#b19">[20]</ref> in their time series annotation model.</p><p>The likelihood function is similar to the global model in Equation 10 as shown below.</p><formula xml:id="formula_17">log L = M m=1 log a m * p(a m 1 . . . a m K |a m * ; F k , τ 2 k )p(a m * ; Θ, σ 2 ) da m *</formula><p>However the integral here is with respect to the flattened vector vec(a m * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">EM algorithm</head><p>The EM algorithm for the time series annotation model is listed below. Complete derivations can be found in Appendix C. Initialization Unlike the global annotation model, we initialize a m * randomly since we observed better performance when compared to initializing it with the annotation means. Given this a m * , the model parameters are estimated as described in the maximization step below.</p><p>E-step In this step we assign a m * to the mode of the conditional distribution q(a m * ) = p(a m * |a m 1 , . . . , a m K ). Since this distribution is normal (see appendix B) finding the mode is equivalent to minimizing the following expression.</p><formula xml:id="formula_18">a m * = argmin a m * k d ||a m,d k -F d k vec(a m * )|| 2 2 + ||vec(a m * ) -vec(X m Θ)|| 2 2</formula><p>M-step Given the estimate for a m * from the E-step, we substitute it in the likelihood function and maximize with respect to the parameters in the M-step. The estimates for the different parameters are shown below.</p><formula xml:id="formula_19">Θ = M m=1 X T m X m -1 M m=1 X T m a m * f d k = M k m=1 A T A -1 M k m=1 A T a m,d k σ 2 = 1 M T D M m=1 ||vec(a m K ) -vec(X m Θ)|| 2 2 τ 2 k = 1 M k T D M k m=1 d ||a m,d k -F d k vec(a m * )|| 2 2</formula><p>M k is the number of files annotated by user k; A is a matrix obtained by reshaping vec(a m * ) as described in subsection C.1.2.</p><p>Termination We run the algorithm until convergence, and stop model training when the change in log-likelihood falls below a threshold of 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS &amp; RESULTS</head><p>We evaluate the models described above on three different types of data: synthetic data, an artificial task with human annotations, and finally with real data. We describe these below. We compare our joint models with their independent counterparts as baselines, in which each annotation dimension is modeled separately. This allows us to highlight the benefits of moving to a multi-dimensional annotation fusion scheme with everything else kept constant. Update equations for the independent model can be obtained by running the models described above for each dimension separately with D = 1. Note that the independent model is similar in the global setting to the regression model proposed in <ref type="bibr" target="#b7">[8]</ref> (with ground truth scaled by the singleton f d k ). In the time series setting it is identical to the model proposed by <ref type="bibr" target="#b19">[20]</ref>.</p><p>The models are evaluated by comparing the estimated a m * with the actual ground truth. We report model performance using two metrics: the Concordance correlation coefficient (ρ c ) <ref type="bibr" target="#b34">[35]</ref> and the Pearson's correlation coefficient (ρ). ρ c measures any departures from the concordance line (line passing through the origin at 45 • angle). Hence it is sensitive to rotations or rescaling in the predicted ground truth. Given two samples x and y, the sample concordance coefficient ρc is defined as shown below. We also report results in Pearson's correlation to highlight the accuracy of the models in the presence of rotations.</p><p>As noted before, the models proposed in this paper are closely related to the Factor Analysis model, which is vulnerable to issues of unidentifiability <ref type="bibr" target="#b35">[36]</ref>, due to the matrix factorization. Different types of unidentifiability have been studied in literature, such as factor rotation, scaling and label switching. In our experiments, we handle label switching through manual judgment (by reassigning the estimated ground truth between dimensions if necessary) as is common in psychology <ref type="bibr" target="#b36">[37]</ref>, but defer the task of choosing an appropriate prior on the rotation matrix F k to address other unidentifiabilities for future work.</p><p>We report aggregate test set results using C-fold cross validation. To address overfitting, within each fold, we evaluate the parameters obtained after each iteration of the EM algorithm by estimating the ground truth on a disjoint validation set, and pick those with the highest performance in concordance correlation ρ c as the parameter estimates of the model. We then estimate the performance of this parameter set in predicting the ground truth from a separate held out test set for that fold. Finally, we also report statistically significant differences between the joint and independent models at 5% false-positive rate (α = 0.05) in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global annotation model</head><p>The global annotation model uses the EM algorithm described in Section 4.2.2 to estimate the ground truth for discrete annotations. We evaluate the model in three different settings described below. Statistical significance tests were run by computing bootstrap confidence intervals <ref type="bibr" target="#b37">[38]</ref> on the differences in model performances across the C-folds. To establish the statistical significance, we ran the joint and independent models to obtain C test set model predictions from C folds. Given these, we ran 1000 bootstrap iterations in which the test set predictions were sampled with replacement, from which ρ and ρ c were estimated for each dimension. We conclude significance if the evaluation metric being examined was higher in at least 95% of the bootstrap runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Synthetic data</head><p>We created synthetic data according to the model described in Section 4.2 with random features X ∈ IR 500 for 100 ). 10 artificial annotators, each with unique random F k matrices were used to produce annotations for all the data instances. Elements of the feature matrices were sampled from the standard normal distribution, while the elements of F k matrices were sampled from U(0, 1). Elements of ground truth a m * were sampled from U(-1, 1) and θ was estimated from a m * and X. Since its off diagonal elements are non-zero, our choice of F k represents tasks in which the annotation dimensions are related to each other.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the performance of joint and independent models in predicting the ground truth a m * . For both dimensions, the proposed joint model predicts the a m * with considerably higher accuracy as shown by the higher correlations, highlighting the advantages of modeling the annotation dimensions jointly when they are expected to be related to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Artificial data</head><p>Since crowdsourcing experiments typically involve collecting subjective annotations, they seldom have well defined ground truth. As a result, most annotation models are evaluated on expert annotations collected by specially trained users. For example, while collecting annotations on medical data, labels estimated by fusing annotations from naive users may be evaluated against those provided by experts such as doctors. However, this poses a circular problem since the expert annotations themselves may be subjective and combining them to estimate the ground truth is not straightforward. To address this, we created an artificial task with controlled ground truth on which we collect annotations from multiple annotators and evaluate the fused annotation values with the known ground truth values, similar to <ref type="bibr" target="#b38">[39]</ref>. In our task, the annotators were asked to provide their best estimates on perceived saturation and brightness values for monochromatic images. The relationship between perceived saturation and brightness is well known as the Helmholtz-Kohlrausch effect <ref type="bibr" target="#b39">[40]</ref>, according to which, increasing the saturation of an image leads to an increase in the perceived brightness, even if the actual brightness was constant.</p><p>In our experiments, we collected annotations on images from two regimes: one with fixed saturation and varying brightness, and vice versa. This approach was chosen since it would allow us to evaluate the impact of change in either brightness or saturation while the other was held constant. The color of the images were chosen randomly (and independent of the image's saturation and brightness) between green and blue. Annotations were collected on Mturk and the annotators were asked to familiarize themselves with saturation and brightness using an online interactive tool before providing their ratings. In both experiments, a reference image with fixed brightness and saturation was inserted after every ten annotation images to prevent any bias in the annotators. The reference images were hidden from the annotators and appeared as regular annotation images. For parameter estimation, RGB values were chosen as the features for each image.</p><p>We used the joint model to estimate the ground truth for the two regimes separately since we expect the relationship between saturation and brightness to be dissimilar in the two cases. From each experiment, predicted values of the underlying dimension being varied was compared with the actual a m * values. For example, in the experiment with varying saturation and fixed brightness, the joint model was run on full annotations, but only the estimated values of saturation were compared with ground truth saturation. For the independent model, we use annotation values of the underlying dimension being varied from each regime, and compare the estimated values with ground truth.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the performance of the joint and independent models for this experiment. The joint model leads to better estimates of saturation when compared to the independent model by making use of the annotations on brightness. This agrees with the Helmholtz-Kohlrausch phenomenon described above, since the annotators can perceive the changing saturation as a change in brightness, leading to correlated annotations for the two dimensions. On the other hand, the independent model leads to better estimates of brightness, which seems to have no effect on perceived saturation annotations. This experiment highlights the benefits of jointly modeling annotations in cases where the annotation dimensions may be correlated or dependent on one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Real data</head><p>Our final experiment for the global model was on the task of annotating news headlines in which the annotators provide numeric ratings for various emotions. This dataset was first described in the 2007 SemEval task on affective text <ref type="bibr" target="#b40">[41]</ref>. Numeric ratings from the original task were labeled by trained annotators and we treat these as expert annotations. We use Mturk annotations from <ref type="bibr" target="#b4">[5]</ref> as the actual input to our model. Sentence level annotations are provided on seven emotions (D=7): anger, disgust, fear, joy, sadness, surprise and valence (positive/negative polarity). We use sentence level embeddings computed using the pre-trained sentence embedding model sent2vec 3 <ref type="bibr" target="#b41">[42]</ref> as feature vectors x for the model.</p><p>Figure <ref type="figure">7</ref> shows the performance of the joint and independent models on this task. The joint model shows better performance in predicting the reference emotion labels for anger, disgust, fear, joy and sadness, but performs worse than the independent model in predicting surprise and valence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Time series annotation model</head><p>In this setting, the annotations are collected on data with a temporal dimension, such as time series data, video or audio signals. Similar to the global model, we evaluate this model in 3 settings: synthetic, artificial and on real data. The evaluation metrics ρ c and ρ are computed over estimated and actual ground truth vectors a m * by concatenating the data instances into a single vector. The time series models have the window size W as an additional hyperparameter, which is selected using a validation set. In each fold of the dataset, we train model parameters for different window sizes from the set {5, 10, 20, 50}, and pick W and related parameters with the highest concordance correlation ρ c on the validation set. These are then evaluated on a disjoint test set, and we repeat the process for each fold. In each experiment, the parameters were initialized randomly, and the process was repeated 20 times at different random initializations, selecting the best starting point using the validation set. To identify significant differences, we compute the test set performance of the two models for each fold, and run the paired t-test between the C sized samples of ρ and ρ c corresponding to the joint and independent models. We do not bootstrap confidence intervals due to smaller test set sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Synthetic data</head><p>The synthetic dataset was created using the model described in Section 4.3. Elements of the feature matrix were sampled from the standard normal distribution while elements of F k and ground truth were sampled from U(0, 1). In this setting each data instance includes T feature vectors, one for each time stamp. The time dependent feature matrices were created using a random walk model without drift but with lag to mimic a real world task. In other words, while creating the P dimensional time series, the features vectors were held fixed for a time period arbitrarily chosen to be between 2 to 4 time stamps. This was done because in most tasks the underlying dimension (such as emotion) is expected to remain constant at least for a few seconds.</p><p>In addition, the transition between changes in the feature vectors were linear and not abrupt. In our experiments, we chose P = 500, T = 350, D = 2, M = 18 and the number of annotators K = 6.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows the aggregate results across C-folds (C = 5) for the joint and independent models in the 3 settings. In the synthetic dataset, the joint model achieves higher values for Pearson's correlation ρ for both the dimensions and higher value for ρ c for dimension 1. For dimension 2 however, the independent model achieves better ρ c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Artificial data</head><p>We collected annotations on videos with the artificial task of identifying saturation and brightness, described in the previous section. The videos consisted of monochromatic images with the underlying saturation and brightness varied independent of each other. The dimensions were created using a random walk model with lag as described in Section 5.2.1. The annotations were collected in house using an annotation system developed using the Robot Operating System <ref type="bibr" target="#b42">[43]</ref>. 10 graduate students gave their ratings on the two dimensions. Each dimension was annotated independently using a mouse controlled slider. For parameter estimation, the feature vectors for each time stamp were RGB values.</p><p>As seen in Figure <ref type="figure" target="#fig_6">8</ref>, both models achieve similar performance in predicting the ground truth for saturation and brightness in terms of ρ, as well as in predicting saturation in terms of ρ c . The independent model achieves slightly better performance in predicting brightness in terms of concordance correlation (though not statistically significant); however, their performance in terms of ρ suggests that the joint model output differs only in terms of a linear scaling. The joint model appears to be at par with the independent model for the most part, suggesting that the transformation matrix F k connecting the two dimensions for each annotator, is unable to accurately capture the dependencies between the dimensions, likely due to the fact that, unlike the global annotation model, the underlying brightness and saturation were varied simultaneously and independent of each other (leading to non-linear dependencies between them), and that we limit F k to only capture linear relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Real data</head><p>We finally evaluate our model on a real world task with time series annotations. We chose the task of predicting the affective dimensions of valence and arousal from movie clips, first described in <ref type="bibr" target="#b26">[27]</ref>. The associated corpus includes time series annotations of valence and arousal on contiguous 30 minute video segments from 12 Academy Award winning movies. This task was chosen because the data set includes both expert annotations as well as annotations from naive users. We treat the expert annotations as reference and evaluate the estimated dimensions against them; however, we note that the expert labels were provided by just one annotator, which may itself be noisy.</p><p>For each movie clip, 6 annotators provide annotations on their perceived valence and arousal using the Feeltrace <ref type="bibr" target="#b11">[12]</ref> annotation tool. The features used in our parameter estimation include combined audio and video features extracted separately. The audio features were estimated using the emotion recognition baseline features from Opensmile <ref type="bibr" target="#b43">[44]</ref> at 25 fps (same frame rate as the video clips) and aggregated at a window size of 5 seconds using the following statistical functionals: mean, max, min, std, range, kurtosis, and inter-quartile range. The video features were extracted using OpenCV <ref type="bibr" target="#b44">[45]</ref> and included frame level luminance, intensity, Hue-Saturation-Value (HSV) color histograms and optical flow <ref type="bibr" target="#b45">[46]</ref>, which were also aggregated to 5 seconds using simple averaging. The combined features were of size P = 1225 for each frame.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows the performance of the two models in estimating the affective dimensions for the dataset. The joint model seems to considerably outperform the independent model while estimating arousal while the independent models seem to produce better estimates of valence from the annotations. The independent model seems to perform poorly in arousal prediction, but the joint model shows a balanced performance, with the joint modeling constraint likely acting as a regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of dependency among dimensions</head><p>To evaluate the impact of the magnitude of dependency between the annotation dimensions on the performance of the models, we created a set of synthetic annotations for the global model similar to Section 5.1.1. We created 10 synthetic datasets, each with constant F k matrices across all annotators. The principal diagonal elements were fixed to 1 while the off diagonal elements were increased between 0.1 to 1 with a step size of 0.1. Similar to the previous setting, we created 100 annotators, each operating on 10 files. Note that despite the annotators having identical F k matrices, their annotations on a given file were different because of the noise term η k in Equation <ref type="formula" target="#formula_6">7</ref>.</p><p>Figure <ref type="figure">9</ref> shows the 5-fold cross validated performance of the joint and independent models on this task. As seen in the figure, the joint model consistently outperforms the independent model in both metrics. Both the models start with similar performance when the off diagonal elements are close to zero since this implies no dependency between 0.1 0.4 0. Fig. <ref type="figure">9</ref>: Effect of varying dependency between annotation dimensions for the synthetic model the annotation dimensions, and the performance of both models continues to degrade as the off diagonal elements increase. However, the joint model is able to make better predictions of the ground truth by making use of the dependency between the dimensions, highlighting the benefits of modeling the annotation dimensions jointly. Visualizations for averaged estimates of the F k matrices from this experiment can be found in Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented a model to combine multi-dimensional annotations from crowdsourcing platforms such as Mturk. The model assumes the ground truth to be latent and distorted by the annotators. The latent ground truth and the model parameters are estimated using the EM algorithm. EM updates are derived for both global and time series annotation settings. We evaluate the model on synthetic and real data. We also propose an artificial task with controlled ground truth and evaluate the model. Weaknesses of the model include vulnerability to unidentifiability issues like most variants of factor analysis <ref type="bibr" target="#b35">[36]</ref>. Typical strategies to address this issue involve adapting a suitable prior constraint on the factor matrix. For example, in PCA, the factors are ordered such that they are orthogonal to each other and arranged in decreasing order of variance.</p><p>In our experiments, the model was found to be vulnerable to unidenfiability due to label switching, which was addressed through manual judgements. We defer the task of choosing an appropriate prior constraint on F k for future work.</p><p>Future work includes generalizing the model with Bayesian extensions, in which case the parameters can be estimated using variational inference, in addition to adding model constraints to ensure identifiability of all model parameters. Though we limit our analysis here to linear relationships between the transformation matrix F k and the ground truth vector a m * , we note that extending the model to capture non-linear relationships is straightforward. For example, the vector a m * in Equation <ref type="formula" target="#formula_6">7</ref>can be replaced by one that includes a non-linear dependence on a m * . Providing theoretical bounds to the model performance, specially with respect to the sample complexity may also be possible since we have assumed normal distributions throughout the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SUPPLEMENTARY ANALYSES A.1 Annotator specific correlations</head><p>Figure <ref type="figure" target="#fig_0">10</ref> highlights the correlations between annotation dimensions for 4 annotators from the movie emotions <ref type="bibr" target="#b26">[27]</ref> and RECOLA <ref type="bibr" target="#b32">[33]</ref> corpora. As noted earlier, different annotators may exhibit different degree of associations between the annotation dimensions, leading to the observed differences in correlations both within and between the two corpora. This difference in annotator behavior also leads to the different inter-dimension correlations observed among the corpora in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Effect of dependency among dimensions</head><p>The model we present includes the annotator specific parameter F k which measures the relationships between the annotation dimensions. To highlight the ability of the model to recover this parameter, in Figure <ref type="figure" target="#fig_0">11</ref>, we show a plot of averages of all predicted F k matrices for different step sizes from the synthetic experiment described in Section 5.3. In each case, the predicted F k matrices closely resemble the actual matrices for the annotators highlighting the accuracy of the joint model. However, as we get closer to step size 1, the estimated F k matrices appear to be washed out (despite being accurate to a scaling term), with all terms of the estimated F k close to 0.5 instead of 1 (Figure <ref type="figure" target="#fig_0">11f</ref>), due to model unidentifiability. To help with the model formulation, we first derive parameters of the joint distribution p(a m 1 . . . a m K , a m * ). Since the product of two normal distributions is also normal <ref type="bibr" target="#b46">[47]</ref>, this joint distribution is also normal and is given by,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B EM DERIVATION FOR GLOBAL</head><formula xml:id="formula_20">     a m * a m 1 . . . a m K      ∼ N           Θ T x m F 1 Θ T x m . . . F K Θ T x m      ,      Σ * * Σ * 1 . . . Σ * K Σ 1 * Σ 11 . . . Σ 1K . . . . . . . . . . . . Σ K * Σ K1 . . . Σ KK          <label>(15)</label></formula><p>The different components of the covariance matrix from Equation 15 are derived below.</p><formula xml:id="formula_21">Σ * * = Cov(a m * ) = σ 2 * I Σ k * = E[a m k (a m * ) T ] -E[a m k ] E[(a m * ) T ] = E[(F k a m * + η k )(a m * ) T ] -E[F k a m * + η k ] E[(a m * ) T ] = F k (σ 2 * I) Σ kk = Cov(F k a m * + η k ) = Cov(F k a m * ) + τ 2 k I = F k Σ * * F T k + τ 2 k I = σ 2 * F k F T k + τ 2 k I Σ kikj = E a m * [Cov(a m k1 , a m k2 |a m * )] + Cov(E[a m k1 |a m * ], E[a m k2 |a m * ]) = Cov(E[a m k1 |a m * ], E[a m k2 |a m * ]) = Cov(F k1 a m * , F k2 a m * ) = F k1 Σ * * (F k2 ) T = σ 2 * F k1 F T k2</formula><p>In the derivation of Σ kikj , the first equation is a direct application of the law of total covariance and the second equation is because of the conditional independence assumption of annotation values a m ki given the ground truth a m * Finally, owing to the jointly normal distributions, </p><formula xml:id="formula_22">p(a m * |a m 1 . . . a m K ) is also normal: p(a m * |a m 1 . . . a m K ) ∼ N (µ a m * |a m 1 ...a m K |Σ a m * |a m 1 ...a m K ) Also,</formula><formula xml:id="formula_23">µ x1|x2 = µ 1 + Σ 12 Σ -1 22 (x 2 -µ 2 )<label>(16)</label></formula><formula xml:id="formula_24">Σ x1|x2 = Σ 11 -Σ 12 Σ -1 22 Σ 21<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 EM Formulation</head><p>We begin by introducing a new distribution q(a m * ) in Equation 10. We drop the parameters Φ from the likelihood function expansion for convenience.</p><formula xml:id="formula_25">log L = M m=1 log a m * q(a m * ) p(a m 1 . . . a m K |a m * )p(a m * ) q(a m * ) da m *<label>(18)</label></formula><p>Using Jensen's inequality over log of expectation, we can write the above as follows,</p><formula xml:id="formula_26">log L ≥ M m=1 a m * q(a m * ) log p(a m 1 . . . a m K |a m * )p(a m * ) q(a m * ) da m *<label>(19)</label></formula><p>The bound above becomes tight when the expectation is taken over a constant value, i.e. The E-step involves simply assuming q(a m * ) to follow the conditional distribution p(a m * |a m 1 . . . a m K ). To help with future computations, we also compute the following expectations, where the first two are a result of equations 16 and 17; third equation is by definition of covariance and the last one is a standard result (see the matrix cookbook eq. 327).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(a</head><formula xml:id="formula_27">m 1 . . . a m K |a m * )p(a m * ) q(a m * ) = c</formula><formula xml:id="formula_28">E a m * |a m 1 ...a m K [a m * ] = Θ T x m + Σ a m * ,a m 1 ...a m K (Σ a m 1 ...a m K ,a m 1 ...a m K ) -1 (a m -µ m ) Σ a m * |a m 1 ...a m K [a m * ] = Σ a m * ,a m * -Σ a m * ,a m 1 ...a m K (Σ a m 1 ...a m K ,a m 1 ...a m K ) -1 Σ a m 1 ...a m K ,a m * E a m * |a m 1 ...a m</formula><p>concatenating the K annotation vectors a m 1 , . . . a m K and their corresponding expected values F 1 Θ T x m . . . F K Θ T x m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 M-step</head><p>In the M-step, we find the parameters of the model by maximizing Equation <ref type="formula" target="#formula_26">19</ref>. We first write this equation as an expectation and an equality. The expectation below is with respect to q(a m * ) = p(a m * |a m 1 . . . a m K ); we drop the subscript for ease of exposition</p><formula xml:id="formula_29">log L = M m=1 E a m * |a m 1 ...a m K log p(a m 1 . . . a m K |a m * )p(a m * ) q(a m * ) log L = M m=1 E log p(a m 1 . . . a m K |a m * ) + E log p(a m * ) + H log L = M m=1 K k=1 E log p(a m k |a m * ) + E log p(a m * ) + H (20)</formula><p>where p(a m * ) and p(a m k |a m * ) are given by equations 8 and 9 respectively. The last equation above uses that fact that we assume independence among annotators given the ground truth. Also expectation commutes with the linear sum over the K terms.</p><p>Here, H is the entropy of p(a m * |a m 1 . . . a m K ). We maximize Equation 20 with respect to each of the parameters to obtain the M-step updates.</p><p>Estimating F k Differentiating Equation <ref type="bibr" target="#b19">(20)</ref> with respect to F k and equating the derivative to 0</p><formula xml:id="formula_30">∆ F k Q = 0 ∆ F k M k m=1 E[(a m k -F k a m * ) T (τ 2 k I) -1 (a m k -F k a m * )] = 0 ∆ F k 1 τ 2 k M k m=1 E[(a m k -F k a m * ) T (a m k -F k a m * )] = 0 M k m=1 -2a m k E[(a m * ) T ] + 2F k E[a m * (a m * ) T ] = 0 ∴ F k = M k m=1 a m k E[(a m * ) T ] M k m=1 E[a m * (a m * ) T ] -1</formula><p>where, M k is the number of points annotated by user k.</p><p>We used the following facts in the above derivation: trace(x) = x for scalar x; trace(AB) = trace(BA); ∆ A trace(A T x) = x and ∆ A trace(A T AB) = AB + AB T for matrix A. We also make use of the fact that expectation and trace of a matrix are commutative since trace is a linear sum.</p><p>Estimating Θ Similarly, to find Θ, we differentiate Equa-tion <ref type="bibr" target="#b19">(20)</ref> with respect to Θ and equate it to 0.</p><formula xml:id="formula_31">∆ Θ Q = 0 ∆ Θ M m=1 E[(a m ∆ τ k Q = 0 ∆ τ k M k m=1 -D log τ k - 1 2τ 2 k (a m k ) T a m k -2tr(F T k a m k E[(a m * ) T ])+ tr(F T k F k E[a m * (a m * ) T ]) = 0 M k m=1 - D τ k + 1 τ 3 k (a m k ) T a m k -2tr(F T k a m k E[(a m</formula><p>Since we assume that each annotator is independent of the others given the ground truth, we have For convenience, we reshape a m * into a vector and optimize with respect to the flattened vector. If we choose vec(a m * ) = v and vec(X m Θ) = y, the objective becomes,</p><formula xml:id="formula_32">Q(v) = k d ||a m,d k -F d k v|| 2 2 + ||v -y|| 2 2</formula><p>Differentiating Q with respect to v and equating the gradient to 0, we get In the last step we make use of the fact that a m k depends on a m * through Gaussian noise. We also discard all other dimensions d = d since these do not depend on f d k . To estimate f d k , we can rearrange F d k vec(a m * ) such that f d k is now the parameter vector of a linear regression problem with the independent variables represented by matrix A which is obtained by creating a filtering matrix out of vec(a m * ). Hence, the optimization problem becomes</p><formula xml:id="formula_33">∆ v Q = 0</formula><formula xml:id="formula_34">∆ f d k M k m=1 ||a m,d k -Af d k || 2 2 = 0 ∴ f d k = M k m=1 A T A -1 M k m=1</formula><p>A T a m,d k Estimating τ k Differentiating Equation ( <ref type="formula">22</ref>) with respect to τ k and equating the gradient to 0, we have. Estimating Θ Differentiating Equation <ref type="bibr" target="#b21">(22)</ref> with respect to Θ and equating the gradient to 0, we have.</p><formula xml:id="formula_35">∆ τ k Q = 0 ∆ τ k</formula><formula xml:id="formula_36">∆ Θ Q = 0 ∆ Θ M m=1 ||vec(a m * ) -vec(X m Θ)|| 2 2 = 0</formula><p>By definition, each column of Θ is independent of each other. Hence we can estimate each θ d separately (taking derivatives with respect to above equation would cancel all terms except those in θ d ). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Plate notation for a basic annotation model. a m,d * is the latent ground truth for the given data instance (for the d th question) and a m,d k is the rating provided by the k th annotator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Annotation model proposed by<ref type="bibr" target="#b7">[8]</ref> with a jointly learned predictor. x m is the set of features for the m th data instance; a m,d * is the d th dimension of the latent ground truth which is modeled as a function of x m ; a m,d k is the rating provided by the k th annotator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig. 3: Correlation heatmaps for annotations from a representative sample of emotion annotated datasets; v -valence, aarousal, d -dominance, p -power</figDesc><graphic coords="5,71.38,43.70,82.25,86.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Performance of global annotation model on synthetic dataset; *-statistically significant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Performance of global annotation model on artificial dataset; Sat-Saturation, Bri-Brightness; *-statistically significant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 .Fig. 7 :</head><label>37</label><figDesc>Fig. 7: Performance of global annotation model on the text emotions dataset; *-statistically significant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Concordance and Pearson correlation coefficients between ground truth/reference and model predictions for the time series annotation model; *-statistically significant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>ANNOTATION MODEL B.0.1 Deriving p(a m 1 . . . a m K , a m * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Fig. 10: Annotator specific correlations between annotation dimensions for the Movie emotions and RECOLA corpora; v-valence, a-arousal</figDesc><graphic coords="14,113.54,330.31,70.00,73.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>* are defined using IID Gaussian noise, the above maximization problem is equivalent to the following minimization. m * )vec(X m Θ)|| 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>+ 22 )</head><label>22</label><figDesc>k ) T a m,d k + v T (F d k ) T F d k v-2(a m,d k ) T F d k v + (v T v -2y T v + y T y) = 0 k d 2(F d k ) T F d k v -2(F d k ) T a m,d kWe can extract a m * by reshaping v back into a matrix.C.1.2 M-stepGiven the point estimate for a m * , the log-likelihood Equation (21) can now be written as a function of the model parameters.a m k |a m * ; F d k , τ k ) + log p(a m * ; Θ, σ)In the M-step, we optimize the above equation with respect to the parameters Φ = {F k , τ k , Θ, σ}.Q(F k , τ k , Θ, σ) = M m=1 K k=1 log p(a m k |a m * ; F d k , τ k )+ log p(a m * ; Θ, σ) (Estimating F d k :Since each F d k is a filter matrix constructed from a vector f d k ∈ IR WD , we differentiate 22 with respect to f d k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>by definitions of conditional normal distributions, given a normal vector of the form</figDesc><table><row><cell></cell><cell>x 1 x 2</cell><cell>∼ N</cell><cell>µ 1 µ 2</cell><cell>,</cell><cell>Σ 11 Σ 12 Σ 21 Σ 22</cell></row><row><cell>the</cell><cell cols="2">conditional</cell><cell cols="3">distribution</cell><cell>p(x 1 |x 2 )</cell><cell>∼</cell></row><row><cell cols="6">N (µ x1|x2 , Σ x1|x2 ) has the following form.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>The authors would like to thank <rs type="person">Zisis Skordilis</rs> for all the helpful discussions and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anil Ramakrishna received his B.E. degree from the Visvesvaraya Technological University in 2010, M.S in Computer Science in 2014, M.S in Electrical Engineering in 2019, Ph.D. in Computer Science in 2019, all from the University of Southern California (USC). His dissertation focused on developing computational models for multidimensional annotation fusion, leveraging relationships between annotation dimensions in estimating the ground truth, which is modeled as a latent variable. His research interests include sentiment analysis, natural language processing, machine learning and more recently, spoken language understanding. He is a member of the IEEE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rahul</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C EM DERIVATION FOR TIME SERIES ANNOTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 EM Formulation</head><p>Similar to the process described in Appendix B, the log likelihood function for the time series model is shown below (similar to Equation <ref type="formula">19</ref>).</p><p>The bound becomes tight when q(a m * ) = p(a m * |a m 1 . . . a m K ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 E-step</head><p>Computing the expectation function over the entire distribution of q(a m * ) is computationally expensive since a m * is a matrix. To avoid this, we instead use Hard-EM in which we assume a dirac-delta distribution for a m * which is centered at the mode of q(a m * ). This is a common practice in latent models and is the approach followed by <ref type="bibr" target="#b19">[20]</ref> in estimating the annotator filter parameters. We assign this value to a m</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The wisdom of crowds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Surowiecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Anchor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inferring ground truth from subjective labelling of venus images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A globally-variant locallyconstant model for fusion of labels from multiple diverse experts without using reference labels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="769" to="783" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotation and processing of continuous emotional attributes: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Workshop on Emotion Representation, Analysis and Synthesis in Continuous Time and Space</title>
		<imprint>
			<date type="published" when="2013-04">EmoSPACE 2013. April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anvil-a generic annotation tool for multimodal dialogue</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1367" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FEELtrace: An instrument for recording perceived emotion in real time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savvidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sawey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schr Öder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA tutorial and research workshop (ITRW) on speech and emotion</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EMuJoy: Software for continuous measurement of perceived emotions in music</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kopiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Altenm Üller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="290" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gtrace: General trace program compatible with emotionml</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sawey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaimovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fyans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stapleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</title>
		<meeting>the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="709" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DARMA: Software for dual axis rating and media annotation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="902" to="909" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oudjat: A configurable and usable annotation tool for the study of facial expressions of emotion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dupre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Akpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnefond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tcherkassof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic probabilistic CCA for analysis of affective behavior and fusion of continuous annotations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1299" to="1311" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Correcting time-continuous emotional labels by modeling the reaction lag of evaluators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="108" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling multiple time series annotations as noisy distortions of the ground truth: An expectation-maximization approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jacokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="89" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling annotator behaviors for crowd labeling</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Genc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Aran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="141" to="156" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Approval voting and incentives in crowdsourcing</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05696</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Get another label? improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On traits and temperament: General and specific factors of emotional experience and their relation to the five-factor model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="441" to="476" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the bipolarity of positive and negative affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A supervised approach to movie emotion tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zlatintsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2376" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An expectation maximization approach to joint modeling of multidimensional ratings derived from multiple annotators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1555" to="1559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A functional data analysis approach for continuous 2-d emotion annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Van Den Broek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2019">2019</date>
			<publisher>IOS Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">Modern factor analysis</title>
		<imprint>
			<publisher>University of Chicago press</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language resources and evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introducing the recola multimodal corpus of remote collaborative and affective interactions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sonderegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A concordance correlation coefficient to evaluate reproducibility</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating the use of exploratory factor analysis in psychological research</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Fabrigar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Strahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">272</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The varimax criterion for analytic rotation in factor analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A novel method for human bias correction of continuous-time annotations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mundnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3091" to="3095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The brightness of colour</title>
		<author>
			<persName><forename type="first">D</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 14: Affective text</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, ser. SemEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, ser. SemEval &apos;07</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno>abs/1703.02507</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ros: an open-source robot operating system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gerkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leibs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl. Conf. on Robotics and Automation (ICRA) Workshop on Open Source Robotics</title>
		<meeting>of the IEEE Intl. Conf. on Robotics and Automation (ICRA) Workshop on Open Source Robotics<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Opensmile: The Munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia, ser. MM &apos;10</title>
		<meeting>the 18th ACM International Conference on Multimedia, ser. MM &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal human emotion/expression recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Third IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>Third IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
