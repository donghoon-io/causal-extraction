<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributional Drift Adaptation with Temporal Conditional Variational Autoencoder for Multivariate Time Series Forecasting</title>
				<funder ref="#_dpwdNHm">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_5aa5z8x">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_De9feWF">
					<orgName type="full">Engineering Research Center of Integration and Application of Digital Learning Technology, Ministry of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-04-02">2 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hui</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>zhangqics@tongji.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
							<email>yikun@bit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kaize</forename><surname>Shi</surname></persName>
							<email>kaize.shi@uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
							<email>zniu@bit.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
							<email>long-bing.cao@mq.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Medical Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Data Science and Machine Intelligence Lab</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<postCode>2007</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Research Center of Integration and Application of Digital Learning Technol- ogy</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">DataX Research Centre</orgName>
								<orgName type="department" key="dep2">School of Comput- ing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<postCode>2109</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributional Drift Adaptation with Temporal Conditional Variational Autoencoder for Multivariate Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-02">2 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.00654v4[cs.LG]</idno>
					<note type="submission">received April 19, 2021; revised August 16, 2021.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multivariate time series</term>
					<term>forecasting</term>
					<term>distributional drift</term>
					<term>variational autoencoder (VAE)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the non-stationary nature, the distribution of real-world multivariate time series (MTS) changes over time, which is known as distribution drift. Most existing MTS forecasting models greatly suffer from distribution drift and degrade the forecasting performance over time. Existing methods address distribution drift via adapting to the latest arrived data or selfcorrecting per the meta knowledge derived from future data. Despite their great success in MTS forecasting, these methods hardly capture the intrinsic distribution changes, especially from a distributional perspective. Accordingly, we propose a novel framework temporal conditional variational autoencoder (TCVAE) to model the dynamic distributional dependencies over time between historical observations and future data in MTSs and infer the dependencies as a temporal conditional distribution to leverage latent variables. Specifically, a novel temporal Hawkes attention mechanism represents temporal factors subsequently fed into feed-forward networks to estimate the prior Gaussian distribution of latent variables. The representation of temporal factors further dynamically adjusts the structures of Transformer-based encoder and decoder to distribution changes by leveraging a gated attention mechanism. Moreover, we introduce conditional continuous normalization flow to transform the prior Gaussian to a complex and form-free distribution to facilitate flexible inference of the temporal conditional distribution. Extensive experiments conducted on six real-world MTS datasets demonstrate the TCVAE's superior robustness and effectiveness over the state-of-the-art MTS forecasting baselines. We further illustrate the TCVAE applicability through multifaceted case studies and visualization in real-world scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>M ULTIVARIATE time series (MTS) is a continuous and intermittent unfolding of time-stamped variables over time. MTS forecasting predicts the future values of timestamped variables based on their extremely substantial historical observations. MTS forecasting is widely applicable for various applications, such as traffic management <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, financial trading <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, energy optimization <ref type="bibr" target="#b5">[6]</ref>, user modeling <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and epidemic propagation studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. A recent typical example is to predict the case trends of COVID-19 infections, which can provide hints or evidence for intervention policies and prompt actions to contain the virus spread or resurgence and large-scale irreparable socioeconomic impact <ref type="bibr" target="#b10">[11]</ref>.</p><p>Traditional models such as vector autoregression (VAR) <ref type="bibr" target="#b11">[12]</ref> and probabilistic models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> show appealing interpretability and theoretical guarantees for MTS forecasting. Recent advanced models such as LSTM <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref>, TCN <ref type="bibr" target="#b19">[20]</ref> and Transformer <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b25">[26]</ref> are more competent in capturing temporal dependencies. These models, despite their successful applications in various domains, generally assume that the distribution of time series keeps stationary through time, requiring constant conditional dependencies between future predictions and historical observations. The above assumption oversimplifies the highly non-stationary and non-IID (or non-i.i.d., non-independent and identically distributed) nature of real-world MTSs <ref type="bibr" target="#b26">[27]</ref>, inapplicable to distribution-evolving MTSs and causing forecasting performance downgrade over time. Such a phenomenon forms distribution drift <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, where the statistical properties of MTSs change over time. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the traffic flows recorded by three sensors #3, #17 and #33 from 9/1/2018 to 12/28/2018 are shown in the upper part, where the flows are split into six successive time slices. The flows sharply descend and ascend, forming the turning points between the 2nd and 3rd slices and within the 5th and 6th slices (marked in red ellipse). In addition, we take out the six slices of sensor #3 and show its statistic evolution in terms of different time slices and cumulative slices in the lower left and right parts, respectively. We can see distribution drifts occur between any successive time series slices. For clarity, we only highlight the most intense shift, i.e., a mean right shift from the 4th to the 5th slice, as shown in the left part of Fig. <ref type="figure" target="#fig_0">1</ref>, and a mean left shift from the 1st-2nd time slices to the 1st-6th time slices, as shown in the right part of Fig. <ref type="figure" target="#fig_0">1</ref>. Obviously, the distribution of successive time series slices changes over time, significantly challenging even well-trained forecasting models. Therefore, addressing MTS distribution drift becomes a critical but challenging issue of MTS forecasting.</p><p>Few studies focus on capturing the distribution drift for MTS forecasting to date. Previous methods <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref> address the distribution drift via adapting to the latest arrived data and applying the adapted models for future data prediction. However, such methods which hardly capture the intrinsic distribution drift suffer catastrophic forgetting <ref type="bibr" target="#b35">[36]</ref> and are unsuitable for scenarios with a sudden distributional change. Subsequently, recent adaptation methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref> focus on future data and bridge the gaps between historical observations and future data by generating training samples/features following future data distribution or calculating their gradient difference. These methods learn to correlate the distribution drift with meta knowledge derived from future data and correct the bias caused by their distribution drift in a discriminative meta-learning framework. Despite their great success in MTS forecasting, these methods do not capture significant changes of distributional dependencies.</p><p>Different from previous adaptation-based methods neutralizing the bias caused by the distribution drift, we aim to model the dynamic distributional dependencies over time between historical observations (X ) and future data (Y). Inspired by the superiority of generative models, e.g., variational autoencoder (VAE), in approximating data distribution in various fields <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>, we infer the distributional dependencies as a conditional distribution p(Y|X ) by leveraging latent variables Z in a generative modeling framework. To achieve the dynamics of the conditional distribution over time, we further regularize the latent variables sampled from a prior Gaussian distribution conditional on temporal factors C (statistic features of X such as mean and start values), i.e., p(Z|C). As the distributional dependence p(Y|X ) is approximated by the latent variables, it accordingly evolves to a temporal conditional distribution p(Y|X , C), where the dynamic dependencies between historical observations and future data vary with temporal factors and competently depict the distribution drift.</p><p>In light of the above discussion, we propose a novel framework called temporal conditional variational autoencoder (TCVAE) to infer the distributional drift in non-stationary MTSs. TCVAE aims to address three major challenges: (a) accurately approximating the temporal Gaussian distribution p(Z|C) of the latent variables; (b) dynamically adapting the encoder and decoder structures to distributional changes; (c) flexibly inferring the temporal conditional distribution p(Y|X , C) from the latent variables. Regarding (a), TCVAE introduces a Hawkes process-based attention mechanism to represent temporal factors and feed-forward networks to learn the mean and variance of the Gaussian distribution p(Z|C). Subsequently, the representation of temporal factors is utilized in a gated attention mechanism that adjusts the structures of Transformer-based encoder and decoder in a meta-learning manner to handle (b). Moreover, conditional continuous normalizing flow (CCNF) is exploited to invertibly transform the Gaussian distribution into a complex distribution that is more flexible to infer the temporal conditional distribution to address (c). Our contributions are summarized as follows:</p><p>• A temporal conditional variational autoencoder (TCVAE) adapts to distribution drift in MTSs. To our knowledge, TCVAE is the first attempt to depict distribution drift in a generative framework for MTS forecasting. • A temporal Hawkes attention mechanism represents temporal factors that estimate the temporal Gaussian and a gated attention mechanism dynamically adapts the network structure of the encoder and decoder to distributional changes. • CCNF is used to transform the temporal Gaussian to a flexible and form-free distribution for effectively inferring the temporal conditional distribution. Extensive experimental results on six public MTS datasets show the superior robustness and effectiveness of TCVAE over state-of-the-art baselines. Case studies on traffic and pandemic data further interpret the applicability of TCVAE in real-world environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multivariate Time Series Forecasting</head><p>In time series forecasting, traditional statistical models, such as vector autoregression (VAR) <ref type="bibr" target="#b44">[45]</ref> and Gaussian process (GP) <ref type="bibr" target="#b45">[46]</ref>, are simple but interpretable for time series forecasting. However, they make strong assumptions about stationary processes and fall short in capturing non-stationary to non-IID scenarios <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Recently, various deep learning models have become prevalent for MTS forecasting. RNN-based <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, CNN-based <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, Transformer-based <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref> and MLP-based <ref type="bibr" target="#b50">[51]</ref> models are free from stationary assumptions and are capable of modeling nonlinear longand short-term temporal patterns. These models pay more attention to capturing the intra-series temporal patterns, but generally neglect the inter-series couplings between multiple variables <ref type="bibr" target="#b46">[47]</ref>, which weakens their forecasting capacity. Further, the recent DNN models <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> organize MTSs by a graph with variables as nodes to portray the correlations between variables and graph neural networks (GNNs) to learn the inter-series correlations. GNNs highly depend on a predefined topology structure of inter-series correlations, hence, are hardly applicable to MTSs with evolving dependencies and distributional drift. Subsequently, MTGNN <ref type="bibr" target="#b53">[54]</ref>, AGCRN <ref type="bibr" target="#b0">[1]</ref>, StemGNN <ref type="bibr" target="#b9">[10]</ref> and FourierGNN <ref type="bibr" target="#b54">[55]</ref> extract the uni-directed relations among variables and capture shared patterns by graph learning rather than predefined priors. Similarly, CATN <ref type="bibr" target="#b55">[56]</ref> introduces a tree (i.e., an ordered graph) to structure multivariable time series with a clear hierarchy and proposes a tree-aware network to learn the hierarchical and grouped correlations among multiple variables. Due to the complex structures of MTSs, it is difficult to learn general graphs, especially for high-dimensional data <ref type="bibr" target="#b56">[57]</ref>. More recent deep MTS models capture interactions across multiple MTSs <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref> and high-dimensional dependencies <ref type="bibr" target="#b18">[19]</ref>.</p><p>Due to the non-stationarity of the real-world environment, distributional drift has been an essential issue in time series data. However, few existing methods focus on the distributional drift of time series. Some methods <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b34">[35]</ref> strategically cater to future data via adapting their models with the newly arrived data or historical significant fluctuations. For instance, the studies <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref> use static or learnable statistics to adaptively normalize the newly arrived input for stationarization and then denormalize the output for better predictability. However, they hardly learn the intrinsic distributional drift, limiting generalization ability at the data level. In this light, adaptation methods <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref> are designed to generate new training samples/features following future data distribution or calculating gradient differences between historical observations and future data. For example, DDG-DA <ref type="bibr" target="#b27">[28]</ref> predicts the evolution of data distribution from a meta-learning perspective through training a predictor to estimate future data distribution and generating corresponding samples for training. AdaRNN <ref type="bibr" target="#b37">[38]</ref> characterizes and matches temporal distribution to generate common knowledge shared among training samples, which can be generalized well on unseen future data. LLF <ref type="bibr" target="#b38">[39]</ref> mitigates the impact of distributional drift via computing the difference between the gradients on historical data and future data. More recently, DoubleAdapt <ref type="bibr" target="#b40">[41]</ref> proposes two meta-learners aimed at adapting data to a locally stationary distribution, while also equipping the model with task-specific parameters under mitigated distribution drifts. Different from the above methods neutralizing the bias caused by distributional drift, we aim to model the dynamic distributional dependencies between historical observations and future data over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Variational Autoencoder</head><p>Variational autoencoders (VAEs) as deep generative models are capable of approximating data distribution and have enjoyed great success in various real-world applications, including time series analysis <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref>, dialog generation <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, text-to-speech <ref type="bibr" target="#b63">[64]</ref>, recommendation <ref type="bibr" target="#b64">[65]</ref> and image processing <ref type="bibr" target="#b65">[66]</ref>. In time series analysis, Nguyen et al. <ref type="bibr" target="#b60">[61]</ref> introduced a temporal regularization and noise mechanism into a temporal latent autoencoder network to model the predictive distribution of time series implicitly. To well learn the discrete representations of time series, Fortuin et al. <ref type="bibr" target="#b61">[62]</ref> overcame the non-differentiability of discrete representations and used a gradient-based self-organizing map algorithm. Dasai et al. <ref type="bibr" target="#b62">[63]</ref> relied on user-defined distribution, such as level, trend, and seasonality, to generate interpretable time series forecasting. More recently, He et al. <ref type="bibr" target="#b66">[67]</ref> combined GNN, LSTM and VAE to model spatial and temporal dependency for anomaly detection of a complex cloud system. Transparently, these models mainly focus on addressing challenges related to highdimensionality, missing labels, and interpretability. In addition, based on the robust representation of VAE, conditional VAE regularizes distributional dependencies between historical observations and future data conditional on certain constraints. Inspired by controllable dialog generation <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b67">[68]</ref>, we adopt temporal conditional VAE to depict the according dynamic distributional dependencies varying with temporal factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Continuous Normalizing Flow</head><p>Continuous normalizing flow (CNF) is a continuous version of normalizing flows, which replaces the layer-wise transformations of normalizing flows with ODEs. It is widely used in image/video generation <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, molecular graph generation <ref type="bibr" target="#b70">[71]</ref>, 3D face recognition <ref type="bibr" target="#b71">[72]</ref> and time series forecasting <ref type="bibr" target="#b72">[73]</ref>. Luo et al. <ref type="bibr" target="#b70">[71]</ref> proposed a discrete latent variable model GraphDF based on CNF methods for the molecular graph generation. Zhang et al. <ref type="bibr" target="#b71">[72]</ref> learned distributional representation and then transformed this distribution into a flexible form by CNF for low-quality 3D face recognition. These models focus on estimating distribution with a flexible form but conditional constraints need to be considered for capturing complex relationships among variables in reality. For example, given a set of attributes, Abdal et al. <ref type="bibr" target="#b68">[69]</ref> devised a CNF-based technique to conditionally resample images with high quality from the GAN latent space. Rasul et al. <ref type="bibr" target="#b72">[73]</ref> used a CNF to represent the data distribution in an autoregressive deep learning model. Inspired by these two types of time point modeling processes <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b71">[72]</ref>, we design a CCNF to invertibly transform Gaussian distribution into a complex distribution to flexibly represent temporal conditional distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARY</head><p>A time series x i = {x i 1 , x i 2 , ..., x i T } ∈ R T records the observed values of variable i with T timestamps. A multivariate time series is represented as X = {x 1 , x 2 , ..., x i , ..., x dx } ∈ R dx×T , where d x is the variable dimension (the number of univariate time series) and the sampling time interval between two adjacent observed values is constant for all time series. We reformulate the multivariate time series X = {x 1 , x 2 , ..., x t , ..., x T } ∈ R T ×dx along the timeline, where x t denotes the multivariate values at timestamp t.</p><p>Problem Statement Under the rolling forecasting setting, given a fixed input time window w and time horizon h, we have the historical input of w steps X t = {x t-w+1 , x t-w+2 , ..., x τ , ..., x t } ∈ R w×dx at timestamp t, and our target is to forecast the future sequence of h steps Y t = {y t+1 , y t+2 , ..., y τ , ..., y t+h } ∈ R h×dy successive to VAE Architecture VAE gains creativity for its capability of accurately estimating meaningful latent distribution rather than a single data point as in traditional AE. The architecture of VAE consists of an encoder and a decoder. The encoder first encodes the input representation X to a latent distribution and generates Z ∈ Z obeying this distribution, then the decoder decodes the original input representation X from Z. The model distribution p θ (x) is formulated as:</p><formula xml:id="formula_0">p θ (x) = Z p θ (x|z)p θ (z)dµ(z), ∀x ∈ X, z ∈ Z<label>(1)</label></formula><p>where p θ (z) is a prior distribution over the latent Z and p θ (x|z) is the estimated distribution. µ(z) is the base measure of the latent space Z. With the above architecture, while the encoder gives service to model the posterior distribution q ϕ (z|x), the decoder gives service to approximate the estimated distribution p θ (x|z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>In this section, we propose TCVAE to address the distributional drift problem of MTS forecasting. The overview of the TCVAE architecture is shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preprocessing</head><p>To guarantee the robustness of TCVAE, the raw MTS data X = {x 1 , x 2 , ..., x t , ..., x T } ∈ R T ×dx is normalized and then converted to time-series windows such as X t ∈ R w×dx at timestamp t, both for training and testing. Following <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, we perform normalization on X as:</p><formula xml:id="formula_1">x t ← x t -min(X ) max(X ) -min(X ) + ϵ ′<label>(2)</label></formula><p>where min(•) and max(•) are the minimum and maximum vectors respectively. To avoid zero-division, a constant offset</p><formula xml:id="formula_2">ϵ ′ = | min(X )| is required.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input Representation</head><p>Transformer is used as the skeleton of the encoder (T e ) and the decoder (T d ) as shown in Fig. <ref type="figure">2</ref>. Transformer-based models use a self-attention mechanism and timestamps served as global context. Intuitively, we regard the distributional drift problem as identifying the shifting context from global information like hierarchical timestamps (A.M. and P.M., day, week, month, year) and agnostic timestamps (holidays, special events). Assuming we have the multivariate values x τ ∈ X t at timestamp τ and the corresponding global timestamp set S τ ∈ R r which contains r = 6 types of global timestamps including month, day, week, hour, minute and A.M./P.M.. Following the input representation in <ref type="bibr" target="#b20">[21]</ref>, we embed global timestamps using trainable stamp embeddings</p><formula xml:id="formula_3">SE(τ ) = r-1 i=0 DS τ [i] ∈ R d ,</formula><p>where d is the embedding dimension and D ∈ R d×1 is an embedding matrix. In addition, we use a fixed position embedding to preserve the local context, i.e., PE(pos, 2j) = sin (pos/(2w) 2j/d ), PE(pos, 2j + 1) = cos (pos/(2w) 2j/d ),</p><p>where j ∈ {1, ..., ⌊d/2⌋}. Then, we adopt a token embedding, i.e., WE(x τ ) = Conv1d(x τ ) where Conv1d(•) performs 1-D convolution (kernel width=3, stride=1), and projects x τ into a d-dim vector. Thus, the final embedding output can be formulated as:</p><formula xml:id="formula_4">xτ = ρWE(x τ ) + PE(τ ) + SE(τ ) X = Concat({x τ } t τ =t-w+1 )<label>(3)</label></formula><p>where xτ represents the embedding at timestamp τ , X ∈ R w×d and ρ is a learnable factor balancing the magnitude between the token embedding and position/stamp embeddings. The embeddings output from WE, PE and SE are denoted as {u t-w+1 , ..., u τ , ..., u t }, {p t-w+1 , ..., p τ , ..., p t } and {s t-w+1 , ..., s τ , ..., s t } in a feature-wise manner respectively, where</p><formula xml:id="formula_5">u τ = WE(x τ ) ∈ R d , p τ = PE(τ ) ∈ R d and s τ = SE(τ ) ∈ R d . Let X and X = [X t ; X 0 ] denote the inputs</formula><p>of T e and T d respectively, where X t is the start token and X 0 is the target sequence that masks the variable values to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Factor Representation</head><p>Temporal Hawkes Attention In a time slice, the values at each time have different impacts on future values. Therefore, when calculating temporal factor information, we should weigh the critical moments that affect future values <ref type="bibr" target="#b73">[74]</ref>. Assuming that the current time is t, we employ a temporal attention mechanism ϑ(•) which learns to decide the critical time points (timestamps). This mechanism aggregates temporal features U = [u t-w+1 , ..., u τ , ..., u t ] ∈ R d×w from different time points into an overall representation using learned attention weights α τ for each time point t -w + 1 ≤ τ ≤ t. We formulate this mechanism as:</p><formula xml:id="formula_6">α τ = exp(u T τ W U) τ exp(u T τ W U)<label>(4)</label></formula><formula xml:id="formula_7">ζ τ = α τ u τ (5) ϑ(U) = τ ζ τ (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where W is a learned linear transform, T denotes transpose and α τ denotes the attention weights used to aggregate all temporal features. The Hawkes process is a self-excited time point process, where the discrete event sequence in continuous time is modeled. Each previous event excites this process to varying degrees. For example, the release of financial policies and crisis information will affect future prices corresponding to varying degrees in the stock market. Although ϑ(U) has captured this complex excitement relationship, the hidden representation cannot well retain the original interpretable factors. Inspired by <ref type="bibr" target="#b2">[3]</ref>, we propose a temporal Hawkes attention (THA) mechanism to enhance the temporal attention mechanism ϑ(•) with interpretability by using a Hawkes process. THA learns an excitation parameter ϵ corresponding to a time point τ and a decay parameter γ to learn the decay rate of this induced excitement. We compute the temporal features B:</p><formula xml:id="formula_9">B = τ =0,∆tτ ≥0 (ζ τ + ϵ max(ζ τ , 0)e -γ∆tτ )<label>(7)</label></formula><p>where ∆t τ is the time interval between the current and the past time point τ .</p><p>Factor Extraction Given the temporal features B ∈ R w×d , we manually construct temporal factors for each feature series b i ∈ R w , 1 ≤ i ≤ d. Specifically, we obtain the start value sta, end value end, maximum max, minimum min, med-value med, mean µ and standard deviation std for each feature series and calculate the standard deviation of the means µ and the standard deviation std, i.e., std(µ) = std µ(b i )</p><formula xml:id="formula_10">d i=1</formula><p>and std(std) = std std(b i )</p><formula xml:id="formula_11">d i=1</formula><p>respectively. Note that we use replication operation on std(µ) and std(std) to align dimensions. Accordingly, for each feature series, we construct a feature vector of q temporal factors {sta, end, max, min, med, µ, std, std(µ), std(std)} ∈ R q×d , where q = 9 is predefined. Stacking all the feature vectors, we obtain C ∈ R q×d . The temporal factors of T d are denoted as C, similar to those (C) of T e as shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gated Attention Mechanism</head><p>We believe that the encoding and decoding processes related to prediction should be adapted to distribution changesas shown in the introduction. Here, the traditional multihead self-attention of encoder T e and decoder T d is briefly introduced. Given each training sample X ∈ R w×d , we perform canonical scaled-dot product attention on the tuple input (Q (query),</p><formula xml:id="formula_12">K (key), V (value)) as A(Q, K, V) = Softmax( QK T √ d )V</formula><p>, where d is the input representation dimension. The Softmax operation shapes the convex combination weights for the values in V, allowing the matrix V to be compressed into a smaller representative vector for simplified reasoning in the downstream neural network. A uses a √ d term to scale the weights to reduce the variance of the weights. Multi-head self-attention is applied in the form of passing Q, K, V through n heads and feed-forward layers.</p><p>To control the information flow existing in multiple heads adapted to changing temporal factors, we propose a gated attention mechanism (GAM) that extends the above popular scalar attention mechanism by calculating a vector gate instead of a scalar value <ref type="bibr" target="#b74">[75]</ref>. The temporal factors are utilized in the vector gate with the soft or hard mode to adjust the structures of the encoder and decoder. Let H i stand for the ith head where i ∈ Z n and H = {H 1 , H 2 , ..., H n }. If we have the temporal factors C in different time slices, the information flow passing through H i and guided by C is controlled by the gate vector g i with soft mode as g i = δ(f (C, H i )), where δ represents the element-wise Sigmoid function. However, the g i of H i is also affected by other heads. Inspired by <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, we adapt the gate vector not only to temporal factors and a single head but also to temporal factors and the entire set of heads H, see Fig. <ref type="figure">2</ref>. To calculate the gate of head H i , each head in the entire set of heads H and temporal factors will present an individual gate 'vote'. Then we aggregate the votes to calculate gate g i for H i . The calculation process for g i is as follows:</p><formula xml:id="formula_13">v j = W H j + b (8) v C = W C + b<label>(9)</label></formula><formula xml:id="formula_14">s j i = H T i v j<label>(10)</label></formula><formula xml:id="formula_15">s C i = H T i v C<label>(11)</label></formula><formula xml:id="formula_16">β j i = exp(s j i ) k∈[1,...,n] exp(s k i ) + exp(s C i )<label>(12)</label></formula><formula xml:id="formula_17">β C i = exp(s C i ) k∈[1,...,n] exp(s k i ) + exp(s C i )<label>(13)</label></formula><formula xml:id="formula_18">g i = f i (C, H) = δ( j (β j i H j ) + β C i C)<label>(14)</label></formula><p>Here, j ∈ Z n , W and b are learnable weights and biases shared among functions f 1 , ..f n . The parameterized function f is more flexible in modeling the interaction between vectors C and H. Vectors v j and v C are the outputs of H j and C after linear transformation. s j i is the unnormalized attention score that input H j put on H i and β j i is the normalized score. s C i is the unnormalized attention score that temporal factor C put on H i and β C i is the normalized score. The temporal factors are calculated together and the soft weights of n heads are learned. Then we apply to calculate the multihead self-attention as M = [g 1 , ..., g n ], where [•, •] denotes a concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Flexible Distribution Approximation</head><p>In MTS forecasting, the forecasting sequence and reconstruction sequence are similar generation processes <ref type="bibr" target="#b63">[64]</ref>. The standard VAE generation models assume that the latent variable Z follows a simple prior distribution such as the Gaussian distribution N (µ, σ 2 I) with learnable parameters mean µ and variance σ 2 . To make the training process differentiable, the re-parameterization trick can be adopted:</p><formula xml:id="formula_19">Z = µ + εσ (15) ε ∼ N (0, I)<label>(16)</label></formula><p>However, the latent space is affected by underlying factors over time and becomes increasingly dynamic and complicated to estimate. Motivated by <ref type="bibr" target="#b42">[43]</ref>, we propose a flexible distribution approximation (FDA) module where temporal factors are involved in accurately approximating the prior and posterior distributions as shown in Fig. <ref type="figure">2</ref>. We feed the temporal factors C and the output M of the GAM into the process of learning the latent variable. By transforming random noise ϵ using neural networks, we sample from the prior Gaussian distribution and posterior distribution over the latent variables. In particular, while the prior sample Ẑ ∼ p θ ( Ẑ|C) is produced by a generator G from temporal-dependent random noise ε, the approximated posterior sample Z ∼ q ϕ (Z|C, M) is produced by a generator Q from temporal-dependent random noise ϵ. G and Q are feed-forward neural networks. Both ε and ϵ, whose mean covariance matrix are computed from C with the prior network P and recognition network R respectively, are subject to a normal distribution: X, X, C, C ← X t by Equations ( <ref type="formula" target="#formula_1">2</ref>)- <ref type="bibr" target="#b6">(7)</ref> 5:</p><formula xml:id="formula_20">μ log σ2 = Ŵ P θ (C) + b (17) ε ∼ N (ϵ; μ, σ2 I)<label>(18</label></formula><p>µ, σ 2 , μ, σ2 ← T e (X, C)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Ẑ ← G θ (ε), ε ← μ, σ2 , ϵ by Equations ( <ref type="formula" target="#formula_20">18</ref>), <ref type="bibr" target="#b18">(19)</ref> 7:</p><p>Z ← Q ϕ (ϵ), ϵ ← µ, σ 2 , ϵ by Equations ( <ref type="formula" target="#formula_23">21</ref>), <ref type="bibr" target="#b21">(22)</ref> 8:</p><p>Compute the transferred latent variable, i.e. q(Z * |M, C) and p( Ẑ * |C) by Equation ( <ref type="formula">25</ref>)</p><formula xml:id="formula_21">9: Ŷ ← F(T d ( X, C, Z * )), X ← B(T d ( X, C, Z * )) 10:</formula><p>Compute the stochastic gradient of Θ based on L(Θ) by Equation ( <ref type="formula" target="#formula_29">26</ref>), <ref type="bibr" target="#b26">(27)</ref> 11:</p><p>Update Θ based on the gradients and learning rate γ 12:</p><formula xml:id="formula_22">n ← n + 1 13: end while Ẑ = G θ (ε)<label>(19)</label></formula><formula xml:id="formula_23">µ log σ 2 = W R ϕ ( M C ) + b (20) ϵ ∼ N (ϵ; µ, σ 2 I)<label>(21)</label></formula><formula xml:id="formula_24">Z = Q ϕ (ϵ)<label>(22)</label></formula><p>where</p><formula xml:id="formula_25">G θ (•), P θ (•), Q ϕ (•) and R ϕ (•) are feed-forward neural networks.</formula><p>Conditional Continuous Normalizing Flow The posterior distribution q(Z|M, C) is intractable but usually approximated by a Gaussian distribution. Nevertheless, it is unrealistic to think of the posterior as a Gaussian distribution. Regardless of the family of distributions we choose to estimate the posterior, it may not fit. Inspired by <ref type="bibr" target="#b71">[72]</ref>, we use CCNF to invertibly transform the Gaussian distribution into a complex distribution for flexibly inferring the temporal conditional distribution. First, a latent variable obeys a Gaussian distribution N (µ, σ). A CCNF Φ is then used to convert the sample of this Gaussian distribution. After applying continuous-time dynamics, the latent variable Z * ∈ R w×k , where k denotes the representation dimension of the latent variable, is computed as follows:</p><formula xml:id="formula_26">Z * = Φ(Z(t 0 )) = Z(t 0 ) + t1 t0 Ω(Z(t), t)dt<label>(23)</label></formula><formula xml:id="formula_27">Z(t 0 ) ∼ N (µ, σ)<label>(24)</label></formula><p>where Z = Z(t 0 ), Z * = Z(t 1 ), and Ω is a continuous mapping as Ω(Z(t), t) = ∂Z(t) ∂t with the initial value Z(t 0 ) = Z 0 . Therefore, the log-density of the transferred latent variable is:  </p><formula xml:id="formula_28">log q(Z * |M, C) = log q(Z(t 0 )|M, C) -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Learning Objective</head><p>We input the flexible posterior distribution q(Z * |M, C), temporal factors C and input representation X together into the decoder T d for decoding. The decoding results flow to the backcasting network B and the forecasting network F and generate outputs denoted by X and Ŷ, respectively. Given the generated waveforms X and Ŷ, we thus want to maximize the variational lower bound, also alluded to as the evidence lower bound (ELBO):</p><formula xml:id="formula_29">max θ,ϕ,ψ,Υ E q ϕ (Z * |M,C) log p ψ ( X|Z * , C, X) + E q ϕ (Z * |M,C) log p Υ ( Ŷ|Z * , C, X) -λKL(q ϕ (Z * |M, C)||p θ ( Ẑ * |C))<label>(26)</label></formula><formula xml:id="formula_30">KL(q ϕ (Z * |M, C)||p θ ( Ẑ * |C)) = E q ϕ (Z * |M,C) log q ϕ (Z * |M, C) -E p θ ( Ẑ * |C) log p θ ( Ẑ * |C) = E q ϕ (Z * |M,C) [ Z * 2 2 - t1 t0</formula><p>Tr( ∂Ω ∂Z(t)</p><p>)dt]</p><formula xml:id="formula_31">-E p θ ( Ẑ * |C) [ Ẑ * 2 2 - t1 t0 Tr( ∂Ω ∂ Ẑ(t) )dt]<label>(27)</label></formula><p>where p θ ( Ẑ * |C) and q ϕ (Z * |M, C) are neural networks implementing Equations ( <ref type="formula">17</ref>)- <ref type="bibr" target="#b24">(25)</ref>. p( Ẑ * |C) is the prior distribution given condition C. p ψ ( X|Z * , C, X) is the integration of decoder T d and backcasting network B. p Υ ( Ŷ|Z * , C, X) is the integration of decoder T d and forecasting network F. λ is a trade-off parameter.</p><p>The pseudo-code for the overall training process is summarized in Algorithm 1. We aim at minimizing the training loss L(Θ), i.e., the negative ELBO that can be considered as the sum of reconstruction loss -log p ψ ( X|Z * , C, X), forecasting loss -log p ψ ( Ŷ|Z * , C, X) and Kullback-Leibler KL divergence q ϕ (Z * |M, C)||p θ ( Ẑ * |C). Θ is the model parameter and we set the loss weight for all experiments.</p><p>V. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Metrics</head><p>We select the following six representative real-world multivariate time series datasets from different application scenarios for experimental evaluation and summarize them in Table <ref type="table" target="#tab_2">I</ref>. Real-world datasets suffer from non-stationarity, i.e., the distribution changes over time. Following <ref type="bibr" target="#b31">[32]</ref>, we especially chose the Augmented Dick-Fuller (ADF) test statistic as the metric to quantitatively measure the degree of distribution drift. A larger average ADF test statistic means a higher level of nonstationarity, i.e., more severe distribution drifts. According to the average ADF test, we can see that our datasets exhibit high levels of non-stationarity derived from the distribution drift in the datasets. For instance, the METR-LA dataset exhibits intense and irregular fluctuations and temporal correlations and trends, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. These patterns are significantly distinct from normal fluctuation, which typically appears uniform, regular, and unrelated to variables or temporal factors.</p><p>Traffic COVID-19 4 The dataset records the daily number of newly confirmed cases from 1/22/2020 to 11/25/2021 collected from the COVID-19 Data Repository.</p><p>PeMSD7(M) 5 It is from the sensors spanning the freeway system of California and consists of 11,232 timestamps and 228 variables at a 5-minute interval.</p><p>METR-LA 6 The dataset records traffic information from 207 sensors of the Los Angeles County's highway. It contains 34,272 timestamps and 207 variables at a 5-minute interval.</p><p>Following previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we evaluate all comparative methods via three metrics: Mean Absolute Error</p><formula xml:id="formula_32">M AE = 1 N N i=1 |y i -ŷi |, Root Mean Squared Error RM SE = 1 N N i=1 (y i -ŷi ) 2 and Mean Absolute Percent Error M AP E = 1 N N i=1 |y i -ŷ i | y i 1{|y i | &gt; 0},</formula><p>where N denotes the number of samples in the testing set, ŷi and y i denote the prediction and groundtruth respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We elaborately compare our proposed TCVAE with the following eleven competitive MTS forecasting baselines from different classes. To be specific, two representative RNNbased methods (LSTNet and AdaRNN), three CNN-based methods (DSANet, STNorm and TS2VEC) and four advanced Transformer-based methods (Informer, Autoformer, 1 <ref type="url" target="https://archive.ics.uci.edu/ml/datasets/PEMS-SF">https://archive.ics.uci.edu/ml/datasets/PEMS-SF</ref> 2 <ref type="url" target="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014">https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</ref> 3 <ref type="url" target="https://www.nrel.gov/grid/solar-power-data.html">https://www.nrel.gov/grid/solar-power-data.html</ref>  4  (2) AdaRNN <ref type="bibr" target="#b37">[38]</ref>: It proposes a temporal distribution characterization module and a temporal distribution matching module for non-stationary MTS forecasting;</p><p>(3) DSANet <ref type="bibr" target="#b48">[49]</ref>: It constructs global and local temporal convolution to extract complicated temporal patterns and employs self-attention to model dependencies;</p><p>(4) STNorm <ref type="bibr" target="#b49">[50]</ref>: It employs temporal and spatial normalization modules which separately refine the local component and the high-frequency component underlying the raw data. We use the version with Wavenet as the backbone;</p><p>(5) StemGNN <ref type="bibr" target="#b9">[10]</ref>: It uses a graph network to capture inter-series and intra-series correlation jointly in the spectral domain;</p><p>(6) AGCRN <ref type="bibr" target="#b0">[1]</ref>: It has a graph-learning component and a personalized RNN component to extract fine-grained spatial and temporal correlations automatically;</p><p>(7) TS2VEC <ref type="bibr" target="#b36">[37]</ref>: It focuses on using timestamp masking and random cropping to learn augmented context views for dealing with level shifts of time series data;</p><p>(8) Informer <ref type="bibr" target="#b20">[21]</ref>: It is a Transformer-like architecture with a sparse-attention mechanism to maintain a higher capacity for long sequence prediction;</p><p>(9) Autoformer <ref type="bibr" target="#b22">[23]</ref>: It designs a decomposition framework with an efficient and accurate auto-correlation mechanism for long-term MTS forecasting;</p><p>(10) FEDformer <ref type="bibr" target="#b23">[24]</ref>: It introduces a frequency-enhanced attention mechanism with Fourier and Wavelet transform and a mixture of experts decomposition to extract trend components for long-term MTS forecasting;</p><p>(11) Nstatformer <ref type="bibr" target="#b31">[32]</ref>: It introduces a series stationarization module with statistics for better predictability and a destationary attention module to re-integrate the inherent non-stationary information for non-stationary MTS forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details and Settings</head><p>Following the commonly-used settings in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, we predict future h = 24 timestamps using historical w = 24 timestamps. Additionally, we vary the prediction timestamps in a range of h ∈ {48, 72, 96, 180, 720} on PeMSD7(M) and Solar-Energy respectively to investigate the performance of long-term forecasting. We use an Adam optimizer with a fixed learning rate of 0.001, a batch size of 64, and epochs of 50 to train our model. In addition, we elaborately tune the hyperparameters and select the settings with the best performance: specifically, the trade-off parameter λ ∈ {1, 10 -1 , 10 -2 , 10 -3 }, the number of heads n ∈ {4, 8, 16}, the representation dimension d ∈ {256, 512, 1024} and the latent variable dimension k ∈ {256, 512, 1024}. Regarding the rolling step, we consider a commonly-used setting <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b76">[77]</ref> for a multistep forecasting setup that the rolling step is fixed to 1. In the parameter sensitivity experiment, the window size w is selected from {24, 48, 96}. We implement TCVAE and other baselines on Python 3.6.13 with the package Pytorch 1.8.1. The codes of our model and baselines are carried out on Ubuntu 18.04.6 LTS, with one Inter(R) Xeon(R) CPU @ 2.10GHz and four NVIDIA GeForce 3090 GPU cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Overall Comparison:</head><p>We adopt three widely used metrics, MAE, RMSE, MAPE, to measure the performance of our proposed TCVAE and all the comparative models. Table II shows the overall experimental results for the default forecasting setup of w = 24 and h = 24, where the best results are highlighted in bold and the suboptimal results are underlined. A smaller value indicates better performance. Remarkably, TCVAE establishes a new state-of-the-art and achieves 0.91%-15% MAE improvement, 3.95%-25.36% RMSE improvement and 0.48%-2.39% MAPE improvement over all baselines on most of the datasets. Especially for some of the datasets, such as Traffic and METR-LA, the improvement is even more significant (over 10%). Regarding the temporal-aware sequential models, the Transformer-based models, i.e., Informer, Autoformer and FEDformer, achieve obviously better performance over RNN-and CNN-based models (including LSTNet, DSANet and STNorm), showing For example, StemGNN shows competitive performance on PeMSD7(M) and METR-LA, and AGCRN achieves desirable performance on Traffic and Electricity. The reason lies in that the models use parallel parameter space for time series to model temporal and topology dynamics jointly without the need for a predefined graph structure. However, the GNNbased models, especially AGCRN, require higher memory space compared with other models when dealing with highdimensional data such as Traffic (963 dimensions). In addition, the above GNN-based models and temporal-aware sequence models have performance limitations and fall behind Nstatformer and our proposed TCVAE, this is attributed to the fact that the distributions of adjacent time slices are diverse and change over time and the potential of current deep models is still constrained by such non-stationary data. Nstatformer tackles the issue by recovering the intrinsic non-stationary information into temporal dependencies and achieves impressive performance second only to TCVAE. TS2VEC and AdaRNN also deal with the distribution drift problem, but their performance is unsatisfactory on some datasets, which may be due to the limitations of RNN and CNN as backbones for learning temporal dependencies. From all the results, we conclude that, in addition to capturing temporal information and inter-series correlations, using temporal factors to guide distributional drift learning is important and effective.</p><p>To investigate the ability of the comparative models in long sequence forecasting, we further conduct experiments by fixing w = 24 and enlarging h to 48, 72, 96, 180 and 720. The experimental results are shown in Table <ref type="table" target="#tab_5">III</ref>, where the best results are highlighted in bold, the suboptimal results are underlined, and OOM denotes running out of memory. We compare TCVAE with all baselines on PeMSD7(M) and Solar-Energy except for LSTNet, DSANet, and STNorm, which are the bottom-3 baselines from the above experiments. From the results, we can observe: (1) TCVAE significantly outperforms most of the baselines and achieves 3.80%-11.24% MAE improvement, 1.63%-9.49% RMSE improvement and 0.08%-24.87% MAPE improvement over all baselines on most of the datasets, indicating the superiority of TCVAE in long sequence forecasting; (2) it is worth noting that AGCRN highly benefits from modeling inter-series correlations within a topological structure and achieves suboptimal and even better performance than TCVAE on PeMSD7(M) with h ∈ {96, 180, 720}. This may be attributed to the fact that long time series facilitates a more accurate learned structure but bring some challenges for learning distributional drifts. However, when the total number of time steps is extremely large, e.g., on Solar-Energy, AGCRN and StemGNN run out of memory (as marked in OOM in Table <ref type="table" target="#tab_5">III</ref>) with the prediction step h = 720. In contrast, TCVAE functions well on Solar-Energy, further indicating the high scalability of TCVAE, relative to GNN-based models;</p><p>(3) when h ∈ {180, 720}, the advantage of AdaRNN in characterizing and matching temporal distribution information decreases compared with Nstatformer and our TCVAE, validating an RNN-type backbone is difficult to model the long sequences and easily affected by the cumulative errors; (4) on the PeMSD7(M) and Solar-Energy datasets, Informer achieves appreciable performance for long-range forecasting second only to our TCVAE. We attribute this close performance to the fact that the above datasets exhibit relatively weak nonstationary properties, which is consistent with the ADF test statistic in Table <ref type="table" target="#tab_2">I</ref>.</p><p>2) Ablation Study: To better understand the effectiveness of different components in TCVAE, we perform additional experiments on Electricity and PeMSD7(M) with ablation consideration by removing different components from TCVAE.</p><p>In the experiments, we select a setting of w = 24 and h = 24. Table <ref type="table" target="#tab_6">IV</ref> summarizes the results and shows that all the   Flexible Distribution Approximation To study the validity of our proposed FDA insightfully, we select three time windows 8:00-10:00 A.M. (the 96th window), 3:00-5:00 P.M. (the 192nd window) and 10:00-12:00 P.M. (the 288th window) at a 7-hour interval. We fit the flexible posterior of FDA and a Gaussian posterior in the left part of Fig. <ref type="figure" target="#fig_3">4</ref>. The target distribution, predicted distribution, and Gaussian distribution are compared in the right part. We can see that more details are reserved by the flexible posterior. A specific distribution such as a univariate Gaussian cannot approximate a target distribution (e.g. a multivariate distribution) accurately (red arrows in Fig. <ref type="figure" target="#fig_3">4</ref>). The visualization is consistent with the study <ref type="bibr" target="#b77">[78]</ref>: the distribution of time series data that changes over time can be better approximated by a dynamic mixture distribution. Although there is a mean left shift (brown arrows in Fig. <ref type="figure" target="#fig_3">4</ref>) between the target distribution and the predicted distribution, the deviation is reduced via learning a flexible distributional representation adaptive to temporal factors. addition, we choose four sensors from METR-LA and show their corresponding locations on Google Map (left part in Fig. <ref type="figure" target="#fig_4">5</ref>). In this experiment, we introduce additional intermediate variables Z # = M(Z ⋆ ) which can reflect the patterns learned by Z ⋆ and X # = M( X), where M(Z ⋆ ) : R k → R dx and M( X) : R d → R dx are two feed-forward mappers before decoder T d and d x = 207 denotes the number of sensors. Since each latent column vector in Z # always corresponds to a unique sensor in raw multivariate time series, the correlation matrix of different sensors is computed by Z # on two-time windows 8:00-10:00 A.M. and 10:00-12:00 P.M. (the right part in Fig. <ref type="figure" target="#fig_4">5</ref>). The ith column in the matrix embodies the correlation strength between sensor #i and other sensors in the real-world environment. We can observe that the correlation between sensor #3 and sensor #4 is high and hardly varies, while the low correlation between sensor #3 and sensor #12, #17 varies dramatically over time. The results suggest that some sensors are always closely related to each other because of the fixed spatial relationship, however, other sensors are closely related in one-time slice but 'leave apart' relations in other time slices. This dynamic is reasonable, since sensor #3 and sensor #12, #17 are located on different main roads surrounded by different environments (schools, parks, etc.). Therefore, the module FDA can capture both invariance and dynamics of time series in the process of distributional drift adaptation.</p><p>Gated Attention Mechanism Next, we qualitatively analyze the time-guided component of TCVAE by comparing GAM and traditional multi-head attention (MHA) over a 24-  <ref type="figure">6</ref>(a). We see that TCVAE using MHA predicts the 25th day CC with a relative error of 11.18% from the actual value, whereas TCVAE using GAM predicts that closer to the actual value (1.23%). Despite the varying trends throughout the historical window, GAM focuses on the sharp upscent trend (e.g., the 21st day, the 23rd day) towards the end of the window, whereas MHA learns weighted scores and captures a gentle upscent trend (e.g., the 1st day, the 15th day). The results of GAM are consistent with the theory <ref type="bibr" target="#b73">[74]</ref>: the impact of long-distance events gradually decays, potentially because GAM effectively adapts the information flow of multiple heads to temporal factors to decay the impact of long-distance time points enabling TCVAE to focus on more recent salient time points, which better reflects the temporal state of event development. In Fig. <ref type="figure">6</ref>(b), we further look back 24-hour Occupancy Rate (OR) of a randomly selected sensor from the test set of Traffic with obvious periodical patterns <ref type="bibr" target="#b78">[79]</ref>. TCVAE using GAM predicts the 25th hour OR closer to the actual value than using MHA. In terms of hour-level attention, GAM accurately assigns larger attention scores to hours with an upscent trend (e.g., the 1st hour, the 22nd hour), whereas MHA assigns larger weights to hours with a descent trend (e.g., the 11th hour, the 12th hour). On periodic data, we note that GAM hardly reflects the regularity that the impact of longdistance events gradually decays, which is reasonable because periodic data generally exhibits repeated patterns where distant time points may receive high attention scores due to similar patterns.</p><p>Temporal Hawkes Attention To understand what is of importance the temporal Hawkes attention learns, we compare THA and temporal attention (TA) denoted as ϑ(•) in Section IV.C over a 24-day lookback for the number of CC in Beijing from the COVID-19 training set in June 2020. Accordingly, the visualization of day-level attention and the predicted curves using THA and TA are given in Fig. <ref type="figure">7</ref>. TCVAE using TA predicts the 25th day CC with a relative error of 19.72% from the actual value, whereas using THA, predicts the cases closer to the actual value (6.30%). THA receives more attention scores concentrated in the later part of the window and captures the fast-rising trend, whereas TA learns more scattered weighted scores and captures a gentle uptrend, probably because THA better captures the excitation induced by influential events, i.e., a clustered epidemic outbreak starting in the middle of the window. 5) Computational Complexity: With the multivariate setting and all the methods' current finest implementation, we perform a statistical comparison of the time costs and parameter volumes on Traffic in Table <ref type="table" target="#tab_8">VI</ref>. We investigate the computational Although TCVAE needs relatively large parameters and time costs, it has addressed the challenging distribution drift issue instead of the general MTS forecasting issue and achieved a significant performance improvement over the baselines, see Table <ref type="table" target="#tab_4">II</ref>. In addition, since TCVAE shows a considerable improvement over w/o CCNF, TCVAE still maintains the timeconsuming CCNF module to achieve higher performance and transform the Gaussian distribution to a complex and form-free distribution, see Table <ref type="table" target="#tab_6">IV</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>. In summary, considering the significant performance improvement of TCVAE and its effectiveness in tackling the challenging distribution drift in MTSs, the computation and time costs of TCVAE are moderate and acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this work, a novel variational autoencoder architecture TCVAE for distributional drift adaptation is introduced to model the dynamic dependencies between historical observations and future data varying with time in MTS. Specifically, we design a temporal Hawkes attention mechanism to represent temporal factors that estimate the temporal Gaussian and a gated attention mechanism to dynamically adapt the network structure of the Transformer-based encoder and decoder. We propose to take advantage of transforming the temporal Gaussian into a flexible distribution that breaks the limitations of distributional form for inferring the temporal conditional distribution. In a variety of MTS forecasting applications, TCVAE based on more robust representation regularly outperforms previous techniques.</p><p>We will consider future work from two perspectives. First, TCVAE learns the distributional drift adaptation in estimating the conditional distribution of future data based on the assumption that the distributional drift frequencies of the training and testing sets are the same. The cases of different distribution frequencies often exist in some scenarios. We will consider the dynamics to improve our model in the future. Second, we will explore its application in other real distribution-drift scenarios, such as research hot spots, stock trend forecasting, and product demand analysis.</p><p>Longbing Cao (@SM in 2006) received PhD degree in pattern recognition and intelligent systems from the Chinese Academy of Science, China, and another PhD degree in computing sciences from the University of Technology Sydney, Australia. He is a Distinguished Chair Professor with Macquarie University, an ARC Future Fellow (professorial level), and the EiCs of IEEE Intelligent Systems and Journal of Data Science and Analytics. His research interests include artificial intelligence, data science, machine learning, behavior informatics, and their enterprise applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of MTS traffic flow derived from METR-LA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>By this means, the Gaussian distribution is turned into a form-free posterior. The same operation is also performed on Ẑ to obtain Ẑ * . This transformation facilitates our model to learn a more flexible distribution p( Ẑ * |C) and q(Z * |M, C). CCNF can potentially learn a less entangled internal representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Parameters sensitivity analysis of TCVAE on MAE and MAPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distribution of different time windows, METR-LA. The left part is a comparison of flexible and Gaussian posterior in FDA. Comparisons of the target distribution, predicted distribution, and Gaussian distribution are shown in the right part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of correlation matrix at 8:00-10:00 A.M. (the 96th window) and 10:00-12:00 P.M. (the 288th window) on 9/1/2018, METR-LA.</figDesc><graphic coords="12,48.96,155.43,137.90,90.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Day/hour-level attention visualization and prediction using TCVAE with GAM and MHA methods, where darker colors indicate higher weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. The overall architecture of TCVAE for MTS forecasting. (1) The input Xt is first fed into the input representation module and {u t-w+1 , ..., uτ , ..., ut} is extracted separately for temporal factor representation; (2) The encoder Te takes X as input and controls the information flow output from multiple heads by introducing temporal factors C into gated attention. The corresponding details of the decoder T d are similar to the encoder Te with input X and C; (3) The final loss is a combination of forecasting loss, backcasting loss, and KL divergence.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Input Representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder 𝓣 𝒆</cell></row><row><cell>𝒖 "#$%&amp;</cell><cell>𝒖 "#$%'</cell><cell>𝒳 " …</cell><cell>𝒖 O …</cell><cell>𝒖 "</cell><cell>+ + 𝜌</cell><cell cols="2">𝒖 "#$%&amp; 𝒔 "#$%&amp; 𝒑 "#$%&amp;</cell><cell cols="2">… … … Temporal Factor 𝒖 "#$%' 𝒖 O … 𝒔 "#$%' 𝒔 O … 𝒑 "#$%' … 𝒑 O Representation</cell><cell>𝒖 " 𝒔 " 𝒑 "</cell><cell cols="3">… 𝑐 , 𝒖 O =WE( 𝐱 O ) 𝑐 &amp; 𝑐 ' 𝒔 O =SE(𝜏) 𝒑 O =PE(𝜏) 𝐂</cell><cell>𝐗</cell><cell>Attention</cell><cell>Multi-Head</cell><cell>𝐂</cell><cell>𝑓 &amp; 𝑓 ' 𝑓 . Gated Attention Mechanism 𝐇 &amp; 𝐇 ' Feed 𝐇 . Forward …</cell><cell>𝐌 …</cell><cell>⨁</cell><cell>ℛ 𝒫</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝐂 G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Forecast Loss Backcast Loss</cell><cell>Y J X J</cell><cell>ℱ ℬ</cell><cell></cell><cell></cell><cell>⨁</cell><cell>…</cell><cell>Feed Forward</cell><cell>𝑓 &amp; 𝑓 . 𝑓 ' …</cell><cell></cell><cell>𝐇 &amp; 𝐇 ' 𝐇 .</cell><cell>Attention</cell><cell>Multi-Head</cell><cell>𝐗 F</cell><cell cols="2">𝐙  *  𝐙 9  *  𝕂𝕃</cell><cell></cell><cell>CCNF Φ</cell><cell>𝐙 𝐙 9</cell><cell>𝒢</cell><cell>𝜖 𝜖Q 𝜇, 𝜎 𝜇 @, 𝜎 @</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝐌</cell><cell cols="4">Gated Attention Mechanism</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Decoder 𝓣 𝒅</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flexible Distribution Approximation</cell></row></table><note><p><p>timestamp t, where d y is the variable dimension in forecasting, x τ and y τ denote the multivariate values at timestamp τ in the historical input and future sequence respectively.</p>Distributional Drift At timestamp t, assume Y is drawn from a conditional distribution p(Y|X ). In the stationary case, p(Y t |X t ) known by a learning model at timestamp t is applicable to future timestamps; however the time series data generally changes over time, a distributional drift may occur at timestamp t, indicating p(Y|X ) ̸ = p(Y t |X t ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Algorithm 1 The TCVAE training algorithm Require: window-sized input X t , encoder T e , decoder T d , feed-forward networks G, Q, F, B, iteration limit N , learning rate γ 1: Initialize model parameters Θ 2: n ← 0 3: while n&lt;N do</figDesc><table><row><cell>4:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I DATASET</head><label>I</label><figDesc>STATISTICS, WHERE LARGER AVERAGE ADF TEST STATISTIC INDICATES MORE SEVERE DISTRIBUTION DRIFTS</figDesc><table><row><cell>Datasets</cell><cell>Traffic</cell><cell>Electricity</cell><cell>Solar-Energy</cell><cell>COVID-19</cell><cell>PeMSD7(M)</cell><cell>METR-LA</cell></row><row><cell>Samples</cell><cell>10,560</cell><cell>25,968</cell><cell>52,560</cell><cell>673</cell><cell>11,232</cell><cell>34,272</cell></row><row><cell>Variables</cell><cell>963</cell><cell>370</cell><cell>137</cell><cell>280</cell><cell>228</cell><cell>207</cell></row><row><cell>Sample Interval</cell><cell>1 hour</cell><cell>1 hour</cell><cell>10 minutes</cell><cell>1 day</cell><cell>5 minutes</cell><cell>5 minutes</cell></row><row><cell>Start Time</cell><cell>1/1/2015</cell><cell>1/1/2011</cell><cell>1/1/2006</cell><cell>1/22/2020</cell><cell>7/1/2016</cell><cell>9/1/2018</cell></row><row><cell>Average ADF Test Statistic</cell><cell>-9.89</cell><cell>-7.62</cell><cell>-37.23</cell><cell>0.06</cell><cell>-14.20</cell><cell>-16.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 It records the hourly occupancy rate of different lanes on San Francisco highway from 2015 to 2016 and contains 10,560 timestamps and 963 sensors. Electricity 2 It collects the hourly electricity consumption of 370 clients and involves 25,968 timestamps. Solar-Energy 3 It collects the solar power production records every 10 minutes from 137 PV plants in Alabama State in 2006 and includes 52,560 timestamps.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II OVERALL</head><label>II</label><figDesc>COMPARISON OF DIFFERENT METHODS ON SIX DATASETS WITH w = 24, h = 24</figDesc><table><row><cell>Datasets</cell><cell>Traffic</cell><cell>Electricity</cell><cell>Solar-Energy</cell><cell>COVID-19</cell><cell>PeMSD7(M)</cell><cell>METR-LA</cell></row><row><cell>Metrics</cell><cell cols="6">MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE</cell></row><row><cell cols="7">LSTNet 0.067 0.099 1.971 0.139 0.191 0.568 0.169 0.254 4.437 0.319 0.392 0.467 0.155 0.183 0.339 0.189 0.279 1.370</cell></row><row><cell cols="7">AdaRNN 0.043 0.070 0.894 0.108 0.162 0.601 0.106 0.123 2.564 0.423 0.482 0.500 0.093 0.133 0.272 0.163 0.227 0.167</cell></row><row><cell cols="7">DSANet 0.059 0.089 1.236 0.084 0.128 5.948 0.153 0.230 4.247 0.164 0.230 0.720 0.154 0.198 0.337 0.168 0.316 0.228</cell></row><row><cell cols="7">STNorm 0.056 0.080 1.143 0.153 0.167 2.476 0.120 0.167 3.898 0.263 0.264 0.541 0.094 0.178 0.247 0.119 0.240 0.196</cell></row><row><cell cols="7">StemGNN 0.047 0.076 1.129 0.193 0.268 0.607 0.086 0.110 3.848 0.528 0.635 0.530 0.077 0.123 0.232 0.116 0.209 0.167</cell></row><row><cell cols="7">AGCRN 0.041 0.079 0.469 0.062 0.110 0.293 0.082 0.175 3.459 0.110 0.182 0.192 0.083 0.150 0.233 0.129 0.269 0.790</cell></row><row><cell cols="7">TS2VEC 0.041 0.068 0.883 0.114 0.163 0.574 0.088 0.104 2.854 0.296 0.407 0.366 0.078 0.119 0.216 0.135 0.221 0.174</cell></row><row><cell cols="7">Informer 0.045 0.073 0.876 0.070 0.105 0.330 0.078 0.109 3.747 0.318 0.391 0.419 0.079 0.125 0.215 0.133 0.242 0.219</cell></row><row><cell cols="7">Autoformer 0.048 0.077 1.065 0.065 0.092 0.388 0.150 0.205 3.620 0.116 0.161 0.177 0.124 0.179 0.307 0.159 0.265 0.224</cell></row><row><cell cols="7">FEDformer 0.045 0.979 0.062 0.089 0.380 0.091 0.131 4.205 0.117 0.155 0.212 0.104 0.160 0.271 0.139 0.246 0.205</cell></row><row><cell cols="7">Nstatformer 0.040 0.068 0.531 0.061 0.076 0.323 0.077 0.147 2.329 0.110 0.152 0.170 0.079 0.136 0.209 0.119 0.240 0.171</cell></row><row><cell cols="7">TCVAE 0.034 0.055 0.553 0.061 0.073 0.286 0.075 0.099 3.069 0.109 0.145 0.167 0.075 0.111 0.208 0.104 0.156 0.467</cell></row><row><cell cols="4">FEDformer, and Nstatformer) are good at capturing temporal</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">patterns with different ranges such as short-and long-range,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">wherein, TS2VEC, AdaRNN and Nstatformer utilize special</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">designs to tackle non-stationary time series. Additionally,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">StemGNN and AGCRN are based on graph neural networks.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">All baselines adopt the same normalization method as TCVAE.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(1) LSTNet [15]: It employs two key components including</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">a convolutional neural network and an LSTM with recurrent-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">skip connection in the time dimension;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>https://github.com/CSSEGISandData/COVID-19/tree/master 5 http://pems.dot.ca.gov/?dnode=Clearinghouse&amp;type=station 5min&amp; district id=7&amp;submit=Submit</p>6 https://github.com/liyaguang/DCRNN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III OVERALL</head><label>III</label><figDesc>ACCURACY ON PEMSD7(M) AND SOLAR-ENERGY WITH w = 24, h = {48, 72, 96, 180, 720} Compared with the temporal-aware models, the GNN-based models, i.e., StemGNN and AGCRN, can capture dynamic correlations explicitly among multiple time series and show better performance on several datasets.</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell>h = 48 MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE h = 72 h = 96 h = 180 h = 720</cell></row><row><cell></cell><cell cols="2">AdaRNN 0.097 0.137 0.281 0.089 0.129 0.278 0.092 0.135 0.260 0.112 0.157 0.322 0.147 0.213 0.503</cell></row><row><cell></cell><cell cols="2">StemGNN 0.098 0.150 0.320 0.106 0.162 0.364 0.117 0.175 0.410 0.140 0.205 0.507 0.142 0.204 0.501</cell></row><row><cell>PeMSD7(M)</cell><cell>AGCRN</cell><cell>0.088 0.157 0.240 0.090 0.156 0.246 0.086 0.152 0.223 0.094 0.162 0.264 0.119 0.159 0.262</cell></row><row><cell></cell><cell>TS2VEC</cell><cell>0.088 0.132 0.255 0.090 0.135 0.270 0.092 0.137 0.279 0.114 0.159 0.285 0.116 0.164 0.302</cell></row><row><cell></cell><cell>Informer</cell><cell>0.085 0.134 0.212 0.089 0.127 0.207 0.081 0.128 0.218 0.111 0.154 0.252 0.124 0.178 0.401</cell></row><row><cell></cell><cell cols="2">Autoformer 0.145 0.206 0.392 0.160 0.240 0.466 0.167 0.242 0.468 0.195 0.274 0.532 0.179 0.256 0.479</cell></row><row><cell></cell><cell cols="2">FEDformer 0.125 0.186 0.354 0.125 0.182 0.354 0.127 0.186 0.365 0.174 0.240 0.460 0.180 0.260 0.493</cell></row><row><cell></cell><cell cols="2">Nstatformer 0.093 0.165 0.279 0.106 0.182 0.323 0.115 0.194 0.352 0.133 0.214 0.411 0.128 0.208 0.398</cell></row><row><cell></cell><cell>TCVAE</cell><cell>0.087 0.126 0.189 0.079 0.124 0.196 0.105 0.144 0.214 0.110 0.150 0.218 0.111 0.154 0.238</cell></row><row><cell></cell><cell cols="2">AdaRNN 0.121 0.217 2.904 0.166 0.248 5.290 0.164 0.260 5.592 0.168 0.266 5.457 0.180 0.271 5.685</cell></row><row><cell></cell><cell cols="2">StemGNN 0.098 0.161 4.044 0.113 0.180 6.100 0.122 0.186 6.215 0.112 0.172 5.661 OOM OOM OOM</cell></row><row><cell>Solar-Energy</cell><cell>AGCRN</cell><cell>0.161 0.249 3.660 0.150 0.230 3.647 0.118 0.194 3.545 0.137 0.208 3.548 OOM OOM OOM</cell></row><row><cell></cell><cell>TS2VEC</cell><cell>0.111 0.153 4.587 0.130 0.172 5.647 0.128 0.169 5.540 0.114 0.159 4.894 0.125 0.173 5.506</cell></row><row><cell></cell><cell>Informer</cell><cell>0.079 0.115 4.043 0.081 0.123 4.284 0.099 0.134 5.571 0.089 0.137 5.612 0.092 0.141 5.804</cell></row><row><cell></cell><cell cols="2">Autoformer 0.221 0.282 5.097 0.205 0.260 5.137 0.208 0.270 3.859 0.206 0.275 4.481 0.200 0.249 4.704</cell></row><row><cell></cell><cell cols="2">FEDformer 0.147 0.207 4.923 0.141 0.183 4.464 0.129 0.174 4.728 0.122 0.180 5.064 0.135 0.190 5.643</cell></row><row><cell></cell><cell cols="2">Nstatformer 0.128 0.215 3.258 0.162 0.256 3.514 0.166 0.259 3.530 0.151 0.243 4.132 0.170 0.270 4.747</cell></row><row><cell></cell><cell>TCVAE</cell><cell>0.076 0.111 3.507 0.093 0.121 3.472 0.093 0.126 3.510 0.082 0.124 3.545 0.102 0.139 3.534</cell></row><row><cell cols="3">the benefit of Transformers or variants combining various</cell></row><row><cell cols="3">attention mechanisms in capturing temporal dependencies in</cell></row><row><cell cols="3">modeling sequential data.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF ABLATION STUDY ON ELECTRICITY AND PEMSD7(M) RMSE and MAPE respectively), verifying our design of using the Hawkes process to model time series as temporal point processes. A possible reason is that THA better captures the excitation induced by influential events while learning the distributed scores of every time step, which could also be observed from the visualization in Fig.7. The superiority of TCVAE over variant w/o GAM verifies the effect of temporal factors in guiding the generation of information flow. Due to the adaptive control of GAM, the valuable information generated by multiple heads is identified and the noise parts are greatly silenced. We investigate the sensitivity of TCVAE in terms of the time window size w ∈ {24, 48, 96}, the</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.40 0.42</cell><cell cols="2">with window size w=24 with window size w=48 with window size w=96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.38</cell><cell></cell></row><row><cell>Datasets</cell><cell>Electricity</cell><cell>PeMSD7(M)</cell><cell>MAPE</cell><cell>0.34 0.36</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="2">MAE RMSE MAPE MAE RMSE MAPE</cell><cell></cell><cell>0.32</cell><cell></cell></row><row><cell cols="5">w/o THA w/o GAM w/o FDA w/o CCNF w/o KL w/o Backcasting 0.062 0.089 0.065 0.093 0.064 0.092 0.067 0.095 0.064 0.092 0.065 0.093 TCVAE 0.061 0.073 components are integral. Specifically, w/o THA is a variant 0.294 0.076 0.116 0.287 0.077 0.115 0.212 0.309 0.076 0.116 0.219 0.289 0.076 0.112 0.213 0.287 0.090 0.124 0.221 0.288 0.075 0.115 0.217 0.286 0.075 0.111 0.208 where the Hawkes process is removed from TCVAE, and its temporal factor representation focuses on global-location rather than local-location contribution. Comparing TCVAE and w/o THA, we observe that temporal Hawkes attention significantly improves temporal attention (+3.74%, +12.91%, +3.44% corresponding to MAE, In addition, TCVAE shows significant improvement (+5.14%, +13.74%, +6.23%) over variant w/o FDA using a standard CVAE and removing CCNF Φ, indicating that breaking the specific distribution form and using temporal factors as distri-bution guides are of great significance. More finely-grained, w/o CCNF excludes the conditional continuous normalizing flow of the FDA module, thereby the TCVAE degenerates to a temporal factor-guided CVAE which directly matches the prior and posterior distributions with KL divergence. Comparing TCVAE and w/o CCNF, an important conclusion is that the flexible posterior approximated by CCNF surpasses the Gaussian posterior on both datasets. We argue that the intrinsic cause is that the simple Gaussian posterior can not estimate the true posterior well, using CCNF can help learn a more flexible density. And the improvements of TCVAE over variants w/o Backcasting and w/o KL prove that both temporal conditions and backcasting designs can introduce additional information and improve sequence representation capacity. Back to Table IV, the MAE, RMSE and MAPE of TCVAE are significantly improved (+11.41%, +16.00%, +3.12%) over w/o KL, showing that KL loss helps emphasize the constraints of temporal factors in the latent space during the training. We note that removing backcasting hardly leads to significant degradation in performance, probably because the backcasting (reconstruction) loss has the same or duplicated effect as the forecasting loss. 3) Parameter Sensitivity: 12 0.28 0.217 0.30</cell><cell>24</cell><cell>48</cell><cell>72 horizon</cell><cell>96</cell><cell>180 720</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The results report that 4-head attention achieves the best performance in predicting 12 and 48 timestamps. The forecasting quality drops off with too many heads n = 16. (4) Representation dimension d: The representation dimension of inputs and temporal factors in TCVAE highly determines the parameter effectiveness in temporal factor guidance and the capability of the learned representations. Fig.3 (d)shows the impact of various representation dimensions on Electricity. When the representation dimension is set to 512, TCVAE achieves the best performance. An excessively small or large representation dimension will result in poor performance. (5) Latent variable dimension k: As shown in TableV, similar to the representation dimension, k = 512 results in the best performance of TCVAE, confirming that a larger k improves the representation capability of TCVAE but may easily lead to the overfitting problem.4) Case Study: We deliver some case examples and analyze the pattern learned by several key components, i.e., FDA, GAM and THA of TCVAE, respectively. Traffic and pandemic data are highly dynamic and volatile, providing excellent scenarios for case studies.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">EFFECT OF DIFFERENT LATENT VARIABLE DIMENSIONS</cell></row><row><cell>Datasets</cell><cell></cell><cell>Electricity</cell><cell></cell><cell></cell><cell>METR-LA</cell><cell></cell></row><row><cell>k</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>256</cell><cell>512</cell><cell>1024</cell></row><row><cell>MAE</cell><cell cols="6">0.064 0.061 0.064 0.183 0.104 0.127</cell></row><row><cell>RMSE</cell><cell cols="6">0.091 0.073 0.090 0.229 0.156 0.213</cell></row><row><cell>MAPE</cell><cell cols="6">0.294 0.286 0.291 0.792 0.467 0.751</cell></row><row><cell cols="7">confirm that a larger λ imposes a stronger emphasis on VAE to</cell></row><row><cell cols="7">find the underlying factors leading to drift; however, a large</cell></row><row><cell cols="7">λ (e.g., λ = 1) may hurt the performance. (3) Number of</cell></row><row><cell>heads n:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI STATISTICS</head><label>VI</label><figDesc>OF MODEL COMPLEXITY IN TERMS OF TRAINING/TEST TIME COSTS AND MODEL PARAMETERS, WHERE SEC DENOTES SECONDS Models LSTNet AdaRNN DSANet STNorm StemGNN AGCRN TS2VEC Informer Autoformer FEDformer Nstatformer TCVAE w/o CCNF of all the comparative baselines, TCVAE and its variant w/o CCNF. From the experimental results, we can observe that: (1) TCVAE requires more parameters than the Transformer-and GNN-based methods, aiming to learn temporal dependencies to address the distribution drift in MTSs; (2) regarding training/test time, TCVAE runs faster than the GNN-based methods but slower than the Transformer-based methods; (3) StemGNN and AGCRN have higher time costs than the SOTA Transformer-based methods despite achieving competitive forecasting performance; (4) compared to the RNN-and CNN-based methods (except for DSANet), the Transformer-and GNN-based methods require much more parameters and training/test time costs but achieve much better multi-step forecasting performance; (5) the variant of TCVAE, w/o CCNF achieves both high efficiency and desirable forecasting performance compared with the baselines.</figDesc><table><row><cell cols="2">Parameters (M) 0.55</cell><cell>0.44</cell><cell>39.75</cell><cell>0.14</cell><cell>7.08</cell><cell>0.76</cell><cell>0.70</cell><cell>14.83</cell><cell>15.43</cell><cell>17.00</cell><cell>14.50</cell><cell>18.34</cell><cell>15.21</cell></row><row><cell>Training Time (Sec/Epoch)</cell><cell>9.54</cell><cell>15.52</cell><cell>90.99</cell><cell>9.26</cell><cell>67.16</cell><cell>71.24</cell><cell>8.77</cell><cell>26.83</cell><cell>32.88</cell><cell>35.45</cell><cell>34.43</cell><cell>65.4</cell><cell>37.29</cell></row><row><cell cols="2">Test Time (Sec) 4.94</cell><cell>13.60</cell><cell>25.60</cell><cell>1.06</cell><cell>24.31</cell><cell>17.89</cell><cell>4.06</cell><cell>3.96</cell><cell>11.21</cell><cell>8.78</cell><cell>10.39</cell><cell>16.04</cell><cell>8.65</cell></row></table><note><p>complexity</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by the <rs type="funder">Engineering Research Center of Integration and Application of Digital Learning Technology, Ministry of Education</rs> (<rs type="grantNumber">1331001</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62272048</rs>), and <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2019YFB1406302</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_De9feWF">
					<idno type="grant-number">1331001</idno>
				</org>
				<org type="funding" xml:id="_5aa5z8x">
					<idno type="grant-number">62272048</idno>
				</org>
				<org type="funding" xml:id="_dpwdNHm">
					<idno type="grant-number">2019YFB1406302</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional recurrent network for traffic forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards spatiotemporal aware traffic time series forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cirstea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2900" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stock price prediction via discovering multi-frequency trading patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stock market forecasting with superhigh dimensional time-series data using convlstm, trend sampling, and specialized data augmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">113704</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal attention-augmented bilinear network for financial time-series data analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kanniainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1407" to="1418" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tensorized LSTM with adaptive shared memory for learning trends in multivariate time series</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-based recommendation: a review of ontology-based recommender systems for e-learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural time-aware sequential recommendation by jointly modeling preference dynamics and explicit feature couplings</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5125" to="5137" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pay attention to evolution: Time series forecasting with deep graphevolution learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Spadon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brandoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F R</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5368" to="5384" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral temporal graph neural network for multivariate time-series forecasting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">COVID-19 modeling: A review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<idno>abs/2104.12556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vector autoregressions and cointegration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Working Paper Series, Macroeconomic Issues</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian intermittent demand forecasting for large inventories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4646" to="4654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian temporal factorization for multidimensional time series prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal pattern attention for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1421" to="1441" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lstm-msnet: Leveraging forecasts on sets of related time series with multiple seasonal patterns</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hewamalage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multivariate time series prediction based on temporal change information learning method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Copula variational lstm for high-dimensional cross-market multivariate dependence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4838" to="4847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2021</title>
		<meeting>NeurIPS, 2021</meeting>
		<imprint>
			<biblScope unit="page">430</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="27" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crossformer: Transformer utilizing crossdimension dependency for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mr-transformer: Multiresolution transformer for multivariate time series prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-iidness learning in behavioral and social data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1358" to="1370" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DDG-DA: data distribution generation for predictable concept drift adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4092" to="4100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hybrid spiking neurons embedded lstm network for multivariate time series learning under concept-drift environment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust online matching with user arrival distribution drift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Concept drift adaptation for CTR prediction in online advertising systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2204.05101</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-stationary transformers: Rethinking the stationarity in time series forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep adaptive input normalization for time series forecasting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kanniainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3760" to="3765" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Markovian rnn: An adaptive time series prediction network with hmm-based switching for nonstationary environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ilhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Karaahmetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dishts: A general paradigm for alleviating distribution shift in time series forecasting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7522" to="7529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ts2vec: Towards universal representation of time series</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8980" to="8987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adarnn: Adaptive learning and forecasting of time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to learn the future: Modeling concept drifts in time series prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2434" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning data streams with changing distributions and temporal dependency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3952" to="3965" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Doubleadapt: A meta-learning approach to incremental learning for stock trend forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3492" to="3503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eskénazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dialogwae: Multimodal response generation with conditional wasserstein auto-encoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR (Poster)</title>
		<meeting>ICLR (Poster)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Progressive open-domain response generation with multiple controllable attributes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3279" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Time series vector autoregression prediction of the ecological footprint based on energy parameters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jankovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amelio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910">1910.11800, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High-dimensional multivariate forecasting with low-rank gaussian copula processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bohlke-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Callot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Medico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6824" to="6834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">BiCMTS: Bidirectional coupled multivariate learning of irregular time series with missing values</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM, 2021</title>
		<meeting>CIKM, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3493" to="3497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multistep prediction of dynamic systems with recurrent neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mohajerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3370" to="3383" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dsanet: Dual self-attention network for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2129" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">St-norm: Spatial and temporal normalization for multi-variate time series forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Frequency-domain mlps are more effective learners in time series forecasting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph wavenet for deep spatial-temporal graph modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1907" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fouriergnn: Rethinking multivariate time series forecasting from a pure graph perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CATN: cross attentive tree-aware network for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4030" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Treernn: Topology-preserving deep graph embedding and learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7493" to="7499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning decomposed spatial relations for multi-variate timeseries modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7530" to="7538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep spectral copula mechanisms modeling coupled and volatile multivariate time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DSAA</title>
		<meeting>DSAA</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep coupling network for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2024-03">Mar. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal latent auto-encoder: A method for probabilistic multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2021</title>
		<meeting>AAAI, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="9117" to="9125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SOM-VAE: interpretable discrete representation learning on time series</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hüser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR (Poster)</title>
		<meeting>ICLR (Poster)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Timevae: A variational auto-encoder for multivariate time series generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beaver</surname></persName>
		</author>
		<idno>abs/2111.08095</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5530" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Variational autoencoders for top-k recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">B</forename><surname>Askari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szlichta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salehi-Abari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR, 2021</title>
		<meeting>SIGIR, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="2061" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fully unsupervised diversity denoising with convolutional variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A spatiotemporal deep learning approach for unsupervised anomaly detection in cloud systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1705" to="1719" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Constrained generation of semantically valid graphs via regularizing variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7113" to="7124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Styleflow: Attributeconditioned exploration of stylegan-generated images using conditional continuous normalizing flows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Videoflow: A conditional flow-based model for stochastic video generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Graphdf: A discrete flow model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="7192" to="7203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning flexibly distributional representation for low-quality 3d face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2021</title>
		<meeting>AAAI, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="3465" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multivariate probabilistic time series forecasting via conditioned normalizing flows</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Stock selection via spatiotemporal hypergraph attention network: A learning to rank approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A gated self-attention memory network for answer selection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP/IJCNLP</title>
		<meeting>EMNLP/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5952" to="5958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attentionrank: Unsupervised keyphrase extraction using self and cross attentions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP (1)</title>
		<meeting>EMNLP (1)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning informative representation for fairness-aware multivariate time-series forecasting: A group-based perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dynamic gaussian mixture based deep generative model for robust forecasting on sparse multivariate time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="651" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">His primary research interests include collaborative filtering, recommendation, learning to hash, and MTS analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has authored highquality papers in premier conferences and journals, including AAAI Conference on Artificial Intelligence (AAAI), International Joint Conferences on Artificial Intelligence (IJCAI), International World Wide Web Conference (TheWebConf), IEEE Transactions on Knowledge and Data Engineering (TKDE), IEEE Transactions on Neural Networks and Learning Systems (TNNLS), and ACM Transactions on Information Systems (TOIS)</title>
		<meeting><address><addrLine>Shanghai, China; Beijing, China; Sydney, NSW, Australia; Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2022. 2020. 2020</date>
		</imprint>
		<respStmt>
			<orgName>Beijing Institute of Technology ; jing Institute of Technology, Beijing, China and the University of Technology Sydney ; Research Fellow with Tongji University</orgName>
		</respStmt>
	</monogr>
	<note>Her current research interests focus on multivariate time series analysis and knowledge services. Kun Yi is currently working toward PhD degree with the Beijing Institute of Technology, China. His current research interests include multivariate time series forecasting, data science, and knowledge discovery</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">His research interests include natural language generation, social computing, cyber-physical-social systems, meteorological knowledge services, intelligent transportation, and artificial intelligence technology. He is the associate editor of IEEE Transactions on Computational Social Systems (IEEE TCSS) and academic editor of PeerJ Computer Science and Wireless Communications and Mobile Computing</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include informational retrieval, software architecture, digital libraries</title>
		<meeting><address><addrLine>Pittsburgh, PA, USA; Pittsburgh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996 to 1998. 1999 to 2004. 2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Data Science and Machine Intelligence Lab, University of Technology Sydney ; Beijing Institute of Technology, China, and the University of Technology Sydney, Australia ; University of Pittsburgh ; Adjunct Faculty Member with Carnegie Mellon University ; University of Pittsburgh ; Computer Science and Technology, Beijing Institute of Technology, Beijing</orgName>
		</respStmt>
	</monogr>
	<note>He is a Professor with the School of. and web-based learning. He received the International Business Machines Corporation (IBM) Faculty Innovation Award in 2005 and the New Century Excellent Talents in University of the Ministry of Education of China</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
