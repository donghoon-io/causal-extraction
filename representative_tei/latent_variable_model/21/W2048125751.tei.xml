<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Spatial and Temporal Priors for Articulated Human Tracking with Online Learning</title>
				<funder ref="#_rjRUUfU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_NpvV9AJ">
					<orgName type="full">Oklahoma Center for the Advancement of Science and Technology</orgName>
					<orgName type="abbreviated">OCAST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Oklahoma State University</orgName>
								<address>
									<settlement>Stillwater Oklahoma</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guoliang</forename><surname>Fan</surname></persName>
							<email>guoliang.fan@okstate.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Oklahoma State University</orgName>
								<address>
									<settlement>Stillwater Oklahoma</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Spatial and Temporal Priors for Articulated Human Tracking with Online Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study articulated human tracking by combining spatial and temporal priors in an integrated online learning and inference framework, where body parts can be localized and segmented simultaneously. The temporal prior is represented by the motion trajectory in a low dimensional latent space learned from tracking history, and it predicts the configuration of each body part for the next frame. The spatial prior is encoded by a star-structured graphical model and embedded in the temporal prior, and it can be constructed "on-the-fly" from the predicted pose and used to evaluate and correct the prediction by assembling part detection results. Both temporal and spatial priors can be online learned incrementally through the Back Constrained-Gaussian Process Latent Variable Model (BC-GPLVM) that involves a temporal sliding window for online learning. Experiments show that the proposed algorithm can achieve accurate and robust tracking results for different walking subjects with significant appearance and motion variability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Markerless human tracking and pose estimation have recently attracted intensive attention in the computer vision community because of their wide applications. Meanwhile they also remain to be one of the most challenging research issues largely due to the ambiguity, complexity and nonlinearities in observed video sequences. Using appropriate prior knowledge (such as motion or shape) would make the problem better defined and hopefully easier to tackle. The major advantages of using priors are to reduce the search space by taking advantages of various constraints and to ensure a plausible solution. Two commonly used priors are spatial and temporal priors, both of which play important roles for human detection and tracking, and have been well studied by many researchers in different context.</p><p>Spatial priors are usually defined on body parts and characterize the spatial configuration of a certain pose <ref type="bibr" target="#b2">[3]</ref>. One important question is how to make one spatial prior adaptable to a large number of pose variations. One straightforward extension is to train separate spatial priors for several typical poses <ref type="bibr" target="#b8">[9]</ref>. However, a spatial prior representation that can only handle a discrete pose variable has difficulty to characterize the smooth and continuous pose transition in a video sequence. On the other hand, temporal priors specify certain dynamic constraint of human motion <ref type="bibr" target="#b18">[19]</ref>, and they can ensure the temporal continuation across adjacent poses. Most temporal models do not impose a strong spatial constraint among body parts or treat each part independently for tracking <ref type="bibr" target="#b17">[18]</ref>. How to learn the two priors are also of great interest. There are two kinds of learning strategies. Off-line learning normally requires sufficient and/or diverse training samples and usually leads to the learned priors that favor the training data. Online learning can learn the priors "on-the-fly" that is more favorable and effective to deal with human motion with significant variability even from different activities <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this work, we propose a new framework for articulated human tracking that integrates both spatial and temporal priors and is supported by online learning. The idea of combining both priors has been well acknowledged and incorporated into most tracking algorithms where both priors are usually learned off-line and one prior often overshadows the other one during inference. In our work, the spatial prior is embedded in the temporal prior, and both priors are learned online from past tracking history in an incremental way. Specifically, the temporal prior can predict the pose for the next frame that induces a pose specific spatial prior. This spatial prior in return is used to evaluate and correct the pose prediction by assembling part-level detection. Our approach distinguishes itself from others in that it incorporates both online learned spatial and temporal priors in one integrated inference framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The biological vision model proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref> suggested two perception pathways in motion perception, the appearance pathway and the motion pathway. It was considered as one of the major breakthroughs in recent vision research <ref type="bibr" target="#b0">[1]</ref>, and motivates researchers to involve both spatial (appearance) and temporal (motion) priors in their tracking algorithms. Broadly speaking, related work can be classified into two groups: the temporal-prior dominated approaches and the spatial-prior dominated approaches.</p><p>In <ref type="bibr" target="#b8">[9]</ref>, a unified spatial-temporal articulated model was proposed for human tracking, where the pose is a discrete variable and defined as the hidden state of a hidden Markov model (HMM). The temporal prior is incorporated as a state transition matrix, and then the tracking task is formulated as a Bayesian estimation problem. In <ref type="bibr" target="#b14">[15]</ref>, a single pictorial structure graph model was extended into a dynamic Bayesian network (DBN), where the probabilistic relationships between joints at a given time instant as well as those over time can be learned from motion capture data. Then belief propagation is used as the inference engine to effectively incorporate the top-down spatial prior with bottomup part detection for articulated human tracking. Along the same venue, a temporal pictorial structure model was developed in <ref type="bibr" target="#b12">[13]</ref>, which mainly relies on appearance priors for human tracking. Above methods are considered as the spatial-prior dominated ones where the spatial prior plays a more important role and only weak temporal priors are involved for dealing with activity variation.</p><p>The human pose can be represented in a high dimensional (HD) parameter space where the distribution of plausible human poses is very sparse. Various non-linear dimensionality reduction (DR) techniques were proposed to explore the low-dimensional (LD) intrinsic structures for a compact pose representation. The Gaussian Process Latent Variable Model (GPLVM) <ref type="bibr" target="#b9">[10]</ref> is an effective DR technique that offers a smooth mapping from the LD latent space to the HD kinematic space. Several GPLVM variants were developed for temporal series analysis. For examples, Gaussian Processing Dynamic Models (GPDM) <ref type="bibr" target="#b18">[19]</ref> were specifically designed for human motion tracking by introducing a dynamic model on the latent variable that can be used to produce tracking hypothesis in a latent space <ref type="bibr" target="#b17">[18]</ref>. Back Constrained-GPLVM (BC-GPLVM) <ref type="bibr" target="#b10">[11]</ref> improves the continuity in the latent space by enforcing the local proximities in both the LD and HD spaces. Consequentially, BC-GPLVM produces a smooth motion trajectory in the latent space that can be used as a non-parametric dynamic model for human tracking <ref type="bibr" target="#b7">[8]</ref>. All of above DR methods focus on the exploration and exploitation of temporal priors of human motion, and they do not involve spatial (kinematic) priors explicitly. Therefore, we consider them as temporal-prior dominated approaches.</p><p>Motivated by previous research, we want to take advantage of the complementary nature of the above two methodologies. On the one hand, our work is similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> in the sense of how the spatial prior is represented. But we involve a strong temporal prior that can handle a continuous pose variable. On the other hand, our algorithm inherits some ideas from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> regarding how the temporal prior is developed for top-down prediction. However, we use a structured spatial prior that fuses part detection results to evaluate and correct the prediction. Moreover, we explore the synergy between the two priors in the context of online learning, which is inspired by the local mixed Gaussian process regressors proposed in <ref type="bibr" target="#b16">[17]</ref>. To the best of our knowledge, there is no prior research on how to combine spatial and temporal priors in an online learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background Review</head><p>We firstly briefly review the two major building blocks regarding the representations of temporal and spatial priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal Prior Modeling: GPLVM</head><p>The Gaussian process latent variable model (GPLVM) <ref type="bibr" target="#b9">[10]</ref> is an effective method to learn</p><formula xml:id="formula_0">X = {x i } N i=1 , x i ∈ R q in a LD latent space from Y = {y i } N i=1 , y i ∈ R D (D &gt;&gt; q) in</formula><p>a HD observation space, and it also provides a probabilistic mapping from X to Y . We refer the readers to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref> for more details. Assuming each observed data point, y i is generated through a noisy process from a latent variable x i ,</p><formula xml:id="formula_1">y i = f (x i ) + , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where ∼ N (0, β -1 I). Assuming a Gaussian distribution over functions f ∼ N (0, k(x i , x j )). The covariance k(x i , x j ) characterizes the nature of the functions. One widely used covariance function is</p><formula xml:id="formula_3">k(x i , x j ) = θ 1 e -θ 2 2 xi-xj 2 + θ 3 + β -1 δ i,j ,<label>(2)</label></formula><p>where the parameters are given by Φ = {θ 1 , θ 2 , θ 3 , β} and δ i,j is the Kronecker's delta function. The scalar k(x i , x j ) models the proximity between two points x i and x j . After GPLVM learning, given a new latent variable x * , the likelihood of the corresponding HD data point y * is:</p><formula xml:id="formula_4">p(y * |X, x * ) = N (y * |μ, σ 2 ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">μ = Y T K -1 X,X k X,x * ,<label>(4)</label></formula><p>where K X,X = {K i,j = k(x i , x j )}, and k X,x * is a column vector developed from computing the elements of the kernel matrix between the learn latent state data X and the new point x * . The variance that is then given below will increase as x * deviates from the training data X.</p><formula xml:id="formula_6">σ 2 = k(x * , x * ) -k T X,x * K -1 X,X k X,x * . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>To ensure a smooth trajectory in the latent state space for temporal series data, BC-GPLVM was proposed in <ref type="bibr" target="#b10">[11]</ref> that enforces local proximities in both the LD and HD spaces. In our work, BC-GPLVM is used to learn a compact LD representation of human motion in the latent space and a probabilistic reverse mapping from the LD latent space to the HD observation space. We adopt the BC-GPLVM to a local online learning strategy <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial prior: Star-structured Graphic Model</head><p>The pictorial structure based spatial prior representation has become an increasingly compelling approach for articulated human body tracking. Following <ref type="bibr" target="#b2">[3]</ref>, we represent the spatial prior for a pose by a star-structured graphical model Ψ. Let us regard pose y = (l 1 , ..., l d ) as a vector of 2D configuration (position and orientation) of d body parts. The joint distribution of d part configuration with respect to pose y can be written as the following:</p><formula xml:id="formula_8">p Ψ (y) = p Ψ (l 1 , ..., l d ) = p Ψ (l r ) k =r p Ψ (l k |l r ),<label>(6)</label></formula><p>where l k and l r are the configuration parameters for nonreference part k and the reference part r respectively. By assuming the conditional probability density functions for p Ψ (l k |l r ) following the Gaussian distribution. Then, for each non-reference part k, the conditional distribution of its configuration with respect to pose Ψ is defined below,</p><formula xml:id="formula_9">p Ψ (l k |l r ) = N (l k -l r |μ k , Σ k ).<label>(7)</label></formula><p>We can also assume a Gaussian distribution for p(l r ).</p><formula xml:id="formula_10">p Ψ (l r ) = N (l r |μ r , Σ r ).<label>(8)</label></formula><p>In an off-line learning framework, the parameters of the star model are often obtained by a maximum-likelihood estimator (MLE) from the labeled training data. For a test image, this spatial prior is used to assembly part detection results, or called map images, which indicate the confidence of the existence of each part at every pixel location. Edge histogram-based part detection was used in <ref type="bibr" target="#b2">[3]</ref> where a distance transform-based fast inference algorithm is also developed to assemble map images for detection and localization. In <ref type="bibr" target="#b1">[2]</ref>, a segmentation-based hypothesis-and-test method was proposed to produce more salient map images for part detection that improves the whole-part localization accuracy. We will make two extensions to the star modelbased spatial prior in this work. <ref type="bibr" target="#b0">(1)</ref> The spatial prior is time variant and is able to handle a continuous pose variable rather than a discrete pose variable in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref>. (2) The spatial prior is embedded in the temporal prior, and can be constructed "on-the-fly" based on the temporal prediction for every incoming frame rather than learned offline <ref type="bibr" target="#b2">[3]</ref>. Our algorithm is featured by the marriage of two powerful mathematical tools, BC-GPLVM and the star-structured graphical model, which is elaborated in the context of online learning. The synergy between the two priors is explored by embedding the spatial prior into the temporal prior and learning them together. The proposed algorithm involves four major steps as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Research</head><p>• Online learning and Pose Prediction: From past tracking history, we learn a smooth motion trajectory in the latent space via BC-GPLVM, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), which can be used as a non-parametric dynamic model to predict the next pose in the latent space by B-spine extrapolation.</p><p>• Spatial Prior Construction: Based on the prediction in the latent space, we can predict the next pose in the HD observation space via the LD-HD reverse mapping, as shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>). The predicted pose specifies the possible location of each body part in the next frame that enables the efficient local search of body parts. Also, a star model is constructed accordingly to represent the pose specific spatial prior.</p><p>• Local Part Detection: Based on the pose prediction, local part detection is performed for d (the number of body parts) body parts that results in d localized map images that are shown together in Fig. <ref type="figure" target="#fig_0">1(c</ref>).</p><p>• Pose Correction: The pose specific star model is used to assemble the part detection outputs and produce the final localization results for the whole body as well as body parts, as shown Fig. <ref type="figure" target="#fig_0">1(d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Problem Formulation</head><p>Given N image frames I 1:N , we want to estimate the pose y i = (l</p><formula xml:id="formula_11">(i) 1 , ..., l (i) d )</formula><p>for each frame where y i is a vector of 2D configuration (position and orientation) of d body parts at frame i. Let x i to be the latent state associated with y i . Let</p><formula xml:id="formula_12">P i = {p (i) 1 , ..., p (i)</formula><p>d } be the appearance models (e.g., an object template) of the d body parts. Given the pose estimation results of N previous frames, i.e., Y 1:N = {y 1 , ..., y N }, current body part models p N and next frame I N +1 , part localization (tracking) results can be obtained by maximize the posterior probability:</p><formula xml:id="formula_13">y * N +1 = arg max yN+1 P (y N +1 |I N +1 , P N , y 1:N ). (9)</formula><p>Generally, it is intractable to find the y * N +1 directly due to its HD nature. Hence we use a prediction-and-correction framework to attack this problem, as shown in Fig. <ref type="figure" target="#fig_4">2</ref>.    }. Second, it defines a pose specific spatial prior represented by a star model Ψ N +1 . Following the same idea as in <ref type="bibr" target="#b2">[3]</ref>, these map images can be assembled by Ψ N +1 in the form of star-structured graphical model. Then the tracking problem can be reformulated as maximizing the posterior probability:</p><formula xml:id="formula_14">y * N +1 = arg max yN+1 P (y N +1 |M (N +1) maps , Ψ N +1 ),<label>(10)</label></formula><p>where Ψ N +1 is the spatial prior represented in <ref type="bibr" target="#b5">(6)</ref>. The optimization problem of (10) can be efficiently solved using the fast inference algorithm developed in <ref type="bibr" target="#b2">[3]</ref>. Then y N +1 can be used to achieve the updated appearance models P N +1 based on I N +1 , and will be involved in the next step BC-GPLVM learning to predict y N +2 as the temporal sliding window moves forward one frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning and Inference</head><p>In this section, we detail the four major steps for learning and inference in our tracking algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Online Learning and Dynamic Prediction</head><p>In general, a pose can be represented by a HD vector that records joint angles or positions. Here we use a simple body representation with six body parts where each part is specified by the 2D position and orientation in the image domain. Given a pose series Y 1:N , BC-GPLVM can be used to learn the kernel parameters Φ = {θ 1 , θ 2 , θ 3 , β} and the latent variable series X 1:N . Different from off-line learning, we use a temporal sliding window to involve recently estimated poses for online (local) BC-GPLVM learning. The learned model is only used once for pose prediction in the next frame. As shown in Fig. <ref type="figure">3</ref>, although there is no explicit dynamic model involved in BC-GPLVM, the temporal constraint is well-reflected by the smooth motion trajectory in the LD latent space. We can extrapolate this motion trajectory to predict the pose in the next frame. Let x i = (a i , b i ) T , we can apply the B-spline regression process on the latent states X = {x i } N i=1 , as shown in Fig. <ref type="figure">3</ref>. The two obtained B-spline functions A(•) and B(•) will satisfy the following constraints:</p><formula xml:id="formula_15">A(i) ∼ = a i , B(i) ∼ = b i , .<label>(11)</label></formula><p>where i = 1, ..., N. Then through B-spline extrapolation a N +1 = A(N + 1) and b N +1 = B(N + 1), we can compute the predicted latent state for the next frame (N + 1) as x N +1 = (a N +1 , b N +1 ) T , (as indicated by the circled marker in Fig. <ref type="figure">3</ref>). From the predicted latent variable x N +1 , the associated pose in the image space y N +1 can be constructed through the reverse LD-HD mapping given in (4), and defined as:</p><formula xml:id="formula_16">ŷN+1 = Y T K -1 X,X k X,xN+1 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Constructing the Spatial Prior</head><p>The uncertainty of ŷN+1 is reflected by the variance defined in <ref type="bibr" target="#b4">(5)</ref>. So far, it is assumed that the configurations of d parts are independent, indicating a weak spatial constraint if only the temporal prior is used. In order to incorporate the spatial constraint among body parts, we need to construct the pose specific spatial prior represented by the star model from ŷN+1 . That means we need to estimate the conditional distributions between each non-reference part and the reference part, which can be derived straightforwardly from the Gaussian assumption of ŷN+1 . Therefore, the conditional distribution defined in (7) will become:</p><formula xml:id="formula_17">p Ψ (l k |l r ) = N (l k -l r |y k (N +1) -y r (N +1) , 2σ 2 • I), (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where σ 2 is given (5), y r (N +1) is the configuration of the reference part, and y k (N +1) is the relative configuration of nonreference part k with respect to the reference part. Similar to <ref type="bibr" target="#b7">(8)</ref>, the distribution of the reference part will become:</p><formula xml:id="formula_19">p Ψ (l r ) = N (l r |y r (N +1) , σ 2 • I)). (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>Strictly speaking, the covariance matrices of ( <ref type="formula" target="#formula_17">13</ref>) and ( <ref type="formula" target="#formula_19">14</ref>) need to be added with an additional error term to accommodate the prediction error in the latent space. In this work, the value of this prediction error term is set by experiment, and it is found that the tracking performance is not very sensitive to the choice of this value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two arms</head><p>Head Torso Two legs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Part Detection</head><p>The predicted pose ŷN+1 specifies the possible locations of d body parts that could define a search region for each part. Ideally, each search region should be isotropic and determined by <ref type="bibr" target="#b4">(5)</ref>. For simplicity, we use a square region of 21 × 21 for local part detection which is centered with the part position encoded in ŷN+1 . Similar to <ref type="bibr" target="#b1">[2]</ref>, we resort to a segmentation-based hypothesis-and-test method for part detection where off-line learned shape models are used, as shown in Fig. <ref type="figure" target="#fig_6">4</ref>. These shape models can be further represented by the coupled region-edge shape priors which are used to compute map images given an image represented by watershed cells. At each hypothesized location, the region prior is used to form a segmentation by merging watershed cells, while the edge prior is applied to evaluate the formed segmentation in terms of shape similarity and boundary smoothness.</p><p>To take advantage of localization results in the previous frame, we modify the evaluation criterion for computing the map images by replacing the boundary smoothness with the template matching score. Given a segmentation Z formed in a location, we represent the boundary of Z by Γ(Z), and the new evaluation score for Z is given by ρ M (Z) = -d(Γ(Z), M)) + ζSAD(I (N +1) , P N ), <ref type="bibr" target="#b14">(15)</ref> where the first term is the chamfer distance indicating the shape similarity between Γ(Z) and the off-line learned edge prior M, and the second term is the SAD (Sum of absolute differences) that reflects the degree of match between the online learned template  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Pose Correction</head><p>The part-based map images will be assembled by the online learned spatial prior through the star-structured graphical model. The same as in <ref type="bibr" target="#b2">[3]</ref>, given the set of map images M </p><p>Using Bayes law,</p><formula xml:id="formula_22">p Ψ (y (N +1) |M (N +1) maps ) ∝ p Ψ (M (N +1) maps |y (N +1) ), p Ψ (y (N +1) ).<label>(17)</label></formula><p>where</p><formula xml:id="formula_23">P Ψ (M (N +1) maps |y (N +1) ) = k=d k=1 M (N +1) k y k (N +1) ,</formula><p>where M</p><formula xml:id="formula_24">(N +1) k y k (N +1</formula><p>) is the value of the map image of part k in location y k (N +1) . Also p Ψ (y (N +1) ) can be evaluated through the learned star-structured graphical model defined in <ref type="bibr" target="#b5">(6)</ref>. A fast distance transform based inference method can be used to solve this problem efficiently <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Occlusion Handling</head><p>Occlusion handling is an important issue for articulated human tracking where some body parts may be invisible for some poses. In this work, we are interested self-occlusion, and our method is inspired by the multi-object tracking theory proposed in <ref type="bibr" target="#b3">[4]</ref>, where the notion of "object files" was developed to store episodic representations for real-world objects. Each object file contains the joint spatio-temporal information (such as appearance and motion) about a particular object in a scene. An "object file" is established for each body part being tracked that plays an important role for occlusion handling. The algorithm flow is shown in Fig. <ref type="figure" target="#fig_10">6</ref> where three occlusion-related issues are addressed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Occlusion detection:</head><p>The map images can be directly used for occlusion detection. Given a detection threshold S, if minM o (•) &gt; S, we can declare that part o is occluded. The position estimation of part o only depends on the prior knowledge encoded in the graphical model Ψ.</p><p>Prediction of an occluded part: Given part o that is declared in I N , its "object file" can be used for predicting its position in I N +1 as follows:</p><formula xml:id="formula_25">ŷo (N +1) = ŷr (N +1) + Δl o ,<label>(18)</label></formula><p>where ŷr (N +1) is the predicted position of the reference part (assuming the reference part is never occluded), and Δl o is the relative configuration between part o and the reference part r that can be retrieved from the "object file" of part o.</p><p>Learning and inference for occlusion: When part o is occluded in I N , we disable its map image by setting M o (.) = 0. Its configuration (given by the spatial prior) will be ignored in the online BC-GPLVM learning for I N +1 .</p><p>The above occlusion handling technique is simple yet effective, and could be extended to handle more sophisticated cases. The synergistic use of spatial and temporal priors allows the tracking algorithm to have more flexibility and capability of handling occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Results</head><p>The algorithm was implemented in C++, and the testbed is a PC computer with a Core 2 Duo E4700/2.6GHz CPU and 2GB RAM. We used the CMU Mobo database for algorithm validation <ref type="bibr" target="#b6">[7]</ref>, which includes video sequences captured from 25 individuals walking on a treadmill. Our algorithm was tested on four side-view sequences: vr03 7, vr08 7, vr21 7 and vr14 7, and we selected a walking cycle of 33 frames for each sequence. The first three have a normal walking style with different appearances (body shapes and colors), and the last one has an abnormal pattern where the subject touched his nose during walking. We have three specific experiments. The first one evaluates the tracking accuracy that is measured by the localization accuracy of each body part over a complete cycle. The second shows the capability of handling an unseen activity with an occluded body part. The last one presents body part segmentation that is part of online appearance learning.</p><p>The size of the temporal sliding window for training sample selection has to be determined in practice. We found that a number between 5-12 frames is acceptable. It takes about 200 ms for BC-GPLVM training (100 iterations) over 12 frames. Part-based evaluation is about 200 ms per frame that include the localization and segmentation of each body part. After the initialization on the first 5-12 frames, the proposed tracking algorithm can run at about 2 fps for the following frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Part localization</head><p>To evaluate the accuracy of articulated human tracking, we have manually obtained the ground-truth (position and orientation) of six body parts in all test video frames, i.e., the Head, Torso, Right arm (R arm), Left arm (L arm), Right leg (R leg), Left leg (L leg). Similar to <ref type="bibr" target="#b14">[15]</ref>, we evaluate the tracking accuracy by comparing the estimated position/orientation with the ground-truth ones. There are two competing algorithms both of which involve the same partbard spatial prior that were trained off-line for each typical pose (i.e., High-point, Contact and Passing). One is the 1fan method <ref type="bibr" target="#b2">[3]</ref> that involves the edge histogram for part detection, and the other is the hybrid approach <ref type="bibr" target="#b1">[2]</ref> where coupled edge-region shape priors are used for part detection. Both algorithms require several pose specific spatial priors that are learned off line, and they do not involve any temporal prior by treating each frame independently. Although the hybrid approach improves the localization accuracy for six body parts compared with the 1-fan method, it is timeconsuming due to the fact that segmentation is involved in part detection. The proposed algorithm is much more efficient and effective because the combined spatio-temporal priors are online learnt for each pose (a continuous variable) and dramatically narrows the local search for part detection. The comparative results are shown in Table <ref type="table" target="#tab_0">1</ref>, where the results from the 1-fan and hybrid approaches are the averages over three typical poses, while that of ours is the average over a complete walking cycle. Our algorithm demonstrates significant improvements over the two competing algorithms in tracking accuracy and efficiency. The localization performance is quite consistent over all three test videos. Some visual comparisons for two test videos can be seen from Fig. <ref type="figure" target="#fig_11">7</ref>, where we can see obvious advantages of our method even under occlusion. The comparison above shows the advantage of using the combined spatio-temporal priors over one with the spatial prior alone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Special Case Handling</head><p>One major advantage of online learning is the ability to handle unseen motion patterns and occlusion. For example, Sequence vr14 7 shows an abnormal walking pattern that is hard to cope with for a tracker that relies on off-line learning. Moreover, one arm is occluded most of time during the walking cycle. The two competing algorithms fails in this case since the spatial prior learned off line is not able to deal with this abnormality. However, the proposed algorithm can accurately detect all visible body parts, regardless of the unusual motion pattern and one occluded arm. Some tracking results are shown in Fig. <ref type="figure" target="#fig_12">8</ref>. It is shown that the proposed tracker can effectively localize the arm that deviates from its normal motion pattern, proving the usefulness of online learning. Although the majority of one arm is occluded in most frames, the tracking results of other body parts are not affected, showing the effectiveness of occlusion handling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Part Segmentation</head><p>The proposed algorithm can also online learn part-based appearance models from frame to frame during tracking. Part detection in this work is similar to the one used in <ref type="bibr" target="#b1">[2]</ref>. After we localize the whole body (we use the Torso as the reference part) in the current frame, we can localize and segment each body part correspondingly. The segmented body parts can be used to speed-up part detection by providing an object template that can be updated sequentially by the tracker. Some examples of online learned part appearances are shown in Fig. <ref type="figure" target="#fig_13">9</ref>. Although each body parts exhibit significant shape/color variability among four test sequences, the segmentation results are quite accurate and robust. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Discussion</head><p>Compared with the methods using holistic appearance models (e.g., <ref type="bibr" target="#b11">[12]</ref>), this work is focused on articulated body tracking that is able to detect, track and segment body parts. Moreover, the major advantage of using part-based spatial and temporal priors is the ability of occlusion handling during tracking. Also, the part-based representation makes model learning more flexible that can involve both online and offline learning and be extended to handle some unseen activities. One may wonder how about the tracking results of using the temporal prior only. It was shown in our experiments, when the background is clean and no occlusion, the temporal prior alone could be sufficient given reasonable part-based appearance models. However, when the background is cluttered (with many false alarms) or occlusion occurs, the contribution from the spatial prior cannot be neglected. Other temporal prior models could be possible, such as Kalman filters. However, the major challenge will be the high dimension of the state space considering the number of body parts and the possible configuration of each body part, while BC-GPLVM is able to provide a LD latent space for effective state prediction via extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have proposed a new algorithm for articulated human tracking that combines both the spatial and temporal priors in an online learning framework. Compared with prior efforts, we want to fully take advantage of both the spatial and temporal priors in a balanced way in order to optimize the tracking performance. Although there might be certain redundancy between the two priors, the synergistic use of them greatly enhances the robustness and flexibility of the tracker, especially in a challenging environment with complex background or occlusion. The online learning mechanism makes the proposed algorithm effective to track subjects with significant appearance and motion variability. All of these makes our algorithm a promising tool to support video-based human motion analysis in a general setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The algorithm flow. (a) Online learning and dynamic prediction in the latent space. (b) Pose prediction in the observation space and construction of the star model. (c) Local part detection according to the prediction. (d) Localization by assembling the part detection results via the star model.</figDesc><graphic coords="3,480.07,122.36,63.01,108.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Problem formulation by a graphical model. Assume we can learn a smooth motion trajectory X 1:N = {x 1 , ..., x N } in the latent space via BC-GPLVM based on past tracking history Y 1:N . We can predict the next pose in the latent space first, xN+1 , which can be converted to ŷN+1 in the image space. ŷN+1 has two implications. First, it can be used to produce d localized map images, M (N +1) maps = {M (N +1) 1 , ..., M (N +1) d</figDesc><graphic coords="4,320.15,347.54,201.71,124.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 +Figure 3 .</head><label>13</label><figDesc>Figure 3. An example of BC-GPLVM online learning and dynamic prediction via B-spline extrapolation in the 2D latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Off-line leaned part shape models where the average orientation of each part is also given.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>P N and I (N +1) . ζ balances the relative importance between the off-line and online learned part priors. Some part detection examples are shown in Fig. 5(b) where a dark pixel value indicates a high possibility of the existence of a part. The computation of these map images can be very efficient due to local part detection constrained by ŷN+1 , instead of the full search used in [3, 2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Part detection results. (a) An image with predicted local search regions. (b) The localized map images for six body parts.</figDesc><graphic coords="5,370.55,311.17,59.65,84.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>) k |k = 1, ..., d}, the optimal y (N +1) can be obtained by re-writing (10) as, y * (N +1) = arg max y (N +1) p Ψ (y (N +1) |M (N +1) maps ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The flow of tracking and occlusion handling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The comparison between the hybrid approach [2] (top) and the proposed one (bottom) for three frames from two test videos.</figDesc><graphic coords="7,304.50,156.05,68.47,78.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Tracking results for an abnormal activity.</figDesc><graphic coords="7,318.96,342.33,216.00,213.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Online learned part appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison of localization error (in pixel).</figDesc><table><row><cell>Methods</cell><cell cols="6">Head Torso R arm L arm R leg L leg</cell></row><row><cell>1-fan [3]</cell><cell>5.3</cell><cell>6.8</cell><cell>12.2</cell><cell>11.1</cell><cell>12.7</cell><cell>12.8</cell></row><row><cell>hybrid [2]</cell><cell>5.9</cell><cell>6.0</cell><cell>9.8</cell><cell>8.6</cell><cell>4.8</cell><cell>5.6</cell></row><row><cell>vr03 7</cell><cell>0.6</cell><cell>0.9</cell><cell>1.3</cell><cell>1.8</cell><cell>1.3</cell><cell>1.0</cell></row><row><cell>vr08 7</cell><cell>0.6</cell><cell>1.8</cell><cell>0.9</cell><cell>4.8</cell><cell>1.7</cell><cell>1.1</cell></row><row><cell>vr21 7</cell><cell>0.9</cell><cell>1.0</cell><cell>1.5</cell><cell>7.4</cell><cell>2.7</cell><cell>2.7</cell></row><row><cell>Average</cell><cell>0.7</cell><cell>1.3</cell><cell>1.2</cell><cell>4.7</cell><cell>1.9</cell><cell>1.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported in part by the <rs type="funder">National Science Foundation (NSF)</rs> under Grant <rs type="grantNumber">IIS-0347613</rs>, and an <rs type="grantName">Oklahoma NASA EPSCoR Research Initiation Grant</rs> in 2009, and an <rs type="grantName">OHRS award</rs> (<rs type="grantNumber">HR09-030</rs>) from the <rs type="funder">Oklahoma Center for the Advancement of Science and Technology (OCAST)</rs>. The authors also thank the anonymous reviewers for their comments and suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rjRUUfU">
					<idno type="grant-number">IIS-0347613</idno>
					<orgName type="grant-name">Oklahoma NASA EPSCoR Research Initiation Grant</orgName>
				</org>
				<org type="funding" xml:id="_NpvV9AJ">
					<idno type="grant-number">HR09-030</idno>
					<orgName type="grant-name">OHRS award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision: The world through picket fences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hybrid body represenation for integrated pose recognition, localization and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial priors for part-based recognition using statistical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The reviewing of object files: object specific integration of information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Terisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="175" to="219" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for human pose estimation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed hierarchical processing in primate cerebral cortex</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E D C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The CMU motion of body (MoBo) database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>CMU-RI-TR-01-18</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time body tracking using a gaussian process latent variable model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bromiley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified spatio-temporal articulated model for tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic non-linear principal component analysis with gaussian process latent variable models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1783C" to="1816" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local distance preservation in the gp-lvm through back constraints</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling view and posture manifolds for tracking</title>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking people by learning their appearance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using gaussian process annealing particle filter for 3d human tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Raskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rudzsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking loose-limbed people</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isardy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Two cortical visual systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishkin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="549" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse probabilistic regression for activityindependent human pose inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
