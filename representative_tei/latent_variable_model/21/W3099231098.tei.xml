<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation</title>
				<funder>
					<orgName type="full">Adobe Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
							<email>kangminyoo@europa.snu.ac.kr</email>
						</author>
						<author>
							<persName><surname>Yoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanbit</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
							<email>dernonco@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
							<email>bui@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Chang</surname></persName>
							<email>wachang@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
							<email>sglee@europa.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers' robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogsdialog response generation and user simulation, where our model outperforms previous strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data augmentation, a technique that augments the training set with label-preserving synthetic samples, is commonly employed in modern machine learning approaches. It has been used extensively in visual learning pipelines <ref type="bibr" target="#b42">(Shorten and Khoshgoftaar, 2019)</ref> but less frequently for NLP tasks due to the lack of well-established techniques in the area. While some notable work exists in text classification <ref type="bibr" target="#b52">(Zhang et al., 2015)</ref>, spoken language understanding <ref type="bibr">(Yoo et al., 2019)</ref>, and machine translation <ref type="bibr" target="#b12">(Fadaee et al., 2017)</ref>, we still lack the full understanding of utilizing generative models for text augmentation.</p><p>Ideally, a data augmentation technique for supervised tasks must synthesize distribution-preserving and sufficiently realistic samples. Current approaches for data augmentation in NLP tasks mostly revolve around thesaurus data augmentation <ref type="bibr" target="#b52">(Zhang et al., 2015)</ref>, in which words that belong to the same semantic role are substituted with one another using a preconstructed lexicon, and noisy data augmentation <ref type="bibr" target="#b47">(Wei and Zou, 2019)</ref> where random editing operations create perturbations in the language space. Thesaurus data augmentation requires a set of handcrafted semantic dictionaries, which are costly to build and maintain, whereas noisy data augmentation does not synthesize sufficiently realistic samples. The recent trend <ref type="bibr" target="#b23">(Hu et al., 2017;</ref><ref type="bibr">Yoo et al., 2019;</ref><ref type="bibr" target="#b41">Shin et al., 2019)</ref> gravitates towards generative data augmentation (GDA), a class of techniques that leverage deep generative models such as VAEs to delegate the automatic discovery of novel class-preserving samples to machine learning. In this work, we explore GDA in the context of dialog modeling and contextual understanding.</p><p>Goal-oriented dialogs occur between a user and a system that communicates verbally to accomplish the user's goals (Table <ref type="table" target="#tab_7">6</ref>). However, because the user's goals and the system's possible actions are not transparent to each other, both parties must rely on verbal communications to infer and take appropriate actions to resolve the goals. Dialog state tracker is a core component of such systems, enabling it to track the dialog's latest status <ref type="bibr" target="#b17">(Henderson et al., 2014)</ref>. A dialog state typically consists of inform and request types of slot values.</p><p>For example, a user may verbally refer to a previously mentioned food type as the preferred one -e.g., Asian (inform(food=asian)). Given the user utterance and historical turns, the state tracker must infer the user's current goals. As such, we can view dialog state tracking as a sparse sequential multi-class classification problem. Modeling goal-oriented dialogs for GDA requires a novel approach that simultaneously solves state tracking, user simulation <ref type="bibr" target="#b38">(Schatzmann et al., 2007)</ref>, and utterance generation.</p><p>Various deep models exist for modeling dialogs. The Markov approach <ref type="bibr" target="#b40">(Serban et al., 2017)</ref> employs a sequence-to-sequence variational autoencoder (VAE) <ref type="bibr" target="#b27">(Kingma and Welling, 2013)</ref> structure to predict the next utterance given a deterministic context representation, while the holistic approach <ref type="bibr" target="#b35">(Park et al., 2018)</ref> utilizes a set of global latent variables to encode the entire dialog, improving the awareness in general dialog structures. However, current approaches are limited to linguistic features. Recently, <ref type="bibr" target="#b2">Bak and Oh (2019)</ref> proposed a hierarchical VAE structure that incorporates the speaker's information, but we have yet to explore a universal approach for encompassing fundamental aspects of goal-oriented dialogs. Such a unified model capable of disentangling latents into specific dialog aspects can increase the modeling efficiency and enable interesting extensions based on the finegrained controllability.</p><p>This paper proposes a novel multi-level hierarchical and recurrent VAE structure called Variational Hierarchical Dialog Autoencoder (VHDA). Our model enables modeling all aspects (speaker information, goals, dialog acts, utterances, and general dialog flow) of goal-oriented dialogs in a disentangled manner by assigning latents to each aspect. However, complex and autoregressive VAEs are known to suffer from the risk of inference collapse <ref type="bibr" target="#b9">(Cremer et al., 2018)</ref>, in which the model converges to a local optimum where the generator network neglects the latents, reducing the generation controllability. To mitigate the issue, we devise two simple but effective training strategies.</p><p>Our contributions are summarized as follows.</p><p>1. We propose a novel deep latent model for modeling dialog utterances and their relationships with the goal-oriented annotations. We show that the strong level of coherence and accuracy displayed by the model allows it to be used for augmenting dialog state tracking datasets.</p><p>2. Leveraging the model's generation capabilities, we show that generative data augmentation is attainable even for the complex dialogrelated tasks that pertain to both hierarchical and sequential annotations.</p><p>3. We propose simple but effective training policies for our VAE-based model, which have applications in other similar VAE structures.</p><p>The code for reproducing this paper is available at github<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Dialog State Tracking. Dialog state tracking (DST) predicts the user's current goals and dialog acts, given the dialog context. Historically, DST models have gradually evolved from hand-crafted finite-state automata and multi-stage models <ref type="bibr" target="#b11">(Dybkjaer and Minker, 2008;</ref><ref type="bibr" target="#b43">Thomson and Young, 2010;</ref><ref type="bibr" target="#b46">Wang and Lemon, 2013)</ref> to end-to-end models that directly predict dialog states from dialog features <ref type="bibr" target="#b55">(Zilka and Jurcicek, 2015;</ref><ref type="bibr" target="#b33">Mrkšić et al., 2017;</ref><ref type="bibr" target="#b54">Zhong et al., 2018;</ref><ref type="bibr" target="#b34">Nouri and Hosseini-Asl, 2018)</ref>.</p><p>Among the proposed models, Neural Belief Tracker (NBT) <ref type="bibr" target="#b33">(Mrkšić et al., 2017)</ref> decreases reliance on handcrafted semantic dictionaries by reformulating the classification problem. Globallocally Self-attentive Dialog tracker (GLAD) <ref type="bibr" target="#b54">(Zhong et al., 2018)</ref> introduces global modules for sharing parameters across slots and local modules, allowing the learning of slot-specific feature representations. Globally-Conditioned Encoder (GCE) <ref type="bibr" target="#b34">(Nouri and Hosseini-Asl, 2018)</ref> improves further by forgoing the separation of global and local modules, allowing the unified module to take slot embeddings for distinction. Recently, dialog state trackers based on pre-trained language models have demonstrated their strong performance in many DST tasks <ref type="bibr" target="#b49">(Wu et al., 2019;</ref><ref type="bibr" target="#b24">Kim et al., 2019;</ref><ref type="bibr" target="#b21">Hosseini-Asl et al., 2020)</ref>. While the utilization of large-scale pre-trained language models is not within our scope, we wish to explore further concerning the recent advances in the area. Conversation Modeling. While the previous approaches for hierarchical dialog modeling relate to the Markov assumption <ref type="bibr" target="#b40">(Serban et al., 2017)</ref>, recent approaches have geared towards utilizing global latent variables for representing the holistic dialog structure <ref type="bibr" target="#b35">(Park et al., 2018;</ref><ref type="bibr" target="#b13">Gu et al., 2018;</ref><ref type="bibr" target="#b2">Bak and Oh, 2019)</ref>, which helps in preserving long-term dependencies and total semantics. In this work, we employ global latent variables to maximize the effectiveness in preserving dialog semantics for data augmentation. Data Augmentation. Transformation-based data augmentation is popular in vision learning <ref type="bibr" target="#b42">(Shorten and Khoshgoftaar, 2019)</ref> and speech signal processing <ref type="bibr" target="#b28">(Ko et al., 2015)</ref>, while thesaurus and noisy data augmentation techniques are more common for text. <ref type="bibr" target="#b52">(Zhang et al., 2015;</ref><ref type="bibr" target="#b47">Wei and Zou, 2019)</ref>. Recently, generative data augmentation (GDA), augmenting data gather from samples generated from fine-tuned deep generative models, have gained traction in several NLP tasks <ref type="bibr" target="#b23">(Hu et al., 2017;</ref><ref type="bibr" target="#b22">Hou et al., 2018;</ref><ref type="bibr">Yoo et al., 2019;</ref><ref type="bibr" target="#b41">Shin et al., 2019)</ref>. GDA can be seen as a form of unsupervised data augmentation, delegating the automatic discovery of novel data to machine learning without injecting external knowledge or data sources. While most works utilize VAE for the generative model, some works achieved a similar effect without employing variational inference <ref type="bibr" target="#b29">(Kurata et al., 2016;</ref><ref type="bibr" target="#b22">Hou et al., 2018)</ref>. In contrast to unsupervised data augmentation, another line of work has explored self-supervision mechanisms to fine-tune the generators for specific tasks <ref type="bibr" target="#b45">(Tran et al., 2017;</ref><ref type="bibr" target="#b0">Antoniou et al., 2017;</ref><ref type="bibr" target="#b10">Cubuk et al., 2018)</ref>. Recent work proposed a reinforcement learning-based noisy data augmentation framework for state tracking <ref type="bibr" target="#b50">(Yin et al., 2019)</ref>. Our work belongs to the family of unsupervised GDA, which can incorporate selfsupervision mechanisms. We wish to explore further in this regard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>This section describes VHDA, our latent variable model for generating goal-oriented dialog datasets. We first introduce a set of notations for describing core concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>A dialog dataset D is a set of N i.i.d samples {c 1 , . . . , c N }, where each c is a sequence of turns (v 1 , . . . , v T ). Each goal-oriented dialog turn v is a tuple of speaker information r, the speaker's goals g, dialog state s, and the speaker's utterance u: v = (r, g, s, u). Each utterance u is a sequence of words (w 1 , . . . , w |u| ). Goals g or a dialog state s is defined as a set of the smallest unit of dialog act specification a <ref type="bibr" target="#b17">(Henderson et al., 2014)</ref>, which is a tuple of dialog act, slot and value defined over the space of T , S , and V : g = a 1 , . . . , a |g| , s = a 1 , . . . , a |s| , where</p><formula xml:id="formula_0">h 1 z (r) 1 z (g) 1 z (s) 1 z (u) 1 r 1 g 1 s 1 u 1 h 2 z (r) 2 z (g) 2 z (s) 2 z (u) 2 r 2 g 2 s 2 u 2 h 3 z (r) 3 z (g) 3 z (s) 3 z (u) 3 r 3 g 3 s 3 u 3 z (c) t = 1 t = 2 t = 3</formula><formula xml:id="formula_1">a i ∈ A = (T , S ,V ).</formula><p>A dialog act specification is represented as &lt;act&gt;(&lt;slot&gt;=&lt;value&gt;).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VHCR</head><p>Given a conversation c, Variational Hierarchical Conversational RNN (VHCR) <ref type="bibr" target="#b35">(Park et al., 2018)</ref> models the holistic features of the conversation and the individual utterances u using a hierarchical and recurrent VAE model. The model introduces global-level latent variables z (c) for encoding the high-level dialog structure and, at each turn t, locallevel latent variables z (u) t responsible for encoding and generating the utterance at turn t. The local latent variables z (u) conditionally depends on z (c) and previous observations, forming a hierarchical structure with the global latents. Furthermore, hidden variables h t , which are conditionally dependent on the global information and the hidden variables from the previous step h t-1 , facilitate the latent inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VHDA</head><p>We propose Variational Hierarchical Dialog Autoencoder (VHDA) to generate dialogs and their underlying dialog annotations simultaneously (Figure <ref type="figure" target="#fig_0">1</ref>). Like VHCR, we employ a hierarchical VAE structure to capture holistic dialog semantics using the conversation latent variables z (c) . Our model incorporates full dialog features using turn-level latents z (r) (speaker), z (g) (goal), z (s) (dialog state), and z (u) (utterance), motivated by speech act theory <ref type="bibr" target="#b39">(Searle et al., 1980)</ref>. Specifically, at a given dialog turn, the information about the speaker, the speaker's goals, the speaker's turn-level dialog acts, and the utterance all cumulatively determine one after the other in that order.</p><p>VHDA consists of multiple encoder and decoder modules, each responsible for encoding or generating a particular dialog feature. The encoders share the identical sequence-encoding architecture described as follows. Sequence Encoder Architecture. Given a sequence of variable number of elements X = [x 1 ; . . . ; x n ] ∈ R n×d , where n is the number of elements, the goal of a sequence encoder is to extract a fixed-size representation h ∈ R d , where d is the dimensionality of the hidden representation. For our implementation, we employ the self-attention mechanism over hidden outputs of bidirectional LSTM <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber, 1997)</ref> cells produced from the input sequence. We also allow the attention mechanism to be optionally queried by Q, enabling the sequence to depend on external conditions, such as using the dialog context to attend over an utterance:</p><formula xml:id="formula_2">H = [ ----→ LSTM(X); ← ---- LSTM(X)] ∈ R n×d a = softmax([H; Q]w + b) ∈ R n h = H a ∈ R d .</formula><p>Here, Q ∈ R n×dq is a collection of query vectors of size d q where each vector corresponds to one element in the sequence; w ∈ R d+dq and b ∈ R are learnable parameters. We encapsulate the above operations with the following notation:</p><formula xml:id="formula_3">E : R n×d (×R n×dq ) → R d .</formula><p>Our model utilizes the E structure for encoding dialog features of variable lengths.</p><p>Encoder Networks. Based on the E architecture, feature encoders are responsible for encoding dialog features from their respective raw feature spaces to hidden representations. For goals and turn states, the encoding consists of two steps. Initially, the multi-purpose dialog act encoder E (a) processes each dialog act triple of the goals a (g) ∈ g and turn states a (s) ∈ s into a fixed-size representation h (a) ∈ R d (a) . The encoder treats the dialog act triples as sequences of tokens. Subsequently, the goal encoder and the turn state encoder process those dialog act representations to produce goal representations and turn state representations, respectively:</p><formula xml:id="formula_4">h (g) = E (g) ([E (a) (a (g) 1 ); . . . ; E (a) (a (g) |g| )]) h (s) = E (s) ([E (a) (a (s) 1 ); . . . ; E (a) (a (s) |s| )]).</formula><p>Note that, as the model is sensitive to the order of the dialog acts, we randomize the order during training to prevent overfitting. The utterances are encoded using the utterance encoder from the word embeddings space:</p><formula xml:id="formula_5">h (u) = E (u) ([w 1 ; . . . ; w |u| ]),</formula><p>while the entire conversation is encoded by the conversation encoder from the encoded utterance vectors:</p><formula xml:id="formula_6">h (c) = E (c) ([h (u) 1 ; . . . ; h (u) T ]</formula><p>). All sequence encoders mentioned above depend on the global latent variables z (c) via the query vector. For the speaker information, we use the speaker embedding matrix W (r) ∈ R n (r) ×d (r) to encode the speaker vectors h (r) , where n (r) is the number of participants and d (r) is the embedding size. Main Architecture. At the top level, our architecture consists of five E encoders, a context encoder C , and four types of decoder D . The context encoder C is different from the other encoders, as it does not utilize the bidirectional E architecture but a uni-directional LSTM cell. The four decoders D (r) , D (g) , D (s) , and D (u) generate respective dialog features.</p><p>C is responsible for keeping track of the dialog context by encoding all features generated so far. The context vector at t (h t ) is updated using the historical information from the previous step:</p><formula xml:id="formula_7">v t-1 = [h (r) t-1 ; h (g) t-1 ; h (s) t-1 ; h (u) t-1 ] h t = C (h t-1 , v t-1 )</formula><p>where v t is represents all features at the step t.</p><p>VHDA uses the context information to successively generate turn-level latent variables using a series of generator networks:</p><formula xml:id="formula_8">p θ (z (r) t |h t , z (c) ) = N (µ (r) t , σ (r) t I) p θ (z (g) t |h t , z (c) , z (r) t ) = N (µ (g) t , σ (g) t I) p θ (z (s) t |h t , z (c) , z (r) t , z (g) t ) = N (µ (s) t , σ (s) t I) p θ (z (u) t |h t , z (c) , z (r) t , z (g) t , z (s) t ) = N (µ (u) t , σ (u) t I)</formula><p>where all latents are assumed to be Gaussian. In addition, we assume the standard Gaussian for the global latents: p(z (c) ) = N (0, I). We implemented the Gaussian distribution encoders (µ and σ) using fully-connected networks f . We also apply softplus on the output of the networks to infer the variance of the distributions. Employing the reparameterization trick (Kingma and Welling, 2013) allows standard backpropagation during training of our model.</p><p>Approximate Posterior Networks. We use a separate set of parameters φ and encoders to approximate the posterior distributions of latent variables from the evidence. In particular, the model infers the global latents z (c) using the conversation encoder E (c) solely from the linguistic features:</p><formula xml:id="formula_9">q φ (z (c) |h (u) 1 , . . . , h (u) T ) = N (µ (c) , σ (c) I).</formula><p>Similarly, the approximate posterior distributions of all turn-level latent variables are estimated from the evidence in cascade, while maintaining the global conditioning:</p><formula xml:id="formula_10">q φ (z (r) t |h t , z (c) , h (r) t ) = N (µ (r ) t , σ (r ) t I) q φ (z (g) t |h t , z (c) , z (r) t , h (g) t ) = N (µ (g ) t , σ (g ) t I) q φ (z (s) t |h t , . . . , z (g) t , h (s) t ) = N (µ (s ) t , σ (s ) t I) q φ (z (u) t |h t , . . . , z (s) t , h (u) t ) = N (µ (u ) t , σ (u ) t I),</formula><p>where all Gaussian parameters are estimated using fully-connected layers, parameterized by φ.</p><p>Realization Networks. A series of generator networks successively decodes dialog features from their respective latent spaces to realize the surface forms:</p><formula xml:id="formula_11">p θ (r t |h t , z (c) , z (r) t ) = D (r) θ (h t , z (c) , z (r) t ) p θ (g t |h t , . . . , z (g) t ) = D (g) θ (h t , . . . , z (g) t ) p θ (s t |h t , . . . , z (s) t ) = D (s) θ (h t , . . . , z (s) t ) p θ (u t |h t , . . . , z (u) t ) = D (u) θ (h t , . . . , z (u) t ).</formula><p>The utterance decoder D (u) is implemented using the LSTM cell. To alleviate sparseness in goals and turn-level dialog acts, we formulate the classification problem as a set of binary classification problems <ref type="bibr" target="#b33">(Mrkšić et al., 2017)</ref>. Specifically, given a candidate dialog act a,</p><formula xml:id="formula_12">p θ (a ∈ s t |v &lt;t , . . .) = σ(o (s) t • E (a) (a))</formula><p>where σ is the sigmoid function and o</p><formula xml:id="formula_13">(s) t ∈ R d (a)</formula><p>is the output of a feedforward network parameterized by θ that predicts the dialog act specification embeddings. Goals are predicted analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective</head><p>Given all the latent variables z in our model, we optimize the evidence lower-bound (ELBO) of the goal-oriented dialog samples c:</p><formula xml:id="formula_14">L VHDA =E q φ [log p θ (c | z)] -D KL (q φ (z | c) p(z)).</formula><p>(1)</p><p>The reconstruction term of Equation 5 can be factorized into posterior probabilities in the realization networks. Similarly, the KL-divergence term can be factorized and reformulated in approximate posterior networks and conditional priors based on the graphical structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Minimizing Inference Collapse</head><p>Inference collapse is a relatively common phenomenon among autoregressive VAE structures <ref type="bibr" target="#b53">(Zhao et al., 2017)</ref>. The hierarchical and recurrent nature of our model makes it especially vulnerable. The standard treatment for alleviating the inference collapse problem include (1) annealing the KL-divergence term weight during the initial training stage and (2) employing word dropouts on the decoder inputs <ref type="bibr" target="#b5">(Bowman et al., 2016)</ref>. For our model, we observe that the basic techniques are insufficient (Table <ref type="table" target="#tab_3">3</ref>). While more recent treatments exist <ref type="bibr" target="#b25">(Kim et al., 2018;</ref><ref type="bibr" target="#b16">He et al., 2019)</ref>, they incur high computational costs that prohibit practical deployment in our cases. We introduce two simpler but effective methods to prevent encoder degeneration.</p><p>Mutual Information Maximization. The KLdivergence term in the standard VAE ELBO can be decomposed to reveal the mutual information term (Hoffman and Johnson, 2016):</p><formula xml:id="formula_15">E p d [D KL (q φ (z | x) p(z))] = D KL (q φ (z) p(z)) + I q φ (x; z)</formula><p>where p d is the empirical distribution of the data. Re-weighting the decomposed terms for optimizing the VAE behaviors has been explored previously <ref type="bibr" target="#b7">(Chen et al., 2018;</ref><ref type="bibr" target="#b53">Zhao et al., 2017;</ref><ref type="bibr" target="#b44">Tolstikhin et al., 2018)</ref>. In this work, we propose simply canceling out the mutual information term by performing mutual information estimation as a postprocedure. Since the preservation of the conversation encoder E (c) and global latents is vital for generation controlability, we specifically maximize mutual information between the global latents and the evidence:</p><formula xml:id="formula_16">L VHDA =E q φ [log p θ (c | z)]</formula><p>(2)</p><formula xml:id="formula_17">-D KL (q φ (z | c) p(z)) + I q φ (c; z (c) ).</formula><p>In our work, the mutual information term is computed empirically using the Monte-Carlo estimator for each mini-batch. The details are provided in the supplementary material.</p><p>Hierarchically-scaled Dropout. Extending word dropouts and utterance dropouts <ref type="bibr" target="#b35">Park et al. (2018)</ref>, we apply dropouts discriminatively to all dialog features (goals and dialog acts) according to the feature hierarchy level. We hypothesize that employing dropouts could be detrimental to the learning of lower-level latent variables, as information dropouts stack multiplicatively along the hierarchy. However, it is also necessary in order to encourage meaningful encoding of latent variables. Specifically, we propose a novel dropout scheme that scales exponentially along with the hierarchical depth, allowing higher-level information to flow towards lower levels easily. For our implementation, we set the dropout ratio between two adjacent levels to 1.5, resulting in the dropout probabilities of [0.1, 0.15, 0.23, 0.34, 0.51] for speaker information to utterances. We confirm our hypothesis in § 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Following the protocol in (Yoo et al., 2019), we generate three independent sets of synthetic dialog samples, and, for each augmented dataset, we repeatedly train the same dialog state tracker three times with different seeds. We compare the aggregated results from all nine trials with the baseline results. Ultimately, we repeat this procedure for all combinations of state trackers and datasets. For non-augmented baselines, we repeat the experiments ten times. Implementation Details. The hidden size of dialog vectors is 1000, and the hidden size of utterance, dialog act specification, turn state, and turn goal representations is 500. The dimensionality for latent variables is between 100 and 200. We use GloVe <ref type="bibr" target="#b36">(Pennington et al., 2014)</ref> and character <ref type="bibr" target="#b15">(Hashimoto et al., 2017)</ref>  Besides, modifications are applied to these trackers to stabilize the performance on random seeds (denoted as GLAD + and GCE + ). Specifically, we enrich the word embeddings with subword information <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref> and apply dropout on word embeddings (dropout rate of 0.2). Furthermore, we also conduct experiments on a simpler architecture that shares a similar structure with GCE but does not employ self-attention for the sequence encoders (denoted as RNN).</p><p>Evaluation Measures. Joint goal accuracy (goal for short) measures the ratio of the number of turns whose goals a tracker has correctly identified over the total number of turns. Similarly, request accuracy, or request, measures the turn-level accuracy of request-type dialog acts, while inform accuracy (inform) measures the turn-level accuracy of inform-type dialog acts. Turn-level goals accumulate from inform-type dialog acts starting from the beginning of the dialog until respective dialog turns, and thus they can be inferred from historical inform-type dialog acts (Table <ref type="table" target="#tab_7">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Augmentation Results</head><p>Main Results. We present the data augmentation results in Table <ref type="table" target="#tab_1">1</ref>. The results strongly suggest that generative data augmentation for dialog state tracking is a viable strategy for improving existing DST models without modifying them, as improvements were observed at statistically significant levels regardless of the tracker and dataset. The margin of improvements was more signifi- cant for less expressive state trackers (RNN) than the more expressive ones (GLAD + and GCE + ). Even so, we observed varying degrees of improvements (zero to two percent in joint goal accuracy) even for the more expressive trackers, suggesting that GDA is effective regardless of downstream model expressiveness.</p><p>We observe larger improvement margins for inform-type dialog acts (or subsequently goals) from comparing performances between the dialog act types. This observation is because request-type dialog acts are generally more dependent on the user utterance in the same turn rather than requiring resolution of long-term dependencies, as illustrated in the dialog sample (Table <ref type="table" target="#tab_7">6</ref>). The observation supports our hypothesis that more diverse synthetic dialogs can benefit data augmentation by exploring unseen dialog dynamics.</p><p>Note that the goal tracking performances have relatively high variances due to the accumulative effect of tracking dialogs. However, as an additional benefit of employing GDA, we observe that synthetic dialogs help stabilize downstream tracking performances on DSTC2 and MultiWoZ-R datasets. can be inferred from turn-level inform-type dialog acts, it may seem redundant to incorporate goal modeling into our model. To verify its effectiveness, we train a variant of VHDA, where the model does not explicitly track goals. The results (Table <ref type="table" target="#tab_2">2</ref>) show that VDHA without explicit goal tracking suffers in joint goal accuracy but performs better in turn request accuracy for some instances. We conjecture that explicit goal tracking helps the model reinforce long-term dialog goals; however, the model does so in the minor expense of shortterm state tracking (as evident from lower state tracking accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Employing Training Techniques.</head><p>To demonstrate the effectiveness of the two proposed training techniques, we compare (1) the data augmentation results and (2) the KL-divergence between the posterior and prior of the dialog latents z (c) (Table <ref type="table" target="#tab_3">3</ref>). The results support our hypothesis that the proposed measures reduce the risk of inference collapse. We also confirm that exponentiallyscaled dropouts are more or comparably effective at preventing posterior collapse than uniform  dropouts while generating more coherent samples (evident from higher data augmentation results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Evaluation</head><p>To understand the effect of joint learning of various dialog features on language generation, we compare our model with a model that only learns linguistic features. Following the evaluation protocol from prior work <ref type="bibr" target="#b48">(Wen et al., 2017;</ref><ref type="bibr" target="#b2">Bak and Oh, 2019)</ref>, we use ROUGE-L F1-score <ref type="bibr" target="#b31">(Lin, 2004)</ref> to evaluate the linguistic quality and utterance-level unigram cross-entropy (Serban et al., 2017) (regarding the training corpus distribution) to evaluate diversity. Table <ref type="table">4</ref> shows that our model generates better and more diverse utterances than the previous strong baseline on conversation modeling. These results supports the idea that joint learning of dialog annotations improves utterance generation, thereby increasing the chance of generating novel samples that improve the downstream trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">User Simulation Evaluation</head><p>Simulating human participants has become a crucial feature for training dialog policy models using reinforcement learning and automatic evaluation of dialog systems <ref type="bibr" target="#b1">(Asri et al., 2016)</ref>. Although our model does not specialize in user simulation, our experiments show that the model outperforms the previous model (VHUS 2 ) <ref type="bibr" target="#b14">(Gür et al., 2018)</ref> in terms of accuracy and creativeness (diversity). We evaluate the user simulation quality using the pre-  diction accuracy on the test sets and the diversity using the entropy 3 of predicted dialog act specifications (act-slot-value triples). We present the results in Table <ref type="table" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">z (c) -interpolation</head><p>We conduct z (c) -interpolation experiments to demonstrate that our model can generalize the dataset space and learn to decode plausible samples from unseen latent space. The generated sample (Table <ref type="table" target="#tab_7">6</ref>) shows that our model can maintain coherence while generalizing key dialog features, such as the user goal and the dialog length. As a specific example, given both anchors' user goals (food=mediterranean and food=indian, respectively) 4 , the generated midpoint between the two data points is a novel dialog with no specific food type (food=dontcare).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel hierarchical and recurrent VAE-based architecture to capture accurately the semantics of fully annotated goal-oriented dialog 3 The entropy is calculated with respect to the training set distribution 4 The supplementary material includes the full examples.</p><p>corpora. To reduce the risk of inference collapse while maximizing the generation quality, we directly modified the training objective and devised a technique to scale dropouts along the hierarchy. We showed that our proposed model VHDA was able to achieve significant improvements for various competitive dialog state trackers in diverse corpora through extensive experiments. With recent trends in goal-oriented dialog systems gravitating towards end-to-end approaches <ref type="bibr" target="#b30">(Lei et al., 2018)</ref>, we wish to explore a self-supervised model, which discriminatively generates samples that directly benefit the downstream models for the target task. We would also like to explore different implementations in line with recent advances in dialog models, especially using large-scale pre-trained language models.</p><p>During the training of VAEs, inference collapse occurs when the model converges to a local optimum where the approximate posterior q φ (z | x) collapses to the prior p(z), indicating the vanishment of the encoder network due to the decoder's negligence of the encoder signals. Quantifying, diagnosing, and devising a mitigation technique for the inference collapse phenomenon have been studied extensively in the past <ref type="bibr" target="#b8">(Chen et al., 2016;</ref><ref type="bibr" target="#b53">Zhao et al., 2017;</ref><ref type="bibr" target="#b9">Cremer et al., 2018;</ref><ref type="bibr" target="#b37">Razavi et al., 2018;</ref><ref type="bibr" target="#b16">He et al., 2019)</ref>. However, current approaches for mitigating inference collapse are limited to significant modifications to the existing VAE framework <ref type="bibr" target="#b16">(He et al., 2019;</ref><ref type="bibr" target="#b25">Kim et al., 2018)</ref> or limited to specific architectural designs <ref type="bibr" target="#b37">(Razavi et al., 2018)</ref>. Current approaches do not work well on our model due to the complexity of our VAE structure. Instead, we employ a relatively simple technique that directly modifies the VAE objective. By doing so, we mitigate any significant changes to the main VAE framework while achieving satisfactory results on inference collapse mitigation. Though not covered in this paper, our method has applications in other VAE structures. In this appendix, we wish to delve more in-depth into the intuitions and detailed implementation of our approach.</p><p>Motivation. As first noted by Hoffman and Johnson (2016) (and subsequently utilized by <ref type="bibr" target="#b53">(Zhao et al., 2017;</ref><ref type="bibr" target="#b7">Chen et al., 2018)</ref>), the KLdivergence term of the ELBO objective can be decomposed into two terms: (1) the KL-divergence between the aggregate posterior and the prior and (2) the mutual information between the latent variables and the data:</p><formula xml:id="formula_18">E p d [D KL (q φ (z | x) p(z))] = D KL (q φ (z) p(z)) + I q φ (x; z)<label>(3)</label></formula><p>where p d is the empirical distribution of data and the aggregate posterior q φ (z) is obtained by marginalizing the approximate posterior using the empirical distribution:</p><formula xml:id="formula_19">q φ (z) = E x∼p d [q φ (z | x)].<label>(4)</label></formula><p>Using the definition of inference collapse, we can deduce that the KL-divergence term D KL (q φ (z | x) p(z)) is zero during inference collapse. This fact implies that both decomposed terms in Equation 3 must be zero since both terms are nonnegative.</p><p>Our preliminary studies show an interesting pattern in the KL-divergence term and its decomposed terms during basic training (training without inference-collapse treatments) (Figure <ref type="figure" target="#fig_2">2</ref>). We observe that the KL-divergence of the aggregate posterior term vanishes earlier than the mutual information does. We also observe that the mutual information term, which represents the encoder effectiveness, vanishes eventually. This collapse happens after the KL-divergence cannot be minimized without sacrificing the encoder's expressiveness. Note that optimization of the ELBO objective minimizes the ELBO's KL-divergence term and its underlying terms, one of which is directly related to the encoder health. Although the reconstruction term in the ELBO encourages maximization of the mutual information, the autoregressive property of the decoder and the complexity of the reconstruction loss "dilutes" the goal of maximizing mutual information. Hence, to minimize inference collapse, we propose a modified VAE objective that explicitly maximizes the mutual information between the latents and the data by "canceling" out the mutual information term in the KL-divergence<ref type="foot" target="#foot_1">foot_1</ref> :</p><formula xml:id="formula_20">L VHDA =E p d [E q φ [log p θ (c | z)]] -E p d [D KL (q φ (z | c) p(z))]</formula><p>+ I q φ (c; z).</p><p>(5)</p><p>Note that some notations (expectation over the empirical distribution) have been omitted in the main paper for clarity.</p><p>Relation to Prior Work. Our approach is related to previous work on manipulating the VAE objective for customizing the VAE behavior <ref type="bibr" target="#b53">(Zhao et al., 2017;</ref><ref type="bibr" target="#b7">Chen et al., 2018)</ref>. It can also be thought of as a special case of Wasserstein Autoencoders <ref type="bibr" target="#b44">(Tolstikhin et al., 2018)</ref> Although not all related works were original proposed to directly combat inference collapse, our approach can be considered a special case of InfoVAE <ref type="bibr" target="#b53">(Zhao et al., 2017)</ref> and β-TCVAE <ref type="bibr" target="#b7">(Chen et al., 2018)</ref>. Specifically, <ref type="bibr" target="#b53">Zhao et al. (2017)</ref> proposed a modified VAE objective as I q E pd [D KL (q (z|x)||p(z))] D KL (q (z)||p(z)) follows:</p><formula xml:id="formula_21">L InfoVAE =E p d [E q φ [log p θ (x | z)]] -(1 -α)E p d [D KL (q φ (z | x) p(z))] -(α + λ -1)D KL (q φ (z) p(z)).<label>(6)</label></formula><p>Rearranging the equation, we can express the same objective related to the mutual information:</p><formula xml:id="formula_22">L InfoVAE =E p d [E q φ [log p θ (x | z)]]</formula><p>-λD KL (q φ (z) p(z))</p><formula xml:id="formula_23">-(1 -α)I q φ (x; z).<label>(7)</label></formula><p>Hence, our method is a special case of InfoVAE where α = 1 and λ = 1. Meanwhile, <ref type="bibr" target="#b7">Chen et al. (2018)</ref> proposed an extended modification to β-VAE <ref type="bibr" target="#b18">(Higgins et al., 2017)</ref> to further decompose the KL-divergence of the aggregate posterior in terms of latent correlation:</p><formula xml:id="formula_24">L InfoVAE =E p d [E q φ [log p θ (x | z)]]</formula><p>-αI q φ (x; z)</p><formula xml:id="formula_25">-βD KL (q φ (z) i q φ (z i )) -γ i D KL (q φ (z i ) p(z i )). (8)</formula><p>In the equation above, our approach corresponds the case where α = 0 and β = γ = 1.</p><p>Mutual Information Estimation. We can estimate the mutual information between the latents and the data under the empirical distribution of x using Monte Carlo sampling. However, this estimation method is known to be biased <ref type="bibr" target="#b3">(Belghazi et al., 2018)</ref>. Despite recent advances in MI estimation techniques, we find that our unparameterized method is sufficient for achieving inference collapse mitigation and probing.:</p><p>The equation for estimating the mutual information is shown in Equation <ref type="formula">9</ref>. where x is sampled from the empirical distribution of the dataset and N , M and L are hyperparameters. In practice, the estimation is performed over the data samples in a mini-batch for computational efficiency. Given a mini-batch of size N , we further approximate the estimation by sampling the latent variables z once for each data point (M = 1) (Equation <ref type="formula">10</ref>).</p><p>We visualize the variance in our mutual information estimation method in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Architectural Diagram</head><p>We include a more detailed architectural diagram (Figure <ref type="figure">4</ref>) depicting the latent variables and the model inference, which we could not illustrate in Figure <ref type="figure" target="#fig_0">1</ref> due to space constraints. Note that the orange crosses denote decoder dropouts. The figure also illustrates the hierarchically-scaled dropout scheme, motivated by the need to minimize information loss while discouraging the decoders from relying on training signals, leading to exposure bias. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of VHDA. Solid and dashed arrows represent generation and recognition respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Failed training behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,117.35,477.98,362.84,259.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We annealed the KL-divergence weights over 250,000 training steps. For data synthesis, we employ ancestral sampling to generate samples from the empirical posterior distribution. We fixed the ratio of synthetic to original data samples to 1. Datasets. We conduct experiments on four state tracking corpora: WoZ2.0<ref type="bibr" target="#b48">(Wen et al., 2017)</ref>, DSTC2<ref type="bibr" target="#b17">(Henderson et al., 2014)</ref>, Multi-WoZ<ref type="bibr" target="#b6">(Budzianowski et al., 2018)</ref>, and DialEdit<ref type="bibr" target="#b32">(Manuvinakurike et al., 2018)</ref>. These corpora cover a variety of domains (restaurant booking, hotel reservation, and image editing). Note that, because the MultiWoZ dataset is a multi-domain corpus, we extract single-domain dialog samples from the two most prominent domains (hotel and restaurant, denoted by MultiWoZ-H and MultiWoZ-R, respectively). Dialog State Trackers. We use GLAD and GCE as the two competitive baselines for state tracking.</figDesc><table /><note><p>embeddings as pre-trained word emebddings (400 dimensions total) for word and dialog act tokens. All models used Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 1e-3,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>‡ 96.7 ‡ 74.2 † 97.0 ‡ 49.6 † 73.4 † 31.0 † 59.7 † 36.4 † 96.8 Results of data augmentation using VHDA for dialog state tracking on various datasets and state trackers. Note that we report inform accuracies for MultiWoZ datasets instead, as request-type prediction is trivial for those.</figDesc><table><row><cell cols="2">GDA</cell><cell cols="2">MODEL</cell><cell cols="2">WOZ2.0</cell><cell cols="2">DSTC2</cell><cell cols="2">MWOZ-R</cell><cell cols="2">MWOZ-H</cell><cell>DIALEDIT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">GOAL REQ GOAL REQ GOAL</cell><cell>INF</cell><cell>GOAL</cell><cell>INF</cell><cell>GOAL REQ</cell></row><row><cell>-</cell><cell></cell><cell>RNN</cell><cell></cell><cell>74.5</cell><cell>96.1</cell><cell>69.7</cell><cell>96.0</cell><cell>43.7</cell><cell>69.4</cell><cell>25.7</cell><cell>55.6</cell><cell>35.8</cell><cell>96.6</cell></row><row><cell cols="5">VHDA RNN 78.7  -GLAD + 87.8</cell><cell>96.8</cell><cell>74.5</cell><cell>96.4</cell><cell>58.9</cell><cell>76.3</cell><cell>33.4</cell><cell>58.9</cell><cell>35.9</cell><cell>96.7</cell></row><row><cell cols="4">VHDA GLAD +</cell><cell>88.4</cell><cell>96.6</cell><cell cols="4">75.5  ‡ 96.8  † 61.5  † 77.4</cell><cell cols="3">37.8  ‡ 61.3  ‡ 37.1  † 96.8</cell></row><row><cell>-</cell><cell></cell><cell cols="2">GCE +</cell><cell>88.7</cell><cell>97.0</cell><cell>74.8</cell><cell>96.3</cell><cell>60.5</cell><cell>76.7</cell><cell>36.5</cell><cell>61.0</cell><cell>36.1</cell><cell>96.6</cell></row><row><cell cols="4">VHDA GCE +</cell><cell cols="2">89.3  ‡ 97.1</cell><cell cols="3">76.0  ‡ 96.7  † 63.3</cell><cell>77.2</cell><cell>38.3</cell><cell cols="2">63.1  † 37.6  † 96.8</cell></row><row><cell cols="2">† p &lt; 0.1</cell><cell cols="2">‡ p &lt; 0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GOAL DST</cell><cell></cell><cell cols="2">WOZ2.0</cell><cell cols="2">DSTC2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">GOAL REQ GOAL REQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/O</cell><cell>RNN</cell><cell></cell><cell>77.8</cell><cell>96.4</cell><cell>71.2</cell><cell>97.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/</cell><cell>RNN</cell><cell></cell><cell>78.7</cell><cell>96.7</cell><cell>74.2</cell><cell>97.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/O</cell><cell cols="2">GLAD +</cell><cell>86.5</cell><cell>96.9</cell><cell>74.7</cell><cell>97.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/</cell><cell cols="2">GLAD +</cell><cell>88.4</cell><cell>96.6</cell><cell>75.5</cell><cell>96.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/O</cell><cell>GCE +</cell><cell></cell><cell>86.4</cell><cell>96.3</cell><cell>75.5</cell><cell>96.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/</cell><cell>GCE +</cell><cell></cell><cell>89.3</cell><cell>97.1</cell><cell>76.0</cell><cell>96.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of data augmentation results between VHDA with and without explicit goal tracking.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effects of Joint Goal Tracking. Since user goals Ablation studies on the training techniques using GCE</figDesc><table><row><cell cols="2">DROP. OBJ.</cell><cell>z (c) -KL</cell><cell>WOZ2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GOAL</cell><cell>REQ</cell></row><row><cell>0.00</cell><cell>STD.</cell><cell>5.63</cell><cell cols="2">84.1±0.9 95.9±0.6</cell></row><row><cell>0.00</cell><cell>MIM</cell><cell>5.79</cell><cell cols="2">86.0±0.2 96.1±0.2</cell></row><row><cell>0.25</cell><cell>STD.</cell><cell>10.44</cell><cell cols="2">88.5±1.4 96.9±0.1</cell></row><row><cell>0.25</cell><cell>MIM</cell><cell>11.31</cell><cell cols="2">88.9±0.4 97.0±0.2</cell></row><row><cell>0.50</cell><cell>STD.</cell><cell>14.68</cell><cell cols="2">88.6±1.0 96.9±0.2</cell></row><row><cell>0.50</cell><cell>MIM</cell><cell>16.33</cell><cell cols="2">89.2±0.8 96.9±0.2</cell></row><row><cell>HIER.</cell><cell>STD.</cell><cell>14.34</cell><cell cols="2">88.2±1.0 97.1±0.2</cell></row><row><cell>HIER.</cell><cell>MIM</cell><cell>16.27</cell><cell cols="2">89.3±0.4 97.1±0.2</cell></row></table><note><p>+ as the tracker. The effect of different dropout schemes and training objectives is quantified. MIM refers to mutual information maximization ( § 3.5).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of user simulation performances.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note><p>A sample generated from the midpoint between two latent variables in the z (c) space encoded from two anchor data points.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/kaniblu/vhda</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>On a side note, we did not observe any "lag" in the inference network, as described by<ref type="bibr" target="#b16">He et al. (2019)</ref>. This observation is evident from the sustained mutual information level throughout the training session (Figure2). Hence we did not employ the recently proposed method.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank <rs type="person">Hyunsoo Cho</rs> for his help with implementations and <rs type="person">Jihun Choi</rs> for the thoughtful feedback. We also gratefully acknowledge support from <rs type="funder">Adobe Inc.</rs> in the form of a generous gift to <rs type="institution">Seoul National University</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Mutual Information</head><p>Maximization for Mitigating Inference Collapse  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Full Results of Data Augmentation</head><p>The full data augmentation results are shown below, including the statistics. Note that generative data augmentation also has the effect of reducing the variance of the downstream models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Exhibits of Synthetic Samples</head><p>This section describes the method we use to sample synthetic data points from our model's posterior and presents some synthetic samples generated from our model using the described technique. We use ancestral sampling <ref type="bibr" target="#b16">(He et al., 2019)</ref>, or the posterior sampling technique <ref type="bibr">(Yoo et al., 2019)</ref>, to sample data points from the empirical distribution of the latent space. Specifically, we choose an anchor data point from the dialog dataset: c ∼ p d (c), where p d is the empirical distribution of goaloriented dialogs. Then, we sample a set of latent variables z (c) from the encoded distribution of c: z (c) ∼ q φ (z (c) | c). Next, we decode a sample c that maximizes the log-likelihood for each sampled conversational latents:</p><p>We use these samples to augment the original dataset. Also, we fix the ratio of the synthetic dataset to the original dataset to 1. In our experiments, we observe that all of the synthetic samples generated via ancestral sampling are mostly coherent and, most importantly, novel, i.e., each synthetic data point is somehow different from the original anchor point (e.g., variations in utterances, dialog-level semantics, or sometimes annotation errors).</p><p>In the following tables, we showcase few dialog samples from our augmentation datasets. The tables present the generated samples along with their reference dialog samples.</p><p>Appendix E z (c) Interpolation Results (Including Both Anchors)</p><p>Visualizing samples from a linear interpolation of two points in the latent space <ref type="bibr" target="#b5">(Bowman et al., 2016)</ref> is a popular way to showcase the generative capability of VAEs. Given two dialog samples c 1 and c 2 , we map the data points onto the conversational latent space to obtain z (c) 1 and z (c) 2 . Multiple equidistant samples z 1 , ..., z N are selected from the linear interpolation between the two points: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A sequence-to-sequence model for user simulation in spoken dialogue systems</title>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1151" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational hierarchical user-based conversation model</title>
		<author>
			<persName><forename type="first">Jinyeong</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1941" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIGNLL</title>
		<imprint>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiwoz-a large-scale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<title level="m">Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03558</idno>
		<title level="m">Inference suboptimality in variational autoencoders</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recent Trends in Discourse and Dialogue</title>
		<author>
			<persName><forename type="first">Laila</forename><surname>Dybkjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Minker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dialogwae: Multimodal response generation with conditional wasserstein autoencoder</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User modeling for task oriented dialogues</title>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="900" to="906" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple nlp tasks</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1923" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05534</idno>
		<title level="m">Lagging inference networks and posterior collapse in variational autoencoders</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the NIPS Workshop in Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00796</idno>
		<title level="m">A simple language model for task-oriented dialogue</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence data augmentation for dialogue language understanding</title>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1234" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03906</idno>
		<title level="m">Efficient dialogue state tracking by selectively overwriting memory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-amortized variational autoencoders</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
	<note>David Sontag, and Alexander Rush</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Leveraging sentence-level information with encoder lstm for semantic slot filling</title>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2077" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
	<note>Xiangnan He, and Dawei Yin</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dialedit: Annotations for spoken conversational image editing</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Manuvinakurike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Brixey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 14th Joint ACL-ISO Workshop on Interoperable Semantic Annotation</title>
		<meeting>14th Joint ACL-ISO Workshop on Interoperable Semantic Annotation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsunghsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Elnaz</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00899</idno>
		<title level="m">Toward scalable neural dialogue state tracking model</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A hierarchical latent structure for variational conversation modeling</title>
		<author>
			<persName><forename type="first">Yookoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL:HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, Ben Poole, and Oriol Vinyals</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Preventing posterior collapse with delta-vaes</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT: NAACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>John R Searle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><surname>Bierwisch</surname></persName>
		</author>
		<title level="m">Speech act theory and pragmatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Utterance generation with variational auto-encoder for slot filling in spoken language understanding</title>
		<author>
			<persName><forename type="first">Youhyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanggoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="505" to="509" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems</title>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="562" to="588" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wasserstein autoencoders</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2797" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina M Rojas</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08743</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07795</idno>
		<title level="m">Dialog state tracking with reinforced data augmentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data augmentation for spoken language understanding via joint variational generation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youhyun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanggoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7402" to="7409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive encoder for dialogue state tracking</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1458" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Incremental lstmbased dialog state tracker</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Zilka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Jurcicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on ASRU</title>
		<imprint>
			<biblScope unit="page" from="757" to="762" />
			<date type="published" when="2015">2015. 2015</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
