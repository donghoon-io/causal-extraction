<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Information Criterion for Inferring Coupling of Distributed Dynamical Systems</title>
				<funder>
					<orgName type="full">New South Wales Government</orgName>
				</funder>
				<funder ref="#_GtHcKF3">
					<orgName type="full">Faculty of Engineering &amp; Information Technologies, The University of Sydney</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Centre for Field Robotics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-11-28">28 November 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Oliver</forename><forename type="middle">M</forename><surname>Cliff</surname></persName>
							<email>o.cliff@acfr.usyd.edu.au</email>
							<affiliation key="aff3">
								<orgName type="department">Australian Centre for Field Robotics</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikhail</forename><surname>Prokopenko</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Complex Systems Research Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Fitch</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Australian Centre for Field Robotics</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Centre for Autonomous Systems</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Goethe University</orgName>
								<address>
									<settlement>Frankfurt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Raul Vicente</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Society</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Information Criterion for Inferring Coupling of Distributed Dynamical Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-28">28 November 2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3389/frobt.2016.00071</idno>
					<note type="submission">This article was submitted to Computational Intelligence, a section of the journal Frontiers in Robotics and AI Received: 19 August 2016 Accepted: 31 October 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>complex networks</term>
					<term>structure learning</term>
					<term>dynamic Bayesian networks</term>
					<term>graph dynamical systems</term>
					<term>information theory</term>
					<term>dynamical systems</term>
					<term>state space reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The behavior of many real-world phenomena can be modeled by non-linear dynamical systems whereby a latent system state is observed through a filter. We are interested in interacting subsystems of this form, which we model by a set of coupled maps as a synchronous update graph dynamical system. Specifically, we study the structure learning problem for spatially distributed dynamical systems coupled via a directed acyclic graph. Unlike established structure learning procedures that find locally maximum posterior probabilities of a network structure containing latent variables, our work exploits the properties of dynamical systems to compute globally optimal approximations of these distributions. We arrive at this result by the use of time delay embedding theorems. Taking an information-theoretic perspective, we show that the log-likelihood has an intuitive interpretation in terms of information transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Complex systems are broadly defined as systems that comprise interacting non-linear components <ref type="bibr" target="#b1">(Boccaletti et al., 2006)</ref>. Discrete-time complex systems can be represented using graphical models such as graph dynamical systems (GDSs) <ref type="bibr" target="#b26">(Mortveit and Reidys, 2001;</ref><ref type="bibr" target="#b41">Wu, 2005)</ref>, where spatially distributed dynamical units are coupled via a directed graph. The task of learning the structure of such a system is to infer directed relationships between variables; in the case of dynamical systems, these variables are typically hidden <ref type="bibr" target="#b19">(Kantz and Schreiber, 2004)</ref>. In this paper, we study the structure learning problem for complex networks of non-linear dynamical systems coupled via a directed acyclic graph (DAG). Specifically, we formulate synchronous update GDSs as dynamic Bayesian networks (DBNs) and study this problem from the perspective of information theory.</p><p>The structure learning problem for distributed dynamical systems is a precursor to inference in systems that are not fully observable. This case encompasses many practical problems of known artificial, biological, and chemical systems, such as neural networks <ref type="bibr" target="#b22">(Lizier et al., 2011;</ref><ref type="bibr" target="#b40">Vicente et al., 2011;</ref><ref type="bibr" target="#b31">Schumacher et al., 2015)</ref>, multi-agent systems <ref type="bibr" target="#b42">(Xu et al., 2013;</ref><ref type="bibr" target="#b9">Gan et al., 2014;</ref><ref type="bibr" target="#b5">Cliff et al., 2016;</ref><ref type="bibr" target="#b39">Umenberger and Manchester, 2016)</ref>, and various others <ref type="bibr" target="#b1">(Boccaletti et al., 2006)</ref>. Modeling a partially observable system as a dynamical network presents a challenge in synthesizing these models and capturing their global properties <ref type="bibr" target="#b1">(Boccaletti et al., 2006)</ref>. In addressing this challenge, we draw on probabilistic graphical models (specifically Bayesian network (BN) structure learning) and non-linear time series analysis (differential topology).</p><p>In this paper, we exploit the properties of discrete-time multivariate dynamical systems in inferring coupling between latent variables in a DAG. Specifically, the main focus of this paper is to analytically derive a measure (score) for evaluating the fitness of a candidate DAG, given data. We assume the data are generated by a certain family of multivariate dynamical system and are thus able to overcome the issue of latent variables faced by established structure learning algorithms. That is, under certain assumptions of the dynamical system, we are able to employ time delay embedding theorems <ref type="bibr" target="#b35">(Stark et al., 2003;</ref><ref type="bibr" target="#b7">Deyle and Sugihara, 2011)</ref> to compute our scores.</p><p>Our main result is a tractable form of the log-likelihood function for synchronous GDSs. Using this result, we are able to directly compute the Bayesian information criterion (BIC) <ref type="bibr" target="#b32">(Schwarz, 1978)</ref> and Akaike information criterion (AIC) <ref type="bibr" target="#b0">(Akaike, 1974)</ref> and thus achieve globally optimal approximations of the posterior distribution of the graph. Finally, we show that the log-likelihood and log-likelihood ratio can be expressed in terms of collective transfer entropy <ref type="bibr" target="#b24">(Lizier et al., 2010;</ref><ref type="bibr" target="#b40">Vicente et al., 2011)</ref>. This result places our work in the context of effective network analysis <ref type="bibr" target="#b33">(Sporns et al., 2004;</ref><ref type="bibr" target="#b27">Park and Friston, 2013)</ref> based on information transfer <ref type="bibr" target="#b16">(Honey et al., 2007;</ref><ref type="bibr" target="#b22">Lizier et al., 2011;</ref><ref type="bibr" target="#b6">Cliff et al., 2013</ref><ref type="bibr" target="#b5">Cliff et al., , 2016</ref>) and relates to the information processing intrinsic to distributed computation <ref type="bibr" target="#b23">(Lizier et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We are interested in classes of systems whereby dynamical units are coupled via a graph structure. These types of systems have been studied under several names, including complex dynamical networks <ref type="bibr" target="#b1">(Boccaletti et al., 2006)</ref>, spatially distributed dynamical systems <ref type="bibr" target="#b19">(Kantz and Schreiber, 2004;</ref><ref type="bibr" target="#b31">Schumacher et al., 2015)</ref>, master-slave configurations (or systems with a skew product structure) <ref type="bibr" target="#b20">(Kocarev and Parlitz, 1996)</ref>, and coupled maps <ref type="bibr" target="#b18">(Kaneko, 1992)</ref>. Common to each of these definitions is that the multivariate state of the system comprises individual subsystem states, the dynamics of which are given by a set of either discretetime maps or first-order ordinary differential equations (ODEs), called a flow. We assume the discrete-time formulation, where a map can be obtained numerically by integrating differential equations or recording experimental data (observations) at discretetime intervals <ref type="bibr" target="#b19">(Kantz and Schreiber, 2004)</ref>. The literature on coupled dynamical systems is often focused on the analysis of characteristics such as stability and synchrony of the system. In this work, we draw on the fields of BN structure learning and non-linear time series analysis to infer coupling between spatially distributed dynamical systems.</p><p>BN structure learning comprises two subproblems: evaluating the fitness of a graph and identifying the optimal graph given this fitness criterion <ref type="bibr" target="#b4">(Chickering, 2002)</ref>. The evaluation problem is particularly challenging in the case of graph dynamical systems, which include both latent and observed variables. A number of theoretically optimal techniques exist for the evaluation problem for BNs with complete data <ref type="bibr" target="#b2">(Bouckaert, 1994;</ref><ref type="bibr" target="#b21">Lam and Bacchus, 1994;</ref><ref type="bibr" target="#b14">Heckerman et al., 1995)</ref>, which have been extended to DBNs <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. With incomplete data, however, the common approach is to resort to approximations that find local optima, e.g., expectation-maximization (EM) <ref type="bibr" target="#b8">(Friedman et al., 1998;</ref><ref type="bibr" target="#b10">Ghahramani, 1998)</ref>. An additional caveat with respect to structure learning is that algorithms find an equivalence class of networks with the same Markov structure, and not a unique solution <ref type="bibr" target="#b4">(Chickering, 2002)</ref>.</p><p>In non-linear time series analysis, the problem of inferring coupling strength and causality in complex systems has received significant attention recently <ref type="bibr" target="#b30">(Schreiber, 2000;</ref><ref type="bibr" target="#b17">Hoyer et al., 2009)</ref>. Early work by Granger defined causality in terms of the predictability of one system linearly coupled to another <ref type="bibr" target="#b12">(Granger, 1969)</ref>. Although this measure is popular for identifying coupling, it requires systems are linear statistical models and is considered insufficient for inferring coupling between dynamical systems due to inseparability <ref type="bibr" target="#b36">(Sugihara et al., 2012)</ref>. Another method popular in neuroscience is transfer entropy, which was introduced to quantify the information transfer between non-linear (finiteorder Markov) systems <ref type="bibr" target="#b30">(Schreiber, 2000)</ref>. Transfer entropy has been used to recover interaction networks in numerous fields such as multi-agent systems <ref type="bibr" target="#b5">(Cliff et al., 2016)</ref> and effective networks in neuroscience <ref type="bibr" target="#b22">(Lizier et al., 2011;</ref><ref type="bibr" target="#b40">Vicente et al., 2011;</ref><ref type="bibr" target="#b25">Lizier and Rubinov, 2012)</ref>. More recently, researchers have used the additive noise model <ref type="bibr" target="#b17">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b28">Peters et al., 2011)</ref> to infer unidirectional cause and effect relationships with observed random variables and find a unique DAG (as opposed to an equivalence class). These studies have been extended by exploring weakly additive noise models for learning the structure of systems of observed variables with non-linear coupling <ref type="bibr" target="#b13">(Gretton et al., 2009)</ref>.</p><p>A recent approach to inferring causality is convergent crossmapping (CCM), which is based on Takens theorem <ref type="bibr" target="#b37">(Takens, 1981)</ref> and tests for causation (predictability) by considering the history of observed data of a hidden variable in predicting the outcome of another <ref type="bibr" target="#b36">(Sugihara et al., 2012)</ref>. Using a similar approach, <ref type="bibr" target="#b31">Schumacher et al. (2015)</ref> used Stark's bundle delay embedding theorem <ref type="bibr" target="#b34">(Stark, 1999;</ref><ref type="bibr" target="#b35">Stark et al., 2003)</ref> to predict one subsystem from another using Gaussian processes. This algorithm can thus be used to infer the driving systems in spatially distributed dynamical systems in a similar manner to our work. However, both papers do not consider the problem of inference over the entire network structure, or formally derive the measures used therein. In our work, we provide a rigorous proof based on established structure learning procedures and discuss the problem of inference within a distributed dynamical system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BACKGROUND</head><p>This section summarizes relevant technical concepts used throughout the paper. First, a stochastic temporal process X is defined as a sequence of random variables (X 1 , X 2 , . . ., X N ) with a realization (x 1 , x 2 , . . ., x N ) for countable time indices n ∈ N. Consider a collection of M processes, and denote the ith process X i to have associated realization x i n at temporal index n, and x n as all realizations at that index</p><formula xml:id="formula_0">x n = ⟨x 1 n , x 2 n , . . . , x M n ⟩. If X i</formula><p>n is a discrete random variable, the number of values the variable can take on is denoted |X i n |. The following sections collect results from DBN literature, attractor reconstruction, and information theory that are relevant to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Bayesian Networks</head><p>DBNs are a general graphical representation of a temporal model, representing a probability distribution over infinite trajectories of random variables (Z 1 , Z 2 , . . .) compactly <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. These models are a more expressive framework than the hidden Markov model (HMM) and Kalman filter model (KFM) (or linear dynamical system) <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. In this work, we denote Z n = {X n , Y n } as the set of hidden and observed variables, respectively, where n ∈ {1, 2, . . .} is the temporal index.</p><p>BNs B = (G, Θ) represent a joint distribution p(z) graphically and consist of: a DAG G and a set of conditional probability distribution (CPD) parameters Θ. DBNs B = (B 1 , B → ) extend the BN to model temporal processes and comprise two parts: the prior BN B 1 = (G 1 , Θ 1 ), which defines the joint distribution p B 1 (z 1 ); and the two-time-slice Bayesian network (2TBN) B → = (G → , Θ → ), which defines a first-order Markov process p B → (z n+1 |z n ) <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. This formulation allows for a variable to be conditioned on its respective parent set</p><formula xml:id="formula_1">Π G → (Z i n+1</formula><p>) that can come from the preceding time slice or the current time slice, as long as G → forms a DAG. The 2TBN probability distribution factorizes according to G → with a local CPD p D estimated from an observed dataset. That is, given a set of stochastic processes (Z 1 , Z 2 , . . ., Z N ), the realization of which constitutes a dataset D = (z 1 , z 2 , . . ., z N ), we obtain the 2TBN distribution as</p><formula xml:id="formula_2">p B → (z n+1 |z n ) = ∏ i p B → (z i n+1 |π G → (Z i n+1 )),<label>(1)</label></formula><p>where π G → (Z i n+1 ) denotes the (index-ordered) set of realizations {z</p><formula xml:id="formula_3">j o : Z j o ∈ Π G → (Z i n+1 )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedding Theory</head><p>Embedding theory refers to methods from differential topology for inferring the (hidden) state of a dynamical system from a reconstructed sequence of observations. The state of a discretetime dynamical system is given by a point x n confined to a d-dimensional manifold M. The time evolution of this state is described by a map f : M → M, so that the sequence of states (x n ) is given by x n+1 = f (x n ). In many situations, we only have access to a filtered, scalar representation of the state, i.e., the measurement y n = ψ (x n ) given by some measurement function ψ : M → R <ref type="bibr" target="#b37">(Takens, 1981;</ref><ref type="bibr" target="#b34">Stark, 1999)</ref>. The celebrated Takens' theorem <ref type="bibr" target="#b37">(Takens, 1981)</ref> shows that for typical f and ψ, it is possible to reconstruct f from the observed time series up to some smooth coordinate change. More precisely, fix some κ (the embedding dimension) and some τ (the time delay), then define the delay embedding map</p><formula xml:id="formula_4">Φ Φ Φ f,ψ : M → R κ by Φ Φ Φ f,ψ (x n ) = y (κ) n = ⟨ y n , y n-τ , y n-2τ , . . . , y n-(κ-1)τ ⟩. (2)</formula><p>In differential topology, an embedding refers to a smooth map Ψ: M → N between manifolds M and N if it maps M diffeomorphically onto its image; therefore, Φ Φ Φ f,ψ has a smooth inverse Φ Φ Φ -1 f,ψ . The implication of Takens' theorem is that for typical f and ψ, the image Φ Φ Φ f,ψ (M) of M is completely equivalent to M itself, apart from the smooth invertible change of coordinates given by the mapping Φ Φ Φ f,ψ . An important consequence of this theorem is that we can define a map <ref type="bibr">, 1999)</ref>. There are technical assumptions for Takens' theorem (and the generalized versions employed herein) to hold. These assumptions require: (f, ψ) to be generic functions (in terms of Baire space), a restricted number of periodic points, and distinct eigenvalues at each neighborhood of these points <ref type="bibr" target="#b37">(Takens, 1981;</ref><ref type="bibr" target="#b34">Stark, 1999;</ref><ref type="bibr" target="#b35">Stark et al., 2003;</ref><ref type="bibr" target="#b7">Deyle and Sugihara, 2011)</ref>.</p><formula xml:id="formula_5">F = Φ Φ Φ f,ψ • f • Φ Φ Φ -1 f,ψ on Φ Φ Φ f,ψ , such that y (κ) n+1 = F(y (κ) n ) (Stark</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Information Theoretic Measures</head><p>Conditional entropy represents the uncertainty of a random variable X after taking into account the outcomes of another random variable Y by</p><formula xml:id="formula_6">H(X|Y) = - ∑ x,y p(x, y)log 2 p(x|y). (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>Multivariate transfer entropy is a measure that computes the information transfer from a set of source processes to a set of destination process <ref type="bibr" target="#b22">(Lizier et al., 2011)</ref>. In this work, we use the formulation of collective transfer entropy <ref type="bibr" target="#b24">(Lizier et al., 2010)</ref>, where the information transfer from m source processes V = {Y 1 , Y 2 , . . ., Y m } to a single destination process Y can be decomposed as a sum of conditional entropy terms:</p><formula xml:id="formula_8">T V→Y = H ( Y n+1 |Y (κ) n ) -H ( Y n+1 |Y (κ) n , ⟨Y i,(κ i ) n ⟩ ) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_9">Y i,(κ i ) n = ⟨Y i n , Y i n-τ i , Y i n-2τ i , . . . , Y i n-(κ i -1</formula><p>)τ i ⟩ for some κ i and τ i , and similarly for Y (κ) n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">REPRESENTING NON-LINEAR DYNAMICAL NETWORKS AS DBNs</head><p>We express multivariate dynamical systems as a synchronous update GDS to allow for generic maps. With this model, we can express the time evolution of the GDS as a stationary DBN, and perform inference and learning on the subsequent graph. We formally state the network of dynamical systems as a special case of the sequential GDS <ref type="bibr" target="#b26">(Mortveit and Reidys, 2001)</ref> with an observation function for each vertex.</p><p>Definition 1. Synchronous graph dynamical system (GDS). A synchronous GDS is a tuple (G, x n , y n , {f i }, {ψ i }) that consists of:</p><formula xml:id="formula_10">• a finite, directed graph G = (V, E) with edge-set E = {E i } and M vertices comprising the vertex set V = {V i }; • a multivariate state x n = ⟨x i n ⟩, composed of states for each vertex V i confined to a d i -dimensional manifold x i n ∈ M i ; • an M-variate observation y n = ⟨y i n ⟩, composed of scalar obser- vations for each vertex y i n ∈ R; • a set of local maps {f i } of the form f i : M → M i , which update</formula><p>synchronously and induce a global map f : M → M; and • a set of local observation functions {ψ 1 , ψ 2 , . . ., ψ M } of the form</p><formula xml:id="formula_11">ψ i : M i → R.</formula><p>Without loss of generality, we can use local functions to describe the time evolution of the subsystems:</p><formula xml:id="formula_12">x i n+1 = f i (x i n , ⟨x ij n ⟩ j ) + υ f i (5) y i n+1 = ψ i (x i n+1 ) + υ ψ i . (6)</formula><p>Here, υ f i is i.i.d. additive noise and υ ψ i is noise that is either i.i.d. or dependent on the state, i.e., υ ψ i (x i n+1 ). The subsystem dynamics [equation ( <ref type="formula">5</ref>)] are therefore a function of the subsystem state x i n and the subsystem parents' state ⟨x ij n ⟩ j at the previous time index such that f i : M i × j M ij → M i . Each subsystem observation is given by equation ( <ref type="formula">6</ref>). We assume the functions {f i } and {ψ i } are invariant w.r.t. time and thus the graph G is stationary.</p><p>The time evolution of a synchronous GDS can be modeled as a DBN. First, each subsystem vertex</p><formula xml:id="formula_13">V i = { X i n , Y i n }</formula><p>has an associated state variable X i n and observation variable Y i n ; the parents of subsystem V i are denoted Π G (V i ). Since the graph G → is stationary and synchronous, parents of X i n+1 come strictly from the preceding time slice, and additionally Π G → (Y i n+1 ) = X i n+1 . Thus, we can build the edge set E = {E 1 , E 2 , . . . , E M } in the GDS by means of the DBN. That is, each edge subset E i is built by the DBN edges</p><formula xml:id="formula_14">E i = {V j → V i : X j n ∈ Π G → (X i n+1 ) ∧ V j ∈ V \ V i }, so long as G forms a DAG. As an example, consider the syn- chronous GDS in Figure 1A. The subsystem V 3 is coupled to both subsystem V 1 and V 2 through the edge set E = {V 1 → V 3 , V 2 → V 3 }.</formula><p>The time-evolution of this network is shown in Figure <ref type="figure">1B</ref>, where the top two rows (processes X 1 and Y 1 ) are associated with subsystem V 1 , and similarly for V 2 and V 3 . The distributions for the state [equation ( <ref type="formula">5</ref>)] and observation [equation ( <ref type="formula">6</ref>)] of M arbitrary subsystems can therefore be factorized according to equation (1):</p><formula xml:id="formula_15">p B → (z n+1 |z n ) = M ∏ i=1 p D (x i n+1 |x i n , ⟨x ij n ⟩ j ) • p D (y i n+1 |x i n+1 ). (7)</formula><p>In the rest of the paper, we use simplified notation, given this constrained graph structure. First, since our focus is on learning coupling between distributed systems, the superscripts refer to individual subsystems, not variables. Thus, although the 2TBN</p><formula xml:id="formula_16">B → is constrained such that Π G → (Y i n ) = X i n , the notation Y ij n</formula><p>denotes the measurement variable of the jth parent of subsystem i, e.g., in Figure <ref type="figure">1</ref>, an arbitrary ordering of the parents gives</p><formula xml:id="formula_17">Y 3,1 n = Y 1 n and Y 3,2 n = Y 2 n .</formula><p>Second, the scoring functions for the 2TBN network B → can be computed independently of the prior network B 1 <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. We will assume the prior network is given, and focus on learning the 2TBN. As a result, we drop the subscript and note that all references to the network B are to the 2TBN. Since B → is stationary, learning B → is equivalent to learning the synchronous GDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LEARNING SYNCHRONOUS GDSs FROM DATA</head><p>In this section, we develop the theory for learning the synchronous update GDS from data. We will focus on techniques for learning graphical models using the score and search paradigm, the objective of which is to find a DAG G* that maximizes a score g(B : D). Given such a score, we can then employ established search procedures to find the optimal graph G*. Thus, we can state that our main goal is to derive a tractable scoring function g(B : D) for synchronous GDSs that gives a parsimonious model for describing the data.</p><p>To derive the score, we use the DBN formulation of synchronous GDSs (Sec. 4) to show that we cannot directly compute the posterior probability of the network structure (Sec. 5.1). By making some assumptions about the system, however, we are able to compute scores for GDSs by use of attractor reconstruction methods (Sec. 5.2). We conclude this section by giving an interpretation of the log-likelihood in terms of information transfer (Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head><p>FIGURE 1 | Representation of (A) the synchronous GDS with three vertices (V 1 , V 2 and V 3 ), and (B) the rolled-out DBN of the equivalent structure. Subsystem V 3 is coupled to both subsystems V 1 and V 2 by means of the edges between latent variables X 1 n → X 3 n+1 and X 2 n → X 3 n+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Structure Learning for DBNs</head><p>Ideally, we want to be able to compute the posterior probability of the network structure G, given data D. Using Bayes' rule, we can express this distribution as p (G|D) ∝ p (D|G) p (G), where p(G) encodes any prior assumptions we want to make about the network G. Thus, the problem becomes that of computing the likelihood of the data, given the model, p(D|G). The likelihood can be written in terms of distributions over network parameters <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>:</p><formula xml:id="formula_18">p(D|G) = ∫ p(D|G, Θ)p(Θ|G)dΘ,<label>(8)</label></formula><p>where we denote ℓ( ΘG : D) = log p(D|G, ΘG ) as the loglikelihood function for a choice of parameters ΘG that maximize p(D|G, Θ), given a graph G.</p><p>A common approach to compute equation ( <ref type="formula" target="#formula_18">8</ref>) in closed form is by using Dirichlet priors. This leads to the BD (Bayesian-Dirichlet) score and variants <ref type="bibr" target="#b14">(Heckerman et al., 1995;</ref><ref type="bibr" target="#b8">Friedman et al., 1998)</ref>. However, to obtain this analytic solution, we require counts of the tuples (z i n , π G (Z i n )), which involve hidden variables. We will instead use Schwarz's <ref type="bibr" target="#b32">(Schwarz, 1978)</ref> asymptotic approximation of the posterior distribution, which states that</p><formula xml:id="formula_19">lim N→∞ log p(D|G) ≈ ℓ( ΘG : D) - log N 2 C(G) + O(1),<label>(9)</label></formula><p>where C(G) is the model dimension (i.e., number of parameters needed for the graph G <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>) and O( <ref type="formula" target="#formula_2">1</ref>) is a constant bounded by the number of potential models. The approximation of the posterior [equation ( <ref type="formula" target="#formula_19">9</ref>)] requires that data come from an exponential family of likelihood functions with conjugate priors over the model G, and the parameters given the model Θ G <ref type="bibr" target="#b32">(Schwarz, 1978)</ref>. <ref type="bibr" target="#b0">Akaike (1974)</ref> gives a similar criterion by approximating the KL-divergence of any model from the data. We can compute both criteria in terms of the log-likelihood function ℓ( ΘG : D) and the model dimension C(G), and thus the problem can be generalized to that of deriving an information criterion for scoring the graph of the form</p><formula xml:id="formula_20">g(B : D) = ℓ( ΘG : D) -f(N) • C(G). (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>When f (N) = 1, we have the AIC score <ref type="bibr" target="#b0">(Akaike, 1974)</ref>; f (N) = log (N)/2 yields the BIC score <ref type="bibr" target="#b32">(Schwarz, 1978)</ref>, and f (N) = 0 gives the maximum likelihood score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Deriving the Scores for Synchronous GDSs</head><p>To calculate the information criterion [equation ( <ref type="formula" target="#formula_20">10</ref>)], we require tractable expressions for the log-likelihood function ℓ( ΘG : D) and the model dimension C(G). The form of the CPD in equation ( <ref type="formula">7</ref>) specifies these functions, and for equation ( <ref type="formula" target="#formula_19">9</ref>) to hold, this distribution must come from an exponential family <ref type="bibr" target="#b32">(Schwarz, 1978)</ref>. We do not assume the underlying model is linear-Gaussian or other known distributions, and thus express the log-likelihood as the maximum likelihood estimate for multinomial distributions <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. From equation ( <ref type="formula">7</ref>), the log-likelihood then decomposes as</p><formula xml:id="formula_22">ℓ( ΘG : D) = -N M ∑ i=1 ∑ x i n+1 ∑ ⟨x ij n ⟩ j p D (x i n+1 , x i n , ⟨x ij n ⟩ j ) log p D (x i n+1 |x i n , ⟨x ij n ⟩ j ) -N M ∑ i=1 ∑ x i n+1 ∑ y i n+1 p D (y i n+1 , x i n+1 ) log p D (y i n+1 |x i n+1 )<label>(11)</label></formula><p>Note that although we describe the states and observations as discrete in equation ( <ref type="formula" target="#formula_22">11</ref>), we assume the data are generated by a continuous and stationary process. In theory, it is conceivable to have access to an infinite dataset containing realizations of all potential states and observations. In practice, we have a limited dataset and therefore must implement a discretization scheme. Modeling the dynamical systems with non-parametric techniques requires that the number of parameters scales linearly in the size of the data, and thus C(G) scales linearly with N. Instead, later, we will assume the observation data are discretized, such that there are |Y i n | possible outcomes for an observed random variable Y i n . The log-likelihood function [equation ( <ref type="formula" target="#formula_22">11</ref>)] involves distributions over latent variables, and thus we resort to state-space (attractor) reconstruction. First, Lemma 1 shows that a future observation from a given subsystem can be predicted from a sequence of past observations. Building on this result, we present a computable formulation of the 2TBN distribution p B → (z n+1 |z n ) via Lemma 2. We then derive a tractable form of the log-likelihood function, presented in Lemma 1. It is then shown in Theorem 2 that these lemmas allow us to compute the information criterion equation ( <ref type="formula" target="#formula_20">10</ref>).</p><p>Lemma 1. Consider a synchronous GDS (G, x n , y n , {f i }, {ψ i }), where the graph G is a DAG. Each subsystem state follows the dynamics x i n+1 = f i (x i n , ⟨x ij n ⟩ j ) and emits an observation y i n+1 = ψ i (x i n+1 ); the subsystem observation can be estimated, for some map G i , by</p><formula xml:id="formula_23">y i n+1 = G i ( y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) . (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>Proof. Consider a forced system x n+1 = f (x n , w n ) with forcing dynamics w n+1 = h(w n ) and observation y n = ψ(x n+1 ). Given this type of forced system, the bundle delay embedding theorem <ref type="bibr" target="#b34">(Stark, 1999;</ref><ref type="bibr" target="#b35">Stark et al., 2003)</ref> states that the delay map</p><formula xml:id="formula_25">Φ Φ Φ f,h,ψ (x n , w n ) = y (κ) n</formula><p>is an embedding for generic f, ψ, and h. <ref type="bibr" target="#b34">Stark (1999)</ref> proved this result in the case of forcing dynamics h that are independent of the state x.<ref type="foot" target="#foot_1">foot_1</ref> For notational simplicity, we omit dependence on the noise process for the map Φ Φ Φ f,h,ψ ; the noise can be considered an additional forcing system so long as υ f is i.i.d and υ ψ is either i.i.d or dependent on the state <ref type="bibr" target="#b35">(Stark et al., 2003)</ref>.</p><p>Given a DAG G, any ancestor of the subsystem V i is not dependent on V i . As such, the sequence</p><formula xml:id="formula_26">y i,(κ i ) n = Φ Φ Φ f i ,⟨f ij ⟩ j ,ψ i ( x i n , ⟨x ij n ⟩ j )<label>(13)</label></formula><p>is an embedding, since ⟨x ij n ⟩ j is independent of x i n . Let ⟨x ijk n ⟩ k be the index-ordered set of parents of node X ij n (which itself is the jth parent of the node X i n ). Under the constraint that G is a DAG, where the state</p><formula xml:id="formula_27">x i n+1 = f i (x i n , ⟨x ij n ⟩ j ) + υ f i ,</formula><p>it follows from the bundle delay embedding theorem <ref type="bibr" target="#b34">(Stark, 1999;</ref><ref type="bibr" target="#b35">Stark et al., 2003)</ref> that there exists a map F i that is well defined and a diffeomorphism between observation sequences. From equation ( <ref type="formula" target="#formula_26">13</ref>), we can write this map</p><formula xml:id="formula_28">y i,(κ i ) n+1 = Φ Φ Φ f i ,⟨f ij ⟩ j ,ψ i ( f i ( x i n , ⟨x ij n ⟩ j ) , ⟨ f ij ( x ij n , ⟨x ijk n ⟩ k )⟩ j ) = Φ Φ Φ f i ,⟨f ij ⟩ j ,ψ i ( f i ( Φ Φ Φ -1 f i ,⟨f ij ⟩ j ,ψ i (y i,(κ i ) n ), ⟨ Φ Φ Φ -1 f ij ,⟨f ijk ⟩ k ,ψ ij (y ij,(κ ij ) n ) ⟩ j ) ) . (<label>14</label></formula><formula xml:id="formula_29">)</formula><p>Denote the RHS of equation ( <ref type="formula" target="#formula_28">14</ref>) as</p><formula xml:id="formula_30">F i (y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ); the last κ i + ∑ j κ ij components of F i are trivial. Denote the first component as G i : R κ i × j R κ ij → R, then we arrive at equation (12).</formula><p>Lemma 2. Given an observed dataset D = (y 1 , y 2 , . . ., y N ) where y n ∈ R M are generated by a directed and acyclic synchronous GDS (G, x n , y n , {f i }, {ψ i }), the 2TBN distribution can be written as</p><formula xml:id="formula_31">M ∏ i=1 p D (x i n+1 |x i n , ⟨x ij n ⟩ j ) • p D (y i n+1 |x i n+1 ) = ∏ M i=1 p D (y i n+1 |y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) p D (x n |⟨y i,(κ i ) n ⟩) . (<label>15</label></formula><formula xml:id="formula_32">)</formula><p>Proof. The generalized time delay embedding theorem <ref type="bibr" target="#b7">(Deyle and Sugihara, 2011)</ref> states that, under certain technical assumptions, and given M inhomogeneous observation functions {ψ 1 , ψ<ref type="foot" target="#foot_3">foot_3</ref> , . . . , ψ M }, the map</p><formula xml:id="formula_33">Φ Φ Φ f,ψ (x) = ⟨Φ Φ Φ f 1 ,ψ 1 (x), Φ Φ Φ f 2 ,ψ 2 (x), . . . , Φ Φ Φ f M ,ψ M (x)⟩<label>(16)</label></formula><p>is an embedding where each subsystem (local) map Φ Φ Φ f i ,ψ i : M → R κ i , and, at time index n is described by</p><formula xml:id="formula_34">Φ Φ Φ f i ,ψ i (x n ) = y i,(κ i ) n = ⟨ψ i (x n ) , ψ i (x n-τ i ), ψ i (x n-2τ i ), . . . , ψ i (x n-(κ i -1)τ i )⟩</formula><p>where <ref type="bibr" target="#b7">and Sugihara, 2011</ref>). 2 Therefore, the global map equation ( <ref type="formula" target="#formula_33">16</ref>) is given by Φ</p><formula xml:id="formula_35">∑ i κ i = 2d + 1 (Deyle</formula><formula xml:id="formula_36">Φ Φ f,ψ (x n ) = ⟨y i,(κ i ) n</formula><p>⟩ and there must exist an inverse map</p><formula xml:id="formula_37">x n = Φ Φ Φ -1 f,ψ ( ⟨y i,(κ i ) n ⟩ ) . Given Lemma 1, the existence of Φ Φ Φ -1 f,ψ , and since ∀i, {y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j } ⊆ ⟨y i,(κ i ) n</formula><p>⟩, we arrive at the following equation:</p><formula xml:id="formula_38">M ∏ i=1 p D ( Y i n+1 = G i ( y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) | y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) = p D ( X n = Φ Φ Φ -1 f,ψ ( ⟨y i,(κ i ) n ⟩ ) |⟨y i,(κ i ) n ⟩ ) × M ∏ i=1 p D ( X i n+1 = f i (x i n , ⟨x ij n ⟩ j ) | x i n , ⟨x ij n ⟩ j ) × M ∏ i=1 p D ( Y i n+1 = ψ i (x i n+1 )|x i n+1 ) . (<label>17</label></formula><formula xml:id="formula_39">)</formula><p>Rearranging equation ( <ref type="formula" target="#formula_38">17</ref>) gives the equality in equation ( <ref type="formula" target="#formula_31">15</ref>). Lemma 2 shows that the distributions can be reformulated by conditioning on delay vectors. The RHS of equation ( <ref type="formula" target="#formula_31">15</ref>) can be used to perform inference in the 2TBN (7). The numerator is a product of local CPDs of scalar variables, and can thus be computed by either counting (for discrete variables) or density estimation (for continuous variables). The denominator is used to compute the probability that the hidden state occured, given an observed delay vector; fortunately, <ref type="bibr" target="#b3">Casdagli et al. (1991)</ref> established methods to compute this CPDs for a variety of practical scenarios. Therefore, Lemma 2 provides a method to perform exact inference. Using this delay vector representation, we arrive at the following theorem.</p><p>Theorem 1. Consider a synchronous GDS (G, x n , y n , {f i }, {ψ i }), where the graph G is a DAG. Each subsystem state follows the dynamics x i n+1 = f i (x i n , ⟨x ij n ⟩ j ) and generates an observation y i n+1 = ψ i (x i n+1 ); a complete dataset is given by the sequence of observations D = (y 1 , y 2 , . . ., y N ). The log-likelihood of the data given a network structure can be computed in terms of conditional entropy:</p><formula xml:id="formula_40">ℓ( ΘG : D) = N • H(X n |⟨Y i,(κ i ) n ⟩) -N • M ∑ i=1 H(Y i n+1 |Y i,(κ i ) n , ⟨Y ij,(κ ij ) n ⟩ j )<label>(18)</label></formula><p>Proof. Substituting equation (15) into equation ( <ref type="formula" target="#formula_22">11</ref>) gives the log-likelihood ℓ( ΘG : D) as</p><formula xml:id="formula_41">N M ∑ i=1 ∑ y i n+1 ∑ y i,(κ i ) n ∑ ⟨y ij,(κ ij ) n ⟩ j p D (y i n+1 , y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) × log p D (y i n+1 |y i,(κ i ) n , ⟨y ij,(κ ij ) n ⟩ j ) -N M ∑ i=1 ∑ x n ∑ ⟨y i,(κ i ) n ⟩ p D (x n , ⟨y i,(κ i ) n ⟩)log p D (x n |⟨y i,(κ i ) n ⟩). (<label>19</label></formula><formula xml:id="formula_42">)</formula><p>In equation ( <ref type="formula" target="#formula_41">19</ref>), we have removed arguments of the joint distributions that will be nullified when multiplied with the CPD. Expressing equation ( <ref type="formula" target="#formula_41">19</ref>) in terms of conditional entropy [equation (3)], we arrive at equation ( <ref type="formula" target="#formula_40">18</ref>).</p><p>Theorem 2. The information criterion [equation ( <ref type="formula" target="#formula_20">10</ref>)] for synchronous GDS can be computed as:</p><formula xml:id="formula_43">g(B : D) = -N • M ∑ i=1 H(Y i n+1 |Y i,(κ i ) n , ⟨Y ij,(κ ij ) n ⟩ j ) -f(N) • M ∑ i=1   |Y i n | κ i • (|Y i n | -1) • ∏ V p ∈Π G (V i ) |Y p n | κ p   .</formula><p>(20)</p><p>Proof. The distributions for the first term in equation ( <ref type="formula" target="#formula_40">18</ref>) do not depend on the parents of a subsystem and thus are independent of the graph G being considered. Therefore, we have the following equation for maximum log-likelihood:</p><formula xml:id="formula_44">max G ℓ( ΘG : D) = O(N)-N•min G M ∑ i=1 H(Y i n+1 |Y i,(κ i ) n , ⟨Y ij,(κ ij ) n ⟩ j ).</formula><p>(21) We can now compute the number of parameters needed to specify the model as <ref type="bibr" target="#b8">(Friedman et al., 1998</ref>)</p><formula xml:id="formula_45">C(G) = M ∑ i=1   |Y i n | κ i • ( |Y i n | -1 ) • ∏ V p ∈Π G (V i ) |Y p n | κ p   . (<label>22</label></formula><formula xml:id="formula_46">)</formula><p>Since we are searching for the graph G* = max G g(B : D), holding N constant, we can substitute equation ( <ref type="formula">21</ref>) and equation ( <ref type="formula" target="#formula_45">22</ref>) into equation ( <ref type="formula" target="#formula_20">10</ref>) and ignore the constant term O(N) in (21).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The Log-Likelihood and Information Transfer</head><p>To conclude our study of the scores, we look at the log-likelihood in the context of information transfer. First, rearranging the terms of collective transfer entropy [equation ( <ref type="formula" target="#formula_8">4</ref>)], we can rewrite the log-likelihood function [equation ( <ref type="formula" target="#formula_40">18</ref>)], leading to the following result.</p><p>Proposition 1. The log-likelihood function for the synchronous GDS [equation ( <ref type="formula" target="#formula_40">18</ref>)] decomposes as follows:</p><formula xml:id="formula_47">ℓ( ΘG : D) = N • H(X n |⟨Y i,(κ i ) n ⟩) -N • M ∑ i=1 H(Y i n+1 |Y i,(κ i ) n ) + N • M ∑ i=1 T ⟨Y ij ⟩ j →Y i . (<label>23</label></formula><formula xml:id="formula_48">)</formula><p>Again, the first two terms in equation ( <ref type="formula" target="#formula_47">23</ref>) do not depend on the proposed graph structure, and thus maximizing loglikelihood is equivalent to maximizing collective transfer entropy. This becomes clear when we consider the log-likelihood ratio. This ratio quantifies the gain in likelihood by modeling the data D by a candidate network B instead of the empty network B ∅ , i.e.,</p><formula xml:id="formula_49">ℓ( ΘG : D) -ℓ( ΘG ∅ : D) ∝ log p(B|D) p(B ∅ |D) .</formula><p>Recall that the empty DAG G ∅ is one with no parents for all vertices ∀i, Π G ( </p><formula xml:id="formula_50">V i ) = ⟨Y ij,(κ ij ) n ⟩ j = ∅.</formula><formula xml:id="formula_51">: D) = N • M ∑ i=1 T ⟨Y ij ⟩ j →Y i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION AND FUTURE WORK</head><p>We have presented a principled method to score the structure of non-linear dynamical networks, where dynamical units are coupled via a DAG. We approached the problem by modeling the time evolution of a synchronous GDS as a DBN. We then derived the AIC and BIC scoring functions for the DBN based on time delay embedding theorems. Finally, we have shown that the log-likelihood of the synchronous GDS can be interpreted in the context of information transfer.</p><p>The representation of synchronous GDSs as DBNs allows for inference of coupling in dynamical networks and facilitates techniques for synthesis in these systems. DBNs are an expressive framework that allows representation of generic systems, as well as a numerous general purpose inference techniques that can be used for filtering, prediction, and smoothing <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. Our representation therefore allows for probabilistic reasoning for purposes of planning and prediction in complex systems.</p><p>Theorem 2 captures an interesting parallel between learning from complete data and learning non-linear dynamical networks. If the embedding dimension κ and time delay τ are unity, then the information criterion becomes identical to learning a DBN from complete data <ref type="bibr" target="#b8">(Friedman et al., 1998)</ref>. Thus, our result could be considered a generalization of typical structure learning procedures.</p><p>The results presented here provoke new insights into the concepts of structure learning, non-linear time series analysis, and effective network analysis <ref type="bibr" target="#b33">(Sporns et al., 2004;</ref><ref type="bibr" target="#b27">Park and Friston, 2013)</ref> based on information transfer <ref type="bibr" target="#b16">(Honey et al., 2007;</ref><ref type="bibr" target="#b22">Lizier et al., 2011;</ref><ref type="bibr" target="#b6">Cliff et al., 2013</ref><ref type="bibr" target="#b5">Cliff et al., , 2016))</ref>. The information-theoretic interpretation of the log-likelihood has interesting consequences in the context of information dynamics and information thermodynamics of non-linear dynamical networks. The transfer entropy terms in Propositions 1 and 2 show that the optimal structure of a synchronous GDS is immediately related to the information processing of distributed computation <ref type="bibr" target="#b23">(Lizier et al., 2008)</ref>, as well as the thermodynamic costs of information transfer <ref type="bibr" target="#b29">(Prokopenko and Lizier, 2014)</ref>.</p><p>In the future, we aim to perform empirical studies to exemplify the properties of the presented scoring functions. Specifically, the empirical studies should yield insight into the effect of weak, moderate and strong coupling between dynamical units. An important concept to consider in stochastic systems is the convergence of the shadow (reconstructed) manifold to the true manifold <ref type="bibr" target="#b36">(Sugihara et al., 2012)</ref>; we have implicitly accounted for this phenomena by using CPDs in our model, however, it is important to investigate the property of convergence with different density estimation techniques. In addition, we are interested in the effect of synchrony in these networks and the relationship to previous results for dynamical systems coupled by spanning trees <ref type="bibr" target="#b41">(Wu, 2005)</ref>. We conjecture that approach used here will allow us to derive scoring functions without the assumption of multinomial observations, and thus afford the use of non-parametric density estimators. Parametric techniques, such as learning the parameters of dynamical systems <ref type="bibr" target="#b11">(Ghahramani and Roweis, 1999;</ref><ref type="bibr" target="#b15">Hefny et al., 2015)</ref>, could be considered in place of the posterior approximations.</p><p>Finally, the reconstruction theorems used in this paper typically make the assumption that the map (or flow) is a diffeomorphism (invertible in time). Thus, given any state, the past and future are uniquely determined and the time delay τ can be taken positive or negative. In certain cases, however, the timereversed system is acausal, giving a map that is not time-invertible (an endomorphism). Ideally, we would aim to have methods to infer coupling for both endomorphisms and diffeomorphisms. <ref type="bibr" target="#b38">Takens (2002)</ref> showed that if the map is an endomorphism, taking the delay vector of temporally previous observations forms an embedding. The generalized theorems in <ref type="bibr" target="#b34">Stark (1999)</ref>, <ref type="bibr" target="#b35">Stark et al. (2003)</ref>, and <ref type="bibr" target="#b7">Deyle and Sugihara (2011)</ref>, however, were established for diffeomorphisms, rather than endomorphisms; we can only conjecture that taking a delay of past observations (as we have done throughout this paper) follows for these results. Empirical studies using the measures presented in this paper would indicate whether it is an important line of inquiry to prove the generalized reconstruction theorems for endomorphisms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The ratio of the log-likelihood [equation (18)] of a candidate DAG G to the empty network G ∅ can be expressed as ℓ( ΘG : D) -ℓ( ΘG ∅</figDesc><table><row><cell>definition into equation (18) [or, alternatively equation (23)] gives</cell></row><row><cell>the following result.</cell></row><row><cell>Proposition 2.</cell></row><row><cell>Substituting this</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Frontiers in Robotics and AI | www.frontiersin.org November 2016 | Volume 3 | Article 71</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p><ref type="bibr" target="#b34">Stark (1999)</ref> conjectures that the theorem should generalise to functions h that are not independent of x. To the best of our knowledge, this result remains to be proven.Frontiers in Robotics and AI | www.frontiersin.org November</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016" xml:id="foot_2"><p>| Volume 3 | Article 71</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>The original proof<ref type="bibr" target="#b7">(Deyle and Sugihara, 2011)</ref> uses positive lags; however, the authors note that the use of negative lags also applies [and should be used in the case of endomorphisms<ref type="bibr" target="#b38">(Takens, 2002)</ref>].</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank <rs type="person">Joseph Lizier</rs>, <rs type="person">Jürgen Jost</rs>, and <rs type="person">Wolfram Martens</rs> for many helpful discussions, particularly in regards to embedding theory. This work was supported in part by the <rs type="funder">Australian Centre for Field Robotics</rs>; the <rs type="funder">New South Wales Government</rs>; and the <rs type="funder">Faculty of Engineering &amp; Information Technologies, The University of Sydney</rs>, under the <rs type="programName">Faculty Research Cluster Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GtHcKF3">
					<orgName type="program" subtype="full">Faculty Research Cluster Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>OC co-wrote the manuscript, derived and proved the theorems, lemmas, and propositions. MP co-wrote the manuscript, assisted with the proofs, and supervised. RF co-wrote the manuscript, assisted with the proofs, and supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest Statement:</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><p>Copyright © 2016 <ref type="bibr">Cliff, Prokopenko and Fitch.</ref> This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.1974.1100705</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complex networks: structure and dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boccaletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-U</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physrep.2005.10.009</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rep</title>
		<imprint>
			<biblScope unit="volume">424</biblScope>
			<biblScope unit="page" from="175" to="308" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Properties of Bayesian belief network learning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bouckaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AUAI UAI</title>
		<meeting>of AUAI UAI<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State space reconstruction in the presence of noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Casdagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eubank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gibson</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2789(91)90222-U</idno>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="52" to="98" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of Bayesian-network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delayed spatio-temporal interactions and coherent structure in multiagent team dynamics</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Obst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<idno type="DOI">10.1162/ARTL_a_00221</idno>
	</analytic>
	<monogr>
		<title level="j">Art. Life</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards quantifying interaction networks in a football match</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Obst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RoboCup 2013: Robot World Cup XVII</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Visser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized theorems for nonlinear state space reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sugihara</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0018295</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18295</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the structure of dynamic probabilistic networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AUAI UAI</title>
		<meeting>of AUAI UAI<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online decentralized information gathering with spatial-temporal constraints</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukkarieh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10514-013-9369-5</idno>
	</analytic>
	<monogr>
		<title level="j">Auton. Robots</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning dynamic Bayesian networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Processing of Sequences and Data Structures</title>
		<title level="s">Lecture Notes in Comp. Sci</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</editor>
		<meeting><address><addrLine>Vietri sul Mare</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1387</biblScope>
			<biblScope unit="page" from="168" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning nonlinear dynamical systems using an EM algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 11</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="431" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
		<idno type="DOI">10.2307/1912791</idno>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1847" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: the combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022623210503</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="20" to="197" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised learning for dynamical system learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1963" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Network structure of cerebral cortex shapes functional connectivity on multiple time scales</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Honey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kötter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0701519104</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U.S.A</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="10240" to="10245" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of coupled map lattices</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kaneko</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.165869</idno>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="279" to="282" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Kantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Nonlinear Time Series Analysis</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Chirikov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cvitanović</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Moss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Swinney</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized synchronization, predictability, and equivalence of unidirectionally coupled dynamical systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Parlitz</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.76.1816</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">1816</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Bayesian belief networks: an approach based on the MDL principle</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8640.1994.tb00166.x</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="269" to="293" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multivariate information-theoretic measures reveal directed information structure and task relevant changes in fMRI connectivity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10827-010-0271-2</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="85" to="107" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local information transfer as a spatiotemporal filter for complex systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.77.026110</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E Stat. Nonlin. Soft. Matter. Phys</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">26110</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information modification and particle collisions in distributed computation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.3486801</idno>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37109" to="37113" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multivariate Construction of Effective Computational Networks from Observational Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
		<respStmt>
			<orgName>Max Planck Institute for Mathematics in the Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MIS-Preprint 25</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discrete, sequential dynamical systems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Mortveit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Reidys</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0012-365X(00)00115-1</idno>
	</analytic>
	<monogr>
		<title level="j">Discrete Math</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="281" to="295" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural and functional brain networks: from connections to cognition</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1238411</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page">1238411</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causal inference on discrete data using additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2011.71</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2436" to="2450" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transfer entropy and transient limits of computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep05394</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5394</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measuring information transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.85.461</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A statistical framework to infer delay and direction of information flow from measurements of complex systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wunderle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jäkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00756</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1555" to="1608" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Organization, development and function of complex brain networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Chialvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hilgetag</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2004.07.008</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="418" to="425" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Delay embeddings for forced systems. I. Deterministic forcing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<idno type="DOI">10.1007/s003329900072</idno>
	</analytic>
	<monogr>
		<title level="j">J. Nonlin. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="255" to="332" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Delay embeddings for forced systems. II. Stochastic forcing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huke</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00332-003-0534-4</idno>
	</analytic>
	<monogr>
		<title level="j">J. Nonlin. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="519" to="577" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting causality in complex ecosystems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sugihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fogarty</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1227079</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Dynamical Systems and Turbulence</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L-S</forename><surname>Young</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="366" to="381" />
			<date type="published" when="1981">1981</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Warwick</pubPlace>
		</imprint>
	</monogr>
	<note>Lecture Notes in Math</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The reconstruction theorem for endomorphisms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
		<idno type="DOI">10.1007/s005740200012</idno>
	</analytic>
	<monogr>
		<title level="j">Bull. Br. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="231" to="262" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable identification of stable positive systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Umenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Manchester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CDC</title>
		<meeting>of IEEE CDC<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transfer entropy -a model-free measure of effective connectivity for the neurosciences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10827-010-0262-3</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Neurosci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="45" to="67" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Synchronization in networks of nonlinear dynamical systems coupled via a directed graph</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1088/0951-7715/18/3/007</idno>
	</analytic>
	<monogr>
		<title level="j">Nonlinearity</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1057</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decentralized coordinated tracking with mixed discrete-continuous decisions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukkarieh</surname></persName>
		</author>
		<idno type="DOI">10.1002/rob.21471</idno>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="717" to="740" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
