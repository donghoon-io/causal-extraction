<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GroupedMixer: An Entropy Model with Group-wise Token-Mixers for Learned Image Compression</title>
				<funder>
					<orgName type="full">Strategic Research</orgName>
				</funder>
				<funder ref="#_psp8hUE">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_sZU2GKG">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_yU7qvy7">
					<orgName type="full">Chinese Academy of Engineering</orgName>
				</funder>
				<funder ref="#_W5VNsmv">
					<orgName type="full">Heilongjiang Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_ehvhKaj #_XEbm4PR #_eUjKuef #_Sfwwv9a">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-02">2 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yuanchao</forename><surname>Bai</surname></persName>
							<email>yuanchao.bai@hit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<email>cswangkai@stu.hit.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
							<email>jiangjun-jun@hit.edu</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beĳing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GroupedMixer: An Entropy Model with Group-wise Token-Mixers for Learned Image Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-02">2 May 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.01170v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Entropy Model</term>
					<term>Transformer</term>
					<term>Lossy Image Compression I. Introduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based entropy models have gained prominence in recent years due to their superior ability to capture long-range dependencies in probability distribution estimation compared to convolution-based methods. However, previous transformer-based entropy models suffer from a sluggish coding process due to pixel-wise autoregression or duplicated computation during inference. In this paper, we propose a novel transformer-based entropy model called GroupedMixer, which enjoys both faster coding speed and better compression performance than previous transformer-based methods. Specifically, our approach builds upon group-wise autoregression by first partitioning the latent variables into groups along spatial-channel dimensions, and then entropy coding the groups with the proposed transformer-based entropy model. The global causal selfattention is decomposed into more efficient group-wise interactions, implemented using inner-group and cross-group token-mixers. The inner-group token-mixer incorporates contextual elements within a group while the cross-group tokenmixer interacts with previously decoded groups. Alternate arrangement of two token-mixers enables global contextual reference. To further expedite the network inference, we introduce context cache optimization to GroupedMixer, which caches attention activation values in cross-group tokenmixers and avoids complex and duplicated computation. Experimental results demonstrate that the proposed Grouped-Mixer yields the state-of-the-art rate-distortion performance with fast compression speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L OSSY image compression is a long-standing and vi- tal research topic in signal processing and computer vision fields, which has been widely used to reduce the cost of storage and transmission. Due to the rapidly increasing user demand for online sharing and communication via prevalent applications such as WeChat, Twitter, etc., the number of images on the Internet has grown exponentially. Considering the enormous amount of image data nowadays, better compression methods are constantly sought to preserve higher fidelity under the desired bitrate constraint. On the other hand, a better compression method should also be computationally efficient, which enables high-throughput processing.</p><p>With the rapid advancement of deep learning, endto-end optimized lossy image compression has made remarkable progress over the last few years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Currently, state-of-the-art (SOTA) learning-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref> have achieved superior performance to that of traditional compression methods, such as VVC <ref type="bibr" target="#b23">[24]</ref>, which implies a promising future for learned image compression. Most learned image compression methods are based on an auto-encoder framework. The original input image is mapped to latent representations using nonlinear transforms and then quantized to discrete values. The discrete latent variables are encoded by arithmetic coding tools with the probability distribution estimated by an entropy model. As shown in <ref type="bibr" target="#b1">[2]</ref>, the final bitrate to losslessly compress the latent variables is the cross entropy between the prior and estimated distribution of the latent variables. It indicates that more accurate estimation leads to fewer bits required to compress an image, which motivates researchers to design more powerful entropy models.</p><p>Recently, in light of the success of Transformer in natural language processing <ref type="bibr" target="#b24">[25]</ref>, many researchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> have adopted transformer architectures in image compression field. Transformer-based entropy models, such as Entroformer <ref type="bibr" target="#b8">[9]</ref> and Contextformer <ref type="bibr" target="#b9">[10]</ref>, have gained prominence due to their superior ability to capture long-range dependencies in probability distribution estimation compared to CNNs. Initially, the latent variables are rearranged into a token sequence. TABLE I: Comparing GroupedMixer with other entropy models: "Conv." and "Trans." refer to the use of convolutional modules and transformer modules in entropy model. "Sp." and "ch." denote spatial and channel contexts. "G" is the group count, and "Dec." represents average encoding/decoding time for a 768 × 512 image, "Share" denotes that building different autoregressive steps and mining different types of correlations with a set of shared parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Type Context G Dec.(&lt;1s) Share</p><p>Autoregressive <ref type="bibr" target="#b4">[5]</ref> Conv. sp. HW</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group-based Entropy Model</head><p>Checkerboard <ref type="bibr" target="#b5">[6]</ref> Conv. sp.</p><p>2 ChARM <ref type="bibr" target="#b6">[7]</ref> Conv. ch. 10 ELIC <ref type="bibr" target="#b7">[8]</ref> Conv. sp.+ch. 10 TCM <ref type="bibr" target="#b10">[11]</ref> Trans. sp.+ch. 10 MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> Trans. sp.+ch. <ref type="bibr" target="#b19">20</ref> Autoregressive Transformer Entroformer <ref type="bibr" target="#b8">[9]</ref> Trans. sp. HW Entroformer-P <ref type="bibr" target="#b8">[9]</ref> Trans.</p><p>sp. Then, it is proposed to aggregate contexts with a stack of masked transformer decoder blocks, yielding large receptive fields, as well as content-adaptive transform. However, previous transformer-based entropy models still have limitations. As illustrated in Table <ref type="table">I</ref>, Entroformer <ref type="bibr" target="#b8">[9]</ref> only considers spatial correlation and suffers from a spatial serial decoding process. Its parallel variant Entroformer-P reduces decoding steps to 2, but the compression performance drops significantly. Compared to Entroformer, Contextformer <ref type="bibr" target="#b9">[10]</ref> further takes channel-wise correlation into account, but suffers from a even more complex spatial-channel serial coding process. Moreover, both Entroformer and Contextformer overlook redundant context computation in the attention layer during inference. All these factors make previous transformer-based entropy models time-consuming.</p><p>In this paper, we introduce a novel transformer-based entropy model with group-wise token-mixers, dubbed GroupedMixer, to surmount the aforementioned challenges. In GroupedMixer, we first partition the latent variables into groups along spatial and channel dimensions. Then, we employ two group-wise token-mixers, i.e., inner-group and cross-group token-mixers to integrate contextual information within each group and across previously decoded groups respectively. Moreover, we carefully design position encodings for the proposed group-wise token-mixers to provide spatialchannel positional information. Alternate arrangement of these two token-mixers enables global contextual ref-erence. We compare our approach with previous entropy models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b12">[13]</ref> to distinguish it from various aspects in Table <ref type="table">I</ref>. Firstly, our approach explores spatial-channel correlations existing in latent variables, as contrast to models focusing solely on spatial or channel dimensions <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Secondly, our method employs a transformer architecture for global referencing instead of convolution <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, implemented with decomposed attention mechanism with lower computation rather than full attention <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Thirdly, relying on group-wise autoregression instead of pixel-wise autoregression <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, our approach only requires a constant number of inferences during coding. Fourthly, in contrast to previous group-based entropy models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, our method shares weights across different group predictions. This approach enables more effective modeling of complex correlations. Additionally, we introduce context cache optimization to GroupedMixer to further accelerate the network inference, which caches attention activation values in cross-group token-mixer and avoids duplicated computation of contextual information. The context cache optimization not only enables acceleration of coding speed to under 1 second, but also effectively reduces multi-step fine-tuning time, which eliminates train-test gap and further boosts our performance. Experimental results demonstrate that our proposed Grouped-Mixer achieves SOTA RD performance, while maintaining fast coding speed.</p><p>The contributions of our work are summarized as follows:</p><p>• We present an entropy model named as Grouped-Mixer, a group-wise autoregressive transformer, which globally explores spatial-channel redundancies simultaneously. To lower the computation complexity, we propose to decompose the causal selfattention into two group-wise token-mixers, namely, inner-group and cross-group token-mixers. • To further accelerate the network inference, we introduce context cache optimization to Grouped-Mixer, which enables faster coding speed and also can be applied to reduce multi-step fine-tuning time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learned Image Compression</head><p>Learning-based image compression has witnessed remarkable advances in recent years, including lossless image compression <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>, near-lossless image compression <ref type="bibr" target="#b30">[31]</ref> and lossy image compression <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Learning-based lossy image compression originated from the work of Toderici et al. <ref type="bibr" target="#b0">[1]</ref>. Ballé et al. presented an end-to-end optimized framework based on CNN in <ref type="bibr" target="#b1">[2]</ref>, which can be formulated as variational auto-encoder (VAE) <ref type="bibr" target="#b31">[32]</ref>. Jiang et al. <ref type="bibr" target="#b3">[4]</ref> innovate with a convolutional neural network based end-to-end compression framework. Guo et al. <ref type="bibr" target="#b19">[20]</ref> present a novel causal context model based on CNNs to capture spatial-channel correlations in latent spaces. Wang et al. <ref type="bibr" target="#b21">[22]</ref> explore ratedistortion optimization in image compression through ensemble learning. Wu et al. <ref type="bibr" target="#b16">[17]</ref> introduce a blockbased hybrid image compression framework, enhancing efficiency and reducing memory issues through novel prediction and post-processing techniques. Finally, Bao et al. <ref type="bibr" target="#b20">[21]</ref> reveal a novel perspective on nonlinear transforms in learned image compression, proposing a nonlinear modulation-like transform inspired by communication techniques. In contrast to the papers listed, this study introduces GroupedMixer, a novel transformerbased entropy model for learned image compression. This model leverages group-wise autoregression and decomposes full self-attention into more efficient groupwise token-mixers. Additionally, it incorporates context cache optimization for faster inference. Our compression model adapts VAE framework as in <ref type="bibr" target="#b1">[2]</ref>. In this framework, given an input natural image x in distribution p x , an analysis transform g a is applied to decorrelate it into the compact latent representations y. The latent variables y are then quantized to discrete values ŷ and further losslessly compressed using entropy coding techniques with an estimated distribution p ŷ ( ŷ). At the decoder side, the received quantized latent variables ŷ are recovered to a reconstruction x using a synthesis transform g s . We formulate the above process as:</p><formula xml:id="formula_0">y = g a (x; ϕ), ŷ = Q(y), x = g s (ŷ; θ),<label>(1)</label></formula><p>where ϕ and θ denote the parameters of analysis transform and synthesis transform respectively. In learned image compression, the network can be trained with a loss which is a trade-off between rate and distortion as follows:</p><formula xml:id="formula_1">R + λ • D = E x∼px [-log 2 p ŷ ( ŷ)] + λ • E x∼px [d(x, x)],</formula><p>(2) where the first term denotes the rate of the latent variables, the second term measures the reconstruction quality with a predefined distortion metric d (e.g., PSNR and MS-SSIM), and λ is a Lagrange multiplier which controls the RD trade-off during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Entropy Model</head><p>Entropy model plays a crucial role in estimating the prior distribution of quantized latent variables p ŷ ( ŷ).</p><p>According to <ref type="bibr" target="#b1">[2]</ref>, more accurate estimation leads to lower bitrate. To capture general structural information in latent variables, Ballé et al. <ref type="bibr" target="#b1">[2]</ref> introduced hyperprior z, which is additionally encoded as side information to estimate distribution of ŷ. Minnen et al. <ref type="bibr" target="#b4">[5]</ref> proposed a context model based on masked convolution layer to exploit spatial redundancies in latent variables. Thus, equipped with hyperprior and context model, rate term in (2) can be reformulated as:</p><formula xml:id="formula_2">R = E x∼px i -log 2 p ŷi| ẑ, ŷ&lt;i (ŷ i | ẑ, ŷ&lt;i ; ψ) ,<label>(3)</label></formula><p>where ψ is the parameter of entropy model. Since then, many works have been proposed to improve the context model. We summarize prior efforts related to ours as following two main categories: 1) Global Prediction: Several studies have expanded the context model's receptive field beyond local areas. Guo et al. <ref type="bibr" target="#b19">[20]</ref> split latent representations along the channel dimension, using global prediction for the second group based on similarity calculations with the first group's elements. Entroformer <ref type="bibr" target="#b8">[9]</ref> developed a transformer-based context model leveraging global spatial correlations and accelerated it using a checkerboard-like parallelization. Contextformer <ref type="bibr" target="#b9">[10]</ref> also considered channel-wise correlations. In these transformer-based methods, latent variables are initially rearranged into token sequences. The introduction of stacked masked transformer decoder blocks in place of masked convolution layers offers global receptive fields and context-adaptive transformation. Stacking transformer blocks enhances feature collection and improves prediction accuracy.</p><p>2) Group-based Autoregression: Spatial autoregression, traditionally requiring serial coding, is accelerated through group-based methods. These methods divide latent variables into k groups across spatial-channel dimensions for parallelized coding within groups, leading to k-step inference. He et al. <ref type="bibr" target="#b5">[6]</ref> spatially split latent variables into two groups, employing checkerboardlike masked convolution for a two-step parallel model. ChARM <ref type="bibr" target="#b6">[7]</ref> segmented latent variables into k channel slices, introducing a channel-wise context model. ELIC <ref type="bibr" target="#b7">[8]</ref> developed an unevenly grouped channelconditional model combined with a checkerboard spatial context, effectively creating spatial-channel group-based models. However, these methods, often reliant on convolutional layers, are limited to local receptive fields and struggle with long-range dependencies. Recent studies have concentrated on enhancing group-wise autoregression approaches with transformer modules. TCM <ref type="bibr" target="#b10">[11]</ref> improved the predictive power of the ChARM network by integrating Swin-Transformer blocks. MLIC <ref type="bibr" target="#b11">[12]</ref> was designed to capture channel-wise features using CNNs and to address both local and global spatial correlations through masked transformer modules. An advancement over MLIC <ref type="bibr" target="#b11">[12]</ref>, MLIC++ <ref type="bibr" target="#b12">[13]</ref> further improved upon this by altering the network architecture and incorporating linear attention computation. It's important to note that previous group-based models, including TCM <ref type="bibr" target="#b10">[11]</ref> and MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, used various sets of CNNs and transformer modules to model each conditional distribution in the autoregressive model, differing fundamentally from our approach. Our entropy model, GroupedMixer, is a group-wise autoregressive transformer. This means that GroupedMixer shares weights across predictions of different groups by exploiting common predictive patterns. This sharedweight approach not only streamlines the model but also allows GroupedMixer to more effectively model complex correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Proposed Method</head><p>Given an input image x, we employ an analysis transform g a to decorrelate it into the latent variables y. The latent variables y are then quantized to discrete values ŷ and losslessly compressed using entropy coding techniques, such as arithmetic coding, based on an estimated distribution. At the decoder side, the received quantized latent variables ŷ are reconstructed to x using a synthesis transform g s . The latent variables are assumed to follow a single Gaussian distribution N (µ, σ) and the parameters µ, σ are jointly estimated with a hyperprior model and our proposed entropy model, GroupedMixer. In the following, we overview the network architecture of the proposed GroupedMixer and specify the details of the group-wise token-mixers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of GroupedMixer</head><p>We first introduce the network architecture of the proposed entropy model GroupedMixer, which is illustrated in Figure <ref type="figure" target="#fig_0">1a</ref>. Given a quantized latent representation ŷ ∈ R H×W ×C , we first separate it into a constant number of groups along both spatial and channel dimensions. We evenly split the latent variables into k c slices along the channel dimension, and then each channel slice is spatially segmented into k h × k w groups using a spatial splitting scheme, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We define the group partition function f and its inverse ungroup function f -1 as follows:</p><formula xml:id="formula_3">f : R H×W ×C → R G×hw×c , f -1 : R G×hw×c → R H×W ×C ,<label>(4)</label></formula><p>where the number of group is</p><formula xml:id="formula_4">G = k c • k h • k w and the size of each group is hw × c, h = H k h , w = W kw , c = C kc .</formula><p>We introduce two kinds of spatial splitting schemes, enabling 2 and 4 steps in the spatial autoregression respectively. For the 2-step splitting scheme, we adopt <ref type="bibr" target="#b5">[6]</ref> and partition the latent variables into a checkerboard pattern, which implies that k h = 1, k w = 2. For the 4-step splitting scheme, we select k h = 2, k w = 2 and divide latent variables as shown in Figure <ref type="figure" target="#fig_1">2</ref>. With these spatial splitting schemes, our context model can leverage bidirectional spatial contextual information. In the following sections, the notation k c × k h k w is consistently employed to represent the method of latent variables division. This denotes that the division process initially splits along the channel into k c slices, followed by a k h k w -step spatial division to further group each slice. In our study, two configurations of latent variables division are employed: 10 × 4 for the base model and 5 × 2 for the fast model, respectively.</p><p>After the grouping operation, the latent variables ŷ are divided into groups ŷG1 , ŷG2 , • • • , ŷG G , where G denotes the set of all indices of symbols in the latent variables and it satisfies</p><formula xml:id="formula_5">G = G 1 ∪ G 2 • • • ∪ G G .</formula><p>Each group of representation satisfies that y Gi ∈ R hw×c . Our entropy model factorizes the joint probability of the latent variables as a group-wise autoregression as follows:</p><formula xml:id="formula_6">p( ŷ | ẑ) = G i=1 p( ŷGi | ŷG&lt;i , ẑ).<label>(5)</label></formula><p>Compared with previous group-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, instead of modeling each conditional distribution with a separate network, we decouple the number of groups with network design and parameterize group-wise autoregressive model with a shared transformer architecture, GroupedMixer.</p><p>In GroupedMixer, instead of directly attending to all decoded values in previous groups, which is computationally intensive and usually impractical, we decompose the interactions with previous values into two tokenmixers: cross-group and inner-group token-mixers, to model global dependency across groups and within groups respectively. Specifically, we first map the cchannel representation into D dimensions using an input embedding layer. Then the grouped representation is fed to our transformer model, which consists of multiple GroupedMixer modules, as detailed in Figure <ref type="figure" target="#fig_0">1a</ref>. Each module comprises cross-group and inner-group tokenmixers. Given the grouped and mapped input feature X 1 ∈ R G×hw×D , a GroupedMixer module can be expressed as: where Y l and Z l are intermediate feature and output feature in l-th layer module. The whole GroupedMixer architecture is constructed by repeatedly stacking the inner-group and cross-group token-mixers.</p><formula xml:id="formula_7">Y l = Cross-Group Token-Mixer l (X l ), Z l = Inner-Group Token-Mixer l (Y l ),<label>(6)</label></formula><p>The output feature Z L ∈ R G×hw×D from the transformer blocks is shifted rightward along the second dimension, thus discarding the last group of features. A learnable parameter θ h ∈ R hw×1×D is then inserted at the first group's position, which is formulated as:</p><formula xml:id="formula_8">Z s L = [θ h , Z L (:, : G -1, :)],<label>(7)</label></formula><p>where [•] denotes concatenation operation along the second dimension. The motivation behind the shift operation is to ensure that the values of the current group is not accessible while predicting its parameters, aligning with the objectives of previous studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Then the shifted feature undergoes an output embedding layer to map it back to the original dimension c from D.</p><p>The representation is further concatenated with grouped hyper prior along the channel dimension and sent to parameter net. In parameter net, we first unsqueeze features to recover spatial dimension, from R G×hw×c to R G×c×h×w , and then aggregate hyperprior and context information within the same group using a stack of multiple convolution layers, and finally squeeze feature to original shape, as detailed in Figure <ref type="figure" target="#fig_2">3</ref>. At the end, the predicted parameters are rearranged to the same shape as the latent variables ŷ using ungroup operation f -1 and split into µ, σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Group-wise Token Mixers with Acceleration</head><p>We next specify the details of two token-mixers, and further propose a context cache optimization scheme to accelerate the network inference:</p><p>1) Inner-Group Token-Mixer: Inner-group token-mixer is designed to model the dependency within each group. As shown in the top subfigure of Figure <ref type="figure" target="#fig_0">1b</ref>, given the grouped representation Y ∈ R G×hw×D , we apply an attention process along the spatial dimension and share the weights across the group dimension. Specifically, Y is sent to a multi-head self-attention module to mix information along the spatial dimension and produce the output feature Y o ∈ R G×hw×D . In the attention mechanism, we necessitate position embedding to provide the following attention process with the information of the current position. Considering discrepancy of image resolution in training and testing phases, instead of using diamond-shape position embedding in <ref type="bibr" target="#b8">[9]</ref>, we introduce a position embedding generator (PEG) <ref type="bibr" target="#b32">[33]</ref> using depthwise convolution, which introduces inductive bias of translation invariance in training phase. We define our position encoding scheme as follows:</p><formula xml:id="formula_9">Y P = DWConv(Y ) + Y ,<label>(8)</label></formula><p>where the weights are initialized with zeros. We empirically find that the proposed position encoding scheme generalizes better on large resolution datasets in Section IV-F3. We then linearly project position-aware input Y P to queries, keys and values, and then split them into m representations of each head, yielding</p><formula xml:id="formula_10">Q P i , K P i , V P i ∈ R G×hw×d h , ∀i = 1, • • • , m</formula><p>, where d h is the number of head dimensions. For spatial dimension, Then we can formulate multi-head attention process as follows:</p><formula xml:id="formula_11">A i = σ Q P i K P ⊤ i √ d h V P i , Y o = W (A 1 ⊕ • • • ⊕ A m ) + Y ,<label>(9)</label></formula><p>where σ denotes the softmax function. Then Y o is sent to a feed-forward network to mix information along the channel dimension using two linear layers with expansion ratio 4 and produce the output feature Z ∈ R G×hw×D . In this way, different tokens in each group can be adequately mixed along spatial and channel dimensions for generating output features.</p><p>2) Cross-Group Token-Mixer: Cross-group token-mixer exchanges information across different groups, and thus makes the model to aggregate global spatial-channel information from previous decoded values. As shown in the bottom subfigure of Figure <ref type="figure" target="#fig_0">1b</ref>, the cross-group token-mixer is implemented by applying masked attention mechanism on the group dimension. For implementation, given the input feature X ∈ R G×hw×D , we first permute the group and spatial dimensions, and get the rearranged intermediate feature X r ∈ R hw×G×D . The input feature X r undergoes a masked attention process to integrate information across the group dimension while maintaining the same weights on the spatial dimension. We reformulate (9) as:</p><formula xml:id="formula_12">A i = σ Q i K ⊤ i + Q i P C √ d h + M V i (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where P C ∈ R G×G×d denotes relative position embedding, and M ∈ R G×G signifies the mask matrix, in which the lower triangle is zero and the rest is -∞. To encode position information for distinguishing each group, consider the group dimension as a 3D space with size (k c , k h , k w ). Each group index i is thus assigned a 3D coordinate (x i , y i , z i ). For indexes u, v, the relative position vector r u,v is computed as</p><formula xml:id="formula_14">r u,v = (x u -x v , y u -y v , z u -z v )</formula><p>, where each component of the vector</p><formula xml:id="formula_15">(x u -x v , y u -y v , z u -z v ) falls within the range [-k h + 1, k h -1], [-k w + 1, k w -1], [-k c + 1, k c -1]</formula><p>respectively, and is then mapped to a positive scalar s:</p><formula xml:id="formula_16">s = (z u -z v + k c -1) • (2k h + 1) • (2k w + 1) +(x u -x v +k h -1) • (2k w + 1) +y u -y v + k w -1.<label>(11)</label></formula><p>The relative position embedding P C u,v ∈ R d between group u and v can be indexed as:</p><formula xml:id="formula_17">P C u,v = W s,: ,<label>(12)</label></formula><p>where W ∈ R np×d is a 2D learnable parameter and initialized with truncated normal distribution and n p = (2k c -1) • (2k h -1) • (2k w -1). Similar to inner-group token-mixer, the output feature is sent to a MLP module to further mix information along the channel dimension and produce the output feature Y ∈ R G×hw×D . Note that this cross-group process implies that information is aggregated across accessible groups but restricted to the same spatial location, as illustrated in the bottom subfigure of Figure <ref type="figure" target="#fig_0">1b</ref>. By alternately stacking innergroup and cross-group token-mixers, the model can adequately mix information along the spatial, group and channel dimensions for capturing global information.</p><p>3) Context Cache Optimization for Acceleration: At inference time, we estimate the distribution of the latent variables using group-wise autoregression, which requires G times inference of the context model. However, the inference speed remains a challenge due to the large model size and intensive computation involved in the autoregressive process. One critical factor overlooked by previous works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> is the repeated context computation during inference, which implies that the network inference inputs a gradually enlarging context from 0 to G -1 groups of tokens at each of the G steps. Inspired by the attention cache optimization to expedite sequence generation <ref type="bibr" target="#b33">[34]</ref> in NLP, we propose context cache optimization for faster group-wise autoregressive distribution prediction, as illustrated in Fig- <ref type="figure" target="#fig_3">ure 4</ref>. Considering that only cross-group token-mixer performs interactions with previously decoded groups, the corresponding activations need to be cached and are utilized for being attended in later inference stages. Therefore, we cache activations including each group of keys and values in every cross-group token-mixers at each inference time. Before performing attention process, key and value produced by the current group are concatenated with cached activation values. Note that mask is canceled in attention because all concatenated decoded keys and values can be accessed at current step. The selfattention with context cache optimization in the crossgroup token-mixer at step t is implemented in (13):</p><formula xml:id="formula_18">Q t i = X t i W Q , K t i = [Cache_K t-1 , X t i W K ], V t i = [Cache_V t-1 , X t i W V ],</formula><p>A t i = σ(</p><formula xml:id="formula_19">Q t i K t⊤ i + Q t i P ⊤ c √ D )V t i ,<label>(13)</label></formula><p>where concatenated keys and values satisfy</p><formula xml:id="formula_20">Q t i ∈ R hw×1×D , K t i ∈ R hw×t×D , V t i ∈ R hw×t×D .</formula><p>Our context cache optimization diverges from <ref type="bibr" target="#b33">[34]</ref> in the following aspects: (1) Unlike the approach in <ref type="bibr" target="#b33">[34]</ref> where attention results from all blocks are stored, we cache attention activation values specific to group-wise autoregression in crossgroup token-mixers. (2) Our strategy operates at the group level, caching and concatenating intermediate activation values R hw×1×D for a group of tokens rather than individual tokens. These modifications not only tailor the attention activation cache concept to our GroupedMixer's architecture but also represent a novel application of this strategy. Equipped with context cache optimization, we only need to feed a single group of latent variables ŷGt to GroupedMixer each time instead of all decoded groups ŷG&lt;t . Thus, the inference process of GroupedMixer is significantly accelerated, leading to faster image coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis 1) Computation Complexity:</head><p>In the GroupedMixer module, instead of directly applying full self-attention process on all tokens which entails computational complexity:</p><formula xml:id="formula_21">O(Attn) = 4(Ghw)d 2 h + 2(Ghw) 2 d h . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>We decompose the global attention into two parts: inner-group and cross-group token-mixers. The computational complexity of the inner-group token-mixer is O(4(hw)d 2 h +2(hw) 2 d h ), and the computational complexity of the cross-group token-mixer is O(4Gd 2 h + 2G 2 d h ). The overall complexity of this decomposition is reduced to follows:</p><formula xml:id="formula_23">O(GM ) = 4(hw + G)d 2 h + 2((hw) 2 + G 2 )d h ,<label>(15)</label></formula><p>where the major computation and memory cost comes from the self-attention process in inner-group tokenmixer. This decomposition also enables us to scale to larger G, which is configured to 10, 40 in our implementation. To better understand the computational complexity, consider an image size of 768 × 512, downscaled to 32 × 48 latent codes and then grouped into 40 groups (10 × 4 configuration), retrieving h = 16, w = 24. The overall computational complexity of our GroupedMixer module is: </p><formula xml:id="formula_24">O(GM ) =</formula><p>2) Insights for Enhanced Performance: In this section, we delve deeper into the underlying reasons for the enhanced compression performance, increased coding speed, and reduced parameter count achieved by GroupedMixer. Contrasting with existing group-based entropy models, such as ChARM <ref type="bibr" target="#b6">[7]</ref>, ELIC <ref type="bibr" target="#b7">[8]</ref>, which employ convolutional layers to model dependencies, our approach leverages the transformer architecture to capture long-range dependencies, outperforming conventional methods in terms of performance. Moreover, our model significantly reduces the number of parameters by utilizing a shared transformer architecture, as opposed to modeling each group with distinct network parameters as in ChARM <ref type="bibr" target="#b6">[7]</ref>, ELIC <ref type="bibr" target="#b7">[8]</ref>, TCM <ref type="bibr" target="#b10">[11]</ref>, MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This design choice enables more effctive modeling complex correlations with the same amount of aprameters and enhances its scalability to larger group sizes. Different from previous transformer-based entropy models Entroformer <ref type="bibr" target="#b8">[9]</ref>, Contextformer <ref type="bibr" target="#b9">[10]</ref> that utilize pixelwise autoregression and full self-attention across all tokens, our model employs group-wise autoregression and innovatively splits the attention mechanism into inner-group and cross-group token-mixers. These designs decrease the number of autoregression steps and further lower computational complexity of each inference. Alongside with context cache optimization, they speed up inference, making a notable advancement over Entroformer <ref type="bibr" target="#b8">[9]</ref>, Contextformer <ref type="bibr" target="#b9">[10]</ref>. These also facilitate efficient multi-step fine-tuning, addressing train-test mismatch for improved performance, as shown in Section IV-F4. Our model addresses redundancies across the spatial and channel dimensions, rather than solely the spatial dimension as in Entroformer <ref type="bibr" target="#b8">[9]</ref>. Due to reducing computation complexity, our approach can segment latent variables into more slices across the channel dimension. This, coupled with carefully designed position embeddings, results in enhanced performance compared to Contextformer <ref type="bibr" target="#b9">[10]</ref>. Consequently, our model attains superior performance to earlier transformer-based entropy models with even less complex analysis and synthesis transform network, coupled with the benefits of faster inference speeds. This represents a significant advancement in the field of efficient learned image compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings 1) Implementation Details:</head><p>As our main contribution is to design a novel entropy model, we directly adopt the transform network design in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. We share the same analysis transform g a , synthesis transform g s , hyper analysis transform h a and hyper synthesis transform h s with ELIC-SM <ref type="bibr" target="#b7">[8]</ref> and TCM <ref type="bibr" target="#b10">[11]</ref>. We set the number of channels N = 192 and M = 320 for all models. For architecture of transformer, we employ depth L = 6, the embedding channels d e = 384 and the number of heads m = 12 as our default configuration. We initialize the weights of GroupedMixer with a truncated normal distribution as used in ViT <ref type="bibr" target="#b34">[35]</ref>. In our study, for rapid comparison, we employ a transform network identical to that in ELIC-SM and opt for the 10 × 4 partitioning method as the foundational variant (GroupedMixer). We also use the 5 × 2 method for a faster variant, termed GroupedMixer-Fast. To present the best performance on compression and speed, we substitute the transform network in GroupedMixer-Fast with a mixed Transformer-CNN architecture, as proposed in TCM <ref type="bibr" target="#b10">[11]</ref>. This modified version is designated as GroupedMixer-Large. For training, we use the Adam <ref type="bibr" target="#b35">[36]</ref> optimizer with β 1 = 0.9, β 2 = 0.999. All models are trained for over 2M iterations. Gradient clipping technique is also employed for stable training and the clip value is set to 1.0. We optimize our models with rate-distortion loss as Equation <ref type="bibr" target="#b1">(2)</ref>. During training, a Lagrange multiplier λ balances the rate-distortion trade-off. When optimizing for MSE, we train different models with values of λ ∈ {4, 8, 16, 32, 75, 150, 300, 450, 900, 1500} × 10 -4 , where D = MSE(x, x). When optimizing for MS-SSIM, we utilize λ ∈ {2, 4, 8, 16, 32, 64}, where D = 1-MS-SSIM(x, x).</p><p>Due to the repeated invocation of the entropy coder during decoding in the group-based entropy model, we have achieved speed acceleration by optimizing the calling interface, based on CompressAI <ref type="bibr" target="#b36">[37]</ref>. Specifically, we modified the data transfer method used when calling the entropy coder, changing it from list transfer to tensor transfer, which speeds up entropy coding process. In our method, tensor reorganization is a crucial step to align data for subsequent processing. This is achieved through a rearrangement operation, which we implement using the einops.rearrange function <ref type="bibr" target="#b37">[38]</ref>. This function allows us to efficiently transform the multi-dimensional structure of our data tensors into the desired format. Our model is implemented based on Pytorch 2.0 <ref type="bibr" target="#b38">[39]</ref>, utilizing torch.nn.functional.scaled_dot_product_attention to accelerate the computation process of attention.</p><p>2) Datasets: Our compression model is trained on randomly selected 5 × 10 4 images from DIV2K <ref type="bibr" target="#b39">[40]</ref>, FLICKR 2K <ref type="bibr" target="#b40">[41]</ref>, ImageNet <ref type="bibr" target="#b41">[42]</ref> with resolution larger than 480 × 480 as our training set. We evaluate the rate-distortion performance on three publicly available datasets: Kodak dataset <ref type="bibr" target="#b42">[43]</ref> with 24 images with a size of 768 × 512, Tecnick dataset <ref type="bibr" target="#b43">[44]</ref> with 100 images with a size of 1200 × 1200, and CLIC'21 Test dataset <ref type="bibr" target="#b44">[45]</ref> with 60 images with a size of 2k resolution. We measure the distortion of reconstruction using PSNR and MS-SSIM, and convert MS-SSIM to -log 10 (1-MS-SSIM) for clearer comparison. For CLIC <ref type="bibr">'21 Test [45]</ref> and Tecnick <ref type="bibr" target="#b43">[44]</ref>, we adhere to the testing protocol employed in CompressAI <ref type="bibr" target="#b36">[37]</ref>, where the images are padded to multiples of 64 beforehand, while the rate is derived from padded images and distortion is evaluated on images without padding.</p><p>3) Three-Stage Training Scheme: Following previous works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref> and community discussion 1 , we encode ⌈y -µ⌋ into bitstream which enhances RD performance, and thus the decoder side receives the reconstructed latent variables ŷ = ⌈y -µ⌋ + µ. To train a group-based transformer, we adapt a three-stage training strategy. In the first stage, we employ one-pass training scheme for faster forward process, and feed differentiable estimation y + U(-1 2 , 1 2 ) to the context model, STE(y) to the decoder, where U(-1 2 , 1 2 ) is the uniform distribution and STE(•) is the straight-through estimator <ref type="bibr" target="#b18">[19]</ref>. The original images are cropped to 256 × 256 patches before being fed into the network. Minibatches of 16 of these patches are used to update the network parameters and the learning rate is set to 1 × 10 -4 . The first training stage lasts for 1.2M-step iterations. From the second stage on, we enlarge the size of patches from 256 to 384 to train our transformer for better global context modeling ability and batch size is set to 8. The learning rate drops to 5 × 10 -5 at the beginning of this stage, and drops to 1 × 10 -5 at 1.5M steps. The second stage lasts for 0.6M iterations. In the third stage, we apply multi-step fine-tuning to close the train-test gap, which improves performance for group-based entropy models. Specifically, we conduct G times inference of context model to retrieve real reconstruction ŷ = STE(y -µ)+µ, and feed it to both the context model and the decoder. The learning rate starts at 4 × 10 -5 and the learning rate is then divided by 2 when the evaluation loss reaches a plateau (we use a patience of 1 × 10 4 iterations). This stage finishes after at most 1M steps or when the learning rate is decayed below a certain threshold value, which is set to 1 × 10 -6 . Thanks to the context cache optimization introduced in section III-C1, our model can be trained with less computation and memory cost in this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rate-Distortion Performance</head><p>We compare our method with other learned image compression methods, including MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, TCM <ref type="bibr" target="#b10">[11]</ref>, Contextformer <ref type="bibr" target="#b9">[10]</ref>, Entroformer <ref type="bibr" target="#b8">[9]</ref>, ELIC, ELIC-SM <ref type="bibr" target="#b7">[8]</ref>, STF <ref type="bibr" target="#b22">[23]</ref>, ChARM <ref type="bibr" target="#b6">[7]</ref>, Xie et al. <ref type="bibr" target="#b17">[18]</ref>, Cheng et al. <ref type="bibr" target="#b2">[3]</ref>, Minnen et al. <ref type="bibr" target="#b4">[5]</ref>, as well as some traditional codecs, like VVC intra (VTM 17.0) <ref type="bibr" target="#b23">[24]</ref> and BPG <ref type="bibr" target="#b45">[46]</ref>.  For ChARM <ref type="bibr" target="#b6">[7]</ref>, we reproduce the method based on CompressAI <ref type="bibr" target="#b36">[37]</ref> for fair comparison. For ELIC, ELIC-SM <ref type="bibr" target="#b7">[8]</ref>, we use publicly available unofficial implementation provided by <ref type="bibr" target="#b46">[47]</ref>, which is based on Compres-sAI <ref type="bibr" target="#b36">[37]</ref>. For <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b4">[5]</ref>, we evaluate the codes provided by CompressAI <ref type="bibr" target="#b36">[37]</ref>. The other results are obtained from their official repositories or emails with authors.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the rate-distortion performance on the Kodak dataset with the model optimized for MSE and MS-SSIM respectively. We also report BD-rates reduction of our method and state-of-the-art learned methods in Table <ref type="table" target="#tab_3">II</ref>. The BD-rate is computed from the Rate-PSNR curve with VTM as the anchor method. This demonstrates that our methods outperform all listed methods in terms of PSNR and MS-SSIM. Our Grouped-Mixer, GroupedMixer-Fast, GroupedMixer-Large reduce BD-rates by 8.31%, 6.03% and 17.81% on Kodak dataset over VVC when measured in PSNR. From Figure <ref type="figure" target="#fig_4">5</ref> we can see, the performance gain is more significant for higher bitrates. Compared to ELIC-SM <ref type="bibr" target="#b7">[8]</ref>, our Grouped-Mixer, which employs the same transform network as ELIC-SM, achieves a 7.24% BD-Rate saving. Additionally, it is capable of increasing the PSNR by up to approximately 0.5dB at the same bitrate. Compared with TCM <ref type="bibr" target="#b10">[11]</ref>, our GroupedMixer-Large, utilizing the same transform network as TCM, realizes a 6.07% BD-Rate saving. This demonstrates that our proposed entropy model surpasses the convolution-based entropy model presented in ELIC <ref type="bibr" target="#b7">[8]</ref> and the Swin-Transformer Attention based entropy model used in TCM <ref type="bibr" target="#b10">[11]</ref>. Compared with previous SOTA MLIC++ <ref type="bibr" target="#b12">[13]</ref>, our GroupedMixer-Large also achieves 2.74% BD-Rate saving. Compared with Entroformer, our GroupedMixer achieves 11.04% BD-rate reduction. Even equipped with a lighter transform network than Contextformer <ref type="bibr" target="#b9">[10]</ref> and ELIC <ref type="bibr" target="#b7">[8]</ref>, our GroupedMixer still outperforms them by 1.07% and 0.87% BD-rate reduction respectively, especially at higher bitrate regions.</p><p>The performance evaluation results on high-resolution image datasets CLIC'21 Test and Tecnick are respectively presented in Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref>. As shown, our approach surpasses all other codecs. The GroupedMixer-Large model demonstrates superior performance with BD-rate savings of 19.77% and 22.56% on the CLIC'21 Test and Tecnick datasets, respectively. This marks a significant improvement over the previous SOTA method, MLIC++ <ref type="bibr" target="#b12">[13]</ref>, which achieved BD-rate savings of 15.60% and 18.36% on these datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Complexity</head><p>We report encoding/decoding latency and number of parameters in Table <ref type="table" target="#tab_3">II</ref>. For evaluation of model speed, we limit the accessible computation resources to one 3090 GPU card and AMD Ryzen 9 5900X 12-Core CPU. We use same testing protocol as ELIC <ref type="bibr" target="#b7">[8]</ref> to measure the latency. Note that entropy coding details is not mentioned in ELIC <ref type="bibr" target="#b7">[8]</ref>, so we directly use the publicly accessible reimplementation <ref type="bibr" target="#b46">[47]</ref> to test coding latency. The table reports total and individual latencies for transform and entropy coding time respectively, averaged on all images in Kodak dataset. For computation of entropy coding time, we count inference time of hyperprior, context model and entropy coder.</p><p>As shown in Table <ref type="table" target="#tab_3">II</ref>, our method not only achieves SOTA RD performance but also excels in efficiency. When compared with other transformer-based entropy models, our approach demonstrates significantly faster speeds. For instance, our base model records average encoding and decoding times of 0.27s and 0.26s, respectively. This is 466 times faster than Entroformer <ref type="bibr" target="#b8">[9]</ref> and 169 times faster than Contextformer <ref type="bibr" target="#b9">[10]</ref> in decoding. Our GroupedMixer-Fast variant further improves these speeds to 0.07s for encoding and 0.06s for decoding. It even surpasses ELIC-SM <ref type="bibr" target="#b7">[8]</ref> in decoding speed while maintaining superior RD performance. These enhancements are primarily due to the context cache optimization and our refined entropy coding process, leading to a fivefold and twofold increase in decoding speed respectively.</p><p>Additionally, our experiments on varying image sizes reveal a direct correlation between resolution and coding time in GroupedMixer. For example, decoding time for a 256 × 256 image is 241.15 ms, increasing to 1755.38 ms for a 2048 × 2048 image. In contrast, the GroupedMixer-Fast variant shows a rise from 59.39 ms to 1147.26 ms for these resolutions. Notably, the latency growth rate accelerates with higher resolutions. Since the time complexity is predominantly influenced by the innergroup block complexity, which is proportional to the square of the number of latent variables.</p><p>In Table <ref type="table" target="#tab_3">II</ref>, we demonstrate that weight sharing across groups maintains a nearly constant parameter count, even as the number of groups increases. In fact, due to reductions in the input/output embedding layer size, the parameter count may decrease. This represents an improvement over models like ChARM <ref type="bibr" target="#b6">[7]</ref>, STF <ref type="bibr" target="#b22">[23]</ref>, TCM <ref type="bibr" target="#b10">[11]</ref> and MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, as our entropy model conserves parameters effectively, even with more groups. Compared to previous models like ELIC <ref type="bibr" target="#b7">[8]</ref>, Entroformer <ref type="bibr" target="#b8">[9]</ref>, and Contextformer <ref type="bibr" target="#b9">[10]</ref>, GroupedMixer stands out for its superior RD performance, achieved with a comparable or lower total parameter count and a lighter transform network. Detailed analysis reveals that our GroupedMixer has slightly more parameters than Contextformer <ref type="bibr" target="#b9">[10]</ref> (18.3M vs. 15.9M), and the hyperprior network does carry more parameters (10.5M vs. 4.0M). Although our context model and hyperprior network are slightly more parameter-intensive compared to Contextformer <ref type="bibr" target="#b9">[10]</ref>, our strategic choices enable us to maintain leading encoding speeds, a more crucial aspect in practical applications. We can observe that enhancing the transform network's capacity increases inference latency marginally, but significantly boosts performance. Our GroupedMixer-Large surpasses both TCM <ref type="bibr" target="#b10">[11]</ref> and MLIC <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> in performance, even with fewer parameters. Specifically, our model saves 43.9M parameters compared to MLIC++ <ref type="bibr" target="#b12">[13]</ref> and achieves better compression performance, demonstrating the efficient modeling capabilities of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Progressive Decoding</head><p>In literature, learned image compression with groupbased entropy model can also be applied to implement progressive coding <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In this section, we also evaluate the ability of our method on progressive decoding. We directly adapt our model (10 × 4 variant) trained with MSE distortion and λ = 0.045 for progressive decoding without any additional post-training. Assuming that the decoder side already receives bitstreams of k latent groups, where k = 0 denotes the case that no bitstream of ŷ but only ẑ is received. We first reconstruct k groups of latent variables from bitstreams using entropy coding based on the predicted distribution. For the remaining G -k groups, we view the context model as a generative model and sample  Fig. <ref type="figure" target="#fig_0">10</ref>: Comparative study of token-Mixer order, autoregression order, position embedding, and multi-step fine-tuning. In this table, "i-c" vs. "c-i" represent innergroup vs. cross-group token-mixer orders. "sfo" (spatialfirst-order) and "cfo" (channel-first-order) refer to different autoregression orders. "Rel", "Abs" and "DRel" is relative, absolute and diamond-shape relative position embedding. "ms-ft" stands for multi-step fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Token-Mixer Order Study:</head><p>We design two different token-mixers to construct the GroupedMixer Module. To investigate the most efficient way to combine them, we evaluated the model using 2 permutations. As depicted in Figure <ref type="figure" target="#fig_0">10a</ref>, it is beneficial to position the cross-group token-mixer prior to the inner-group token-mixer, as opposed to the converse arrangement. This preference may be attributed to the fact that tokens in cross-group token-mixer, whether derived from their proximate spatial positions or otherwise, demonstrate an increased capacity for aggregating more complex information.</p><p>2) Spatial-Channel Order: Partitioning the latent variables into groups enables two different autoregression orders, namely, spatial-first-order (sfo) and channel-firstorder (cfo), which can be expressed as follows respectively:</p><formula xml:id="formula_26">f sfo : R H×W ×C → R kck h kw× H k h W kw × C kc f cfo : R H×W ×C → R k h kwkc× H k h W kw × C kc (18)</formula><p>Different coding orders prioritize contextual information from different dimensions (spatial or channel), which may result in different compression performance. The comparison result is illustrated in Figure <ref type="figure" target="#fig_0">10a</ref>. On average, the channel-first-order (cfo) performs almost the same as the model in spatial-first-order (sfo) configuration. We speculate that two different partition methods provide two almost equivalent contextual information in the amortized sense. We adopt the spatial-first-order (sfo) in our final model.</p><p>3) Position Embedding: We first examine the effects of position embedding used in cross-group token-mixer. We build models with different position encoding methods, including absolute position embedding, 3D relative position embedding and diamond-shape 3D relative position embedding, which extends diamond position embedding proposed in <ref type="bibr" target="#b8">[9]</ref> from 2D to 3D. The results are presented in Figure <ref type="figure" target="#fig_0">10b</ref>. We observe that the 3D relative position embedding outperforms the others. Moreover, we notice that introducing region constraint in crossgroup token-mixer is neither necessary nor effective. Hence, we adopt 3D relative position embedding in our final model. In inner-group token-mixer, PEG plays a vital role in generalizing model to high resolution images. We also conduct an experiment to verify this, where configuration follows IV-A. We compare PEG with previous proposed effective 2D position embedding, diamond position embedding proposed in <ref type="bibr" target="#b8">[9]</ref>. As shown in Figure <ref type="figure" target="#fig_0">10c</ref>, even though the two models, one with PEG and the other without PEG, perform very closely on the Kodak dataset, they show significant differences on the larger resolution dataset CLIC'21 Test, with the model without PEG performing much worse than the one with PEG. This is because PEG is implemented based on depth-wise convolution, which allows for better generalization across images of varying scales compared to diamond position embedding.</p><p>4) Multi-step Fine-tuning: To assess the impact of multi-step fine-tuning on training performance, an ablation study on multi-step fine-tuning was conducted, where configuration follows Section IV-A, as shown in Figure <ref type="figure" target="#fig_0">10d</ref> In this comparison, "w. ms-ft" represents the outcome with the final multi-step fine-tuning, while "w/o. ms-ft" corresponds to the results where the third stage was replaced with a one-pass training approach. It was observed that employing multi-step fine-tuning in the training process effectively narrowed the gap between training and testing performance, leading to an overall improvement. Quantitatively, this approach resulted in a reduction of 1.39% in BD-Rate, underscoring the effectiveness of multi-step fine-tuning in enhancing model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Depth and Embedding Dimension:</head><p>In Table <ref type="table" target="#tab_6">IV</ref> we conduct architecture ablations to examine the relationships between model architecture and compression performance. We explore the following architectural changes: decreasing the number of hidden dimension, holding model depth relatively constant L = 3; decreasing model depth, holding the hidden dimension constant d e = 384. The head dimension d h is consistently set to a constant 32. Our results show that decreasing the hidden dimension size leads to performance deterioration, causing 1.68 % BD-Rate increase from d e = 384 to d e = 192 and 9.09% BD-Rate increase from d e = 384 to d e = 96. Likewise, reducing the model depth impairs the performance, leading to 8.98% BD-Rate increase from L = 6 to L = 1 and 2.83% BD-Rate increase from L = 6 to L = 3. This is because both the hidden dimension and the depth are critical factors for the model, which influence the model's capacity to encode the contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Conclusion</head><p>In this paper, we present an entropy model called GroupedMixer, a group-wise autoregressive transformer for learned image compression. To construct a spatialchannel context model that enables global reference and also entails lower complexity, we propose to decompose self-attention into two group-wise token-mixers, namely, inner-group and cross-group token-mixers. We then apply context cache optimization to expedite inference speed, which allows faster coding process and multi-step finetuning. We demonstrate that GroupedMixer achieves SOTA compression performance with fast coding speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) GroupedMixer overview. Preprocessed latent representations ŷ are then passed through GroupedMixer modules to aggregate group-wise context, and are finally projected as distribution parameters. (b) Illustration of token-mixers. Cross-group token-mixer mixes the information between previously decoded groups, while innergroup token-mixer mixes the information within groups. (c) Detailed network architectures of two token-mixers, where MSA represents multi-head self-attention, and PEG denotes position embedding generator.</figDesc><graphic coords="4,72.99,56.07,466.02,215.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Grouping scheme for modeling spatial-channel context. The latent representations are separated along channel and spatial dimensions into G = k c • k h • k w groups sequentially, and number indicates order of autoregression. In this figure, we use (k c , k h , k w ) = (2, 2, 2) as an example.</figDesc><graphic coords="4,336.55,359.82,201.91,189.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Detailed structure of Parameters Net. c is the number of channels in each group.</figDesc><graphic coords="5,336.55,56.07,201.91,89.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Context cache optimization at inference time. Each block denotes a group of attention activation values.</figDesc><graphic coords="6,112.70,56.07,123.60,187.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Performance evaluation on the Kodak dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Performance evaluation on the CLIC'21 Test dataset. Fig. 7: Performance evaluation on the Tecnick dataset.</figDesc><graphic coords="9,64.66,243.55,232.44,155.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) k = 0 (0.007 / 14.72) (b) k = 1 (0.069 / 17.78) (c) k = 2 (0.125 / 18.64) (d) k = 3 (0.235 / 19.97) (e) k = 4 (0.429 / 21.19) (f) k = 8 (0.601 / 22.21) (g) k = 16 (0.756 /23.00) (h) k = 24 (1.009 / 26.16) (i) k = 32 (1.207 / 29.04) (j) k = 40 (1.412 / 36.21) F. Model Analysis In this section, we conduct more experiments to further analyze GroupedMixer. In the following experiments, for rapid validation, we adopt simpler network architecture and training scheme. Specifically, we select the base configuration of GroupedMixer as {L = 3, d e = 384, m = 12} and use the variant 5 × 2 as default unless otherwise specified. For the training strategy, we first train our model with λ = 0.045 for 1M iterations with base learning rate 1 × 10 -4 . Then, we fine-tune the model with λ = 0.03, 0.015 respectively, covering λ = 0.015, 0.03, 0.045 regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,78.30,56.07,455.40,200.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Evaluation of different methods on the Kodak dataset. "BD-Rate" is computed with VTM as anchor method. "Tot. Enc." and "Tot. Dec." indicate total encoding and decoding times in seconds (s), "T. Dec." covers network transformation time during decoding, and "E. Dec." includes entropy coding time (hyper prior, context model, arithmetic coding). Total parameter ("Tot. Param.") are in millions (M), with "T. Param." and "E. Param." for transformation and entropy model parameters, respectively. * marks results from original papers, † denotes our reimplemented model tests. "CCO" refers to context cache optimization.</figDesc><table><row><cell>Model</cell><cell>BD-Rate(%)</cell><cell>Tot. Enc.</cell><cell cols="2">Latency(s) Tot. Dec. T. Dec.</cell><cell>E. Dec.</cell><cell>Tot. Param.</cell><cell>Params(M) T. Param.</cell><cell>E. Param.</cell></row><row><cell>VVC [24]</cell><cell>0.00</cell><cell>129.21</cell><cell>0.14</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>Minnen [5]</cell><cell>10.90</cell><cell>1.98</cell><cell>4.60</cell><cell>6e-3</cell><cell>4.59</cell><cell>25.5</cell><cell>7.0</cell><cell>18.5</cell></row><row><cell>Cheng [3]</cell><cell>5.44</cell><cell>1.98</cell><cell>4.69</cell><cell>0.03</cell><cell>4.66</cell><cell>29.6</cell><cell>19.0</cell><cell>10.6</cell></row><row><cell>Xie [18]</cell><cell>-0.78</cell><cell>2.93</cell><cell>6.00</cell><cell>1.92</cell><cell>4.08</cell><cell>50.0</cell><cell>39.3</cell><cell>10.7</cell></row><row><cell>Group-based Entropy Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChARM [7]</cell><cell>1.38</cell><cell>0.08  †</cell><cell>0.09  †</cell><cell>6e-3  †</cell><cell>0.08  †</cell><cell>126.7  †</cell><cell>7.0  †</cell><cell>119.7  †</cell></row><row><cell>STF [23]</cell><cell>-4.31</cell><cell>0.14</cell><cell>0.13</cell><cell>0.03</cell><cell>0.10</cell><cell>99.9</cell><cell>13.8</cell><cell>86.1</cell></row><row><cell>ELIC-SM [8]</cell><cell>-1.07</cell><cell>0.06  †</cell><cell>0.08  †</cell><cell>6e-3  †</cell><cell>0.07  †</cell><cell>29.8  †</cell><cell>7.6  †</cell><cell>22.2  †</cell></row><row><cell>ELIC [8]</cell><cell>-7.24</cell><cell>0.07  †</cell><cell>0.09  †</cell><cell>0.02  †</cell><cell>0.07  †</cell><cell>36.9  †</cell><cell>14.6  †</cell><cell>22.2  †</cell></row><row><cell>TCM [11]</cell><cell>-11.74</cell><cell>0.16</cell><cell>0.15</cell><cell>0.07</cell><cell>0.08</cell><cell>76.6</cell><cell>28.9</cell><cell>47.7</cell></row><row><cell>MLIC+ [12]</cell><cell>-13.11</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>MLIC++ [13]</cell><cell>-15.07</cell><cell>0.16</cell><cell>0.18</cell><cell>0.02</cell><cell>0.16</cell><cell>116.7</cell><cell>16.4</cell><cell>100.3</cell></row><row><cell>Autoregressive Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Entroformer [9]</cell><cell>2.73</cell><cell>3.86</cell><cell>121.19</cell><cell>0.03</cell><cell>121.16</cell><cell>45.0</cell><cell>7.6</cell><cell>37.4</cell></row><row><cell>Entroformer-P [9]</cell><cell>5.50</cell><cell>4.28</cell><cell>7.95</cell><cell>0.03</cell><cell>7.93</cell><cell>45.0</cell><cell>7.6</cell><cell>37.4</cell></row><row><cell>Contextformer [10]</cell><cell>-7.44</cell><cell>40  *</cell><cell>44  *</cell><cell>/</cell><cell>/</cell><cell>37.4  *</cell><cell>17.5  *</cell><cell>19.9  *</cell></row><row><cell cols="2">Group-wise Autoregressive Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GroupedMixer</cell><cell>-8.31</cell><cell>0.27</cell><cell>0.26</cell><cell>6e-3</cell><cell>0.25</cell><cell>36.4</cell><cell>7.6</cell><cell>28.8</cell></row><row><cell>-w/o CCO</cell><cell>-</cell><cell>1.06</cell><cell>1.01</cell><cell>6e-3</cell><cell>1.01</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GroupedMixer-Fast</cell><cell>-6.03</cell><cell>0.07</cell><cell>0.06</cell><cell>6e-3</cell><cell>0.05</cell><cell>36.8</cell><cell>7.6</cell><cell>29.2</cell></row><row><cell>GroupedMixer-Large</cell><cell>-17.84</cell><cell>0.17</cell><cell>0.17</cell><cell>0.07</cell><cell>0.10</cell><cell>72.8</cell><cell>28.9</cell><cell>43.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Speed performance analysis of two models at varying resolutions. This comparison focuses on the latency metrics in milliseconds (ms) for both encoding and decoding processes across different image resolutions.</figDesc><table><row><cell>Model</cell><cell>Size</cell><cell>Latency(</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>ms) Tot. Enc. Tot. Dec. T. Dec. E. Dec.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>256 × 256</cell><cell>242.40</cell><cell>241.15</cell><cell>1.46</cell><cell>239.69</cell></row><row><cell>GM</cell><cell cols="2">512 × 512 1024 × 1024 363.20 256.52</cell><cell>251.34 356.63</cell><cell cols="2">4.27 15.50 341.12 247.07</cell></row><row><cell></cell><cell cols="3">2048 × 2048 1814.28 1755.38</cell><cell cols="2">59.31 1696.07</cell></row><row><cell></cell><cell>256 × 256</cell><cell>60.90</cell><cell>59.39</cell><cell>1.71</cell><cell>57.68</cell></row><row><cell>GM-Fast</cell><cell cols="2">512 × 512 1024 × 1024 172.44 68.67</cell><cell>63.26 158.91</cell><cell cols="2">4.43 15.64 143.28 58.82</cell></row><row><cell></cell><cell cols="3">2048 × 2048 1218.46 1147.26</cell><cell cols="2">59.23 1088.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of different depths and dimensions for transformer blocks used in GroupedMixer.</figDesc><table><row><cell cols="5">Model Depth Dimension BD-Rate(%) Param.</cell></row><row><cell></cell><cell>3</cell><cell>96</cell><cell>11.92</cell><cell>12.5M</cell></row><row><cell></cell><cell>3</cell><cell>192</cell><cell>4.51</cell><cell>14.5M</cell></row><row><cell>5 × 2</cell><cell>1</cell><cell>384</cell><cell>8.98</cell><cell>15.4M</cell></row><row><cell></cell><cell>3</cell><cell>384</cell><cell>2.83</cell><cell>22.6M</cell></row><row><cell></cell><cell>6</cell><cell>384</cell><cell>0.00</cell><cell>36.8M</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by <rs type="funder">National Key Research and Development Program of China</rs> under Grant <rs type="grantNumber">2022YFF1202104</rs>, in part by <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">62301188</rs>, <rs type="grantNumber">92270116</rs>, <rs type="grantNumber">62071155</rs> and <rs type="grantNumber">U23B2009</rs>, in part by the <rs type="funder">Strategic Research</rs>, and Consulting Project by the <rs type="funder">Chinese Academy of Engineering</rs> under Grant <rs type="grantNumber">2023-XY-39</rs>, in part by <rs type="funder">China Postdoctoral Science Foundation</rs> under Grant <rs type="grantNumber">2022M710958</rs>, and in part by <rs type="funder">Heilongjiang Postdoctoral Science Foundation</rs> under Grant <rs type="grantNumber">LBH-Z22156</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_psp8hUE">
					<idno type="grant-number">2022YFF1202104</idno>
				</org>
				<org type="funding" xml:id="_ehvhKaj">
					<idno type="grant-number">62301188</idno>
				</org>
				<org type="funding" xml:id="_XEbm4PR">
					<idno type="grant-number">92270116</idno>
				</org>
				<org type="funding" xml:id="_eUjKuef">
					<idno type="grant-number">62071155</idno>
				</org>
				<org type="funding" xml:id="_Sfwwv9a">
					<idno type="grant-number">U23B2009</idno>
				</org>
				<org type="funding" xml:id="_yU7qvy7">
					<idno type="grant-number">2023-XY-39</idno>
				</org>
				<org type="funding" xml:id="_sZU2GKG">
					<idno type="grant-number">2022M710958</idno>
				</org>
				<org type="funding" xml:id="_W5VNsmv">
					<idno type="grant-number">LBH-Z22156</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variable rate image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06085</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learned image compression with discretized gaussian mixture likelihoods and attention modules</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7939" to="7948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An endto-end compression framework based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3007" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint autoregressive and hierarchical priors for learned image compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Checkerboard Context Model for Efficient Learned Image Compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">775</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Channel-Wise Autoregressive Entropy Models for Learned Image Compression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="3339" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entroformer: A transformer-based entropy model for learned image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextformer: A transformer with spatio-channel attention for context modeling in learned image compression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Koyuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gaikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Steinbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learned image compression with mixed transformer-cnn architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">397</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mlic: Multi-reference entropy model for learned image compression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7618" to="7627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MLIC$^{++}$: Linear complexity multireference entropy modeling for learned image compression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hxIpcSoz2t" />
	</analytic>
	<monogr>
		<title level="m">ICML 2023 Workshop Neural Compression: From Information Theory to Applications</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning end-to-end lossy image compression: A benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4194" to="4211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning accurate entropy model with global reference for image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural image compression via attentional multi-scale back projection and frequency decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned blockbased hybrid image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3978" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced invertible encoding for learned image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM international conference on multimedia</title>
		<meeting>the 29th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International C/ballonference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal contextual prediction for learned image compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2329" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear Transforms in Learned Image Compression From a Communication Perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2023-04">Apr. 2023</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1922" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble Learning-Based Rate-Distortion Optimization for End-to-End Image Compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021-03">Mar. 2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1193" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Devil Is in the Details: Window-based Attention for Image Compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">480</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overview of the Versatile Video Coding (VVC) Standard and its Applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021-10">Oct. 2021</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3736" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards end-to-end image compression and analysis with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep lossy plus residual coding for lossless and near-lossless image compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3577" to="3594" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bit-swap: Recursive bits-back coding for lossless compression with hierarchical latent variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3408" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Practical lossless compression with latent variables using bits back coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">iflow: Numerically invertible flows for efficient lossless compression via a uniform coder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5822" to="5833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning scalable ℓ∞-constrained near-lossless image compression via joint lossy image and residual compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page">955</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fastseq: Make sequence generation faster</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhendawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Compressai: a pytorch library and evaluation platform for end-to-end compression research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bégaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Racapé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feltman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pushparaja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Einops: Clear and reliable tensor manipulations with einstein-like notation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogozhnikov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oapKSVM2bcj" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">853</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (ĲCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kodak lossless true color image suite</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kodak</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>photocd pcd0992</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Testimages: a large-scale archive for testing visual devices and basic image processing algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Asuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STAG</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">CLIC Challenge on Learned Image Compression</orgName>
		</author>
		<ptr target="http://compression.cc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bpg image format</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/bpg" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unofficial elic</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="https://github.com/JiangWeibeta/ELIC" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enhanced Invertible Encoding for Learned Image Compression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
