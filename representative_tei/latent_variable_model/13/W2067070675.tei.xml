<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_pT6EXEG #_CEuqF4Y">
					<orgName type="full">MI 48109, USA</orgName>
				</funder>
				<funder ref="#_EyEYJMc">
					<orgName type="full">ARO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-layer graph analytics for social networks Brandon Oselio, Student Member, IEEE, Alex Kulesza, Member, IEEE, Alfred O. Hero, III, Fellow, IEEE Abstract-Modern social networks frequently encompass multiple distinct types of connectivity information; for instance, explicitly acknowledged friend relationships might complement behavioral measures that link users according to their actions or interests. One way to represent these networks is as multi-layer graphs, where each layer contains a unique set of edges over the same underlying vertices (users). Edges in different layers typically have related but distinct semantics; depending on the application, multiple layers might be used to reduce noise through averaging, perform multifaceted analyses, or a combination of the two. However, it is not obvious how to extend standard graph analysis techniques to the multi-layer setting in a flexible way. In this paper we develop latent variable models and methods for mining multi-layer networks for connectivity patterns based on noisy data. Index Terms-Hypergraphs, multigraphs, mixture graphical models, Pareto optimality Multi-layer networks arise naturally when we have more than one source of connectivity information for a group of users. In a social networking context, we often have knowledge of direct communication links, i.e., relational information. However, we might also derive behavioral relationships based on user actions or interests. The question that this paper attempts to address is how to deal with these multiple layers of a social network when attempting to perform tasks like inference, clustering, and anomaly detection.</p><p>We propose a generative hierarchical latent-variable model for multi-layer networks, and show how to perform inference on its parameters. Using techniques from Bayesian Model Averaging <ref type="bibr" target="#b0">[1]</ref>, we conditionally decouple the layers of the network using a latent selection variable; this makes it possible to write the posterior probability of the latent variables given the multi-layer network. The resulting mixture can be viewed as a scalarization of a multi-objective optimization problem <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. When the posterior probability functions are convex, the scalarization of the multiobjective problem is both optimal and consistent with the Bayesian context <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>We then step back from the Bayesian setting and discuss how multi-objective optimization can be used to perform MAP estimation of the desired latent variables. Using the concept of Pareto optimality <ref type="bibr" target="#b3">[4]</ref>, we can define an entire front of solutions; this allows a user to define a preference over optimization functions and tune the algorithm accordingly. The result is a level of supervised optimization and inference that still utilizes the structure of multi-layer networks.</p><p>We perform experiments on a simulated example, showing that our method yields improved clustering performance in noisy conditions. We discuss how our framework can be combined with existing models, and describe the details of this process for the dynamic stochastic block model (DSBM) <ref type="bibr" target="#b5">[6]</ref>, which captures a variety of complex temporal network phenomena. Finally, we apply the multi-layer DSBM to a real-world data set drawn from the ENRON email corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. MULTI-LAYER NETWORKS</head><p>A multi-layer graph G = (V, E) comprises vertices V = {v 1 , . . . , v p }, common to all layers, and edges E = (E 1 , . . . , E L ) in L layers, where E i is the edge set for layer i. We write A i ∈ {0, 1} p×p to denote the adjacency matrix of layer i:</p><formula xml:id="formula_0">[A i ] uv = I[(u, v) ∈ E i ].</formula><p>We will assume that the data observed in practice are noisy reflections of this true underlying multi-layer graph, and we denote by W i ∈ R p×p the observed adjacency weight matrix. In some cases W i might be binary, reflecting merely the presence or absence of an observed connection-for instance, whether two users were seen to communicate. In other settings, such as measuring temporal or content correlation scores between users, the entries of W i could be real-valued. Note that the observed matrix W i may depend on A j for i = j; see Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HIERARCHICAL MODEL DESCRIPTION</head><p>We wish to estimate A 1 , . . . , A L given the observations W 1 , . . . , W L . Using standard parametric methods this will require computing posterior distributions of A 1 , . . . , A L , which may be quite complex since the layers are coupled.</p><p>Instead, we propose a modified hierarchical model that simplifies the inference procedure. For simplicity, let us specialize to the case where L = 2. (For instance, imagine the setting described in the introduction: one layer of the network represents the observed extrinsic relationships between users, and the other their correlated intrinsic behaviors.)</p><p>We first introduce a latent variable denoted Y (see Figure <ref type="figure">2</ref>) that allows the model to continue to express coupling between layers while conditionally decoupling their posterior distributions:</p><formula xml:id="formula_1">P (W 1 , W 2 |A 1 , A 2 , Y ) = P (W 1 |A 1 , Y )P (W 2 |A 2 , Y ) . (1)</formula><p>Since the variables A 1 , . . . , A L are now intermediaries between Y and the observed weight matrices, we will simplify by collapsing them into W 1 , . . . , W L and simply inferring</p><formula xml:id="formula_2">Y Latent variables A 1 W 2 W 1 A 2 Y Observed matrices Fig. 2.</formula><p>Latent variable model. The latent variable Y determines the distributions of the adjacency matrices and, through them, the observation matrices.</p><p>itself. (If desired, we can reconstruct A 1 , . . . , A L later once we know the distribution of Y .) Decomposing Y = (W, Z), we end up with the graphical model in Figure <ref type="figure" target="#fig_1">3</ref>, where W ∈ R p×p is a latent adjacency (or similarity) matrix describing the underlying connections between vertices, and Z ∈ {1, 2} is a model selection variable with P (Z = 1) = α and P (Z = 2) = 1 -α.</p><p>Here we are making the implicit assumption that there is a common connectivity structure W that informs all layers of the network; due to the different attributes of each layer, they may reveal this underlying structure in different ways, or obfuscate it altogether. In a sense the model produces observed matrices that correspond to multiple views of the latent variable W . The model selection variable Z will decouple the posterior distribution of W given both layers into a weighted sum of marginalized posteriors given each individual layer.</p><p>The distributions P (W 1 |W, Z) and P (W 2 |W, Z) are in general task-dependent (e.g., they could be Gaussian, Wishart, Bernoulli, etc.), but we will make the simplifying assumption that Z acts as a selector variable, so that W and W 1 are conditionally independent Z = 2, and likewise W and W 2 are conditionally independent when Z = 1. Formally, using the notation P z to denote conditioning on Z = z, we have</p><formula xml:id="formula_3">P 2 (W 1 |W ) = P 2 (W 1 ) (2) P 1 (W 2 |W ) = P 1 (W 2 ) .<label>(3)</label></formula><p>We are interested in the posterior distribution of the latent variable W given the observed variables W 1 , W 2 :</p><formula xml:id="formula_4">P (W |W 1 , W 2 ) = ξP (W |W 1 , W 2 , Z = 1) + (1 -ξ)P (W |W 1 , W 2 , Z = 2) ,<label>(4)</label></formula><p>where ξ = P (Z = 1|W 1 , W 2 ). Let's consider the first term. We have</p><formula xml:id="formula_5">P (W |W 1 , W 2 , Z = 1) = P (W )P 1 (W 1 |W )P 1 (W 2 ) Ŵ P ( Ŵ )P 1 (W 1 | Ŵ )P 1 (W 2 )</formula><p>.</p><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>Since P 1 (W 2 ) does not depend on W , it factors out of the sum in the denominator and cancels; thus we have</p><formula xml:id="formula_7">P (W |W 1 , W 2 ) (6) = ξ P (W )P 1 (W 1 |W ) P 1 (W 1 ) + (1 -ξ) P (W )P 2 (W 2 |W ) P 2 (W 2 ) (7) = P (W ) [γ 1 P 1 (W 1 |W ) + γ 2 P 2 (W 2 |W )] ,<label>(8)</label></formula><p>where γ 1 = ξ/P 1 (W 1 ) and γ 2 = (1-ξ)/P 2 (W 2 ) are constants with respect to W . If we assume the prior on W is uniform, then the MAP value of W is also the maximum likelihood estimate, and can be written as</p><formula xml:id="formula_8">Ŵ = argmax W [γ 1 P 1 (W 1 |W ) + γ 2 P 2 (W 2 |W )] . (9)</formula><p>For example, assume that both P (W 1 |W ) and P (W 2 |W ) are distributed as isometric Gaussians, i.e.,</p><formula xml:id="formula_9">P (W 1 |W ) = N (W, σ 2 1 I p ) (10) P (W 2 |W ) = N (W, σ 2 2 I p ) . (<label>11</label></formula><formula xml:id="formula_10">)</formula><p>Then the solution to Equation 9 has the form</p><formula xml:id="formula_11">Ŵ = βW 1 + (1 -β)W 2<label>(12)</label></formula><p>for some 0 ≤ β ≤ 1.</p><p>The above describes not only one MAP estimate of W , but rather a family of MAP estimates based on the priors assigned to each model by α (which affects ξ and γ in turn). Qualitatively, this can be viewed as a relative confidence measure on the layers; if W 1 is more trustworthy than W 2 , then the best choice of α would be greater than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PARETO SUMMARIZATIONS</head><p>Of course, in practice it may be difficult to effectively set the prior α directly; as a result it might be more useful to generate the entire family of possible solutions and then choose among them afterward. We can view this procedure as a special case of a general framework that offers more flexibility in the inference and estimation procedure.</p><p>Let us consider a straightforward multi-objective optimization problem</p><formula xml:id="formula_12">Ŵ = argmin W [f 1 (W ), f 2 (W )] . (<label>13</label></formula><formula xml:id="formula_13">)</formula><p>For the model derived in the previous section, we have</p><formula xml:id="formula_14">f 1 (W ) = -P 1 (W 1 |W ) and f 2 (W ) = -P 2 (W 2 |W ).</formula><p>One potential approach to the multi-objective optimization problem above is scalarization of the two objective functions, so that the new problem to be solved is</p><formula xml:id="formula_15">Ŵ = argmin W γf 1 (W ) + (1 -γ)f 2 (W ) .<label>(14)</label></formula><p>This view leads to the objective in Equation 9. However, this can be a somewhat naïve approach to this optimization problem, as potentially valuable solutions may be discarded. A more general notion is Pareto optimality. A solution to a multi-objective optimization problem is said to be weakly Pareto optimal (or weakly non-dominated) if it is not possible to improve any objective function without worsening some other objective function <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. More formally, we say that a solution W dominates a solution W if f i (W ) ≤ f i (W ) for every objective function f i and there exists some j such that f j (W ) &lt; f j (W ). The first Pareto front is the set of weakly non-dominated points.</p><p>In general, the scalarization technique described above identifies a subset of Pareto optimal points. This subset is complete in some cases; for instance, if the solution space is a convex set and the individual objective functions are convex functions, scalarization gives the full Pareto front <ref type="bibr" target="#b4">[5]</ref>. However, when such convexity conditions are not met, the scalarization technique yields an incomplete family of solutions. In our setting, the posterior distributions in Equation 13 are frequently non-convex. Thus, by employing the concept of Pareto optimality, we are extending our list of possible optimal solutions, and generalizing the MAP estimate of Equation <ref type="formula">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION EXAMPLE</head><p>We use simulations to show that clustering of nodes in a weighted graph can be improved using the MAP estimate of W . Two random graphs with 500 nodes are constructed with 10 known clusters. The weights between nodes in the same cluster are normally distributed as N (5, 0.5), and weights between nodes that are not in the same cluster are normally distributed as N (4.7, 0.5). Both layers come from this underlying similarity structure, but are corrupted with i.i.d. Gaussian noise with zero mean and different variances σ 1 and σ 2 . For various choices of β, the networks are clustered using a normalizedcut spectral clustering algorithm, and the Adjusted Rand Indices (ARI) <ref type="bibr" target="#b6">[7]</ref> are computed. For each of several different levels of variance, this experiment is run 50 times, and the results are averaged. Figure <ref type="figure" target="#fig_2">4</ref> shows a plot of the results, and Table <ref type="table">I</ref> reports optimal values of β. These results show that using Equation 9 to estimate the mixture of networks improves the clustering. Note that even with unequal variance, optimal β is consistently near 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ENRON EXAMPLE</head><p>We next look at the ENRON email data set 1 . This data set consists of approximately half a million email messages sent or To explore dynamic multi-level structure, we create two layers from the ENRON dataset over a series of time periods. The information that builds the layers is chosen so that one layer represents the extrinsic, "relational" information between users, and the other represents intrinsic, "behavioral" information between users.</p><p>First, a relational network is recovered from the headers of emails by identifying the sender and receiver(s) of each message, including Cc and Bcc recipients. For each week in the dataset, a separate network of employees is constructed from the emails sent during that week. A second set of behavioral networks are recovered using the contents of email messages. On the same weekly basis the contents of all emails originating from each user are combined to form long "documents", for which term frequency-inverse document frequency (TF-IDF) scores are calculated <ref type="bibr" target="#b7">[8]</ref>. Using the vector of TF-IDF scores for each user, we then apply the standard metric of cosine similarity and obtain a symmetric matrix that forms the second observed layer for a given week.</p><p>In order to perform inference on this node set, we employ the dynamic stochastic block model (DSBM) <ref type="bibr" target="#b5">[6]</ref>. This method infers the probabilities of connection inside and outside of communities, and treats members of the same community as statistically identical. It then propagates this model through time using an extended Kalman filter structure. Since we wish to use this framework, it is necessary to transform the weighted edge network into a binary network. To do this, the similarity scores are thresholded. To be roughly consistent with the density of the relational network, we keep the top 15% greatest correlations between users at each time step, setting all other connections to 0. This allows us to create networks of similar sparsity level. The above procedure yields a two-layer binary dynamic network that we can use to obtain insight into the structural dynamics of the ENRON data. For the DSBM structure, we group employees by their role in the company (CEO, President, Director, etc.).</p><p>Combining the two networks as in Section II, we run the DSBM for different levels of the mixing parameter α. Because of the use of binary networks in this example, the α parameter is used as the probability that the combined data will choose to use the relational network when the two layers disagree with each other. The objective in this particular example is to show that using this method we can not only reduce noise, but also discover interesting multifaceted behavior that is not obvious from one layer alone.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the betweeness centrality of the Directors group over time as the mixing parameter is varied. In general, the centrality measure increases approximately monotonically as α is varied; however, from week 95 to week 115, betweenness centrality is significantly increased when using a combined dynamic network-that is, an intermediate value of α. This time corresponds to the beginning of the company's upheaval and public disclosure of troubles. Perhaps by examining both network layers simultaneously we have removed some of the edges between other classes, and thus the centrality score of this particular group increased. It is true that during this time, when overall email usage increased, the betweenness centrality measure went down, as there were more shortest paths through users from other groups. Using the combination of layers, however, there appears to be an increase in the number of shortest paths through the Directors group.</p><p>On the other hand, we can also see well-behaved monotonic correlations in some cases. Figure <ref type="figure" target="#fig_4">6</ref> shows a transition of degree centrality for the class of CEOs (of which there were four during this time period). The behavioral network shows more connectivity for the CEO class. This phenomenon makes sense, as the behavioral data takes into account all written documents, which could be correlated with those of other users, while the relational network only takes into account direct communication between the CEOs and others. In reality, much of that communication is performed through third parties (such as assistants), and thus CEOs probably do not send as much email as the average employee. Increasingly anomalous behavior occurs toward the end of the time period. We hypothesize that this is due to a larger volume of unusual emails sent directly to the CEO during this tumultuous period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We introduced a novel method for inference on multilayer networks. A hierarchical model was used to jointly describe the noisy observation matrices and MAP estimation was performed on the relevant latent variable. A simulation example using clustering demonstrated that the mixture of layers under the correct circumstances can lead to better results, and possibly a better understanding of the underlying structure between users. A real-data example was also discussed using the ENRON email dataset. This paper also leads the way for future work; in addition to trying more noise models that are not so simply reproduced or even non-convex, one can use multi-objective optimization to explore other objective functions that could be useful in describing a multi-layer network, such as network smoothness or the centrality distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Simple graphical model (L = 2). In general, each observation matrix may be influenced by multiple adjacency matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Model with similarity matrix and selection variable. W and Z take the place of Y , and the adjacency matrices have been collapsed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Clustering simulation. This surface plot shows the ARI for different values of σ 2 and β. Note that in all cases, β that is around 0.5 tends to produce the best clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Betweenness centrality for directors. This centrality is a measure of how connected a node is to the rest of the network. Larger centrality scores occur for intermediate values of α, particularly between time 95 and 115.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Degree centrality for CEOs. Higher degree centrality for α near one signifies greater activity in the behavioral network. Anomalous behavior can be seen in the later time steps as activity patterns shift.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The authors are with the <rs type="institution">Department of Electrical Engineering</rs> and <rs type="person">Computer Science</rs>, <rs type="affiliation">University of Michigan</rs>, <rs type="person">Ann Arbor</rs>, <rs type="funder">MI 48109, USA</rs>. Tel: <rs type="grantNumber">1-734-763-0564</rs>. Fax: <rs type="grantNumber">1-734-763-8041</rs>. Emails: {boselio, kulesza, hero}@umich.edu.</p><p>This work was partially supported by <rs type="funder">ARO</rs> grant #<rs type="grantNumber">W911NF-12-1-0443</rs>.</p></div>
<div><head>VII. ACKNOWLEDGEMENTS</head><p>We would like to thank <rs type="person">Kevin Xu</rs> for providing the code for the DSBM model and his suggestions for utilizing it, as well as his general comments on the content of the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pT6EXEG">
					<idno type="grant-number">1-734-763-0564</idno>
				</org>
				<org type="funding" xml:id="_CEuqF4Y">
					<idno type="grant-number">1-734-763-8041</idno>
				</org>
				<org type="funding" xml:id="_EyEYJMc">
					<idno type="grant-number">W911NF-12-1-0443</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian model selection in social research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="111" to="164" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiobjective optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ehrgott</surname></persName>
		</author>
		<ptr target="http://search.proquest.com.proxy.lib.umich.edu/docview/208128027?accountid=14667" />
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiobjective Optimization</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470640425.ch18</idno>
		<ptr target="http://dx.doi.org/10.1002/9780470640425.ch18" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>John Wiley and Sons, Inc</publisher>
			<biblScope unit="page" from="231" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pareto multi objective optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ngatchou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zarei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Sharkawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on</title>
		<meeting>the 13th International Conference on</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
	<note>Intelligent Systems Application to Power Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proper efficiency and the theory of vector maximization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Geoffrion</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0022247X68902011" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="618" to="630" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic stochastic blockmodels: Statistical models for time-evolving networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O H</forename><surname>Iii</surname></persName>
		</author>
		<idno>abs/1304.5974</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01908075</idno>
		<ptr target="http://dx.doi.org/10.1007/BF01908075" />
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tf-sidf: Term frequency, sketched inverse document frequency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baena-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carmona-Cejudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morales-Bueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1044" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
