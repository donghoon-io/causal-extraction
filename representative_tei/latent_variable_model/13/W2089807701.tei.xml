<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Model of Terrain for Autonomous Navigation in Vegetation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carl</forename><surname>Wellington</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Département d&apos;Informatique</orgName>
								<orgName type="institution">Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<email>aaron.courville@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal Montréal</orgName>
								<address>
									<postCode>H3C 3J7</postCode>
									<region>Quebec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><surname>Stentz</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Model of Terrain for Autonomous Navigation in Vegetation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current approaches to off-road autonomous navigation are often limited by their ability to build a terrain model from sensor data. Available sensors make very indirect measurements of quantities of interest such as the supporting ground height and the location of obstacles, especially in domains where vegetation may hide the ground surface or partially obscure obstacles. We introduce a generative, probabilistic terrain model that exploits natural structure found in off-road environments to constrain the problem and use ambiguous sensor data more effectively. The model includes two Markov random fields that encode the assumptions that ground heights smoothly vary and terrain classes tend to cluster. The model also includes a latent variable that encodes the assumption that vegetation of a single type has a similar height. The model parameters can be trained by simply driving through representative terrain. Results from a number of challenging test scenarios in an agricultural domain reveal that exploiting the 3D structure inherent in outdoor domains significantly improves ground estimates and obstacle detection accuracy, and allows the system to infer the supporting ground surface even when it is hidden under dense vegetation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous navigation in outdoor environments, such as those encountered in agriculture, mining, and exploration applications, is often performed using a model predictive control framework <ref type="bibr" target="#b0">[1]</ref>. In this framework, a terrain model is used in combination with a model of the vehicle to find a dynamic trajectory that avoids obstacles while protecting against roll-over, body collisions, high-centering, and other safety conditions. While faithful models of vehicle dynamics are often available, acquiring an accurate terrain model that includes a description of the supporting ground surface and any obstacles remains a considerable challenge.</p><p>Outdoor, off-road environments do not possess structured landmark features such as road markers, straight walls or a flat ground plane. This "unstructured" nature of outdoor environments complicates terrain model estimation and has often been cited as one of the reasons why the development of terrain models of these environments is considered to be challenging <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. However, such environments do possess a great deal of structure that humans frequently exploit in the performance of tasks we wish to automate. For example, consider a vehicle navigating through a field of vegetation. We can use the knowledge that ground is generally smooth and that vegetation has approximately constant height (with respect to the ground) to permit stable and robust navigation, even through areas where the ground is not directly observed. The challenge lies in expressing this type of structure in a way that can be made useful in autonomous navigation tasks.</p><p>In this paper, we describe a generative, probabilistic approach to modeling terrain. The model exploits the 3D spatial structure inherent in outdoor domains and an array of noisy but abundant sensor data to simultaneously estimate ground height, vegetation height and classify obstacles. Joint inference of ground height, class height and class identity over the whole model results in more accurate estimation of each quantity. For example, inferring the vegetation height permits an improved estimate of the height of the underlying ground. Similarly, knowing the ground height helps disambiguate solid obstacles from the ground surface.</p><p>To help constrain the problem of joint obstacle detection and supporting ground surface estimation, our terrain model incorporates a number of spatial assumptions. Two of these are encoded by distinct but interacting Markov random field (MRF) models. Markov random fields specify the relationship between variables through local dependencies. In our terrain model, one MRF models ground height and enforces our assumption that ground height is smoothly varying. The second MRF encodes our assumption that regions associated with a particular class tend to cluster (for example, patches of vegetation of a single type tend to be found together). In addition to the MRF layers, a simple latent variable model is used to express an assumption that vegetation of the same type has a similar height above the ground. These three components interact through a hidden semi-Markov model (HSMM). The hidden semi-Markov model generalizes the hidden Markov model <ref type="bibr" target="#b3">[4]</ref> by including an explicit probability distribution over the duration of each state. In our model, the HSMM enforces vertical structural assumptions such as the understanding that vegetation grows on top of ground.</p><p>The spatial constraints embodied in the terrain model are combined with information from multiple sensors on the vehicle. The sensor models are automatically learned from training data collected by driving over representative terrain. Training these sensor models to directly detect obstacles is particularly challenging because of the wide variety of objects that might be considered obstacles. People, buildings, equipment, fences and even holes are all obstacles we wish to avoid while navigating in off-road environments. There may be little in common between these things that we wish to classify as obstacles, and it is impractical to train classifiers to individually detect all possible types of obstacles. Instead, our approach is to treat obstacles as having uncertain attributes relative to the more accurate models of the known classes (such as vegetation or ground). As a result, the model considers surprising and otherwise unexplainable measurements as those most likely to be associated with an obstacle. This is a common approach in anomaly detection, and has been used for detecting obstacles in an agricultural domain <ref type="bibr" target="#b4">[5]</ref>.</p><p>Our use of MRFs to encode horizontal spatial correlations precludes an analytical solution to the inference problem. However, by exploiting an organization of our data into vertical HSMM columns, our approach allows us to model 3D structure in a reasonably efficient inference scheme. Gibbs sampling over the MRF structures lets us perform exact inference in the HSMM models using an efficient dynamic programming algorithm. This substantially reduces computation time over a full 3D MRF model, and although our approach is computationally demanding, the system can run in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Early work in terrain perception considered domains where the supporting ground surface is directly observable and objects above the ground are generally obstacles <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. In these domains, the range measurements can be used directly to estimate the ground surface and find positive obstacles, which enables successful navigation. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the presence of vegetation makes the problem much more difficult because the range points from forward looking sensors such as a laser range-finder or stereo cameras do not generally give the supporting ground surface. Classification of vegetation <ref type="bibr" target="#b5">[6]</ref> (also common in the remote sensing community -see references in <ref type="bibr" target="#b6">[7]</ref>) is not sufficient for this task because a grassy area on a steep slope may be dangerous to drive on whereas the same grass on a flat area could be easily traversable. For successful navigation in many off-road environments, a system must be able to recover the supporting ground height and the true obstacles, even in the presence of vegetation.  A number of researchers have investigated methods that use range data to discriminate sparse vegetation (as shown in Figure <ref type="figure" target="#fig_1">2</ref>) from solid substances such as the ground or obstacles. These techniques exploit the fact that range measurements often penetrate sparse vegetation, but do not penetrate solid obstacles. The properties used for discrimination fall into two categories: shape and density.</p><p>Shape-based methods begin with a 3D cloud of range points and look at local features of the data points to discriminate between the random spread of points in sparse vegetation and the organized structure of points on solid objects. Researchers have modeled the statistics of laser penetration in grass to find solid objects <ref type="bibr" target="#b7">[8]</ref>, and they have compared measurements across time and space to filter out areas where the penetration is continually changing <ref type="bibr" target="#b8">[9]</ref>. A comparison between techniques that look for the range shadow of solid obstacles and techniques based on local point statistics is given in <ref type="bibr" target="#b6">[7]</ref>. The strategy of computing local statistics about the spread of points was expanded in <ref type="bibr" target="#b9">[10]</ref> to discriminate between sparse vegetation, solid surfaces, linear structures such as branches, and even concertina wire <ref type="bibr" target="#b10">[11]</ref>.</p><p>Density-based methods attempt to use range measurements to explicitly measure the density of objects in the environment. This has been done by dividing the world into small volumes of space and then maintaining density scores by keeping track of lidar hits and pass-throughs <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref> or lidar and radar measurements <ref type="bibr" target="#b12">[13]</ref>.</p><p>The above methods have shown promising results in sparse vegetation, but they do not address the problem of estimating the ground surface in dense vegetation where the ground is completely hidden as in Figure <ref type="figure" target="#fig_2">3</ref>. We have previously used online learning methods to automatically learn the mapping from features of the sensor data to ground height predictions <ref type="bibr" target="#b13">[14]</ref>. This system was able to predict the location of the ground height under dense vegetation while still finding solid obstacles. However, the system explicitly learns a vegetation height so it is not general across vegetation of the same type with unknown height, and the ground height estimates in dense vegetation are very noisy and have large errors. One reason for this is that this method makes the strong assumption of independence between terrain patches and makes predictions locally without incorporating spatial context. This can make it difficult to disambiguate data from tall vegetation and data from short vegetation, resulting in poor estimates of the hidden ground height.</p><p>In this article we relax the independence assumption through the inclusion of spatial correlations. This added structure allows the model to infer the height of vegetation from the local sensor data instead of training on it explicitly, and enables the model to produce a smooth ground height estimate that is well suited to autonomous navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VEHICLE PLATFORM AND DATA REPRESENTATION</head><p>Our project team has automated a John Deere 6410 tractor (see figure <ref type="figure" target="#fig_0">1</ref>) and equipped it with many sensors for localization and perception <ref type="bibr" target="#b13">[14]</ref>. The vehicle has a highresolution stereo pair of digital cameras, an infrared camera, and two SICK laser range-finders (lidar) mounted on custom actively-controlled scanning mounts. The first scanning lidar is mounted on the roof to get range data over a large area in front of the vehicle, and the second scanning lidar is mounted on the bumper to get high density measurements of nearby terrain and better penetrate upcoming vegetation. The cameras and scanned lidars are precisely calibrated and tightly synchronized with an accurate global vehicle pose estimate, allowing the system to accumulate data into a high quality global map, as shown in Figure <ref type="figure">4</ref>.</p><p>The basic representational structure of our terrain model is the voxel: a 15cm 3 box-shaped region of 3 dimensional space. We represent the vehicle's local spatial environment as a voxel lattice of size I x J x K, where the ijkth voxel is in the ijth position of a horizontal 2D grid and the kth position above an arbitrary subterranean origin.</p><p>Accurate global vehicle pose allows us to assign lidar points corresponding to the same region of space to the same voxel. Exploiting the precise synchronization of the sensors, we project lidar points into the most recent color and infrared images, so that each lidar point results in a vector of appearance measurements for that voxel, including laser remission (reflectance), infrared temperature, and color. <ref type="foot" target="#foot_0">1</ref>The voxel representation also allows us to maintain a density estimate throughout space by comparing how many lidar rays pass through each voxel (pass-throughs) with the number of lidar rays that hit something in that voxel (hits). Density information is valuable when trying to separate sparse Fig. <ref type="figure">4</ref>. Colorized lidar points of farm test site buildings as the vehicle drove down a path to the field vegetation that contains a mixture of hits and pass-throughs from solid objects that contain a majority of hits and only a few pass-throughs due to sensor noise <ref type="bibr">[3] [14]</ref>.</p><p>Although our data representation is based on the voxel, vehicle navigation is generally performed on a 2D surface, so our ground height estimates and classification results are made in terms of voxel columns. In our model, the ijth voxel column class is described with a multinomial distributed random variable C ij taking on values related to the possible contents of the column, C ij = c with e.g. c ∈ {ground , vegetation, obstacle}.</p><p>Associated with the kth voxel in the ijth voxel column is the voxel state X k ij , a multinomial distributed random variable that describes the nature of the material inside the voxel, X k ij ∈ {ground , c, free-space}, where c is the class of the ijth voxel column. <ref type="foot" target="#foot_1">2</ref> The ijkth voxel is also associated with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TERRAIN MODEL</head><p>We use a probabilistic generative model to encode the desired spatial correlation assumptions described in the introduction. In this section we describe the model at three different levels:</p><p>• Voxel observation models • Vertical column structure • Horizontal neighborhood structure These levels build on each other from the inside out, going from an individual voxel to a voxel column to the entire grid of voxel columns. Figure <ref type="figure" target="#fig_4">5</ref> gives a graphical representation of each of these three levels. Before describing each level of the model in detail, we offer a perspective on the entire model by exploring the process of generating measurement data. The model contains two Markov random fields. One MRF codifies our assumption that the ground height varies smoothly with distance. Thus we expect the ground height of voxel column ij to be similar to the ground heights of the neighboring voxel columns. From a generative perspective, conditioned on the values of the neighboring ground heights we sample the ground height of voxel column ij. Similarly, we assume that neighboring voxel columns will tend to be members of the same class. Once again, this assumption is represented as an MRF and by conditioning on the class memberships of the neighboring columns we can directly sample a class membership for voxel column ij. To recover the ground heights and class memberships of all the voxel columns this process is repeated for all ij.</p><formula xml:id="formula_0">rem col ir den Y rem M Y den N Y rem 1 Y ir M Y ir 1 Y den 1 Y col 1 Y col M X ij k (a) Voxel model C = Vegetation Ground Vegetation Free-Space Y ij 5 X ij 1 X ij 2 X ij 3 ij C = Ground ij X ij 4 X ij 5 X ij 6 X ij k X ij K Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K Y ij 5 Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K X ij 1 X ij 2 X ij 3 X ij 4 X ij 5 X ij 6 X ij k X ij K C = Obstacle Y ij 5 X ij 1 X ij 2 X ij 3 ij X ij 4 X ij 5 X ij 6 X ij k X ij K Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K Free-Space Free-Space Ground Obstacle Ground H ij c H ij g (b) HSMM models Y ij C ij X ij H ij c H c H ij g C Nij C Nij C Nij C Nij H Nij g H Nij g H Nij g H Nij g (c) MRF model</formula><p>In addition to the ground height and class membership, a vegetation class height (relative to ground height) must also be determined for each voxel column with a vegetation class membership. We model the distribution of heights of a particular vegetation class as a Gaussian centered about some global mean height. For each vegetation class, a global class mean height is sampled, and then conditioned on this mean height, the individual class heights of the corresponding voxel columns are sampled.</p><p>Once the ground height, class membership and vegetation class heights are determined, their values are used to constrain the properties of each voxel column which is described using a hidden semi-Markov model. Each class has an associated HSMM model that determines the state values for each voxel in the vertical chain. For instance, below the sampled ground height, all states assume the value ground. For the vegetation classes, the states between the ground height and the vegetation height are vegetation and above that the voxels are free-space.</p><p>Conditioned on the values of the voxel states, the material properties, such as color, laser remission, infrared temperature and density are all sampled independently from the appropriate state specific distribution (e.g. ground or vegetation). Finally, conditioned on the values of each material property, measurements are sampled independently from the various measurement models.</p><p>While it is instructive to understand the model from a generative perspective, in general we are more interested in the opposite problem: how can one use the model to infer ground heights, class memberships and vegetation class heights from real sensor data. This will be the subject of section V. We now describe the three levels of the model in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Voxel Observation Models</head><p>We assume that voxels form the smallest indistinguishable element of space, occupied completely by one (and only one) voxel state. Each voxel state maintains a distribution over material properties including density, remission, infrared temperature, and color that describe the characteristics of that state, but the material inside a single voxel is assumed to be uniform. For example, the vegetation state may include a range of colors, and therefore different voxels in vegetation may have different colors, but we assume that the color of the vegetation within each voxel is uniform.</p><p>There are a variable number of observations of the material properties in each voxel, and care must be taken in how we combine these measurements. Observations are commonly modeled as being independent given the state, so that accumulating measurements leads to an arbitrary certainty in the state value. However, such an approach fails to take account of the inherent uncertainty in the distribution of material properties for a given voxel state value.</p><p>This phenomena is best illustrated with an example taken from one of the experimental scenarios we use to validate the model (see section IX-A). This example includes a white shed next to a white road. If the system accumulates color measurements for a particular voxel, then it can become arbitrarily certain of the color of that voxel, but knowing the true color material property of the voxel does not lead to certainty regarding the voxel's state value. Both the ground and obstacle states can exhibit a range of colors, and gathering evidence regarding the color of the voxel does not resolve the uncertainty in material properties possessed by the states.</p><p>Statistically, this problem reduces to the fact that in our model, multiple sensor measurements are not independent conditional solely on the value of the state. For example, a voxel may contain a vegetation state that includes both green and yellow material properties. Observing green for that voxel would then increase the probability of subsequent green observations, even if one knows the true state of the voxel. However, the measurements are independent given the true value of the measured material property. Continuing with the example, knowing that the true color material property of a voxel is green makes the color measurements of that voxel independent. By explicitly introducing the true unobserved material properties in to the observation model, we can properly account for the non-independence of sensor measurements with respect to the state values.</p><p>The graphical model in Figure <ref type="figure" target="#fig_4">5</ref>(a) illustrates the conditional independencies between the voxel state X k ij , the material property random variables den, rem, ir and col , and the measurements. Conditional on X k ij , the material properties are independent, and conditional on the material properties, the measurements are independent. The voxel material properties are not directly observed, and we are not concerned with their values beyond what they reveal about the state. Thus the material properties constitute nuisance variables that we remove from the observation models through marginalization, as described below.</p><p>1) Appearance: The distributions over the voxel appearance properties, including infrared temperature, laser remission, and color are all inherently multi-modal and thus not well described by a simple parametric distribution. For example, remission values in vegetation are either high because of the strong reflectivity of chlorophyll, or very low due to small cross-sectional area. We resort to a Gaussian mixture model (GMM) to describe the distribution of the material properties within a state.</p><p>We develop the marginal distribution for the remission values, but the infrared and color data are determined analogously. The true material property rem for state value x is modeled as a GMM with R mixture components. Each individual mixture component i is parameterized with mean µ i , variance σ 2 i , and mixing coefficient P (i). Note that these parameters are unique to the rem material property. Color and infrared data are parameterized independently.</p><formula xml:id="formula_1">p(rem|X k ij = x) = R i=1 P (i) 1 2πσ 2 i exp - (rem -µ i ) 2 2σ 2 i</formula><p>(1) Conditional on the true material property rem, the measurements y m rem are assumed to be normally distributed, y m rem ∼ N (rem, σ 2 y ). We integrate out the nuisance variable rem to get the marginal likelihood for all the remission data</p><formula xml:id="formula_2">y rem = [y 1 rem , . . . , y M rem ],</formula><p>resulting in a mixture of Gaussians that is a function of the data mean ȳrem (see <ref type="bibr" target="#b14">[15]</ref>).</p><formula xml:id="formula_3">p(y rem | X k ij = x) = p(y rem | rem)p(rem | X k ij = x) d(rem) = R i=1 P (i) M m=1 p(y m rem | rem)p(rem | µ i ) d(rem) = R i=1 P (i) 1 2π σ 2 i + σ 2 y M exp   - (ȳ rem -µ i ) 2 2 σ 2 i + σ 2 y M  <label>(2)</label></formula><p>As discussed above, although the measurements are independent given the material property: p(y rem | rem) = M m=1 p(y m rem | rem), equation 2 shows that the measurements are not independent when conditioned on the state:</p><formula xml:id="formula_4">p(y rem | X k ij = x) = M m=1 p(y m rem | X k ij = x)</formula><p>. Measurements are not independent sources of evidence with respect to the value of the state and only influence state inference indirectly through the latent material property.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> further illustrates the effect of including the material property explicitly in the observation model. Figure <ref type="figure" target="#fig_5">6(a)</ref> shows the GMM remission material property distribution (as given in equation 1) associated with the ground, vegetation and obstacle states. As the figure shows, the remission values are a moderately informative feature of state, with lower remission values tending to support a hypothesis of ground over vegetation. With only one measurement (M = 1), the remission observation model of equation 2 results in the remission probability distribution shown in Figure <ref type="figure" target="#fig_5">6(b)</ref>. In this case there are two significant sources of uncertainty: the remission material property associated with each state (as shown in Figure <ref type="figure" target="#fig_5">6(a)</ref>) and the uncertainty associated with the measurement itself (represented by the quantity σ y ). These two sources of uncertainty combine to form the relatively high variance distribution shown in Figure <ref type="figure" target="#fig_5">6</ref> This is also seen by inspection of equation 2. As M → ∞ and as ȳrem → rem (by the law of large numbers), equation 2 converges to equation 1.</p><p>2) Density: Voxel density values range from empty space (den = 0) to completely solid (den = 1). Analogous to the GMM models used for voxel appearance properties, the distribution of density values for a given state x can be well modeled using a beta distribution B(a x , b x ) which operates in the range [0, 1] and has parameters a x and b x that together specify its mean and variance.</p><p>Density is represented as the binary vector Y n den (lidar hit or pass-through), and we use a binomial distribution to describe the number of hits M = N n=1 Y n den out of N total rays for a given voxel density property den. As above, we integrate over the nuisance parameter den, and we recover the beta-binomial distribution as the marginal likelihood observation model.</p><formula xml:id="formula_5">P (M = m | X k ij = x) = P (m | den)p(den | X k ij = x) d(den) = N M B(a x + M, b x + N -M ) B(a x , b x )<label>(3)</label></formula><p>This model makes the assumption that a voxel with density den generates lidar hits that follow a binomial distribution (the outcome of repeated flips of a biased coin with P (heads) = P (hit) = den). However, since a given state x has a range of possible densities, which we model with a beta distribution, the distribution over hits M for a given state x becomes a beta-binomial, which has greater variance than a binomial for low amounts of data N , but converges to a binomial as N becomes large.</p><p>3) Free-space: The free-space state does not possess any meaningful material properties beyond density den. Lidar hits occurring in free-space are generally the result of noise so we model the non-density material properties as matching the material properties of the states in contact with free-space. For example, the voxel above a ground state voxel may contain many pass-throughs with a single hit due to noise that has an appearance that matches the ground state. If we modeled the appearance of free-space as uniform, then the strong match in appearance data with the ground state may overwhelm the density information and prevent the voxel from correctly being classified as free-space. By setting the appearance properties of free-space to match the state it is in contact with (ground in this example), the transition to free-space is decided solely on density information.</p><p>4) Obstacles: Although we expect obstacles to generally have a fairly high density den, we cannot hope to build an accurate observation model for the appearance of each of the innumerable obstacles one might encounter in outdoor environments, so we simply use a single obstacle state with a corresponding uniform distribution over the observable range of material appearance properties. We rely on accurately modeling the features of the trained states to detect obstacles as a default option when none of the other states are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vertical Column Structure</head><p>As we move from lower to higher voxels within a column, we expect to move from ground to vegetation, or perhaps ground to obstacle and eventually to free-space. We never expect free-space to be found below ground, nor do we expect vegetation to be suspended above free-space. This type of structure is naturally imposed by introducing a Markov dependency between voxel states that restricts vertical transitions, thus defining a hidden Markov model within each voxel column. However, the duration of states such as ground and vegetation are not well modeled by repeated self-transitions in a Markov chain because this would induce a geometric distribution on the duration of those states. We resort instead to a hidden semi-Markov model (HSMM) <ref type="bibr" target="#b15">[16]</ref> over voxel states, which explicitly represents a state duration (or height distribution) over voxels for each state value.</p><p>Figure <ref type="figure" target="#fig_4">5</ref>(b) gives the intuition for a hidden semi-Markov model. Unlike an HMM which transitions (possibly to the same state) at every step, an HSMM remains in a single state value for some random duration (e.g. H c ij in Figure <ref type="figure" target="#fig_4">5(b)</ref>), generating observations from that state at each step. At the completion of the duration, the state transitions to a new value according to the transition matrix and the process repeats. The transitions between states remain Markov, but the individual steps are not Markov since the probability of transitioning depends on how long the system has been in that state.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>(b), we associate a single HSMM chain structure with each column class C ij , making the resulting column model a mixture of HSMMs. The durations of the ground and class states describe the height of those terrain elements and are given by H g ij and H c ij .</p><p>The vertical structure helps constrain the location of variables of interest such as the ground height hidden beneath dense vegetation. For example, in Figure <ref type="figure" target="#fig_4">5</ref>(b), if we observe measurements that make X 5 ij likely to be vegetation and X 6 ij likely to be free-space, then the HSMM chain for column class C ij = vegetation would be likely, and we would also expect a ground to vegetation transition somewhere below X 5 ij because of our vertical transition structure. As described in the next section, we can incorporate other information such as the expected vegetation height and the location of surrounding ground to further constrain the ground height even if it is hidden below dense vegetation and has no direct observations. This neighborhood information is incorporated into the column HSMM models as a prior over the duration H g ij of the ground state and the duration H c ij of the class state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Horizontal Neighborhood Structure</head><p>In addition to the vertical structure captured by the HSMM column models, there are also significant horizontal dependencies between neighboring columns. The model encodes assumptions of class continuity, the smoothness of ground, and the similarity of vegetation height. This contextual information helps constrain the problem, especially in areas where dense vegetation hides the ground surface or in areas with missing or ambiguous sensor data. We use Markov random fields to encode our assumptions about class continuity and smooth ground.</p><p>The Markov random field (MRF) <ref type="bibr" target="#b16">[17]</ref> is an extension of one-dimensional Markov chain models to two (or more) dimensions. It is commonly used in spatial domains to model structure such as smoothness, class continuity, or other properties correlating neighboring nodes. It has been successfully applied to many applications including segmentation, noise reduction, surface reconstruction, and texture classification <ref type="bibr" target="#b17">[18]</ref>.</p><p>Our terrain model uses a standard two-dimensional MRF, consisting of an undirected graph of nodes on a lattice structure that connects the voxel columns horizontally and encapsulates the conditional dependencies in the model. For example, the connections in Figure <ref type="figure" target="#fig_4">5</ref>(c) show the Markov property for the ground height MRF:</p><formula xml:id="formula_6">P (H g ij | H g S ) = P (H g ij | H g Nij )<label>(4)</label></formula><p>where H g S is the set of all ground height nodes and H g Nij is the set of neighbors of the ijth voxel column ground height H g ij . The set of neighbors H g Nij is also referred to as the Markov blanket of the node H g ij . A key property of the MRF is that the joint probability over the entire MRF is uniquely specified by the conditional distributions that characterize its local Markov relationships <ref type="bibr" target="#b17">[18]</ref>. Therefore, from easily expressible local relationships such as the notion that ground is smooth (with ground height changing little between neighboring voxel columns), we are able to infer global quantities such as the supporting ground surface heights of every voxel column.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>(c), we model horizontal structure using two distinct but interacting Markov random fields for class C ij and ground height H g ij , and a latent variable for the common class height H c across all columns. These variables interact through the HSMM column models by imposing a prior over the state durations associated with H c ij and H g ij and over the column class models C ij . The neighborhood dependency of C ij reflects the prior assumption that class identities are positively correlated with their neighbors so voxel columns tend to cluster in contiguous groups of the same class. We express this preference using the conditional MRF distribution</p><formula xml:id="formula_7">P (C ij = c | C Nij ) ∝ exp -λ C {s,t}∈Nij (c = c st )<label>(5)</label></formula><p>where N ij is the set of neighboring indices and C Nij is the set of classes in the neighborhood of the ijth voxel column. Ground height varies smoothly from one patch of ground to the next, so we expect that H g ij will be tightly correlated with nearby values. We express this belief using a Gaussian Markov random field</p><formula xml:id="formula_8">P (H g ij = h | H g Nij ) ∝ exp - 1 2σ 2 G h - 1 |N ij | {s,t}∈Nij h g st 2 (6)</formula><p>where |N ij | is the size of the neighborhood (in our experiments we used a 4-connected neighborhood).</p><p>We also expect that vegetation of the same class c has a similar height H c with some variation. This assumption may not be valid for obstacles, so we only apply it to vegetation classes. Given the common height of the vegetation in this area H c , we model the expected variation with a Gaussian truncated by the interval of possible class heights</p><formula xml:id="formula_9">I [h c min ,h c max ] P (H c ij = h | H c ) ∝ I [h c min ,h c max ] exp - 1 2σ 2 H c (h -h c ) 2 (7)</formula><p>Because of the variation between different types of obstacles, we treat the class height of obstacle columns as independent.</p><p>Other constraints can also be included in the model. For example, we can fix the ground height under the wheels of the vehicle since these ground heights are known. The smooth ground prior in equation 6 then allows this information to propagate through the model and helps constrain the surrounding area.</p><p>Our use of Markov random fields to encode spatial correlations among variables is reminiscent of recent work in texture segmentation, where an image is assumed to be composed of a number of contiguous regions of uniform texture. One approach uses one MRF layer to classify textures and a second MRF layer that generates textures using separate parameters for each class <ref type="bibr" target="#b18">[19]</ref>. This "double Markov random field" structure is related but distinct from our use of two MRFs in the terrain model described above. Similar to the double MRF, we maintain a single MRF for class segmentation that interacts with another MRF representing the ground surface, but rather than the parameters of one MRF depending on the states of the other, we interpret columns of data based on both the class MRF and the ground MRF.</p><p>Markov random fields have also been used in medical imaging applications to segment various tissues from 3D volumetric data <ref type="bibr" target="#b19">[20]</ref>. Here material properties were represented in a voxelbased representation similar to our terrain model. However, in <ref type="bibr" target="#b19">[20]</ref> the MRF was defined over the full 3D neighborhood of the voxel, whereas we are able to exploit inherent structure in our problem domain to keep the computationally demanding MRF structures to 2D and handling our 3D data in terms of voxel columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. INFERENCE</head><p>The interacting Markov random fields of this model capture important structure, but these dependencies prevent analytic determination of the posterior distribution</p><formula xml:id="formula_10">P (C, H g , H c | Y ).</formula><p>The set of HSMMs that describe the data in each column of voxels can efficiently produce distributions over the state durations, which makes it easy to sample from the conditional distribution</p><formula xml:id="formula_11">P (C ij , H g ij , H c ij | Y ij , C Nij , H g Nij , H c )<label>(8)</label></formula><p>so we use Gibbs sampling <ref type="bibr" target="#b16">[17]</ref> to approximate inference. Algorithm 1 describes the application of Gibbs sampling to our model. The HSMM column models require a distribution over class heights which comes from the common class height latent variable H c , as shown in Figure <ref type="figure" target="#fig_4">5(c</ref>). Samples of the common class height are produced from its conditional distribution given the current column class height samples h c ij</p><formula xml:id="formula_12">P (H c = h | H c ij∈IJ ) ∝ exp -1 2σ 2 H c /D c h- 1 D c ij∈IJ,cij =c P (Y k ij | x)P (H x ij = h | H g Nij , H c )α k-h ij,c (x -)<label>(10)</label></formula><formula xml:id="formula_13">β k ij,c (x) = P (Y k+1:K ij | state x ends at k, C ij = c, H g Nij , H c ) = x h P (Y k+1:K ij |X k ij = x,X k+h ij = x ,H x + ij = h,C ij ,H g Nij ,H c ) = h k+h k =k+1 P (Y k ij | x + )P (H x + ij = h | H g Nij , H c )β k+h ij,c (x + )<label>(11)</label></formula><p>Since we know by assumption that the chain must end in the final state x = free-space, the probability of the data for class c is the final value of α in that state.</p><formula xml:id="formula_14">P (Y ij | C ij = c, H g Nij , H c ) = α K ij,c (x = free-space) (12)</formula><p>As described in Algorithm 1, this is combined with the class prior P (C ij | C Nij ) to find the distribution over classes, which is used to sample a new class.</p><p>Finding the distribution over state durations involves combining α and β. As above, equation 13 takes advantage of the deterministic transitions of the chain structures to reduce computation.</p><formula xml:id="formula_15">ζ x ij,c (h) = P (state x has duration h | Y ij , C ij = c, H g Nij , H c ) = k P (X k ij = x, X k-h ij = x -| Y ij , C ij , H g Nij , H c ) = k k k =k-h+1 P (Y k ij |x)P (H x ij = h|H g Nij , H c )α k-h ij,c (x -)β k ij,c (x)<label>(13)</label></formula><p>We know that in each chain, every state transition must occur after some duration, so we can normalize by h ζ x ij,c (h) to get the posterior on ground and class height conditional on the neighbors. Samples are then drawn from these distributions.</p><formula xml:id="formula_16">P (H g ij = h | C ij = c, Y ij , H g Nij , H c ) = ζ x=ground ij,c<label>(h)</label></formula><formula xml:id="formula_17">P (H c ij = h | C ij = c, Y ij , H g Nij , H c ) = ζ x=state c ij,c<label>(h)</label></formula><p>The time complexity of HSMM calculations is greater than an HMM because of the sum over possible durations, but the observation likelihood products P (Y k ij |x) can be pre-computed and the state durations to search over can be constrained based on the priors to reduce the complexity to O(numVoxels * numStates * maxDuration) for a single chain.</p><p>Although it is typically difficult to show that Gibbs sampling has converged, we have found empirically that the model finds a good estimate quickly, allowing for real-time execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LEARNING</head><p>The model described in section IV incorporates prior knowledge about the structure of the environment, but the specific model parameters must be learned from training data. These parameters include the sensor observation models for each state and the neighborhood interactions for class, class height, and ground height. The generative nature of our model allows us to decouple the learning problems, and train each of these observation and neighborhood interaction models individually, thus greatly simplifying the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observation Models</head><p>Collecting labeled training data is often expensive, especially in outdoor environments where there can be high variation in sensor readings so that a large training set is needed. We use an approach based on <ref type="bibr" target="#b13">[14]</ref> to collect large quantities of labeled training data to automatically train our observation models. Specifically, we drive through representative terrain of a single class such as vegetation and store the sensor measurements from the voxels of columns that we drive over as training examples for that class. This process is then repeated for other classes such as ground. Unlike <ref type="bibr" target="#b13">[14]</ref> which directly trains on the height of different types of vegetation, we only train on the various material properties of vegetation voxels, allowing us to remain general across vegetation heights.</p><p>Each labeled voxel collected by driving through representative terrain is used as a training example for the observation models in equations 1, 2, and 3. For appearance data such as remission, infrared and color, the mean values from each voxel are used to train the GMM observation models (i.e. for remission data µ i , σ 2 i , P (i) in equation <ref type="formula">1</ref>) and the variance of measurements within the voxels is used as the GMM measurement model variance (σ 2 y in equation 2). Hit and pass-through data from the labeled training voxels are used to find the maximum likelihood parameters of the beta-binomial density model (a x and b x in equation 3) for each class state x using a Newton-Raphson method <ref type="bibr" target="#b20">[21]</ref>. This handles class states like ground and vegetation, but the density of obstacle and free-space states must also be trained. The freespace density can be trained using data that includes insects or dust that occasionally returns a lidar point, or it can just be set manually to strongly favor empty space. Similarly, the obstacle density can be trained using hit and pass-through data from representative obstacles, or it can be set manually to favor dense objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neighborhood Models</head><p>The priors given in equations 5 and 6 describe how class and ground height depend on their neighbors, and the prior in equation 7 describes how column class heights are related to the common class height. Each of these priors contains parameters that quantify the tendency for classes to clump together, the smoothness of the ground, and the variability of vegetation class heights. As above, we train these parameters by driving over representative terrain.</p><p>As we drive over an area, we record the ground heights measured by the location of our wheels. We use these height sequences to find the standard deviation σ G of typical ground height variation between voxel columns, which gives us the maximum likelihood estimate of our Gaussian MRF ground neighborhood prior <ref type="bibr" target="#b21">[22]</ref>.</p><p>Similarly, as we drive through vegetation, we get an approximate vegetation height measurement by taking the highest lidar hit and subtracting the known ground height (from the wheel locations). Since we assume that vegetation heights are independent given the common vegetation height in the area, we can find the class prior standard deviation σ H c directly from this sequence of class heights.</p><p>The class interaction prior λ C gives the probability that a class transitions to a different class. This could be estimated directly using pseudo-likelihood methods <ref type="bibr" target="#b17">[18]</ref> with class-labeled data over a large area that includes many class transitions, but unlike the labeled data for the observation models or the ground and class height interactions, this type of training data is difficult to collect. However, changing the class interaction prior affects the system output in an intuitive way by controlling how much classes tend to clump together, so this parameter can be set manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RUN TIME</head><p>Performing inference in this model is computationally intensive because of the repeated calculations necessary to sample from the model. We maintain a 150x150 grid of 15cm square voxel columns in our terrain map, which covers a 22.5 meter square area. Our system runs a loop that updates the local terrain map at approximately 1Hz. Within this loop, the system computes the observation likelihood products, calculates 20 samples from each column in the map, and updates the mean ground height, class height, and most likely class from the samples in each column.</p><p>At a vehicle speed of 1m/s, our procedure results in approximately 200 samples for a given terrain patch before the vehicle reaches it. Although sampling convergence is difficult to prove, the system generally finds the solution quite rapidly in our experiments, allowing us to run the system in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. SIMULATION RESULTS</head><p>We have presented a terrain model that includes spatial correlations to better handle missing and ambiguous data in dense non-penetrable vegetation. The model is designed to recover an accurate estimate of the ground surface despite having only indirect observations and without needing to explicitly train on the vegetation height. This section shows a simple two-class simulation result that verifies these properties.</p><p>Figure <ref type="figure" target="#fig_6">7</ref>(a) shows simulated data of a transition from ground to tall vegetation. Imagine the vehicle is approaching from the left, so initially the sensors get measurements of the ground, the front of the vegetation, and the top of the vegetation, but since the vegetation is dense there are no range measurements of the ground surface under the vegetation. The dashed line gives the true ground height, showing that the ground under the vegetation is flat and then angles up a hill. There are some columns with missing data, and some voxels in the vegetation match the appearance of ground, as shown by their light gray color. Although there is no data under the ground or vegetation surfaces, the voxels above the ground and vegetation are full of pass-throughs. The ground and vegetation appearance models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground</head><p>Light yellow vegetation Obstacle Dark green vegetation were set to match the simulated input appearance data so we could study the effects of spatial correlation in the model. Since this example assumes the vehicle is approaching from the left, the system was initialized with the ground column class, a ground height of 2, and a voxel state class height of 0 (the voxel state ground class has no height). The sampling inference procedure given in algorithm 1 was then run for 100 iterations (each iteration produces samples from every column) which took 0.5 seconds. The final 50 samples from each column were used to find the most common class, the mean ground height, and the mean class height (although we allowed 50 iterations of "burn in" time to let the sampling procedure converge, the system actually converged after approximately 5 iterations).</p><p>Figure <ref type="figure" target="#fig_6">7</ref>(b) shows the ground height estimates, Figure <ref type="figure" target="#fig_6">7</ref>(c) gives the class height estimates, and both show the classification results for each column. These values represent the most likely explanation of the data given the prior knowledge encapsulated in the model. Although the system was never trained on the height of the vegetation, it was able to recover the vegetation height and use it to estimate the ground height including the hill. The ground smoothness and similar vegetation height assumptions combine to constrain the ground height at the transition from visible ground to vegetation and propagate the observed vegetation height at the transition through the rest of the simulated region.</p><p>The model structure also allowed the system to handle missing and ambiguous data. The class prior makes it likely that the areas in the vegetation with missing data are also vegetation. The ambiguous data patches in the vegetation have appearance properties similar to ground, but the ground smoothness prior makes it extremely unlikely for the ground to be at that height, so the model infers that these areas are actually vegetation.</p><p>The class height estimates in Figure <ref type="figure" target="#fig_6">7</ref>(c) are not completely uniform. There is a crease where the hill starts because the model ground prior enforces a smooth transition from the flat region to the hill in Figure <ref type="figure" target="#fig_6">7</ref>(b) whereas the simulated data in Figure <ref type="figure" target="#fig_6">7</ref>(a) has an abrupt angle change. The class heights at the far right become slightly larger because of the conflict between the ground prior enforcing a smooth flat ground estimate and the class height prior enforcing a uniform estimate of the vegetation height.</p><p>The class height predictions are slightly lower in the missing data areas because of asymmetric data evidence. In the absence of any data, the class prior would give the missing data areas a symmetric class height distribution around the true class height. However, the large number of pass-throughs above the missing data areas produces a strong constraint that cuts off the top of the class height prior distribution. Therefore the class height samples in areas with missing data are biased low. Since there are no hits in that patch, it is reasonable to expect that the vegetation height is lower in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. RESULTS</head><p>We have tested this model with real data at a nearby working farm and an undeveloped area with tall weeds. The following examples highlight the benefits of including spatial correlations in areas with dense non-penetrable vegetation. We demonstrate the system's ability to detect obstacles in vegetation and we also show what happens when the smooth ground assumption is violated.</p><p>In each case, after training the model on representative terrain, we drive the vehicle through the test area while letting the Gibbs sampler run continuously. Running at 1Hz, the system calculates the observation likelihoods, computes samples from the model, and updates the local terrain map with the most commonly sampled class and the mean of the ground height and class height samples.</p><p>As mentioned in section I, our goal is to find the supporting ground surface and the location of obstacles. The result figures in this section show a single class color label (see Figure <ref type="figure" target="#fig_7">8</ref>) for each column, giving the most commonly sampled class. Each column also displays a single height based on the mean of one of the transitions in that column, but which transition is displayed is a function of the column's class. For columns labeled as ground or vegetation, we care about the supporting ground surface, so the results show the inferred ground height (the transition from ground to free-space or ground to vegetation). For obstacle columns, we care about the height of the obstacle, so the results show the class height on top of the ground height (the transition from obstacle to free-space).</p><p>To show the benefit of including spatial correlations, we compare our model result with a system that uses the same trained observation models but makes independent classifications for each column instead of incorporating spatial structure. This comparison system produces the most likely class for each column using only the observations in the voxels in that column. To produce results comparable to our model output, it uses the highest hit in the column as the transition to freespace. For vegetation class columns, it uses the lowest hit or pass-through in that column as its estimate of ground height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. White shed</head><p>Figure <ref type="figure" target="#fig_8">9</ref> shows the tractor as it approaches a white shed. This is a large obstacle that could be reliably detected in a variety of ways, but it will serve as a good example of how the various pieces of our model interact to produce the correct result. Figure <ref type="figure" target="#fig_8">9</ref>(b) shows the output of the model, including a useful classification of the scene and a smooth ground estimate that would work well for vehicle navigation. It classifies the shed as an obstacle and correctly captures the hill sloping down to the right despite the presence of sparse vegetation.</p><p>This example is interesting because the voxels containing the walls of the shed have observations that make the ground class much more likely than the broad uniform obstacle class. However, the spatial prior on a smooth ground surface makes it extremely unlikely that the ground height would have a tall step discontinuity at the shed wall. Since the density and appearance data are not well described by the vegetation class, the shed is correctly classified as an obstacle.</p><p>Figure <ref type="figure" target="#fig_8">9</ref>(c) shows the output of the system when the neighborhood interactions are ignored and the columns are assumed to be independent. Without neighborhood information, classification is based solely on the data likelihood for each column HSMM model. Lacking the smooth ground prior, the wall is classified as a collection of tall columns of ground voxels. A vision system that ignores 3D structure and only makes a classification based on the observation models we use would produce a similar result. Figure <ref type="figure" target="#fig_8">9</ref>(c) also shows that without the spatial ground and class priors, the ground height estimates and classification labels are generally more noisy.</p><p>If the white shed actually was an earthen cliff, our model would produce similar predictions and the wall of dirt would be labeled as an obstacle. Our ground smoothness prior encapsulates the assumption that such abrupt changes in the ground surface don't occur. This is not a problem since the output terrain model is used to find a drivable ground surface for robot navigation so a wall of dirt should be classified as an obstacle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tall vegetation</head><p>Figure <ref type="figure" target="#fig_0">10</ref> shows a challenging scene containing a camouflaged person in tall weeds with low grass and a small dirt mound to the right. Both the person and the dirt mound have high infrared temperature, so a simple obstacle detection system that performs a threshold on temperature may classify both of these objects as obstacles. We show that the structural assumptions embodied in our model allow the system to disambiguate these two objects.</p><p>We trained the model on bare ground, low grass, and tall weeds. Although that area is actually low grass, since the system has no data from the area, ground is a reasonable estimate.</p><p>Using the model structure and the known ground height under the vehicle allows the system to produce reasonable estimates of the ground height even in areas where the ground is hidden. In addition to providing a smoothing prior, neighborhood interactions allow information to propagate. Fixing the heights under the wheels affects the ground estimates in the surrounding area. Columns with little or no data can still produce useful estimates by relying on their neighborhood. The system can infer the common vegetation height of the tall weeds from areas where the ground is more directly The assumption of a common vegetation height, in turn, allows the system to infer the ground height in areas where the ground is not directly observable. Knowing the ground height allows the model to explain the dirt mound as a rise in the ground but the person as an obstacle.</p><p>Assuming independence prevents information from propagating. Figure <ref type="figure" target="#fig_0">10(c)</ref> shows the lowest point and the class estimates when no neighborhood information is used. The lowest point does not penetrate the dense vegetation so it provides poor estimates of the ground height. Both the dirt mound and the person are classified as a mixture of dirt and obstacle columns. Also without neighborhood information, the vegetation class predictions contain many isolated misclassifications due to ambiguous data.</p><p>Figure <ref type="figure" target="#fig_11">11</ref> illustrates the quality of the ground height estimates from Figure <ref type="figure" target="#fig_0">10</ref>. After computing estimates of the ground height using our model, we drove through the scene toward the area between the person and the dirt mound, and made measurements of the ground height using our wheel locations. This trajectory is marked as "True height" in Figure <ref type="figure" target="#fig_11">11</ref>, and offers a comparison for the estimates produced by the model and those using the lowest hit or pass-through in Current approaches that filter out vegetation from the ground surface generally rely on the deepest range penetration, but for dense non-penetrable vegetation this performs very poorly since there are no range points that reach the ground. Therefore, in addition to comparing our model to the lowest hit or pass-through, we also compare it to an approach that adjusts the lowest hit in each column based on the independent column classifications shown in Figure <ref type="figure" target="#fig_0">10(c</ref>). Instead of using spatial structure to infer the vegetation height from the data as in our model, this approach simply uses the average height of each class from the training data for the offset. Figure <ref type="figure" target="#fig_11">11</ref> shows that this can work well when the classification is correct and the actual vegetation height matches the training data, but it suffers from misclassification and the lack of a smoothing prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vegetation on a Slope</head><p>The previous section demonstrated how the model's structural assumptions enabled improved ground height estimations. However, in that example the ground was generally flat so a system that simply classified all vegetation as safe could have succeeded without correctly estimating the ground surface (although detecting positive obstacles such as the person would be more difficult without the ground height). This section shows an example where treating vegetation as drivable could lead to dangerous behavior but finding the supporting ground surface enables the vehicle to stay safe.</p><p>Figure <ref type="figure" target="#fig_12">12</ref> shows the tractor as it is driving along a path on a steep 14-degree side slope with dense vegetation at the bottom of the hill. The vegetation at the bottom covers and hides the supporting ground surface underneath. Figures <ref type="figure" target="#fig_12">12(b</ref>) and 12(c) show a view from in front of the tractor of the range point data, as well as estimates of the ground height and the classifications. As in the other result figures, each column is colored using the class label of that column, and the height of the vegetation columns shows the transition from ground to vegetation. If instead we showed the states of the individual voxels, then the portions of the columns visible in Figures <ref type="figure" target="#fig_12">12(b</ref>) and 12(c) would be ground state voxels and there would be vegetation state voxels above the displayed transition from ground to vegetation.</p><p>The path that the tractor is driving on has a high side slope, and the ground becomes even steeper on the lower part of the hill under the vegetation, which could result in a roll-over hazard. The dense vegetation prevents laser measurements of the ground, so a system that uses the lowest point for the ground height would treat the top of the vegetation as the supporting surface, as shown by the ground height predictions in Figure <ref type="figure" target="#fig_12">12(c</ref>). This would make that part of the hill appear to be fairly flat and traversable, when it actually contains a steep side slope and could represent a roll-over hazard.</p><p>Figure <ref type="figure" target="#fig_12">12</ref>(b) shows the spatial model ground height estimates. This approach correctly infers a ground surface below the vegetation, and the resulting ground surface predicts a high slope in that area that could be used with a vehicle model to check for roll-over conditions.</p><p>The model assumptions of smooth ground and similar vegetation height enable the system to infer the ground surface below the vegetation, even though the system was never trained on the height of the vegetation. As in the simulation example in section VIII, the transition from ground to vegetation at the edge of the path allows the system to infer a vegetation height which then propagates through the spatial correlations in the model to drop the ground surface below the top of the vegetation. The system also tries to produce smooth ground estimates between the observed ground height on the path near the tractor and the data from the bottom of the slope (not visible in the figures). These various constraints combine to produce an accurate ground estimate in this difficult example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ledge Step Hazard</head><p>The previous sections looked at examples where the model assumptions of smooth ground, class continuity, and similar vegetation height were generally correct. This section explores what happens when model assumptions are incorrect.</p><p>Our main test area did not have any areas with non-smooth ground so a wooden ledge was constructed. Figure <ref type="figure" target="#fig_2">13</ref> shows the tractor as it approaches this ledge, and Figure <ref type="figure" target="#fig_2">13(b)</ref> gives the model ground height estimates. The ledge has an appearance that matches the ground class, so there is a row of columns classified as ground, but the ground prior makes the taller portions of the ledge unlikely to be ground and their appearance does not match any other class, so the ledge is reasonably classified as an obstacle.</p><p>As shown in Figure <ref type="figure" target="#fig_2">13</ref>(b), the ground estimates beyond the ledge are significantly lower than the true ground height. The model has explained the higher data points beyond the ledge with dense medium height vegetation on low ground instead of short vegetation on higher ground. Figure <ref type="figure" target="#fig_2">13</ref>(c) provides some insight into why the model produces this estimate. The vehicle is positioned in taller vegetation of similar appearance to the shorter vegetation beyond the ledge. Based on the similarity in appearance, the system infers that the two patches of vegetation belong to the same vegetation class. Consequently, the assumption of common vegetation height propagates the height of the taller vegetation surrounding the vehicle to the area beyond the ledge. This combines with the smooth ground prior to underestimate the ground height beyond the ledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Longer run through varied vegetation</head><p>Figure <ref type="figure" target="#fig_14">14</ref> shows ground height estimates for a longer test set through varied vegetation. Unlike Figure <ref type="figure" target="#fig_11">11</ref>, which presents a snapshot of the predictions at different distances in front of the vehicle at a given time, Figure <ref type="figure" target="#fig_14">14</ref> shows predictions at a constant 6 meters in front of the vehicle as the vehicle moves forward. The lowest hit line shows that the first 70m of the path contained two sections of tall dense non-penetrable vegetation, and the remainder of the path consisted of low vegetation with various tall sparse vegetation and a few small patches of dense vegetation (e.g. 170m). The model output is generally smooth and closely matches the true height, whereas the lowest hit rarely reaches the ground, and the estimate using lowest hit with class offset is often correct but very noisy because of misclassifications due to its independence assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION</head><p>Our probabilistic terrain model provides a natural way of combining different types of sensor data with reasonable assumptions about the world-particularly in agricultural applications-such as ground smoothness, class continuity, and similarity in vegetation height. These assumptions are incorporated as a model prior and help constrain the problem when the sensor data is ambiguous. Except for the class neighborhood prior, which can be easily tuned by hand, all model parameters that control the difficult task of weighting the different sensor data appropriately are automatically learned by the system by simply driving through representative terrain.</p><p>Our approach can find obstacles without needing to explicitly model them or collect obstacle appearance training data. We have applied this approach in realistic test conditions and have shown that the use of spatial structure in our model improves ground height estimation and obstacle classification over an equivalent model that ignores spatial structure. Although computationally intensive, the algorithm can run in real time for moderate vehicle speeds of 1-2m/s.</p><p>In areas where our model assumptions are generally true, the model structure allows the system to infer the supporting ground surface even when the ground is hidden by dense vegetation. Joint inference of class, ground height, and vegetation height allows the system to produce better estimates of each, since knowing the ground height helps disambiguate obstacles from the ground, and knowing the class and vegetation height helps determine the ground height.</p><p>When model assumptions are violated, the model often produces a desirable result, such as treating a ground discon-tinuity as an obstacle, even though the resulting ground height estimates are inaccurate. This model could also be extended to include additional class models for overhanging obstacles and holes in the hope of broadening the set of environments where our assumptions are valid.</p><p>Although we can generally perform inference in the model in real time for moderate vehicle speeds, the algorithm presented is still computationally demanding. Perhaps other approximate inference schemes would be less computationally intensive than Gibbs sampling, while still offering the benefits of including the spatial constraints presented in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work was supported by John Deere under contract 476169</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tractor test platform</figDesc><graphic coords="1,351.11,221.48,172.78,129.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Side view of penetrable vegetation data, with approximate ground height. There are range points on the ground and the vegetation.</figDesc><graphic coords="2,338.56,54.31,197.88,75.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Side view of data showing transition from low grass on the left to tall dense non-penetrable vegetation on the right, with approximate ground height. In the tall dense vegetation, there are only range points on the top of the vegetation and the ground remains hidden.</figDesc><graphic coords="2,338.63,171.30,198.37,74.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>an observation vector formed by the concatenation of multiple sensor measurement vectors from an array of sensor modalities, Y k ij = [Y den , Y rem , Y ir , Y col ]. The N lidar measurements are encoded as a binary vector Y den consisting of M ones representing hits and N -M zeros representing passthroughs. Associated with each of the M hit measurements are corresponding measurements of laser remission, infrared temperatures and color (i.e. Y den = [Y 1 den , . . . , Y N den ], Y rem = [Y 1 rem , . . . , Y M rem ]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A graphical description of the model showing (a) the voxel, (b) the voxel column, and (c) the connections between voxel columns. For each voxel column ij, the model contains voxel states X k ij , observations Y k ij , and a class C ij , class height H c ij , and ground height H g ij that interact with neighbors N ij and the common class height H c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The remission observation model (b) and (c) converges to the material property distribution (a) as the number of measurements increases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Simulation experiment showing transition from ground to tall vegetation with missing and ambiguous data. The dashed line in (a) shows the true ground height. The model is able to infer the hidden ground surface (b), the height of the vegetation (c), and the correct classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Classes used in results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The white shed (a) has an appearance that is similar to the ground class, but the smooth ground prior makes such a height change very unlikely so the spatial model (b) classifies the shed as an obstacle. Classifying each column independently (c) results in the shed being classified as ground.</figDesc><graphic coords="11,52.45,54.00,159.36,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 10. (a) A challenging scene with a person in camouflage in tall weeds with low grass and a dirt mound. The person and dirt mound both have a high temperature that matches the ground class, but the spatial model (b) disambiguates the two by inferring the hidden ground beneath the vegetation. The independent result (c) shows the ground heights on top of the dense tall weeds and misclassifies parts of the person and dirt mound.</figDesc><graphic coords="12,52.45,54.00,159.37,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure10shows a challenging scene containing a camouflaged person in tall weeds with low grass and a small dirt mound to the right. Both the person and the dirt mound have high infrared temperature, so a simple obstacle detection system that performs a threshold on temperature may classify both of these objects as obstacles. We show that the structural assumptions embodied in our model allow the system to disambiguate these two objects.We trained the model on bare ground, low grass, and tall weeds. Figure10(b)gives the model ground height and classification results. Inference over the model results in the correct classification of the person and the dirt mound, as well as the two types of vegetation. The area to the right of the person in the shadow of the tall weeds is classified as ground. Although that area is actually low grass, since the system has no data from the area, ground is a reasonable estimate.Using the model structure and the known ground height under the vehicle allows the system to produce reasonable estimates of the ground height even in areas where the ground is hidden. In addition to providing a smoothing prior, neighborhood interactions allow information to propagate. Fixing the heights under the wheels affects the ground estimates in the surrounding area. Columns with little or no data can still produce useful estimates by relying on their neighborhood. The system can infer the common vegetation height of the tall weeds from areas where the ground is more directly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Ground estimate comparison for the area in Figures 10(b) &amp; 10(c), showing the improved ground height estimates of the spatial model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The tractor (a) is on a steep side slope with dense vegetation at the bottom of the hill. The spatial model (b) infers the ground height beneath the vegetation which could correctly trigger a roll-over hazard. The independent model (c) incorrectly predicts that the tractor could drive on top of the vegetation.</figDesc><graphic coords="13,77.55,54.00,109.15,134.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Fig. 13. (a) A ledge creating a discontinuity in the ground surface. The ledge violates the smooth ground assumption so the model (b) makes the reasonable inference that the ledge is an obstacle. (c) The ground height predictions beyond the ledge are lower than the true ground height.</figDesc><graphic coords="14,52.45,54.00,159.37,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Ground estimate comparison for longer test path through varied vegetation, showing predictions made 6m in front of the vehicle as the vehicle drove (note the two axes have very different scales)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The lidar scans come in at a much higher rate than the image data so multiple scans are projected into the same image. However, the high pixel density of the images means that we collect approximately 100 pixels for every lidar point. This coupled with the continual movement of the scanning lidars makes it unlikely that a single pixel is used more than once, so we treat each color and infrared tagged lidar point as an independent measurement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In our implementation, the possibility of a voxel column simultaneously containing obstacle and vegetation is excluded, though its inclusion is a trivial extension of the model we present.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where D c is the number of columns with class c.</p><p>Once the common class heights H c have been sampled, each voxel column is sampled. The first step of the sampling procedure is to find the priors over class C ij , class height H c ij and ground height H g ij from the neighbors, as given in equations 5 and 6, and the common class heights H c as given in equation 7. The priors on H c ij and H g ij are then incorporated into the HSMM model as priors over state durations and are shown in the subsequent equations as</p><p>Once the prior distributions are found, the class HSMM structures are used to find the probability of the data and the state duration probabilities for each class. HSMMs use a variant of the standard forward-backward dynamic programming solution used for inference in regular HMMs <ref type="bibr" target="#b15">[16]</ref>. As shown in figure <ref type="figure">5</ref>(b), an HSMM maintains durations (corresponding to height in our case) so that a single state is active over a number of spatial steps up the chain. This formalism is very natural for finding ground height or class height because the neighborhood information can be included as a prior on the corresponding state duration. </p><p>The forward-backward computations are still performed over the individual spatial steps X k ij as in an HMM, but with an HSMM one must solve for the duration of each state, so in addition to summing over possible state transitions x , we also sum over possible state durations h. Equations 10 and 11 give the HSMM forward and backward probabilities α k ij,c and β k ij,c</p><p>for spatial step k of the class c chain in MRF voxel column ij.</p><p>We use the observation independencies and the deterministic transitions of our chain structures to reduce the computational complexity. We use the notation x -and x + to refer to the previous and next states in the chain of the current class.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rough autonomous mobility -part 2: An active vision, predictive control approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="198" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomous cross-country navigation with the ALV</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keirsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Payton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1988-04">April 1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="718" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomous mobility for the Demo III experimental unmanned vehicles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lacaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delgiorno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assoc. for Unmanned Vehicle Systems Int. Conf. on Unmanned Vehicles (AUVSI 02)</title>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision-based perception for an autonomous harvester</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="1997-09">September 1997</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1838" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Terrain typing for real robots</title>
		<author>
			<persName><forename type="first">I</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="1995-09">September 1995</date>
			<biblScope unit="page" from="400" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation and comparison of terrain classification techniques from LADAR data for autonomous navigation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Donamukkala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Army Science Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ladar-based discrimination of grass from obstacles for autonomous navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symp. on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Foliage discrimination using a rotating ladar</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural terrain classification using 3-D ladar data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akapuria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding organized structures in 3-d ladar data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward reliable off-road autonomous vehicle operating in challenging environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural method for obstacle detection and terrain classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jochem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Unmanned Ground Vehicle Technology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online adaptive rough-terrain navigation in vegetation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning a terrain model for autonomous navigation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wellington</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From HMMs to segment models: a unified view of stochastic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Digalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="360" to="378" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Markov Random Field Modeling in Image Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>ser. Computer Science Workbench</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Double markov random fields and bayesian image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of brain MR images through a hidden markov random field model and the expectationmaximization algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithm AS 189: Maximum likelihood estimation of the parameters of the beta binomial distribution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="204" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ML parameter estimation for markov random fields with applications to bayesian tomography</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Saquib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1029" to="1044" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
