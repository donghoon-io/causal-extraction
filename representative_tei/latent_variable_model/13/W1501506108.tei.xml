<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interacting Markov Random Fields for Simultaneous Terrain Modeling and Obstacle Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carl</forename><surname>Wellington</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<email>aaronc@ri.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><surname>Stentz</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Robotics Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interacting Markov Random Fields for Simultaneous Terrain Modeling and Obstacle Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous navigation in outdoor environments with vegetation is difficult because available sensors make very indirect measurements on quantities of interest such as the supporting ground height and the location of obstacles. We introduce a terrain model that includes spatial constraints on these quantities to exploit structure found in outdoor domains and use available sensor data more effectively. The model consists of a latent variable that establishes a prior that favors vegetation of a similar height, plus multiple Markov random fields that incorporate neighborhood interactions and impose a prior on smooth ground and class continuity. These Markov random fields interact through a hidden semi-Markov model that enforces a prior on the vertical structure of elements in the environment. The system runs in real-time and has been trained and tested using real data from an agricultural setting. Results show that exploiting the 3D structure inherent in outdoor domains significantly improves ground height estimates and obstacle detection accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Outdoor environments such as those encountered in agriculture, mining, and the exploration of hazardous environments are often viewed as being "unstructured". This absence of structured features, such as road markers, straight walls and a flat ground plane has often been cited as one of the reasons navigating within these environments is considered to be challenging <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. However, such environments do possess a great deal of structure that humans frequently exploit in the performance of tasks we wish to automate. For example, consider a vehicle navigating through a field of vegetation or crop. We can use the knowledge that the ground is generally smooth and the vegetation has approximately constant height to infer the ground height and allow navigation even through areas where the ground is not directly observed. The challenge lies in expressing this type of structure in a way that can be made useful in autonomous navigation tasks.</p><p>Local autonomous navigation in outdoor environments is often performed in a model predictive control framework that searches over dynamically feasible control arcs for a safe trajectory <ref type="bibr" target="#b1">[2]</ref>. In this framework, a terrain model with obstacles and the supporting surface is used in combination with a model of the vehicle to find a dynamic trajectory that avoids obstacles while protecting against roll-over, body collisions, high-centering, and other safety conditions <ref type="bibr" target="#b3">[4]</ref>. While faithful models of vehicle dynamics are often available, acquiring an Early work in terrain perception assumed smooth terrain with discrete obstacles <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>, and achieved good results in these domains by looking at the height of the sensor readings from individual grid cells. More recent work has included cluttered environments with vegetation that are much more difficult because the range points from sensors do not generally give the load-bearing surface. To handle these challenging domains, researchers have tried to model various parts of the problem. A common approach is to model how a range sensor penetrates vegetation to discriminate between vegetation and solid objects <ref type="bibr" target="#b2">[3]</ref> [5] <ref type="bibr" target="#b5">[6]</ref>. Another approach begins with a cloud of range points and looks for various features or structure at a local level <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>. We have used online learning methods to automatically learn the ground height in vegetation from features <ref type="bibr" target="#b3">[4]</ref>. A common characteristic among these approaches is that they make the strong assumption of independence between patches of terrain for estimating ground height or class. We hope to achieve better results by relaxing this independence assumption through the inclusion of spatial correlations.</p><p>Spatial correlations in images have often been expressed using Markov random fields <ref type="bibr" target="#b8">[9]</ref> [10] and these techniques have been used for vision problems such as the segmentation of various land types in satellite images <ref type="bibr" target="#b10">[11]</ref>, but these approaches use only 2D image data instead of the 3D data that is generally available to an off-road robotic system.</p><p>In this paper, we describe a generative, probabilistic approach to modeling terrain. We exploit 3D spatial structure inherent in off-road domains and an array of noisy but abundant sensor data to jointly produce better estimates of the ground height and more accurate classification of obstacles and other areas of interest, even in dense non-penetrable vegetation.</p><p>Our terrain model consists of two distinct but interacting Markov random field models (MRFs) and a latent variable for common vegetation height. One MRF models ground height and enforces our assumption that ground height is smoothly varying. The second MRF encodes our assumption that class patches of space tend to cluster (for example, patches of vegetation of a single type tend to be found together). The latent variable for common vegetation height enforces our assumption that vegetation of the same type generally has a similar height. These three components interact through a hidden semi-Markov model (HSMM) that enforces vertical structural assumptions such as the understanding that vegetation grows on top of ground.</p><p>The structure in the terrain model is combined with information from multiple sensors on the vehicle using sensor models that are automatically learned from training data. Obstacles are treated as having uncertain attributes so obstacle appearance does not need to be explicitly trained.</p><p>Joint inference of ground height, class height and class identity over the whole model results in more accurate estimation of each quantity. For example, inferring the vegetation height allows for an improved estimate of the height of the underlying ground. Similarly, knowing the ground height helps disambiguate solid obstacles from the ground surface.</p><p>Our approach allows us to model 3D structure in a reasonably efficient inference scheme. Gibbs sampling over the MRF structures lets us perform exact inference in the HSMM models using an efficient dynamic programming algorithm. This substantially reduces computation time over a fully 3D MRF model, and allows our system to run in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VEHICLE PLATFORM AND DATA REPRESENTATION</head><p>Our project team has automated a John Deere 6410 tractor (see figure <ref type="figure" target="#fig_0">1</ref>) and equipped it with many sensors for localization and perception <ref type="bibr" target="#b3">[4]</ref>. The vehicle has a highresolution stereo pair of digital cameras, an infrared camera, and two SICK laser range-finders (ladar) mounted on custom actively-controlled scanning mounts. The first scanning ladar is mounted on the roof to get range data over a large area in front of the vehicle, and the second scanning ladar is mounted on the bumper to get high density measurements of nearby terrain and better penetrate upcoming vegetation. The cameras and scanned ladars are precisely calibrated and tightly synchronized with an accurate global vehicle pose estimate.</p><p>The basic representational structure of our terrain model is the voxel: a 15cm 3 box-shaped region of 3 dimensional space. We represent the vehicle's local spatial environment as a voxel lattice of size I x J x K, where the ijkth voxel is in the ijth position of a horizontal 2D grid and the kth position above an arbitrary subterranean origin.</p><p>Accurate global vehicle pose allows us to assign ladar points corresponding to the same region of space to the same voxel. Exploiting the precise synchronization of the sensors, we project ladar points into the most recent color and infrared images, so that each ladar point results in a vector of appearance measurements for that voxel, including laser remission (reflectance), infrared temperature, and color. <ref type="foot" target="#foot_0">1</ref>The voxel representation also allows us to maintain a density estimate throughout space by comparing how many ladar rays pass through each voxel (pass-throughs) with the number of ladar rays that hit something in that voxel (hits). Density information is valuable when trying to separate sparse vegetation that contains a mixture of hits and pass-throughs from solid objects that contain a majority of hits and only a few pass-throughs due to sensor noise <ref type="bibr">[3] [4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TERRAIN MODEL</head><p>Although our data representation is based on the voxel, vehicle navigation is generally performed on a 2D surface, so our ground height estimates and classification results are made in terms of voxel columns. In our model, the ijth voxel column class is described with a multinomial distributed random variable C ij taking on values related to the possible contents of the column, C ij = c with e.g. c ∈ {ground , vegetation, obstacle}.</p><p>Associated with the kth voxel in the ijth voxel column is the voxel state X k ij , a multinomial distributed random variable that describes the nature of the material inside the voxel, X k ij ∈ {ground , c, free-space}, where c is the class of the ijth voxel column. <ref type="foot" target="#foot_1">2</ref> The ijkth voxel is also associated with the observation vector </p><formula xml:id="formula_0">Y k ij = [Y den , Y rem , Y ir , Y col ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observation Models</head><p>We assume that voxels form the smallest indistinguishable element of space, occupied completely by one (and only one) voxel state. Each voxel state maintains a distribution over material properties including density, remission, infrared temperature, and color that describe the characteristics of that state, but the material inside a single voxel is assumed to be uniform. For example, the vegetation state may include a range of colors, and therefore different voxels in vegetation may have different colors, but we assume that the color of the vegetation within each voxel is uniform.</p><p>The measurement vector, Y k ij contains a variable number of noisy measurements of the material properties. The graphical  <ref type="figure" target="#fig_1">2</ref>(a) illustrates the conditional independencies between the voxel state X k ij , the material property random variables den, rem, ir and col , and the measurements. Conditional on X k ij , the material properties are independent, and conditional on the material properties, the measurements are independent. The voxel material properties are not directly observed, and we are not concerned with their values beyond what they reveal about the state. Thus material properties constitute nuisance variables that we remove from the observation models through marginalization.</p><formula xml:id="formula_1">rem col ir den Y rem M Y den N Y rem 1 Y ir M Y ir 1 Y den 1 Y col 1 Y col M X ij k (a) Voxel model C = Vegetation Ground Vegetation Free-Space Y ij 5 X ij 1 X ij 2 X ij 3 ij C = Ground ij X ij 4 X ij 5 X ij 6 X ij k X ij K Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K Y ij 5 Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K X ij 1 X ij 2 X ij 3 X ij 4 X ij 5 X ij 6 X ij k X ij K C = Obstacle Y ij 5 X ij 1 X ij 2 X ij 3 ij X ij 4 X ij 5 X ij 6 X ij k X ij K Y ij 1 Y ij 2 Y ij 3 Y ij 4 Y ij 6 Y ij k Y ij K Free-Space Free-Space Ground Obstacle Ground H ij c H ij g (b) HSMM models Y ij C ij X ij H ij c H c H ij g C Nij C Nij C Nij C Nij H Nij g H Nij g H Nij g H Nij g (c) MRF model</formula><p>Density values range from empty space (den = 0) to completely solid (den = 1), and we use a beta distribution B(a x , b x ) to describe the density values of each state x. The measurements of density Y n den are binary (ladar hit or passthrough), so we use a binomial distribution to describe the number of hits M = N n=1 Y n den out of N total rays. When we integrate over the nuisance parameter den, we recover the beta-binomial distribution as the marginal likelihood observation model.</p><formula xml:id="formula_2">P (M = m | X k ij = x) = P (m | den)p(den | X k ij = x) d(den) = N M B(a x + M, b x + N -M ) B(a x , b x )<label>(1)</label></formula><p>The distributions over the voxel appearance properties, including infrared temperature, laser remission and color, are all inherently multi-modal and thus not well described by a simple parametric distribution. For example, remission values in vegetation are either high because of the strong reflectivity of chlorophyll, or very low due to small cross-sectional area. We resort to a mixture of Gaussians to describe the distribution of the material properties within a state.</p><p>We develop the marginal distribution for the remission values, but the infrared and color data are determined analogously. The true material property rem for state x is modeled as a mixture of Gaussians with individual mixture means rem i , variances σ 2 i , and mixing coefficients P (i).</p><formula xml:id="formula_3">p(rem|X k ij = x) = R i=1 P (i) 1 2πσ 2 i exp - (rem -rem i ) 2 2σ 2 i</formula><p>(2) Conditional on the true material property rem, the measurements y m rem are assumed to be normally distributed, y m rem ∼ N (rem, σ 2 y ). As with the density, we integrate out the nuisance variable rem to get the marginal likelihood for all the remission data y rem = [y 1 rem , . . . , y M rem ], resulting in a mixture of Gaussians that is a function of the data mean ȳrem .</p><formula xml:id="formula_4">p(y rem | X k ij = x) = p(y rem | rem)p(rem | X k ij = x) d(rem) = R i=1 P (i) M m=1 p(y m rem | rem)p(rem | rem i ) d(rem) = R i=1 P (i) 1 2π σ 2 i + σ 2 y M exp   - (ȳ rem -rem i ) 2 2 σ 2 i + σ 2 y M  <label>(3)</label></formula><p>Equation 3 shows that the marginal appearance distributions become more broad when there are few data points (M is small), reflecting the increased uncertainty in the material property and hence the state.</p><p>The free-space state does not possess any meaningful material properties beyond density den. Ladar hits occurring in free-space are generally the result of noise so we model the non-density material properties as matching the material properties of the states in contact with free-space.</p><p>Although we expect obstacles to generally have a fairly high density den, we cannot hope to build an accurate observation model for the appearance of each of the innumerable obstacles one might encounter in outdoor environments, so we simply use a single obstacle state with a corresponding uniform distribution over the observable range of material appearance properties. We rely on accurately modeling the features of the trained states to detect obstacles as a default option when none of the other states are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mixture of Hidden Semi-Markov Chains of Voxel Columns</head><p>When moving from lower to higher voxels within a column, we expect to move from ground to vegetation, or perhaps ground to obstacle, and eventually to free-space. We never expect free-space to be found below ground, nor do we expect vegetation to be suspended above free-space.</p><p>This type of structure is naturally imposed by introducing a Markov dependency between voxel states that restricts vertical transitions, thus defining a hidden Markov model within each voxel column. However, the duration of states such as ground and vegetation are not well modeled as states in a Markov chain which would induce a geometric distribution on the duration of states. We resort instead to a hidden semi-Markov model (HSMM) <ref type="bibr" target="#b11">[12]</ref> over voxel states, which explicitly represents a state duration (or height distribution) over voxels for each state value.</p><p>As shown in figure <ref type="figure" target="#fig_1">2</ref>(b), we associate a single HSMM chain structure with each column class C ij , which makes the resulting column model a mixture of HSMMs. The durations of the ground and class states describe the height of those terrain elements and are given by H g ij and H c ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Markov Random Field Model of Interacting Voxel Columns</head><p>The HSMM column models capture the vertical structure between the states, but there are also significant horizontal dependencies between neighboring columns. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(c), we model these dependencies using two distinct but interacting Markov random fields (MRFs) <ref type="bibr" target="#b9">[10]</ref> for class C ij and ground height H g ij , each dependent on the values of their respective neighbors, and a latent variable for the common class height H c across all columns. These variables interact through the HSMM column models by imposing a prior on the state durations associated with H c ij and H g ij and imposing a prior over HSMM class models C ij .</p><p>The neighborhood dependency of C ij reflects the prior assumption that class identities are positively correlated with their neighbors so voxel columns tend to cluster in contiguous groups of the same class. We express this preference using the conditional MRF distribution</p><formula xml:id="formula_5">P (C ij = c | C Nij ) ∝ exp -λ C {s,t}∈Nij (c = c st )<label>(4)</label></formula><p>where N ij is the set of neighboring indices and C Nij is the set of classes in the neighborhood of the ijth voxel column. Ground height varies smoothly from one patch of ground to the next, so we expect that H g ij will be tightly correlated with nearby values. We express this belief using a Gaussian Markov random field</p><formula xml:id="formula_6">P (H g ij = h | H g Nij ) ∝ exp - 1 2σ 2 G h - 1 |N ij | {s,t}∈Nij h g st 2 (5)</formula><p>where |N ij | is the size of the neighborhood.</p><p>We expect that vegetation of the same class c has a similar height H c with some variation. This assumption may not be valid for obstacles, so we only apply it to vegetation classes. Given the common height of the vegetation in this area H c , we model the expected variation with a Gaussian</p><formula xml:id="formula_7">P (H c ij = h | H c ) ∝ exp - 1 2σ 2 H c (h -h c ) 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INFERENCE</head><p>The interacting Markov random fields of this model capture important structure, but these dependencies prevent analytic determination of the posterior distribution P (C, H g , H c | Y ). The set of HSMMs that describe the data in each column of voxels can efficiently produce distributions over the state durations, which makes it easy to sample from the conditional distribution</p><formula xml:id="formula_8">P (C ij , H g ij , H c ij | Y ij , C Nij , H g Nij , H c )<label>(7)</label></formula><p>so we use Gibbs sampling <ref type="bibr" target="#b8">[9]</ref> for approximate inference. Algorithm 1 gives the application of Gibbs sampling to our model. The HSMM column models require a distribution over class heights which comes from the common class height latent variable H c , as shown in Figure <ref type="figure" target="#fig_1">2(c</ref>). Samples of the common class height are produced from its conditional distribution given the current column class height samples h c ij</p><formula xml:id="formula_9">P (H c = h | H c ij∈IJ ) ∝ exp -1 2σ 2 H c /D c h- 1 D c ij∈IJ,cij =c h c ij 2<label>(8</label></formula><p>) where D c is the number of columns with class c.</p><p>Once the common class heights H c have been sampled, each voxel column is sampled. The first step of the sampling procedure is to find the priors over class C ij , class height H c ij and ground height H g ij from the neighbors, as given in equations 4 and 5, and the common class heights H c as given in equation 6. The priors on H c ij and H g ij are then incorporated into the HSMM model as priors over state durations and are shown in the subsequent equations as</p><formula xml:id="formula_10">P (H c ij = h | H c ) for the class state x = c or P (H g ij = h | H g Nij ) for the ground state x = g. (9) β k ij,c (x) = P (Y k+1:K ij | state x ends at k, C ij = c, H g Nij , H c ) = x h P (Y k+1:K ij |X k ij = x,X k+h ij = x ,H x + ij = h,C ij ,H g Nij ,H c ) = h k+h k =k+1 P (Y k ij | x + )P (H x + ij = h | H g Nij , H c )β k+h ij,c (x + )<label>(10)</label></formula><p>Since we know by assumption that the chain must end in the final state x = free-space, the probability of the data for class c is the final value of α in that state.</p><formula xml:id="formula_11">P (Y ij | C ij = c, H g Nij , H c ) = α K ij,c (x = free-space) (11)</formula><p>As described in Algorithm 1, this is combined with the class prior P (C ij | C Nij ) to find the distribution over classes, which is used to sample a new class.</p><p>Finding the distribution over state durations involves combining α and β.</p><formula xml:id="formula_12">ζ x ij,c (h) = P (state x has duration h | Y ij , C ij = c, H g Nij , H c ) = k P (X k ij = x, X k-h ij = x -| Y ij , C ij , H g Nij , H c ) = k k k =k-h+1 P (Y k ij |x)P (H x ij = h|H g Nij , H c )α k-h ij,c (x -)β k ij,c (x)<label>(12)</label></formula><p>We know that in each chain, every state transition must occur after some duration, so we can normalize by h ζ x ij,c (h) to get the posterior on ground and class height conditional on the neighbors. Samples are then drawn from these distributions.</p><formula xml:id="formula_13">P (H g ij = h | C ij = c, Y ij , H g Nij , H c ) = ζ x=ground ij,c<label>(h)</label></formula><formula xml:id="formula_14">P (H c ij = h | C ij = c, Y ij , H g Nij , H c ) = ζ x=state c ij,c (h)<label>(13)</label></formula><p>The time complexity of HSMM calculations is greater than an HMM because of the sum over possible durations, but the observation likelihood products can be pre-computed and the state durations to search over can be constrained based on the priors to reduce the complexity to O(numVoxels * numStates * maxDuration) for a single chain.</p><p>Although it is typically difficult to show that Gibbs sampling has converged, we have found empirically that the model finds a good estimate quickly, allowing for real-time execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNING</head><p>The model described in section III incorporates prior knowledge about the structure of the environment, but the specific model parameters must be learned from training data. These parameters include the sensor observation models for each state and the neighborhood interactions for class, class height, and ground height. The generative nature of our model allows us to decouple the learning problems, and train each of these observation and neighborhood interaction models individually, thus greatly simplifying the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observation Models</head><p>Collecting labeled training data is often expensive, especially in outdoor environments where there can be high variation in sensor readings so that a large training set is needed. We use an approach based on <ref type="bibr" target="#b3">[4]</ref> to collect large quantities of labeled training data to automatically train our observation models. Specifically, we drive through representative terrain of a single class such as vegetation and store the sensor measurements from the voxels of columns that we drive over as training examples for that class. This process is then repeated for other classes such as ground. Unlike <ref type="bibr" target="#b3">[4]</ref> which directly trains on the height of different types of vegetation, we only train on the various material properties of vegetation voxels, allowing us to remain general across vegetation heights.</p><p>Each labeled voxel collected by driving through representative terrain is used as a training example for the observation models in equations 1, 2, and 3. For appearance data such as remission, infrared and color, the mean values from each voxel are used to train the GMM observation models (i.e. rem i , σ 2 i , P (i) in equation <ref type="formula">2</ref>) and the variance of measurements within the voxels is used as the GMM measurement model variance (σ 2 y in equation 3). Hit and pass-through data from the labeled training voxels are used to find the maximum likelihood parameters of the beta-binomial density model (a x and b x in equation 1) for each class state x using a Newton-Raphson method <ref type="bibr" target="#b12">[13]</ref>. This handles class states like ground and vegetation, but the density of obstacle and free-space states must also be trained. The freespace density can be trained using data that includes insects or dust that occasionally returns a ladar point, or it can just be set manually to strongly favor empty space. Similarly, the obstacle density can be trained using hit and pass-through data from representative obstacles, or it can be set manually to favor dense objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neighborhood Models</head><p>The priors given in equations 4 and 5 describe how class and ground height depend on their neighbors, and the prior in equation 6 describes how column class heights are related to the common class height. Each of these priors contains a parameter that gives the strength of the prior, and describes how much classes tend to clump together, how smooth the ground is, and how little class heights vary. As above, we train these parameters by driving over representative terrain.</p><p>As we drive over an area, we record the ground heights measured by the location of our wheels. We use these height sequences to find the standard deviation σ G of typical ground height variation between voxel columns, which gives us the maximum likelihood estimate of our Gaussian MRF ground neighborhood prior.</p><p>Similarly, as we drive through vegetation, we get an approximate vegetation height measurement by taking the highest ladar hit and subtracting the known ground height (from the wheel locations). Since we assume that vegetation heights are independent given the common vegetation height in the area, we can find the class prior standard deviation σ H c directly from this sequence of class heights.</p><p>The class interaction prior λ C gives the probability that a class transitions to a different class. This could be estimated directly with class-labeled data over a large area that includes many class transitions, but unlike the labeled data for the observation models or the ground and class height interactions, this type of training data is difficult to collect. However, changing the class interaction prior affects the system output in an intuitive way by controlling how much classes tend to clump together, so this parameter can be set manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>We have tested this model in a nearby working farm and an undeveloped area with tall weeds. The following three examples demonstrate the improved performance gained from using the structure we have built into our model.</p><p>In each case, after training the model on representative terrain, we drive the vehicle through the test area, while letting the Gibbs sampler run continuously. Running at 1Hz, the system calculates observation likelihood products, computes samples from the model, and updates the local terrain map with the maximum a posteriori (MAP) class label, mean ground height, and mean class height from the samples in each column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. White shed</head><p>Figure <ref type="figure">3</ref> shows the view from the tractor as it approaches a white shed. This is a large obstacle that could be reliably detected in a variety of ways, but it will serve as a good example of how the various pieces of our model interact to produce the correct result. Figure <ref type="figure">4</ref> shows the output of our model including the MAP class labels: obstacle (red), vegetation (green), ground (gray), and the mean ground height for driveable areas. Obstacle columns are shown at their class height. The model produces a reasonable classification of the scene and a smooth ground estimate that would work well for vehicle navigation. It classifies the shed as an obstacle and correctly captures the hill sloping down to the right despite the presence of sparse vegetation.</p><p>This example is interesting because on a voxel basis the ground class is much more likely than the broad uniform obstacle class for the voxels from the shed. However, the MRF and HSMM spatial constraints imposed on the ground surface make it extremely unlikely that the ground height would have a tall step discontinuity at the shed wall. Since the density and appearance data are not well described by the vegetation class, the shed is correctly classified as an obstacle.</p><p>Figure <ref type="figure">5</ref> shows the output of the system when the neighborhood interactions are ignored and the columns are assumed to be independent. Without neighborhood information, classification is based solely on the data likelihood for each column HSMM model. Lacking the smooth ground prior, the wall is classified as a collection of tall columns of ground voxels. Figure <ref type="figure">5</ref> also shows that without the ground and class priors, the ground height estimates and classification labels are generally more noisy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tall vegetation</head><p>Figure <ref type="figure">6</ref> shows the view from the tractor in a challenging scene: a camouflaged person in tall weeds with low grass and a small dirt mound to the right. Both the person and the dirt mound have high infrared temperature. We trained on the two types of vegetation and bare ground. Figure <ref type="figure">7</ref> gives the mean ground heights and MAP classification results. Inference over the model results in the correct classification of the person and the dirt mound, as well as the two types of vegetation. The area to the right of the person in the shadow of the tall weeds is classified as ground. Although that area is actually low grass, since the system has no data from the area, ground is a reasonable estimate.</p><p>Using the model structure and the known ground height under the vehicle allows the system to produce reasonable estimates of the ground height even in areas where the ground is hidden. In addition to providing a smoothing prior, neighborhood interactions allow information to propagate. Fixing the heights under the wheels affects the ground estimates in the surrounding area. Columns with little or no data can still produce useful estimates using their neighborhood. The system can infer the common vegetation height of the tall weeds from areas where it is observable, such as under the vehicle or the transition to tall weeds behind the person. The assumption of vegetation height similarity then allows the system to infer the ground height in areas where the ground is not directly observable. Knowing the ground height allows the model to explain the dirt mound as a rise in the ground but the person as an obstacle.</p><p>The range points do not penetrate the tall weeds in this example, as shown in Figure <ref type="figure">8</ref>, which uses the lowest hit or pass-through in each vegetation column for ground height and the MAP class labels when no neighborhood information is used. Assuming independence prevents information from propagating and the resulting ground height estimates are poor. Also, both the person and the dirt mound contain a mixture of obstacle and ground classes.</p><p>Figure <ref type="figure" target="#fig_4">9</ref> shows a plot of the quality of the ground height estimates from Figures <ref type="figure">7</ref> and<ref type="figure">8</ref>. After computing estimates of the ground height using our model, we drove through the scene toward the area between the person and the dirt mound, and made measurements of the ground height using our wheel locations. This trajectory is marked as "True height" in Figure <ref type="figure" target="#fig_4">9</ref>, and offers a comparison for the estimates produced by the model and those using the lowest hit or pass-through in each column. The model ground estimates are fairly smooth and stay within approximately 20cm of the true value.</p><p>As another comparison, we show an approach that adjusts the lowest hit in each column based on that column's independent classification. Instead of using spatial structure to infer the vegetation height from the data as in our model, this approach simply uses the average height of each class from the training data for the offset. Figure <ref type="figure" target="#fig_4">9</ref> shows that this can work well when the classification is correct and the actual vegetation height  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Longer run through varied vegetation</head><p>Figure <ref type="figure" target="#fig_5">10</ref> shows ground height estimates for a longer test set through varied vegetation. Unlike Figure <ref type="figure" target="#fig_4">9</ref>, which presents a snapshot of the predictions at different distances in front of the vehicle at a given time, Figure <ref type="figure" target="#fig_5">10</ref> shows predictions 6m in front of the vehicle over time as the vehicle drove. The lowest hit line shows that the first 70m of the path contains two sections of tall dense non-penetrable vegetation, and the remainder of the path has low vegetation with various tall sparse vegetation and a few small patches of dense vegetation (e.g. 170m). The model output is generally smooth and closely matches the true height, whereas the lowest hit rarely reaches the ground, and the lowest hit with class offset is often correct but very noisy because of misclassifications due to its independence assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE WORK</head><p>We have described a novel model structure that allows multiple Markov random fields to interact through a hidden semi-Markov model for improved ground height estimation and classification for outdoor navigation. This structure enforces spatial constraints within a column and between neighboring columns. The model provides a natural way of combining different types of sensor data. It can find obstacles without needing to explicitly model them or collect obstacle appearance training data. It can infer vegetation height to produce estimates of the supporting ground surface even when the ground is hidden by dense vegetation. Except for the class neighborhood prior, the sensor and interaction model parameters can be easily trained by simply driving through representative areas. The system runs in real-time on real data and we showed that including the neighborhood structure significantly improved both the ground height estimates and the obstacle classification over an equivalent model without neighborhood interactions.</p><p>We are currently working on several improvements to the model. The system is set up essentially as a batch process, even though data is continually coming in and the sampling procedure continues over time. We would like to make it a true online algorithm. We are experimenting with further class models to handle hanging obstacles and holes. Finally, we are looking into belief propagation and other approximate inference schemes that might be less computationally intensive than Gibbs sampling. ACKNOWLEDGEMENT This work was supported by John Deere under contract 476169</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tractor test platform</figDesc><graphic coords="1,351.11,221.48,172.78,129.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A graphical description of the model showing (a) the voxel, (b) the voxel column, and (c) the connections between voxel columns. For each voxel column ij, the model contains voxel states X k ij , observations Y k ij , and a class C ij , class height H c ij , and ground height H g ij that interact with neighbors N ij and the common class height H c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .Fig. 5 .Fig. 6 .</head><label>3456</label><figDesc>Fig. 3. View from the tractor of a white shed</figDesc><graphic coords="7,48.96,54.01,164.49,123.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. System output, including ground heights and classification Fig. 8. Lowest hit or pass for ground height in vegetation and independent classification, showing poor ground height estimates and misclassifications</figDesc><graphic coords="7,223.74,225.30,164.50,123.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison between ground estimates of the area in Figures 7 &amp; 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparison between ground estimates for longer test path, showing predictions made 6m in front of the vehicle as the vehicle drove</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The ladar scans come in at a much higher rate than the image data so multiple scans are projected into the same image. However, the high pixel density of the images means that we collect approximately 100 pixels for every ladar point. This coupled with the continual movement of the scanning ladars makes it unlikely that a single pixel is used more than once, so we treat each color and infrared tagged ladar point as an independent measurement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In our implementation, the possibility of a voxel column simultaneously containing obstacle and vegetation is excluded, though its inclusion is a trivial extension of the model we present.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Gibbs sampling from the model Sample common class heights h c from P (H c | H c ij∈IJ ) using all the column class height samples of the same class for all MRF voxel columns ij do Find ground and class priors from neighbors:</p><p>Find class height prior from common class height of same class:</p><p>) Use class HSMM to find probability of the data and distributions over the ground and class height:</p><p>Once the prior distributions are found, the class HSMM structures are used to find the probability of the data and the state duration probabilities for each class. HSMMs use a variant of the standard forward-backward dynamic programming solution used for inference in regular HMMs <ref type="bibr" target="#b11">[12]</ref>. As shown in figure <ref type="figure">2</ref>(b), an HSMM maintains durations (corresponding to height in our case) so that a single state is active over a number of spatial steps up the chain. This formalism is very natural for finding ground height or class height because the neighborhood information can be included as a prior on the corresponding state duration.</p><p>The forward-backward computations are still performed over the individual spatial steps X k ij as in an HMM, but an HSMM must solve for the duration of each state, so in addition to summing over possible state transitions x , we also sum over possible state durations h. Equations 9 and 10 give the HSMM forward and backward probabilities α k ij,c and β k ij,c for spatial step k of the class c chain in MRF voxel column ij. We use the observation independencies and the deterministic transitions of our chain structures to reduce the computational complexity. We use the notation x -and x + to refer to the previous and next states in the chain of the current class.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autonomous cross-country navigation with the ALV</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keirsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Payton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1988-04">April 1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="718" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rough terrain autonomous mobility -part 2: An active vision, predictive control approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="198" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomous mobility for the Demo III experimental unmanned vehicles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lacaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delgiorno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assoc. for Unmanned Vehicle Systems Int. Conf. on Unmanned Vehicles (AUVSI 02)</title>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online adaptive rough-terrain navigation in vegetation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ladar-based discrimination of grass from obstacles for autonomous navigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symp. on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Foliage discrimination using a rotating ladar</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural terrain classification using 3-D ladar data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akapuria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding organized structures in 3-d ladar data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Markov Random Field Modeling in Image Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>ser. Computer Science Workbench</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Double markov random fields and bayesian image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From HMMs to segment models: a unified view of stochastic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Digalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="360" to="378" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Algorithm AS 189: Maximum likelihood estimation of the parameters of the beta binomial distribution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="204" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
