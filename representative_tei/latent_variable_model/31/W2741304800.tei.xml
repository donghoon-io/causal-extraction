<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Mixtures of Gaussian Processes for Classification</title>
				<funder ref="#_Nruv8Cx">
					<orgName type="full">Shanghai Knowledge Service Platform Project</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_46Wtk6M #_thuGrJr">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
							<email>slsun@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<addrLine>3663 North Zhongshan Road</addrLine>
									<postCode>200062</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Mixtures of Gaussian Processes for Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gaussian Processes (GPs) are powerful tools for machine learning which have been applied to both classification and regression. The mixture models of GPs were later proposed to further improve GPs for data modeling. However, these models are formulated for regression problems. In this work, we propose a new Mixture of Gaussian Processes for Classification (MGPC). Instead of the Gaussian likelihood for regression, MGPC employs the logistic function as likelihood to obtain the class probabilities, which is suitable for classification problems. The posterior distribution of latent variables is approximated through variational inference. The hyperparameters are optimized through the variational EM method and a greedy algorithm. Experiments are performed on multiple real-world datasets which show improvements over five widely used methods on predictive performance. The results also indicate that for classification MGPC is significantly better than the regression model with mixtures of GPs, different from the existing consensus that their single model counterparts are comparable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian Processes (GPs) specify a collection of latent random variables f which have a joint Gaussian distribution <ref type="bibr" target="#b3">[Rasmussen and Williams, 2006]</ref>. GP is a powerful tool for probability modeling in machine learning and has been applied to both classification and regression problems with different kinds of likelihood p(y|f ) where variables y are outputs. Gaussian likelihood is the typical choice for regression models, which leads to convenience in derivation. Logistic and probit functions are commonly used in GP classification models where inferences are not in closed form. In general, both GP regression and classification models are discriminative models, i.e., only the conditional distribution p(f |X) is formulated.</p><p>GP-based mixture models have been proposed for overcoming the limitations of single GP models: First, GP is incapable of handling multi-modality in data. Second, the computational complexity for calculating the inverse kernel matrix is O(N 3 ) with N being the number of training points. <ref type="bibr">Tresp [2001]</ref> proposed the mixture of GP regression models with finite mixture components. Rasmussen and Ghahramani <ref type="bibr">[2002]</ref> introduced a Dirichlet Process (DP) based gating network and extended the mixture of GPs to accommodate an infinite number of components. The first limitation is overcome by incorporating multiple GPs. The cubic computational complexity is resolved through replacing the inversion of a large kernel matrix by inversions of multiple small matrices. For each component, only a subset of training set is used for calculating the kernel matrix, which leads to lower computational complexity. In <ref type="bibr" target="#b3">Meeds and Osindero [2006]</ref>, inputs are assumed to be Gaussian distributed. The distribution of inputs is involved in gating network for effective estimations of mixture weights. This modification adapts previous discriminative models to generative ones. Markov Chain Monte Carlo (MCMC) methods are adopted for approximate posterior inference in the above work. However, MCMC methods demand expensive time for both training and prediction. The convergence of these methods is also difficult to identify.</p><p>Variational inference is a deterministic approximate technique which is a commonly used alternative for MCMC methods <ref type="bibr" target="#b0">[Bishop, 2006]</ref>. The idea of variational inference is approximating intractable true posterior distribution p(z|X) with a variational distribution q(z) by minimizing the KL divergence between q(z) and p(z|X). It presents an analytical formulation of approximate posterior distribution, which simplifies the integrals in model training and prediction.</p><p>A finite mixture model of GP experts which employs variational inference has been proposed in <ref type="bibr" target="#b5">[Yuan and Neubauer, 2009]</ref>. <ref type="bibr">Recently, Sun and Xu [2011]</ref> proposed a new variational approximation algorithm for the infinite mixture model of GPs and applied it to traffic prediction problems. In the above work, the variational distribution of latent variables is assumed to have a fully factorized form, i.e., latent variables are independent of each other. Thus, the expectations taken with respect to q(z) are decoupled, which simplifies calculations greatly. Following this assumption, the technique is known as mean field variational inference. Although with simple posterior assumptions, mean field variational inference is a good choice for approximate posterior inference <ref type="bibr" target="#b0">[Blei et al., 2016]</ref> and commonly used in recent work <ref type="bibr" target="#b2">[Gal et al., 2015;</ref><ref type="bibr" target="#b2">Hensman et al., 2015]</ref> The rest of this paper is organized as follows. Section 2 introduces our model with a brief overview of necessary background. Then, Section 3 gives the details of variational inference and optimization algorithms. In Section 4, we present classification performances on real-world datasets and provide interesting discussions. Finally, we give conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Model</head><p>The GP jointly models outputs as a multivariate Gaussian distribution. The linear GP model is an equivalent parametric representation to the GP, which introduces an intermediate variable for breaking the dependencies among outputs. The conditional independencies not only facilitate derivations of variational inference but also simplify the predictive distribution, which we will show in Section 3. In this section, we first introduce the GP and linear GP model with their equivalence. Then we give a brief introduction about the stick-breaking construction of DPs. Finally, we show the graphical representation and details of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gaussian Processes</head><p>A noise-free GP f is specified as</p><formula xml:id="formula_0">f ∼ GP(0, κ(•, •))<label>(1)</label></formula><p>where κ(•, •) is the kernel function (a.k.a., covariance function). For inputs X = {x n } N n=1 , the joint distribution of outputs is given by a multivariate Gaussian distribution,</p><formula xml:id="formula_1">p(f |X) = N (0, K xx ) (2)</formula><p>where K xx is the N × N kernel matrix obtained by κ(•, •).</p><p>We can see that the dependencies among f are specified by K xx implicitly. Now we introduce the intermediate variable w to formulate the linear GP model. Variable w is Gaussian distributed as N (0, K -1 xx ). The linear GP model is specified by a univariate Gaussian distribution as follows,</p><formula xml:id="formula_2">p(f |x, w, r) = N (w φ(x), r -1 ) (3)</formula><p>where r is the inverse variance and φ(x) is a vector defined by covariance function</p><formula xml:id="formula_3">φ(x) = [κ(x, x 1 ), κ(x, x 2 ), ..., κ(x, x N )] . (4) Actually, K xx = [φ(x 1 ), φ(x 2 ), ...φ(x N )]. We recover f = [f 1 , f 2 , ...f N ] through f = K xx w + ξ</formula><p>where ξ has a Gaussian distribution N (0, r -1 ). It shows that f has a multivarate Gaussian distribution N (0, K xx + r -1 E) which is equivalent to GP with independent noise N (0, r -1 E), where E is the identity matrix. Further, given {x, w, r}, the outputs are conditional independent. Now, we introduce the mixture of GPs. Each component in our model has the formulation of Eq. ( <ref type="formula">3</ref>) with different parameters and is assumed to have a support set I t of M training instances, which is a subset of the complete training set. z is a variable which indicates the corresponding component that the instance belongs to. Given z = t, f is distributed as</p><formula xml:id="formula_4">N (w t φ t (x), r -1 t ). φ t (x) and K t are calculated over support set I t through covariance function κ t (•, •). Variable w t has a Gaussian distribution N (0, U -1 t ) where U t = K t + σ 2 tb E. The additional term σ 2</formula><p>tb E aims to avoiding matrix singularity. We assume that r r has a gamma distribution Γ(r t |a 0 , b 0 ).</p><p>We choose the radial basis function kernel with automatic relevance determination <ref type="bibr" target="#b3">[Rasmussen and Williams, 2006]</ref> for each component, whose formulation is</p><formula xml:id="formula_5">κ t (x i , x j ) = σ 2 tf exp - 1 2 (x i -x j ) Λ -1 (x i -x j )<label>(5)</label></formula><p>where Λ = diag(σ 2 t1 , σ 2 t2 , ..., σ 2 td ) with d denoting the dimension of x. The hypermeters that define the kth GP component are covariance function parameter θ t = {σ tb , σ tf , σ t1 , σ t2 , ..., σ td } and support set I t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dirichlet Processes</head><p>The DP <ref type="bibr" target="#b1">[Ferguson, 1973]</ref> is a commonly used prior model for Bayesian nonparametric modeling. A draw from a DP is a discrete distribution over countably infinite atoms. Thus, the DPs have been adopted to extend finite mixture models to accommodate countably infinite components, where each atom represents a mixture component. The stick-breaking construction of DPs <ref type="bibr" target="#b4">[Sethuraman, 1994]</ref> is adopted for our model.</p><p>Suppose that H is a base distribution.</p><formula xml:id="formula_6">Φ = {Φ 1 , Φ 2 , ..., Φ ∞ } and ν = {ν 1 , ν 2 , ..., ν ∞ }</formula><p>are two infinite sets of independent random variables which are drawn from H and Beta(1, α 0 ), respectively. Correspondingly, infinite proportion variables π = {π 1 , π 2 , ..., π ∞ } are introduced for representing the probabilities for atoms in Φ. For the kth atom,</p><formula xml:id="formula_7">π i = ν i i-1 j=1 (1 -ν j ). The stick-breaking construc- tion of the DP G is formulated as G = ∞ i=1 π i δ Φi (6)</formula><p>where δ Φi is the delta function at Φ i . This construction is analogous to breaking a stick with unit length for infinite times. First, we break the stick into two parts at position ν 1 , i.e., π 1 . The stick of length-π 1 is the current stick and the rest is the remaining stick which has length-(1ν 1 ). Then we repeat breaking the remaining stick, infinitely. Clearly,</p><formula xml:id="formula_8">∞ i=1 π i = 1, and G is a discrete distribution.</formula><p>The distribution of component indicator variable z is regarded as a categorical distribution Cat(π). More formally, given ν, the distribution p(z|ν) is</p><formula xml:id="formula_9">p(z|ν) = ∞ t=1 (1 -ν t ) 1[z&gt;t] ν 1[z=t] t (7)</formula><p>where 1[z &gt; t] and 1[z = t] are indicator functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mixtures of Gaussian Processes for Classification</head><p>In this section, we will introduce the details of MGPC. First, suppose that training set</p><formula xml:id="formula_10">D is {(x n , y n )} N n=1 . The complete latent variable set is Ω = {ν, µ, R, w, r, z, f }, where ν = {ν 1 , ν 2 , ..., ν ∞ }, µ = {µ 1 , µ 2 , ..., µ ∞ }, R = {R 1 , R 2 , ..., R ∞ }, w = {w 1 , w 2 , ..., w ∞ }, r = {r 1 , r 2 , ..., r ∞ }, z = {z 1 , z 2 , ..., z N }, and f = {f 1 , f 2 , ..., f N }. The complete hyperparameter set Θ is {θ, I, α 0 , µ 0 , R 0 , W 0 , ν 0 , a 0 , b 0 }, where θ = {θ 1 , ..., θ ∞ } and I = {I 1 , ..., I ∞ }.</formula><p>In the following, we will explain the details of these terms.</p><p>Figure <ref type="figure">1</ref> shows the graphical representation of MGPC. We demonstrate the model in a top-down order. For clarity, the indices of inputs and outputs are omitted. We use +1/ -1 labels for output y. Given f , the probability p(y = +1|f ) = σ(f ) where σ(•) is the logistic function. As the symmetry of the logistic function, the above probability can be written as p(y|f ) = σ(yf ). The latent variable f is given by the mixture of GPs and σ(•) is deterministic. Therefore, y also has multi-modality. For the kth component, we have a Gaussian distribution N (w k φ k (x), r -1 k ), i.e., a linear GP model as described previously. Given the mixture component k, the input x is Gaussian distributed where µ k and R k are corresponding mean and inverse covariance, respectively. Further, µ k is Gaussian distributed N (µ 0 , R -1 0 ), and R k is Wishart distributed W(W 0 , ν 0 ). z is the latent variable that indicates the component assignment for instance {x, y}.</p><p>The joint distribution of our model is</p><formula xml:id="formula_11">p(D, Ω|Θ) = ∞ k=1 p(ν k )p(w k )p(r k ) × N n=1 p(z n |ν)p(x n |z n , µ, R)p(f n |x n , z n , w, r)p(y n |f n )</formula><p>(8) where x n and f n obey the mixture of Gaussian distributions and mixture of GPs, respectively,</p><formula xml:id="formula_12">p(x n |z n , µ, R) = ∞ k=1 p(x n |z n = k, µ k , R k ) 1[zn=k] , (9) p(f n |x n , z n , w, r) = ∞ k=1 p(f n |z n = k, x n , w k , r t ) 1[zn=k] . (10) α 0 µ 0 R 0 W 0 ν 0 a 0 b 0 ν k µ k R k w k θ k r k I k z i x i f i y i k = 1 : ∞ i = 1 : N Figure 1: Graphical model representation of MGPC.</formula><p>Note that x n and f n share the infinite components. Thus, given an input x, the mixture weight of the kth component is dependent on x, which is given by</p><formula xml:id="formula_13">p(z|x) = p(z)p(x|z) z p(z)p(x|z) . (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The details of the inference and optimization of our model will be introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Inference and Optimization Algorithms</head><p>The posterior distribution p(Ω|D) is not in closed form because p(Ω, D)dΩ is intractable. So we resort to mean field variational inference to approximate p(Ω|D).</p><p>For general latent variables z and observation X, the objective function, i.e., the lower bound, is given by</p><formula xml:id="formula_15">L(q) = q(z) ln{ p(X, z|θ) q(z) }dz.<label>(12)</label></formula><p>With the mean field assumption of variational distributions, the optimization algorithm for maximizing the lower bound is an iterative procedure. Each factor is updated in turn while fixing the others, which is known as the coordinate ascent algorithm. For the ith latent variable z i , the solution is</p><formula xml:id="formula_16">q(z i ) ∝ exp{E q(z-i) log[p(z i , z -i , X)]}<label>(13)</label></formula><p>In our model, the solutions of updating q(ν), q(µ), q(R), q(w), q(r) and q(z) are in closed form. For q(f ), we adopt a gradient-based method to optimize it by maximizing the lower bound. A specific form of q(f ) is required . Then the parameters are updated according to their gradients.</p><p>All of the hyperparameters except θ in the covariance function and support set I are fixed because such hyperparameters are generic without the need for further estimation. We set such hyperparameters following <ref type="bibr" target="#b0">[Bishop and Svenskn, 2003;</ref><ref type="bibr" target="#b0">Blei and Jordan, 2006;</ref><ref type="bibr" target="#b5">Yuan and Neubauer, 2009;</ref><ref type="bibr">Sun and Xu, 2011]</ref>. The variational EM algorithm is employed for learning θ where the expectation is calculated over variational distribution and a greedy algorithm is used to update support set I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Inference</head><p>Following the fully factorization assumption and truncated stick-breaking representation of DPs, the variational distribution is formulated as</p><formula xml:id="formula_17">q(Ω) = T -1 t=1 q(ν t ) T k=1 q(w k )q(r k )q(R k )q(µ k ) N n=1 q(z n )q(f n ) (14)</formula><p>where T is the truncation level of the DP.</p><p>When optimizing variational distribution according to Eq. ( <ref type="formula" target="#formula_16">13</ref>), all of the solutions are in closed form except q(f ) <ref type="bibr" target="#b0">[Blei et al., 2016]</ref>. The derivations of variational distributions in closed form is standard and analogous to the Gaussian likelihood situation <ref type="bibr" target="#b5">[Yuan and Neubauer, 2009;</ref><ref type="bibr">Sun and Xu, 2011]</ref>. Thus, the details of such solutions are omitted in this paper. Now, we present the details of updating q(f n ). Each f n is given by a mixture of Gaussian distributions and output y n is generated by the weighted average of T components. We assume that q(f n ) has a Gaussian distribution N (µ n , σ n ), i.e., a best single Gaussian distribution is desired for approximating the original mixture of Gaussian distributions. Substituting the Gaussian probability density function into Eq. ( <ref type="formula" target="#formula_15">12</ref>) and absorbing independent terms into the constant, we obtain the following objective function,</p><formula xml:id="formula_18">L(q(f n )) = T t=1</formula><p>q(z n = t)E q(wt)q(rt)q(fn) ln p(f n |w t , x n , r t ) + E q(fn) ln p(y n |f n ) -E q(fn) ln q(f n ) + const.</p><p>(15) However, the second integral, namely, E q(fn) ln p(y n |f n ), is not in closed form. A Monte Carlo-based approximate method <ref type="bibr" target="#b2">[Gal et al., 2015]</ref> has been proposed to handle such situation with the cost of sampling. Instead, we optimize a lower bound of such integral which is obtained by E ln 1 + e -ynfn ln E 1 + e -ynfn . The first integral is analytical and the last one is the entropy of the Gaussian distribution. Thus, the surrogate objective function is formulated as follows,</p><formula xml:id="formula_19">L(q(f n )) = 1 2 T t=1 q(z n = t)[E ln r k -Er k (Ef 2 n -2Ef n Ew t φ t (x) + Ew t φ t (x)φ t (x) w t )] -E ln 1 + e -ynfn + 1 2 ln 2σ 2 n eπ + const. (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>All the expectations are calculated with respect to the variational distribution. The derivatives of parameters are provided below,</p><formula xml:id="formula_21">∂ L(q(f n )) ∂µ n = T t=1 q(z n = t)Er t µ n -Ew t φ t (x n ) - y n e 1 2 yn(-2µn+ynσ 2 n ) 1 + e 1 2 yn(-2µn+ynσ 2 n ) ,<label>(17)</label></formula><formula xml:id="formula_22">∂ L(q(f n )) ∂σ n = T t=1 q(z n = t)Er t σ n + y 2 n σ n e 1 2 yn(-2µn+ynσ 2 n ) 1 + e 1 2 yn(-2µn+ynσ 2 n ) + 1 σ n . (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>With the gradients of the parameters, the conjugate gradient method is employed for maximizing the surrogate objective function given by Eq. ( <ref type="formula" target="#formula_19">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization for Hyperparameters</head><p>Let Θ = {θ 1:T , I 1:T } be the hyperparameters to be optimized. The other hyperparameters are fixed to generic values. For covariance function variables θ, we adopt the variational EM algorithm to maximize E q(Ω) ln p(D, Ω|θ). For support sets I, we follow the method in [Smola and <ref type="bibr" target="#b4">Bartlett, 2001;</ref><ref type="bibr" target="#b5">Yuan and Neubauer, 2009;</ref><ref type="bibr">Sun and Xu, 2011]</ref>.</p><p>When optimizing θ, the objective function is</p><formula xml:id="formula_24">E{ T t=1 ln p(w t ) + N n=1 ln p(f n |x n , z n , w, r)}<label>(19)</label></formula><p>where the irrelevant terms to θ are omitted. Suppose the variational distributions of w t and f n are N (µ, Σ) and N (µ n , σ n ), respectively. Substituting the corresponding probability density functions and calculating the expectations, the objective function with respect to each θ k is simplified as</p><formula xml:id="formula_25">ln(|U t |) -tr(U k A) -b N n=1 q(z n = t)[φ t (x n ) Aφ t (x n ) -2µ n φ t (x n ) µ] (20) where A = (Σ + µµ ), b = Er t .</formula><p>Then we maximize the objective function through the conjugate gradient method. The derivation of gradients are omitted.</p><p>The support sets I are optimized by a greedy algorithm <ref type="bibr" target="#b4">[Smola and Bartlett, 2001;</ref><ref type="bibr" target="#b5">Yuan and Neubauer, 2009]</ref>. For each support set I t , the objective is the density of q(w t ) at its mean. As w t has a Gaussian distribution, the above objective is equivalent to maximizing the determinant of the inverse covariance matrix. The instances are greedily selected for maximizing the objective from candidate sets which are randomly sampled from the training set. Now, we introduce the whole procedure of model training. First, the hyperparameters Θ are initialized. The support sets are initialized by the K-means algorithm which clusters the training set into T components. Then variational inference is run to obtain the approximate posterior distribution q(Ω). The variational EM algorithm is performed to update θ. Fixing θ and variational distributions except q(w), each support set I t is filled by the greedy algorithm in turn. Based on the updated support sets, the above steps are repeated until the variations of the support sets are under a predefined threshold or the maximum iteration number is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predictive Distribution</head><p>The predictive distribution for a new input x * is given by</p><formula xml:id="formula_26">p(y * = +1) = σ(f * )p(f * |x * , D, Ω, Θ)df * .<label>(21)</label></formula><p>Approximations are necessary for computations of both p(f * |x * , D, Ω, Θ) and the integral. For p(f * |x * , D, Ω, Θ), we approximate it as follows,</p><formula xml:id="formula_27">p(f * |x * , D, Ω, Θ) = p(f * |x * , Ω, Θ)p(Ω|D, Θ)dΩ p(f * |x * , Ω, Θ)q(Ω)dΩ p(f * |x * , f , Ω\f , Θ)q(f )df = T t=1 p(z * = t|x * , Ω) p(f * |x * , z * = t, f , Ω\f , Θ)q(f )df (22)</formula><p>where the true posterior distribution is approximated by the variational distribution and then posterior means Ω are further employed. As the parametric representation of GPs, we have the conditional independence of the outputs f . Thus the predictive distribution is simplified as,</p><formula xml:id="formula_28">T t=1 p(z * = t|x * , Ω) σ(f * )N (f * | ŵ t φ t (x), r-1 t )df * .</formula><p>(23) Because the integral in Eq. ( <ref type="formula">23</ref>) is intractable, we adopt the approximate method from <ref type="bibr" target="#b0">[Bishop, 2006]</ref> as</p><formula xml:id="formula_29">σ(f * )N (f * | ŵ t φ t (x), r-1 t )df * σ((1 + πr -1 t /8) -1/2 ŵ t φ t (x)).<label>(24)</label></formula><p>The mixture weight p(z * = t|x * , Ω) is calculated as in Eq. ( <ref type="formula" target="#formula_13">11</ref>) with posterior means of the variational distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our proposed model MGPC on multiple real-world datasets and compare it with existing classification models including Gaussian Process Classification models (GPC), SVM and Logistic Regression (LR). As mentioned in Section 1, regression models can also perform binary classification. We evaluate classification performances of Gaussian Process Regression models (GPR) and Mixtures of Gaussian Processes for Regression (MGPR) <ref type="bibr">[Sun and Xu, 2011]</ref>. We report the experimental results with corresponding analyses from three viewpoints: comparisons of classification performances, classification versus regression models, and mixture versus single models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setups</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the information about the used datasets. All of the datasets are available on UCI data repository <ref type="bibr" target="#b3">[Lichman, 2013]</ref>. The iris dataset has 3 classes in total. We perform experiment on the instances of label "Versicolour" and "Virginica" because the classification accuracies are consistently 100% on other combinations for each model. All of the datasets are randomly split into the training, validation and test set by a ratio of 4:3:3. The truncation level T and the initializations for variance parameters of q(f n ) are selected using the validation set. T is set to range from 2 to 4, and the corresponding size of the support set for each component is set to N train /T . The variance σ n are initially set to range in 0.005 <ref type="bibr">× [1, 2, 4, 8, 16, 32]</ref>. We run experiments on randomly split datasets for 10 times and report the average accuracies in percentage with corresponding standard deviations in Table <ref type="table" target="#tab_2">2</ref>. The comparisons of predictive log likelihoods of MGPC, MGPR, GPC and GPR on the test sets are also provided in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Performances</head><p>From Table <ref type="table" target="#tab_2">2</ref>, we can see that MGPC outperforms all of the other models on 7/9 datasets. For the rest of the datasets, GPC and GPR obtain the best performance, respectively. We also run paired t-test on average accuracies over all datasets for further comparisons of MGPC and other models. The results are reported in Table <ref type="table" target="#tab_3">3</ref>. As we can see, all of the p-values are less than 5%, which indicates the significant improvements of MGPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification versus Regression Models</head><p>For binary classification, regression models are also amenable. Empirical results show that the classification performances of GPC and GPR are typically comparable <ref type="bibr">[Kapoor et al., 2010]</ref>. For further evaluating performance differences between classification and regression models, we evaluate regression models including MGPR and GPR and compare the performances with MGPC and GPC, respectively. The classification accuracies have been shown in Table 2 as well as the average predictive log likelihoods with standard deviations in Figure <ref type="figure" target="#fig_0">2</ref>. For clarifying the performance differences between classification and regression models, paired t-test results are indicated for each datasets with arrows, respectively.  As shown in Table <ref type="table" target="#tab_2">2</ref>, the performances of GPC and GPR are not significantly different, which is in accordance with previous empirical consensus. However, this phenomenon is not preserved for MGPC and MGPR. Significant improvements are obtained by MGPC on 4/9 datasets.</p><p>Additionally, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, the predictive log likelihoods for classification models are much larger than that for regression models. The regression models make predictions according to the signs of outputs and do not evaluate probabilities of predicted labels directly, which leads to inferior estimations of the distribution of the test data. When the predictive distributions are highly aggregated at wrong labels (i.e., wrongly classifying test instances in high confidence), the likelihoods will be close to 0, which leads to small log likelihoods. Another outcome from Figure <ref type="figure" target="#fig_0">2</ref> is that mixture models have a lower variance of predictive log likelihoods over single models across the used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mixture versus Single Models</head><p>We further compare the differences of mixture and single models. The classification accuracies of MGPC are higher than GPC on 8/9 datasets, which shows the advantage of our model. But for average log likelihoods, the differences are not conclusive other than that generally a lower variance on each dataset is obtained with mixture models. MGPR and GPR are not significantly different for classification performances, and the mixture model has a lower variance of predictive log likelihoods across different datasets as stated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>We have presented comparisons of MGPC against other models. Now, we turn to discuss the behaviours of different truncation level T . In the experiments, we set the size of the support set to N train /T and select appropriate truncated level T according to the performances on the validation set rather than inferring it from data. Actually, this setting for support sets could hinder the capability of DPs to converge to appropriate T . Because different T leads to different sizes of support sets. When the dataset is small, large T leads to small support sets which are insufficient for learning each component. Thus, a small T will be preferred for comparatively small datasets. Only when a sufficient support set is provided, large T will be possible. Although more refined methods of specifying support sets could be tried, the current experimental results have already shown the advantages of MGPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented MGPC with mean field variational inference learning algorithms. MGPC is constructed in a fully generative way where inputs and outputs are modeled by the mixture of Gaussian distributions and mixture of GPs, respectively. Different from previous mixture models of GPs, MGPC employs the logistic likelihood which is suitable for binary classification. The improvements of MGPC have been shown from the experiments on multiple real-world datasets, from which we also get some interesting findings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average predictive log likelihoods. The average predictive log-likelihoods with deviations are plotted in the same order as Tabel 1. For clarity, the names of the datasets are omitted. In the right figure, four points which have extremely small values are omitted for GPR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. All of the previous work about mixtures of GPs only fo-cuses on regression problems, i.e., the Gaussian likelihood is employed. Inspired by the GP classification model and mixtures of GPs, we propose a generative Mixture of Gaussian Processes for Classification (MGPC) which extends mixtures of GP regression models to a classification model. Similar with Sun and Xu[2011], a linear GP model is employed. The linear GP model is equivalent to GPs and breaks the dependencies among outputs. This property enables mean field variational inference for mixtures of GPs feasible. We validate our model on multiple binary classification datasets. Because binary classification problems can be regard as restricted regression problems whose outputs only take values in {-1, +1}. The mentioned regression models, including the GP regression model and mixture model of GPs, are also amenable for classification. The signs of outputs of such models are used for making predictions. The experiments are performed with both classification and regression models for comprehensive comparisons.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset description.</figDesc><table><row><cell></cell><cell cols="2"># of instances # of features</cell></row><row><cell>Blood</cell><cell>748</cell><cell>3</cell></row><row><cell>Fertility</cell><cell>100</cell><cell>9</cell></row><row><cell>Haberman</cell><cell>306</cell><cell>3</cell></row><row><cell>Housevotes</cell><cell>435</cell><cell>16</cell></row><row><cell cols="2">Mammographic 830</cell><cell>5</cell></row><row><cell>Parkinsons</cell><cell>195</cell><cell>22</cell></row><row><cell>Pima</cell><cell>768</cell><cell>8</cell></row><row><cell>Heart</cell><cell>270</cell><cell>13</cell></row><row><cell>Iris</cell><cell>150</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies for UCI datasets. The p-values of the paired t-test over accuracies obtained by and MGPR are listed. For single models, such results are not shown because there are no significant differences on all datasets with threshold 5%.</figDesc><table><row><cell>Dataset</cell><cell>MGPC</cell><cell cols="2">Mixture Model MGPR</cell><cell>GPC</cell><cell cols="2">Single Model GPR</cell><cell>SVM</cell><cell>LR</cell></row><row><cell>Blood</cell><cell cols="2">78.50 ± 2.53(0.4681)</cell><cell cols="3">77.63 ± 2.49 77.92 ± 3.36</cell><cell>77.79 ± 3.55</cell><cell>77.78 ± 2.43 77.74 ± 3.25</cell></row><row><cell>Fertility</cell><cell cols="2">88.00 ± 4.83(0.0575)</cell><cell cols="3">84.67 ± 5.26 85.67 ± 4.98</cell><cell>85.33 ± 5.49</cell><cell>86.67 ± 4.71 80.33 ± 6.56</cell></row><row><cell>Haberman</cell><cell cols="2">75.41 ± 4.67(0.1724)</cell><cell cols="3">73.68 ± 3.04 72.80 ± 2.88</cell><cell>72.13 ± 2.23</cell><cell>73.13 ± 2.48 72.68 ± 2.34</cell></row><row><cell>Housevotes</cell><cell cols="5">95.33 ± 1.96(0.0386) 93.27 ± 3.97 94.48 ± 2.37</cell><cell>94.33 ± 2.28</cell><cell>95.10 ± 2.27 94.33 ± 1.83</cell></row><row><cell cols="3">Mammographic 83.49 ± 2.31(0.6380)</cell><cell cols="3">83.21 ± 1.81 84.02 ± 2.62</cell><cell cols="2">84.62 ± 2.26 81.57 ± 1.91 82.37 ± 2.51</cell></row><row><cell>Parkinsons</cell><cell cols="5">87.56 ± 2.49(0.0276) 82.92 ± 4.17 86.59 ± 5.38</cell><cell>83.34 ± 5.79</cell><cell>85.89 ± 5.27 75.23 ± 8.43</cell></row><row><cell>Pima</cell><cell cols="5">76.91 ± 3.34(0.0014) 73.57 ± 1.45 75.28 ± 3.18</cell><cell>74.80 ± 3.47</cell><cell>76.33 ± 2.15 76.11 ± 1.67</cell></row><row><cell>Heart</cell><cell cols="2">80.62 ± 3.19(0.0437)</cell><cell cols="4">76.05 ± 5.15 80.74 ± 4.20 78.77 ± 4.50</cell><cell>80.59 ± 3.79 75.56 ± 5.91</cell></row><row><cell>Iris</cell><cell cols="2">96.00 ± 3.06(0.3938)</cell><cell cols="3">95.00 ± 2.83 93.67 ± 4.29</cell><cell>92.33 ± 3.53</cell><cell>94.67 ± 4.77 94.33 ± 3.87</cell></row><row><cell>MGPR</cell><cell>GPC</cell><cell>GPR</cell><cell>SVM</cell><cell>LR</cell><cell></cell><cell></cell></row><row><cell cols="5">MGPC 0.0020 0.0132 0.0063 0.0026 0.0247</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>P-values of paired t-test.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Proceedings of the Twenty-Sixth International Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The corresponding author <rs type="person">Shiliang Sun</rs> thanks supports from <rs type="funder">NSFC</rs> Projects <rs type="grantNumber">61673179</rs> and <rs type="grantNumber">61370175</rs>, <rs type="funder">Shanghai Knowledge Service Platform Project</rs> (No. <rs type="grantNumber">ZF1213</rs>), and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_46Wtk6M">
					<idno type="grant-number">61673179</idno>
				</org>
				<org type="funding" xml:id="_thuGrJr">
					<idno type="grant-number">61370175</idno>
				</org>
				<org type="funding" xml:id="_Nruv8Cx">
					<idno type="grant-number">ZF1213</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jordan. Variational inference for Dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Svenskn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Svenskn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I ;</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauliffe</surname></persName>
		</author>
		<idno>ArXiv e-prints:1601.00670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><surname>Blei</surname></persName>
		</editor>
		<meeting>the 19th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2003. 2006. 2006. 2006. 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="144" />
		</imprint>
	</monogr>
	<note>Variational inference: A review for statisticians</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973">1973. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Gaussian processes for distribution estimation of multivariate categorical data</title>
		<author>
			<persName><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<editor>
			<persName><surname>Kapoor</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2015. 2015. 2015. 2015. 2010</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="169" to="188" />
		</imprint>
	</monogr>
	<note>Gaussian processes for object categorization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An alternative infinite mixture of Gaussian process experts</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">M</forename><surname>Lichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Carl Edward Rasmussen and Zoubin Ghahramani</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2002">2013. 2013. 2006. 2006. 2002. 2002. 2006. 2006</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
	<note>Gaussian Processes for Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sun and Xu, 2011] Shiliang Sun and Xin Xu. Variational inference for infinite mixtures of Gaussian processes with applications to traffic flow prediction</title>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Sethuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Sethuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="654" to="660" />
			<date type="published" when="1994">1994. 1994. 2001. 2001. 2011. 2001</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational mixture of Gaussian process experts</title>
		<author>
			<persName><forename type="first">Neubauer</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claus</forename><surname>Neubauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1897" to="1904" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
